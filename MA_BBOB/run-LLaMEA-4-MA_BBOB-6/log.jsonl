{"id": "4b01b70d-a9f2-4e10-b69b-d160d643906a", "fitness": -Infinity, "name": "HybridDE_NM", "description": "Population-based algorithm that combines the exploration of Differential Evolution with the exploitation of a Nelder-Mead simplex.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridDE_NM:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.7, Cr=0.9, nm_restarts=3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.Cr = Cr\n        self.nm_restarts = nm_restarts\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        \n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.eval_count += self.pop_size\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Differential Evolution mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = population[a] + self.F * (population[b] - population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        \n            # Nelder-Mead local search on the best individual\n            if self.eval_count < self.budget:\n                best_idx = np.argmin(fitness)\n                x0 = population[best_idx].copy()\n                \n                # Define a local function to minimize\n                def local_func(x):\n                    return func(x)\n                \n                # Nelder-Mead optimization with restarts\n                for _ in range(self.nm_restarts):\n                    if self.eval_count < self.budget:\n                         res = minimize(local_func, x0, method='Nelder-Mead',\n                                        options={'maxfev': (self.budget - self.eval_count) // self.nm_restarts})\n\n                         if res.success:\n                             if res.fun < self.f_opt:\n                                 self.f_opt = res.fun\n                                 self.x_opt = res.x\n                             x0 = np.random.uniform(self.lb, self.ub, size=self.dim) #restart from new location\n                             self.eval_count += res.nfev\n                         else:\n                             self.eval_count += (self.budget - self.eval_count) // self.nm_restarts #account for budget used\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "An exception occurred: name 'minimize' is not defined.", "error": "", "parent_ids": [], "operator": null, "metadata": {}}
{"id": "144ad936-3228-4dd2-be9e-be846b82affd", "fitness": -Infinity, "name": "CMAES", "description": "Covariance matrix adaptation evolution strategy with mirrored sampling and a repair mechanism to keep solutions within bounds.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damp=None, c_cov=None, c_mu=None):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.m = None\n        self.sigma = 0.3  # exploration parameter\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.chiN = None\n\n        self.cs = cs\n        self.damp = damp if damp is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1]) + self.cs\n        self.c_cov = c_cov if c_cov is not None else 2 / ((self.dim + np.sqrt(2))**2)\n        self.c_mu = c_mu if c_mu is not None else min(1 - self.c_cov, 2 * (self.weights[0] - 1 + self.weights[0]**2) / ((self.mu + 6 + 6*self.dim**0.5)))\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1 / (4 * self.dim) + 1 / (21 * self.dim**2))\n        \n        used_budget = 0\n        while used_budget < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.m[:, np.newaxis] + self.sigma * C_sqrt @ z\n            \n            # Mirroring to handle bounds\n            xmir = self.m[:, np.newaxis] - self.sigma * C_sqrt @ z\n            \n            # Repair mechanism to ensure solutions are within bounds\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            xmir = np.clip(xmir, func.bounds.lb, func.bounds.ub)\n\n            f = np.array([func(x[:, i]) if used_budget < self.budget else np.inf for i in range(self.popsize)])\n            fmir = np.array([func(xmir[:, i]) if used_budget + self.popsize < self.budget else np.inf for i in range(self.popsize)])\n            \n            used_budget += 2 * self.popsize\n            \n            if np.min(f) < np.min(fmir):\n                y = x\n                fitness = f\n                best_index = np.argmin(f)\n                if f[best_index] < self.f_opt:\n                    self.f_opt = f[best_index]\n                    self.x_opt = x[:, best_index]\n            else:\n                y = xmir\n                fitness = fmir\n                best_index = np.argmin(fmir)\n                if fmir[best_index] < self.f_opt:\n                    self.f_opt = fmir[best_index]\n                    self.x_opt = xmir[:, best_index]\n                    \n            \n            arindex = np.argsort(fitness)\n            xmu = y[:, arindex[:self.mu]]\n            zmu = z[:, arindex[:self.mu]]\n            \n            y_w = xmu - self.m[:, np.newaxis]\n            m_new = np.sum(xmu * self.weights[np.newaxis, :], axis=1)\n            \n            ps_new = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs)) * (C_sqrt @ np.sum(zmu * self.weights[np.newaxis, :], axis=1))\n            self.ps = ps_new\n            \n            pc_new = (1 - self.c_cov) * self.pc + np.sqrt(self.c_cov * (2 - self.c_cov)) * (m_new - self.m) / self.sigma\n            self.pc = pc_new\n\n            self.m = m_new\n\n            norm_ps = np.linalg.norm(self.ps)\n            factor = self.c_cov * (1 / (np.min([1, (1000000 * norm_ps**4 / self.dim)])))\n            \n            self.C = (1 - self.c_cov - self.c_mu) * self.C + self.c_cov * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :]) + self.c_mu * (xmu - self.m[:, np.newaxis]) @ np.diag(self.weights) @ (xmu - self.m[:, np.newaxis]).T / self.sigma**2\n            self.sigma *= np.exp((self.cs / self.damp) * (norm_ps / self.chiN - 1))\n            \n            # Ensure C remains positive definite\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)  # Reset covariance matrix if it becomes non-positive definite\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "An exception occurred: operands could not be broadcast together with shapes (2,6) (2,) (2,) .", "error": "", "parent_ids": [], "operator": null, "metadata": {}}
{"id": "f94d28d2-78ef-4c75-85b4-2bef775fb513", "fitness": 0.0, "name": "AdaptiveHybridOptimization", "description": "Population-based algorithm that combines elements of particle swarm optimization and differential evolution, adaptively adjusting parameters based on population diversity and fitness improvements.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimization:\n    def __init__(self, budget=10000, dim=10, pop_size=20, w=0.7, c1=1.5, c2=1.5, cr=0.7, f=0.8):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w = w  # Inertia weight\n        self.c1 = c1  # Cognitive coefficient\n        self.c2 = c2  # Social coefficient\n        self.cr = cr  # Crossover rate\n        self.f = f    # Mutation factor\n        self.population = None\n        self.fitness = None\n        self.best_positions = None\n        self.global_best_fitness = np.inf\n        self.global_best_position = None\n        self.eval_count = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        self.best_positions = self.population.copy()\n        self.global_best_position = self.population[np.argmin(self.fitness)].copy()\n        self.global_best_fitness = np.min(self.fitness)\n\n    def update_parameters(self):\n        # Adaptive parameter control based on population diversity and fitness improvement\n        diversity = np.std(self.population)\n        fitness_improvement = self.global_best_fitness - np.min(self.fitness)\n\n        # Adjust inertia weight\n        self.w = 0.9 - 0.2 * (diversity / np.max(np.std(self.population, axis=0)))\n\n        # Adjust acceleration coefficients\n        self.c1 = 1.5 + 0.5 * (fitness_improvement / abs(self.global_best_fitness + 1e-8))\n        self.c2 = 1.5 + 0.5 * (fitness_improvement / abs(self.global_best_fitness + 1e-8))\n        \n        self.f = 0.8 + 0.2 * (diversity / np.max(np.std(self.population, axis=0)))\n\n    def pso_step(self):\n        velocity = (self.w * np.random.rand(self.pop_size, self.dim) * (self.best_positions - self.population) +\n                    self.c1 * np.random.rand(self.pop_size, self.dim) * (self.best_positions - self.population) +\n                    self.c2 * np.random.rand(self.pop_size, self.dim) * (self.global_best_position - self.population))\n        return velocity\n\n    def de_step(self):\n         # Differential Evolution Mutation\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_1, x_2, x_3 = self.population[idxs]\n            v_i = x_1 + self.f * (x_2 - x_3)\n            \n            # Crossover\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() > self.cr and j != j_rand:\n                    v_i[j] = self.population[i, j]\n            \n            yield i, v_i\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            self.update_parameters()\n            \n            # PSO part\n            velocity = self.pso_step()\n            new_population_pso = self.population + velocity\n            new_population_pso = np.clip(new_population_pso, func.bounds.lb, func.bounds.ub)\n\n\n            # DE Part\n            new_population_de = self.population.copy()\n            for i, v_i in self.de_step():\n                new_population_de[i] = v_i\n            new_population_de = np.clip(new_population_de, func.bounds.lb, func.bounds.ub)\n\n\n            # Evaluate and Selection\n            fitness_pso = np.array([func(x) for x in new_population_pso])\n            fitness_de = np.array([func(x) for x in new_population_de])\n            self.eval_count += 2*self.pop_size\n\n\n            for i in range(self.pop_size):\n                # Compete PSO and DE offspring\n                if fitness_pso[i] < fitness_de[i]:\n                    if fitness_pso[i] < self.fitness[i]:\n                        self.population[i] = new_population_pso[i]\n                        self.fitness[i] = fitness_pso[i]\n                        if fitness_pso[i] < self.global_best_fitness:\n                            self.global_best_fitness = fitness_pso[i]\n                            self.global_best_position = new_population_pso[i].copy()\n                            self.best_positions[i] = new_population_pso[i].copy()\n                    elif self.fitness[i] < self.global_best_fitness:\n                      self.best_positions[i] = self.population[i].copy()\n                else:\n                   if fitness_de[i] < self.fitness[i]:\n                        self.population[i] = new_population_de[i]\n                        self.fitness[i] = fitness_de[i]\n                        if fitness_de[i] < self.global_best_fitness:\n                            self.global_best_fitness = fitness_de[i]\n                            self.global_best_position = new_population_de[i].copy()\n                            self.best_positions[i] = new_population_de[i].copy()\n                   elif self.fitness[i] < self.global_best_fitness:\n                      self.best_positions[i] = self.population[i].copy()\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveHybridOptimization scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0]}}
{"id": "5982128c-a5fc-432c-8fe7-df9c6473696d", "fitness": -Infinity, "name": "CMAES", "description": "Covariance Matrix Adaptation Evolution Strategy with adaptive step size control and active covariance matrix update.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damps=None, c_cov=None, c_mu=None, mu_eff=None):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n\n        # Strategy parameter setting: Selection\n        self.mu = self.popsize // 2           # number of parents/individuals for recombination\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)   # normalize recombination weights array\n        self.mu_eff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n        # Strategy parameter setting: Adaptation\n        self.cs = cs  # damping for step-size increasing\n        self.damps = damps if damps is not None else 1 + 2 * max(0, np.sqrt((self.mu_eff - 1) / (self.dim + 1)) - 1) + self.cs # damping for step-size decreasing\n        self.c_cov = c_cov if c_cov is not None else (1 / 3) * (self.mu_eff / (self.dim + 1.3) + (1 - self.mu_eff / (self.dim + 1.3)) * self.cs) # learning rate for rank-one update\n        self.c_mu = c_mu if c_mu is not None else min(1 - self.c_cov, 2 * (self.mu_eff - 2 + 1/self.mu_eff) / ((self.dim + 2)**2 + self.mu_eff))  # and for rank-mu update\n        \n        # Initialize dynamic (internal) strategy parameters and constants\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.B = np.eye(self.dim)       # B defines the coordinate system\n        self.D = np.ones(self.dim)       # diagonal D defines the scaling\n        self.C = np.eye(self.dim)       # covariance matrix C = B diag(D.^2) B'\n        self.chiN = self.dim**0.5 * (1 - (1/(4*self.dim)) + 1/(21*self.dim**2))  # expectation of ||N(0,I)|| == norm(randn(dim,1))\n\n        self.sigma = 0.3 # step size\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        self.n_evals = 0\n        while self.n_evals < self.budget:\n            # Generate and evaluate lambda offspring\n            x = np.random.randn(self.dim, self.popsize)\n            arz = self.sigma * self.B.dot(self.D * x)\n            arx = func.bounds.clip(np.tile(self.x_opt if self.x_opt is not None else (func.bounds.ub + func.bounds.lb) / 2, (self.popsize, 1)).T + arz)\n            \n            arfitness = np.array([func(arx[:, i]) for i in range(self.popsize)])\n            self.n_evals += self.popsize\n\n            # Sort by fitness and update mean\n            idx = np.argsort(arfitness)\n            arfitness = arfitness[idx]\n            x = x[:, idx]\n            xmean = np.sum(x[:, :self.mu] * self.weights, axis=1)\n\n            # Cumulation: Update evolution paths\n            self.ps = (1 - self.cs) * self.ps + (np.sqrt(self.cs * (2 - self.cs) * self.mu_eff) / self.sigma) * self.B.dot(self.D * xmean)\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * (self.n_evals / self.popsize))) / self.chiN) < (1.4 + 2 / (self.dim + 1))\n            self.pc = (1 - self.c_cov) * self.pc + hsig * np.sqrt(self.c_cov * (2 - self.c_cov) * self.mu_eff) * self.B.dot(xmean)\n\n            # Adapt covariance matrix C\n            artmp = (1/self.sigma) * self.B.dot(x[:, :self.mu])\n            self.C = (1 - self.c_cov - self.c_mu) * self.C + self.c_cov * (self.pc[:, None].dot(self.pc[None, :]) + (1 - hsig) * self.c_cov * self.C) + self.c_mu * artmp.dot(np.diag(self.weights)).dot(artmp.T)\n\n            # Adapt step size sigma\n            self.sigma = self.sigma * np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            \n            # Decomposition of C (updates B and D)\n            if self.n_evals > self.budget:\n                break\n            try:\n                self.D, self.B = np.linalg.eig(self.C)\n                self.D = np.sqrt(np.abs(self.D)) # Robustness: keep sigma > 0\n            except np.linalg.LinAlgError:\n                self.D = np.ones(self.dim)\n                self.B = np.eye(self.dim)\n                \n            # Update optimal values\n            if arfitness[0] < self.f_opt:\n                self.f_opt = arfitness[0]\n                self.x_opt = arx[:, idx[0]]\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "An exception occurred: operands could not be broadcast together with shapes (2,) (2,6) .", "error": "", "parent_ids": [], "operator": null, "metadata": {}}
{"id": "5bf8233c-3f92-436e-9259-dfbd6bb5f3f1", "fitness": -Infinity, "name": "AdaptiveSOMA", "description": "Implements a Self-Organizing Migrating Algorithm (SOMA) with adaptive step size and migration loop length.", "code": "import numpy as np\n\nclass AdaptiveSOMA:\n    def __init__(self, budget=10000, dim=10, pop_size=20, path_length=3.0, step_size=0.1, perturbation=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.path_length = path_length  # Maximum distance an individual can move in one migration loop\n        self.step_size = step_size  # Resolution of movement along the path\n        self.perturbation = perturbation # Probability of random perturbation\n        self.population = None\n        self.fitness = None\n        self.leader = None\n        self.eval_count = 0\n        self.migration_loop_length = 10\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        self.leader = self.population[np.argmin(self.fitness)].copy()\n\n    def migrate(self, func):\n        for i in range(self.pop_size):\n            if np.all(self.population[i] == self.leader):\n                continue  # Skip the leader\n            \n            for step in np.arange(self.step_size, self.path_length + self.step_size, self.step_size):\n                new_position = self.population[i] + step * (self.leader - self.population[i])\n                \n                # Perturbation\n                for j in range(self.dim):\n                    if np.random.rand() < self.perturbation:\n                        new_position[j] = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                \n                new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n                \n                new_fitness = func(new_position)\n                self.eval_count += 1\n\n                if new_fitness < self.fitness[i]:\n                    self.population[i] = new_position\n                    self.fitness[i] = new_fitness\n\n                    if new_fitness < np.min(self.fitness):\n                        self.leader = self.population[np.argmin(self.fitness)].copy()\n        \n\n    def update_parameters(self):\n        # Adapt migration loop length based on stagnation\n        if self.eval_count % (self.pop_size * self.migration_loop_length) == 0:\n            unique_fitness_values = len(np.unique(self.fitness))\n            if unique_fitness_values < self.pop_size / 4:\n                self.migration_loop_length = min(self.migration_loop_length * 2, 50)  # Increase if stagnant\n            else:\n                self.migration_loop_length = max(self.migration_loop_length // 2, 5)   # Decrease if diverse\n\n        # Adapt step size based on leader fitness improvement\n        if self.eval_count > self.pop_size:\n            previous_leader_fitness = self.fitness[np.argmin(self.fitness)] #previous leader fitness\n            current_leader_fitness = np.min(self.fitness)\n\n            if current_leader_fitness < previous_leader_fitness:\n                self.step_size = min(self.step_size * 1.1, 0.5)  # Increase step size if improvement\n            else:\n                self.step_size = max(self.step_size * 0.9, 0.01) # Decrease step size if no improvement\n        \n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            self.update_parameters()\n            self.migrate(func)\n\n        return np.min(self.fitness), self.leader", "configspace": "", "generation": 1, "feedback": "An exception occurred: setting an array element with a sequence..", "error": "", "parent_ids": ["f94d28d2-78ef-4c75-85b4-2bef775fb513"], "operator": null, "metadata": {}}
{"id": "14b13a0f-143a-4c00-866b-4bdcb7381647", "fitness": -Infinity, "name": "HybridDE_NM", "description": "Combines Differential Evolution with Nelder-Mead, using adaptive Nelder-Mead restarts based on the remaining budget and population diversity, and implements a repair mechanism to keep solutions within bounds.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridDE_NM:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.7, Cr=0.9, nm_restarts=3, nm_reduction_factor=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.Cr = Cr\n        self.nm_restarts = nm_restarts\n        self.lb = -5.0\n        self.ub = 5.0\n        self.nm_reduction_factor = nm_reduction_factor\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        \n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.eval_count += self.pop_size\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Differential Evolution mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = population[a] + self.F * (population[b] - population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Repair mechanism: Ensure trial solution is within bounds\n                trial = np.clip(trial, self.lb, self.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        \n            # Nelder-Mead local search on the best individual, adaptive restarts\n            if self.eval_count < self.budget:\n                best_idx = np.argmin(fitness)\n                x0 = population[best_idx].copy()\n                \n                # Define a local function to minimize\n                def local_func(x):\n                    return func(x)\n                \n                # Adaptive Nelder-Mead optimization with restarts, reduce nm_restarts if near the budget\n                remaining_evals = self.budget - self.eval_count\n                adaptive_nm_restarts = min(self.nm_restarts, remaining_evals // 100)  #Reduce restarts as budget diminishes\n\n                for _ in range(adaptive_nm_restarts):\n                    if self.eval_count < self.budget:\n                         maxfev_allowed = (self.budget - self.eval_count) // (adaptive_nm_restarts - _) #Ensure restarts can complete\n                         res = minimize(local_func, x0, method='Nelder-Mead',\n                                        options={'maxfev': maxfev_allowed})\n\n                         if res.success:\n                             if res.fun < self.f_opt:\n                                 self.f_opt = res.fun\n                                 self.x_opt = res.x\n                             x0 = np.random.uniform(self.lb, self.ub, size=self.dim) #restart from new location\n                             self.eval_count += res.nfev\n                         else:\n                             self.eval_count += res.nfev\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "An exception occurred: name 'minimize' is not defined.", "error": "", "parent_ids": ["4b01b70d-a9f2-4e10-b69b-d160d643906a"], "operator": null, "metadata": {}}
{"id": "af4a3163-f999-47d6-ad61-68d23c6a50ed", "fitness": 0.0, "name": "PSO_SA", "description": "A population-based algorithm that combines the advantages of Particle Swarm Optimization (PSO) and Simulated Annealing (SA), using PSO for exploration and SA for local refinement.", "code": "import numpy as np\n\nclass PSO_SA:\n    def __init__(self, budget=10000, dim=10, pop_size=20, w=0.7, c1=1.5, c2=1.5, temp_init=1.0, temp_decay=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w = w  # Inertia weight\n        self.c1 = c1  # Cognitive coefficient\n        self.c2 = c2  # Social coefficient\n        self.temp_init = temp_init # Initial temperature for SA\n        self.temp_decay = temp_decay # Temperature decay rate\n        self.population = None\n        self.fitness = None\n        self.velocities = None\n        self.best_positions = None\n        self.global_best_fitness = np.inf\n        self.global_best_position = None\n        self.eval_count = 0\n        self.temperature = temp_init\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        self.velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))  # Initialize velocities\n        self.best_positions = self.population.copy()\n        self.global_best_position = self.population[np.argmin(self.fitness)].copy()\n        self.global_best_fitness = np.min(self.fitness)\n\n    def pso_step(self, func):\n        self.velocities = (self.w * self.velocities +\n                           self.c1 * np.random.rand(self.pop_size, self.dim) * (self.best_positions - self.population) +\n                           self.c2 * np.random.rand(self.pop_size, self.dim) * (self.global_best_position - self.population))\n        new_population = self.population + self.velocities\n        new_population = np.clip(new_population, func.bounds.lb, func.bounds.ub)\n        return new_population\n\n    def simulated_annealing_step(self, func, x, f):\n        x_new = x + np.random.normal(0, 0.1, size=self.dim)  # Generate a neighbor\n        x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n        f_new = func(x_new)\n        self.eval_count += 1\n\n        if f_new < f:\n            return x_new, f_new\n        else:\n            acceptance_probability = np.exp((f - f_new) / self.temperature)\n            if np.random.rand() < acceptance_probability:\n                return x_new, f_new\n            else:\n                return x, f\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            # PSO step\n            new_population = self.pso_step(func)\n            new_fitness = np.array([func(x) for x in new_population])\n            self.eval_count += self.pop_size\n\n            # Selection and update\n            for i in range(self.pop_size):\n                # Simulated Annealing for local refinement\n                x_refined, f_refined = self.simulated_annealing_step(func, new_population[i].copy(), new_fitness[i])\n                \n                if f_refined < self.fitness[i]:\n                    self.population[i] = x_refined\n                    self.fitness[i] = f_refined\n                    if f_refined < self.global_best_fitness:\n                        self.global_best_fitness = f_refined\n                        self.global_best_position = x_refined.copy()\n                \n                # Update personal best\n                if self.fitness[i] < self.fitness[i]: # Previously best_positions was compared to fitness again. Corrected now.\n                    self.best_positions[i] = self.population[i].copy()\n                \n                #Global Best is updated after SA\n                if self.fitness[i] < self.global_best_fitness:\n                    self.global_best_fitness = self.fitness[i]\n                    self.global_best_position = self.population[i].copy()\n            \n            #Cooling\n            self.temperature *= self.temp_decay\n\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 1, "feedback": "The algorithm PSO_SA scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f94d28d2-78ef-4c75-85b4-2bef775fb513"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "385a4711-bc06-4397-bdc0-293bab47bb05", "fitness": -Infinity, "name": "HybridDE_NM", "description": "Combines Differential Evolution with Nelder-Mead, using adaptive parameters and a restart strategy for Nelder-Mead to better explore the search space.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridDE_NM:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.7, Cr=0.9, nm_restarts=3, adapt_params=True):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.Cr = Cr\n        self.nm_restarts = nm_restarts\n        self.lb = -5.0\n        self.ub = 5.0\n        self.adapt_params = adapt_params  # Flag for adaptive parameter adjustment\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        \n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.eval_count += self.pop_size\n\n        archive = [] # Archive for storing discarded solutions\n        archive_fitness = []\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Differential Evolution mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = population[a] + self.F * (population[b] - population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.eval_count += 1\n                if f_trial < fitness[i]:\n                    # Store the replaced individual in the archive\n                    archive.append(population[i].copy())\n                    archive_fitness.append(fitness[i])\n                    \n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                else:\n                    # If the trial vector is not better, archive it\n                    archive.append(trial)\n                    archive_fitness.append(f_trial)\n                    \n\n            # Adaptive parameter adjustment (optional)\n            if self.adapt_params and self.eval_count % (self.pop_size * 5) == 0: # Adapt every 5 generations\n                if len(archive) > 0:\n                    # Adjust F and Cr based on the success of previous trials\n                    successful_F = []\n                    successful_Cr = []\n\n                    for i in range(len(archive)):\n                        if archive_fitness[i] < fitness[np.argmin(fitness)]: # if the archived solution is better than the current worst in population\n                            successful_F.append(self.F)  # Assume current F and Cr led to this result (simplified)\n                            successful_Cr.append(self.Cr)\n\n                    if successful_F:\n                        self.F = np.mean(successful_F)\n                        self.Cr = np.mean(successful_Cr)\n\n                    # Reset archive\n                    archive = []\n                    archive_fitness = []\n                        \n            # Nelder-Mead local search on the best individual with restarts\n            if self.eval_count < self.budget:\n                best_idx = np.argmin(fitness)\n                x0 = population[best_idx].copy()\n                \n                # Define a local function to minimize\n                def local_func(x):\n                    return func(x)\n                \n                # Nelder-Mead optimization with restarts\n                remaining_budget_nm = (self.budget - self.eval_count)\n                fev_per_restart = remaining_budget_nm // self.nm_restarts if self.nm_restarts > 0 else remaining_budget_nm\n                \n                for _ in range(self.nm_restarts):\n                    if self.eval_count < self.budget:\n                         res = minimize(local_func, x0, method='Nelder-Mead',\n                                        options={'maxfev': fev_per_restart, 'xatol': 1e-4, 'fatol': 1e-4}) #added tolerances\n\n                         if res.success or res.nfev > 0: # only consider results that converged or used some budget\n                             if res.fun < self.f_opt:\n                                 self.f_opt = res.fun\n                                 self.x_opt = res.x\n\n                             self.eval_count += res.nfev\n                             if self.eval_count < self.budget:\n                                 x0 = np.random.uniform(self.lb, self.ub, size=self.dim) #restart from new location\n                         else:\n                             self.eval_count += fev_per_restart  #account for budget used, even if NM failed immediately\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "An exception occurred: name 'minimize' is not defined.", "error": "", "parent_ids": ["4b01b70d-a9f2-4e10-b69b-d160d643906a"], "operator": null, "metadata": {}}
{"id": "dcd82239-8c4f-435c-aec6-b6c7054cc1d9", "fitness": -Infinity, "name": "CMAES", "description": "Fixes broadcasting error in CMAES by ensuring x_opt is correctly reshaped when used as the mean.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damps=None, c_cov=None, c_mu=None, mu_eff=None):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n\n        # Strategy parameter setting: Selection\n        self.mu = self.popsize // 2           # number of parents/individuals for recombination\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)   # normalize recombination weights array\n        self.mu_eff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n        # Strategy parameter setting: Adaptation\n        self.cs = cs  # damping for step-size increasing\n        self.damps = damps if damps is not None else 1 + 2 * max(0, np.sqrt((self.mu_eff - 1) / (self.dim + 1)) - 1) + self.cs # damping for step-size decreasing\n        self.c_cov = c_cov if c_cov is not None else (1 / 3) * (self.mu_eff / (self.dim + 1.3) + (1 - self.mu_eff / (self.dim + 1.3)) * self.cs) # learning rate for rank-one update\n        self.c_mu = c_mu if c_mu is not None else min(1 - self.c_cov, 2 * (self.mu_eff - 2 + 1/self.mu_eff) / ((self.dim + 2)**2 + self.mu_eff))  # and for rank-mu update\n        \n        # Initialize dynamic (internal) strategy parameters and constants\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.B = np.eye(self.dim)       # B defines the coordinate system\n        self.D = np.ones(self.dim)       # diagonal D defines the scaling\n        self.C = np.eye(self.dim)       # covariance matrix C = B diag(D.^2) B'\n        self.chiN = self.dim**0.5 * (1 - (1/(4*self.dim)) + 1/(21*self.dim**2))  # expectation of ||N(0,I)|| == norm(randn(dim,1))\n\n        self.sigma = 0.3 # step size\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        self.n_evals = 0\n        while self.n_evals < self.budget:\n            # Generate and evaluate lambda offspring\n            x = np.random.randn(self.dim, self.popsize)\n            arz = self.sigma * self.B.dot(self.D * x)\n            \n            if self.x_opt is not None:\n                mean = self.x_opt\n            else:\n                mean = (func.bounds.ub + func.bounds.lb) / 2\n            \n            arx = func.bounds.clip(np.tile(mean, (self.popsize, 1)).T + arz)\n            \n            arfitness = np.array([func(arx[:, i]) for i in range(self.popsize)])\n            self.n_evals += self.popsize\n\n            # Sort by fitness and update mean\n            idx = np.argsort(arfitness)\n            arfitness = arfitness[idx]\n            x = x[:, idx]\n            xmean = np.sum(x[:, :self.mu] * self.weights, axis=1)\n\n            # Cumulation: Update evolution paths\n            self.ps = (1 - self.cs) * self.ps + (np.sqrt(self.cs * (2 - self.cs) * self.mu_eff) / self.sigma) * self.B.dot(self.D * xmean)\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * (self.n_evals / self.popsize))) / self.chiN) < (1.4 + 2 / (self.dim + 1))\n            self.pc = (1 - self.c_cov) * self.pc + hsig * np.sqrt(self.c_cov * (2 - self.c_cov) * self.mu_eff) * self.B.dot(xmean)\n\n            # Adapt covariance matrix C\n            artmp = (1/self.sigma) * self.B.dot(x[:, :self.mu])\n            self.C = (1 - self.c_cov - self.c_mu) * self.C + self.c_cov * (self.pc[:, None].dot(self.pc[None, :]) + (1 - hsig) * self.c_cov * self.C) + self.c_mu * artmp.dot(np.diag(self.weights)).dot(artmp.T)\n\n            # Adapt step size sigma\n            self.sigma = self.sigma * np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            \n            # Decomposition of C (updates B and D)\n            if self.n_evals > self.budget:\n                break\n            try:\n                self.D, self.B = np.linalg.eig(self.C)\n                self.D = np.sqrt(np.abs(self.D)) # Robustness: keep sigma > 0\n            except np.linalg.LinAlgError:\n                self.D = np.ones(self.dim)\n                self.B = np.eye(self.dim)\n                \n            # Update optimal values\n            if arfitness[0] < self.f_opt:\n                self.f_opt = arfitness[0]\n                self.x_opt = arx[:, idx[0]]\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "An exception occurred: operands could not be broadcast together with shapes (2,) (2,6) .", "error": "", "parent_ids": ["5982128c-a5fc-432c-8fe7-df9c6473696d"], "operator": null, "metadata": {}}
{"id": "8ba23326-2d62-4c20-ba48-87519d55e474", "fitness": 0.0, "name": "PSO_GradientDescent", "description": "A population-based algorithm that blends Particle Swarm Optimization with a Gradient-based local search, adaptively switching between global exploration and local exploitation based on fitness stagnation.", "code": "import numpy as np\n\nclass PSO_GradientDescent:\n    def __init__(self, budget=10000, dim=10, pop_size=20, w=0.7, c1=1.5, c2=1.5, lr=0.01, stagnation_threshold=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w = w\n        self.c1 = c1\n        self.c2 = c2\n        self.lr = lr  # Learning rate for gradient descent\n        self.stagnation_threshold = stagnation_threshold # Number of iterations without improvement to switch to gradient descent\n        self.population = None\n        self.fitness = None\n        self.velocities = None\n        self.best_positions = None\n        self.global_best_fitness = np.inf\n        self.global_best_position = None\n        self.eval_count = 0\n        self.stagnation_counter = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        self.velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))  # Initialize velocities\n        self.best_positions = self.population.copy()\n        self.global_best_position = self.population[np.argmin(self.fitness)].copy()\n        self.global_best_fitness = np.min(self.fitness)\n\n    def pso_step(self, func):\n        self.velocities = (self.w * self.velocities +\n                           self.c1 * np.random.rand(self.pop_size, self.dim) * (self.best_positions - self.population) +\n                           self.c2 * np.random.rand(self.pop_size, self.dim) * (self.global_best_position - self.population))\n        new_population = self.population + self.velocities\n        new_population = np.clip(new_population, func.bounds.lb, func.bounds.ub)\n        return new_population\n\n    def gradient_descent_step(self, func, x):\n        # Simple gradient estimation (you could use a more sophisticated method)\n        gradient = np.zeros(self.dim)\n        delta = 1e-5\n        for i in range(self.dim):\n            x_plus = x.copy()\n            x_minus = x.copy()\n            x_plus[i] += delta\n            x_minus[i] -= delta\n            x_plus = np.clip(x_plus, func.bounds.lb, func.bounds.ub)\n            x_minus = np.clip(x_minus, func.bounds.lb, func.bounds.ub)\n            gradient[i] = (func(x_plus) - func(x_minus)) / (2 * delta)\n            self.eval_count += 2 # Update eval count because the function is called twice in gradient estimation\n\n        new_x = x - self.lr * gradient\n        new_x = np.clip(new_x, func.bounds.lb, func.bounds.ub)\n        return new_x\n    \n    def __call__(self, func):\n        self.initialize_population(func)\n        best_fitness_history = []\n\n        while self.eval_count < self.budget:\n            if len(best_fitness_history) > self.stagnation_threshold and np.std(best_fitness_history[-self.stagnation_threshold:]) < 1e-6:\n                # Fitness has stagnated, switch to gradient descent for all particles.\n                for i in range(self.pop_size):\n                    self.population[i] = self.gradient_descent_step(func, self.population[i])\n                    fitness = func(self.population[i])\n                    self.eval_count += 1\n\n                    if fitness < self.fitness[i]:\n                        self.fitness[i] = fitness\n                        self.best_positions[i] = self.population[i].copy()\n\n                        if fitness < self.global_best_fitness:\n                            self.global_best_fitness = fitness\n                            self.global_best_position = self.population[i].copy()\n                \n                best_fitness_history.append(self.global_best_fitness) # Append to history after gradient descent update\n            else:\n                # PSO Step\n                new_population = self.pso_step(func)\n                new_fitness = np.array([func(x) for x in new_population])\n                self.eval_count += self.pop_size\n\n                for i in range(self.pop_size):\n                    if new_fitness[i] < self.fitness[i]:\n                        self.population[i] = new_population[i]\n                        self.fitness[i] = new_fitness[i]\n                        self.best_positions[i] = self.population[i].copy()\n\n                        if new_fitness[i] < self.global_best_fitness:\n                            self.global_best_fitness = new_fitness[i]\n                            self.global_best_position = self.population[i].copy()\n                best_fitness_history.append(self.global_best_fitness) # Append to history after pso update\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 1, "feedback": "The algorithm PSO_GradientDescent scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f94d28d2-78ef-4c75-85b4-2bef775fb513"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "f1f70311-6e84-459e-b824-3c8c5ca9c375", "fitness": -Infinity, "name": "CMAES", "description": "Improved CMA-ES with boundary handling using mirroring and a more stable covariance matrix adaptation.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damps=None, c_cov=None, c_mu=None, mu_eff=None, mirror_alpha=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mirror_alpha = mirror_alpha\n\n        # Strategy parameter setting: Selection\n        self.mu = self.popsize // 2           # number of parents/individuals for recombination\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)   # normalize recombination weights array\n        self.mu_eff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n        # Strategy parameter setting: Adaptation\n        self.cs = cs  # damping for step-size increasing\n        self.damps = damps if damps is not None else 1 + 2 * max(0, np.sqrt((self.mu_eff - 1) / (self.dim + 1)) - 1) + self.cs # damping for step-size decreasing\n        self.c_cov = c_cov if c_cov is not None else (1 / 3) * (self.mu_eff / (self.dim + 1.3) + (1 - self.mu_eff / (self.dim + 1.3)) * self.cs) # learning rate for rank-one update\n        self.c_mu = c_mu if c_mu is not None else min(1 - self.c_cov, 2 * (self.mu_eff - 2 + 1/self.mu_eff) / ((self.dim + 2)**2 + self.mu_eff))  # and for rank-mu update\n        \n        # Initialize dynamic (internal) strategy parameters and constants\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.B = np.eye(self.dim)       # B defines the coordinate system\n        self.D = np.ones(self.dim)       # diagonal D defines the scaling\n        self.C = np.eye(self.dim)       # covariance matrix C = B diag(D.^2) B'\n        self.chiN = self.dim**0.5 * (1 - (1/(4*self.dim)) + 1/(21*self.dim**2))  # expectation of ||N(0,I)|| == norm(randn(dim,1))\n\n        self.sigma = 0.3 # step size\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.x_mean = (func.bounds.ub + func.bounds.lb) / 2  # Initialize mean\n\n    def __call__(self, func):\n        self.n_evals = 0\n        while self.n_evals < self.budget:\n            # Generate and evaluate lambda offspring\n            arz = np.random.randn(self.dim, self.popsize)\n            arx = self.x_mean[:, np.newaxis] + self.sigma * self.B.dot(self.D * arz)\n            \n            # Boundary Handling with mirroring\n            for i in range(self.popsize):\n                for j in range(self.dim):\n                    if arx[j, i] < func.bounds.lb[j]:\n                        arx[j, i] = func.bounds.lb[j] + self.mirror_alpha * (func.bounds.lb[j] - arx[j, i])\n                    elif arx[j, i] > func.bounds.ub[j]:\n                        arx[j, i] = func.bounds.ub[j] - self.mirror_alpha * (arx[j, i] - func.bounds.ub[j])\n            arx = func.bounds.clip(arx)\n            \n            arfitness = np.array([func(arx[:, i]) for i in range(self.popsize)])\n            self.n_evals += self.popsize\n\n            # Sort by fitness and update mean\n            idx = np.argsort(arfitness)\n            arfitness = arfitness[idx]\n            arz = arz[:, idx]\n            arx_sorted = arx[:, idx]\n            xmean = np.sum(arz[:, :self.mu] * self.weights, axis=1)\n\n            # Cumulation: Update evolution paths\n            self.ps = (1 - self.cs) * self.ps + (np.sqrt(self.cs * (2 - self.cs) * self.mu_eff) / self.sigma) * self.B.dot(self.D * xmean)\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * (self.n_evals / self.popsize))) / self.chiN) < (1.4 + 2 / (self.dim + 1))\n            self.pc = (1 - self.c_cov) * self.pc + hsig * np.sqrt(self.c_cov * (2 - self.c_cov) * self.mu_eff) * self.B.dot(xmean)\n\n            # Adapt covariance matrix C\n            artmp = (1/self.sigma) * self.B.dot(arz[:, :self.mu])\n            self.C = (1 - self.c_cov - self.c_mu) * self.C + self.c_cov * (self.pc[:, None].dot(self.pc[None, :]) + (1 - hsig) * self.c_cov * self.C) + self.c_mu * artmp.dot(np.diag(self.weights)).dot(artmp.T)\n\n            # Adapt step size sigma\n            self.sigma = self.sigma * np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            \n            # Decomposition of C (updates B and D)\n            if self.n_evals > self.budget:\n                break\n            try:\n                self.C = np.triu(self.C) + np.triu(self.C,1).T # enforce symmetry\n                self.D, self.B = np.linalg.eigh(self.C) #eigenvalue decomposition, returns real eigenvalues\n                self.D = np.sqrt(np.abs(self.D)) # Robustness: keep sigma > 0\n            except np.linalg.LinAlgError:\n                self.D = np.ones(self.dim)\n                self.B = np.eye(self.dim)\n            \n            self.x_mean = np.sum(arx_sorted[:, :self.mu] * self.weights, axis=1)\n\n            # Update optimal values\n            if arfitness[0] < self.f_opt:\n                self.f_opt = arfitness[0]\n                self.x_opt = arx_sorted[:, 0]\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "An exception occurred: name 'func' is not defined.", "error": "", "parent_ids": ["5982128c-a5fc-432c-8fe7-df9c6473696d"], "operator": null, "metadata": {}}
{"id": "d5a2b9ac-1a36-427f-b1c7-e127dd33f835", "fitness": -Infinity, "name": "CMAES", "description": "CMA-ES with rank-one updates to the covariance matrix and clipping to enforce bounds.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damp=None, c_cov=None, c_mu=None):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.m = None\n        self.sigma = 0.3  # exploration parameter\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.chiN = None\n\n        self.cs = cs\n        self.damp = damp if damp is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1]) + self.cs\n        self.c_cov = c_cov if c_cov is not None else 2 / ((self.dim + np.sqrt(2))**2)\n        self.c_mu = c_mu if c_mu is not None else min(1 - self.c_cov, 2 * (self.weights[0] - 1 + self.weights[0]**2) / ((self.mu + 6 + 6*self.dim**0.5)))\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1 / (4 * self.dim) + 1 / (21 * self.dim**2))\n        \n        used_budget = 0\n        while used_budget < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.m[:, np.newaxis] + self.sigma * C_sqrt @ z\n            \n            # Mirroring to handle bounds\n            xmir = self.m[:, np.newaxis] - self.sigma * C_sqrt @ z\n            \n            # Repair mechanism to ensure solutions are within bounds\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            xmir = np.clip(xmir, func.bounds.lb, func.bounds.ub)\n\n            f = np.array([func(x[:, i]) if used_budget + 1 <= self.budget else np.inf for i in range(self.popsize)])\n            fmir = np.array([func(xmir[:, i]) if used_budget + self.popsize + 1 <= self.budget else np.inf for i in range(self.popsize)])\n            \n            used_budget += 2*self.popsize\n            \n            if np.min(f) < np.min(fmir):\n                y = x\n                fitness = f\n                best_index = np.argmin(f)\n                if f[best_index] < self.f_opt:\n                    self.f_opt = f[best_index]\n                    self.x_opt = x[:, best_index].copy()\n            else:\n                y = xmir\n                fitness = fmir\n                best_index = np.argmin(fmir)\n                if fmir[best_index] < self.f_opt:\n                    self.f_opt = fmir[best_index]\n                    self.x_opt = xmir[:, best_index].copy()\n                    \n            \n            arindex = np.argsort(fitness)\n            xmu = y[:, arindex[:self.mu]]\n            zmu = z[:, arindex[:self.mu]]\n            \n            m_old = self.m.copy()\n            self.m = np.sum(xmu * self.weights[np.newaxis, :], axis=1)\n            \n            ps_new = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs)) * (C_sqrt @ np.sum(zmu * self.weights[np.newaxis, :], axis=1))\n            self.ps = ps_new\n            \n            pc_new = (1 - self.c_cov) * self.pc + np.sqrt(self.c_cov * (2 - self.c_cov)) * (self.m - m_old) / self.sigma\n            self.pc = pc_new\n            \n            norm_ps = np.linalg.norm(self.ps)\n            factor = self.c_cov * (1 / (np.min([1, (1000000 * norm_ps**4 / self.dim)])))\n            \n            self.C = (1 - self.c_cov - self.c_mu) * self.C + self.c_cov * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :])\n            \n            for i in range(self.mu):\n                y_diff = (xmu[:,i] - m_old)\n                self.C += self.c_mu * self.weights[i] * (y_diff[:, np.newaxis] @ y_diff[np.newaxis, :]) / self.sigma**2\n            \n            self.sigma *= np.exp((self.cs / self.damp) * (norm_ps / self.chiN - 1))\n            \n            # Ensure C remains positive definite\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)  # Reset covariance matrix if it becomes non-positive definite\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "An exception occurred: operands could not be broadcast together with shapes (2,6) (2,) (2,) .", "error": "", "parent_ids": ["144ad936-3228-4dd2-be9e-be846b82affd"], "operator": null, "metadata": {}}
{"id": "3378ec50-77bf-49c8-a6bc-c67002c9464b", "fitness": 0.0, "name": "AdaptiveHybridOptimization", "description": "Combines PSO and DE with adaptive parameter control based on fitness landscape characteristics, using a probabilistic selection between PSO and DE updates for each particle and incorporating a local search step.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimization:\n    def __init__(self, budget=10000, dim=10, pop_size=20, w_start=0.9, w_end=0.4, c1=2.0, c2=2.0, cr=0.7, f=0.8, ls_rate=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w_start = w_start  # Inertia weight start\n        self.w_end = w_end      # Inertia weight end\n        self.c1 = c1  # Cognitive coefficient\n        self.c2 = c2  # Social coefficient\n        self.cr = cr  # Crossover rate\n        self.f = f    # Mutation factor\n        self.ls_rate = ls_rate #Local search rate\n        self.population = None\n        self.fitness = None\n        self.best_positions = None\n        self.global_best_fitness = np.inf\n        self.global_best_position = None\n        self.eval_count = 0\n        self.generation = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        self.best_positions = self.population.copy()\n        self.global_best_position = self.population[np.argmin(self.fitness)].copy()\n        self.global_best_fitness = np.min(self.fitness)\n\n    def update_parameters(self):\n        # Adaptive inertia weight (linearly decreasing)\n        self.w = self.w_start - (self.w_start - self.w_end) * (self.generation / (self.budget / self.pop_size))\n\n        # No adaptation of c1 and c2 for simplicity and potential robustness.\n        # Adaptive parameter control based on population diversity and fitness improvement\n        # diversity = np.std(self.population)\n        # fitness_improvement = self.global_best_fitness - np.min(self.fitness)\n\n        # # Adjust inertia weight\n        # self.w = 0.9 - 0.2 * (diversity / np.max(np.std(self.population, axis=0)))\n\n        # # Adjust acceleration coefficients\n        # self.c1 = 1.5 + 0.5 * (fitness_improvement / abs(self.global_best_fitness + 1e-8))\n        # self.c2 = 1.5 + 0.5 * (fitness_improvement / abs(self.global_best_fitness + 1e-8))\n        \n        # self.f = 0.8 + 0.2 * (diversity / np.max(np.std(self.population, axis=0)))\n\n\n    def pso_step(self):\n        velocity = (self.w * np.random.rand(self.pop_size, self.dim) * (self.population - self.best_positions) +\n                    self.c1 * np.random.rand(self.pop_size, self.dim) * (self.best_positions - self.population) +\n                    self.c2 * np.random.rand(self.pop_size, self.dim) * (self.global_best_position - self.population))\n        return velocity\n\n    def de_step(self):\n         # Differential Evolution Mutation\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_1, x_2, x_3 = self.population[idxs]\n            v_i = x_1 + self.f * (x_2 - x_3)\n            \n            # Crossover\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() > self.cr and j != j_rand:\n                    v_i[j] = self.population[i, j]\n            \n            yield i, v_i\n    \n    def local_search(self, x, func, step_size=0.1):\n        \"\"\"Performs a simple local search around a given solution.\"\"\"\n        new_x = x.copy()\n        for i in range(self.dim):\n            # Explore both directions\n            delta = np.random.uniform(-step_size, step_size)\n            new_x[i] += delta\n\n            # Clip to bounds\n            new_x = np.clip(new_x, func.bounds.lb, func.bounds.ub)\n\n        f_new = func(new_x)\n        self.eval_count +=1\n\n        return new_x, f_new\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            self.update_parameters()\n            self.generation += 1\n            \n            new_population = self.population.copy()\n            fitness_new = self.fitness.copy()\n\n            for i in range(self.pop_size):\n                # Probabilistic selection of PSO or DE\n                if np.random.rand() < 0.5:\n                    # PSO update\n                    velocity = self.pso_step()[i]\n                    new_x = self.population[i] + velocity\n                    new_x = np.clip(new_x, func.bounds.lb, func.bounds.ub)\n                    f_new = func(new_x)\n                    self.eval_count += 1\n                else:\n                    # DE update\n                    de_gen = self.de_step()\n                    for j, v_i in de_gen:\n                        if i == j:\n                            new_x = v_i\n                            new_x = np.clip(new_x, func.bounds.lb, func.bounds.ub)\n                            f_new = func(new_x)\n                            self.eval_count += 1\n                            break\n\n                #Local Search\n                if np.random.rand() < self.ls_rate:\n                   new_x, f_new = self.local_search(new_x, func)\n\n                #Greedy selection\n                if f_new < self.fitness[i]:\n                    new_population[i] = new_x\n                    fitness_new[i] = f_new\n\n                    #Update global best\n                    if f_new < self.global_best_fitness:\n                        self.global_best_fitness = f_new\n                        self.global_best_position = new_x.copy()\n\n                    self.best_positions[i] = new_x.copy()\n                elif self.fitness[i] < self.global_best_fitness:\n                     self.best_positions[i] = self.population[i].copy()\n\n            self.population = new_population\n            self.fitness = fitness_new\n\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveHybridOptimization scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f94d28d2-78ef-4c75-85b4-2bef775fb513"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "84df2ed2-9ea6-43c5-82d9-f117b91816ab", "fitness": 0.32588864572484316, "name": "CMAES", "description": "Improved CMA-ES with better bound handling using a combination of mirroring and re-sampling, and numerically stable rank-one update to the covariance matrix.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damp=None, c_cov=None, c_mu=None):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.m = None\n        self.sigma = 0.3  # exploration parameter\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.chiN = None\n\n        self.cs = cs\n        self.damp = damp if damp is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1]) + self.cs\n        self.c_cov = c_cov if c_cov is not None else 2 / ((self.dim + np.sqrt(2))**2)\n        self.c_mu = c_mu if c_mu is not None else min(1 - self.c_cov, 2 * (self.weights[0] - 1 + self.weights[0]**2) / ((self.mu + 6 + 6*self.dim**0.5)))\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1 / (4 * self.dim) + 1 / (21 * self.dim**2))\n        \n        used_budget = 0\n        while used_budget < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.m[:, np.newaxis] + self.sigma * C_sqrt @ z\n            \n            # Mirroring to handle bounds\n            xmir = self.m[:, np.newaxis] - self.sigma * C_sqrt @ z\n            \n            # Repair mechanism to ensure solutions are within bounds and resample if necessary\n            for i in range(self.popsize):\n                if not func.bounds.lb[0] <= x[0, i] <= func.bounds.ub[0]:\n                    x[:, i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                if not func.bounds.lb[0] <= xmir[0, i] <= func.bounds.ub[0]:\n                    xmir[:, i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                    \n            f = np.array([func(x[:, i]) if used_budget < self.budget else np.inf for i in range(self.popsize)])\n            fmir = np.array([func(xmir[:, i]) if used_budget + self.popsize < self.budget else np.inf for i in range(self.popsize)])\n            \n            used_budget += 2 * self.popsize\n            \n            if np.min(f) < np.min(fmir):\n                y = x\n                fitness = f\n                best_index = np.argmin(f)\n                if f[best_index] < self.f_opt:\n                    self.f_opt = f[best_index]\n                    self.x_opt = x[:, best_index].copy()\n            else:\n                y = xmir\n                fitness = fmir\n                best_index = np.argmin(fmir)\n                if fmir[best_index] < self.f_opt:\n                    self.f_opt = fmir[best_index]\n                    self.x_opt = xmir[:, best_index].copy()\n                    \n            \n            arindex = np.argsort(fitness)\n            xmu = y[:, arindex[:self.mu]]\n            zmu = z[:, arindex[:self.mu]]\n            \n            m_old = self.m.copy()\n            self.m = np.sum(xmu * self.weights[np.newaxis, :], axis=1)\n            \n            ps_new = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs)) * (C_sqrt @ np.sum(zmu * self.weights[np.newaxis, :], axis=1))\n            self.ps = ps_new\n            \n            pc_new = (1 - self.c_cov) * self.pc + np.sqrt(self.c_cov * (2 - self.c_cov)) * (self.m - m_old) / self.sigma\n            self.pc = pc_new\n            \n            norm_ps = np.linalg.norm(self.ps)\n            factor = self.c_cov * (1 / (np.min([1, (1000000 * norm_ps**4 / self.dim)])))\n            \n            # Rank-one update of C\n            self.C = (1 - self.c_cov - self.c_mu) * self.C + self.c_cov * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :])\n            \n            # Rank-mu update of C\n            for i in range(self.mu):\n                y_diff = (xmu[:, i] - m_old) / self.sigma\n                self.C += self.c_mu * self.weights[i] * (y_diff[:, np.newaxis] @ y_diff[np.newaxis, :])\n                \n            self.sigma *= np.exp((self.cs / self.damp) * (norm_ps / self.chiN - 1))\n            \n            # Ensure C remains positive definite\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)  # Reset covariance matrix if it becomes non-positive definite\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm CMAES scored 0.326 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["144ad936-3228-4dd2-be9e-be846b82affd"], "operator": null, "metadata": {"aucs": [0.07352815934408119, 0.16318616737366087, 0.28980157254037076, 0.20285522892108243, 0.6467145980585652, 0.12203209719857244, 0.2627507844985788, 0.7065434048725362, 0.17479837315191982, 0.10620272153655319, 0.23250122097699322, 0.9950971720219772, 0.2533391880454219, 0.5757396038533775, 0.5583108138316613, 0.2965584029319428, 0.2621717153583121, 0.3231890626848717, 0.09529077253646867, 0.17716185475991575]}}
{"id": "04457d0f-33fa-4838-90ad-ac73d8f7723f", "fitness": -Infinity, "name": "CMAES", "description": "Improved CMA-ES with bound constraints handling and rank-one update correction to prevent broadcasting errors.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damp=None, c_cov=None, c_mu=None):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.m = None\n        self.sigma = 0.3  # exploration parameter\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.chiN = None\n\n        self.cs = cs\n        self.damp = damp if damp is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1]) + self.cs\n        self.c_cov = c_cov if c_cov is not None else 2 / ((self.dim + np.sqrt(2))**2)\n        self.c_mu = c_mu if c_mu is not None else min(1 - self.c_cov, 2 * (self.weights[0] - 1 + self.weights[0]**2) / ((self.mu + 6 + 6*self.dim**0.5)))\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1 / (4 * self.dim) + 1 / (21 * self.dim**2))\n        \n        used_budget = 0\n        while used_budget < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.m[:, np.newaxis] + self.sigma * C_sqrt @ z\n            \n            # Mirroring to handle bounds\n            xmir = self.m[:, np.newaxis] - self.sigma * C_sqrt @ z\n            \n            # Repair mechanism to ensure solutions are within bounds\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            xmir = np.clip(xmir, func.bounds.lb, func.bounds.ub)\n\n            f = np.array([func(x[:, i]) if used_budget < self.budget else np.inf for i in range(self.popsize)])\n            fmir = np.array([func(xmir[:, i]) if used_budget + self.popsize < self.budget else np.inf for i in range(self.popsize)])\n            \n            used_budget += 2 * self.popsize\n            \n            if np.min(f) < np.min(fmir):\n                y = x\n                fitness = f\n                best_index = np.argmin(f)\n                if f[best_index] < self.f_opt:\n                    self.f_opt = f[best_index]\n                    self.x_opt = x[:, best_index]\n            else:\n                y = xmir\n                fitness = fmir\n                best_index = np.argmin(fmir)\n                if fmir[best_index] < self.f_opt:\n                    self.f_opt = fmir[best_index]\n                    self.x_opt = xmir[:, best_index]\n                    \n            \n            arindex = np.argsort(fitness)\n            xmu = y[:, arindex[:self.mu]]\n            zmu = z[:, arindex[:self.mu]]\n            \n            m_new = np.sum(xmu * self.weights[np.newaxis, :], axis=1)\n            \n            ps_new = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs)) * (C_sqrt @ np.sum(zmu * self.weights[np.newaxis, :], axis=1))\n            self.ps = ps_new\n            \n            pc_new = (1 - self.c_cov) * self.pc + np.sqrt(self.c_cov * (2 - self.c_cov)) * (m_new - self.m) / self.sigma\n            self.pc = pc_new\n\n            self.m = m_new\n\n            norm_ps = np.linalg.norm(self.ps)\n            factor = self.c_cov * (1 / (np.min([1, (1000000 * norm_ps**4 / self.dim)])))\n            \n            # Correct rank-one update\n            diff = (xmu - self.m[:, np.newaxis]) / self.sigma\n            weighted_diff = diff @ np.diag(self.weights)\n            self.C = (1 - self.c_cov - self.c_mu) * self.C + self.c_cov * np.outer(self.pc, self.pc) + self.c_mu * weighted_diff @ diff.T\n            \n            self.sigma *= np.exp((self.cs / self.damp) * (norm_ps / self.chiN - 1))\n            \n            # Ensure C remains positive definite\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)  # Reset covariance matrix if it becomes non-positive definite\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "An exception occurred: operands could not be broadcast together with shapes (2,6) (2,) (2,) .", "error": "", "parent_ids": ["144ad936-3228-4dd2-be9e-be846b82affd"], "operator": null, "metadata": {}}
{"id": "4765c500-df51-41df-8955-083950a20f82", "fitness": -Infinity, "name": "CMAES", "description": "Improved CMA-ES with mirroring, bound repair, and adaptive covariance updates, addressing broadcasting errors and budget overruns.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damp=None, c_cov=None, c_mu=None):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.m = None\n        self.sigma = 0.3  # exploration parameter\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.chiN = None\n\n        self.cs = cs\n        self.damp = damp if damp is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1]) + self.cs\n        self.c_cov = c_cov if c_cov is not None else 2 / ((self.dim + np.sqrt(2))**2)\n        self.c_mu = c_mu if c_mu is not None else min(1 - self.c_cov, 2 * (self.weights[0] - 1 + self.weights[0]**2) / ((self.mu + 6 + 6*self.dim**0.5)))\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1 / (4 * self.dim) + 1 / (21 * self.dim**2))\n        \n        used_budget = 0\n        while used_budget < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.m[:, np.newaxis] + self.sigma * C_sqrt @ z\n            \n            # Mirroring to handle bounds\n            xmir = self.m[:, np.newaxis] - self.sigma * C_sqrt @ z\n            \n            # Repair mechanism to ensure solutions are within bounds\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            xmir = np.clip(xmir, func.bounds.lb, func.bounds.ub)\n\n            f = np.full(self.popsize, np.inf)\n            fmir = np.full(self.popsize, np.inf)\n\n            for i in range(self.popsize):\n                if used_budget < self.budget:\n                    f[i] = func(x[:, i])\n                    used_budget += 1\n                if used_budget < self.budget:\n                    fmir[i] = func(xmir[:, i])\n                    used_budget += 1\n            \n            if np.min(f) < np.min(fmir):\n                y = x\n                fitness = f\n                best_index = np.argmin(f)\n                if f[best_index] < self.f_opt:\n                    self.f_opt = f[best_index]\n                    self.x_opt = x[:, best_index].copy()\n            else:\n                y = xmir\n                fitness = fmir\n                best_index = np.argmin(fmir)\n                if fmir[best_index] < self.f_opt:\n                    self.f_opt = fmir[best_index]\n                    self.x_opt = xmir[:, best_index].copy()\n                    \n            \n            arindex = np.argsort(fitness)\n            xmu = y[:, arindex[:self.mu]]\n            zmu = z[:, arindex[:self.mu]]\n            \n            # Ensure xmu and self.m have compatible shapes for broadcasting\n            y_w = xmu - self.m[:, np.newaxis]\n            m_new = np.sum(xmu * self.weights[np.newaxis, :], axis=1)\n            \n            ps_new = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs)) * (C_sqrt @ np.sum(zmu * self.weights[np.newaxis, :], axis=1))\n            self.ps = ps_new\n            \n            pc_new = (1 - self.c_cov) * self.pc + np.sqrt(self.c_cov * (2 - self.c_cov)) * (m_new - self.m) / self.sigma\n            self.pc = pc_new\n\n            self.m = m_new\n\n            norm_ps = np.linalg.norm(self.ps)\n            factor = self.c_cov * (1 / (np.min([1, (1000000 * norm_ps**4 / self.dim)])))\n            \n            # Update C using rank-one and rank-mu updates\n            y_diff = xmu - self.m[:, np.newaxis]\n            self.C = (1 - self.c_cov - self.c_mu) * self.C + self.c_cov * np.outer(self.pc, self.pc) + self.c_mu * y_diff @ np.diag(self.weights) @ y_diff.T / self.sigma**2\n\n            self.sigma *= np.exp((self.cs / self.damp) * (norm_ps / self.chiN - 1))\n            \n            # Ensure C remains positive definite\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)  # Reset covariance matrix if it becomes non-positive definite\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "An exception occurred: operands could not be broadcast together with shapes (2,6) (2,) (2,) .", "error": "", "parent_ids": ["144ad936-3228-4dd2-be9e-be846b82affd"], "operator": null, "metadata": {}}
{"id": "bd101728-897d-4a9a-ad0c-3c7be308fc81", "fitness": -Infinity, "name": "PSO_NelderMead", "description": "Hybrid optimization algorithm combining Particle Swarm Optimization (PSO) for global search with Nelder-Mead Simplex for local refinement, adaptively switching between them based on stagnation detection.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass PSO_NelderMead:\n    def __init__(self, budget=10000, dim=10, pop_size=20, w=0.7, c1=1.5, c2=1.5, stagnation_threshold=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w = w  # Inertia weight\n        self.c1 = c1  # Cognitive coefficient\n        self.c2 = c2  # Social coefficient\n        self.stagnation_threshold = stagnation_threshold\n        self.population = None\n        self.fitness = None\n        self.velocities = None\n        self.best_positions = None\n        self.global_best_fitness = np.inf\n        self.global_best_position = None\n        self.eval_count = 0\n        self.stagnation_counter = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        self.velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))  # Initialize velocities\n        self.best_positions = self.population.copy()\n        self.global_best_position = self.population[np.argmin(self.fitness)].copy()\n        self.global_best_fitness = np.min(self.fitness)\n    \n    def pso_step(self, func):\n        self.velocities = (self.w * self.velocities +\n                           self.c1 * np.random.rand(self.pop_size, self.dim) * (self.best_positions - self.population) +\n                           self.c2 * np.random.rand(self.pop_size, self.dim) * (self.global_best_position - self.population))\n        new_population = self.population + self.velocities\n        new_population = np.clip(new_population, func.bounds.lb, func.bounds.ub)\n        return new_population\n\n    def nelder_mead_step(self, func, x0):\n         # Perform Nelder-Mead optimization starting from x0\n        bounds = func.bounds\n        result = minimize(func, x0, method='Nelder-Mead', options={'maxfev': max(1, self.budget // (self.pop_size*2))}, bounds=list(zip([bounds.lb]*self.dim, [bounds.ub]*self.dim))) #Reduce budget because we have Nelder-Mead\n        self.eval_count += result.nfev\n        return result.fun, result.x\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        previous_best_fitness = self.global_best_fitness\n        \n        while self.eval_count < self.budget:\n            # PSO step\n            new_population = self.pso_step(func)\n            new_fitness = np.array([func(x) for x in new_population])\n            self.eval_count += self.pop_size\n\n            # Selection and update\n            for i in range(self.pop_size):\n                if new_fitness[i] < self.fitness[i]:\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n                    if new_fitness[i] < self.global_best_fitness:\n                        self.global_best_fitness = new_fitness[i]\n                        self.global_best_position = new_population[i].copy()\n                \n                # Update personal best\n                if self.fitness[i] < func(self.best_positions[i]):\n                    self.best_positions[i] = self.population[i].copy()\n                \n                #Global Best is updated after SA\n                if self.fitness[i] < self.global_best_fitness:\n                    self.global_best_fitness = self.fitness[i]\n                    self.global_best_position = self.population[i].copy()\n\n            # Stagnation check\n            if self.global_best_fitness < previous_best_fitness + 1e-9: # Use a tolerance\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n            \n            previous_best_fitness = self.global_best_fitness\n\n            # Nelder-Mead step if stagnated\n            if self.stagnation_counter >= self.stagnation_threshold:\n                f_nm, x_nm = self.nelder_mead_step(func, self.global_best_position)\n                if f_nm < self.global_best_fitness:\n                    self.global_best_fitness = f_nm\n                    self.global_best_position = x_nm.copy()\n                self.stagnation_counter = 0 # Reset stagnation counter\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 2, "feedback": "An exception occurred: name 'minimize' is not defined.", "error": "", "parent_ids": ["af4a3163-f999-47d6-ad61-68d23c6a50ed"], "operator": null, "metadata": {}}
{"id": "c2ef2713-c000-498f-9537-ffa91e6f4549", "fitness": 0.0, "name": "PSO_SA", "description": "Adaptively adjusts PSO parameters and SA temperature based on the search progress and uses a more robust SA neighbor generation.", "code": "import numpy as np\n\nclass PSO_SA:\n    def __init__(self, budget=10000, dim=10, pop_size=20, w_init=0.9, w_final=0.4, c1=2.0, c2=2.0, temp_init=1.0, temp_decay=0.95, sa_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w_init = w_init  # Initial inertia weight\n        self.w_final = w_final # Final inertia weight\n        self.c1 = c1  # Cognitive coefficient\n        self.c2 = c2  # Social coefficient\n        self.temp_init = temp_init # Initial temperature for SA\n        self.temp_decay = temp_decay # Temperature decay rate\n        self.sa_prob = sa_prob # Probability of applying SA\n        self.population = None\n        self.fitness = None\n        self.velocities = None\n        self.best_positions = None\n        self.global_best_fitness = np.inf\n        self.global_best_position = None\n        self.eval_count = 0\n        self.temperature = temp_init\n        self.best_fitness_history = []\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        self.velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))  # Initialize velocities\n        self.best_positions = self.population.copy()\n        self.global_best_position = self.population[np.argmin(self.fitness)].copy()\n        self.global_best_fitness = np.min(self.fitness)\n        self.best_fitness_history.append(self.global_best_fitness)\n\n    def pso_step(self, func, current_eval):\n        # Adaptive inertia weight\n        w = self.w_init - (self.w_init - self.w_final) * (current_eval / self.budget)\n        \n        self.velocities = (w * self.velocities +\n                           self.c1 * np.random.rand(self.pop_size, self.dim) * (self.best_positions - self.population) +\n                           self.c2 * np.random.rand(self.pop_size, self.dim) * (self.global_best_position - self.population))\n        new_population = self.population + self.velocities\n        new_population = np.clip(new_population, func.bounds.lb, func.bounds.ub)\n        return new_population\n\n    def simulated_annealing_step(self, func, x, f):\n        # Adaptive SA step size\n        step_size = 0.1 * (func.bounds.ub - func.bounds.lb) # Scale based on bounds\n\n        x_new = x + np.random.normal(0, step_size, size=self.dim)  # Generate a neighbor\n        x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n        f_new = func(x_new)\n        self.eval_count += 1\n\n        if f_new < f:\n            return x_new, f_new\n        else:\n            acceptance_probability = np.exp((f - f_new) / self.temperature)\n            if np.random.rand() < acceptance_probability:\n                return x_new, f_new\n            else:\n                return x, f\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            current_eval = self.eval_count\n            # PSO step\n            new_population = self.pso_step(func, current_eval)\n            new_fitness = np.array([func(x) for x in new_population])\n            self.eval_count += self.pop_size\n\n            # Selection and update\n            for i in range(self.pop_size):\n                # Apply SA with probability\n                if np.random.rand() < self.sa_prob:\n                    x_refined, f_refined = self.simulated_annealing_step(func, new_population[i].copy(), new_fitness[i])\n                else:\n                    x_refined = new_population[i]\n                    f_refined = new_fitness[i]\n                \n                if f_refined < self.fitness[i]:\n                    self.population[i] = x_refined\n                    self.fitness[i] = f_refined\n                    if f_refined < self.global_best_fitness:\n                        self.global_best_fitness = f_refined\n                        self.global_best_position = x_refined.copy()\n                \n                # Update personal best\n                if self.fitness[i] > f_refined: # Compare with refined version\n                    self.best_positions[i] = self.population[i].copy()\n                    self.fitness[i] = f_refined\n                \n                #Global Best is updated after SA\n                if self.fitness[i] < self.global_best_fitness:\n                    self.global_best_fitness = self.fitness[i]\n                    self.global_best_position = self.population[i].copy()\n            \n            #Cooling\n            self.temperature *= self.temp_decay\n            self.best_fitness_history.append(self.global_best_fitness)\n\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 2, "feedback": "The algorithm PSO_SA scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["af4a3163-f999-47d6-ad61-68d23c6a50ed"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "d686f856-0127-497e-8acf-7c7bcd01da9e", "fitness": -Infinity, "name": "APSO_SA_Aging", "description": "Adaptive Particle Swarm Optimization with Simulated Annealing and an aging mechanism to promote exploration and exploitation balance.", "code": "import numpy as np\n\nclass APSO_SA_Aging:\n    def __init__(self, budget=10000, dim=10, pop_size=20, w_max=0.9, w_min=0.4, c1=2, c2=2, temp_init=1.0, temp_min=0.01, alpha=0.99, age_max=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w_max = w_max\n        self.w_min = w_min\n        self.c1 = c1\n        self.c2 = c2\n        self.temp_init = temp_init\n        self.temp_min = temp_min\n        self.alpha = alpha\n        self.age_max = age_max\n        self.population = None\n        self.fitness = None\n        self.velocities = None\n        self.best_positions = None\n        self.global_best_fitness = np.inf\n        self.global_best_position = None\n        self.eval_count = 0\n        self.ages = None\n        self.temperature = temp_init\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        self.velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))\n        self.best_positions = self.population.copy()\n        self.global_best_position = self.population[np.argmin(self.fitness)].copy()\n        self.global_best_fitness = np.min(self.fitness)\n        self.ages = np.zeros(self.pop_size)\n\n    def pso_step(self, func):\n        # Adaptive inertia weight (linearly decreasing)\n        w = self.w_max - (self.w_max - self.w_min) * (self.eval_count / self.budget)\n\n        self.velocities = (w * self.velocities +\n                           self.c1 * np.random.rand(self.pop_size, self.dim) * (self.best_positions - self.population) +\n                           self.c2 * np.random.rand(self.pop_size, self.dim) * (self.global_best_position - self.population))\n        new_population = self.population + self.velocities\n        new_population = np.clip(new_population, func.bounds.lb, func.bounds.ub)\n        return new_population\n\n    def simulated_annealing(self, current_fitness, new_fitness):\n        if new_fitness < current_fitness:\n            return True\n        else:\n            delta_e = new_fitness - current_fitness\n            probability = np.exp(-delta_e / self.temperature)\n            return np.random.rand() < probability\n        \n    def update_temperature(self):\n          self.temperature = max(self.temp_min, self.temperature * self.alpha)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            new_population = self.pso_step(func)\n            new_fitness = np.array([func(x) for x in new_population])\n            self.eval_count += self.pop_size\n\n            for i in range(self.pop_size):\n                # Simulated Annealing acceptance criterion\n                if self.simulated_annealing(self.fitness[i], new_fitness[i]):\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n\n                    # Update personal best\n                    if new_fitness[i] < self.fitness[i]: #Correct comparison for personal best update.\n                        self.best_positions[i] = self.population[i].copy()\n                        self.ages[i] = 0  # Reset age if personal best is updated\n                    \n                    # Global best update\n                    if new_fitness[i] < self.global_best_fitness:\n                        self.global_best_fitness = new_fitness[i]\n                        self.global_best_position = self.population[i].copy()\n\n                else:\n                    self.ages[i] += 1\n\n            # Aging mechanism: Reset old particles\n            for i in range(self.pop_size):\n                if self.ages[i] > self.age_max:\n                    self.population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                    self.fitness[i] = func(self.population[i])\n                    self.eval_count += 1\n                    self.velocities[i] = np.random.uniform(-1, 1, size=self.dim)\n                    self.ages[i] = 0 # Reset age\n                    if self.fitness[i] < self.best_positions[i]: #comparison with personal best\n                        self.best_positions[i] = self.population[i].copy()\n                    \n                    if self.fitness[i] < self.global_best_fitness:\n                        self.global_best_fitness = self.fitness[i]\n                        self.global_best_position = self.population[i].copy()\n                    \n\n            self.update_temperature()\n\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 2, "feedback": "An exception occurred: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all().", "error": "", "parent_ids": ["8ba23326-2d62-4c20-ba48-87519d55e474"], "operator": null, "metadata": {}}
{"id": "7550fe71-c3b1-49ca-b870-350358b470fb", "fitness": -Infinity, "name": "Adaptive_PSO_SA", "description": "Adaptively adjusts PSO parameters based on population diversity and dynamically scales the SA temperature range relative to the function value range observed so far, to balance exploration and exploitation.", "code": "import numpy as np\n\nclass Adaptive_PSO_SA:\n    def __init__(self, budget=10000, dim=10, pop_size=20, w_init=0.9, w_final=0.4, c1=1.5, c2=1.5, temp_init_factor=0.1, temp_decay=0.95, sa_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w_init = w_init  # Initial inertia weight\n        self.w_final = w_final  # Final inertia weight\n        self.c1 = c1  # Cognitive coefficient\n        self.c2 = c2  # Social coefficient\n        self.temp_init_factor = temp_init_factor  # Factor for initial temperature for SA (relative to fitness range)\n        self.temp_decay = temp_decay  # Temperature decay rate\n        self.sa_prob = sa_prob # Probability of performing SA step\n        self.population = None\n        self.fitness = None\n        self.velocities = None\n        self.best_positions = None\n        self.global_best_fitness = np.inf\n        self.global_best_position = None\n        self.eval_count = 0\n        self.temperature = None\n        self.fitness_history = []\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        self.velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))  # Initialize velocities\n        self.best_positions = self.population.copy()\n        self.global_best_position = self.population[np.argmin(self.fitness)].copy()\n        self.global_best_fitness = np.min(self.fitness)\n        self.fitness_history.append(self.global_best_fitness)\n\n        #Initial Temperature Calculation\n        self.temperature = self.temp_init_factor * np.abs(np.max(self.fitness) - np.min(self.fitness)) + 1e-9 #Avoid zero division\n\n\n    def pso_step(self, func, current_iteration, total_iterations):\n        # Adaptive inertia weight\n        w = self.w_init - (self.w_init - self.w_final) * (current_iteration / total_iterations)\n\n        # Diversity Calculation for Adaptive c1, c2\n        diversity = np.std(self.population)\n        if diversity > 0.1:\n            c1 = self.c1\n            c2 = self.c2\n        else:\n            c1 = self.c1 * 0.5\n            c2 = self.c2 * 1.5\n\n        self.velocities = (w * self.velocities +\n                           c1 * np.random.rand(self.pop_size, self.dim) * (self.best_positions - self.population) +\n                           c2 * np.random.rand(self.pop_size, self.dim) * (self.global_best_position - self.population))\n        new_population = self.population + self.velocities\n        new_population = np.clip(new_population, func.bounds.lb, func.bounds.ub)\n        return new_population\n\n    def simulated_annealing_step(self, func, x, f):\n        x_new = x + np.random.normal(0, 0.1, size=self.dim)  # Generate a neighbor\n        x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n        f_new = func(x_new)\n        self.eval_count += 1\n\n        if f_new < f:\n            return x_new, f_new\n        else:\n            acceptance_probability = np.exp((f - f_new) / self.temperature)\n            if np.random.rand() < acceptance_probability:\n                return x_new, f_new\n            else:\n                return x, f\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        total_iterations = self.budget // self.pop_size # Rough estimate\n\n        iteration = 0\n        while self.eval_count < self.budget:\n            # PSO step\n            new_population = self.pso_step(func, iteration, total_iterations)\n            new_fitness = np.array([func(x) for x in new_population])\n            self.eval_count += self.pop_size\n\n            # Selection and update\n            for i in range(self.pop_size):\n                if np.random.rand() < self.sa_prob:\n                    # Simulated Annealing for local refinement\n                    x_refined, f_refined = self.simulated_annealing_step(func, new_population[i].copy(), new_fitness[i])\n                else:\n                    x_refined, f_refined = new_population[i], new_fitness[i]\n                \n                if f_refined < self.fitness[i]:\n                    self.population[i] = x_refined\n                    self.fitness[i] = f_refined\n                    if f_refined < self.global_best_fitness:\n                        self.global_best_fitness = f_refined\n                        self.global_best_position = x_refined.copy()\n                \n                # Update personal best\n                if self.fitness[i] < self.best_positions[i]: \n                    self.best_positions[i] = self.population[i].copy()\n                \n                #Global Best is updated after SA\n                if self.fitness[i] < self.global_best_fitness:\n                    self.global_best_fitness = self.fitness[i]\n                    self.global_best_position = self.population[i].copy()\n            \n            #Cooling\n            self.temperature *= self.temp_decay\n            self.fitness_history.append(self.global_best_fitness)\n            iteration += 1\n\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 2, "feedback": "An exception occurred: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all().", "error": "", "parent_ids": ["af4a3163-f999-47d6-ad61-68d23c6a50ed"], "operator": null, "metadata": {}}
{"id": "04d1befd-942b-4f5a-9c80-3133b26c4db2", "fitness": -Infinity, "name": "DE_NM", "description": "A hybrid algorithm combining the exploration of Differential Evolution with the exploitation capabilities of Nelder-Mead Simplex, adaptively switching between them based on a stagnation criterion.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DE_NM:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, Cr=0.7, stagnation_threshold=100, nm_restarts=2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.Cr = Cr  # Crossover rate\n        self.stagnation_threshold = stagnation_threshold\n        self.nm_restarts = nm_restarts # Number of Nelder-Mead restarts if stagnation occurs\n        self.population = None\n        self.fitness = None\n        self.eval_count = 0\n        self.best_fitness = np.inf\n        self.best_position = None\n        self.stagnation_counter = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        self.best_position = self.population[np.argmin(self.fitness)].copy()\n        self.best_fitness = np.min(self.fitness)\n\n    def differential_evolution_step(self, func):\n        for i in range(self.pop_size):\n            # Mutation\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = np.random.choice(idxs, 3, replace=False)\n            mutant = self.population[a] + self.F * (self.population[b] - self.population[c])\n            mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            trial_vector = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.Cr or j == j_rand:\n                    trial_vector[j] = mutant[j]\n\n            # Selection\n            f_trial = func(trial_vector)\n            self.eval_count += 1\n\n            if f_trial < self.fitness[i]:\n                self.population[i] = trial_vector\n                self.fitness[i] = f_trial\n\n                if f_trial < self.best_fitness:\n                    self.best_fitness = f_trial\n                    self.best_position = trial_vector.copy()\n\n    def nelder_mead_optimization(self, func, x0):\n         res = minimize(func, x0, method='Nelder-Mead', options={'maxiter': self.budget - self.eval_count, 'maxfev': self.budget - self.eval_count})\n         self.eval_count += res.nfev\n         return res.fun, res.x\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            old_best_fitness = self.best_fitness\n            self.differential_evolution_step(func)\n\n            if self.best_fitness >= old_best_fitness:\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                # Apply Nelder-Mead to the best solution\n                for _ in range(self.nm_restarts):\n                    f_nm, x_nm = self.nelder_mead_optimization(func, self.best_position)\n\n                    if f_nm < self.best_fitness:\n                        self.best_fitness = f_nm\n                        self.best_position = x_nm.copy()\n                        \n                    if self.eval_count >= self.budget:\n                        break # Break the restarts if we've used the budget.\n\n                self.stagnation_counter = 0  # Reset stagnation counter\n\n        return self.best_fitness, self.best_position", "configspace": "", "generation": 2, "feedback": "An exception occurred: name 'minimize' is not defined.", "error": "", "parent_ids": ["af4a3163-f999-47d6-ad61-68d23c6a50ed"], "operator": null, "metadata": {}}
{"id": "c5d79ed0-bf12-4a27-9004-bd6cf37fcb59", "fitness": 0.0, "name": "AdaptiveDifferentialEvolution", "description": "An adaptive Differential Evolution algorithm that adjusts its mutation and crossover rates based on the success of previous generations, with a self-adaptive population size.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, popsize_init=None, F_init=0.5, CR_init=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.popsize_min = 10\n        self.popsize_max = 100\n        self.popsize = popsize_init if popsize_init is not None else 2 * self.dim\n        self.F = F_init\n        self.CR = CR_init\n        self.archive_factor = 2.6  # Archive size relative to population size\n        self.archive = []\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.success_F = []\n        self.success_CR = []\n        self.success_count = 0\n        self.min_successes = 5\n        self.adapt_frequency = 10 # How often to adapt parameters\n        self.pop_adapt_frequency = 50 # How often to adapt popsize\n\n    def __call__(self, func):\n        bounds_lb = func.bounds.lb\n        bounds_ub = func.bounds.ub\n        self.population = np.random.uniform(bounds_lb, bounds_ub, size=(self.popsize, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        eval_count = self.popsize\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = self.population[np.argmin(fitness)].copy()\n\n        generation = 0\n        while eval_count < self.budget:\n            for i in range(self.popsize):\n                # Mutation\n                indices = np.random.choice(self.popsize, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n\n                mutant = x_r1 + self.F * (x_r2 - x_r3)\n                mutant = np.clip(mutant, bounds_lb, bounds_ub)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.CR\n                trial_vector = np.where(crossover_mask, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial_vector)\n                eval_count += 1\n\n                if f_trial < fitness[i]:\n                    self.success_F.append(self.F)\n                    self.success_CR.append(self.CR)\n                    self.success_count += 1\n                    self.archive.append(self.population[i].copy())\n                    if len(self.archive) > self.archive_factor * self.popsize:\n                        self.archive.pop(0)\n\n                    fitness[i] = f_trial\n                    self.population[i] = trial_vector.copy()\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial_vector.copy()\n\n                if eval_count >= self.budget:\n                    break\n\n            generation += 1\n\n            # Adapt parameters\n            if generation % self.adapt_frequency == 0 and self.success_count > self.min_successes:\n                # Adapt F and CR using Lehmer mean\n                if self.success_F:\n                    self.F = np.sum(np.array(self.success_F)**2) / np.sum(np.array(self.success_F))\n                    self.F = np.clip(self.F, 0.1, 1.0)\n                if self.success_CR:\n                    self.CR = np.mean(self.success_CR)\n                    self.CR = np.clip(self.CR, 0.1, 0.9)\n\n                self.success_F = []\n                self.success_CR = []\n                self.success_count = 0\n            elif generation % self.adapt_frequency == 0:\n                self.F = 0.5 # Default F\n                self.CR = 0.7 # Default CR\n\n            # Adapt population size\n            if generation % self.pop_adapt_frequency == 0:\n                # Adjust population size based on stagnation (simplified)\n                if np.std(fitness) < 1e-6:  #Stagnation detected\n                    self.popsize = max(self.popsize_min, self.popsize // 2) # Reduce popsize\n                else:\n                    self.popsize = min(self.popsize_max, int(self.popsize * 1.1)) # Increase popsize if progressing\n                \n                # Resize population (crude approach)\n                if self.popsize != self.population.shape[0]:\n                    new_population = np.random.uniform(bounds_lb, bounds_ub, size=(self.popsize, self.dim))\n                    new_population[:self.population.shape[0]] = self.population\n                    self.population = new_population\n                    fitness = np.array([func(x) for x in self.population])\n                    eval_count += self.popsize - self.population.shape[0]\n                    if eval_count >= self.budget:\n                        break # Ensure we don't exceed budget\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["84df2ed2-9ea6-43c5-82d9-f117b91816ab"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "84289be1-438a-476e-918e-0f9345f5aec3", "fitness": -Infinity, "name": "PSO_GradientDescent", "description": "Integrates a momentum-based PSO update with a Nelder-Mead local search triggered by stagnation, dynamically adjusting parameters based on performance.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass PSO_GradientDescent:\n    def __init__(self, budget=10000, dim=10, pop_size=20, w_start=0.9, w_end=0.4, c1=2.0, c2=2.0, lr=0.1, stagnation_threshold=15, local_search_interval=20):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w_start = w_start  # Initial inertia weight\n        self.w_end = w_end      # Final inertia weight\n        self.c1 = c1\n        self.c2 = c2\n        self.lr = lr\n        self.stagnation_threshold = stagnation_threshold\n        self.local_search_interval = local_search_interval\n        self.population = None\n        self.fitness = None\n        self.velocities = None\n        self.best_positions = None\n        self.global_best_fitness = np.inf\n        self.global_best_position = None\n        self.eval_count = 0\n        self.stagnation_counter = 0\n        self.inertia_reduction_rate = (w_start - w_end) / budget # Linear reduction of inertia\n        self.current_w = w_start\n        self.momentum = 0.9 # Add momentum to velocities\n        self.velocity_clamping = 1.0 # Clamp velocity components to this value to prevent divergence\n        self.best_fitness_history = []\n\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        self.velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))\n        self.best_positions = self.population.copy()\n        self.global_best_position = self.population[np.argmin(self.fitness)].copy()\n        self.global_best_fitness = np.min(self.fitness)\n\n    def pso_step(self, func):\n        # Update inertia weight\n        #self.current_w = self.w_start - (self.w_start - self.w_end) * (self.eval_count / self.budget)\n        self.current_w = max(self.w_end, self.w_start - self.inertia_reduction_rate * self.eval_count)\n\n        # Update velocities with momentum\n        new_velocities = (self.momentum * self.velocities +\n                          self.current_w * np.random.rand(self.pop_size, self.dim) * (self.best_positions - self.population) +\n                          self.c1 * np.random.rand(self.pop_size, self.dim) * (self.best_positions - self.population) +\n                          self.c2 * np.random.rand(self.pop_size, self.dim) * (self.global_best_position - self.population))\n\n        # Velocity clamping\n        new_velocities = np.clip(new_velocities, -self.velocity_clamping, self.velocity_clamping)\n        self.velocities = new_velocities\n\n        new_population = self.population + self.velocities\n        new_population = np.clip(new_population, func.bounds.lb, func.bounds.ub)\n        return new_population\n\n    def local_search(self, func, x):\n        bounds = [(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)] # changed from func.bounds.lb to func.bounds.lb[i]\n        result = minimize(func, x, method='Nelder-Mead', bounds=bounds, options={'maxfev': 50}) # Reduced maxfev\n        self.eval_count += result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.best_fitness_history = [self.global_best_fitness]\n\n        while self.eval_count < self.budget:\n            new_population = self.pso_step(func)\n            new_fitness = np.array([func(x) for x in new_population])\n            self.eval_count += self.pop_size\n\n            for i in range(self.pop_size):\n                if new_fitness[i] < self.fitness[i]:\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n                    self.best_positions[i] = self.population[i].copy()\n\n                    if new_fitness[i] < self.global_best_fitness:\n                        self.global_best_fitness = new_fitness[i]\n                        self.global_best_position = self.population[i].copy()\n\n            # Local search every local_search_interval iterations or if stagnated\n            if self.eval_count % self.local_search_interval == 0 or (len(self.best_fitness_history) > self.stagnation_threshold and np.std(self.best_fitness_history[-self.stagnation_threshold:]) < 1e-6):\n                for i in range(self.pop_size):\n                    x_opt, f_opt = self.local_search(func, self.population[i].copy()) # Copy before local search\n                    if f_opt < self.fitness[i]:\n                        self.population[i] = x_opt\n                        self.fitness[i] = f_opt\n                        self.best_positions[i] = self.population[i].copy()\n                        if f_opt < self.global_best_fitness:\n                            self.global_best_fitness = f_opt\n                            self.global_best_position = self.population[i].copy()\n            self.best_fitness_history.append(self.global_best_fitness)\n            \n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 2, "feedback": "An exception occurred: name 'minimize' is not defined.", "error": "", "parent_ids": ["8ba23326-2d62-4c20-ba48-87519d55e474"], "operator": null, "metadata": {}}
{"id": "86560ce9-b01e-471b-b129-8538b8f266a8", "fitness": -Infinity, "name": "AdaptiveHybridOptimization", "description": "Adaptively adjusts the probabilities of PSO and DE updates, and incorporates a more refined local search based on gradient estimation and Nelder-Mead simplex method.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimization:\n    def __init__(self, budget=10000, dim=10, pop_size=20, w_start=0.9, w_end=0.4, c1=2.0, c2=2.0, cr=0.7, f=0.8, ls_rate=0.05, pso_de_switch_prob=0.5, stagnation_threshold=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w_start = w_start  # Inertia weight start\n        self.w_end = w_end      # Inertia weight end\n        self.c1 = c1  # Cognitive coefficient\n        self.c2 = c2  # Social coefficient\n        self.cr = cr  # Crossover rate\n        self.f = f    # Mutation factor\n        self.ls_rate = ls_rate #Local search rate\n        self.pso_de_switch_prob = pso_de_switch_prob #Initial probability of choosing PSO\n        self.stagnation_threshold = stagnation_threshold\n        self.population = None\n        self.fitness = None\n        self.best_positions = None\n        self.global_best_fitness = np.inf\n        self.global_best_position = None\n        self.eval_count = 0\n        self.generation = 0\n        self.fitness_history = []\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        self.best_positions = self.population.copy()\n        self.global_best_position = self.population[np.argmin(self.fitness)].copy()\n        self.global_best_fitness = np.min(self.fitness)\n        self.fitness_history.append(self.global_best_fitness)\n\n    def update_parameters(self):\n        # Adaptive inertia weight (linearly decreasing)\n        self.w = self.w_start - (self.w_start - self.w_end) * (self.generation / (self.budget / self.pop_size))\n\n        #Adapt PSO/DE switch probability based on fitness stagnation\n        if len(self.fitness_history) > self.stagnation_threshold:\n            if np.std(self.fitness_history[-self.stagnation_threshold:]) < 1e-5:  #Stagnation detected\n                self.pso_de_switch_prob = min(1.0, self.pso_de_switch_prob + 0.1) #Favor DE\n            else:\n                self.pso_de_switch_prob = max(0.0, self.pso_de_switch_prob - 0.05) #Favor PSO\n\n    def pso_step(self):\n        velocity = (self.w * np.random.rand(self.pop_size, self.dim) * (self.population - self.best_positions) +\n                    self.c1 * np.random.rand(self.pop_size, self.dim) * (self.best_positions - self.population) +\n                    self.c2 * np.random.rand(self.pop_size, self.dim) * (self.global_best_position - self.population))\n        return velocity\n\n    def de_step(self):\n         # Differential Evolution Mutation\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_1, x_2, x_3 = self.population[idxs]\n            v_i = x_1 + self.f * (x_2 - x_3)\n            \n            # Crossover\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() > self.cr and j != j_rand:\n                    v_i[j] = self.population[i, j]\n            \n            yield i, v_i\n    \n    def local_search(self, x, func, step_size=0.1):\n        \"\"\"Performs a local search around a given solution using gradient estimation and Nelder-Mead.\"\"\"\n        \n        # 1. Gradient estimation (simplified)\n        grad = np.zeros_like(x)\n        for i in range(self.dim):\n            x_plus = x.copy()\n            x_minus = x.copy()\n            delta = step_size\n            x_plus[i] += delta\n            x_minus[i] -= delta\n            x_plus = np.clip(x_plus, func.bounds.lb, func.bounds.ub)\n            x_minus = np.clip(x_minus, func.bounds.lb, func.bounds.ub)\n            \n            f_plus = func(x_plus)\n            f_minus = func(x_minus)\n            self.eval_count += 2\n            grad[i] = (f_plus - f_minus) / (2 * delta)\n            if self.eval_count >= self.budget:\n                return x, func(x) # early return to avoid exceeding budget\n\n        # 2. Gradient Descent Step\n        learning_rate = 0.01\n        new_x = x - learning_rate * grad\n        new_x = np.clip(new_x, func.bounds.lb, func.bounds.ub)\n        \n        # 3. Nelder-Mead refinement\n        def objective(x_nm):\n            x_nm = np.clip(x_nm, func.bounds.lb, func.bounds.ub)\n            f_nm = func(x_nm)\n            return f_nm\n        \n        result = minimize(objective, new_x, method='Nelder-Mead', options={'maxfev': min(50, self.budget - self.eval_count)})\n\n        self.eval_count += result.nfev\n        if self.eval_count >= self.budget:\n             return x, func(x) # early return to avoid exceeding budget\n        \n        return result.x, result.fun\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            self.update_parameters()\n            self.generation += 1\n            \n            new_population = self.population.copy()\n            fitness_new = self.fitness.copy()\n\n            for i in range(self.pop_size):\n                # Probabilistic selection of PSO or DE\n                if np.random.rand() < self.pso_de_switch_prob:\n                    # PSO update\n                    velocity = self.pso_step()[i]\n                    new_x = self.population[i] + velocity\n                    new_x = np.clip(new_x, func.bounds.lb, func.bounds.ub)\n                    f_new = func(new_x)\n                    self.eval_count += 1\n                else:\n                    # DE update\n                    de_gen = self.de_step()\n                    for j, v_i in de_gen:\n                        if i == j:\n                            new_x = v_i\n                            new_x = np.clip(new_x, func.bounds.lb, func.bounds.ub)\n                            f_new = func(new_x)\n                            self.eval_count += 1\n                            break\n\n                #Local Search\n                if np.random.rand() < self.ls_rate:\n                   new_x, f_new = self.local_search(new_x, func)\n\n                #Greedy selection\n                if f_new < self.fitness[i]:\n                    new_population[i] = new_x\n                    fitness_new[i] = f_new\n\n                    #Update global best\n                    if f_new < self.global_best_fitness:\n                        self.global_best_fitness = f_new\n                        self.global_best_position = new_x.copy()\n\n                    self.best_positions[i] = new_x.copy()\n                elif self.fitness[i] < self.global_best_fitness:\n                     self.best_positions[i] = self.population[i].copy()\n\n            self.population = new_population\n            self.fitness = fitness_new\n            self.fitness_history.append(self.global_best_fitness)\n\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 2, "feedback": "An exception occurred: name 'minimize' is not defined.", "error": "", "parent_ids": ["3378ec50-77bf-49c8-a6bc-c67002c9464b"], "operator": null, "metadata": {}}
{"id": "0ba02fdc-1b2b-4555-b839-a332110644b7", "fitness": 0.0, "name": "AdaptiveHybridOptimization", "description": "Adaptively combines PSO and DE with a self-adaptive local search strategy based on fitness improvement, dynamically adjusting the local search step size.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimization:\n    def __init__(self, budget=10000, dim=10, pop_size=20, w_start=0.9, w_end=0.4, c1=2.0, c2=2.0, cr=0.7, f=0.8, ls_rate=0.05, initial_step_size=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w_start = w_start\n        self.w_end = w_end\n        self.c1 = c1\n        self.c2 = c2\n        self.cr = cr\n        self.f = f\n        self.ls_rate = ls_rate\n        self.initial_step_size = initial_step_size\n        self.step_size = initial_step_size * np.ones(dim)  # Individual step sizes for each dimension\n        self.population = None\n        self.fitness = None\n        self.best_positions = None\n        self.global_best_fitness = np.inf\n        self.global_best_position = None\n        self.eval_count = 0\n        self.generation = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        self.best_positions = self.population.copy()\n        self.global_best_position = self.population[np.argmin(self.fitness)].copy()\n        self.global_best_fitness = np.min(self.fitness)\n\n    def update_parameters(self):\n        self.w = self.w_start - (self.w_start - self.w_end) * (self.generation / (self.budget / self.pop_size))\n\n    def pso_step(self):\n        velocity = (self.w * np.random.rand(self.pop_size, self.dim) * (self.population - self.best_positions) +\n                    self.c1 * np.random.rand(self.pop_size, self.dim) * (self.best_positions - self.population) +\n                    self.c2 * np.random.rand(self.pop_size, self.dim) * (self.global_best_position - self.population))\n        return velocity\n\n    def de_step(self):\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_1, x_2, x_3 = self.population[idxs]\n            v_i = x_1 + self.f * (x_2 - x_3)\n            \n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() > self.cr and j != j_rand:\n                    v_i[j] = self.population[i, j]\n            \n            yield i, v_i\n    \n    def local_search(self, x, func):\n        \"\"\"Performs a local search around a given solution with adaptive step size.\"\"\"\n        new_x = x.copy()\n        new_f = func(x)\n\n        # Iterate through each dimension\n        for i in range(self.dim):\n            # Explore both directions\n            delta = np.random.uniform(-self.step_size[i], self.step_size[i])\n            temp_x = new_x.copy()\n            temp_x[i] += delta\n\n            # Clip to bounds\n            temp_x = np.clip(temp_x, func.bounds.lb, func.bounds.ub)\n\n            temp_f = func(temp_x)\n            self.eval_count += 1\n\n            if temp_f < new_f:\n                new_x = temp_x\n                new_f = temp_f\n\n        return new_x, new_f\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            self.update_parameters()\n            self.generation += 1\n            \n            new_population = self.population.copy()\n            fitness_new = self.fitness.copy()\n\n            for i in range(self.pop_size):\n                # Probabilistic selection of PSO or DE\n                if np.random.rand() < 0.5:\n                    # PSO update\n                    velocity = self.pso_step()[i]\n                    new_x = self.population[i] + velocity\n                    new_x = np.clip(new_x, func.bounds.lb, func.bounds.ub)\n                    f_new = func(new_x)\n                    self.eval_count += 1\n                else:\n                    # DE update\n                    de_gen = self.de_step()\n                    for j, v_i in de_gen:\n                        if i == j:\n                            new_x = v_i\n                            new_x = np.clip(new_x, func.bounds.lb, func.bounds.ub)\n                            f_new = func(new_x)\n                            self.eval_count += 1\n                            break\n\n                #Local Search\n                if np.random.rand() < self.ls_rate:\n                   new_x, f_new = self.local_search(new_x, func)\n\n                   # Adaptive step size adjustment\n                   if f_new < self.fitness[i]: #If local search improved the solution\n                       self.step_size *= np.exp(0.1)  # Increase step size\n                   else:\n                       self.step_size *= np.exp(-0.05) # Decrease step size\n                   self.step_size = np.clip(self.step_size, 1e-6, 0.5*(func.bounds.ub - func.bounds.lb)) #Ensure step sizes stay within reason\n\n                #Greedy selection\n                if f_new < self.fitness[i]:\n                    new_population[i] = new_x\n                    fitness_new[i] = f_new\n\n                    #Update global best\n                    if f_new < self.global_best_fitness:\n                        self.global_best_fitness = f_new\n                        self.global_best_position = new_x.copy()\n\n                    self.best_positions[i] = new_x.copy()\n                elif self.fitness[i] < self.global_best_fitness:\n                     self.best_positions[i] = self.population[i].copy()\n\n            self.population = new_population\n            self.fitness = fitness_new\n\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveHybridOptimization scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3378ec50-77bf-49c8-a6bc-c67002c9464b"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "44621c95-b96b-419e-bdae-7efbb4c9c56b", "fitness": 0.2869603107108098, "name": "CMAES", "description": "Adaptively adjusts the covariance matrix learning rate based on the condition number of the covariance matrix, promoting exploration when the matrix is ill-conditioned and exploitation otherwise, alongside a more aggressive mirroring strategy.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damp=None, c_cov=None, c_mu=None, mirror_factor=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.m = None\n        self.sigma = 0.3  # exploration parameter\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.chiN = None\n\n        self.cs = cs\n        self.damp = damp if damp is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1]) + self.cs\n        self.c_cov = c_cov if c_cov is not None else 2 / ((self.dim + np.sqrt(2))**2)\n        self.c_mu = c_mu if c_mu is not None else min(1 - self.c_cov, 2 * (self.weights[0] - 1 + self.weights[0]**2) / ((self.mu + 6 + 6*self.dim**0.5)))\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.mirror_factor = mirror_factor\n\n    def __call__(self, func):\n        self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1 / (4 * self.dim) + 1 / (21 * self.dim**2))\n        \n        used_budget = 0\n        while used_budget < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.m[:, np.newaxis] + self.sigma * C_sqrt @ z\n            \n            # Mirroring to handle bounds - more aggressive mirroring\n            xmir = self.m[:, np.newaxis] - self.mirror_factor * self.sigma * C_sqrt @ z\n            \n            # Repair mechanism to ensure solutions are within bounds and resample if necessary\n            for i in range(self.popsize):\n                if not func.bounds.lb[0] <= x[0, i] <= func.bounds.ub[0]:\n                    x[:, i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                if not func.bounds.lb[0] <= xmir[0, i] <= func.bounds.ub[0]:\n                    xmir[:, i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                    \n            f = np.array([func(x[:, i]) if used_budget < self.budget else np.inf for i in range(self.popsize)])\n            fmir = np.array([func(xmir[:, i]) if used_budget + self.popsize < self.budget else np.inf for i in range(self.popsize)])\n            \n            used_budget += 2 * self.popsize\n            \n            if np.min(f) < np.min(fmir):\n                y = x\n                fitness = f\n                best_index = np.argmin(f)\n                if f[best_index] < self.f_opt:\n                    self.f_opt = f[best_index]\n                    self.x_opt = x[:, best_index].copy()\n            else:\n                y = xmir\n                fitness = fmir\n                best_index = np.argmin(fmir)\n                if fmir[best_index] < self.f_opt:\n                    self.f_opt = fmir[best_index]\n                    self.x_opt = xmir[:, best_index].copy()\n                    \n            \n            arindex = np.argsort(fitness)\n            xmu = y[:, arindex[:self.mu]]\n            zmu = z[:, arindex[:self.mu]]\n            \n            m_old = self.m.copy()\n            self.m = np.sum(xmu * self.weights[np.newaxis, :], axis=1)\n            \n            ps_new = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs)) * (C_sqrt @ np.sum(zmu * self.weights[np.newaxis, :], axis=1))\n            self.ps = ps_new\n            \n            pc_new = (1 - self.c_cov) * self.pc + np.sqrt(self.c_cov * (2 - self.c_cov)) * (self.m - m_old) / self.sigma\n            self.pc = pc_new\n            \n            norm_ps = np.linalg.norm(self.ps)\n            factor = self.c_cov * (1 / (np.min([1, (1000000 * norm_ps**4 / self.dim)])))\n            \n            # Rank-one update of C\n            self.C = (1 - self.c_cov - self.c_mu) * self.C + self.c_cov * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :])\n            \n            # Rank-mu update of C\n            for i in range(self.mu):\n                y_diff = (xmu[:, i] - m_old) / self.sigma\n                self.C += self.c_mu * self.weights[i] * (y_diff[:, np.newaxis] @ y_diff[np.newaxis, :])\n            \n            # Adaptive c_cov based on condition number\n            condition_number = np.linalg.cond(self.C)\n            if condition_number > 1e10:  # Threshold for ill-conditioning\n                adaptive_c_cov = self.c_cov * 0.1  # Reduce learning rate for exploration\n            else:\n                adaptive_c_cov = self.c_cov # Maintain original learning rate\n            \n            self.c_cov = adaptive_c_cov\n\n            self.sigma *= np.exp((self.cs / self.damp) * (norm_ps / self.chiN - 1))\n            \n            # Ensure C remains positive definite\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)  # Reset covariance matrix if it becomes non-positive definite\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm CMAES scored 0.287 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["84df2ed2-9ea6-43c5-82d9-f117b91816ab"], "operator": null, "metadata": {"aucs": [0.08310149799103184, 0.16837622070603442, 0.43411518736070853, 0.1696091717020638, 0.23407740325432014, 0.8400363897990457, 0.29670097261756145, 0.3250806865639102, 0.15818547738730981, 0.10507064015622525, 0.25942447465848806, 0.245277286792963, 0.24885270068339638, 0.25894967835342375, 0.7325658627651057, 0.2673251351207766, 0.20455405233707113, 0.15689213308699235, 0.16222911546809005, 0.3887821274116777]}}
{"id": "46219b78-3eb5-4e62-98a4-63a027aa56f9", "fitness": -Infinity, "name": "AdaptivePSO_NelderMead", "description": "Adaptively switches between PSO and a Nelder-Mead local search based on stagnation and diversifies the population using a restart mechanism when convergence is too rapid.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptivePSO_NelderMead:\n    def __init__(self, budget=10000, dim=10, pop_size=20, w=0.7, c1=1.5, c2=1.5, stagnation_threshold=10, diversification_threshold=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w = w\n        self.c1 = c1\n        self.c2 = c2\n        self.stagnation_threshold = stagnation_threshold\n        self.diversification_threshold = diversification_threshold  # Threshold for population diversity\n        self.population = None\n        self.fitness = None\n        self.velocities = None\n        self.best_positions = None\n        self.global_best_fitness = np.inf\n        self.global_best_position = None\n        self.eval_count = 0\n        self.stagnation_counter = 0\n        self.diversity_history = []\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        self.velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))\n        self.best_positions = self.population.copy()\n        self.global_best_position = self.population[np.argmin(self.fitness)].copy()\n        self.global_best_fitness = np.min(self.fitness)\n\n    def pso_step(self, func):\n        self.velocities = (self.w * self.velocities +\n                           self.c1 * np.random.rand(self.pop_size, self.dim) * (self.best_positions - self.population) +\n                           self.c2 * np.random.rand(self.pop_size, self.dim) * (self.global_best_position - self.population))\n        new_population = self.population + self.velocities\n        new_population = np.clip(new_population, func.bounds.lb, func.bounds.ub)\n        return new_population\n\n    def nelder_mead_step(self, func, x):\n        # Nelder-Mead local search\n        bounds = [(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)]\n        result = minimize(func, x, method='Nelder-Mead', bounds=bounds, options={'maxfev': min(50, self.budget - self.eval_count)})\n        self.eval_count += result.nfev\n        return result.x, result.fun\n    \n    def calculate_diversity(self):\n        # Calculate the average distance of each particle from the center of the population\n        center = np.mean(self.population, axis=0)\n        distances = np.linalg.norm(self.population - center, axis=1)\n        diversity = np.mean(distances)\n        return diversity\n\n    def restart_population(self, func):\n         # Option to re-initialize the population if diversity is too low or stagnating\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        self.velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))\n        self.best_positions = self.population.copy()\n        self.global_best_position = self.population[np.argmin(self.fitness)].copy()\n        self.global_best_fitness = np.min(self.fitness)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        best_fitness_history = []\n        diversity_history = []\n\n        while self.eval_count < self.budget:\n            diversity = self.calculate_diversity()\n            diversity_history.append(diversity)\n\n            if len(best_fitness_history) > self.stagnation_threshold and np.std(best_fitness_history[-self.stagnation_threshold:]) < 1e-6:\n                # Fitness has stagnated, switch to Nelder-Mead for all particles.\n                for i in range(self.pop_size):\n                    self.population[i], fitness = self.nelder_mead_step(func, self.population[i])\n\n                    if fitness < self.fitness[i]:\n                        self.fitness[i] = fitness\n                        self.best_positions[i] = self.population[i].copy()\n\n                        if fitness < self.global_best_fitness:\n                            self.global_best_fitness = fitness\n                            self.global_best_position = self.population[i].copy()\n                best_fitness_history.append(self.global_best_fitness)\n\n            else:\n                # PSO Step\n                new_population = self.pso_step(func)\n                new_fitness = np.array([func(x) for x in new_population])\n                self.eval_count += self.pop_size\n\n                for i in range(self.pop_size):\n                    if new_fitness[i] < self.fitness[i]:\n                        self.population[i] = new_population[i]\n                        self.fitness[i] = new_fitness[i]\n                        self.best_positions[i] = self.population[i].copy()\n\n                        if new_fitness[i] < self.global_best_fitness:\n                            self.global_best_fitness = new_fitness[i]\n                            self.global_best_position = self.population[i].copy()\n                best_fitness_history.append(self.global_best_fitness)\n            \n            if len(diversity_history) > self.stagnation_threshold and np.std(diversity_history[-self.stagnation_threshold:]) < self.diversification_threshold:\n                self.restart_population(func)\n                best_fitness_history = [] # reset history after restart\n                diversity_history = []\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 2, "feedback": "An exception occurred: name 'minimize' is not defined.", "error": "", "parent_ids": ["8ba23326-2d62-4c20-ba48-87519d55e474"], "operator": null, "metadata": {}}
{"id": "544f0ef8-16fb-4e5d-9033-9a952c4c8bc1", "fitness": 0.24395932368710196, "name": "CMAES_Adaptive", "description": "Improved CMA-ES with adaptive covariance matrix adaptation, dynamically adjusting the learning rate based on the evolution path's alignment and adding a restart mechanism if stagnation is detected.", "code": "import numpy as np\n\nclass CMAES_Adaptive:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damp=None, c_cov=None, c_mu=None, stagnation_threshold=100):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.m = None\n        self.sigma = 0.3  # exploration parameter\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.chiN = None\n\n        self.cs = cs\n        self.damp = damp if damp is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1]) + self.cs\n        self.c_cov = c_cov if c_cov is not None else 2 / ((self.dim + np.sqrt(2))**2)\n        self.c_mu = c_mu if c_mu is not None else min(1 - self.c_cov, 2 * (self.weights[0] - 1 + self.weights[0]**2) / ((self.mu + 6 + 6*self.dim**0.5)))\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.last_f_opt = np.Inf\n        self.learning_rate_C = 1.0 # Adaptive learning rate for covariance matrix\n        self.min_learning_rate = 0.1\n\n    def __call__(self, func):\n        self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1 / (4 * self.dim) + 1 / (21 * self.dim**2))\n        \n        used_budget = 0\n        while used_budget < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.m[:, np.newaxis] + self.sigma * C_sqrt @ z\n            \n            # Mirroring to handle bounds\n            xmir = self.m[:, np.newaxis] - self.sigma * C_sqrt @ z\n            \n            # Repair mechanism to ensure solutions are within bounds\n            for i in range(self.popsize):\n                oob = np.logical_or(x[:, i] < func.bounds.lb, x[:, i] > func.bounds.ub)\n                if np.any(oob):\n                   x[oob, i] = np.random.uniform(func.bounds.lb[oob], func.bounds.ub[oob], size=np.sum(oob))\n\n                oob = np.logical_or(xmir[:, i] < func.bounds.lb, xmir[:, i] > func.bounds.ub)\n                if np.any(oob):\n                   xmir[oob, i] = np.random.uniform(func.bounds.lb[oob], func.bounds.ub[oob], size=np.sum(oob))\n                    \n            f = np.array([func(x[:, i]) if used_budget < self.budget else np.inf for i in range(self.popsize)])\n            fmir = np.array([func(xmir[:, i]) if used_budget + self.popsize < self.budget else np.inf for i in range(self.popsize)])\n            \n            used_budget += 2 * self.popsize\n            \n            if np.min(f) < np.min(fmir):\n                y = x\n                fitness = f\n                best_index = np.argmin(f)\n                if f[best_index] < self.f_opt:\n                    self.f_opt = f[best_index]\n                    self.x_opt = x[:, best_index].copy()\n            else:\n                y = xmir\n                fitness = fmir\n                best_index = np.argmin(fmir)\n                if fmir[best_index] < self.f_opt:\n                    self.f_opt = fmir[best_index]\n                    self.x_opt = xmir[:, best_index].copy()\n            \n            #Stagnation detection\n            if self.f_opt >= self.last_f_opt:\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n            \n            if self.stagnation_counter > self.stagnation_threshold:\n                #Restart mechanism: reset covariance matrix and evolution paths\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.sigma *= 0.8 #Reduce step size\n                self.stagnation_counter = 0\n\n            self.last_f_opt = self.f_opt\n                    \n            arindex = np.argsort(fitness)\n            xmu = y[:, arindex[:self.mu]]\n            zmu = z[:, arindex[:self.mu]]\n            \n            m_old = self.m.copy()\n            self.m = np.sum(xmu * self.weights[np.newaxis, :], axis=1)\n            \n            ps_new = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs)) * (C_sqrt @ np.sum(zmu * self.weights[np.newaxis, :], axis=1))\n            self.ps = ps_new\n            \n            pc_new = (1 - self.c_cov) * self.pc + np.sqrt(self.c_cov * (2 - self.c_cov)) * (self.m - m_old) / self.sigma\n            self.pc = pc_new\n            \n            norm_ps = np.linalg.norm(self.ps)\n            factor = self.c_cov * (1 / (np.min([1, (1000000 * norm_ps**4 / self.dim)])))\n            \n            # Rank-one update of C, Apply adaptive learning rate\n            self.C = (1 - self.c_cov - self.c_mu) * self.C + self.learning_rate_C * self.c_cov * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :])\n            \n            # Rank-mu update of C\n            for i in range(self.mu):\n                y_diff = (xmu[:, i] - m_old) / self.sigma\n                self.C += self.learning_rate_C * self.c_mu * self.weights[i] * (y_diff[:, np.newaxis] @ y_diff[np.newaxis, :])\n                \n            self.sigma *= np.exp((self.cs / self.damp) * (norm_ps / self.chiN - 1))\n\n            #Adaptive learning rate update based on evolution path alignment\n            alignment = np.dot(self.ps, self.pc) / (np.linalg.norm(self.ps) * np.linalg.norm(self.pc) + 1e-8)\n            self.learning_rate_C = np.clip(self.learning_rate_C * np.exp(0.1 * (alignment - 1)), self.min_learning_rate, 1.0)\n            \n            # Ensure C remains positive definite\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)  # Reset covariance matrix if it becomes non-positive definite\n                self.learning_rate_C = 1.0 #Reset the learning rate as well\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm CMAES_Adaptive scored 0.244 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["84df2ed2-9ea6-43c5-82d9-f117b91816ab"], "operator": null, "metadata": {"aucs": [0.05360087861186713, 0.1944496863737618, 0.2244818601130586, 0.17094981549314214, 0.11343936339213745, 0.15189330391484468, 0.2346748950622849, 0.13426365351248082, 0.1444818580575964, 0.1324863020291015, 0.4797234591495684, 0.20946675022175332, 0.3192379985730397, 0.22990772680596683, 0.6233907263286366, 0.32510374077287596, 0.24401181379219994, 0.6581578702851001, 0.10142052136882262, 0.13404424988379893]}}
{"id": "0aa5206b-d194-439b-8d4d-e241d0082d9b", "fitness": -Infinity, "name": "GPSurrogateLocalSearch", "description": "Combines the exploration of a Gaussian process surrogate model with the exploitation of a local search, adaptively balancing exploration and exploitation based on uncertainty.", "code": "import numpy as np\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n\nclass GPSurrogateLocalSearch:\n    def __init__(self, budget=10000, dim=10, n_initial_samples=20, exploration_weight=2.0, local_search_iterations=5):\n        self.budget = budget\n        self.dim = dim\n        self.n_initial_samples = n_initial_samples\n        self.exploration_weight = exploration_weight\n        self.local_search_iterations = local_search_iterations\n        self.X = None\n        self.y = None\n        self.gp = None\n        self.eval_count = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def initialize(self, func):\n        # Latin Hypercube Sampling for initial samples\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        self.X = np.random.uniform(lb, ub, size=(self.n_initial_samples, self.dim))\n        self.y = np.array([func(x) for x in self.X])\n        self.eval_count += self.n_initial_samples\n        \n        best_idx = np.argmin(self.y)\n        self.f_opt = self.y[best_idx]\n        self.x_opt = self.X[best_idx]\n        \n        # Gaussian Process with RBF kernel\n        kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-2, 1e2))\n        self.gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10)\n        self.gp.fit(self.X, self.y)\n\n    def acquisition_function(self, x):\n        x = x.reshape(1, -1)  # Reshape for the GP regressor\n        mu, sigma = self.gp.predict(x, return_std=True)\n        \n        # Exploration-exploitation balance\n        return -mu[0] + self.exploration_weight * sigma[0]\n    \n    def local_search(self, func, x_start):\n        x = x_start.copy()\n        f_best = func(x)\n        self.eval_count += 1\n        \n        for _ in range(self.local_search_iterations):\n            # Generate a small random perturbation\n            step_size = 0.05 * (func.bounds.ub - func.bounds.lb)\n            x_new = x + np.random.normal(0, step_size, size=self.dim)\n            x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n\n            f_new = func(x_new)\n            self.eval_count += 1\n\n            if f_new < f_best:\n                x = x_new\n                f_best = f_new\n\n        return x, f_best\n\n\n    def __call__(self, func):\n        self.initialize(func)\n\n        while self.eval_count < self.budget:\n            # Find the next point to evaluate using the acquisition function\n            from scipy.optimize import minimize\n            \n            bounds = [(func.bounds.lb, func.bounds.ub)] * self.dim\n            \n            # Start from a random location to promote exploration\n            x0 = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n            \n            res = minimize(self.acquisition_function, x0, method='L-BFGS-B', bounds=bounds)\n            x_next = res.x\n\n            # Refine the point with local search\n            x_next, f_next = self.local_search(func, x_next)\n\n            # Update the GP model\n            self.X = np.vstack((self.X, x_next))\n            self.y = np.append(self.y, f_next)\n            self.gp.fit(self.X, self.y)\n            \n            # Update the best solution found so far\n            if f_next < self.f_opt:\n                self.f_opt = f_next\n                self.x_opt = x_next\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "An exception occurred: name 'C' is not defined.", "error": "", "parent_ids": ["c2ef2713-c000-498f-9537-ffa91e6f4549"], "operator": null, "metadata": {}}
{"id": "b2b3367e-013f-4523-a9cf-7ae1a23dd093", "fitness": 0.0, "name": "AdaptiveDifferentialEvolution", "description": "Adaptive Differential Evolution with a modified mutation strategy, a selection mechanism based on probabilistic tournament selection, and adaptive scaling of F and CR parameters.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, popsize_init=None, F_init=0.5, CR_init=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.popsize_min = 10\n        self.popsize_max = 100\n        self.popsize = popsize_init if popsize_init is not None else 2 * self.dim\n        self.F = F_init\n        self.CR = CR_init\n        self.archive_factor = 2.6  # Archive size relative to population size\n        self.archive = []\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.success_F = []\n        self.success_CR = []\n        self.success_count = 0\n        self.min_successes = 5\n        self.adapt_frequency = 10 # How often to adapt parameters\n        self.pop_adapt_frequency = 50 # How often to adapt popsize\n\n    def __call__(self, func):\n        bounds_lb = func.bounds.lb\n        bounds_ub = func.bounds.ub\n        self.population = np.random.uniform(bounds_lb, bounds_ub, size=(self.popsize, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        eval_count = self.popsize\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = self.population[np.argmin(fitness)].copy()\n\n        generation = 0\n        while eval_count < self.budget:\n            for i in range(self.popsize):\n                # Mutation (DE/current-to-best/1)\n                best_index = np.argmin(fitness)\n                indices = np.random.choice(self.popsize, 2, replace=False)\n                x_r1, x_r2 = self.population[indices]\n\n                mutant = self.population[i] + self.F * (self.population[best_index] - self.population[i]) + self.F * (x_r1 - x_r2)\n                mutant = np.clip(mutant, bounds_lb, bounds_ub)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.CR\n                trial_vector = np.where(crossover_mask, mutant, self.population[i])\n\n                # Selection (Probabilistic Tournament Selection)\n                f_trial = func(trial_vector)\n                eval_count += 1\n\n                if np.random.rand() < 1/(1 + np.exp((fitness[i] - f_trial))): # Probabilistic comparison\n                    self.success_F.append(self.F)\n                    self.success_CR.append(self.CR)\n                    self.success_count += 1\n                    self.archive.append(self.population[i].copy())\n                    if len(self.archive) > self.archive_factor * self.popsize:\n                        self.archive.pop(0)\n\n                    fitness[i] = f_trial\n                    self.population[i] = trial_vector.copy()\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial_vector.copy()\n\n                if eval_count >= self.budget:\n                    break\n\n            generation += 1\n\n            # Adapt parameters\n            if generation % self.adapt_frequency == 0 and self.success_count > self.min_successes:\n                # Adapt F and CR using Lehmer mean\n                if self.success_F:\n                    self.F = np.sum(np.array(self.success_F)**2) / np.sum(np.array(self.success_F))\n                    self.F = np.clip(self.F, 0.1, 1.0)\n                if self.success_CR:\n                    self.CR = np.mean(self.success_CR)\n                    self.CR = np.clip(self.CR, 0.1, 0.9)\n\n                self.success_F = []\n                self.success_CR = []\n                self.success_count = 0\n            elif generation % self.adapt_frequency == 0:\n                self.F = 0.5 # Default F\n                self.CR = 0.7 # Default CR\n\n            # Adapt population size\n            if generation % self.pop_adapt_frequency == 0:\n                # Adjust population size based on stagnation (simplified)\n                if np.std(fitness) < 1e-6:  #Stagnation detected\n                    self.popsize = max(self.popsize_min, self.popsize // 2) # Reduce popsize\n                else:\n                    self.popsize = min(self.popsize_max, int(self.popsize * 1.1)) # Increase popsize if progressing\n                \n                # Resize population (crude approach)\n                if self.popsize != self.population.shape[0]:\n                    new_population = np.random.uniform(bounds_lb, bounds_ub, size=(self.popsize, self.dim))\n                    new_population[:self.population.shape[0]] = self.population\n                    self.population = new_population\n                    fitness = np.array([func(x) for x in self.population])\n                    eval_count += self.popsize - self.population.shape[0]\n                    if eval_count >= self.budget:\n                        break # Ensure we don't exceed budget\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c5d79ed0-bf12-4a27-9004-bd6cf37fcb59"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "35ad9b8f-7b78-4352-81f7-7de4f38de8f8", "fitness": 0.0, "name": "PSO_SA", "description": "Combines Particle Swarm Optimization with Simulated Annealing, adaptively adjusting parameters based on fitness landscape ruggedness, using a more sophisticated SA acceptance criterion based on the temperature and fitness variance.", "code": "import numpy as np\n\nclass PSO_SA:\n    def __init__(self, budget=10000, dim=10, pop_size=20, w_init=0.9, w_final=0.4, c1=2.0, c2=2.0, temp_init=1.0, temp_decay=0.95, sa_prob=0.1, ruggedness_window=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w_init = w_init  # Initial inertia weight\n        self.w_final = w_final # Final inertia weight\n        self.c1 = c1  # Cognitive coefficient\n        self.c2 = c2  # Social coefficient\n        self.temp_init = temp_init # Initial temperature for SA\n        self.temp_decay = temp_decay # Temperature decay rate\n        self.sa_prob = sa_prob # Probability of applying SA\n        self.population = None\n        self.fitness = None\n        self.velocities = None\n        self.best_positions = None\n        self.global_best_fitness = np.inf\n        self.global_best_position = None\n        self.eval_count = 0\n        self.temperature = temp_init\n        self.best_fitness_history = []\n        self.ruggedness_window = ruggedness_window\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        self.velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))  # Initialize velocities\n        self.best_positions = self.population.copy()\n        self.global_best_position = self.population[np.argmin(self.fitness)].copy()\n        self.global_best_fitness = np.min(self.fitness)\n        self.best_fitness_history.append(self.global_best_fitness)\n\n    def pso_step(self, func, current_eval):\n        # Adaptive inertia weight\n        w = self.w_init - (self.w_init - self.w_final) * (current_eval / self.budget)\n        \n        self.velocities = (w * self.velocities +\n                           self.c1 * np.random.rand(self.pop_size, self.dim) * (self.best_positions - self.population) +\n                           self.c2 * np.random.rand(self.pop_size, self.dim) * (self.global_best_position - self.population))\n        new_population = self.population + self.velocities\n        new_population = np.clip(new_population, func.bounds.lb, func.bounds.ub)\n        return new_population\n\n    def simulated_annealing_step(self, func, x, f):\n        # Adaptive SA step size\n        step_size = 0.1 * (func.bounds.ub - func.bounds.lb) # Scale based on bounds\n\n        x_new = x + np.random.normal(0, step_size, size=self.dim)  # Generate a neighbor\n        x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n        f_new = func(x_new)\n        self.eval_count += 1\n\n        if f_new < f:\n            return x_new, f_new\n        else:\n            # Adaptive Acceptance probability based on landscape ruggedness\n            if len(self.best_fitness_history) > self.ruggedness_window:\n                fitness_window = self.best_fitness_history[-self.ruggedness_window:]\n                fitness_variance = np.var(fitness_window)\n                acceptance_probability = np.exp((f - f_new) / (self.temperature * (fitness_variance + 1e-8))) # Add small value to prevent division by zero\n            else:\n                 acceptance_probability = np.exp((f - f_new) / self.temperature)\n\n            if np.random.rand() < acceptance_probability:\n                return x_new, f_new\n            else:\n                return x, f\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            current_eval = self.eval_count\n            # PSO step\n            new_population = self.pso_step(func, current_eval)\n            new_fitness = np.array([func(x) for x in new_population])\n            self.eval_count += self.pop_size\n\n            # Selection and update\n            for i in range(self.pop_size):\n                # Apply SA with probability\n                if np.random.rand() < self.sa_prob:\n                    x_refined, f_refined = self.simulated_annealing_step(func, new_population[i].copy(), new_fitness[i])\n                else:\n                    x_refined = new_population[i]\n                    f_refined = new_fitness[i]\n                \n                if f_refined < self.fitness[i]:\n                    self.population[i] = x_refined\n                    self.fitness[i] = f_refined\n                    if f_refined < self.global_best_fitness:\n                        self.global_best_fitness = f_refined\n                        self.global_best_position = x_refined.copy()\n                \n                # Update personal best\n                if self.fitness[i] > f_refined: # Compare with refined version\n                    self.best_positions[i] = self.population[i].copy()\n                    self.fitness[i] = f_refined\n                \n                #Global Best is updated after SA\n                if self.fitness[i] < self.global_best_fitness:\n                    self.global_best_fitness = self.fitness[i]\n                    self.global_best_position = self.population[i].copy()\n            \n            #Cooling\n            self.temperature *= self.temp_decay\n            self.best_fitness_history.append(self.global_best_fitness)\n\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 3, "feedback": "The algorithm PSO_SA scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c2ef2713-c000-498f-9537-ffa91e6f4549"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "da9b3623-df7e-4835-a408-9357350cafa7", "fitness": 0.0, "name": "AdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution with jittering, archive-based mutation, and dynamic adaptation of F and CR based on successful parameter pairs and dynamic population size adjustment.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, popsize_init=None, F_init=0.5, CR_init=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.popsize_min = 10\n        self.popsize_max = 100\n        self.popsize = popsize_init if popsize_init is not None else 2 * self.dim\n        self.F = F_init\n        self.CR = CR_init\n        self.archive_factor = 2.6  # Archive size relative to population size\n        self.archive = []\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.success_params = []  # Store successful (F, CR) pairs\n        self.min_successes = 5\n        self.adapt_frequency = 10  # How often to adapt parameters\n        self.pop_adapt_frequency = 50  # How often to adapt popsize\n        self.jitter_probability = 0.1 # Probability of applying jitter to F and CR\n\n    def __call__(self, func):\n        bounds_lb = func.bounds.lb\n        bounds_ub = func.bounds.ub\n        self.population = np.random.uniform(bounds_lb, bounds_ub, size=(self.popsize, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        eval_count = self.popsize\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = self.population[np.argmin(fitness)].copy()\n\n        generation = 0\n        while eval_count < self.budget:\n            for i in range(self.popsize):\n                # Parameter adaptation for each individual\n                F = self.F\n                CR = self.CR\n                if np.random.rand() < self.jitter_probability:\n                    F += np.random.normal(0, 0.1 * self.F) # Apply jitter to F\n                    CR += np.random.normal(0, 0.1 * self.CR) # Apply jitter to CR\n                    F = np.clip(F, 0.1, 1.0)\n                    CR = np.clip(CR, 0.1, 0.9)\n\n                # Mutation - Archive-based with random selection between population and archive\n                indices = np.random.choice(self.popsize, 2, replace=False)\n                x_r1, x_r2 = self.population[indices]\n\n                if len(self.archive) > 0 and np.random.rand() < 0.2:\n                    x_r3 = self.archive[np.random.randint(len(self.archive))]\n                else:\n                     index3 = np.random.choice(self.popsize, 1, replace=False)[0]\n                     while index3 == i or index3 in indices:\n                         index3 = np.random.choice(self.popsize, 1, replace=False)[0]\n                     x_r3 = self.population[index3]\n\n                mutant = x_r1 + F * (x_r2 - x_r3)\n                mutant = np.clip(mutant, bounds_lb, bounds_ub)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial_vector = np.where(crossover_mask, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial_vector)\n                eval_count += 1\n\n                if f_trial < fitness[i]:\n                    self.success_params.append((F, CR))\n                    self.archive.append(self.population[i].copy())\n                    if len(self.archive) > int(self.archive_factor * self.popsize):\n                        self.archive.pop(0)\n\n                    fitness[i] = f_trial\n                    self.population[i] = trial_vector.copy()\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial_vector.copy()\n\n                if eval_count >= self.budget:\n                    break\n\n            generation += 1\n\n            # Adapt parameters\n            if generation % self.adapt_frequency == 0 and len(self.success_params) > self.min_successes:\n                # Adapt F and CR using weighted average of successful pairs\n                weights = np.linspace(1, 0, len(self.success_params))  # Recency-weighted average\n                weights /= np.sum(weights)\n                F_sum = 0\n                CR_sum = 0\n                for j, (F, CR) in enumerate(self.success_params):\n                  F_sum += weights[j] * F\n                  CR_sum += weights[j] * CR\n                self.F = np.clip(F_sum, 0.1, 1.0)\n                self.CR = np.clip(CR_sum, 0.1, 0.9)\n                self.success_params = []\n\n            elif generation % self.adapt_frequency == 0:\n                self.F = 0.5  # Default F\n                self.CR = 0.7  # Default CR\n\n            # Adapt population size using a more responsive approach\n            if generation % self.pop_adapt_frequency == 0:\n                fitness_std = np.std(fitness)\n                if fitness_std < 1e-6:  # Stagnation detected - increase exploration\n                    self.popsize = min(self.popsize_max, int(self.popsize * 1.2))\n                elif fitness_std > 0.1: # Potential for exploitation\n                    self.popsize = max(self.popsize_min, int(self.popsize * 0.9)) # Reduce popsize to focus\n                # else, keep popsize constant.\n\n                # Resize population (more robust)\n                new_popsize = int(np.clip(self.popsize, self.popsize_min, self.popsize_max))\n                if new_popsize != self.population.shape[0]:\n                    self.popsize = new_popsize\n                    new_population = np.random.uniform(bounds_lb, bounds_ub, size=(self.popsize, self.dim))\n                    if self.population.shape[0] < self.popsize:\n                        new_population[:self.population.shape[0]] = self.population\n                    else:\n                        new_population = self.population[:self.popsize]\n\n                    self.population = new_population\n                    fitness = np.array([func(x) for x in self.population])\n                    eval_count += self.popsize - self.population.shape[0]\n                    if eval_count >= self.budget:\n                        break  # Ensure we don't exceed budget\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c5d79ed0-bf12-4a27-9004-bd6cf37fcb59"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "bf649231-1938-4015-8b26-6a3e667ef8ec", "fitness": -Infinity, "name": "CMAES_Orthogonal", "description": "A Covariance Matrix Adaptation Evolution Strategy (CMA-ES) variant with orthogonal subspace sampling and a dynamic population size adjustment strategy, adaptively tuning the sampling distribution and population diversity to accelerate convergence and escape local optima.", "code": "import numpy as np\n\nclass CMAES_Orthogonal:\n    def __init__(self, budget=10000, dim=10, initial_popsize=None, cs=0.3, damp=None, c_cov=None, c_mu=None, stagnation_threshold=50, popsize_decay=0.95, popsize_increase=1.05):\n        self.budget = budget\n        self.dim = dim\n        self.initial_popsize = initial_popsize if initial_popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.popsize = self.initial_popsize\n        self.mu = int(self.popsize // 2)\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.m = None\n        self.sigma = 0.3  # exploration parameter\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.chiN = None\n\n        self.cs = cs\n        self.damp = damp if damp is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1]) + self.cs\n        self.c_cov = c_cov if c_cov is not None else 2 / ((self.dim + np.sqrt(2))**2)\n        self.c_mu = c_mu if c_mu is not None else min(1 - self.c_cov, 2 * (self.weights[0] - 1 + self.weights[0]**2) / ((self.mu + 6 + 6*self.dim**0.5)))\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.last_f_opt = np.Inf\n        self.popsize_decay = popsize_decay\n        self.popsize_increase = popsize_increase\n\n    def orthogonal_sampling(self, n_samples):\n        H = np.random.normal(0, 1, size=(self.dim, n_samples))\n        Q, _ = np.linalg.qr(H)\n        return Q\n\n    def __call__(self, func):\n        self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1 / (4 * self.dim) + 1 / (21 * self.dim**2))\n        \n        used_budget = 0\n        while used_budget < self.budget:\n            self.mu = int(self.popsize // 2)\n            self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n            self.weights /= np.sum(self.weights)\n            \n            #Orthogonal sampling\n            z = self.orthogonal_sampling(self.popsize)\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.m[:, np.newaxis] + self.sigma * C_sqrt @ z\n\n            #Mirroring to handle bounds\n            xmir = self.m[:, np.newaxis] - self.sigma * C_sqrt @ z\n            \n            # Repair mechanism to ensure solutions are within bounds\n            for i in range(self.popsize):\n                oob = np.logical_or(x[:, i] < func.bounds.lb, x[:, i] > func.bounds.ub)\n                if np.any(oob):\n                   x[oob, i] = np.random.uniform(func.bounds.lb[oob], func.bounds.ub[oob], size=np.sum(oob))\n                \n                oob = np.logical_or(xmir[:, i] < func.bounds.lb, xmir[:, i] > func.bounds.ub)\n                if np.any(oob):\n                   xmir[oob, i] = np.random.uniform(func.bounds.lb[oob], func.bounds.ub[oob], size=np.sum(oob))\n            \n            f = np.array([func(x[:, i]) if used_budget < self.budget else np.inf for i in range(self.popsize)])\n            fmir = np.array([func(xmir[:, i]) if used_budget + self.popsize < self.budget else np.inf for i in range(self.popsize)])\n\n            used_budget += 2 * self.popsize\n            \n            if np.min(f) < np.min(fmir):\n                y = x\n                fitness = f\n                best_index = np.argmin(f)\n                if f[best_index] < self.f_opt:\n                    self.f_opt = f[best_index]\n                    self.x_opt = x[:, best_index].copy()\n            else:\n                y = xmir\n                fitness = fmir\n                best_index = np.argmin(fmir)\n                if fmir[best_index] < self.f_opt:\n                    self.f_opt = fmir[best_index]\n                    self.x_opt = xmir[:, best_index].copy()\n\n            # Stagnation detection\n            if self.f_opt >= self.last_f_opt:\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n            \n            if self.stagnation_counter > self.stagnation_threshold:\n                # Reduce population size and reset covariance matrix\n                self.popsize = max(4, int(self.popsize * self.popsize_decay))\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.sigma *= 0.9\n                self.stagnation_counter = 0\n            else:\n                # Increase population size if improving\n                self.popsize = min(self.initial_popsize * 2, int(self.popsize * self.popsize_increase))\n\n            self.last_f_opt = self.f_opt\n            \n            arindex = np.argsort(fitness)\n            xmu = y[:, arindex[:self.mu]]\n            zmu = z[:, arindex[:self.mu]]\n            \n            m_old = self.m.copy()\n            self.m = np.sum(xmu * self.weights[np.newaxis, :], axis=1)\n            \n            ps_new = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs)) * (C_sqrt @ np.sum(zmu * self.weights[np.newaxis, :], axis=1))\n            self.ps = ps_new\n            \n            pc_new = (1 - self.c_cov) * self.pc + np.sqrt(self.c_cov * (2 - self.c_cov)) * (self.m - m_old) / self.sigma\n            self.pc = pc_new\n            \n            norm_ps = np.linalg.norm(self.ps)\n            factor = self.c_cov * (1 / (np.min([1, (1000000 * norm_ps**4 / self.dim)])))\n            \n            self.C = (1 - self.c_cov - self.c_mu) * self.C + self.c_cov * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :])\n            \n            for i in range(self.mu):\n                y_diff = (xmu[:, i] - m_old) / self.sigma\n                self.C += self.c_mu * self.weights[i] * (y_diff[:, np.newaxis] @ y_diff[np.newaxis, :])\n                \n            self.sigma *= np.exp((self.cs / self.damp) * (norm_ps / self.chiN - 1))\n            \n            # Ensure C remains positive definite\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "An exception occurred: index 2 is out of bounds for axis 1 with size 2.", "error": "", "parent_ids": ["544f0ef8-16fb-4e5d-9033-9a952c4c8bc1"], "operator": null, "metadata": {}}
{"id": "85679c7e-60fd-4eb6-a203-c007e65ce2e2", "fitness": -Infinity, "name": "CMAES_Orthogonal", "description": "CMA-ES variant incorporating orthogonal sampling to enhance exploration in ill-conditioned spaces and adaptive population sizing based on function evaluation distribution.", "code": "import numpy as np\n\nclass CMAES_Orthogonal:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damp=None, c_cov=None, c_mu=None, stagnation_threshold=100, ortho_prob=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.m = None\n        self.sigma = 0.3\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.chiN = None\n\n        self.cs = cs\n        self.damp = damp if damp is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1]) + self.cs\n        self.c_cov = c_cov if c_cov is not None else 2 / ((self.dim + np.sqrt(2))**2)\n        self.c_mu = c_mu if c_mu is not None else min(1 - self.c_cov, 2 * (self.weights[0] - 1 + self.weights[0]**2) / ((self.mu + 6 + 6*self.dim**0.5)))\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.last_f_opt = np.Inf\n        self.ortho_prob = ortho_prob\n        self.learning_rate_C = 1.0\n        self.min_learning_rate = 0.1\n        self.popsize_factor = 1.0 #Factor to adjust population size dynamically\n\n    def __call__(self, func):\n        self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1 / (4 * self.dim) + 1 / (21 * self.dim**2))\n\n        used_budget = 0\n        while used_budget < self.budget:\n            current_popsize = int(self.popsize * self.popsize_factor)\n            if current_popsize < 4:\n                current_popsize = 4\n            z = np.random.normal(0, 1, size=(self.dim, current_popsize))\n\n            # Orthogonal sampling with a certain probability\n            if np.random.rand() < self.ortho_prob:\n                Q, _ = np.linalg.qr(z)\n                z = Q\n\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.m[:, np.newaxis] + self.sigma * C_sqrt @ z\n            xmir = self.m[:, np.newaxis] - self.sigma * C_sqrt @ z\n\n            # Repair mechanism to ensure solutions are within bounds\n            for i in range(current_popsize):\n                oob = np.logical_or(x[:, i] < func.bounds.lb, x[:, i] > func.bounds.ub)\n                if np.any(oob):\n                    x[oob, i] = np.random.uniform(func.bounds.lb[oob], func.bounds.ub[oob], size=np.sum(oob))\n\n                oob = np.logical_or(xmir[:, i] < func.bounds.lb, xmir[:, i] > func.bounds.ub)\n                if np.any(oob):\n                    xmir[oob, i] = np.random.uniform(func.bounds.lb[oob], func.bounds.ub[oob], size=np.sum(oob))\n\n            f = np.array([func(x[:, i]) if used_budget < self.budget else np.inf for i in range(current_popsize)])\n            fmir = np.array([func(xmir[:, i]) if used_budget + current_popsize < self.budget else np.inf for i in range(current_popsize)])\n            used_budget += 2 * current_popsize\n\n            if np.min(f) < np.min(fmir):\n                y = x\n                fitness = f\n                best_index = np.argmin(f)\n                if f[best_index] < self.f_opt:\n                    self.f_opt = f[best_index]\n                    self.x_opt = x[:, best_index].copy()\n            else:\n                y = xmir\n                fitness = fmir\n                best_index = np.argmin(fmir)\n                if fmir[best_index] < self.f_opt:\n                    self.f_opt = fmir[best_index]\n                    self.x_opt = xmir[:, best_index].copy()\n\n            #Stagnation detection\n            if self.f_opt >= self.last_f_opt:\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                #Restart mechanism: reset covariance matrix and evolution paths\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.sigma *= 0.8\n                self.stagnation_counter = 0\n                self.popsize_factor = 1.0\n\n            self.last_f_opt = self.f_opt\n\n            arindex = np.argsort(fitness)\n            xmu = y[:, arindex[:self.mu]]\n            zmu = z[:, arindex[:self.mu]]\n\n            m_old = self.m.copy()\n            self.m = np.sum(xmu * self.weights[np.newaxis, :], axis=1)\n\n            ps_new = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs)) * (C_sqrt @ np.sum(zmu * self.weights[np.newaxis, :], axis=1))\n            self.ps = ps_new\n\n            pc_new = (1 - self.c_cov) * self.pc + np.sqrt(self.c_cov * (2 - self.c_cov)) * (self.m - m_old) / self.sigma\n            self.pc = pc_new\n            norm_ps = np.linalg.norm(self.ps)\n            factor = self.c_cov * (1 / (np.min([1, (1000000 * norm_ps**4 / self.dim)])))\n\n            # Rank-one update of C, apply adaptive learning rate\n            self.C = (1 - self.c_cov - self.c_mu) * self.C + self.learning_rate_C * self.c_cov * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :])\n            # Rank-mu update of C\n            for i in range(self.mu):\n                y_diff = (xmu[:, i] - m_old) / self.sigma\n                self.C += self.learning_rate_C * self.c_mu * self.weights[i] * (y_diff[:, np.newaxis] @ y_diff[np.newaxis, :])\n                \n            self.sigma *= np.exp((self.cs / self.damp) * (norm_ps / self.chiN - 1))\n            \n            #Adaptive learning rate update based on evolution path alignment\n            alignment = np.dot(self.ps, self.pc) / (np.linalg.norm(self.ps) * np.linalg.norm(self.pc) + 1e-8)\n            self.learning_rate_C = np.clip(self.learning_rate_C * np.exp(0.1 * (alignment - 1)), self.min_learning_rate, 1.0)\n            \n            # Ensure C remains positive definite\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)  # Reset covariance matrix if it becomes non-positive definite\n                self.learning_rate_C = 1.0\n\n            # Adaptive population sizing\n            if used_budget > self.budget * 0.25 and self.stagnation_counter > self.stagnation_threshold // 2:\n                self.popsize_factor *= 0.9  # Reduce population size if stagnating\n            elif used_budget > self.budget * 0.5 and self.stagnation_counter < self.stagnation_threshold // 4:\n                self.popsize_factor *= 1.1 # Increase population size if progressing well after halfway point\n\n            self.popsize_factor = np.clip(self.popsize_factor, 0.5, 2.0) # Limit the population size variation\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "An exception occurred: index 5 is out of bounds for axis 1 with size 5.", "error": "", "parent_ids": ["544f0ef8-16fb-4e5d-9033-9a952c4c8bc1"], "operator": null, "metadata": {}}
{"id": "7c5eff6d-6304-4dc5-91ce-8aac1f150ad9", "fitness": 0.2602201974914113, "name": "CMAES_Cauchy", "description": "An adaptive CMA-ES variant that incorporates a Cauchy mutation operator for enhanced exploration, adjusts step size based on fitness variance, and resets the covariance matrix if diversity collapses.", "code": "import numpy as np\n\nclass CMAES_Cauchy:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damp=None, c_cov=None, c_mu=None, stagnation_threshold=100, cauchy_mutation_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.m = None\n        self.sigma = 0.3  # exploration parameter\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.chiN = None\n\n        self.cs = cs\n        self.damp = damp if damp is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1]) + self.cs\n        self.c_cov = c_cov if c_cov is not None else 2 / ((self.dim + np.sqrt(2))**2)\n        self.c_mu = c_mu if c_mu is not None else min(1 - self.c_cov, 2 * (self.weights[0] - 1 + self.weights[0]**2) / ((self.mu + 6 + 6*self.dim**0.5)))\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.last_f_opt = np.Inf\n        self.cauchy_mutation_prob = cauchy_mutation_prob\n        self.diversity_threshold = 1e-8 # threshold for considering diversity collapse\n\n    def __call__(self, func):\n        self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1 / (4 * self.dim) + 1 / (21 * self.dim**2))\n        \n        used_budget = 0\n        while used_budget < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.m[:, np.newaxis] + self.sigma * C_sqrt @ z\n            \n            # Apply Cauchy mutation with a probability\n            for i in range(self.popsize):\n                if np.random.rand() < self.cauchy_mutation_prob:\n                    cauchy_noise = np.random.standard_cauchy(size=self.dim)\n                    x[:, i] += self.sigma * cauchy_noise  #Scale cauchy noise with sigma\n\n            # Mirroring to handle bounds\n            xmir = self.m[:, np.newaxis] - self.sigma * C_sqrt @ z\n            \n            # Repair mechanism to ensure solutions are within bounds\n            for i in range(self.popsize):\n                oob = np.logical_or(x[:, i] < func.bounds.lb, x[:, i] > func.bounds.ub)\n                if np.any(oob):\n                   x[oob, i] = np.random.uniform(func.bounds.lb[oob], func.bounds.ub[oob], size=np.sum(oob))\n\n                oob = np.logical_or(xmir[:, i] < func.bounds.lb, xmir[:, i] > func.bounds.ub)\n                if np.any(oob):\n                   xmir[oob, i] = np.random.uniform(func.bounds.lb[oob], func.bounds.ub[oob], size=np.sum(oob))\n                    \n            f = np.array([func(x[:, i]) if used_budget < self.budget else np.inf for i in range(self.popsize)])\n            fmir = np.array([func(xmir[:, i]) if used_budget + self.popsize < self.budget else np.inf for i in range(self.popsize)])\n            \n            used_budget += 2 * self.popsize\n            \n            if np.min(f) < np.min(fmir):\n                y = x\n                fitness = f\n                best_index = np.argmin(f)\n                if f[best_index] < self.f_opt:\n                    self.f_opt = f[best_index]\n                    self.x_opt = x[:, best_index].copy()\n            else:\n                y = xmir\n                fitness = fmir\n                best_index = np.argmin(fmir)\n                if fmir[best_index] < self.f_opt:\n                    self.f_opt = fmir[best_index]\n                    self.x_opt = xmir[:, best_index].copy()\n            \n            #Stagnation detection\n            if self.f_opt >= self.last_f_opt:\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n            \n            if self.stagnation_counter > self.stagnation_threshold:\n                #Restart mechanism: reset covariance matrix and evolution paths\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.sigma *= 0.8 #Reduce step size\n                self.stagnation_counter = 0\n\n            self.last_f_opt = self.f_opt\n                    \n            arindex = np.argsort(fitness)\n            xmu = y[:, arindex[:self.mu]]\n            zmu = z[:, arindex[:self.mu]]\n            \n            m_old = self.m.copy()\n            self.m = np.sum(xmu * self.weights[np.newaxis, :], axis=1)\n            \n            ps_new = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs)) * (C_sqrt @ np.sum(zmu * self.weights[np.newaxis, :], axis=1))\n            self.ps = ps_new\n            \n            pc_new = (1 - self.c_cov) * self.pc + np.sqrt(self.c_cov * (2 - self.c_cov)) * (self.m - m_old) / self.sigma\n            self.pc = pc_new\n            \n            norm_ps = np.linalg.norm(self.ps)\n            factor = self.c_cov * (1 / (np.min([1, (1000000 * norm_ps**4 / self.dim)])))\n            \n            # Rank-one update of C\n            self.C = (1 - self.c_cov - self.c_mu) * self.C + self.c_cov * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :])\n            \n            # Rank-mu update of C\n            for i in range(self.mu):\n                y_diff = (xmu[:, i] - m_old) / self.sigma\n                self.C += self.c_mu * self.weights[i] * (y_diff[:, np.newaxis] @ y_diff[np.newaxis, :])\n\n            # Adaptive step size adjustment based on fitness variance\n            fitness_variance = np.var(fitness)\n            if fitness_variance > 1e-3:\n                self.sigma *= np.exp(0.05 * (fitness_variance - 1e-3))\n            else:\n                self.sigma *= np.exp(-0.05 * (1e-3 - fitness_variance))\n            \n            self.sigma = max(self.sigma, 1e-8) # Ensure sigma doesn't go to zero\n\n            self.sigma *= np.exp((self.cs / self.damp) * (norm_ps / self.chiN - 1))\n\n            # Check for diversity collapse and reset covariance matrix if needed\n            if np.linalg.norm(self.C - np.diag(np.diag(self.C))) < self.diversity_threshold:\n                self.C = np.eye(self.dim)\n            \n            # Ensure C remains positive definite\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)  # Reset covariance matrix if it becomes non-positive definite\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm CMAES_Cauchy scored 0.260 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["544f0ef8-16fb-4e5d-9033-9a952c4c8bc1"], "operator": null, "metadata": {"aucs": [0.025981350531692482, 0.16918105148257834, 0.6507999358575569, 0.9512082319447598, 0.2091314672680803, 0.14673019179852276, 0.20355571821738694, 0.20402742223231074, 0.1811827135715992, 0.1642238261264256, 0.2811934040976787, 0.14931410981835946, 0.0, 0.17139984476442804, 0.556696525122457, 0.30988680694129944, 0.21222288397873446, 0.28530797764750737, 0.1532649322034093, 0.17909555622343887]}}
{"id": "d3e25166-dcfc-49dd-9cab-47920fdbee21", "fitness": 0.4297659965915406, "name": "CMAES_AdaptiveRestart", "description": "CMA-ES with adaptive learning rate adjustments based on covariance matrix properties, population diversity, and a new restart mechanism triggered by stagnation or ill-conditioned covariance.", "code": "import numpy as np\n\nclass CMAES_AdaptiveRestart:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damp=None, c_cov=None, c_mu=None, mirror_factor=1.0, stagnation_threshold=1e-9):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.m = None\n        self.sigma = 0.3  # exploration parameter\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.chiN = None\n\n        self.cs = cs\n        self.damp = damp if damp is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1]) + self.cs\n        self.c_cov = c_cov if c_cov is not None else 2 / ((self.dim + np.sqrt(2))**2)\n        self.c_mu = c_mu if c_mu is not None else min(1 - self.c_cov, 2 * (self.weights[0] - 1 + self.weights[0]**2) / ((self.mu + 6 + 6*self.dim**0.5)))\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.mirror_factor = mirror_factor\n        self.stagnation_threshold = stagnation_threshold\n        self.last_improvement = 0\n        self.max_no_improvement = 100 # generations before restart\n\n    def __call__(self, func):\n        self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1 / (4 * self.dim) + 1 / (21 * self.dim**2))\n        \n        used_budget = 0\n        generation = 0\n\n        while used_budget < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.m[:, np.newaxis] + self.sigma * C_sqrt @ z\n            \n            # Mirroring to handle bounds - more aggressive mirroring\n            xmir = self.m[:, np.newaxis] - self.mirror_factor * self.sigma * C_sqrt @ z\n            \n            # Repair mechanism to ensure solutions are within bounds and resample if necessary\n            for i in range(self.popsize):\n                if not func.bounds.lb[0] <= x[0, i] <= func.bounds.ub[0]:\n                    x[:, i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                if not func.bounds.lb[0] <= xmir[0, i] <= func.bounds.ub[0]:\n                    xmir[:, i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                    \n            f = np.array([func(x[:, i]) if used_budget < self.budget else np.inf for i in range(self.popsize)])\n            fmir = np.array([func(xmir[:, i]) if used_budget + self.popsize < self.budget else np.inf for i in range(self.popsize)])\n            \n            used_budget += 2 * self.popsize\n            \n            if np.min(f) < np.min(fmir):\n                y = x\n                fitness = f\n                best_index = np.argmin(f)\n                if f[best_index] < self.f_opt:\n                    self.f_opt = f[best_index]\n                    self.x_opt = x[:, best_index].copy()\n                    self.last_improvement = generation\n            else:\n                y = xmir\n                fitness = fmir\n                best_index = np.argmin(fmir)\n                if fmir[best_index] < self.f_opt:\n                    self.f_opt = fmir[best_index]\n                    self.x_opt = xmir[:, best_index].copy()\n                    self.last_improvement = generation\n                    \n            \n            arindex = np.argsort(fitness)\n            xmu = y[:, arindex[:self.mu]]\n            zmu = z[:, arindex[:self.mu]]\n            \n            m_old = self.m.copy()\n            self.m = np.sum(xmu * self.weights[np.newaxis, :], axis=1)\n            \n            ps_new = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs)) * (C_sqrt @ np.sum(zmu * self.weights[np.newaxis, :], axis=1))\n            self.ps = ps_new\n            \n            pc_new = (1 - self.c_cov) * self.pc + np.sqrt(self.c_cov * (2 - self.c_cov)) * (self.m - m_old) / self.sigma\n            self.pc = pc_new\n            \n            norm_ps = np.linalg.norm(self.ps)\n            factor = self.c_cov * (1 / (np.min([1, (1000000 * norm_ps**4 / self.dim)])))\n            \n            # Rank-one update of C\n            self.C = (1 - self.c_cov - self.c_mu) * self.C + self.c_cov * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :])\n            \n            # Rank-mu update of C\n            for i in range(self.mu):\n                y_diff = (xmu[:, i] - m_old) / self.sigma\n                self.C += self.c_mu * self.weights[i] * (y_diff[:, np.newaxis] @ y_diff[np.newaxis, :])\n            \n            # Adaptive c_cov based on condition number\n            condition_number = np.linalg.cond(self.C)\n            \n            # Adaptive sigma based on population diversity\n            pop_diversity = np.std(fitness)\n            if pop_diversity < self.stagnation_threshold:\n                  self.sigma *= 0.9  # Reduce step size if population is too uniform\n            else:\n                self.sigma *= 1.1\n\n            if condition_number > 1e10:  # Threshold for ill-conditioning\n                adaptive_c_cov = self.c_cov * 0.1  # Reduce learning rate for exploration\n            else:\n                adaptive_c_cov = self.c_cov # Maintain original learning rate\n            \n            self.c_cov = adaptive_c_cov\n\n            self.sigma *= np.exp((self.cs / self.damp) * (norm_ps / self.chiN - 1))\n            \n            # Ensure C remains positive definite\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)  # Reset covariance matrix if it becomes non-positive definite\n\n            # Restart mechanism\n            if generation - self.last_improvement > self.max_no_improvement:\n                self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.sigma = 0.3  # Reset sigma to initial value\n                self.last_improvement = generation # reset last improvement\n                \n            generation += 1\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm CMAES_AdaptiveRestart scored 0.430 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["44621c95-b96b-419e-bdae-7efbb4c9c56b"], "operator": null, "metadata": {"aucs": [0.24770257950754804, 0.3019575158666603, 0.4365050574709024, 0.42248802224957127, 0.27637018424502746, 0.7986869233339918, 0.316461692532118, 0.5503132632477592, 0.1890064943359725, 0.16483601337047382, 0.3824704900804693, 0.997424485605042, 0.26668807348959445, 0.2622476511142863, 0.8084513543256248, 0.33968092242720116, 0.26849065858910315, 0.9153305520052766, 0.15689490242736626, 0.4933130956068218]}}
{"id": "91d960d4-19a7-4830-9d58-6d9e633f033c", "fitness": 0.21005419581003149, "name": "CMAES_DualCovariance", "description": "A CMA-ES variant that uses a separate covariance matrix for exploration and exploitation phases, switching between them based on stagnation detection, and introduces orthogonal sampling.", "code": "import numpy as np\n\nclass CMAES_DualCovariance:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damp=None, c_cov=None, c_mu=None, stagnation_threshold=100, orthogonal_factor=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.m = None\n        self.sigma = 0.3\n        self.C_exploit = None  # Covariance matrix for exploitation\n        self.C_explore = None  # Covariance matrix for exploration\n        self.pc_exploit = None\n        self.ps_exploit = None\n        self.pc_explore = None\n        self.ps_explore = None\n        self.chiN = None\n\n        self.cs = cs\n        self.damp = damp if damp is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1]) + self.cs\n        self.c_cov = c_cov if c_cov is not None else 2 / ((self.dim + np.sqrt(2))**2)\n        self.c_mu = c_mu if c_mu is not None else min(1 - self.c_cov, 2 * (self.weights[0] - 1 + self.weights[0]**2) / ((self.mu + 6 + 6*self.dim**0.5)))\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.orthogonal_factor = orthogonal_factor\n        self.exploitation_phase = True\n\n    def __call__(self, func):\n        self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.C_exploit = np.eye(self.dim)\n        self.C_explore = np.eye(self.dim)\n        self.pc_exploit = np.zeros(self.dim)\n        self.ps_exploit = np.zeros(self.dim)\n        self.pc_explore = np.zeros(self.dim)\n        self.ps_explore = np.zeros(self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1 / (4 * self.dim) + 1 / (21 * self.dim**2))\n        \n        used_budget = 0\n        best_f_history = []\n\n        while used_budget < self.budget:\n            if self.exploitation_phase:\n                C = self.C_exploit\n                pc = self.pc_exploit\n                ps = self.ps_exploit\n            else:\n                C = self.C_explore\n                pc = self.pc_explore\n                ps = self.ps_explore\n            \n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n\n            # Orthogonal sampling using a random rotation matrix\n            if self.orthogonal_factor > 0:\n                Q, _ = np.linalg.qr(np.random.randn(self.dim, self.dim))\n                z = self.orthogonal_factor * Q @ z + (1 - self.orthogonal_factor) * z\n\n            C_sqrt = np.linalg.cholesky(C)\n            x = self.m[:, np.newaxis] + self.sigma * C_sqrt @ z\n\n            # Repair mechanism\n            x = np.clip(x, func.bounds.lb[:, np.newaxis], func.bounds.ub[:, np.newaxis])\n\n            f = np.array([func(x[:, i]) if used_budget < self.budget else np.inf for i in range(self.popsize)])\n            used_budget += self.popsize\n\n            arindex = np.argsort(f)\n            xmu = x[:, arindex[:self.mu]]\n            zmu = z[:, arindex[:self.mu]]\n\n            m_old = self.m.copy()\n            self.m = np.sum(xmu * self.weights[np.newaxis, :], axis=1)\n\n            # Update exploitation covariance\n            if self.exploitation_phase:\n                ps_new = (1 - self.cs) * ps + np.sqrt(self.cs * (2 - self.cs)) * (C_sqrt @ np.sum(zmu * self.weights[np.newaxis, :], axis=1))\n                self.ps_exploit = ps_new\n                pc_new = (1 - self.c_cov) * pc + np.sqrt(self.c_cov * (2 - self.c_cov)) * (self.m - m_old) / self.sigma\n                self.pc_exploit = pc_new\n                self.C_exploit = (1 - self.c_cov - self.c_mu) * C + self.c_cov * (pc[:, np.newaxis] @ pc[np.newaxis, :])\n                for i in range(self.mu):\n                    y_diff = (xmu[:, i] - m_old) / self.sigma\n                    self.C_exploit += self.c_mu * self.weights[i] * (y_diff[:, np.newaxis] @ y_diff[np.newaxis, :])\n            \n            # Update exploration covariance\n            else:\n                ps_new = (1 - self.cs) * ps + np.sqrt(self.cs * (2 - self.cs)) * (C_sqrt @ np.sum(zmu * self.weights[np.newaxis, :], axis=1))\n                self.ps_explore = ps_new\n                pc_new = (1 - self.c_cov) * pc + np.sqrt(self.c_cov * (2 - self.c_cov)) * (self.m - m_old) / self.sigma\n                self.pc_explore = pc_new\n                self.C_explore = (1 - self.c_cov - self.c_mu) * C + self.c_cov * (pc[:, np.newaxis] @ pc[np.newaxis, :])\n                for i in range(self.mu):\n                    y_diff = (xmu[:, i] - m_old) / self.sigma\n                    self.C_explore += self.c_mu * self.weights[i] * (y_diff[:, np.newaxis] @ y_diff[np.newaxis, :])\n\n            self.sigma *= np.exp((self.cs / self.damp) * (np.linalg.norm(ps) / self.chiN - 1))\n\n            # Stagnation detection\n            if f[arindex[0]] < self.f_opt:\n                self.f_opt = f[arindex[0]]\n                self.x_opt = x[:, arindex[0]].copy()\n                self.stagnation_counter = 0\n            else:\n                self.stagnation_counter += 1\n\n            # Switch between exploitation and exploration phases\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.exploitation_phase = not self.exploitation_phase\n                self.stagnation_counter = 0\n                self.sigma *= 2  # Increase sigma for exploration\n                \n                # Resetting pc and ps can help exploration\n                if self.exploitation_phase:\n                    self.pc_exploit = np.zeros(self.dim)\n                    self.ps_exploit = np.zeros(self.dim)\n                else:\n                    self.pc_explore = np.zeros(self.dim)\n                    self.ps_explore = np.zeros(self.dim)\n            \n            # Ensure C remains positive definite\n            try:\n                if self.exploitation_phase:\n                    np.linalg.cholesky(self.C_exploit)\n                else:\n                    np.linalg.cholesky(self.C_explore)\n            except np.linalg.LinAlgError:\n                if self.exploitation_phase:\n                     self.C_exploit = np.eye(self.dim)\n                else:\n                    self.C_explore = np.eye(self.dim)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm CMAES_DualCovariance scored 0.210 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["44621c95-b96b-419e-bdae-7efbb4c9c56b"], "operator": null, "metadata": {"aucs": [0.006755961425949808, 0.19871472037784677, 0.22465227569152746, 0.15605666254703776, 0.06923826012656464, 0.13175324952880474, 0.1791339342946212, 0.282567172741361, 0.13320953292332394, 0.08813666385087748, 0.1240522358143129, 0.18510287879456067, 0.34124875852869585, 0.15658859662740343, 0.6224269702841995, 0.2939420763598579, 0.28361839116425, 0.43487043248857316, 0.136316421173713, 0.15269872145714947]}}
{"id": "95c5cb1f-84d1-4750-bec2-f07f24d20671", "fitness": 0.4992036246199996, "name": "AdaptiveDifferentialEvolution", "description": "An adaptive Differential Evolution algorithm that adjusts mutation, crossover, and population size using a weighted Lehmer mean for parameter adaptation and incorporates a more sophisticated stagnation detection and population size adjustment strategy based on fitness diversity and success rate.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, popsize_init=None, F_init=0.5, CR_init=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.popsize_min = 10\n        self.popsize_max = 100\n        self.popsize = popsize_init if popsize_init is not None else 2 * self.dim\n        self.F = F_init\n        self.CR = CR_init\n        self.archive_factor = 2.6  # Archive size relative to population size\n        self.archive = []\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.success_F = []\n        self.success_CR = []\n        self.success_count = 0\n        self.min_successes = 5\n        self.adapt_frequency = 10 # How often to adapt parameters\n        self.pop_adapt_frequency = 50 # How often to adapt popsize\n        self.stagnation_threshold = 1e-8\n        self.success_rate_threshold = 0.2\n\n    def __call__(self, func):\n        bounds_lb = func.bounds.lb\n        bounds_ub = func.bounds.ub\n        self.population = np.random.uniform(bounds_lb, bounds_ub, size=(self.popsize, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        eval_count = self.popsize\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = self.population[np.argmin(fitness)].copy()\n        best_fitness_history = [self.f_opt]\n\n        generation = 0\n        while eval_count < self.budget:\n            for i in range(self.popsize):\n                # Mutation\n                indices = np.random.choice(self.popsize, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n\n                mutant = x_r1 + self.F * (x_r2 - x_r3)\n                mutant = np.clip(mutant, bounds_lb, bounds_ub)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.CR\n                trial_vector = np.where(crossover_mask, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial_vector)\n                eval_count += 1\n\n                if f_trial < fitness[i]:\n                    self.success_F.append(self.F)\n                    self.success_CR.append(self.CR)\n                    self.success_count += 1\n                    self.archive.append(self.population[i].copy())\n                    if len(self.archive) > self.archive_factor * self.popsize:\n                        self.archive.pop(0)\n\n                    fitness[i] = f_trial\n                    self.population[i] = trial_vector.copy()\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial_vector.copy()\n\n                if eval_count >= self.budget:\n                    break\n\n            generation += 1\n            best_fitness_history.append(self.f_opt)\n\n            # Adapt parameters\n            if generation % self.adapt_frequency == 0:\n                if self.success_count > self.min_successes:\n                    # Adapt F and CR using weighted Lehmer mean\n                    weights = np.arange(1, len(self.success_F) + 1)  # Assign higher weights to more recent successes\n                    weights = weights / np.sum(weights) # Normalize weights\n\n                    if self.success_F:\n                        self.F = np.sum(weights * (np.array(self.success_F)**2)) / np.sum(weights * np.array(self.success_F))\n                        self.F = np.clip(self.F, 0.1, 1.0)\n                    if self.success_CR:\n                        self.CR = np.sum(weights * np.array(self.success_CR)) / np.sum(weights)\n                        self.CR = np.clip(self.CR, 0.1, 0.9)\n                else:\n                    self.F = 0.5 # Default F\n                    self.CR = 0.7 # Default CR\n\n                self.success_F = []\n                self.success_CR = []\n                self.success_count = 0\n\n            # Adapt population size\n            if generation % self.pop_adapt_frequency == 0:\n                # Stagnation detection based on fitness history\n                if len(best_fitness_history) > 10:\n                    fitness_improvement = np.mean(np.abs(np.diff(best_fitness_history[-10:])))\n                    stagnated = fitness_improvement < self.stagnation_threshold\n                else:\n                    stagnated = False\n\n                success_rate = self.success_count / (self.popsize * self.pop_adapt_frequency) if generation > 0 else 0\n\n                if stagnated and success_rate < self.success_rate_threshold:\n                    self.popsize = max(self.popsize_min, int(self.popsize * 0.75)) # Reduce popsize more aggressively\n                elif not stagnated and success_rate > 0.3:\n                    self.popsize = min(self.popsize_max, int(self.popsize * 1.2)) # Increase popsize more aggressively\n                \n                # Resize population (crude approach)\n                if self.popsize != self.population.shape[0]:\n                    new_population = np.random.uniform(bounds_lb, bounds_ub, size=(self.popsize, self.dim))\n                    if self.popsize > self.population.shape[0]:\n                        new_population[:self.population.shape[0]] = self.population\n                    else:\n                        new_population = self.population[:self.popsize]\n\n                    self.population = new_population\n                    fitness = np.array([func(x) for x in self.population])\n                    eval_count += abs(self.popsize - self.population.shape[0])\n                    if eval_count >= self.budget:\n                        break # Ensure we don't exceed budget\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.499 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c5d79ed0-bf12-4a27-9004-bd6cf37fcb59"], "operator": null, "metadata": {"aucs": [0.23181798980868806, 0.22035641208070578, 0.5090991018703743, 0.45356771571732024, 0.649760838072665, 0.3490327225796718, 0.5623284083644452, 0.4673502157352458, 0.5693950128450689, 0.24018551304878766, 0.39862651175189234, 0.9990163676369329, 0.28243729456755484, 0.9209018032255335, 0.7026272932614657, 0.7083514990056226, 0.5806645978148808, 0.3702282658335011, 0.2643139537705993, 0.5040109754090348]}}
{"id": "6f7c215a-9f86-428c-88c6-241a811eb437", "fitness": -Infinity, "name": "PSO_NelderMead", "description": "A hybrid algorithm combining Particle Swarm Optimization with a Nelder-Mead simplex search for local refinement.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass PSO_NelderMead:\n    def __init__(self, budget=10000, dim=10, pop_size=20, w_init=0.9, w_final=0.4, c1=2.0, c2=2.0, nm_iters=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w_init = w_init\n        self.w_final = w_final\n        self.c1 = c1\n        self.c2 = c2\n        self.nm_iters = nm_iters # Number of Nelder-Mead iterations\n\n        self.population = None\n        self.fitness = None\n        self.velocities = None\n        self.best_positions = None\n        self.global_best_fitness = np.inf\n        self.global_best_position = None\n        self.eval_count = 0\n        self.best_fitness_history = []\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        self.velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))\n        self.best_positions = self.population.copy()\n        self.global_best_position = self.population[np.argmin(self.fitness)].copy()\n        self.global_best_fitness = np.min(self.fitness)\n        self.best_fitness_history.append(self.global_best_fitness)\n\n    def pso_step(self, func, current_eval):\n        w = self.w_init - (self.w_init - self.w_final) * (current_eval / self.budget)\n        self.velocities = (w * self.velocities +\n                           self.c1 * np.random.rand(self.pop_size, self.dim) * (self.best_positions - self.population) +\n                           self.c2 * np.random.rand(self.pop_size, self.dim) * (self.global_best_position - self.population))\n        new_population = self.population + self.velocities\n        new_population = np.clip(new_population, func.bounds.lb, func.bounds.ub)\n        return new_population\n\n    def nelder_mead_step(self, func, x):\n        bounds = [(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)]\n        result = minimize(func, x, method='Nelder-Mead', options={'maxiter': self.nm_iters})\n        self.eval_count += result.nfev  # Account for function evaluations\n        return result.x, result.fun\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            current_eval = self.eval_count\n            new_population = self.pso_step(func, current_eval)\n            new_fitness = np.array([func(x) for x in new_population])\n            self.eval_count += self.pop_size\n\n            for i in range(self.pop_size):\n                # Apply Nelder-Mead to each particle\n                x_refined, f_refined = self.nelder_mead_step(func, new_population[i].copy())\n\n                if f_refined < self.fitness[i]:\n                    self.population[i] = x_refined\n                    self.fitness[i] = f_refined\n                    if f_refined < self.global_best_fitness:\n                        self.global_best_fitness = f_refined\n                        self.global_best_position = x_refined.copy()\n\n                # Update personal best\n                if self.fitness[i] > f_refined:\n                    self.best_positions[i] = self.population[i].copy()\n                    self.fitness[i] = f_refined\n\n                #Global Best is updated after NM\n                if self.fitness[i] < self.global_best_fitness:\n                    self.global_best_fitness = self.fitness[i]\n                    self.global_best_position = self.population[i].copy()\n\n            self.best_fitness_history.append(self.global_best_fitness)\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 3, "feedback": "An exception occurred: name 'minimize' is not defined.", "error": "", "parent_ids": ["c2ef2713-c000-498f-9537-ffa91e6f4549"], "operator": null, "metadata": {}}
{"id": "5ddd3dd6-2aef-436e-bf3e-75de30ecef00", "fitness": 0.0, "name": "AdaptivePSO", "description": "An adaptive PSO variant with a novel velocity clamping mechanism and a mutation operator triggered by stagnation detection.", "code": "import numpy as np\n\nclass AdaptivePSO:\n    def __init__(self, budget=10000, dim=10, pop_size=20, w_init=0.9, w_final=0.4, c1=2.0, c2=2.0, stagnation_threshold=1e-6, stagnation_window=50, mutation_rate=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w_init = w_init\n        self.w_final = w_final\n        self.c1 = c1\n        self.c2 = c2\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_window = stagnation_window\n        self.mutation_rate = mutation_rate\n        self.population = None\n        self.fitness = None\n        self.velocities = None\n        self.best_positions = None\n        self.global_best_fitness = np.inf\n        self.global_best_position = None\n        self.eval_count = 0\n        self.fitness_history = []\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        self.velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))\n        self.best_positions = self.population.copy()\n        self.global_best_position = self.population[np.argmin(self.fitness)].copy()\n        self.global_best_fitness = np.min(self.fitness)\n        self.fitness_history.append(self.global_best_fitness)\n\n    def pso_step(self, func, current_eval):\n        w = self.w_init - (self.w_init - self.w_final) * (current_eval / self.budget)\n\n        r1 = np.random.rand(self.pop_size, self.dim)\n        r2 = np.random.rand(self.pop_size, self.dim)\n\n        cognitive_component = self.c1 * r1 * (self.best_positions - self.population)\n        social_component = self.c2 * r2 * (self.global_best_position - self.population)\n\n        self.velocities = w * self.velocities + cognitive_component + social_component\n\n        # Velocity clamping: Limit the velocity to a reasonable range\n        v_max = 0.1 * (func.bounds.ub - func.bounds.lb)\n        self.velocities = np.clip(self.velocities, -v_max, v_max)\n\n        new_population = self.population + self.velocities\n        new_population = np.clip(new_population, func.bounds.lb, func.bounds.ub)\n        return new_population\n\n    def stagnation_detection(self):\n        if len(self.fitness_history) > self.stagnation_window:\n            std = np.std(self.fitness_history[-self.stagnation_window:])\n            if std < self.stagnation_threshold:\n                return True\n        return False\n\n    def mutate_population(self, func):\n        for i in range(self.pop_size):\n            if np.random.rand() < self.mutation_rate:\n                # Mutate the particle's position\n                mutation = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                self.population[i] = mutation.copy()\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            current_eval = self.eval_count\n            new_population = self.pso_step(func, current_eval)\n            new_fitness = np.array([func(x) for x in new_population])\n            self.eval_count += self.pop_size\n\n            for i in range(self.pop_size):\n                if new_fitness[i] < self.fitness[i]:\n                    self.population[i] = new_population[i].copy()\n                    self.fitness[i] = new_fitness[i]\n\n                    if new_fitness[i] < self.global_best_fitness:\n                        self.global_best_fitness = new_fitness[i]\n                        self.global_best_position = new_population[i].copy()\n                        \n                        \n                if self.fitness[i] < self.fitness[i]: #original fitness is better than best fitness, update positions\n                    self.best_positions[i] = self.population[i].copy()\n                    self.fitness[i] = new_fitness[i]\n                    \n            if self.stagnation_detection():\n                self.mutate_population(func)\n                self.fitness = np.array([func(x) for x in self.population])\n                self.eval_count += self.pop_size // 2\n                \n            self.fitness_history.append(self.global_best_fitness)\n\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptivePSO scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c2ef2713-c000-498f-9537-ffa91e6f4549"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "831cd5f9-b84e-4ef4-babd-b19bf044a92a", "fitness": -Infinity, "name": "CMAES_DynamicPop", "description": "A CMA-ES variant that dynamically adjusts its population size based on the success rate of generating better solutions and uses a history of the mean to encourage exploration.", "code": "import numpy as np\n\nclass CMAES_DynamicPop:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damp=None, c_cov=None, c_mu=None, success_threshold=0.2, popsize_factor=1.1, stagnation_threshold=50):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.m = None\n        self.sigma = 0.3\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.chiN = None\n        self.cs = cs\n        self.damp = damp if damp is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1]) + self.cs\n        self.c_cov = c_cov if c_cov is not None else 2 / ((self.dim + np.sqrt(2))**2)\n        self.c_mu = c_mu if c_mu is not None else min(1 - self.c_cov, 2 * (self.weights[0] - 1 + self.weights[0]**2) / ((self.mu + 6 + 6*self.dim**0.5)))\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.success_threshold = success_threshold\n        self.popsize_factor = popsize_factor\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.mean_history = []\n        self.mean_history_length = 10\n\n    def __call__(self, func):\n        self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1 / (4 * self.dim) + 1 / (21 * self.dim**2))\n        \n        used_budget = 0\n        success_count = 0\n        generation_count = 0\n\n        while used_budget < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.m[:, np.newaxis] + self.sigma * C_sqrt @ z\n            x = np.clip(x, func.bounds.lb[:, np.newaxis], func.bounds.ub[:, np.newaxis])\n\n            f = np.array([func(x[:, i]) if used_budget < self.budget else np.inf for i in range(self.popsize)])\n            used_budget += self.popsize\n            generation_count += 1\n            \n            arindex = np.argsort(f)\n            xmu = x[:, arindex[:self.mu]]\n            zmu = z[:, arindex[:self.mu]]\n\n            m_old = self.m.copy()\n            self.m = np.sum(xmu * self.weights[np.newaxis, :], axis=1)\n\n            ps_new = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs)) * (C_sqrt @ np.sum(zmu * self.weights[np.newaxis, :], axis=1))\n            self.ps = ps_new\n            self.pc = (1 - self.c_cov) * self.pc + np.sqrt(self.c_cov * (2 - self.c_cov)) * (self.m - m_old) / self.sigma\n            self.C = (1 - self.c_cov - self.c_mu) * self.C + self.c_cov * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :])\n            for i in range(self.mu):\n                y_diff = (xmu[:, i] - m_old) / self.sigma\n                self.C += self.c_mu * self.weights[i] * (y_diff[:, np.newaxis] @ y_diff[np.newaxis, :])\n\n            self.sigma *= np.exp((self.cs / self.damp) * (np.linalg.norm(self.ps) / self.chiN - 1))\n\n            if f[arindex[0]] < self.f_opt:\n                self.f_opt = f[arindex[0]]\n                self.x_opt = x[:, arindex[0]].copy()\n                success_count += 1\n                self.stagnation_counter = 0\n            else:\n                self.stagnation_counter += 1\n\n            # Population size adaptation based on success rate\n            success_rate = success_count / generation_count\n            if success_rate > self.success_threshold:\n                self.popsize = min(self.popsize * self.popsize_factor, 2 * self.dim)\n                self.mu = self.popsize // 2\n                self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n                self.weights /= np.sum(self.weights)\n                print(f\"Increased popsize to {self.popsize}\") # For debugging\n\n            # Stagnation detection and mean history exploration\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.stagnation_counter = 0\n                # Introduce exploration by perturbing the mean based on its history\n                if len(self.mean_history) > 0:\n                    mean_diff = self.m - np.mean(np.array(self.mean_history), axis=0)\n                    self.m += 0.1 * mean_diff  # Small perturbation based on mean history\n                    print(\"Stagnation: Perturbing mean\") # For debugging\n                self.sigma *= 1.5 # Increase step size\n\n            self.mean_history.append(self.m.copy())\n            if len(self.mean_history) > self.mean_history_length:\n                self.mean_history.pop(0)\n\n\n            # Ensure C remains positive definite\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)\n                print(\"Covariance matrix reset\") # For debugging\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "An exception occurred: 'float' object cannot be interpreted as an integer.", "error": "", "parent_ids": ["91d960d4-19a7-4830-9d58-6d9e633f033c"], "operator": null, "metadata": {}}
{"id": "ea5ae5bc-ee97-421e-81b8-6148461f14f8", "fitness": -Infinity, "name": "CMAES_LocalSearch", "description": "A CMA-ES variant with a local search component triggered by stagnation, using Nelder-Mead or BFGS to refine promising solutions.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass CMAES_LocalSearch:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damp=None, c_cov=None, c_mu=None, stagnation_threshold=50, local_search_frequency=100, local_search_type='Nelder-Mead'):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.m = None\n        self.sigma = 0.3\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.chiN = None\n\n        self.cs = cs\n        self.damp = damp if damp is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1]) + self.cs\n        self.c_cov = c_cov if c_cov is not None else 2 / ((self.dim + np.sqrt(2))**2)\n        self.c_mu = c_mu if c_mu is not None else min(1 - self.c_cov, 2 * (self.weights[0] - 1 + self.weights[0]**2) / ((self.mu + 6 + 6*self.dim**0.5)))\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.local_search_frequency = local_search_frequency\n        self.local_search_type = local_search_type\n\n    def __call__(self, func):\n        self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1 / (4 * self.dim) + 1 / (21 * self.dim**2))\n        \n        used_budget = 0\n\n        while used_budget < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.m[:, np.newaxis] + self.sigma * C_sqrt @ z\n\n            # Repair mechanism\n            x = np.clip(x, func.bounds.lb[:, np.newaxis], func.bounds.ub[:, np.newaxis])\n\n            f = np.array([func(x[:, i]) if used_budget < self.budget else np.inf for i in range(self.popsize)])\n            used_budget += self.popsize\n\n            arindex = np.argsort(f)\n            xmu = x[:, arindex[:self.mu]]\n            zmu = z[:, arindex[:self.mu]]\n\n            m_old = self.m.copy()\n            self.m = np.sum(xmu * self.weights[np.newaxis, :], axis=1)\n\n            ps_new = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs)) * (C_sqrt @ np.sum(zmu * self.weights[np.newaxis, :], axis=1))\n            self.ps = ps_new\n            pc_new = (1 - self.c_cov) * self.pc + np.sqrt(self.c_cov * (2 - self.c_cov)) * (self.m - m_old) / self.sigma\n            self.pc = pc_new\n            self.C = (1 - self.c_cov - self.c_mu) * self.C + self.c_cov * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :])\n            for i in range(self.mu):\n                y_diff = (xmu[:, i] - m_old) / self.sigma\n                self.C += self.c_mu * self.weights[i] * (y_diff[:, np.newaxis] @ y_diff[np.newaxis, :])\n\n            self.sigma *= np.exp((self.cs / self.damp) * (np.linalg.norm(self.ps) / self.chiN - 1))\n\n            # Stagnation detection\n            if f[arindex[0]] < self.f_opt:\n                self.f_opt = f[arindex[0]]\n                self.x_opt = x[:, arindex[0]].copy()\n                self.stagnation_counter = 0\n            else:\n                self.stagnation_counter += 1\n\n            # Local Search triggered by stagnation\n            if self.stagnation_counter > self.stagnation_threshold and used_budget < self.budget and (used_budget // self.popsize) % self.local_search_frequency == 0:\n                \n                def local_objective(x):\n                    if func.bounds.lb[0] <= x[0] <= func.bounds.ub[0] and func.bounds.lb[1] <= x[1] <= func.bounds.ub[1] and used_budget < self.budget:\n                        return func(x)\n                    else:\n                        return np.inf\n\n                if self.local_search_type == 'Nelder-Mead':\n                    res = minimize(local_objective, self.x_opt, method='Nelder-Mead', bounds=func.bounds)\n                elif self.local_search_type == 'BFGS':\n                    res = minimize(local_objective, self.x_opt, method='BFGS', bounds=func.bounds)\n                else:\n                    res = minimize(local_objective, self.x_opt, method='Nelder-Mead', bounds=func.bounds) # Default\n                \n                if res.fun < self.f_opt and used_budget < self.budget:\n                    self.f_opt = res.fun\n                    self.x_opt = res.x.copy()\n                \n                self.stagnation_counter = 0 # Reset stagnation after local search\n\n            # Ensure C remains positive definite\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "An exception occurred: name 'minimize' is not defined.", "error": "", "parent_ids": ["91d960d4-19a7-4830-9d58-6d9e633f033c"], "operator": null, "metadata": {}}
{"id": "68c886db-180a-46da-8b94-e80f239271e7", "fitness": -Infinity, "name": "SelfOrganizingScoutsDE", "description": "A differential evolution variant with self-organizing scouts that explore the search space using a separate population and dynamically adjusted mutation strategies based on scout performance.", "code": "import numpy as np\n\nclass SelfOrganizingScoutsDE:\n    def __init__(self, budget=10000, dim=10, popsize=None, scout_popsize=None, F_init=0.5, CR_init=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 2 * self.dim\n        self.scout_popsize = scout_popsize if scout_popsize is not None else self.dim\n        self.F = F_init\n        self.CR = CR_init\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.scout_success_rate = 0.1\n        self.scout_exploration_rate = 0.1\n        self.scout_mutation_factor = 1.0\n        self.main_mutation_factor = 0.5\n        self.adapt_frequency = 20\n\n    def __call__(self, func):\n        bounds_lb = func.bounds.lb\n        bounds_ub = func.bounds.ub\n\n        # Initialize main population\n        self.population = np.random.uniform(bounds_lb, bounds_ub, size=(self.popsize, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        eval_count = self.popsize\n\n        # Initialize scout population\n        self.scout_population = np.random.uniform(bounds_lb, bounds_ub, size=(self.scout_popsize, self.dim))\n        scout_fitness = np.array([func(x) for x in self.scout_population])\n        eval_count += self.scout_popsize\n\n        self.f_opt = np.min(np.concatenate((fitness, scout_fitness)))\n        self.x_opt = self.population[np.argmin(fitness)].copy() if np.min(fitness) < np.min(scout_fitness) else self.scout_population[np.argmin(scout_fitness)].copy()\n        best_fitness_history = [self.f_opt]\n\n        generation = 0\n        while eval_count < self.budget:\n            # Main population DE\n            for i in range(self.popsize):\n                # Mutation\n                indices = np.random.choice(self.popsize, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n\n                mutant = x_r1 + self.main_mutation_factor * (x_r2 - x_r3)\n                mutant = np.clip(mutant, bounds_lb, bounds_ub)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.CR\n                trial_vector = np.where(crossover_mask, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial_vector)\n                eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    self.population[i] = trial_vector.copy()\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial_vector.copy()\n\n                if eval_count >= self.budget:\n                    break\n\n            # Scout population DE\n            for i in range(self.scout_popsize):\n                # Mutation - enhanced exploration\n                indices = np.random.choice(self.scout_popsize, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.scout_population[indices]\n\n                mutant = x_r1 + self.scout_mutation_factor * (x_r2 - x_r3) + self.scout_exploration_rate * (np.random.uniform(bounds_lb, bounds_ub, self.dim) - self.scout_population[i])\n                mutant = np.clip(mutant, bounds_lb, bounds_ub)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.CR\n                trial_vector = np.where(crossover_mask, mutant, self.scout_population[i])\n\n                # Selection\n                f_trial = func(trial_vector)\n                eval_count += 1\n\n                if f_trial < scout_fitness[i]:\n                    scout_fitness[i] = f_trial\n                    self.scout_population[i] = trial_vector.copy()\n\n                    # Replace a random member of the main population if scout is better than current best\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial_vector.copy()\n                        replace_idx = np.random.randint(0, self.popsize)\n                        self.population[replace_idx] = trial_vector.copy()\n                        fitness[replace_idx] = f_trial\n\n\n                if eval_count >= self.budget:\n                    break\n\n            generation += 1\n            best_fitness_history.append(self.f_opt)\n\n            # Adapt scout behavior\n            if generation % self.adapt_frequency == 0:\n                improvement_rate = np.sum(scout_fitness < fitness.min()) / self.scout_popsize\n                \n                if improvement_rate > self.scout_success_rate:\n                    self.scout_mutation_factor *= 1.05\n                    self.scout_exploration_rate *= 0.95\n                else:\n                    self.scout_mutation_factor *= 0.95\n                    self.scout_exploration_rate *= 1.05\n\n                self.scout_mutation_factor = np.clip(self.scout_mutation_factor, 0.1, 2.0)\n                self.scout_exploration_rate = np.clip(self.scout_exploration_rate, 0.01, 0.5)\n\n                # Migrate best scout to main population\n                best_scout_idx = np.argmin(scout_fitness)\n                worst_pop_idx = np.argmax(fitness)\n                if scout_fitness[best_scout_idx] < fitness[worst_pop_idx]:\n                    self.population[worst_pop_idx] = self.scout_population[best_scout_idx].copy()\n                    fitness[worst_pop_idx] = scout_fitness[best_scout_idx]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "An exception occurred: Cannot take a larger sample than population when 'replace=False'.", "error": "", "parent_ids": ["95c5cb1f-84d1-4750-bec2-f07f24d20671"], "operator": null, "metadata": {}}
{"id": "2c232f8e-57fa-46f8-bace-edfb4e259df6", "fitness": -Infinity, "name": "CMAES_AdaptiveRestart", "description": "CMA-ES with adaptive restarts, step size control based on fitness and covariance matrix adaptation that considers both rank-one and rank-mu updates, incorporates mirrored sampling with adaptive factor and dynamic c_cov adjustment based on condition number and stagnation detection with a decay factor.", "code": "import numpy as np\n\nclass CMAES_AdaptiveRestart:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damp=None, c_cov=None, c_mu=None, mirror_factor=1.0, stagnation_threshold=1e-9, stagnation_decay=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.m = None\n        self.sigma = 0.3  # exploration parameter\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.chiN = None\n\n        self.cs = cs\n        self.damp = damp if damp is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1]) + self.cs\n        self.c_cov = c_cov if c_cov is not None else 2 / ((self.dim + np.sqrt(2))**2)\n        self.c_mu = c_mu if c_mu is not None else min(1 - self.c_cov, 2 * (self.weights[0] - 1 + self.weights[0]**2) / ((self.mu + 6 + 6*self.dim**0.5)))\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.mirror_factor = mirror_factor\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_decay = stagnation_decay\n        self.last_improvement = 0\n        self.max_no_improvement = 100 # generations before restart\n        self.condition_number_threshold = 1e10\n\n    def __call__(self, func):\n        self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1 / (4 * self.dim) + 1 / (21 * self.dim**2))\n        \n        used_budget = 0\n        generation = 0\n\n        while used_budget < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.m[:, np.newaxis] + self.sigma * C_sqrt @ z\n            \n            # Mirroring to handle bounds - more aggressive mirroring\n            xmir = self.m[:, np.newaxis] - self.mirror_factor * self.sigma * C_sqrt @ z\n            \n            # Repair mechanism to ensure solutions are within bounds and resample if necessary\n            for i in range(self.popsize):\n                if not func.bounds.lb[0] <= x[0, i] <= func.bounds.ub[0]:\n                    x[:, i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                if not func.bounds.lb[0] <= xmir[0, i] <= func.bounds.ub[0]:\n                    xmir[:, i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                    \n            f = np.array([func(x[:, i]) if used_budget < self.budget else np.inf for i in range(self.popsize)])\n            fmir = np.array([func(xmir[:, i]) if used_budget + self.popsize < self.budget else np.inf for i in range(self.popsize)])\n            \n            used_budget += 2 * self.popsize\n            \n            if np.min(f) < np.min(fmir):\n                y = x\n                fitness = f\n                best_index = np.argmin(f)\n                if f[best_index] < self.f_opt:\n                    self.f_opt = f[best_index]\n                    self.x_opt = x[:, best_index].copy()\n                    self.last_improvement = generation\n            else:\n                y = xmir\n                fitness = fmir\n                best_index = np.argmin(fmir)\n                if fmir[best_index] < self.f_opt:\n                    self.f_opt = fmir[best_index]\n                    self.x_opt = xmir[:, best_index].copy()\n                    self.last_improvement = generation\n                    \n            \n            arindex = np.argsort(fitness)\n            xmu = y[:, arindex[:self.mu]]\n            zmu = z[:, arindex[:self.mu]]\n            \n            m_old = self.m.copy()\n            self.m = np.sum(xmu * self.weights[np.newaxis, :], axis=1)\n            \n            ps_new = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs)) * (C_sqrt @ np.sum(zmu * self.weights[np.newaxis, :], axis=1))\n            self.ps = ps_new\n            \n            pc_new = (1 - self.c_cov) * self.pc + np.sqrt(self.c_cov * (2 - self.c_cov)) * (self.m - m_old) / self.sigma\n            self.pc = pc_new\n            \n            norm_ps = np.linalg.norm(self.ps)\n            factor = self.c_cov * (1 / (np.min([1, (1000000 * norm_ps**4 / self.dim)])))\n            \n            # Rank-one update of C\n            self.C = (1 - self.c_cov - self.c_mu + factor) * self.C + self.c_cov * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :])\n            \n            # Rank-mu update of C\n            for i in range(self.mu):\n                y_diff = (xmu[:, i] - m_old) / self.sigma\n                self.C += self.c_mu * self.weights[i] * (y_diff[:, np.newaxis] @ y_diff[np.newaxis, :])\n            \n            # Adaptive c_cov based on condition number\n            condition_number = np.linalg.cond(self.C)\n            \n            # Adaptive sigma based on population diversity\n            pop_diversity = np.std(fitness)\n            if pop_diversity < self.stagnation_threshold:\n                self.sigma *= 0.9  # Reduce step size if population is too uniform\n                self.stagnation_threshold *= self.stagnation_decay # Decay stagnation threshold\n            else:\n                self.sigma *= 1.1\n                self.stagnation_threshold = stagnation_threshold # reset stagnation threshold\n\n            if condition_number > self.condition_number_threshold:  # Threshold for ill-conditioning\n                adaptive_c_cov = self.c_cov * 0.1  # Reduce learning rate for exploration\n            else:\n                adaptive_c_cov = self.c_cov # Maintain original learning rate\n            \n            self.c_cov = adaptive_c_cov\n\n            self.sigma *= np.exp((self.cs / self.damp) * (norm_ps / self.chiN - 1))\n            \n            # Ensure C remains positive definite\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)  # Reset covariance matrix if it becomes non-positive definite\n\n            # Restart mechanism\n            if generation - self.last_improvement > self.max_no_improvement:\n                self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.sigma = 0.3  # Reset sigma to initial value\n                self.last_improvement = generation # reset last improvement\n                self.stagnation_threshold = stagnation_threshold # Reset stagnation threshold\n                \n            generation += 1\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "An exception occurred: name 'stagnation_threshold' is not defined.", "error": "", "parent_ids": ["d3e25166-dcfc-49dd-9cab-47920fdbee21"], "operator": null, "metadata": {}}
{"id": "f1dd8a2a-4445-4169-bd5d-3087f4bd0b4d", "fitness": 0.0, "name": "CMAES_AdaptiveRestartEnhanced", "description": "Adaptively adjusts CMA-ES parameters and incorporates a dynamic population size strategy based on success rate and fitness diversity, while also employing a more robust bound constraint handling and restart mechanism.", "code": "import numpy as np\n\nclass CMAES_AdaptiveRestartEnhanced:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damp=None, c_cov=None, c_mu=None, mirror_factor=1.0, stagnation_threshold=1e-9, success_threshold=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.m = None\n        self.sigma = 0.3  # exploration parameter\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.chiN = None\n\n        self.cs = cs\n        self.damp = damp if damp is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1]) + self.cs\n        self.c_cov = c_cov if c_cov is not None else 2 / ((self.dim + np.sqrt(2))**2)\n        self.c_mu = c_mu if c_mu is not None else min(1 - self.c_cov, 2 * (self.weights[0] - 1 + self.weights[0]**2) / ((self.mu + 6 + 6*self.dim**0.5)))\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.mirror_factor = mirror_factor\n        self.stagnation_threshold = stagnation_threshold\n        self.last_improvement = 0\n        self.max_no_improvement = 100 # generations before restart\n        self.success_threshold = success_threshold\n        self.success_rate = 0.0\n        self.population_history = []\n\n    def __call__(self, func):\n        self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1 / (4 * self.dim) + 1 / (21 * self.dim**2))\n        \n        used_budget = 0\n        generation = 0\n\n        while used_budget < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.m[:, np.newaxis] + self.sigma * C_sqrt @ z\n            \n            # Mirroring to handle bounds - more aggressive mirroring\n            xmir = self.m[:, np.newaxis] - self.mirror_factor * self.sigma * C_sqrt @ z\n            \n            # Repair mechanism to ensure solutions are within bounds and resample if necessary\n            for i in range(self.popsize):\n                if not all(func.bounds.lb <= x[:, i]) or not all(x[:, i] <= func.bounds.ub):\n                    x[:, i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                if not all(func.bounds.lb <= xmir[:, i]) or not all(xmir[:, i] <= func.bounds.ub):\n                    xmir[:, i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                    \n            f = np.array([func(x[:, i]) if used_budget < self.budget else np.inf for i in range(self.popsize)])\n            fmir = np.array([func(xmir[:, i]) if used_budget + self.popsize < self.budget else np.inf for i in range(self.popsize)])\n            \n            used_budget += 2 * self.popsize\n            \n            if np.min(f) < np.min(fmir):\n                y = x\n                fitness = f\n                best_index = np.argmin(f)\n                if f[best_index] < self.f_opt:\n                    self.f_opt = f[best_index]\n                    self.x_opt = x[:, best_index].copy()\n                    self.last_improvement = generation\n            else:\n                y = xmir\n                fitness = fmir\n                best_index = np.argmin(fmir)\n                if fmir[best_index] < self.f_opt:\n                    self.f_opt = fmir[best_index]\n                    self.x_opt = xmir[:, best_index].copy()\n                    self.last_improvement = generation\n                    \n            \n            arindex = np.argsort(fitness)\n            xmu = y[:, arindex[:self.mu]]\n            zmu = z[:, arindex[:self.mu]]\n            \n            m_old = self.m.copy()\n            self.m = np.sum(xmu * self.weights[np.newaxis, :], axis=1)\n            \n            ps_new = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs)) * (C_sqrt @ np.sum(zmu * self.weights[np.newaxis, :], axis=1))\n            self.ps = ps_new\n            \n            pc_new = (1 - self.c_cov) * self.pc + np.sqrt(self.c_cov * (2 - self.c_cov)) * (self.m - m_old) / self.sigma\n            self.pc = pc_new\n            \n            norm_ps = np.linalg.norm(self.ps)\n            factor = self.c_cov * (1 / (np.min([1, (1000000 * norm_ps**4 / self.dim)])))\n            \n            # Rank-one update of C\n            self.C = (1 - self.c_cov - self.c_mu) * self.C + self.c_cov * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :])\n            \n            # Rank-mu update of C\n            for i in range(self.mu):\n                y_diff = (xmu[:, i] - m_old) / self.sigma\n                self.C += self.c_mu * self.weights[i] * (y_diff[:, np.newaxis] @ y_diff[np.newaxis, :])\n            \n            # Adaptive c_cov based on condition number\n            condition_number = np.linalg.cond(self.C)\n            \n            # Adaptive sigma based on population diversity\n            pop_diversity = np.std(fitness)\n            if pop_diversity < self.stagnation_threshold:\n                  self.sigma *= 0.9  # Reduce step size if population is too uniform\n            else:\n                self.sigma *= 1.1\n\n            if condition_number > 1e10:  # Threshold for ill-conditioning\n                adaptive_c_cov = self.c_cov * 0.1  # Reduce learning rate for exploration\n            else:\n                adaptive_c_cov = self.c_cov # Maintain original learning rate\n            \n            self.c_cov = adaptive_c_cov\n\n            self.sigma *= np.exp((self.cs / self.damp) * (norm_ps / self.chiN - 1))\n            \n            # Ensure C remains positive definite\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)  # Reset covariance matrix if it becomes non-positive definite\n            \n            # Success rate adaptation\n            if generation > 0:\n                if self.f_opt < min(self.population_history):\n                    self.success_rate = 0.9 * self.success_rate + 0.1\n                else:\n                    self.success_rate = 0.9 * self.success_rate\n            \n            # Dynamic population size adjustment based on success rate\n            if self.success_rate > self.success_threshold:\n                self.popsize = min(self.popsize + 1, 2 * self.dim)  # Increase population size\n                self.mu = self.popsize // 2\n                self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n                self.weights /= np.sum(self.weights)\n                self.c_mu = min(1 - self.c_cov, 2 * (self.weights[0] - 1 + self.weights[0]**2) / ((self.mu + 6 + 6*self.dim**0.5)))\n            elif self.success_rate < 0.1:\n                self.popsize = max(self.popsize - 1, 4 + int(3 * np.log(self.dim)))  # Decrease population size\n                self.mu = self.popsize // 2\n                self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n                self.weights /= np.sum(self.weights)\n                self.c_mu = min(1 - self.c_cov, 2 * (self.weights[0] - 1 + self.weights[0]**2) / ((self.mu + 6 + 6*self.dim**0.5)))\n\n            # Restart mechanism\n            if generation - self.last_improvement > self.max_no_improvement:\n                self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.sigma = 0.3  # Reset sigma to initial value\n                self.last_improvement = generation # reset last improvement\n                self.popsize = 4 + int(3 * np.log(self.dim)) # Reset population size\n                self.mu = self.popsize // 2\n                self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n                self.weights /= np.sum(self.weights)\n                self.c_mu = min(1 - self.c_cov, 2 * (self.weights[0] - 1 + self.weights[0]**2) / ((self.mu + 6 + 6*self.dim**0.5)))\n            \n            self.population_history.append(self.f_opt)\n            if len(self.population_history) > 100:\n                self.population_history.pop(0)\n            \n            generation += 1\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm CMAES_AdaptiveRestartEnhanced scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d3e25166-dcfc-49dd-9cab-47920fdbee21"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "63378242-09c3-4eba-96d0-43e509ed4430", "fitness": -Infinity, "name": "AdaptiveCMAES", "description": "Enhanced adaptive CMA-ES with orthogonal sampling, dynamic population size adjustment, and a self-adaptive Cauchy mutation probability.", "code": "import numpy as np\n\nclass AdaptiveCMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damp=None, c_cov=None, c_mu=None, stagnation_threshold=50, initial_cauchy_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.m = None\n        self.sigma = 0.3\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.chiN = None\n\n        self.cs = cs\n        self.damp = damp if damp is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1]) + self.cs\n        self.c_cov = c_cov if c_cov is not None else 2 / ((self.dim + np.sqrt(2))**2)\n        self.c_mu = c_mu if c_mu is not None else min(1 - self.c_cov, 2 * (self.weights[0] - 1 + self.weights[0]**2) / ((self.mu + 6 + 6*self.dim**0.5)))\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.last_f_opt = np.Inf\n        self.cauchy_mutation_prob = initial_cauchy_prob\n        self.initial_cauchy_prob = initial_cauchy_prob #Store initial value\n        self.diversity_threshold = 1e-8\n        self.success_rate = 0.0\n        self.success_history = []\n\n    def orthogonal_sampling(self, z, C_sqrt, num_samples):\n        Q, _ = np.linalg.qr(np.random.randn(self.dim, num_samples))\n        samples = self.m[:, np.newaxis] + self.sigma * C_sqrt @ Q\n        return samples\n\n    def __call__(self, func):\n        self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1 / (4 * self.dim) + 1 / (21 * self.dim**2))\n        used_budget = 0\n\n        while used_budget < self.budget:\n            # Population size adaptation\n            if len(self.success_history) > 10:\n                success_rate = np.mean(self.success_history[-10:])\n                if success_rate > 0.7 and self.popsize > 10:\n                    self.popsize = max(10, self.popsize // 2)\n                    self.mu = self.popsize // 2\n                    self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n                    self.weights /= np.sum(self.weights)\n                elif success_rate < 0.2 and self.popsize < 100:\n                    self.popsize = min(100, self.popsize * 2)\n                    self.mu = self.popsize // 2\n                    self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n                    self.weights /= np.sum(self.weights)\n            \n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.m[:, np.newaxis] + self.sigma * C_sqrt @ z\n            \n            #Orthogonal sampling\n            num_ortho = self.popsize // 4\n            x_ortho = self.orthogonal_sampling(z, C_sqrt, num_ortho)\n            x = np.concatenate((x, x_ortho), axis=1)\n            \n            # Apply Cauchy mutation with self-adaptive probability\n            for i in range(x.shape[1]):  #Iterate over the extended population (x and x_ortho combined)\n                if np.random.rand() < self.cauchy_mutation_prob:\n                    cauchy_noise = np.random.standard_cauchy(size=self.dim)\n                    x[:, i] += self.sigma * cauchy_noise\n\n            # Mirroring\n            xmir = self.m[:, np.newaxis] - self.sigma * C_sqrt @ z\n            xmir_ortho = self.m[:, np.newaxis] - self.sigma * C_sqrt @ np.random.randn(self.dim, num_ortho)  #Mirror ortho\n            xmir = np.concatenate((xmir, xmir_ortho), axis=1) #Concatenate\n\n            # Repair mechanism\n            for i in range(x.shape[1]):\n                oob = np.logical_or(x[:, i] < func.bounds.lb, x[:, i] > func.bounds.ub)\n                if np.any(oob):\n                    x[oob, i] = np.random.uniform(func.bounds.lb[oob], func.bounds.ub[oob], size=np.sum(oob))\n\n                oob = np.logical_or(xmir[:, i] < func.bounds.lb, xmir[:, i] > func.bounds.ub)\n                if np.any(oob):\n                    xmir[oob, i] = np.random.uniform(func.bounds.lb[oob], func.bounds.ub[oob], size=np.sum(oob))\n                    \n            # Evaluate functions\n            f = np.array([func(x[:, i]) if used_budget < self.budget else np.inf for i in range(x.shape[1])])\n            fmir = np.array([func(xmir[:, i]) if used_budget + x.shape[1] < self.budget else np.inf for i in range(xmir.shape[1])])\n            used_budget += x.shape[1] + xmir.shape[1]\n            \n            # Select best solutions\n            if np.min(f) < np.min(fmir):\n                y = x\n                fitness = f\n                best_index = np.argmin(f)\n                if f[best_index] < self.f_opt:\n                    self.f_opt = f[best_index]\n                    self.x_opt = x[:, best_index].copy()\n                    self.success_history.append(1.0)\n                else:\n                     self.success_history.append(0.0)\n            else:\n                y = xmir\n                fitness = fmir\n                best_index = np.argmin(fmir)\n                if fmir[best_index] < self.f_opt:\n                    self.f_opt = fmir[best_index]\n                    self.x_opt = xmir[:, best_index].copy()\n                    self.success_history.append(1.0)\n                else:\n                    self.success_history.append(0.0)\n            \n            #Stagnation detection\n            if self.f_opt >= self.last_f_opt:\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n            \n            # Cauchy probability adaptation based on stagnation\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.sigma *= 0.8\n                self.stagnation_counter = 0\n                self.cauchy_mutation_prob = min(1.0, self.cauchy_mutation_prob * 1.2) #Increase Cauchy prob\n            else:\n                self.cauchy_mutation_prob = max(0.01, self.cauchy_mutation_prob * 0.95) #Decrease Cauchy prob\n\n            self.last_f_opt = self.f_opt\n                    \n            arindex = np.argsort(fitness)[:self.mu]\n            xmu = y[:, arindex]\n            zmu = z[:, arindex[:self.mu]]\n            \n            m_old = self.m.copy()\n            self.m = np.sum(xmu[:,:self.mu] * self.weights[np.newaxis, :], axis=1)  #Use only popsize first elements\n\n            ps_new = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs)) * (C_sqrt @ np.sum(zmu * self.weights[np.newaxis, :], axis=1))\n            self.ps = ps_new\n            \n            pc_new = (1 - self.c_cov) * self.pc + np.sqrt(self.c_cov * (2 - self.c_cov)) * (self.m - m_old) / self.sigma\n            self.pc = pc_new\n            \n            norm_ps = np.linalg.norm(self.ps)\n            factor = self.c_cov * (1 / (np.min([1, (1000000 * norm_ps**4 / self.dim)])))\n            \n            # Rank-one update of C\n            self.C = (1 - self.c_cov - self.c_mu) * self.C + self.c_cov * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :])\n            \n            # Rank-mu update of C\n            for i in range(self.mu):\n                y_diff = (xmu[:, i] - m_old) / self.sigma\n                self.C += self.c_mu * self.weights[i] * (y_diff[:, np.newaxis] @ y_diff[np.newaxis, :])\n\n            fitness_variance = np.var(fitness)\n            if fitness_variance > 1e-3:\n                self.sigma *= np.exp(0.05 * (fitness_variance - 1e-3))\n            else:\n                self.sigma *= np.exp(-0.05 * (1e-3 - fitness_variance))\n            \n            self.sigma = max(self.sigma, 1e-8)\n\n            self.sigma *= np.exp((self.cs / self.damp) * (norm_ps / self.chiN - 1))\n\n            if np.linalg.norm(self.C - np.diag(np.diag(self.C))) < self.diversity_threshold:\n                self.C = np.eye(self.dim)\n            \n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "An exception occurred: index 6 is out of bounds for axis 1 with size 6.", "error": "", "parent_ids": ["7c5eff6d-6304-4dc5-91ce-8aac1f150ad9"], "operator": null, "metadata": {}}
{"id": "f67b95b6-4a84-45b3-bcef-74f2d906f2a5", "fitness": 0.3693030425529244, "name": "CMAES_SpectralRestart", "description": "An adaptive CMA-ES with a simplified covariance matrix adaptation based on a running average, step-size adaptation, and a multi-restart strategy using spectral properties of sampled solutions to enhance exploration and exploitation.", "code": "import numpy as np\n\nclass CMAES_SpectralRestart:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damp=None, c_mean=0.1, restart_trigger=1e-9):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.m = None\n        self.sigma = 0.3\n        self.C = None  # covariance matrix (not explicitly used in full form for updates)\n        self.ps = None # evolution path for sigma\n        self.chiN = None\n        self.cs = cs\n        self.damp = damp if damp is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1]) + self.cs\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.c_mean = c_mean # learning rate for mean update\n        self.restart_trigger = restart_trigger\n        self.generation = 0\n        self.last_restart = 0\n        self.max_generations_no_restart = 100\n\n    def __call__(self, func):\n        self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1 / (4 * self.dim) + 1 / (21 * self.dim**2))\n\n        used_budget = 0\n\n        while used_budget < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            x = self.m[:, np.newaxis] + self.sigma * z\n\n            # Repair mechanism to ensure solutions are within bounds\n            x = np.clip(x, func.bounds.lb[:, np.newaxis], func.bounds.ub[:, np.newaxis])\n\n            f = np.array([func(x[:, i]) if used_budget < self.budget else np.inf for i in range(self.popsize)])\n            used_budget += self.popsize\n\n            arindex = np.argsort(f)\n            xmu = x[:, arindex[:self.mu]]\n            zmu = z[:, arindex[:self.mu]]\n\n            if f[arindex[0]] < self.f_opt:\n                self.f_opt = f[arindex[0]]\n                self.x_opt = x[:, arindex[0]].copy()\n\n            # Mean update\n            m_old = self.m.copy()\n            self.m = (1 - self.c_mean) * self.m + self.c_mean * np.sum(xmu * self.weights[np.newaxis, :], axis=1)\n\n            # Step-size adaptation\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs)) * np.sum(zmu * self.weights[np.newaxis, :], axis=1)\n            self.sigma *= np.exp((self.cs / self.damp) * (np.linalg.norm(self.ps) / self.chiN - 1))\n\n            # Restart mechanism based on spectral properties\n            if (self.generation - self.last_restart) > self.max_generations_no_restart:\n                # Calculate the covariance matrix of the sampled solutions\n                sample_covariance = np.cov(x.T)\n                eigenvalues = np.linalg.eigvalsh(sample_covariance)\n                condition_number = np.max(eigenvalues) / np.min(eigenvalues)\n\n                if condition_number > self.restart_trigger:  # Simplified criterion\n                    self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                    self.sigma = 0.3\n                    self.ps = np.zeros(self.dim)\n                    self.last_restart = self.generation\n                    self.max_generations_no_restart = int(np.random.uniform(50, 150)) # Adaptive restart interval\n\n            self.generation += 1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm CMAES_SpectralRestart scored 0.369 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d3e25166-dcfc-49dd-9cab-47920fdbee21"], "operator": null, "metadata": {"aucs": [0.06535470891066775, 0.16016580682145354, 0.5273280593021927, 0.8962163843339435, 0.26891245770797745, 0.8272838106329589, 0.2978839125012971, 0.29854561039782856, 0.544868741281759, 0.11754673296122964, 0.21583906068412784, 0.16794925936748384, 0.29847728043042787, 0.40226825586243553, 0.657865171578324, 0.31996819710110747, 0.26264163420207165, 0.803814104126273, 0.11562149261794696, 0.1375101702369813]}}
{"id": "21fcc20f-9a65-4198-94df-3758f95634b5", "fitness": 0.26993998959192333, "name": "CMAES_AdaptiveRestart", "description": "CMA-ES with dynamic population sizing based on budget and dimension, covariance matrix adaptation with eigenvalue clipping, and adaptive step size control using success rate and stagnation detection.", "code": "import numpy as np\n\nclass CMAES_AdaptiveRestart:\n    def __init__(self, budget=10000, dim=10, popsize_multiplier=4, cs=0.3, damp=None, c_cov=None, c_mu=None, mirror_factor=1.0, stagnation_threshold=1e-9, eigenvalue_floor=1e-12):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = int(popsize_multiplier + np.floor(3 * np.log(self.dim)))  # Dynamic popsize based on dimension\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.m = None\n        self.sigma = 0.3  # exploration parameter\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.chiN = None\n\n        self.cs = cs\n        self.damp = damp if damp is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1]) + self.cs\n        self.c_cov = c_cov if c_cov is not None else 2 / ((self.dim + np.sqrt(2))**2)\n        self.c_mu = c_mu if c_mu is not None else min(1 - self.c_cov, 2 * (self.weights[0] - 1 + self.weights[0]**2) / ((self.mu + 6 + 6*self.dim**0.5)))\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.mirror_factor = mirror_factor\n        self.stagnation_threshold = stagnation_threshold\n        self.last_improvement = 0\n        self.max_no_improvement = 100 # generations before restart\n        self.eigenvalue_floor = eigenvalue_floor # Minimum eigenvalue for covariance matrix\n\n    def __call__(self, func):\n        self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1 / (4 * self.dim) + 1 / (21 * self.dim**2))\n        \n        used_budget = 0\n        generation = 0\n\n        while used_budget < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.m[:, np.newaxis] + self.sigma * C_sqrt @ z\n            \n            # Mirroring to handle bounds - more aggressive mirroring\n            xmir = self.m[:, np.newaxis] - self.mirror_factor * self.sigma * C_sqrt @ z\n            \n            # Repair mechanism to ensure solutions are within bounds and resample if necessary\n            for i in range(self.popsize):\n                if not func.bounds.lb[0] <= x[0, i] <= func.bounds.ub[0]:\n                    x[:, i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                if not func.bounds.lb[0] <= xmir[0, i] <= func.bounds.ub[0]:\n                    xmir[:, i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                    \n            f = np.array([func(x[:, i]) if used_budget < self.budget else np.inf for i in range(self.popsize)])\n            fmir = np.array([func(xmir[:, i]) if used_budget + self.popsize < self.budget else np.inf for i in range(self.popsize)])\n            \n            used_budget += 2 * self.popsize\n            \n            if np.min(f) < np.min(fmir):\n                y = x\n                fitness = f\n                best_index = np.argmin(f)\n                if f[best_index] < self.f_opt:\n                    self.f_opt = f[best_index]\n                    self.x_opt = x[:, best_index].copy()\n                    self.last_improvement = generation\n            else:\n                y = xmir\n                fitness = fmir\n                best_index = np.argmin(fmir)\n                if fmir[best_index] < self.f_opt:\n                    self.f_opt = fmir[best_index]\n                    self.x_opt = xmir[:, best_index].copy()\n                    self.last_improvement = generation\n                    \n            \n            arindex = np.argsort(fitness)\n            xmu = y[:, arindex[:self.mu]]\n            zmu = z[:, arindex[:self.mu]]\n            \n            m_old = self.m.copy()\n            self.m = np.sum(xmu * self.weights[np.newaxis, :], axis=1)\n            \n            ps_new = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs)) * (C_sqrt @ np.sum(zmu * self.weights[np.newaxis, :], axis=1))\n            self.ps = ps_new\n            \n            pc_new = (1 - self.c_cov) * self.pc + np.sqrt(self.c_cov * (2 - self.c_cov)) * (self.m - m_old) / self.sigma\n            self.pc = pc_new\n            \n            norm_ps = np.linalg.norm(self.ps)\n            factor = self.c_cov * (1 / (np.min([1, (1000000 * norm_ps**4 / self.dim)])))\n            \n            # Rank-one update of C\n            self.C = (1 - self.c_cov - self.c_mu) * self.C + self.c_cov * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :])\n            \n            # Rank-mu update of C\n            for i in range(self.mu):\n                y_diff = (xmu[:, i] - m_old) / self.sigma\n                self.C += self.c_mu * self.weights[i] * (y_diff[:, np.newaxis] @ y_diff[np.newaxis, :])\n            \n            # Eigenvalue clipping to ensure C remains positive definite and well-conditioned\n            try:\n                eigenvalues, eigenvectors = np.linalg.eigh(self.C)\n                eigenvalues = np.maximum(eigenvalues, self.eigenvalue_floor)  # Clip eigenvalues\n                self.C = eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)\n\n            # Adaptive sigma based on success rate\n            success_rate = np.mean(fitness < np.median(fitness))\n            if success_rate > 0.7:\n                self.sigma *= 1.2  # Increase step size\n            elif success_rate < 0.3:\n                self.sigma *= 0.8  # Decrease step size\n\n            # Adaptive sigma based on population diversity\n            pop_diversity = np.std(fitness)\n            if pop_diversity < self.stagnation_threshold:\n                  self.sigma *= 0.9  # Reduce step size if population is too uniform\n\n            self.sigma = min(self.sigma, 5.0) # Limit the maximum step size\n\n            self.sigma *= np.exp((self.cs / self.damp) * (norm_ps / self.chiN - 1))\n            \n            # Ensure C remains positive definite\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)  # Reset covariance matrix if it becomes non-positive definite\n\n            # Restart mechanism\n            if generation - self.last_improvement > self.max_no_improvement:\n                self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.sigma = 0.3  # Reset sigma to initial value\n                self.last_improvement = generation # reset last improvement\n                \n            generation += 1\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm CMAES_AdaptiveRestart scored 0.270 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d3e25166-dcfc-49dd-9cab-47920fdbee21"], "operator": null, "metadata": {"aucs": [0.11058826836470403, 0.2116463433138942, 0.3624950505100146, 0.20411936902041117, 0.2560990780806557, 0.1423886378903817, 0.21945968487127676, 0.3933164143967235, 0.17674669101923357, 0.15934385343229385, 0.1840112647095321, 0.5593805415279163, 0.26462222600751906, 0.19004136626878498, 0.6659473513148393, 0.4405916396954689, 0.28630097737631344, 0.16900941492912402, 0.21763099627325166, 0.18506062283612834]}}
{"id": "c3acf3e8-2e65-4b6e-852e-167139d2907d", "fitness": 0.2555255062841209, "name": "CMAES_Cauchy_Spectral", "description": "Adaptive CMA-ES with Cauchy mutation and spectral correction of the covariance matrix, ensuring positive definiteness and diversity maintenance.", "code": "import numpy as np\n\nclass CMAES_Cauchy_Spectral:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damp=None, c_cov=None, c_mu=None, stagnation_threshold=100, cauchy_mutation_prob=0.1, spectral_floor=1e-12):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.m = None\n        self.sigma = 0.3  # exploration parameter\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.chiN = None\n\n        self.cs = cs\n        self.damp = damp if damp is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1]) + self.cs\n        self.c_cov = c_cov if c_cov is not None else 2 / ((self.dim + np.sqrt(2))**2)\n        self.c_mu = c_mu if c_mu is not None else min(1 - self.c_cov, 2 * (self.weights[0] - 1 + self.weights[0]**2) / ((self.mu + 6 + 6*self.dim**0.5)))\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.last_f_opt = np.Inf\n        self.cauchy_mutation_prob = cauchy_mutation_prob\n        self.diversity_threshold = 1e-8 # threshold for considering diversity collapse\n        self.spectral_floor = spectral_floor # floor for eigenvalue of C\n\n    def __call__(self, func):\n        self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1 / (4 * self.dim) + 1 / (21 * self.dim**2))\n        \n        used_budget = 0\n        while used_budget < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = self.sqrtm(self.C)  # Use custom sqrtm\n            x = self.m[:, np.newaxis] + self.sigma * C_sqrt @ z\n            \n            # Apply Cauchy mutation with a probability\n            for i in range(self.popsize):\n                if np.random.rand() < self.cauchy_mutation_prob:\n                    cauchy_noise = np.random.standard_cauchy(size=self.dim)\n                    x[:, i] += self.sigma * cauchy_noise  #Scale cauchy noise with sigma\n\n            # Mirroring to handle bounds\n            xmir = self.m[:, np.newaxis] - self.sigma * C_sqrt @ z\n            \n            # Repair mechanism to ensure solutions are within bounds\n            for i in range(self.popsize):\n                oob = np.logical_or(x[:, i] < func.bounds.lb, x[:, i] > func.bounds.ub)\n                if np.any(oob):\n                   x[oob, i] = np.random.uniform(func.bounds.lb[oob], func.bounds.ub[oob], size=np.sum(oob))\n\n                oob = np.logical_or(xmir[:, i] < func.bounds.lb, xmir[:, i] > func.bounds.ub)\n                if np.any(oob):\n                   xmir[oob, i] = np.random.uniform(func.bounds.lb[oob], func.bounds.ub[oob], size=np.sum(oob))\n                    \n            f = np.array([func(x[:, i]) if used_budget < self.budget else np.inf for i in range(self.popsize)])\n            fmir = np.array([func(xmir[:, i]) if used_budget + self.popsize < self.budget else np.inf for i in range(self.popsize)])\n            \n            used_budget += 2 * self.popsize\n            \n            if np.min(f) < np.min(fmir):\n                y = x\n                fitness = f\n                best_index = np.argmin(f)\n                if f[best_index] < self.f_opt:\n                    self.f_opt = f[best_index]\n                    self.x_opt = x[:, best_index].copy()\n            else:\n                y = xmir\n                fitness = fmir\n                best_index = np.argmin(fmir)\n                if fmir[best_index] < self.f_opt:\n                    self.f_opt = fmir[best_index]\n                    self.x_opt = xmir[:, best_index].copy()\n            \n            #Stagnation detection\n            if self.f_opt >= self.last_f_opt:\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n            \n            if self.stagnation_counter > self.stagnation_threshold:\n                #Restart mechanism: reset covariance matrix and evolution paths\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.sigma *= 0.8 #Reduce step size\n                self.stagnation_counter = 0\n\n            self.last_f_opt = self.f_opt\n                    \n            arindex = np.argsort(fitness)\n            xmu = y[:, arindex[:self.mu]]\n            zmu = z[:, arindex[:self.mu]]\n            \n            m_old = self.m.copy()\n            self.m = np.sum(xmu * self.weights[np.newaxis, :], axis=1)\n            \n            ps_new = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs)) * (C_sqrt @ np.sum(zmu * self.weights[np.newaxis, :], axis=1))\n            self.ps = ps_new\n            \n            pc_new = (1 - self.c_cov) * self.pc + np.sqrt(self.c_cov * (2 - self.c_cov)) * (self.m - m_old) / self.sigma\n            self.pc = pc_new\n            \n            norm_ps = np.linalg.norm(self.ps)\n            factor = self.c_cov * (1 / (np.min([1, (1000000 * norm_ps**4 / self.dim)])))\n            \n            # Rank-one update of C\n            self.C = (1 - self.c_cov - self.c_mu) * self.C + self.c_cov * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :])\n            \n            # Rank-mu update of C\n            for i in range(self.mu):\n                y_diff = (xmu[:, i] - m_old) / self.sigma\n                self.C += self.c_mu * self.weights[i] * (y_diff[:, np.newaxis] @ y_diff[np.newaxis, :])\n\n            # Spectral correction to ensure C is positive definite and has minimum eigenvalue\n            self.C = self.spectral_clipping(self.C, self.spectral_floor)\n\n            # Adaptive step size adjustment based on fitness variance\n            fitness_variance = np.var(fitness)\n            if fitness_variance > 1e-3:\n                self.sigma *= np.exp(0.05 * (fitness_variance - 1e-3))\n            else:\n                self.sigma *= np.exp(-0.05 * (1e-3 - fitness_variance))\n            \n            self.sigma = max(self.sigma, 1e-8) # Ensure sigma doesn't go to zero\n\n            self.sigma *= np.exp((self.cs / self.damp) * (norm_ps / self.chiN - 1))\n\n            # Check for diversity collapse and reset covariance matrix if needed\n            if np.linalg.norm(self.C - np.diag(np.diag(self.C))) < self.diversity_threshold:\n                self.C = np.eye(self.dim)\n            \n            # Ensure C remains positive definite - Removed redundant check\n            \n        return self.f_opt, self.x_opt\n\n    def spectral_clipping(self, C, floor):\n        \"\"\"\n        Clips the eigenvalues of the covariance matrix C to be at least equal to floor.\n        Ensures C remains positive definite.\n        \"\"\"\n        try:\n            U, S, V = np.linalg.svd(C)\n            S = np.maximum(S, floor)\n            C = U @ np.diag(S) @ V\n        except np.linalg.LinAlgError:\n            C = np.eye(self.dim)  # Reset covariance matrix if SVD fails\n        return C\n\n    def sqrtm(self, C):\n        \"\"\"\n        Calculates the square root of a matrix C using eigenvalue decomposition.\n        Handles potential numerical instability by clipping eigenvalues.\n        \"\"\"\n        try:\n            U, S, V = np.linalg.svd(C)\n            S = np.maximum(S, 0)  # Ensure eigenvalues are non-negative\n            C_sqrt = U @ np.diag(np.sqrt(S)) @ V\n            return C_sqrt\n        except np.linalg.LinAlgError:\n            return np.eye(self.dim) #Return identity matrix if SVD fails", "configspace": "", "generation": 4, "feedback": "The algorithm CMAES_Cauchy_Spectral scored 0.256 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["7c5eff6d-6304-4dc5-91ce-8aac1f150ad9"], "operator": null, "metadata": {"aucs": [0.05433442911723574, 0.1261004273455243, 0.4110228560671829, 0.21536536881770174, 0.21236329140852017, 0.21124923590629363, 0.28523652138892497, 0.20742031019755436, 0.16143143645185687, 0.15476391984608961, 0.21791540937307952, 0.951307821330883, 0.06997708079032361, 0.1711928095496169, 0.580098562567332, 0.2626686269510379, 0.22924830150226405, 0.2770011827920784, 0.14842735784761885, 0.16338517643129913]}}
{"id": "696027db-fd66-4062-8e29-d7ddd86f6cad", "fitness": 0.646619212135261, "name": "AdaptiveDifferentialEvolution", "description": "An enhanced adaptive Differential Evolution algorithm that incorporates orthogonal design for improved mutation, adaptive population diversity control using the distance to the best solution, and parameter adaptation based on success history with a bias towards smaller F values.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, popsize_init=None, F_init=0.5, CR_init=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.popsize_min = 10\n        self.popsize_max = 100\n        self.popsize = popsize_init if popsize_init is not None else 2 * self.dim\n        self.F = F_init\n        self.CR = CR_init\n        self.archive_factor = 2.6  # Archive size relative to population size\n        self.archive = []\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.success_F = []\n        self.success_CR = []\n        self.success_count = 0\n        self.min_successes = 5\n        self.adapt_frequency = 10 # How often to adapt parameters\n        self.pop_adapt_frequency = 50 # How often to adapt popsize\n        self.stagnation_threshold = 1e-8\n        self.success_rate_threshold = 0.2\n        self.diversity_threshold = 0.01\n\n    def __call__(self, func):\n        bounds_lb = func.bounds.lb\n        bounds_ub = func.bounds.ub\n        self.population = np.random.uniform(bounds_lb, bounds_ub, size=(self.popsize, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        eval_count = self.popsize\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = self.population[np.argmin(fitness)].copy()\n        best_fitness_history = [self.f_opt]\n\n        generation = 0\n        while eval_count < self.budget:\n            for i in range(self.popsize):\n                # Mutation using orthogonal design\n                indices = np.random.choice(self.popsize, 4, replace=False)\n                x_r1, x_r2, x_r3, x_r4 = self.population[indices]\n\n                # Orthogonal array based mutation\n                mutant = self.population[i] + self.F * (x_r1 - x_r2 + x_r3 - x_r4)\n                mutant = np.clip(mutant, bounds_lb, bounds_ub)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.CR\n                trial_vector = np.where(crossover_mask, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial_vector)\n                eval_count += 1\n\n                if f_trial < fitness[i]:\n                    self.success_F.append(self.F)\n                    self.success_CR.append(self.CR)\n                    self.success_count += 1\n                    self.archive.append(self.population[i].copy())\n                    if len(self.archive) > self.archive_factor * self.popsize:\n                        self.archive.pop(0)\n\n                    fitness[i] = f_trial\n                    self.population[i] = trial_vector.copy()\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial_vector.copy()\n\n                if eval_count >= self.budget:\n                    break\n\n            generation += 1\n            best_fitness_history.append(self.f_opt)\n\n            # Adapt parameters\n            if generation % self.adapt_frequency == 0:\n                if self.success_count > self.min_successes:\n                    # Adapt F and CR using weighted Lehmer mean, bias F towards smaller values\n                    weights = np.arange(1, len(self.success_F) + 1)  # Assign higher weights to more recent successes\n                    weights = weights / np.sum(weights) # Normalize weights\n\n                    if self.success_F:\n                        self.F = np.sum(weights * (np.array(self.success_F)**2)) / np.sum(weights * np.array(self.success_F))\n                        self.F = np.clip(self.F, 0.1, 0.9) * 0.8 + 0.1 # Bias towards smaller F\n                    if self.success_CR:\n                        self.CR = np.sum(weights * np.array(self.success_CR)) / np.sum(weights)\n                        self.CR = np.clip(self.CR, 0.1, 0.9)\n                else:\n                    self.F = 0.5 # Default F\n                    self.CR = 0.7 # Default CR\n\n                self.success_F = []\n                self.success_CR = []\n                self.success_count = 0\n\n            # Adapt population size\n            if generation % self.pop_adapt_frequency == 0:\n                # Stagnation detection based on fitness history\n                if len(best_fitness_history) > 10:\n                    fitness_improvement = np.mean(np.abs(np.diff(best_fitness_history[-10:])))\n                    stagnated = fitness_improvement < self.stagnation_threshold\n                else:\n                    stagnated = False\n\n                success_rate = self.success_count / (self.popsize * self.pop_adapt_frequency) if generation > 0 else 0\n\n                # Population diversity control\n                distances = np.linalg.norm(self.population - self.x_opt, axis=1)\n                diversity = np.mean(distances)\n\n                if stagnated and success_rate < self.success_rate_threshold:\n                     self.popsize = max(self.popsize_min, int(self.popsize * 0.75)) # Reduce popsize more aggressively\n                elif not stagnated and success_rate > 0.3 and diversity > self.diversity_threshold:\n                    self.popsize = min(self.popsize_max, int(self.popsize * 1.2)) # Increase popsize more aggressively\n                elif diversity < self.diversity_threshold:\n                    # If diversity is too low, re-initialize some individuals\n                    num_reinit = int(self.popsize * 0.2)\n                    indices_to_reinit = np.random.choice(self.popsize, num_reinit, replace=False)\n                    self.population[indices_to_reinit] = np.random.uniform(bounds_lb, bounds_ub, size=(num_reinit, self.dim))\n                    fitness[indices_to_reinit] = [func(x) for x in self.population[indices_to_reinit]]\n                    eval_count += num_reinit\n\n                \n                # Resize population (crude approach)\n                if self.popsize != self.population.shape[0]:\n                    new_population = np.random.uniform(bounds_lb, bounds_ub, size=(self.popsize, self.dim))\n                    if self.popsize > self.population.shape[0]:\n                        new_population[:self.population.shape[0]] = self.population\n                    else:\n                        new_population = self.population[:self.popsize]\n\n                    self.population = new_population\n                    fitness = np.array([func(x) for x in self.population])\n                    eval_count += abs(self.popsize - self.population.shape[0])\n                    if eval_count >= self.budget:\n                        break # Ensure we don't exceed budget\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.647 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["95c5cb1f-84d1-4750-bec2-f07f24d20671"], "operator": null, "metadata": {"aucs": [0.18943770339179566, 0.5112297250267921, 0.761880658926922, 0.9285307714701445, 0.7686197864454659, 0.8776395698166751, 0.4109747345148801, 0.6471814798436786, 0.8270358640363943, 0.2857460906653484, 0.9068212868432024, 0.9976175049154462, 0.4194043006680178, 0.5499524350003363, 0.912652427196696, 0.8473985748918795, 0.49171438357317576, 0.9058796401558903, 0.19143304646522352, 0.5012342588572584]}}
{"id": "c592a247-f09c-4c0c-a442-c83247464d0a", "fitness": -Infinity, "name": "CMAES_ToroidalOrthogonal", "description": "An adaptive CMA-ES variant that uses a toroidal search space by wrapping solutions around the boundaries and incorporates orthogonal learning to refine search directions.", "code": "import numpy as np\n\nclass CMAES_ToroidalOrthogonal:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damp=None, c_cov=None, c_mu=None, stagnation_threshold=100, orthogonal_learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.m = None\n        self.sigma = 0.3\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.chiN = None\n\n        self.cs = cs\n        self.damp = damp if damp is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1]) + self.cs\n        self.c_cov = c_cov if c_cov is not None else 2 / ((self.dim + np.sqrt(2))**2)\n        self.c_mu = c_mu if c_mu is not None else min(1 - self.c_cov, 2 * (self.weights[0] - 1 + self.weights[0]**2) / ((self.mu + 6 + 6*self.dim**0.5)))\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.last_f_opt = np.Inf\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n\n    def __call__(self, func):\n        self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1 / (4 * self.dim) + 1 / (21 * self.dim**2))\n        \n        used_budget = 0\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        while used_budget < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.m[:, np.newaxis] + self.sigma * C_sqrt @ z\n            \n            # Toroidal search space: wrap around boundaries\n            x = lb + (x - lb) % (ub - lb)\n            \n            f = np.array([func(x[:, i]) if used_budget < self.budget else np.inf for i in range(self.popsize)])\n            used_budget += self.popsize\n            \n            arindex = np.argsort(f)\n            xmu = x[:, arindex[:self.mu]]\n            zmu = z[:, arindex[:self.mu]]\n            \n            if f[arindex[0]] < self.f_opt:\n                self.f_opt = f[arindex[0]]\n                self.x_opt = x[:, arindex[0]].copy()\n            \n            #Stagnation detection\n            if self.f_opt >= self.last_f_opt:\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n            \n            if self.stagnation_counter > self.stagnation_threshold:\n                #Restart mechanism: reset covariance matrix and evolution paths\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.sigma *= 0.8 #Reduce step size\n                self.stagnation_counter = 0\n\n            self.last_f_opt = self.f_opt\n            \n            m_old = self.m.copy()\n            self.m = np.sum(xmu * self.weights[np.newaxis, :], axis=1)\n            \n            ps_new = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs)) * (C_sqrt @ np.sum(zmu * self.weights[np.newaxis, :], axis=1))\n            self.ps = ps_new\n            \n            pc_new = (1 - self.c_cov) * self.pc + np.sqrt(self.c_cov * (2 - self.c_cov)) * (self.m - m_old) / self.sigma\n            self.pc = pc_new\n            \n            norm_ps = np.linalg.norm(self.ps)\n            factor = self.c_cov * (1 / (np.min([1, (1000000 * norm_ps**4 / self.dim)])))\n            \n            # Rank-one update of C\n            self.C = (1 - self.c_cov - self.c_mu) * self.C + self.c_cov * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :])\n            \n            # Rank-mu update of C\n            for i in range(self.mu):\n                y_diff = (xmu[:, i] - m_old) / self.sigma\n                self.C += self.c_mu * self.weights[i] * (y_diff[:, np.newaxis] @ y_diff[np.newaxis, :])\n                \n            # Orthogonal Learning: Refine search directions\n            Q, R = np.linalg.qr(np.random.randn(self.dim, self.mu))\n            x_orth = self.m[:, np.newaxis] + self.sigma * C_sqrt @ Q\n            x_orth = lb + (x_orth - lb) % (ub - lb)\n            f_orth = np.array([func(x_orth[:, i]) if used_budget < self.budget else np.inf for i in range(self.mu)])\n            used_budget += self.mu\n            \n            for i in range(self.mu):\n                if f_orth[i] < f[arindex[i]]:\n                    self.m = self.m * (1 - self.orthogonal_learning_rate) + x_orth[:, i] * self.orthogonal_learning_rate\n            \n            # Adaptive step size adjustment based on fitness variance\n            fitness_variance = np.var(f)\n            if fitness_variance > 1e-3:\n                self.sigma *= np.exp(0.05 * (fitness_variance - 1e-3))\n            else:\n                self.sigma *= np.exp(-0.05 * (1e-3 - fitness_variance))\n            \n            self.sigma = max(self.sigma, 1e-8) # Ensure sigma doesn't go to zero\n\n            self.sigma *= np.exp((self.cs / self.damp) * (norm_ps / self.chiN - 1))\n            \n            # Ensure C remains positive definite\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)  # Reset covariance matrix if it becomes non-positive definite\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "An exception occurred: operands could not be broadcast together with shapes (2,6) (2,) .", "error": "", "parent_ids": ["7c5eff6d-6304-4dc5-91ce-8aac1f150ad9"], "operator": null, "metadata": {}}
{"id": "0332e562-e27e-442e-a2d3-5136b086aa44", "fitness": 0.19437561975607454, "name": "CMAES_DualCovarianceAdaptive", "description": "Adaptive CMA-ES with dual covariance matrices and dynamic phase switching based on stagnation and diversity, incorporating a covariance matrix adaptation for better exploration in challenging landscapes and reset strategies.", "code": "import numpy as np\n\nclass CMAES_DualCovarianceAdaptive:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damp=None, c_cov=None, c_mu=None, stagnation_threshold=100, orthogonal_factor=0.5, diversity_threshold=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.m = None\n        self.sigma = 0.3\n        self.C_exploit = None  # Covariance matrix for exploitation\n        self.C_explore = None  # Covariance matrix for exploration\n        self.pc_exploit = None\n        self.ps_exploit = None\n        self.pc_explore = None\n        self.ps_explore = None\n        self.chiN = None\n\n        self.cs = cs\n        self.damp = damp if damp is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1]) + self.cs\n        self.c_cov = c_cov if c_cov is not None else 2 / ((self.dim + np.sqrt(2))**2)\n        self.c_mu = c_mu if c_mu is not None else min(1 - self.c_cov, 2 * (self.weights[0] - 1 + self.weights[0]**2) / ((self.mu + 6 + 6*self.dim**0.5)))\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.orthogonal_factor = orthogonal_factor\n        self.exploitation_phase = True\n        self.diversity_threshold = diversity_threshold\n        self.diversity_metric = None # Store diversity metric of the population to check stagnation\n\n    def __call__(self, func):\n        self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.C_exploit = np.eye(self.dim)\n        self.C_explore = np.eye(self.dim)\n        self.pc_exploit = np.zeros(self.dim)\n        self.ps_exploit = np.zeros(self.dim)\n        self.pc_explore = np.zeros(self.dim)\n        self.ps_explore = np.zeros(self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1 / (4 * self.dim) + 1 / (21 * self.dim**2))\n        \n        used_budget = 0\n        best_f_history = []\n\n        while used_budget < self.budget:\n            if self.exploitation_phase:\n                C = self.C_exploit\n                pc = self.pc_exploit\n                ps = self.ps_exploit\n            else:\n                C = self.C_explore\n                pc = self.pc_explore\n                ps = self.ps_explore\n            \n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n\n            # Orthogonal sampling using a random rotation matrix\n            if self.orthogonal_factor > 0:\n                Q, _ = np.linalg.qr(np.random.randn(self.dim, self.dim))\n                z = self.orthogonal_factor * Q @ z + (1 - self.orthogonal_factor) * z\n\n            C_sqrt = np.linalg.cholesky(C)\n            x = self.m[:, np.newaxis] + self.sigma * C_sqrt @ z\n\n            # Repair mechanism\n            x = np.clip(x, func.bounds.lb[:, np.newaxis], func.bounds.ub[:, np.newaxis])\n\n            f = np.array([func(x[:, i]) if used_budget < self.budget else np.inf for i in range(self.popsize)])\n            used_budget += self.popsize\n\n            arindex = np.argsort(f)\n            xmu = x[:, arindex[:self.mu]]\n            zmu = z[:, arindex[:self.mu]]\n\n            m_old = self.m.copy()\n            self.m = np.sum(xmu * self.weights[np.newaxis, :], axis=1)\n\n            # Update exploitation covariance\n            if self.exploitation_phase:\n                ps_new = (1 - self.cs) * ps + np.sqrt(self.cs * (2 - self.cs)) * (C_sqrt @ np.sum(zmu * self.weights[np.newaxis, :], axis=1))\n                self.ps_exploit = ps_new\n                pc_new = (1 - self.c_cov) * pc + np.sqrt(self.c_cov * (2 - self.c_cov)) * (self.m - m_old) / self.sigma\n                self.pc_exploit = pc_new\n                self.C_exploit = (1 - self.c_cov - self.c_mu) * C + self.c_cov * (pc[:, np.newaxis] @ pc[np.newaxis, :])\n                for i in range(self.mu):\n                    y_diff = (xmu[:, i] - m_old) / self.sigma\n                    self.C_exploit += self.c_mu * self.weights[i] * (y_diff[:, np.newaxis] @ y_diff[np.newaxis, :])\n            \n            # Update exploration covariance\n            else:\n                ps_new = (1 - self.cs) * ps + np.sqrt(self.cs * (2 - self.cs)) * (C_sqrt @ np.sum(zmu * self.weights[np.newaxis, :], axis=1))\n                self.ps_explore = ps_new\n                pc_new = (1 - self.c_cov) * pc + np.sqrt(self.c_cov * (2 - self.c_cov)) * (self.m - m_old) / self.sigma\n                self.pc_explore = pc_new\n                self.C_explore = (1 - self.c_cov - self.c_mu) * C + self.c_cov * (pc[:, np.newaxis] @ pc[np.newaxis, :])\n                for i in range(self.mu):\n                    y_diff = (xmu[:, i] - m_old) / self.sigma\n                    self.C_explore += self.c_mu * self.weights[i] * (y_diff[:, np.newaxis] @ y_diff[np.newaxis, :])\n\n            self.sigma *= np.exp((self.cs / self.damp) * (np.linalg.norm(ps) / self.chiN - 1))\n\n            # Stagnation detection\n            if f[arindex[0]] < self.f_opt:\n                self.f_opt = f[arindex[0]]\n                self.x_opt = x[:, arindex[0]].copy()\n                self.stagnation_counter = 0\n            else:\n                self.stagnation_counter += 1\n            \n            # Diversity Check\n            self.diversity_metric = np.std(x)\n            \n            # Phase Switch based on Stagnation and Diversity\n            if self.stagnation_counter > self.stagnation_threshold or self.diversity_metric < self.diversity_threshold:\n                self.exploitation_phase = not self.exploitation_phase\n                self.stagnation_counter = 0\n                self.sigma *= 2  # Increase sigma for exploration\n                \n                # Resetting pc and ps can help exploration\n                if self.exploitation_phase:\n                    self.pc_exploit = np.zeros(self.dim)\n                    self.ps_exploit = np.zeros(self.dim)\n                    # Adapt C_exploit to be more isotropic\n                    self.C_exploit = np.eye(self.dim)\n                else:\n                    self.pc_explore = np.zeros(self.dim)\n                    self.ps_explore = np.zeros(self.dim)\n                    # Adapt C_explore to be more isotropic\n                    self.C_explore = np.eye(self.dim)\n                    \n            # Covariance Matrix Adaptation based on Eigenvalues in Exploration\n            if not self.exploitation_phase:\n                try:\n                    eigenvalues, eigenvectors = np.linalg.eigh(self.C_explore)\n                    # Adapt covariance matrix based on eigenvalues\n                    if np.min(eigenvalues) < 1e-6:\n                        eigenvalues[eigenvalues < 1e-6] = 1e-6\n                    self.C_explore = eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T\n                except np.linalg.LinAlgError:\n                    self.C_explore = np.eye(self.dim) # Reset if calculation fails\n            \n            # Ensure C remains positive definite\n            try:\n                if self.exploitation_phase:\n                    np.linalg.cholesky(self.C_exploit)\n                else:\n                    np.linalg.cholesky(self.C_explore)\n            except np.linalg.LinAlgError:\n                if self.exploitation_phase:\n                     self.C_exploit = np.eye(self.dim)\n                else:\n                    self.C_explore = np.eye(self.dim)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm CMAES_DualCovarianceAdaptive scored 0.194 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["91d960d4-19a7-4830-9d58-6d9e633f033c"], "operator": null, "metadata": {"aucs": [0.04899580458970543, 0.16917372904804306, 0.1760470861145046, 0.4496906564014589, 0.09892310792179038, 0.1124981770161314, 0.21786143048752082, 0.2089295632190511, 0.15527601848154904, 0.08470187948417118, 0.1704039975263173, 0.12229514172646194, 0.22199284982703038, 0.10988500265548451, 0.6524844404906588, 0.2868912005674401, 0.21775112456623225, 0.1456910314810752, 0.11195577449916805, 0.1260643790176964]}}
{"id": "dbd151e6-1631-406f-b9f3-54f9ceb10156", "fitness": -Infinity, "name": "CMAES_SpectralRestart_Enhanced", "description": "Adaptive CMA-ES with spectral restarts using a more robust condition number estimation and a separate learning rate for covariance matrix adaptation to improve exploration and convergence.", "code": "import numpy as np\n\nclass CMAES_SpectralRestart_Enhanced:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damp=None, c_mean=0.1, c_cov=0.05, restart_trigger=1e3, min_eigenvalue=1e-12):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.m = None\n        self.sigma = 0.3\n        self.C = np.eye(self.dim)  # Initialize covariance matrix\n        self.ps = None # evolution path for sigma\n        self.pc = None # evolution path for covariance\n        self.chiN = None\n        self.cs = cs\n        self.damp = damp if damp is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1]) + self.cs\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.c_mean = c_mean # learning rate for mean update\n        self.c_cov = c_cov  # Learning rate for covariance matrix\n        self.restart_trigger = restart_trigger\n        self.generation = 0\n        self.last_restart = 0\n        self.max_generations_no_restart = 100\n        self.min_eigenvalue = min_eigenvalue\n\n    def __call__(self, func):\n        self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.ps = np.zeros(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1 / (4 * self.dim) + 1 / (21 * self.dim**2))\n\n        used_budget = 0\n\n        while used_budget < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            # Apply the covariance matrix\n            z_weighted = np.linalg.cholesky(self.C) @ z\n            x = self.m[:, np.newaxis] + self.sigma * z_weighted\n\n            # Repair mechanism to ensure solutions are within bounds\n            x = np.clip(x, func.bounds.lb[:, np.newaxis], func.bounds.ub[:, np.newaxis])\n\n            f = np.array([func(x[:, i]) if used_budget < self.budget else np.inf for i in range(self.popsize)])\n            used_budget += self.popsize\n\n            arindex = np.argsort(f)\n            xmu = x[:, arindex[:self.mu]]\n            zmu = z[:, arindex[:self.mu]]\n            zmu_weighted = z_weighted[:, arindex[:self.mu]]\n\n\n            if f[arindex[0]] < self.f_opt:\n                self.f_opt = f[arindex[0]]\n                self.x_opt = x[:, arindex[0]].copy()\n\n            # Mean update\n            m_old = self.m.copy()\n            self.m = (1 - self.c_mean) * self.m + self.c_mean * np.sum(xmu * self.weights[np.newaxis, :], axis=1)\n\n            # Step-size adaptation\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs)) * np.sum(zmu * self.weights[np.newaxis, :], axis=1)\n            self.sigma *= np.exp((self.cs / self.damp) * (np.linalg.norm(self.ps) / self.chiN - 1))\n\n            # Covariance matrix adaptation\n            y = (self.m - m_old) / self.sigma\n            self.pc = (1 - self.cs) * self.pc + np.sqrt(self.cs * (2 - self.cs)) * y\n            \n            delta = (1 - np.linalg.norm(self.pc)**2 / (self.dim * (1 + self.cs**2)))\n            delta = max(0, delta)\n            \n            self.C = (1 - self.c_cov) * self.C + self.c_cov * (delta * self.pc[:, np.newaxis] @ self.pc[np.newaxis, :] + \\\n                                                        np.sum(self.weights[np.newaxis, :] * zmu * zmu.T, axis = 2)) #Correct this line!\n            \n            # Ensure positive definiteness of C\n            eigenvalues, eigenvectors = np.linalg.eigh(self.C)\n            eigenvalues = np.maximum(eigenvalues, self.min_eigenvalue)\n            self.C = eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T\n\n            # Restart mechanism based on spectral properties\n            if (self.generation - self.last_restart) > self.max_generations_no_restart:\n                # Estimate condition number using robust method\n                eigenvalues, _ = np.linalg.eig(self.C)\n                eigenvalues = np.abs(eigenvalues)\n                condition_number = np.max(eigenvalues) / np.min(eigenvalues)\n\n                if condition_number > self.restart_trigger:\n                    self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                    self.sigma = 0.3\n                    self.ps = np.zeros(self.dim)\n                    self.pc = np.zeros(self.dim)\n                    self.C = np.eye(self.dim)\n                    self.last_restart = self.generation\n                    self.max_generations_no_restart = int(np.random.uniform(50, 150)) # Adaptive restart interval\n\n            self.generation += 1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "An exception occurred: operands could not be broadcast together with shapes (2,3) (3,2) .", "error": "", "parent_ids": ["f67b95b6-4a84-45b3-bcef-74f2d906f2a5"], "operator": null, "metadata": {}}
{"id": "81c25b59-7f7f-442d-956c-ada7ece4684d", "fitness": -Infinity, "name": "AdaptiveCMAES_Ortho", "description": "An Adaptive CMA-ES with orthogonal initialization, a simplified rank-one covariance matrix update with decay and selective pressure, and a dynamic population size adjustment based on the optimization progress.", "code": "import numpy as np\n\nclass AdaptiveCMAES_Ortho:\n    def __init__(self, budget=10000, dim=10, initial_popsize=None, c_sigma=0.3, c_cov=0.1, mu_factor=0.25, stagnation_threshold=50, ortho_trials=5):\n        self.budget = budget\n        self.dim = dim\n        self.initial_popsize = initial_popsize if initial_popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.popsize = self.initial_popsize\n        self.mu = int(self.popsize * mu_factor)  # Number of parents\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.m = None\n        self.sigma = 0.3\n        self.C = None\n        self.ps = None\n        self.c_sigma = c_sigma\n        self.c_cov = c_cov\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.last_f_opt = np.Inf\n        self.min_popsize = 4\n        self.ortho_trials = ortho_trials\n        self.mu_factor = mu_factor\n\n    def __call__(self, func):\n        # Orthogonal Initialization\n        best_f = np.inf\n        best_x = None\n        for _ in range(self.ortho_trials):\n            H = self.create_orthogonal_matrix(self.dim)\n            x = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n            x_init = x + self.sigma * H[:, 0]\n            x_init = np.clip(x_init, func.bounds.lb, func.bounds.ub)\n            f = func(x_init)\n            if f < best_f:\n                best_f = f\n                best_x = x_init\n        self.m = best_x\n        self.f_opt = best_f\n        self.x_opt = best_x\n        \n        self.C = np.eye(self.dim)\n        self.ps = np.zeros(self.dim)\n        used_budget = 0\n\n        while used_budget < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C) #Using cholesky decomposition\n\n            x = self.m[:, np.newaxis] + self.sigma * C_sqrt @ z\n            # Repair mechanism to ensure solutions are within bounds\n            x = np.clip(x, func.bounds.lb[:, np.newaxis], func.bounds.ub[:, np.newaxis])\n            \n            f = np.array([func(x[:, i]) if used_budget < self.budget else np.inf for i in range(self.popsize)])\n            used_budget += self.popsize\n            \n            arindex = np.argsort(f)\n            xmu = x[:, arindex[:self.mu]]\n            fmu = f[arindex[:self.mu]]\n\n            if fmu[0] < self.f_opt:\n                self.f_opt = fmu[0]\n                self.x_opt = xmu[:, 0].copy()\n            \n            #Stagnation detection\n            if self.f_opt >= self.last_f_opt:\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                # Restart Mechanism: Reset to best seen and increase population size if possible\n                self.m = self.x_opt.copy()\n                self.C = np.eye(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.stagnation_counter = 0\n                self.popsize = min(self.popsize * 2, self.budget // 2) #Increase population size, ensure we have budget\n                self.mu = int(self.popsize * self.mu_factor)\n                self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n                self.weights /= np.sum(self.weights)\n\n\n            self.last_f_opt = self.f_opt\n\n            # Simplified Rank-One Covariance Matrix Update\n            y = self.m - np.mean(xmu, axis=1)\n            self.ps = (1 - self.c_sigma) * self.ps + np.sqrt(self.c_sigma * (2 - self.c_sigma)) * y / self.sigma\n            self.C = (1 - self.c_cov) * self.C + self.c_cov * (self.ps[:, np.newaxis] @ self.ps[np.newaxis, :]) #Rank-one update\n            \n            # Ensure C remains positive definite - Clipping eigenvalues\n            U, S, V = np.linalg.svd(self.C)\n            S = np.maximum(S, 1e-10)\n            self.C = U @ np.diag(S) @ V\n            \n            # Update mean\n            self.m = np.sum(xmu * self.weights[np.newaxis, :], axis=1)\n            \n            # Adapt step size\n            self.sigma *= np.exp(self.c_sigma / 0.5 * (np.linalg.norm(self.ps) / np.sqrt(self.dim) - 1))\n            self.sigma = max(self.sigma, 1e-8) #Ensure sigma does not become zero\n                \n        return self.f_opt, self.x_opt\n\n    def create_orthogonal_matrix(self, dim):\n        H = np.random.randn(dim, dim)\n        q, r = np.linalg.qr(H)\n        return q", "configspace": "", "generation": 5, "feedback": "An exception occurred: operands could not be broadcast together with shapes (5,2) (1,4) .", "error": "", "parent_ids": ["c3acf3e8-2e65-4b6e-852e-167139d2907d"], "operator": null, "metadata": {}}
{"id": "450f5550-9f6c-45d9-8b66-1fe70f0f03ef", "fitness": 0.0, "name": "CMAES_OrthoLocal", "description": "An adaptive CMA-ES that uses a combination of orthogonal sampling to enhance exploration and a local search strategy to refine promising solutions.", "code": "import numpy as np\nfrom scipy.stats import norm\nfrom scipy.linalg import sqrtm\n\nclass CMAES_OrthoLocal:\n    def __init__(self, budget=10000, dim=10, popsize_multiplier=4, cs=0.3, damp=None, c_cov=None, c_mu=None, local_search_probability=0.1, local_search_radius=0.1, orthogonal_sample_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = int(popsize_multiplier + np.floor(3 * np.log(self.dim)))  # Dynamic popsize based on dimension\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.m = None\n        self.sigma = 0.3\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.chiN = None\n\n        self.cs = cs\n        self.damp = damp if damp is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1]) + self.cs\n        self.c_cov = c_cov if c_cov is not None else 2 / ((self.dim + np.sqrt(2))**2)\n        self.c_mu = c_mu if c_mu is not None else min(1 - self.c_cov, 2 * (self.weights[0] - 1 + self.weights[0]**2) / ((self.mu + 6 + 6*self.dim**0.5)))\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.local_search_probability = local_search_probability\n        self.local_search_radius = local_search_radius\n        self.orthogonal_sample_size = orthogonal_sample_size\n\n    def orthogonal_sampling(self, n_samples):\n        # Generate Latin Hypercube Samples\n        samples = np.zeros((self.dim, n_samples))\n        for i in range(self.dim):\n            samples[i, :] = (np.random.permutation(n_samples) + np.random.rand(n_samples)) / n_samples\n        return samples\n\n    def __call__(self, func):\n        self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1 / (4 * self.dim) + 1 / (21 * self.dim**2))\n        used_budget = 0\n        generation = 0\n\n        while used_budget < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.m[:, np.newaxis] + self.sigma * C_sqrt @ z\n\n            # Repair mechanism to ensure solutions are within bounds and resample if necessary\n            for i in range(self.popsize):\n                x[:, i] = np.clip(x[:, i], func.bounds.lb, func.bounds.ub)\n\n            f = np.array([func(x[:, i]) if used_budget < self.budget else np.inf for i in range(self.popsize)])\n            used_budget += self.popsize\n\n            # Local Search\n            for i in range(self.popsize):\n                if np.random.rand() < self.local_search_probability:\n                    x_local = x[:, i].copy()\n                    # Perturb each dimension with a small random value\n                    x_local += np.random.uniform(-self.local_search_radius, self.local_search_radius, size=self.dim)\n                    x_local = np.clip(x_local, func.bounds.lb, func.bounds.ub)\n\n                    f_local = func(x_local) if used_budget < self.budget else np.inf\n                    used_budget += 1\n                    if f_local < f[i]:\n                        f[i] = f_local\n                        x[:, i] = x_local\n\n            # Orthogonal Sampling around the best solution\n            best_index = np.argmin(f)\n            if f[best_index] < self.f_opt:\n                self.f_opt = f[best_index]\n                self.x_opt = x[:, best_index].copy()\n\n            # Generate orthogonal samples around the best solution\n            lhs_samples = self.orthogonal_sampling(self.orthogonal_sample_size)\n            x_ortho = self.x_opt[:, np.newaxis] + self.sigma * (lhs_samples - 0.5) * (func.bounds.ub[0] - func.bounds.lb[0]) # Scale samples to the search space\n\n            # Evaluate orthogonal samples\n            f_ortho = np.array([func(x_ortho[:, i]) if used_budget < self.budget else np.inf for i in range(self.orthogonal_sample_size)])\n            used_budget += self.orthogonal_sample_size\n\n            # Update best solution if orthogonal samples are better\n            best_ortho_index = np.argmin(f_ortho)\n            if f_ortho[best_ortho_index] < self.f_opt:\n                self.f_opt = f_ortho[best_ortho_index]\n                self.x_opt = x_ortho[:, best_ortho_index].copy()\n                f[best_index] = f_ortho[best_ortho_index]\n                x[:,best_index] = x_ortho[:, best_ortho_index].copy()\n                \n            arindex = np.argsort(f)\n            xmu = x[:, arindex[:self.mu]]\n            zmu = z[:, arindex[:self.mu]]\n\n            m_old = self.m.copy()\n            self.m = np.sum(xmu * self.weights[np.newaxis, :], axis=1)\n\n            ps_new = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs)) * (C_sqrt @ np.sum(zmu * self.weights[np.newaxis, :], axis=1))\n            self.ps = ps_new\n\n            pc_new = (1 - self.c_cov) * self.pc + np.sqrt(self.c_cov * (2 - self.c_cov)) * (self.m - m_old) / self.sigma\n            self.pc = pc_new\n\n            norm_ps = np.linalg.norm(self.ps)\n\n            # Rank-one update of C\n            self.C = (1 - self.c_cov - self.c_mu) * self.C + self.c_cov * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :])\n\n            # Rank-mu update of C\n            for i in range(self.mu):\n                y_diff = (xmu[:, i] - m_old) / self.sigma\n                self.C += self.c_mu * self.weights[i] * (y_diff[:, np.newaxis] @ y_diff[np.newaxis, :])\n\n            # Eigenvalue decomposition and clipping\n            try:\n                eigenvalues, eigenvectors = np.linalg.eigh(self.C)\n                eigenvalues = np.maximum(eigenvalues, 1e-10)  # Prevent negative eigenvalues\n                self.C = eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)\n\n            self.sigma *= np.exp((self.cs / self.damp) * (norm_ps / self.chiN - 1))\n            self.sigma = min(self.sigma, 5.0) # Limit the maximum step size\n\n            generation += 1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm CMAES_OrthoLocal scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["21fcc20f-9a65-4198-94df-3758f95634b5"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "9fb655e6-fa43-4810-bc6f-68b1c14b5e5a", "fitness": -Infinity, "name": "CMAES_Orthogonal", "description": "CMA-ES with orthogonal sampling for improved exploration and exploitation, using a Sobol sequence to generate a diverse set of initial samples and adaptively adjusting the step size and covariance matrix based on success history and population diversity.", "code": "import numpy as np\nfrom scipy.stats import norm\nfrom scipy.linalg import sqrtm\n\nclass CMAES_Orthogonal:\n    def __init__(self, budget=10000, dim=10, popsize_multiplier=4, cs=0.3, damp=None, c_cov=None, c_mu=None, orthogonal_samples=64, stagnation_threshold=1e-9, eigenvalue_floor=1e-12):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = int(popsize_multiplier + np.floor(3 * np.log(self.dim)))  # Dynamic popsize based on dimension\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.m = None\n        self.sigma = 0.3  # exploration parameter\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.chiN = None\n\n        self.cs = cs\n        self.damp = damp if damp is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1]) + self.cs\n        self.c_cov = c_cov if c_cov is not None else 2 / ((self.dim + np.sqrt(2))**2)\n        self.c_mu = c_mu if c_mu is not None else min(1 - self.c_cov, 2 * (self.weights[0] - 1 + self.weights[0]**2) / ((self.mu + 6 + 6*self.dim**0.5)))\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.orthogonal_samples = orthogonal_samples\n        self.stagnation_threshold = stagnation_threshold\n        self.last_improvement = 0\n        self.max_no_improvement = 100  # generations before restart\n        self.eigenvalue_floor = eigenvalue_floor  # Minimum eigenvalue for covariance matrix\n\n    def sobol_sequence(self, n_points, dim):\n        \"\"\"Generate Sobol sequence.\"\"\"\n        V = np.zeros((dim, 32), dtype=int)\n        C = np.zeros(32, dtype=int)\n        X = np.zeros((n_points, dim))\n\n        # Compute direction vectors V[i][j]\n        for i in range(dim):\n            m = 1\n            while m < 2 * dim:\n                m *= 2\n            V[i][0] = 1\n            for j in range(1, 32):\n                V[i][j] = V[i][j - 1] ^ (V[i][j - 1] >> i)\n        \n        for k in range(n_points):\n            C[0] = 1\n            for j in range(1, 32):\n                C[j] = C[j - 1] ^ (C[j - 1] >> 1)\n            for i in range(dim):\n                X[k][i] = 0\n                for j in range(32):\n                    X[k][i] ^= V[i][j] * C[j]\n                X[k][i] /= 2**32\n        return X\n\n    def __call__(self, func):\n        self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1 / (4 * self.dim) + 1 / (21 * self.dim**2))\n        used_budget = 0\n        generation = 0\n\n        # Generate Sobol sequence for orthogonal sampling\n        sobol_seq = self.sobol_sequence(self.orthogonal_samples * self.popsize, self.dim)\n        sobol_seq = func.bounds.lb + (func.bounds.ub - func.bounds.lb) * sobol_seq\n        \n        while used_budget < self.budget:\n            # Orthogonal sampling using Sobol sequence\n            if generation == 0:\n                x = sobol_seq[:self.popsize]\n                used_budget += self.popsize\n            else:\n                z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n                C_sqrt = np.linalg.cholesky(self.C)\n                x = self.m[:, np.newaxis] + self.sigma * C_sqrt @ z\n                x = x.T  # Transpose to match the shape of sobol_seq\n\n                # Repair mechanism to ensure solutions are within bounds\n                x = np.clip(x, func.bounds.lb, func.bounds.ub)\n                used_budget += self.popsize\n                \n            f = np.array([func(x[i]) if used_budget <= self.budget else np.inf for i in range(self.popsize)])\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)].copy()\n                self.last_improvement = generation\n            \n            arindex = np.argsort(f)\n            xmu = x[arindex[:self.mu]]\n            \n            m_old = self.m.copy()\n            self.m = np.sum(xmu * self.weights[np.newaxis, :], axis=0)\n            \n            ps_new = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs)) * (self.m - m_old) / self.sigma\n            self.ps = ps_new\n            \n            pc_new = (1 - self.c_cov) * self.pc + np.sqrt(self.c_cov * (2 - self.c_cov)) * (self.m - m_old) / self.sigma\n            self.pc = pc_new\n            \n            norm_ps = np.linalg.norm(self.ps)\n            factor = self.c_cov * (1 / (np.min([1, (1000000 * norm_ps**4 / self.dim)])))\n            \n            # Rank-one update of C\n            self.C = (1 - self.c_cov - self.c_mu) * self.C + self.c_cov * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :])\n            \n            # Rank-mu update of C\n            for i in range(self.mu):\n                y_diff = (xmu[i] - m_old) / self.sigma\n                self.C += self.c_mu * self.weights[i] * (y_diff[:, np.newaxis] @ y_diff[np.newaxis, :])\n\n            # Eigenvalue clipping to ensure C remains positive definite and well-conditioned\n            try:\n                eigenvalues, eigenvectors = np.linalg.eigh(self.C)\n                eigenvalues = np.maximum(eigenvalues, self.eigenvalue_floor)  # Clip eigenvalues\n                self.C = eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)\n\n            # Adaptive sigma based on success rate\n            success_rate = np.mean(f < np.median(f))\n            if success_rate > 0.7:\n                self.sigma *= 1.2  # Increase step size\n            elif success_rate < 0.3:\n                self.sigma *= 0.8  # Decrease step size\n\n            # Adaptive sigma based on population diversity\n            pop_diversity = np.std(f)\n            if pop_diversity < self.stagnation_threshold:\n                  self.sigma *= 0.9  # Reduce step size if population is too uniform\n\n            self.sigma = min(self.sigma, 5.0) # Limit the maximum step size\n\n            self.sigma *= np.exp((self.cs / self.damp) * (norm_ps / self.chiN - 1))\n            \n            # Ensure C remains positive definite\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)  # Reset covariance matrix if it becomes non-positive definite\n\n            # Restart mechanism\n            if generation - self.last_improvement > self.max_no_improvement:\n                self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.sigma = 0.3  # Reset sigma to initial value\n                self.last_improvement = generation # reset last improvement\n\n            generation += 1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "An exception occurred: ufunc 'bitwise_xor' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''.", "error": "", "parent_ids": ["21fcc20f-9a65-4198-94df-3758f95634b5"], "operator": null, "metadata": {}}
{"id": "117eb17b-6a01-4572-9c1f-3e339ba531ff", "fitness": -Infinity, "name": "CMAES_Cauchy_Spectral_AdaptivePop", "description": "Enhanced CMA-ES with Cauchy mutation, spectral correction, adaptive population sizing, and a more robust stagnation detection and restart mechanism.", "code": "import numpy as np\n\nclass CMAES_Cauchy_Spectral_AdaptivePop:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damp=None, c_cov=None, c_mu=None, stagnation_threshold=50, cauchy_mutation_prob=0.1, spectral_floor=1e-12, initial_sigma=0.3):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize  # Let popsize be determined adaptively\n        self.mu = None  # mu will be set when popsize is known\n\n        self.cs = cs\n        self.damp = damp if damp is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1]) + self.cs\n        self.c_cov = c_cov if c_cov is not None else 2 / ((self.dim + np.sqrt(2))**2)\n        self.c_mu = c_mu if c_mu is not None else min(1 - self.c_cov, 2 * (self.weights[0] - 1 + self.weights[0]**2) / ((self.mu + 6 + 6*self.dim**0.5)))\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.last_f_opt = np.Inf\n        self.cauchy_mutation_prob = cauchy_mutation_prob\n        self.diversity_threshold = 1e-8\n        self.spectral_floor = spectral_floor\n        self.initial_sigma = initial_sigma\n        self.sigma = initial_sigma  # Initialize sigma\n\n        self.m = None\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.chiN = None\n        self.weights = None\n\n    def initialize(self):\n        # Adaptive popsize strategy (e.g., based on dimension)\n        self.popsize = 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.damp = 1 + 2 * np.max([0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1]) + self.cs\n        self.c_cov = 2 / ((self.dim + np.sqrt(2))**2)\n        self.c_mu = min(1 - self.c_cov, 2 * (self.weights[0] - 1 + self.weights[0]**2) / ((self.mu + 6 + 6*self.dim**0.5)))\n        self.chiN = self.dim**0.5 * (1 - 1 / (4 * self.dim) + 1 / (21 * self.dim**2))\n\n\n    def __call__(self, func):\n        self.initialize()\n        self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n\n        used_budget = 0\n        while used_budget < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = self.sqrtm(self.C)\n            x = self.m[:, np.newaxis] + self.sigma * C_sqrt @ z\n\n            # Cauchy mutation\n            for i in range(self.popsize):\n                if np.random.rand() < self.cauchy_mutation_prob:\n                    cauchy_noise = np.random.standard_cauchy(size=self.dim)\n                    x[:, i] += self.sigma * cauchy_noise\n\n            # Mirroring\n            xmir = self.m[:, np.newaxis] - self.sigma * C_sqrt @ z\n\n            # Repair mechanism\n            for i in range(self.popsize):\n                oob = np.logical_or(x[:, i] < func.bounds.lb, x[:, i] > func.bounds.ub)\n                if np.any(oob):\n                    x[oob, i] = np.random.uniform(func.bounds.lb[oob], func.bounds.ub[oob], size=np.sum(oob))\n\n                oob = np.logical_or(xmir[:, i] < func.bounds.lb, xmir[:, i] > func.bounds.ub)\n                if np.any(oob):\n                    xmir[oob, i] = np.random.uniform(func.bounds.lb[oob], func.bounds.ub[oob], size=np.sum(oob))\n\n            f = np.array([func(x[:, i]) if used_budget < self.budget else np.inf for i in range(self.popsize)])\n            fmir = np.array([func(xmir[:, i]) if used_budget + self.popsize < self.budget else np.inf for i in range(self.popsize)])\n\n            used_budget += 2 * self.popsize\n\n            if np.min(f) < np.min(fmir):\n                y = x\n                fitness = f\n                best_index = np.argmin(f)\n                if f[best_index] < self.f_opt:\n                    self.f_opt = f[best_index]\n                    self.x_opt = x[:, best_index].copy()\n            else:\n                y = xmir\n                fitness = fmir\n                best_index = np.argmin(fmir)\n                if fmir[best_index] < self.f_opt:\n                    self.f_opt = fmir[best_index]\n                    self.x_opt = xmir[:, best_index].copy()\n\n            #Stagnation detection (check for improvement)\n            if self.f_opt >= self.last_f_opt - 1e-12:  # Tolerate small numerical differences\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n\n            # Enhanced stagnation restart mechanism\n            if self.stagnation_counter > self.stagnation_threshold:\n                # Adaptive sigma reduction with a floor\n                self.sigma *= 0.7  # Reduce step size more aggressively\n                self.sigma = max(self.sigma, self.initial_sigma * 1e-3) # Ensure sigma doesn't get too small\n\n                # Reset evolution paths and covariance matrix to encourage exploration\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim) #also reset mean\n                self.stagnation_counter = 0\n\n            self.last_f_opt = self.f_opt\n\n            arindex = np.argsort(fitness)\n            xmu = y[:, arindex[:self.mu]]\n            zmu = z[:, arindex[:self.mu]]\n\n            m_old = self.m.copy()\n            self.m = np.sum(xmu * self.weights[np.newaxis, :], axis=1)\n\n            ps_new = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs)) * (C_sqrt @ np.sum(zmu * self.weights[np.newaxis, :], axis=1))\n            self.ps = ps_new\n\n            pc_new = (1 - self.c_cov) * self.pc + np.sqrt(self.c_cov * (2 - self.c_cov)) * (self.m - m_old) / self.sigma\n            self.pc = pc_new\n\n            norm_ps = np.linalg.norm(self.ps)\n            factor = self.c_cov * (1 / (np.min([1, (1000000 * norm_ps**4 / self.dim)])))\n\n            # Rank-one update\n            self.C = (1 - self.c_cov - self.c_mu) * self.C + self.c_cov * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :])\n\n            # Rank-mu update\n            for i in range(self.mu):\n                y_diff = (xmu[:, i] - m_old) / self.sigma\n                self.C += self.c_mu * self.weights[i] * (y_diff[:, np.newaxis] @ y_diff[np.newaxis, :])\n\n            self.C = self.spectral_clipping(self.C, self.spectral_floor) #Keep C positive definite\n\n            self.sigma *= np.exp((self.cs / self.damp) * (norm_ps / self.chiN - 1)) #Cumulation\n\n        return self.f_opt, self.x_opt\n\n    def spectral_clipping(self, C, floor):\n        try:\n            U, S, V = np.linalg.svd(C)\n            S = np.maximum(S, floor)\n            C = U @ np.diag(S) @ V\n        except np.linalg.LinAlgError:\n            C = np.eye(self.dim)\n        return C\n\n    def sqrtm(self, C):\n        try:\n            U, S, V = np.linalg.svd(C)\n            S = np.maximum(S, 0)\n            C_sqrt = U @ np.diag(np.sqrt(S)) @ V\n            return C_sqrt\n        except np.linalg.LinAlgError:\n            return np.eye(self.dim)", "configspace": "", "generation": 5, "feedback": "An exception occurred: unsupported operand type(s) for -: 'NoneType' and 'int'.", "error": "", "parent_ids": ["c3acf3e8-2e65-4b6e-852e-167139d2907d"], "operator": null, "metadata": {}}
{"id": "5063511c-b618-4f84-83be-ebd0a4c544f5", "fitness": 0.14473485620530063, "name": "CMAES_MirroredSampling", "description": "An adaptive CMA-ES variant that incorporates a mirrored sampling strategy to enhance exploration of the search space, coupled with a simplified rank-mu update and adaptive step-size control.", "code": "import numpy as np\n\nclass CMAES_MirroredSampling:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damp=None, c_mean=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.m = None\n        self.sigma = 0.3\n        self.ps = None\n        self.chiN = None\n        self.cs = cs\n        self.damp = damp if damp is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1]) + self.cs\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.c_mean = c_mean\n        self.generation = 0\n\n    def __call__(self, func):\n        self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1 / (4 * self.dim) + 1 / (21 * self.dim**2))\n\n        used_budget = 0\n\n        while used_budget < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize // 2))\n            x = self.m[:, np.newaxis] + self.sigma * z\n\n            # Create mirrored samples\n            x_mirrored = self.m[:, np.newaxis] - self.sigma * z\n\n            # Combine original and mirrored samples\n            x_combined = np.concatenate((x, x_mirrored), axis=1)\n\n            # Repair mechanism to ensure solutions are within bounds\n            x_combined = np.clip(x_combined, func.bounds.lb[:, np.newaxis], func.bounds.ub[:, np.newaxis])\n\n            f = np.array([func(x_combined[:, i]) if used_budget < self.budget else np.inf for i in range(self.popsize)])\n            used_budget += self.popsize\n\n            arindex = np.argsort(f)\n            xmu = x_combined[:, arindex[:self.mu]]\n\n            if f[arindex[0]] < self.f_opt:\n                self.f_opt = f[arindex[0]]\n                self.x_opt = x_combined[:, arindex[0]].copy()\n\n            # Mean update\n            m_old = self.m.copy()\n            self.m = (1 - self.c_mean) * self.m + self.c_mean * np.mean(xmu, axis=1) # Simplified mean update\n\n            # Step-size adaptation\n            zmu = (xmu - m_old[:, np.newaxis]) / self.sigma # Recalculate zmu based on the selected solutions\n\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs)) * np.mean(zmu, axis=1)\n            self.sigma *= np.exp((self.cs / self.damp) * (np.linalg.norm(self.ps) / self.chiN - 1))\n\n            self.generation += 1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm CMAES_MirroredSampling scored 0.145 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f67b95b6-4a84-45b3-bcef-74f2d906f2a5"], "operator": null, "metadata": {"aucs": [9.999999999998899e-05, 0.09660797284840572, 0.21174749851279284, 0.07935535277775763, 0.09417345790483012, 0.14050848581518627, 0.1676608420580833, 0.14487451623831804, 0.1374964237424834, 0.11243944177172671, 0.11737187151047823, 0.15726829455509195, 0.20553171867650866, 0.11347988385428032, 0.38968195158220365, 0.22685079428905697, 0.1475393493409911, 0.1431699694759806, 0.08761453712511702, 0.1212247620267195]}}
{"id": "d276a738-6b34-4f45-bd97-98bfaab32cf3", "fitness": 0.2726282809459498, "name": "CMAES_MirroredSampling", "description": "An adaptive CMA-ES variant using a mirrored sampling strategy and orthogonal initialization to enhance exploration and exploitation, adapting the step size based on success rates and population diversity.", "code": "import numpy as np\n\nclass CMAES_MirroredSampling:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damp=None, c_mean=0.1, ortho_init=True):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.m = None\n        self.sigma = 0.3\n        self.C = None  # covariance matrix (not explicitly used in full form for updates)\n        self.ps = None # evolution path for sigma\n        self.chiN = None\n        self.cs = cs\n        self.damp = damp if damp is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1]) + self.cs\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.c_mean = c_mean # learning rate for mean update\n        self.generation = 0\n        self.success_rate = 0.2\n        self.ortho_init = ortho_init\n\n    def orthogonal_initialization(self, func):\n        # Latin Hypercube Sampling for initial population\n        points = np.zeros((self.popsize, self.dim))\n        for i in range(self.dim):\n            points[:, i] = np.random.permutation(self.popsize)\n        points = (points + np.random.rand(self.popsize, self.dim)) / self.popsize\n        points = func.bounds.lb + points * (func.bounds.ub - func.bounds.lb)\n        return points\n\n    def __call__(self, func):\n        if self.ortho_init:\n            initial_pop = self.orthogonal_initialization(func)\n            self.m = np.mean(initial_pop, axis=0)\n        else:\n            self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        \n        self.ps = np.zeros(self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1 / (4 * self.dim) + 1 / (21 * self.dim**2))\n\n        used_budget = 0\n\n        while used_budget < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize // 2))\n            z = np.concatenate((z, -z), axis=1)  # Mirrored sampling\n            x = self.m[:, np.newaxis] + self.sigma * z\n\n            # Repair mechanism to ensure solutions are within bounds\n            x = np.clip(x, func.bounds.lb[:, np.newaxis], func.bounds.ub[:, np.newaxis])\n\n            f = np.array([func(x[:, i]) if used_budget < self.budget else np.inf for i in range(self.popsize)])\n            used_budget += self.popsize\n\n            arindex = np.argsort(f)\n            xmu = x[:, arindex[:self.mu]]\n            zmu = z[:, arindex[:self.mu]]\n\n            if f[arindex[0]] < self.f_opt:\n                self.f_opt = f[arindex[0]]\n                self.x_opt = x[:, arindex[0]].copy()\n\n            # Mean update\n            m_old = self.m.copy()\n            self.m = (1 - self.c_mean) * self.m + self.c_mean * np.sum(xmu * self.weights[np.newaxis, :], axis=1)\n\n            # Step-size adaptation\n            ps_old = self.ps\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs)) * np.sum(zmu * self.weights[np.newaxis, :], axis=1)\n\n            # Success rate adaptation\n            self.success_rate = (1 - 0.1) * self.success_rate + 0.1 * (np.linalg.norm(self.ps) < np.linalg.norm(ps_old)) # smoothed indicator\n            \n            # Adjust step size based on success rate\n            if self.success_rate > 0.3:\n                self.sigma *= np.exp(0.05 + self.cs / self.damp * (np.linalg.norm(self.ps) / self.chiN - 1))\n            elif self.success_rate < 0.1:\n                self.sigma *= np.exp(-0.25 + self.cs / self.damp * (np.linalg.norm(self.ps) / self.chiN - 1))\n            else:\n                 self.sigma *= np.exp(self.cs / self.damp * (np.linalg.norm(self.ps) / self.chiN - 1))\n\n            self.generation += 1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm CMAES_MirroredSampling scored 0.273 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f67b95b6-4a84-45b3-bcef-74f2d906f2a5"], "operator": null, "metadata": {"aucs": [0.048391697374316034, 0.1764555133617649, 0.6816208031114552, 0.17108435461854932, 0.18547954275779288, 0.19091315888523264, 0.2131761817908392, 0.1894663833716569, 0.17895670156481158, 0.19237319435235578, 0.6620181088297759, 0.18751038492000993, 0.2524594625931247, 0.18073679656358033, 0.5417322788649169, 0.32812978998132714, 0.22184953195659896, 0.2330478474222153, 0.19012235605272865, 0.4270415305459462]}}
{"id": "911ab917-2aa7-47cb-a043-66b9d04a7792", "fitness": 0.28221901730589005, "name": "CMAES_AdaptiveRestart_Improved", "description": "Implements an improved CMA-ES variant with adaptive population sizing, mirroring, a more robust covariance matrix adaptation using spectral regularization with eigenvalue clamping, a simplified adaptation rule for the covariance matrix, success-history adaptation of the step size, and stagnation detection with restarts based on both objective and parameter space stagnation.", "code": "import numpy as np\n\nclass CMAES_AdaptiveRestart_Improved:\n    def __init__(self, budget=10000, dim=10, popsize_multiplier=4, cs=0.3, damp=None, c_cov=None, c_mu=None, mirror_factor=1.0, stagnation_threshold_param=1e-6, stagnation_threshold_obj=1e-9, eigenvalue_floor=1e-12, success_history_length=5):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = int(popsize_multiplier + np.floor(3 * np.log(self.dim)))  # Dynamic popsize based on dimension\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.m = None\n        self.sigma = 0.3  # exploration parameter\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.chiN = None\n\n        self.cs = cs\n        self.damp = damp if damp is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1]) + self.cs\n        self.c_cov = c_cov if c_cov is not None else 2 / ((self.dim + np.sqrt(2))**2)\n        self.c_mu = c_mu if c_mu is not None else min(1 - self.c_cov, 2 * (self.weights[0] - 1 + self.weights[0]**2) / ((self.mu + 6 + 6*self.dim**0.5)))\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.mirror_factor = mirror_factor\n        self.stagnation_threshold_param = stagnation_threshold_param\n        self.stagnation_threshold_obj = stagnation_threshold_obj\n        self.last_improvement = 0\n        self.max_no_improvement = 100  # generations before restart\n        self.eigenvalue_floor = eigenvalue_floor  # Minimum eigenvalue for covariance matrix\n        self.success_history_length = success_history_length\n        self.success_history = np.zeros(self.success_history_length)\n        self.success_history_idx = 0\n        self.prev_m = None\n\n    def __call__(self, func):\n        self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1 / (4 * self.dim) + 1 / (21 * self.dim**2))\n        self.prev_m = self.m.copy()\n        \n        used_budget = 0\n        generation = 0\n\n        while used_budget < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.m[:, np.newaxis] + self.sigma * C_sqrt @ z\n            \n            # Mirroring to handle bounds - more aggressive mirroring\n            xmir = self.m[:, np.newaxis] - self.mirror_factor * self.sigma * C_sqrt @ z\n            \n            # Repair mechanism to ensure solutions are within bounds and resample if necessary\n            for i in range(self.popsize):\n                if not func.bounds.lb[0] <= x[0, i] <= func.bounds.ub[0]:\n                    x[:, i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                if not func.bounds.lb[0] <= xmir[0, i] <= func.bounds.ub[0]:\n                    xmir[:, i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                    \n            f = np.array([func(x[:, i]) if used_budget < self.budget else np.inf for i in range(self.popsize)])\n            fmir = np.array([func(xmir[:, i]) if used_budget + self.popsize < self.budget else np.inf for i in range(self.popsize)])\n            \n            used_budget += 2 * self.popsize\n            \n            if np.min(f) < np.min(fmir):\n                y = x\n                fitness = f\n                best_index = np.argmin(f)\n                if f[best_index] < self.f_opt:\n                    self.f_opt = f[best_index]\n                    self.x_opt = x[:, best_index].copy()\n                    self.last_improvement = generation\n            else:\n                y = xmir\n                fitness = fmir\n                best_index = np.argmin(fmir)\n                if fmir[best_index] < self.f_opt:\n                    self.f_opt = fmir[best_index]\n                    self.x_opt = xmir[:, best_index].copy()\n                    self.last_improvement = generation\n                    \n            \n            arindex = np.argsort(fitness)\n            xmu = y[:, arindex[:self.mu]]\n            zmu = z[:, arindex[:self.mu]]\n            \n            m_old = self.m.copy()\n            self.m = np.sum(xmu * self.weights[np.newaxis, :], axis=1)\n            \n            ps_new = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs)) * (C_sqrt @ np.sum(zmu * self.weights[np.newaxis, :], axis=1))\n            self.ps = ps_new\n            \n            pc_new = (1 - self.c_cov) * self.pc + np.sqrt(self.c_cov * (2 - self.c_cov)) * (self.m - m_old) / self.sigma\n            self.pc = pc_new\n            \n            norm_ps = np.linalg.norm(self.ps)\n            factor = self.c_cov * (1 / (np.min([1, (1000000 * norm_ps**4 / self.dim)])))\n            \n            # Rank-one update of C\n            self.C = (1 - self.c_cov - self.c_mu) * self.C + self.c_cov * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :])\n            \n            # Rank-mu update of C\n            for i in range(self.mu):\n                y_diff = (xmu[:, i] - m_old) / self.sigma\n                self.C += self.c_mu * self.weights[i] * (y_diff[:, np.newaxis] @ y_diff[np.newaxis, :])\n            \n            # Eigenvalue clipping to ensure C remains positive definite and well-conditioned\n            try:\n                eigenvalues, eigenvectors = np.linalg.eigh(self.C)\n                eigenvalues = np.maximum(eigenvalues, self.eigenvalue_floor)  # Clip eigenvalues\n                self.C = eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)\n\n            # Adaptive sigma based on success rate using success history\n            self.success_history[self.success_history_idx] = np.mean(fitness < np.median(fitness))\n            self.success_history_idx = (self.success_history_idx + 1) % self.success_history_length\n            success_rate = np.mean(self.success_history)\n\n            if success_rate > 0.7:\n                self.sigma *= 1.1  # Increase step size, reduced multiplier\n            elif success_rate < 0.3:\n                self.sigma *= 0.9  # Decrease step size, reduced multiplier\n\n\n            self.sigma = min(self.sigma, 5.0) # Limit the maximum step size\n\n            self.sigma *= np.exp((self.cs / self.damp) * (norm_ps / self.chiN - 1))\n            \n            # Ensure C remains positive definite\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)  # Reset covariance matrix if it becomes non-positive definite\n\n            # Stagnation detection based on parameter change and objective change\n            param_change = np.linalg.norm(self.m - self.prev_m)\n            obj_change = np.abs(self.f_opt - np.min(fitness))\n\n            if param_change < self.stagnation_threshold_param or obj_change < self.stagnation_threshold_obj:\n                 if generation - self.last_improvement > self.max_no_improvement/2: # Less strict restart trigger\n                    self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                    self.C = np.eye(self.dim)\n                    self.pc = np.zeros(self.dim)\n                    self.ps = np.zeros(self.dim)\n                    self.sigma = 0.3  # Reset sigma to initial value\n                    self.last_improvement = generation\n                    self.success_history = np.zeros(self.success_history_length) # Reset success history\n\n            # Restart mechanism\n            if generation - self.last_improvement > self.max_no_improvement:\n                self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.sigma = 0.3  # Reset sigma to initial value\n                self.last_improvement = generation # reset last improvement\n                self.success_history = np.zeros(self.success_history_length) # Reset success history\n                \n            self.prev_m = self.m.copy()\n            generation += 1\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm CMAES_AdaptiveRestart_Improved scored 0.282 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["21fcc20f-9a65-4198-94df-3758f95634b5"], "operator": null, "metadata": {"aucs": [0.045572020329310514, 0.22645561791700786, 0.3734307769751627, 0.37625062233908135, 0.07194051377152078, 0.26670607432640503, 0.22987911306594966, 0.19831298873264092, 0.19145772303766118, 0.10090557233200292, 0.27612272312777875, 0.608442264276849, 0.26277406516706925, 0.25539618751362014, 0.5741086884216045, 0.31778071110787665, 0.22309764772783824, 0.6588037031305132, 0.15344933179234432, 0.2334940010255645]}}
{"id": "16031911-567e-40fc-97e3-0778196eea30", "fitness": 0.28654575381299296, "name": "AdaptiveDifferentialEvolution", "description": "Adaptive Differential Evolution with improved mutation using a combination of orthogonal design and best-guided strategy, enhanced parameter adaptation with aging and population diversity maintenance through periodic re-initialization and distance-based scaling.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, popsize_init=None, F_init=0.5, CR_init=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.popsize_min = 10\n        self.popsize_max = 100\n        self.popsize = popsize_init if popsize_init is not None else 2 * self.dim\n        self.F = F_init\n        self.CR = CR_init\n        self.archive_factor = 2.6  # Archive size relative to population size\n        self.archive = []\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.success_F = []\n        self.success_CR = []\n        self.success_count = 0\n        self.min_successes = 5\n        self.adapt_frequency = 10 # How often to adapt parameters\n        self.pop_adapt_frequency = 50 # How often to adapt popsize\n        self.stagnation_threshold = 1e-8\n        self.success_rate_threshold = 0.2\n        self.diversity_threshold = 0.01\n        self.age_limit = 50\n        self.population_age = np.zeros(self.popsize)\n\n    def __call__(self, func):\n        bounds_lb = func.bounds.lb\n        bounds_ub = func.bounds.ub\n        self.population = np.random.uniform(bounds_lb, bounds_ub, size=(self.popsize, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        eval_count = self.popsize\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = self.population[np.argmin(fitness)].copy()\n        best_fitness_history = [self.f_opt]\n\n        generation = 0\n        while eval_count < self.budget:\n            for i in range(self.popsize):\n                # Mutation using orthogonal design and best-guided strategy\n                indices = np.random.choice(self.popsize, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n\n                # Orthogonal array based mutation combined with best solution\n                mutant = self.population[i] + self.F * (self.x_opt - self.population[i] + x_r1 - x_r2 + x_r3 - self.population[i]) # Enhanced exploration around the best\n\n                mutant = np.clip(mutant, bounds_lb, bounds_ub)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.CR\n                trial_vector = np.where(crossover_mask, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial_vector)\n                eval_count += 1\n\n                if f_trial < fitness[i]:\n                    self.success_F.append(self.F)\n                    self.success_CR.append(self.CR)\n                    self.success_count += 1\n                    self.archive.append(self.population[i].copy())\n                    if len(self.archive) > self.archive_factor * self.popsize:\n                        self.archive.pop(0)\n\n                    fitness[i] = f_trial\n                    self.population[i] = trial_vector.copy()\n                    self.population_age[i] = 0 # Reset age upon improvement\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial_vector.copy()\n                else:\n                    self.population_age[i] +=1 # Increase age if no improvement\n\n                if eval_count >= self.budget:\n                    break\n\n            generation += 1\n            best_fitness_history.append(self.f_opt)\n\n            # Adapt parameters\n            if generation % self.adapt_frequency == 0:\n                if self.success_count > self.min_successes:\n                    # Adapt F and CR using weighted Lehmer mean, bias F towards smaller values\n                    weights = np.arange(1, len(self.success_F) + 1)  # Assign higher weights to more recent successes\n                    weights = weights / np.sum(weights) # Normalize weights\n\n                    if self.success_F:\n                        self.F = np.sum(weights * (np.array(self.success_F)**2)) / np.sum(weights * np.array(self.success_F))\n                        self.F = np.clip(self.F, 0.1, 0.9) * 0.8 + 0.1 # Bias towards smaller F\n                    if self.success_CR:\n                        self.CR = np.sum(weights * np.array(self.success_CR)) / np.sum(weights)\n                        self.CR = np.clip(self.CR, 0.1, 0.9)\n                else:\n                    self.F = 0.5 # Default F\n                    self.CR = 0.7 # Default CR\n\n                self.success_F = []\n                self.success_CR = []\n                self.success_count = 0\n\n            # Adapt population size\n            if generation % self.pop_adapt_frequency == 0:\n                # Stagnation detection based on fitness history\n                if len(best_fitness_history) > 10:\n                    fitness_improvement = np.mean(np.abs(np.diff(best_fitness_history[-10:])))\n                    stagnated = fitness_improvement < self.stagnation_threshold\n                else:\n                    stagnated = False\n\n                success_rate = self.success_count / (self.popsize * self.pop_adapt_frequency) if generation > 0 else 0\n\n                # Population diversity control\n                distances = np.linalg.norm(self.population - self.x_opt, axis=1)\n                diversity = np.mean(distances)\n\n                if stagnated and success_rate < self.success_rate_threshold:\n                     self.popsize = max(self.popsize_min, int(self.popsize * 0.75)) # Reduce popsize more aggressively\n                elif not stagnated and success_rate > 0.3 and diversity > self.diversity_threshold:\n                    self.popsize = min(self.popsize_max, int(self.popsize * 1.2)) # Increase popsize more aggressively\n                elif diversity < self.diversity_threshold:\n                    # If diversity is too low, re-initialize some individuals\n                    num_reinit = int(self.popsize * 0.2)\n                    indices_to_reinit = np.random.choice(self.popsize, num_reinit, replace=False)\n                    self.population[indices_to_reinit] = np.random.uniform(bounds_lb, bounds_ub, size=(num_reinit, self.dim))\n                    fitness[indices_to_reinit] = [func(x) for x in self.population[indices_to_reinit]]\n                    self.population_age[indices_to_reinit] = 0 # Reset age for re-initialized individuals\n                    eval_count += num_reinit\n\n                # Aging mechanism: Re-initialize old individuals\n                old_indices = np.where(self.population_age >= self.age_limit)[0]\n                if len(old_indices) > 0:\n                    self.population[old_indices] = np.random.uniform(bounds_lb, bounds_ub, size=(len(old_indices), self.dim))\n                    fitness[old_indices] = [func(x) for x in self.population[old_indices]]\n                    self.population_age[old_indices] = 0  # Reset age\n                    eval_count += len(old_indices)\n                \n                # Resize population (crude approach)\n                if self.popsize != self.population.shape[0]:\n                    new_population = np.random.uniform(bounds_lb, bounds_ub, size=(self.popsize, self.dim))\n                    new_population_age = np.zeros(self.popsize)\n                    if self.popsize > self.population.shape[0]:\n                        new_population[:self.population.shape[0]] = self.population\n                        new_population_age[:self.population.shape[0]] = self.population_age\n                    else:\n                        new_population = self.population[:self.popsize]\n                        new_population_age = self.population_age[:self.popsize]\n\n                    self.population = new_population\n                    self.population_age = new_population_age\n                    fitness = np.array([func(x) for x in self.population])\n                    eval_count += abs(self.popsize - self.population.shape[0])\n                    if eval_count >= self.budget:\n                        break # Ensure we don't exceed budget\n\n            # Distance-based F scaling\n            distances = np.linalg.norm(self.population - self.x_opt, axis=1)\n            self.F = np.clip(np.mean(distances) / (bounds_ub[0] - bounds_lb[0]), 0.1, 0.9) # Dynamic F scaling\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.287 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["696027db-fd66-4062-8e29-d7ddd86f6cad"], "operator": null, "metadata": {"aucs": [0.18336663821571586, 0.20716424378587428, 0.2920264582045363, 0.1848030697102354, 0.21810683347162285, 0.2672627009700236, 0.25594597457427004, 0.32406068494717843, 0.24318531010089495, 0.173051218286694, 0.21935063562532842, 0.9962870949735493, 0.24880747449274554, 0.26275988422335994, 0.20074556388380838, 0.285023047656633, 0.2614139579987701, 0.224821274484428, 0.21776667709758324, 0.46496633355660666]}}
{"id": "88857313-6767-41b7-892d-4a3a886d8e49", "fitness": 0.2851194151391704, "name": "SOM_DE", "description": "An adaptive Differential Evolution algorithm that incorporates a self-organizing map (SOM) for neighborhood-based mutation and local search, enhancing exploration and exploitation based on the SOM's structure.", "code": "import numpy as np\n\nclass SOM_DE:\n    def __init__(self, budget=10000, dim=10, popsize_init=None, F_init=0.5, CR_init=0.7, som_grid_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.popsize_min = 10\n        self.popsize_max = 100\n        self.popsize = popsize_init if popsize_init is not None else 2 * self.dim\n        self.F = F_init\n        self.CR = CR_init\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.success_F = []\n        self.success_CR = []\n        self.success_count = 0\n        self.min_successes = 5\n        self.adapt_frequency = 10\n        self.pop_adapt_frequency = 50\n        self.stagnation_threshold = 1e-8\n        self.success_rate_threshold = 0.2\n        self.diversity_threshold = 0.01\n        self.som_grid_size = som_grid_size # SOM grid dimensions\n\n        # Initialize SOM\n        self.som_weights = np.random.uniform(-1, 1, size=(self.som_grid_size, self.som_grid_size, self.dim)) # SOM weights\n        self.learning_rate = 0.1\n        self.neighborhood_radius = self.som_grid_size // 2\n\n    def __call__(self, func):\n        bounds_lb = func.bounds.lb\n        bounds_ub = func.bounds.ub\n        self.population = np.random.uniform(bounds_lb, bounds_ub, size=(self.popsize, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        eval_count = self.popsize\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = self.population[np.argmin(fitness)].copy()\n        best_fitness_history = [self.f_opt]\n\n        generation = 0\n        while eval_count < self.budget:\n            for i in range(self.popsize):\n                # SOM-guided mutation\n                individual = self.population[i]\n                \n                # Find the best matching unit (BMU) in the SOM\n                bmu_index = self.find_bmu(individual)\n                bmu = self.som_weights[bmu_index[0], bmu_index[1]]\n\n                # Select a neighbor from the SOM based on neighborhood radius\n                neighbor_index = self.select_neighbor(bmu_index)\n                neighbor = self.som_weights[neighbor_index[0], neighbor_index[1]]\n\n                # Mutation using SOM neighbor\n                mutant = individual + self.F * (bmu - neighbor)\n                mutant = np.clip(mutant, bounds_lb, bounds_ub)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.CR\n                trial_vector = np.where(crossover_mask, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial_vector)\n                eval_count += 1\n\n                if f_trial < fitness[i]:\n                    self.success_F.append(self.F)\n                    self.success_CR.append(self.CR)\n                    self.success_count += 1\n\n                    fitness[i] = f_trial\n                    self.population[i] = trial_vector.copy()\n\n                    # Update SOM\n                    self.update_som(trial_vector, bmu_index)\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial_vector.copy()\n\n                if eval_count >= self.budget:\n                    break\n\n            generation += 1\n            best_fitness_history.append(self.f_opt)\n\n            # Adapt parameters\n            if generation % self.adapt_frequency == 0:\n                if self.success_count > self.min_successes:\n                    # Adapt F and CR using weighted Lehmer mean, bias F towards smaller values\n                    weights = np.arange(1, len(self.success_F) + 1)\n                    weights = weights / np.sum(weights)\n\n                    if self.success_F:\n                        self.F = np.sum(weights * (np.array(self.success_F)**2)) / np.sum(weights * np.array(self.success_F))\n                        self.F = np.clip(self.F, 0.1, 0.9) * 0.8 + 0.1\n                    if self.success_CR:\n                        self.CR = np.sum(weights * np.array(self.success_CR)) / np.sum(weights)\n                        self.CR = np.clip(self.CR, 0.1, 0.9)\n                else:\n                    self.F = 0.5\n                    self.CR = 0.7\n\n                self.success_F = []\n                self.success_CR = []\n                self.success_count = 0\n\n            # Adapt population size\n            if generation % self.pop_adapt_frequency == 0:\n                if len(best_fitness_history) > 10:\n                    fitness_improvement = np.mean(np.abs(np.diff(best_fitness_history[-10:])))\n                    stagnated = fitness_improvement < self.stagnation_threshold\n                else:\n                    stagnated = False\n\n                success_rate = self.success_count / (self.popsize * self.pop_adapt_frequency) if generation > 0 else 0\n\n                distances = np.linalg.norm(self.population - self.x_opt, axis=1)\n                diversity = np.mean(distances)\n\n                if stagnated and success_rate < self.success_rate_threshold:\n                     self.popsize = max(self.popsize_min, int(self.popsize * 0.75))\n                elif not stagnated and success_rate > 0.3 and diversity > self.diversity_threshold:\n                    self.popsize = min(self.popsize_max, int(self.popsize * 1.2))\n                elif diversity < self.diversity_threshold:\n                    num_reinit = int(self.popsize * 0.2)\n                    indices_to_reinit = np.random.choice(self.popsize, num_reinit, replace=False)\n                    self.population[indices_to_reinit] = np.random.uniform(bounds_lb, bounds_ub, size=(num_reinit, self.dim))\n                    fitness[indices_to_reinit] = [func(x) for x in self.population[indices_to_reinit]]\n                    eval_count += num_reinit\n                \n                if self.popsize != self.population.shape[0]:\n                    new_population = np.random.uniform(bounds_lb, bounds_ub, size=(self.popsize, self.dim))\n                    if self.popsize > self.population.shape[0]:\n                        new_population[:self.population.shape[0]] = self.population\n                    else:\n                        new_population = self.population[:self.popsize]\n\n                    self.population = new_population\n                    fitness = np.array([func(x) for x in self.population])\n                    eval_count += abs(self.popsize - self.population.shape[0])\n                    if eval_count >= self.budget:\n                        break\n        return self.f_opt, self.x_opt\n\n    def find_bmu(self, individual):\n        \"\"\"Find the best matching unit (BMU) in the SOM.\"\"\"\n        distances = np.linalg.norm(self.som_weights - individual, axis=2)\n        bmu_index = np.unravel_index(np.argmin(distances), distances.shape)\n        return bmu_index\n\n    def select_neighbor(self, bmu_index):\n        \"\"\"Select a neighbor from the SOM grid.\"\"\"\n        row, col = bmu_index\n        neighbors = []\n        for i in range(max(0, row - 1), min(self.som_grid_size, row + 2)):\n            for j in range(max(0, col - 1), min(self.som_grid_size, col + 2)):\n                if (i, j) != (row, col):\n                    neighbors.append((i, j))\n\n        if neighbors:\n            return neighbors[np.random.choice(len(neighbors))]\n        else:\n            # If no neighbors, return the BMU itself\n            return bmu_index\n\n    def update_som(self, individual, bmu_index):\n        \"\"\"Update the SOM weights based on the individual.\"\"\"\n        row, col = bmu_index\n        for i in range(max(0, row - self.neighborhood_radius), min(self.som_grid_size, row + self.neighborhood_radius + 1)):\n            for j in range(max(0, col - self.neighborhood_radius), min(self.som_grid_size, col + self.neighborhood_radius + 1)):\n                distance = np.sqrt((i - row)**2 + (j - col)**2)\n                if distance <= self.neighborhood_radius:\n                    influence = np.exp(-distance**2 / (2 * self.neighborhood_radius**2))\n                    self.som_weights[i, j] += self.learning_rate * influence * (individual - self.som_weights[i, j])", "configspace": "", "generation": 5, "feedback": "The algorithm SOM_DE scored 0.285 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["696027db-fd66-4062-8e29-d7ddd86f6cad"], "operator": null, "metadata": {"aucs": [0.025261927177203325, 0.18778756716240363, 0.2546585747098623, 0.17178208947259865, 0.14904411456075728, 0.17899461280082196, 0.2622033391324362, 0.1641533914922393, 0.25922232496150566, 0.1404397689306195, 0.2494498668702324, 0.9708768139622004, 0.2518923670717661, 0.19357073870424824, 0.5352274991971251, 0.4417610676640569, 0.3029911280713967, 0.3286267684371501, 0.14832123106962114, 0.4861231113351634]}}
{"id": "91fc5bf7-189e-4c51-a13b-7d751b2cda69", "fitness": -Infinity, "name": "AdaptiveDifferentialEvolutionWavelet", "description": "Enhanced Adaptive Differential Evolution with orthogonal design, success-history adaptation, population control based on stagnation and diversity, and wavelet mutation for finer adjustments.", "code": "import numpy as np\nimport pywt\n\nclass AdaptiveDifferentialEvolutionWavelet:\n    def __init__(self, budget=10000, dim=10, popsize_init=None, F_init=0.5, CR_init=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.popsize_min = 10\n        self.popsize_max = 100\n        self.popsize = popsize_init if popsize_init is not None else 2 * self.dim\n        self.F = F_init\n        self.CR = CR_init\n        self.archive_factor = 2.6  # Archive size relative to population size\n        self.archive = []\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.success_F = []\n        self.success_CR = []\n        self.success_count = 0\n        self.min_successes = 5\n        self.adapt_frequency = 10 # How often to adapt parameters\n        self.pop_adapt_frequency = 50 # How often to adapt popsize\n        self.stagnation_threshold = 1e-8\n        self.success_rate_threshold = 0.2\n        self.diversity_threshold = 0.01\n        self.wavelet = 'db1' # Daubechies wavelet\n        self.wavelet_level = 2 # Decomposition level for wavelet mutation\n\n    def wavelet_mutation(self, x, level):\n        coeffs = pywt.wavedec(x, self.wavelet, level=level)\n        detail_coeffs = coeffs[1:]  # Exclude approximation coefficients\n        for i in range(len(detail_coeffs)):\n            detail_coeffs[i] = np.random.normal(0, 0.1, size=detail_coeffs[i].shape) # Add noise to detail coefficients\n        \n        mutated_x = pywt.waverec([coeffs[0]] + detail_coeffs, self.wavelet)\n        return mutated_x\n\n    def __call__(self, func):\n        bounds_lb = func.bounds.lb\n        bounds_ub = func.bounds.ub\n        self.population = np.random.uniform(bounds_lb, bounds_ub, size=(self.popsize, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        eval_count = self.popsize\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = self.population[np.argmin(fitness)].copy()\n        best_fitness_history = [self.f_opt]\n\n        generation = 0\n        while eval_count < self.budget:\n            for i in range(self.popsize):\n                # Mutation using orthogonal design\n                indices = np.random.choice(self.popsize, 4, replace=False)\n                x_r1, x_r2, x_r3, x_r4 = self.population[indices]\n\n                # Orthogonal array based mutation\n                mutant = self.population[i] + self.F * (x_r1 - x_r2 + x_r3 - x_r4)\n                mutant = np.clip(mutant, bounds_lb, bounds_ub)\n\n                # Wavelet mutation - applied with a probability\n                if np.random.rand() < 0.1:\n                    mutant = self.wavelet_mutation(mutant, self.wavelet_level)\n                    mutant = np.clip(mutant, bounds_lb, bounds_ub)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.CR\n                trial_vector = np.where(crossover_mask, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial_vector)\n                eval_count += 1\n\n                if f_trial < fitness[i]:\n                    self.success_F.append(self.F)\n                    self.success_CR.append(self.CR)\n                    self.success_count += 1\n                    self.archive.append(self.population[i].copy())\n                    if len(self.archive) > self.archive_factor * self.popsize:\n                        self.archive.pop(0)\n\n                    fitness[i] = f_trial\n                    self.population[i] = trial_vector.copy()\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial_vector.copy()\n\n                if eval_count >= self.budget:\n                    break\n\n            generation += 1\n            best_fitness_history.append(self.f_opt)\n\n            # Adapt parameters\n            if generation % self.adapt_frequency == 0:\n                if self.success_count > self.min_successes:\n                    # Adapt F and CR using weighted Lehmer mean, bias F towards smaller values\n                    weights = np.arange(1, len(self.success_F) + 1)  # Assign higher weights to more recent successes\n                    weights = weights / np.sum(weights) # Normalize weights\n\n                    if self.success_F:\n                        self.F = np.sum(weights * (np.array(self.success_F)**2)) / np.sum(weights * np.array(self.success_F))\n                        self.F = np.clip(self.F, 0.1, 0.9) * 0.8 + 0.1 # Bias towards smaller F\n                    if self.success_CR:\n                        self.CR = np.sum(weights * np.array(self.success_CR)) / np.sum(weights)\n                        self.CR = np.clip(self.CR, 0.1, 0.9)\n                else:\n                    self.F = 0.5 # Default F\n                    self.CR = 0.7 # Default CR\n\n                self.success_F = []\n                self.success_CR = []\n                self.success_count = 0\n\n            # Adapt population size\n            if generation % self.pop_adapt_frequency == 0:\n                # Stagnation detection based on fitness history\n                if len(best_fitness_history) > 10:\n                    fitness_improvement = np.mean(np.abs(np.diff(best_fitness_history[-10:])))\n                    stagnated = fitness_improvement < self.stagnation_threshold\n                else:\n                    stagnated = False\n\n                success_rate = self.success_count / (self.popsize * self.pop_adapt_frequency) if generation > 0 else 0\n\n                # Population diversity control\n                distances = np.linalg.norm(self.population - self.x_opt, axis=1)\n                diversity = np.mean(distances)\n\n                if stagnated and success_rate < self.success_rate_threshold:\n                     self.popsize = max(self.popsize_min, int(self.popsize * 0.75)) # Reduce popsize more aggressively\n                elif not stagnated and success_rate > 0.3 and diversity > self.diversity_threshold:\n                    self.popsize = min(self.popsize_max, int(self.popsize * 1.2)) # Increase popsize more aggressively\n                elif diversity < self.diversity_threshold:\n                    # If diversity is too low, re-initialize some individuals\n                    num_reinit = int(self.popsize * 0.2)\n                    indices_to_reinit = np.random.choice(self.popsize, num_reinit, replace=False)\n                    self.population[indices_to_reinit] = np.random.uniform(bounds_lb, bounds_ub, size=(num_reinit, self.dim))\n                    fitness[indices_to_reinit] = [func(x) for x in self.population[indices_to_reinit]]\n                    eval_count += num_reinit\n\n                \n                # Resize population (crude approach)\n                if self.popsize != self.population.shape[0]:\n                    new_population = np.random.uniform(bounds_lb, bounds_ub, size=(self.popsize, self.dim))\n                    if self.popsize > self.population.shape[0]:\n                        new_population[:self.population.shape[0]] = self.population\n                    else:\n                        new_population = self.population[:self.popsize]\n\n                    self.population = new_population\n                    fitness = np.array([func(x) for x in self.population])\n                    eval_count += abs(self.popsize - self.population.shape[0])\n                    if eval_count >= self.budget:\n                        break # Ensure we don't exceed budget\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "An exception occurred: No module named 'pywt'.", "error": "", "parent_ids": ["696027db-fd66-4062-8e29-d7ddd86f6cad"], "operator": null, "metadata": {}}
{"id": "f0f5c4e4-d49b-4c38-b393-ace0514b7d6c", "fitness": 0.26948622992583915, "name": "CMAES_Cauchy_Spectral_BiObjective", "description": "Adaptive CMA-ES with Cauchy mutation, spectral correction, and a bi-objective fitness assignment using a weighted sum of function value and diversity measure to balance exploration and exploitation.", "code": "import numpy as np\n\nclass CMAES_Cauchy_Spectral_BiObjective:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damp=None, c_cov=None, c_mu=None, stagnation_threshold=100, cauchy_mutation_prob=0.1, spectral_floor=1e-12, diversity_weight=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.m = None\n        self.sigma = 0.3  # exploration parameter\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.chiN = None\n\n        self.cs = cs\n        self.damp = damp if damp is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1]) + self.cs\n        self.c_cov = c_cov if c_cov is not None else 2 / ((self.dim + np.sqrt(2))**2)\n        self.c_mu = c_mu if c_mu is not None else min(1 - self.c_cov, 2 * (self.weights[0] - 1 + self.weights[0]**2) / ((self.mu + 6 + 6*self.dim**0.5)))\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.last_f_opt = np.Inf\n        self.cauchy_mutation_prob = cauchy_mutation_prob\n        self.diversity_threshold = 1e-8 # threshold for considering diversity collapse\n        self.spectral_floor = spectral_floor # floor for eigenvalue of C\n        self.diversity_weight = diversity_weight # Weight for diversity in bi-objective fitness\n\n    def __call__(self, func):\n        self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1 / (4 * self.dim) + 1 / (21 * self.dim**2))\n        \n        used_budget = 0\n        while used_budget < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = self.sqrtm(self.C)  # Use custom sqrtm\n            x = self.m[:, np.newaxis] + self.sigma * C_sqrt @ z\n            \n            # Apply Cauchy mutation with a probability\n            for i in range(self.popsize):\n                if np.random.rand() < self.cauchy_mutation_prob:\n                    cauchy_noise = np.random.standard_cauchy(size=self.dim)\n                    x[:, i] += self.sigma * cauchy_noise  #Scale cauchy noise with sigma\n\n            # Mirroring to handle bounds\n            xmir = self.m[:, np.newaxis] - self.sigma * C_sqrt @ z\n            \n            # Repair mechanism to ensure solutions are within bounds\n            for i in range(self.popsize):\n                oob = np.logical_or(x[:, i] < func.bounds.lb, x[:, i] > func.bounds.ub)\n                if np.any(oob):\n                   x[oob, i] = np.random.uniform(func.bounds.lb[oob], func.bounds.ub[oob], size=np.sum(oob))\n\n                oob = np.logical_or(xmir[:, i] < func.bounds.lb, xmir[:, i] > func.bounds.ub)\n                if np.any(oob):\n                   xmir[oob, i] = np.random.uniform(func.bounds.lb[oob], func.bounds.ub[oob], size=np.sum(oob))\n                    \n            f = np.array([func(x[:, i]) if used_budget < self.budget else np.inf for i in range(self.popsize)])\n            fmir = np.array([func(xmir[:, i]) if used_budget + self.popsize < self.budget else np.inf for i in range(self.popsize)])\n            \n            used_budget += 2 * self.popsize\n            \n            if np.min(f) < np.min(fmir):\n                y = x\n                fitness = f\n                best_index = np.argmin(f)\n                if f[best_index] < self.f_opt:\n                    self.f_opt = f[best_index]\n                    self.x_opt = x[:, best_index].copy()\n            else:\n                y = xmir\n                fitness = fmir\n                best_index = np.argmin(fmir)\n                if fmir[best_index] < self.f_opt:\n                    self.f_opt = fmir[best_index]\n                    self.x_opt = xmir[:, best_index].copy()\n            \n            #Stagnation detection\n            if self.f_opt >= self.last_f_opt:\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n            \n            if self.stagnation_counter > self.stagnation_threshold:\n                #Restart mechanism: reset covariance matrix and evolution paths\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.sigma *= 0.8 #Reduce step size\n                self.stagnation_counter = 0\n\n            self.last_f_opt = self.f_opt\n\n            # Calculate diversity measure\n            diversity = self.calculate_diversity(y)\n\n            # Bi-objective fitness assignment\n            combined_fitness = fitness + self.diversity_weight * diversity\n\n            arindex = np.argsort(combined_fitness)\n            xmu = y[:, arindex[:self.mu]]\n            zmu = z[:, arindex[:self.mu]]\n            \n            m_old = self.m.copy()\n            self.m = np.sum(xmu * self.weights[np.newaxis, :], axis=1)\n            \n            ps_new = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs)) * (C_sqrt @ np.sum(zmu * self.weights[np.newaxis, :], axis=1))\n            self.ps = ps_new\n            \n            pc_new = (1 - self.c_cov) * self.pc + np.sqrt(self.c_cov * (2 - self.c_cov)) * (self.m - m_old) / self.sigma\n            self.pc = pc_new\n            \n            norm_ps = np.linalg.norm(self.ps)\n            factor = self.c_cov * (1 / (np.min([1, (1000000 * norm_ps**4 / self.dim)])))\n            \n            # Rank-one update of C\n            self.C = (1 - self.c_cov - self.c_mu) * self.C + self.c_cov * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :])\n            \n            # Rank-mu update of C\n            for i in range(self.mu):\n                y_diff = (xmu[:, i] - m_old) / self.sigma\n                self.C += self.c_mu * self.weights[i] * (y_diff[:, np.newaxis] @ y_diff[np.newaxis, :])\n\n            # Spectral correction to ensure C is positive definite and has minimum eigenvalue\n            self.C = self.spectral_clipping(self.C, self.spectral_floor)\n\n            # Adaptive step size adjustment based on fitness variance\n            fitness_variance = np.var(fitness)\n            if fitness_variance > 1e-3:\n                self.sigma *= np.exp(0.05 * (fitness_variance - 1e-3))\n            else:\n                self.sigma *= np.exp(-0.05 * (1e-3 - fitness_variance))\n            \n            self.sigma = max(self.sigma, 1e-8) # Ensure sigma doesn't go to zero\n\n            self.sigma *= np.exp((self.cs / self.damp) * (norm_ps / self.chiN - 1))\n\n            # Check for diversity collapse and reset covariance matrix if needed\n            if np.linalg.norm(self.C - np.diag(np.diag(self.C))) < self.diversity_threshold:\n                self.C = np.eye(self.dim)\n            \n            # Ensure C remains positive definite - Removed redundant check\n            \n        return self.f_opt, self.x_opt\n\n    def spectral_clipping(self, C, floor):\n        \"\"\"\n        Clips the eigenvalues of the covariance matrix C to be at least equal to floor.\n        Ensures C remains positive definite.\n        \"\"\"\n        try:\n            U, S, V = np.linalg.svd(C)\n            S = np.maximum(S, floor)\n            C = U @ np.diag(S) @ V\n        except np.linalg.LinAlgError:\n            C = np.eye(self.dim)  # Reset covariance matrix if SVD fails\n        return C\n\n    def sqrtm(self, C):\n        \"\"\"\n        Calculates the square root of a matrix C using eigenvalue decomposition.\n        Handles potential numerical instability by clipping eigenvalues.\n        \"\"\"\n        try:\n            U, S, V = np.linalg.svd(C)\n            S = np.maximum(S, 0)  # Ensure eigenvalues are non-negative\n            C_sqrt = U @ np.diag(np.sqrt(S)) @ V\n            return C_sqrt\n        except np.linalg.LinAlgError:\n            return np.eye(self.dim) #Return identity matrix if SVD fails\n\n    def calculate_diversity(self, population):\n        \"\"\"\n        Calculates a measure of diversity based on the average distance to the mean.\n        \"\"\"\n        mean = np.mean(population, axis=1, keepdims=True)\n        distances = np.linalg.norm(population - mean, axis=0)\n        diversity = np.mean(distances)\n        return diversity", "configspace": "", "generation": 5, "feedback": "The algorithm CMAES_Cauchy_Spectral_BiObjective scored 0.269 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c3acf3e8-2e65-4b6e-852e-167139d2907d"], "operator": null, "metadata": {"aucs": [0.08090244866724772, 0.15222140984428223, 0.5471638743043238, 0.23371703814014022, 0.21775072665180872, 0.22434842417220424, 0.19106347270642976, 0.19437926895617053, 0.2614493713713597, 0.153676760763636, 0.23285250010815406, 0.9555699296846386, 0.07130091419326456, 0.16627249350522488, 0.5946818535983858, 0.3239124555748061, 0.20671613198717653, 0.24822659544228443, 0.15805785995931598, 0.1754610688859295]}}
{"id": "1b04fe34-27cf-4e18-8830-2f8d1e19dc8e", "fitness": 0.0, "name": "CMAES_RankOne_AdaptivePop", "description": "An adaptive CMA-ES variant that incorporates a simplified rank-one covariance matrix adaptation with a time-varying learning rate for the mean and a population-size adaptation strategy based on the recent success rate.", "code": "import numpy as np\n\nclass CMAES_RankOne_AdaptivePop:\n    def __init__(self, budget=10000, dim=10, popsize=None, c_mean_init=0.5, sigma_init=0.3, adapt_popsize=True):\n        self.budget = budget\n        self.dim = dim\n        self.adapt_popsize = adapt_popsize\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.m = None\n        self.sigma = sigma_init\n        self.C = np.eye(self.dim)  # Rank-one approximation will avoid full matrix ops\n        self.p_c = np.zeros(self.dim) # Evolution path\n        self.c_mean = c_mean_init  # Initial learning rate, adapted over time\n        self.c_sigma = 0.1 # Learning rate for sigma\n        self.damp = 1\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.generation = 0\n        self.success_rate = 0.5  # Initialize success rate\n        self.min_popsize = 4\n        self.max_popsize = 4 * self.dim\n\n    def __call__(self, func):\n        self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n\n        used_budget = 0\n        \n        while used_budget < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            x = self.m[:, np.newaxis] + self.sigma * z\n\n            # Repair mechanism to ensure solutions are within bounds\n            x = np.clip(x, func.bounds.lb[:, np.newaxis], func.bounds.ub[:, np.newaxis])\n            \n            f = np.array([func(x[:, i]) if used_budget < self.budget else np.inf for i in range(self.popsize)])\n            used_budget += self.popsize\n\n            arindex = np.argsort(f)\n            x_best = x[:, arindex[0]]\n\n            if f[arindex[0]] < self.f_opt:\n                self.f_opt = f[arindex[0]]\n                self.x_opt = x_best.copy()\n\n            # Mean update with time-varying learning rate\n            y = x_best - self.m\n            self.p_c = (1 - self.c_sigma) * self.p_c + np.sqrt(self.c_sigma * (2 - self.c_sigma)) * y / self.sigma\n            self.m += self.c_mean * y\n\n            # Rank-one update of covariance matrix (implicit)\n            self.C = (1 - self.c_sigma) * self.C + self.c_sigma * np.outer(self.p_c, self.p_c) #Not really used.\n\n            # Adapt step size\n            self.sigma *= np.exp((self.c_sigma / self.damp) * (np.linalg.norm(self.p_c) - np.sqrt(self.dim)) / np.sqrt(self.dim))\n\n            # Success rate update\n            if f[arindex[0]] < np.min(f):  # Check if the best is truly improved\n                self.success_rate = 0.9 * self.success_rate + 0.1\n            else:\n                self.success_rate = 0.9 * self.success_rate\n\n            # Adapt population size based on success rate\n            if self.adapt_popsize:\n                if self.success_rate > 0.6 and self.popsize < self.max_popsize:\n                    self.popsize = min(self.popsize + 1, self.max_popsize)\n                    self.mu = self.popsize // 2\n                elif self.success_rate < 0.3 and self.popsize > self.min_popsize:\n                    self.popsize = max(self.popsize - 1, self.min_popsize)\n                    self.mu = self.popsize // 2\n            \n            #Adapt the learning rate:\n            self.c_mean = (self.generation+1)**(-0.7) #Learning rate decay\n\n            self.generation += 1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm CMAES_RankOne_AdaptivePop scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d276a738-6b34-4f45-bd97-98bfaab32cf3"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "d28a2188-b2f8-4c8d-9884-7480fa9db7f3", "fitness": -Infinity, "name": "ClusteringDE", "description": "An adaptive Differential Evolution algorithm utilizing a clustering-based mutation strategy to improve exploration and exploitation.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass ClusteringDE:\n    def __init__(self, budget=10000, dim=10, popsize_init=None, F_init=0.5, CR_init=0.7, n_clusters=5):\n        self.budget = budget\n        self.dim = dim\n        self.popsize_min = 10\n        self.popsize_max = 100\n        self.popsize = popsize_init if popsize_init is not None else 2 * self.dim\n        self.F = F_init\n        self.CR = CR_init\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.success_F = []\n        self.success_CR = []\n        self.success_count = 0\n        self.min_successes = 5\n        self.adapt_frequency = 10\n        self.pop_adapt_frequency = 50\n        self.stagnation_threshold = 1e-8\n        self.success_rate_threshold = 0.2\n        self.diversity_threshold = 0.01\n        self.n_clusters = n_clusters # Number of clusters for KMeans\n\n    def __call__(self, func):\n        bounds_lb = func.bounds.lb\n        bounds_ub = func.bounds.ub\n        self.population = np.random.uniform(bounds_lb, bounds_ub, size=(self.popsize, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        eval_count = self.popsize\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = self.population[np.argmin(fitness)].copy()\n        best_fitness_history = [self.f_opt]\n\n        generation = 0\n        while eval_count < self.budget:\n            # Cluster the population\n            kmeans = KMeans(n_clusters=self.n_clusters, random_state=0, n_init='auto')\n            clusters = kmeans.fit_predict(self.population)\n\n            for i in range(self.popsize):\n                # Clustering-based mutation\n                cluster_id = clusters[i]\n                cluster_indices = np.where(clusters == cluster_id)[0]\n\n                if len(cluster_indices) > 1:\n                    # Select two other individuals from the same cluster\n                    indices = np.random.choice(cluster_indices, 2, replace=False)\n                    x_r1 = self.population[indices[0]]\n                    x_r2 = self.population[indices[1]]\n                else:\n                    # If only one individual in the cluster, select two random individuals from the entire population\n                    indices = np.random.choice(self.popsize, 2, replace=False)\n                    x_r1 = self.population[indices[0]]\n                    x_r2 = self.population[indices[1]]\n                \n                # Mutation\n                mutant = self.population[i] + self.F * (x_r1 - x_r2)\n                mutant = np.clip(mutant, bounds_lb, bounds_ub)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.CR\n                trial_vector = np.where(crossover_mask, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial_vector)\n                eval_count += 1\n\n                if f_trial < fitness[i]:\n                    self.success_F.append(self.F)\n                    self.success_CR.append(self.CR)\n                    self.success_count += 1\n\n                    fitness[i] = f_trial\n                    self.population[i] = trial_vector.copy()\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial_vector.copy()\n\n                if eval_count >= self.budget:\n                    break\n\n            generation += 1\n            best_fitness_history.append(self.f_opt)\n\n            # Adapt parameters\n            if generation % self.adapt_frequency == 0:\n                if self.success_count > self.min_successes:\n                    # Adapt F and CR using weighted Lehmer mean, bias F towards smaller values\n                    weights = np.arange(1, len(self.success_F) + 1)\n                    weights = weights / np.sum(weights)\n\n                    if self.success_F:\n                        self.F = np.sum(weights * (np.array(self.success_F)**2)) / np.sum(weights * np.array(self.success_F))\n                        self.F = np.clip(self.F, 0.1, 0.9) * 0.8 + 0.1\n                    if self.success_CR:\n                        self.CR = np.sum(weights * np.array(self.success_CR)) / np.sum(weights)\n                        self.CR = np.clip(self.CR, 0.1, 0.9)\n                else:\n                    self.F = 0.5\n                    self.CR = 0.7\n\n                self.success_F = []\n                self.success_CR = []\n                self.success_count = 0\n\n            # Adapt population size\n            if generation % self.pop_adapt_frequency == 0:\n                if len(best_fitness_history) > 10:\n                    fitness_improvement = np.mean(np.abs(np.diff(best_fitness_history[-10:])))\n                    stagnated = fitness_improvement < self.stagnation_threshold\n                else:\n                    stagnated = False\n\n                success_rate = self.success_count / (self.popsize * self.pop_adapt_frequency) if generation > 0 else 0\n\n                distances = np.linalg.norm(self.population - self.x_opt, axis=1)\n                diversity = np.mean(distances)\n\n                if stagnated and success_rate < self.success_rate_threshold:\n                     self.popsize = max(self.popsize_min, int(self.popsize * 0.75))\n                elif not stagnated and success_rate > 0.3 and diversity > self.diversity_threshold:\n                    self.popsize = min(self.popsize_max, int(self.popsize * 1.2))\n                elif diversity < self.diversity_threshold:\n                    num_reinit = int(self.popsize * 0.2)\n                    indices_to_reinit = np.random.choice(self.popsize, num_reinit, replace=False)\n                    self.population[indices_to_reinit] = np.random.uniform(bounds_lb, bounds_ub, size=(num_reinit, self.dim))\n                    fitness[indices_to_reinit] = [func(x) for x in self.population[indices_to_reinit]]\n                    eval_count += num_reinit\n                \n                if self.popsize != self.population.shape[0]:\n                    new_population = np.random.uniform(bounds_lb, bounds_ub, size=(self.popsize, self.dim))\n                    if self.popsize > self.population.shape[0]:\n                        new_population[:self.population.shape[0]] = self.population\n                    else:\n                        new_population = self.population[:self.popsize]\n\n                    self.population = new_population\n                    fitness = np.array([func(x) for x in self.population])\n                    eval_count += abs(self.popsize - self.population.shape[0])\n                    if eval_count >= self.budget:\n                        break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "An exception occurred: name 'KMeans' is not defined.", "error": "", "parent_ids": ["88857313-6767-41b7-892d-4a3a886d8e49"], "operator": null, "metadata": {}}
{"id": "738c77a4-949b-498d-91f7-a7deb16e337d", "fitness": -Infinity, "name": "CMAES_RankOne_AdaptivePop", "description": "A CMA-ES variant that incorporates a mirrored sampling strategy, orthogonal initialization, and a simplified covariance matrix adaptation through rank-one updates while dynamically adjusting the population size based on the success rate and stagnation detection.", "code": "import numpy as np\n\nclass CMAES_RankOne_AdaptivePop:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damp=None, c_mean=0.1, c_rank_one=None, ortho_init=True):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.m = None\n        self.sigma = 0.3\n        self.C = np.eye(self.dim)  # covariance matrix\n        self.ps = None # evolution path for sigma\n        self.pc = None  # evolution path for covariance\n        self.chiN = None\n        self.cs = cs\n        self.damp = damp if damp is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1]) + self.cs\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.c_mean = c_mean # learning rate for mean update\n        self.c_rank_one = c_rank_one if c_rank_one is not None else 0.1  # Learning rate for rank-one update\n        self.generation = 0\n        self.success_rate = 0.2\n        self.ortho_init = ortho_init\n        self.stagnation_counter = 0\n        self.min_popsize = 4\n        self.max_popsize = 200\n\n    def orthogonal_initialization(self, func):\n        # Latin Hypercube Sampling for initial population\n        points = np.zeros((self.popsize, self.dim))\n        for i in range(self.dim):\n            points[:, i] = np.random.permutation(self.popsize)\n        points = (points + np.random.rand(self.popsize, self.dim)) / self.popsize\n        points = func.bounds.lb + points * (func.bounds.ub - func.bounds.lb)\n        return points\n\n    def __call__(self, func):\n        if self.ortho_init:\n            initial_pop = self.orthogonal_initialization(func)\n            self.m = np.mean(initial_pop, axis=0)\n        else:\n            self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        \n        self.ps = np.zeros(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1 / (4 * self.dim) + 1 / (21 * self.dim**2))\n\n        used_budget = 0\n        old_f_opt = np.Inf\n\n        while used_budget < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize // 2))\n            z = np.concatenate((z, -z), axis=1)  # Mirrored sampling\n\n            # Apply the Cholesky decomposition\n            try:\n                C_sqrt = np.linalg.cholesky(self.C)\n                x = self.m[:, np.newaxis] + self.sigma * C_sqrt @ z\n            except np.linalg.LinAlgError:\n                # If C is not positive definite, regularize it\n                self.C = self.C + 1e-6 * np.eye(self.dim)\n                C_sqrt = np.linalg.cholesky(self.C)\n                x = self.m[:, np.newaxis] + self.sigma * C_sqrt @ z\n                \n            # Repair mechanism to ensure solutions are within bounds\n            x = np.clip(x, func.bounds.lb[:, np.newaxis], func.bounds.ub[:, np.newaxis])\n\n            f = np.array([func(x[:, i]) if used_budget < self.budget else np.inf for i in range(self.popsize)])\n            used_budget += self.popsize\n\n            arindex = np.argsort(f)\n            xmu = x[:, arindex[:self.mu]]\n            zmu = z[:, arindex[:self.mu]]\n\n            if f[arindex[0]] < self.f_opt:\n                self.f_opt = f[arindex[0]]\n                self.x_opt = x[:, arindex[0]].copy()\n                self.stagnation_counter = 0\n            else:\n                self.stagnation_counter += 1\n                \n            # Mean update\n            m_old = self.m.copy()\n            self.m = (1 - self.c_mean) * self.m + self.c_mean * np.sum(xmu * self.weights[np.newaxis, :], axis=1)\n\n            # Step-size adaptation\n            ps_old = self.ps\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs)) * np.sum(zmu * self.weights[np.newaxis, :], axis=1)\n            self.pc = (1 - self.c_mean) * self.pc + np.sqrt(self.c_mean * (2 - self.c_mean)) * (self.m - m_old) / self.sigma\n\n            # Rank-one update of covariance matrix\n            self.C = (1 - self.c_rank_one) * self.C + self.c_rank_one * np.outer(self.pc, self.pc)\n\n            # Success rate adaptation\n            self.success_rate = (1 - 0.1) * self.success_rate + 0.1 * (np.linalg.norm(self.ps) < np.linalg.norm(ps_old)) # smoothed indicator\n            \n            # Adjust step size based on success rate\n            if self.success_rate > 0.3:\n                self.sigma *= np.exp(0.05 + self.cs / self.damp * (np.linalg.norm(self.ps) / self.chiN - 1))\n            elif self.success_rate < 0.1:\n                self.sigma *= np.exp(-0.25 + self.cs / self.damp * (np.linalg.norm(self.ps) / self.chiN - 1))\n            else:\n                 self.sigma *= np.exp(self.cs / self.damp * (np.linalg.norm(self.ps) / self.chiN - 1))\n                 \n            # Adjust population size based on stagnation\n            if self.stagnation_counter > 50:\n                self.popsize = max(self.min_popsize, self.popsize // 2)\n                self.stagnation_counter = 0\n            elif self.f_opt < old_f_opt:\n                self.popsize = min(self.max_popsize, self.popsize + 2)\n            \n            self.mu = self.popsize // 2\n            self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n            self.weights /= np.sum(self.weights)\n            \n            old_f_opt = self.f_opt\n\n            self.generation += 1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "An exception occurred: index 6 is out of bounds for axis 1 with size 6.", "error": "", "parent_ids": ["d276a738-6b34-4f45-bd97-98bfaab32cf3"], "operator": null, "metadata": {}}
{"id": "7a484ce4-27c9-4cac-bd9a-a6b2788f29a6", "fitness": -Infinity, "name": "CMAES_GP_Surrogate", "description": "An adaptive CMA-ES variant that incorporates a surrogate model using Gaussian Process Regression to accelerate the search process, coupled with a trust-region mechanism to balance exploration and exploitation.", "code": "import numpy as np\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel\nfrom scipy.stats import norm\n\nclass CMAES_GP_Surrogate:\n    def __init__(self, budget=10000, dim=10, popsize_multiplier=4, initial_sigma=0.3, trust_region_factor=0.1, gp_update_interval=20, exploration_weight=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = int(popsize_multiplier + np.floor(3 * np.log(self.dim)))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.m = None\n        self.sigma = initial_sigma\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.chiN = self.dim**0.5 * (1 - 1 / (4 * self.dim) + 1 / (21 * self.dim**2))\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.X_train = []\n        self.y_train = []\n        self.gp = GaussianProcessRegressor(kernel=ConstantKernel(1.0) * RBF(length_scale=1.0), n_restarts_optimizer=5)\n        self.trust_region_factor = trust_region_factor\n        self.gp_update_interval = gp_update_interval\n        self.exploration_weight = exploration_weight # Adjust this parameter to balance exploration and exploitation\n        self.generation = 0\n\n\n    def expected_improvement(self, x, gp):\n        \"\"\"\n        Compute the expected improvement of sampling at point x.\n        \"\"\"\n        mu, sigma = gp.predict(x.reshape(1, -1), return_std=True)\n        best = np.min(self.y_train)\n        if sigma == 0:\n            return 0\n        gamma = (best - mu) / sigma\n        ei = sigma * (gamma * norm.cdf(gamma) + norm.pdf(gamma))\n        return ei\n\n    def __call__(self, func):\n        self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n\n        used_budget = 0\n        while used_budget < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.m[:, np.newaxis] + self.sigma * C_sqrt @ z\n            \n            # Repair mechanism to ensure solutions are within bounds\n            x = np.clip(x, func.bounds.lb[:, np.newaxis], func.bounds.ub[:, np.newaxis])\n\n            # Evaluate points, using surrogate model to guide the search\n            fitness = np.zeros(self.popsize)\n            for i in range(self.popsize):\n                if self.generation > 0 and len(self.X_train) > self.dim:\n                    # Trust region enforcement\n                    if np.linalg.norm(x[:, i] - self.m) > self.trust_region_factor * self.sigma:\n                        x[:, i] = self.m + (x[:, i] - self.m) / np.linalg.norm(x[:, i] - self.m) * self.trust_region_factor * self.sigma # Clip to trust region\n                    ei = self.expected_improvement(x[:, i], self.gp)\n                    # Decide whether to evaluate using the true function or the surrogate\n                    if np.random.rand() < self.exploration_weight:\n                        f = func(x[:, i])\n                        used_budget += 1\n                    else:\n                        f, _ = self.gp.predict(x[:, i].reshape(1, -1), return_std=True) # Use GP prediction\n                        f = f[0]\n                else:\n                    f = func(x[:, i])\n                    used_budget += 1\n                fitness[i] = f\n\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = x[:, i].copy()\n\n            # Update training data for the Gaussian Process\n            for i in range(self.popsize):\n                if len(self.X_train) < min(10 * self.dim, self.budget // 10):\n                    self.X_train.append(x[:, i])\n                    self.y_train.append(fitness[i])\n                else:\n                    # Replace worst value in the GP model\n                    worst_index = np.argmax(self.y_train)\n                    if fitness[i] < self.y_train[worst_index]:\n                        self.X_train[worst_index] = x[:, i]\n                        self.y_train[worst_index] = fitness[i]\n\n\n            # Train Gaussian Process\n            if self.generation % self.gp_update_interval == 0 and len(self.X_train) > self.dim:\n                try:\n                    self.gp.fit(np.array(self.X_train), np.array(self.y_train))\n                except Exception as e:\n                    print(f\"GP fit failed: {e}\")\n\n\n            arindex = np.argsort(fitness)\n            xmu = x[:, arindex[:self.mu]]\n            zmu = z[:, arindex[:self.mu]]\n            \n            m_old = self.m.copy()\n            self.m = np.sum(xmu * self.weights[np.newaxis, :], axis=1)\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs)) * (np.linalg.solve(C_sqrt, (self.m - m_old)))\n            self.pc = (1 - self.cc) * self.pc + np.sqrt(self.cc * (2 - self.cc)) * (self.m - m_old)\n            self.C = (1 - self.c_cov1 - self.c_covmu) * self.C + self.c_cov1 * np.outer(self.pc, self.pc)\n            for i in range(self.mu):\n                y_diff = xmu[:, i] - m_old\n                self.C += self.c_covmu * self.weights[i] * np.outer(y_diff, y_diff)\n\n\n            eigenvalues, eigenvectors = np.linalg.eigh(self.C)\n            eigenvalues = np.maximum(eigenvalues, 1e-12)\n            self.C = eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T\n\n\n            self.sigma *= np.exp((np.linalg.norm(self.ps) / self.chiN - 1) * self.cs / self.damp)\n\n\n            self.generation += 1\n\n        return self.f_opt, self.x_opt\n\n    @property\n    def cc(self):\n        return (4 + self.mu / self.dim) / (self.dim + 4 + 2 * self.mu / self.dim)\n\n    @property\n    def cs(self):\n        return (self.mu + 2) / (self.dim + self.mu + 5)\n\n    @property\n    def c_cov1(self):\n        return 2 / ((self.dim + 1.3)**2 + self.mu)\n\n    @property\n    def c_covmu(self):\n        return 2 * (self.mu - 1 + 1 / self.mu) / ((self.dim + 2)**2 + self.mu)\n\n    @property\n    def damp(self):\n        return 0.3 + 2 * np.max([0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1]) + self.cs", "configspace": "", "generation": 6, "feedback": "An exception occurred: name 'GaussianProcessRegressor' is not defined.", "error": "", "parent_ids": ["911ab917-2aa7-47cb-a043-66b9d04a7792"], "operator": null, "metadata": {}}
{"id": "c634d9ca-fc86-436a-bc01-caf4739a2dce", "fitness": -Infinity, "name": "SOM_DE", "description": "Improved SOM-DE with adaptive SOM learning rate, enhanced neighborhood selection, and aging mechanism for SOM weights to promote exploration.", "code": "import numpy as np\n\nclass SOM_DE:\n    def __init__(self, budget=10000, dim=10, popsize_init=None, F_init=0.5, CR_init=0.7, som_grid_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.popsize_min = 10\n        self.popsize_max = 100\n        self.popsize = popsize_init if popsize_init is not None else 2 * self.dim\n        self.F = F_init\n        self.CR = CR_init\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.success_F = []\n        self.success_CR = []\n        self.success_count = 0\n        self.min_successes = 5\n        self.adapt_frequency = 10\n        self.pop_adapt_frequency = 50\n        self.stagnation_threshold = 1e-8\n        self.success_rate_threshold = 0.2\n        self.diversity_threshold = 0.01\n        self.som_grid_size = som_grid_size # SOM grid dimensions\n\n        # Initialize SOM\n        self.som_weights = np.random.uniform(-1, 1, size=(self.som_grid_size, self.som_grid_size, self.dim)) # SOM weights\n        self.learning_rate_init = 0.1\n        self.learning_rate = self.learning_rate_init\n        self.neighborhood_radius = self.som_grid_size // 2\n        self.som_age = np.zeros((self.som_grid_size, self.som_grid_size))\n\n    def __call__(self, func):\n        bounds_lb = func.bounds.lb\n        bounds_ub = func.bounds.ub\n        self.population = np.random.uniform(bounds_lb, bounds_ub, size=(self.popsize, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        eval_count = self.popsize\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = self.population[np.argmin(fitness)].copy()\n        best_fitness_history = [self.f_opt]\n\n        generation = 0\n        while eval_count < self.budget:\n            for i in range(self.popsize):\n                # SOM-guided mutation\n                individual = self.population[i]\n                \n                # Find the best matching unit (BMU) in the SOM\n                bmu_index = self.find_bmu(individual)\n                bmu = self.som_weights[bmu_index[0], bmu_index[1]]\n\n                # Select a neighbor from the SOM based on neighborhood radius, favoring less visited cells\n                neighbor_index = self.select_neighbor(bmu_index)\n                neighbor = self.som_weights[neighbor_index[0], neighbor_index[1]]\n\n                # Mutation using SOM neighbor\n                mutant = individual + self.F * (bmu - neighbor)\n                mutant = np.clip(mutant, bounds_lb, bounds_ub)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.CR\n                trial_vector = np.where(crossover_mask, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial_vector)\n                eval_count += 1\n\n                if f_trial < fitness[i]:\n                    self.success_F.append(self.F)\n                    self.success_CR.append(self.CR)\n                    self.success_count += 1\n\n                    fitness[i] = f_trial\n                    self.population[i] = trial_vector.copy()\n\n                    # Update SOM\n                    self.update_som(trial_vector, bmu_index)\n                    self.som_age[bmu_index[0], bmu_index[1]] = 0  # Reset age after update\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial_vector.copy()\n\n                if eval_count >= self.budget:\n                    break\n\n            generation += 1\n            best_fitness_history.append(self.f_opt)\n\n            # Adapt parameters\n            if generation % self.adapt_frequency == 0:\n                if self.success_count > self.min_successes:\n                    # Adapt F and CR using weighted Lehmer mean, bias F towards smaller values\n                    weights = np.arange(1, len(self.success_F) + 1)\n                    weights = weights / np.sum(weights)\n\n                    if self.success_F:\n                        self.F = np.sum(weights * (np.array(self.success_F)**2)) / np.sum(weights * np.array(self.success_F))\n                        self.F = np.clip(self.F, 0.1, 0.9) * 0.8 + 0.1\n                    if self.success_CR:\n                        self.CR = np.sum(weights * np.array(self.success_CR)) / np.sum(weights)\n                        self.CR = np.clip(self.CR, 0.1, 0.9)\n                else:\n                    self.F = 0.5\n                    self.CR = 0.7\n\n                self.success_F = []\n                self.success_CR = []\n                self.success_count = 0\n\n                # Adaptive learning rate for SOM\n                self.learning_rate = self.learning_rate_init * np.exp(-generation / (self.budget / 20))\n\n            # Adapt population size\n            if generation % self.pop_adapt_frequency == 0:\n                if len(best_fitness_history) > 10:\n                    fitness_improvement = np.mean(np.abs(np.diff(best_fitness_history[-10:])))\n                    stagnated = fitness_improvement < self.stagnation_threshold\n                else:\n                    stagnated = False\n\n                success_rate = self.success_count / (self.popsize * self.pop_adapt_frequency) if generation > 0 else 0\n\n                distances = np.linalg.norm(self.population - self.x_opt, axis=1)\n                diversity = np.mean(distances)\n\n                if stagnated and success_rate < self.success_rate_threshold:\n                     self.popsize = max(self.popsize_min, int(self.popsize * 0.75))\n                elif not stagnated and success_rate > 0.3 and diversity > self.diversity_threshold:\n                    self.popsize = min(self.popsize_max, int(self.popsize * 1.2))\n                elif diversity < self.diversity_threshold:\n                    num_reinit = int(self.popsize * 0.2)\n                    indices_to_reinit = np.random.choice(self.popsize, num_reinit, replace=False)\n                    self.population[indices_to_reinit] = np.random.uniform(bounds_lb, bounds_ub, size=(num_reinit, self.dim))\n                    fitness[indices_to_reinit] = [func(x) for x in self.population[indices_to_reinit]]\n                    eval_count += num_reinit\n                \n                if self.popsize != self.population.shape[0]:\n                    new_population = np.random.uniform(bounds_lb, bounds_ub, size=(self.popsize, self.dim))\n                    if self.popsize > self.population.shape[0]:\n                        new_population[:self.population.shape[0]] = self.population\n                    else:\n                        new_population = self.population[:self.popsize]\n\n                    self.population = new_population\n                    fitness = np.array([func(x) for x in self.population])\n                    eval_count += abs(self.popsize - self.population.shape[0])\n                    if eval_count >= self.budget:\n                        break\n\n            # Age the SOM units\n            self.som_age += 1\n\n        return self.f_opt, self.x_opt\n\n    def find_bmu(self, individual):\n        \"\"\"Find the best matching unit (BMU) in the SOM.\"\"\"\n        distances = np.linalg.norm(self.som_weights - individual, axis=2)\n        bmu_index = np.unravel_index(np.argmin(distances), distances.shape)\n        return bmu_index\n\n    def select_neighbor(self, bmu_index):\n        \"\"\"Select a neighbor from the SOM grid, favoring less visited cells (exploration).\"\"\"\n        row, col = bmu_index\n        neighbors = []\n        for i in range(max(0, row - 1), min(self.som_grid_size, row + 2)):\n            for j in range(max(0, col - 1), min(self.som_grid_size, col + 2)):\n                if (i, j) != (row, col):\n                    neighbors.append((i, j))\n\n        if neighbors:\n            # Weight selection towards less visited neighbors using SOM age\n            neighbor_ages = [self.som_age[n[0], n[1]] for n in neighbors]\n            probabilities = np.exp(np.array(neighbor_ages) / np.max(neighbor_ages)) # Softmax\n            probabilities /= np.sum(probabilities) # Normalize\n\n            return neighbors[np.random.choice(len(neighbors), p=probabilities)]\n        else:\n            # If no neighbors, return the BMU itself\n            return bmu_index\n\n    def update_som(self, individual, bmu_index):\n        \"\"\"Update the SOM weights based on the individual.\"\"\"\n        row, col = bmu_index\n        for i in range(max(0, row - self.neighborhood_radius), min(self.som_grid_size, row + self.neighborhood_radius + 1)):\n            for j in range(max(0, col - self.neighborhood_radius), min(self.som_grid_size, col + self.neighborhood_radius + 1)):\n                distance = np.sqrt((i - row)**2 + (j - col)**2)\n                if distance <= self.neighborhood_radius:\n                    influence = np.exp(-distance**2 / (2 * self.neighborhood_radius**2))\n                    self.som_weights[i, j] += self.learning_rate * influence * (individual - self.som_weights[i, j])", "configspace": "", "generation": 6, "feedback": "An exception occurred: probabilities contain NaN.", "error": "", "parent_ids": ["88857313-6767-41b7-892d-4a3a886d8e49"], "operator": null, "metadata": {}}
{"id": "b93ca634-13d8-4a09-bc09-abbb8794ca47", "fitness": -Infinity, "name": "CMAES_NelderMead", "description": "An adaptive CMA-ES variant incorporating a local search operator based on Nelder-Mead simplex method to refine promising solutions during the evolution.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass CMAES_NelderMead:\n    def __init__(self, budget=10000, dim=10, popsize_multiplier=4, cs=0.3, damp=None, c_cov=None, c_mu=None, mirror_factor=1.0, stagnation_threshold_param=1e-6, stagnation_threshold_obj=1e-9, eigenvalue_floor=1e-12, success_history_length=5, nelder_mead_iterations=5):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = int(popsize_multiplier + np.floor(3 * np.log(self.dim)))  # Dynamic popsize based on dimension\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.m = None\n        self.sigma = 0.3  # exploration parameter\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.chiN = None\n\n        self.cs = cs\n        self.damp = damp if damp is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1]) + self.cs\n        self.c_cov = c_cov if c_cov is not None else 2 / ((self.dim + np.sqrt(2))**2)\n        self.c_mu = c_mu if c_mu is not None else min(1 - self.c_cov, 2 * (self.weights[0] - 1 + self.weights[0]**2) / ((self.mu + 6 + 6*self.dim**0.5)))\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.mirror_factor = mirror_factor\n        self.stagnation_threshold_param = stagnation_threshold_param\n        self.stagnation_threshold_obj = stagnation_threshold_obj\n        self.last_improvement = 0\n        self.max_no_improvement = 100  # generations before restart\n        self.eigenvalue_floor = eigenvalue_floor  # Minimum eigenvalue for covariance matrix\n        self.success_history_length = success_history_length\n        self.success_history = np.zeros(self.success_history_length)\n        self.success_history_idx = 0\n        self.prev_m = None\n        self.nelder_mead_iterations = nelder_mead_iterations\n\n    def __call__(self, func):\n        self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1 / (4 * self.dim) + 1 / (21 * self.dim**2))\n        self.prev_m = self.m.copy()\n        \n        used_budget = 0\n        generation = 0\n\n        while used_budget < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.m[:, np.newaxis] + self.sigma * C_sqrt @ z\n            \n            # Mirroring to handle bounds - more aggressive mirroring\n            xmir = self.m[:, np.newaxis] - self.mirror_factor * self.sigma * C_sqrt @ z\n            \n            # Repair mechanism to ensure solutions are within bounds and resample if necessary\n            for i in range(self.popsize):\n                if not func.bounds.lb[0] <= x[0, i] <= func.bounds.ub[0]:\n                    x[:, i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                if not func.bounds.lb[0] <= xmir[0, i] <= func.bounds.ub[0]:\n                    xmir[:, i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                    \n            f = np.array([func(x[:, i]) if used_budget < self.budget else np.inf for i in range(self.popsize)])\n            fmir = np.array([func(xmir[:, i]) if used_budget + self.popsize < self.budget else np.inf for i in range(self.popsize)])\n            \n            used_budget += 2 * self.popsize\n            \n            if np.min(f) < np.min(fmir):\n                y = x\n                fitness = f\n                best_index = np.argmin(f)\n                if f[best_index] < self.f_opt:\n                    self.f_opt = f[best_index]\n                    self.x_opt = x[:, best_index].copy()\n                    self.last_improvement = generation\n            else:\n                y = xmir\n                fitness = fmir\n                best_index = np.argmin(fmir)\n                if fmir[best_index] < self.f_opt:\n                    self.f_opt = fmir[best_index]\n                    self.x_opt = xmir[:, best_index].copy()\n                    self.last_improvement = generation\n                    \n            # Apply Nelder-Mead to the best solution\n            nm_budget = min(self.nelder_mead_iterations, self.budget - used_budget)\n            if nm_budget > 0:\n                nm_result = minimize(func, self.x_opt, method='Nelder-Mead', options={'maxfev': nm_budget})\n                used_budget += nm_result.nfev\n                if nm_result.fun < self.f_opt:\n                     self.f_opt = nm_result.fun\n                     self.x_opt = nm_result.x.copy()\n                     self.last_improvement = generation\n\n            arindex = np.argsort(fitness)\n            xmu = y[:, arindex[:self.mu]]\n            zmu = z[:, arindex[:self.mu]]\n            \n            m_old = self.m.copy()\n            self.m = np.sum(xmu * self.weights[np.newaxis, :], axis=1)\n            \n            ps_new = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs)) * (C_sqrt @ np.sum(zmu * self.weights[np.newaxis, :], axis=1))\n            self.ps = ps_new\n            \n            pc_new = (1 - self.c_cov) * self.pc + np.sqrt(self.c_cov * (2 - self.c_cov)) * (self.m - m_old) / self.sigma\n            self.pc = pc_new\n            \n            norm_ps = np.linalg.norm(self.ps)\n            factor = self.c_cov * (1 / (np.min([1, (1000000 * norm_ps**4 / self.dim)])))\n            \n            # Rank-one update of C\n            self.C = (1 - self.c_cov - self.c_mu) * self.C + self.c_cov * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :])\n            \n            # Rank-mu update of C\n            for i in range(self.mu):\n                y_diff = (xmu[:, i] - m_old) / self.sigma\n                self.C += self.c_mu * self.weights[i] * (y_diff[:, np.newaxis] @ y_diff[np.newaxis, :])\n            \n            # Eigenvalue clipping to ensure C remains positive definite and well-conditioned\n            try:\n                eigenvalues, eigenvectors = np.linalg.eigh(self.C)\n                eigenvalues = np.maximum(eigenvalues, self.eigenvalue_floor)  # Clip eigenvalues\n                self.C = eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)\n\n            # Adaptive sigma based on success rate using success history\n            self.success_history[self.success_history_idx] = np.mean(fitness < np.median(fitness))\n            self.success_history_idx = (self.success_history_idx + 1) % self.success_history_length\n            success_rate = np.mean(self.success_history)\n\n            if success_rate > 0.7:\n                self.sigma *= 1.1  # Increase step size, reduced multiplier\n            elif success_rate < 0.3:\n                self.sigma *= 0.9  # Decrease step size, reduced multiplier\n\n\n            self.sigma = min(self.sigma, 5.0) # Limit the maximum step size\n\n            self.sigma *= np.exp((self.cs / self.damp) * (norm_ps / self.chiN - 1))\n            \n            # Ensure C remains positive definite\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)  # Reset covariance matrix if it becomes non-positive definite\n\n            # Stagnation detection based on parameter change and objective change\n            param_change = np.linalg.norm(self.m - self.prev_m)\n            obj_change = np.abs(self.f_opt - np.min(fitness))\n\n            if param_change < self.stagnation_threshold_param or obj_change < self.stagnation_threshold_obj:\n                 if generation - self.last_improvement > self.max_no_improvement/2: # Less strict restart trigger\n                    self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                    self.C = np.eye(self.dim)\n                    self.pc = np.zeros(self.dim)\n                    self.ps = np.zeros(self.dim)\n                    self.sigma = 0.3  # Reset sigma to initial value\n                    self.last_improvement = generation\n                    self.success_history = np.zeros(self.success_history_length) # Reset success history\n\n            # Restart mechanism\n            if generation - self.last_improvement > self.max_no_improvement:\n                self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.sigma = 0.3  # Reset sigma to initial value\n                self.last_improvement = generation # reset last improvement\n                self.success_history = np.zeros(self.success_history_length) # Reset success history\n                \n            self.prev_m = self.m.copy()\n            generation += 1\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "An exception occurred: name 'minimize' is not defined.", "error": "", "parent_ids": ["911ab917-2aa7-47cb-a043-66b9d04a7792"], "operator": null, "metadata": {}}
{"id": "f2b035d2-d28e-43d8-aa42-3543c1ec5cd9", "fitness": 0.17387425054214217, "name": "CMAES_RankOne_Mirrored", "description": "An adaptive CMA-ES variant using mirrored sampling and a simplified rank-one covariance matrix update to reduce computational complexity, combined with a trend-following step-size adaptation.", "code": "import numpy as np\n\nclass CMAES_RankOne_Mirrored:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damp=None, c_mean=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.m = None\n        self.sigma = 0.3\n        self.C = None\n        self.ps = None\n        self.chiN = None\n        self.cs = cs\n        self.damp = damp if damp is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1]) + self.cs\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.c_mean = c_mean\n        self.generation = 0\n        self.success_history = []\n        self.success_rate = 0.2\n        self.lr_sigma = 0.1 # Step size adaptation learning rate\n\n    def __call__(self, func):\n        self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.ps = np.zeros(self.dim)\n        self.C = np.eye(self.dim) # Identity matrix as initial covariance\n        self.chiN = self.dim**0.5 * (1 - 1 / (4 * self.dim) + 1 / (21 * self.dim**2))\n\n        used_budget = 0\n        \n        while used_budget < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize // 2))\n            z = np.concatenate((z, -z), axis=1)  # Mirrored sampling\n            \n            # Apply the covariance matrix\n            x = self.m[:, np.newaxis] + self.sigma * np.dot(np.linalg.cholesky(self.C), z)\n\n            # Repair mechanism\n            x = np.clip(x, func.bounds.lb[:, np.newaxis], func.bounds.ub[:, np.newaxis])\n            \n            f = np.array([func(x[:, i]) if used_budget < self.budget else np.inf for i in range(self.popsize)])\n            used_budget += self.popsize\n\n            arindex = np.argsort(f)\n            xmu = x[:, arindex[:self.mu]]\n            zmu = z[:, arindex[:self.mu]]\n\n            if f[arindex[0]] < self.f_opt:\n                self.f_opt = f[arindex[0]]\n                self.x_opt = x[:, arindex[0]].copy()\n\n            # Mean update\n            self.m = (1 - self.c_mean) * self.m + self.c_mean * np.sum(xmu * self.weights[np.newaxis, :], axis=1)\n\n            # Simplified rank-one update\n            zmu_w = np.sum(zmu * self.weights[np.newaxis, :], axis=1)\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs)) * zmu_w\n            self.C = (1 - self.c_mean) * self.C + self.c_mean * np.outer(self.ps, self.ps)\n\n            # Trend-following Step-size adaptation\n            self.success_history.append(f[arindex[0]])\n            if len(self.success_history) > 10:\n                self.success_history.pop(0)  # Keep only recent history\n                trend = self.success_history[-1] - self.success_history[0]\n                self.sigma *= np.exp(self.lr_sigma * trend / self.chiN)\n\n            self.sigma = np.clip(self.sigma, 1e-10, 10)\n            \n            self.generation += 1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm CMAES_RankOne_Mirrored scored 0.174 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d276a738-6b34-4f45-bd97-98bfaab32cf3"], "operator": null, "metadata": {"aucs": [0.027290509235307958, 0.027028661026982315, 0.252954897111593, 0.13239222008127816, 0.09638244404792784, 0.4917778592618832, 0.1665794421822493, 0.056761958051592565, 0.09681113828333088, 0.12539795307102664, 0.17159927842176148, 0.13045232558211917, 0.08216982323478572, 0.058989744133761235, 0.647499227013919, 0.3395383085645911, 0.13068216845965275, 0.17095745456845324, 0.11329581085301432, 0.1589237876576135]}}
{"id": "7bcb5016-e6b2-445d-8ff4-c03cfe2c96a5", "fitness": 0.5496137471309432, "name": "AdaptiveDynamicDE", "description": "An adaptive differential evolution algorithm employing a dynamic population size and a switching mutation strategy between DE/rand/1 and DE/best/1 based on the optimization phase.", "code": "import numpy as np\n\nclass AdaptiveDynamicDE:\n    def __init__(self, budget=10000, dim=10, pop_multiplier=10, F=0.5, Cr=0.9, switch_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = dim * pop_multiplier  # Initial population size\n        self.F = F  # Mutation factor\n        self.Cr = Cr  # Crossover rate\n        self.switch_prob = switch_prob # Probability of switching mutation strategy\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.phase = 0 # Optimization phase: 0 for exploration, 1 for exploitation\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.pop[best_idx].copy()\n\n    def mutate_rand(self, pop, i):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n        return a + self.F * (b - c)\n\n    def mutate_best(self, pop, fitness, i):\n        best_idx = np.argmin(fitness)\n        best = pop[best_idx]\n        idxs = np.random.choice(len(pop), 2, replace=False)\n        a, b = pop[idxs]\n        return best + self.F * (a - b)\n\n    def crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.Cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        generation = 0\n\n        while self.eval_count < self.budget:\n            # Dynamic population sizing: Increase if stagnation detected, decrease otherwise\n            if generation % 50 == 0:\n                if np.std(self.fitness) < 1e-6:  #Stagnation detected\n                    self.pop_size = min(self.pop_size * 2, 5 * self.dim)  # Increase population size\n                    new_pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size - len(self.pop), self.dim))\n                    new_fitness = np.array([func(x) for x in new_pop])\n                    self.eval_count += len(new_pop)\n                    self.pop = np.vstack((self.pop, new_pop))\n                    self.fitness = np.concatenate((self.fitness, new_fitness))\n\n                else:\n                     if self.pop_size > self.dim * 5:\n                        self.pop_size = max(self.pop_size // 2, self.dim * 5)\n                        best_indices = np.argsort(self.fitness)[:self.pop_size] # Keep only best individuals\n                        self.pop = self.pop[best_indices]\n                        self.fitness = self.fitness[best_indices]\n\n\n            for i in range(len(self.pop)):\n                target = self.pop[i]\n\n                # Adaptive mutation strategy switching\n                if np.random.rand() < self.switch_prob:\n                   mutant = self.mutate_rand(self.pop, i)\n                else:\n                    mutant = self.mutate_best(self.pop, self.fitness, i)\n                   \n\n                trial = self.crossover(target, mutant)\n\n                # Repair mechanism\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial.copy()\n\n                if self.eval_count >= self.budget:\n                    break\n\n            generation += 1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveDynamicDE scored 0.550 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["911ab917-2aa7-47cb-a043-66b9d04a7792"], "operator": null, "metadata": {"aucs": [0.21943185207787574, 0.3714319103455692, 0.8699531712370113, 0.9694631142357013, 0.9563221829619335, 0.950811902394722, 0.2592919579626144, 0.42714364145815176, 0.3050281913263033, 0.17950406897403848, 0.9726841486485651, 0.9959846497922348, 0.45016066076064465, 0.26573744733998794, 0.965687803258969, 0.46045220006217646, 0.34472126578829543, 0.30334543318230456, 0.21728063765105898, 0.5078387031607057]}}
{"id": "b6ff32f5-f85f-4aeb-878e-69998de8c5de", "fitness": 0.6163029029862482, "name": "DE_CMAES", "description": "Hybrid optimization strategy combining differential evolution with a covariance matrix adaptation evolution strategy (CMA-ES) inspired mutation, utilizing both global exploration and local refinement.", "code": "import numpy as np\n\nclass DE_CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, F=0.5, CR=0.7, cma_sigma=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 2 * self.dim\n        self.F = F\n        self.CR = CR\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.cma_sigma = cma_sigma  # Step size for CMA-ES-like mutation\n        self.cma_C = np.eye(dim)     # Covariance matrix for CMA-ES-like mutation\n        self.cma_D = np.ones(dim)    # Diagonal of the covariance matrix\n        self.cma_mu = self.popsize // 2 # Number of parents\n        self.cma_weights = np.log(self.cma_mu + 1/2) - np.log(np.arange(1, self.cma_mu + 1))\n        self.cma_weights /= np.sum(self.cma_weights)\n        self.cma_mueff = np.sum(self.cma_weights)**2 / np.sum(self.cma_weights**2)\n        self.cma_cs = (self.cma_mueff + 2) / (self.dim + self.cma_mueff + 5)\n        self.cma_damps = 1 + 2 * max(0, np.sqrt((self.cma_mueff - 1)/(self.dim + 1)) - 1) + self.cma_cs\n\n        self.pc = np.zeros(self.dim) # Evolution path for rank-one update of C\n        self.ps = np.zeros(self.dim)  # Evolution path for sigma control\n        self.chiN = np.sqrt(self.dim) * (1 - (1/(4 * self.dim)) + 1/(21 * self.dim**2))\n\n    def __call__(self, func):\n        bounds_lb = func.bounds.lb\n        bounds_ub = func.bounds.ub\n        self.population = np.random.uniform(bounds_lb, bounds_ub, size=(self.popsize, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        eval_count = self.popsize\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = self.population[np.argmin(fitness)].copy()\n\n        while eval_count < self.budget:\n            for i in range(self.popsize):\n                # DE mutation\n                idxs = np.random.choice(self.popsize, 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                mutant_de = self.population[i] + self.F * (x2 - x3)\n                mutant_de = np.clip(mutant_de, bounds_lb, bounds_ub)\n\n                # CMA-ES-like mutation\n                z = np.random.normal(0, 1, self.dim)\n                mutant_cmaes = self.population[i] + self.cma_sigma * self.cma_D * z\n\n                mutant_cmaes = np.clip(mutant_cmaes, bounds_lb, bounds_ub)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.CR\n                trial_vector = np.where(crossover_mask, mutant_de, mutant_cmaes)\n\n                # Selection\n                f_trial = func(trial_vector)\n                eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    self.population[i] = trial_vector.copy()\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial_vector.copy()\n            \n            #Adapt CMA-ES parameters\n            indices = np.argsort(fitness)\n            xmean = np.sum(self.population[indices[:self.cma_mu]].T * self.cma_weights, axis=1)\n\n            self.ps = (1 - self.cma_cs) * self.ps + np.sqrt(self.cma_cs * (2 - self.cma_cs) * self.cma_mueff) * (xmean - np.mean(self.population, axis=0)) / self.cma_sigma\n            self.cma_sigma *= np.exp((self.cma_cs / self.cma_damps) * (np.linalg.norm(self.ps)/ self.chiN -1))\n\n            if eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm DE_CMAES scored 0.616 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["88857313-6767-41b7-892d-4a3a886d8e49"], "operator": null, "metadata": {"aucs": [0.1839871889063075, 0.4652878568900999, 0.7083716229979775, 0.8379277863532263, 0.5878168470065152, 0.8471399681716492, 0.358631585882788, 0.6998074040105943, 0.6863566808924527, 0.23614088770528874, 0.8611031775304595, 0.9978942990948213, 0.5341162482997275, 0.5614529575612954, 0.8949700525155563, 0.8038845322692021, 0.4909351105623947, 0.8792976486115605, 0.19405513269459784, 0.49688107176845164]}}
{"id": "6f5f77b9-c0eb-44a2-a1b4-133850e6c1af", "fitness": 0.28600494907219554, "name": "CMAES_AdaptiveRestart_Improved", "description": "Implements an improved CMA-ES variant with adaptive population sizing, mirroring, a more robust covariance matrix adaptation using spectral regularization with eigenvalue clamping, a simplified adaptation rule for the covariance matrix, success-history adaptation of the step size, stagnation detection with restarts, and introduces a more aggressive step size adaptation based on the fitness landscape curvature.", "code": "import numpy as np\n\nclass CMAES_AdaptiveRestart_Improved:\n    def __init__(self, budget=10000, dim=10, popsize_multiplier=4, cs=0.3, damp=None, c_cov=None, c_mu=None, mirror_factor=1.0, stagnation_threshold_param=1e-6, stagnation_threshold_obj=1e-9, eigenvalue_floor=1e-12, success_history_length=5, curvature_adaptation_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = int(popsize_multiplier + np.floor(3 * np.log(self.dim)))  # Dynamic popsize based on dimension\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.m = None\n        self.sigma = 0.3  # exploration parameter\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.chiN = None\n\n        self.cs = cs\n        self.damp = damp if damp is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1]) + self.cs\n        self.c_cov = c_cov if c_cov is not None else 2 / ((self.dim + np.sqrt(2))**2)\n        self.c_mu = c_mu if c_mu is not None else min(1 - self.c_cov, 2 * (self.weights[0] - 1 + self.weights[0]**2) / ((self.mu + 6 + 6*self.dim**0.5)))\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.mirror_factor = mirror_factor\n        self.stagnation_threshold_param = stagnation_threshold_param\n        self.stagnation_threshold_obj = stagnation_threshold_obj\n        self.last_improvement = 0\n        self.max_no_improvement = 100  # generations before restart\n        self.eigenvalue_floor = eigenvalue_floor  # Minimum eigenvalue for covariance matrix\n        self.success_history_length = success_history_length\n        self.success_history = np.zeros(self.success_history_length)\n        self.success_history_idx = 0\n        self.prev_m = None\n        self.curvature_adaptation_rate = curvature_adaptation_rate\n        self.prev_fitness = np.inf\n\n    def __call__(self, func):\n        self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1 / (4 * self.dim) + 1 / (21 * self.dim**2))\n        self.prev_m = self.m.copy()\n        \n        used_budget = 0\n        generation = 0\n\n        while used_budget < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.m[:, np.newaxis] + self.sigma * C_sqrt @ z\n            \n            # Mirroring to handle bounds - more aggressive mirroring\n            xmir = self.m[:, np.newaxis] - self.mirror_factor * self.sigma * C_sqrt @ z\n            \n            # Repair mechanism to ensure solutions are within bounds and resample if necessary\n            for i in range(self.popsize):\n                if not func.bounds.lb[0] <= x[0, i] <= func.bounds.ub[0]:\n                    x[:, i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                if not func.bounds.lb[0] <= xmir[0, i] <= func.bounds.ub[0]:\n                    xmir[:, i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                    \n            f = np.array([func(x[:, i]) if used_budget < self.budget else np.inf for i in range(self.popsize)])\n            fmir = np.array([func(xmir[:, i]) if used_budget + self.popsize < self.budget else np.inf for i in range(self.popsize)])\n            \n            used_budget += 2 * self.popsize\n            \n            if np.min(f) < np.min(fmir):\n                y = x\n                fitness = f\n                best_index = np.argmin(f)\n                if f[best_index] < self.f_opt:\n                    self.f_opt = f[best_index]\n                    self.x_opt = x[:, best_index].copy()\n                    self.last_improvement = generation\n            else:\n                y = xmir\n                fitness = fmir\n                best_index = np.argmin(fmir)\n                if fmir[best_index] < self.f_opt:\n                    self.f_opt = fmir[best_index]\n                    self.x_opt = xmir[:, best_index].copy()\n                    self.last_improvement = generation\n                    \n            \n            arindex = np.argsort(fitness)\n            xmu = y[:, arindex[:self.mu]]\n            zmu = z[:, arindex[:self.mu]]\n            \n            m_old = self.m.copy()\n            self.m = np.sum(xmu * self.weights[np.newaxis, :], axis=1)\n            \n            ps_new = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs)) * (C_sqrt @ np.sum(zmu * self.weights[np.newaxis, :], axis=1))\n            self.ps = ps_new\n            \n            pc_new = (1 - self.c_cov) * self.pc + np.sqrt(self.c_cov * (2 - self.c_cov)) * (self.m - m_old) / self.sigma\n            self.pc = pc_new\n            \n            norm_ps = np.linalg.norm(self.ps)\n            factor = self.c_cov * (1 / (np.min([1, (1000000 * norm_ps**4 / self.dim)])))\n            \n            # Rank-one update of C\n            self.C = (1 - self.c_cov - self.c_mu) * self.C + self.c_cov * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :])\n            \n            # Rank-mu update of C\n            for i in range(self.mu):\n                y_diff = (xmu[:, i] - m_old) / self.sigma\n                self.C += self.c_mu * self.weights[i] * (y_diff[:, np.newaxis] @ y_diff[np.newaxis, :])\n            \n            # Eigenvalue clipping to ensure C remains positive definite and well-conditioned\n            try:\n                eigenvalues, eigenvectors = np.linalg.eigh(self.C)\n                eigenvalues = np.maximum(eigenvalues, self.eigenvalue_floor)  # Clip eigenvalues\n                self.C = eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)\n\n            # Adaptive sigma based on success rate using success history\n            self.success_history[self.success_history_idx] = np.mean(fitness < np.median(fitness))\n            self.success_history_idx = (self.success_history_idx + 1) % self.success_history_length\n            success_rate = np.mean(self.success_history)\n\n            # Curvature-based step size adaptation\n            fitness_improvement = self.prev_fitness - np.min(fitness)\n            if fitness_improvement > 0:\n                self.sigma *= (1 + self.curvature_adaptation_rate * fitness_improvement / (np.abs(self.prev_fitness) + 1e-9))  # Increase step size if improvement\n            else:\n                self.sigma *= (1 - self.curvature_adaptation_rate)  # Decrease step size if no improvement\n            self.prev_fitness = np.min(fitness)\n\n            if success_rate > 0.7:\n                self.sigma *= 1.1  # Increase step size, reduced multiplier\n            elif success_rate < 0.3:\n                self.sigma *= 0.9  # Decrease step size, reduced multiplier\n\n\n            self.sigma = min(self.sigma, 5.0) # Limit the maximum step size\n\n            self.sigma *= np.exp((self.cs / self.damp) * (norm_ps / self.chiN - 1))\n            \n            # Ensure C remains positive definite\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)  # Reset covariance matrix if it becomes non-positive definite\n\n            # Stagnation detection based on parameter change and objective change\n            param_change = np.linalg.norm(self.m - self.prev_m)\n            obj_change = np.abs(self.f_opt - np.min(fitness))\n\n            if param_change < self.stagnation_threshold_param or obj_change < self.stagnation_threshold_obj:\n                 if generation - self.last_improvement > self.max_no_improvement/2: # Less strict restart trigger\n                    self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                    self.C = np.eye(self.dim)\n                    self.pc = np.zeros(self.dim)\n                    self.ps = np.zeros(self.dim)\n                    self.sigma = 0.3  # Reset sigma to initial value\n                    self.last_improvement = generation\n                    self.success_history = np.zeros(self.success_history_length) # Reset success history\n                    self.prev_fitness = np.inf\n\n            # Restart mechanism\n            if generation - self.last_improvement > self.max_no_improvement:\n                self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.sigma = 0.3  # Reset sigma to initial value\n                self.last_improvement = generation # reset last improvement\n                self.success_history = np.zeros(self.success_history_length) # Reset success history\n                self.prev_fitness = np.inf\n                \n            self.prev_m = self.m.copy()\n            generation += 1\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm CMAES_AdaptiveRestart_Improved scored 0.286 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["911ab917-2aa7-47cb-a043-66b9d04a7792"], "operator": null, "metadata": {"aucs": [0.11382803159938171, 0.2031771053550534, 0.2772099448351415, 0.21632995776409725, 0.20895551675556023, 0.25526414109351503, 0.24467472973028648, 0.21184699238729787, 0.20774420146809713, 0.16012553751335368, 0.23220686975339255, 0.9974394819580333, 0.2460982596100273, 0.2043100538994559, 0.5488197686165214, 0.26755487720260007, 0.22307009215534057, 0.27628234060228585, 0.16202420193002576, 0.46313687721444285]}}
{"id": "5b72f23d-b077-4bc6-bcf1-d87cb473c451", "fitness": -Infinity, "name": "CMAES_AdaptiveRestart_Improved2", "description": "Enhanced CMA-ES with orthogonal initialization, dynamic population sizing, mirroring, spectral regularization, success-history adaptation, parameter-based step size control and adaptive restarts using objective, parameter space stagnation, and low-discrepancy sampling.", "code": "import numpy as np\nfrom scipy.stats import qmc\n\nclass CMAES_AdaptiveRestart_Improved2:\n    def __init__(self, budget=10000, dim=10, popsize_multiplier=4, cs=0.3, damp=None, c_cov=None, c_mu=None, mirror_factor=1.0, stagnation_threshold_param=1e-6, stagnation_threshold_obj=1e-9, eigenvalue_floor=1e-12, success_history_length=5, restart_trigger=50):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = int(popsize_multiplier + np.floor(3 * np.log(self.dim)))  # Dynamic popsize based on dimension\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.m = None\n        self.sigma = 0.3  # exploration parameter\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.chiN = None\n\n        self.cs = cs\n        self.damp = damp if damp is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1]) + self.cs\n        self.c_cov = c_cov if c_cov is not None else 2 / ((self.dim + np.sqrt(2))**2)\n        self.c_mu = c_mu if c_mu is not None else min(1 - self.c_cov, 2 * (self.weights[0] - 1 + self.weights[0]**2) / ((self.mu + 6 + 6*self.dim**0.5)))\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.mirror_factor = mirror_factor\n        self.stagnation_threshold_param = stagnation_threshold_param\n        self.stagnation_threshold_obj = stagnation_threshold_obj\n        self.last_improvement = 0\n        self.max_no_improvement = 100  # generations before restart\n        self.eigenvalue_floor = eigenvalue_floor  # Minimum eigenvalue for covariance matrix\n        self.success_history_length = success_history_length\n        self.success_history = np.zeros(self.success_history_length)\n        self.success_history_idx = 0\n        self.prev_m = None\n        self.orth_basis = None\n        self.restart_trigger = restart_trigger\n\n\n    def __call__(self, func):\n        # Orthogonal initialization\n        self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        # Generate orthogonal basis\n        self.orth_basis = np.linalg.qr(np.random.randn(self.dim, self.dim))[0]\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1 / (4 * self.dim) + 1 / (21 * self.dim**2))\n        self.prev_m = self.m.copy()\n        \n        used_budget = 0\n        generation = 0\n\n        while used_budget < self.budget:\n            # Low-discrepancy sampling using Halton sequence\n            sampler = qmc.Halton(d=self.dim, scramble=False)\n            z = sampler.random(n=self.popsize).T\n            z = np.random.normal(0,1, size = (self.dim, self.popsize))\n\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.m[:, np.newaxis] + self.sigma * C_sqrt @ z\n            \n            # Mirroring to handle bounds - more aggressive mirroring\n            xmir = self.m[:, np.newaxis] - self.mirror_factor * self.sigma * C_sqrt @ z\n            \n            # Repair mechanism to ensure solutions are within bounds and resample if necessary\n            for i in range(self.popsize):\n                if not np.all(func.bounds.lb <= x[:, i]) or not np.all(x[:, i] <= func.bounds.ub):\n                    x[:, i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                if not np.all(func.bounds.lb <= xmir[:, i]) or not np.all(xmir[:, i] <= func.bounds.ub):\n                    xmir[:, i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                    \n            f = np.array([func(x[:, i]) if used_budget < self.budget else np.inf for i in range(self.popsize)])\n            fmir = np.array([func(xmir[:, i]) if used_budget + self.popsize < self.budget else np.inf for i in range(self.popsize)])\n            \n            used_budget += 2 * self.popsize\n            \n            if np.min(f) < np.min(fmir):\n                y = x\n                fitness = f\n                best_index = np.argmin(f)\n                if f[best_index] < self.f_opt:\n                    self.f_opt = f[best_index]\n                    self.x_opt = x[:, best_index].copy()\n                    self.last_improvement = generation\n            else:\n                y = xmir\n                fitness = fmir\n                best_index = np.argmin(fmir)\n                if fmir[best_index] < self.f_opt:\n                    self.f_opt = fmir[best_index]\n                    self.x_opt = xmir[:, best_index].copy()\n                    self.last_improvement = generation\n                    \n            \n            arindex = np.argsort(fitness)\n            xmu = y[:, arindex[:self.mu]]\n            zmu = z[:, arindex[:self.mu]]\n            \n            m_old = self.m.copy()\n            self.m = np.sum(xmu * self.weights[np.newaxis, :], axis=1)\n            \n            ps_new = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs)) * (C_sqrt @ np.sum(zmu * self.weights[np.newaxis, :], axis=1))\n            self.ps = ps_new\n            \n            pc_new = (1 - self.c_cov) * self.pc + np.sqrt(self.c_cov * (2 - self.c_cov)) * (self.m - m_old) / self.sigma\n            self.pc = pc_new\n            \n            norm_ps = np.linalg.norm(self.ps)\n            factor = self.c_cov * (1 / (np.min([1, (1000000 * norm_ps**4 / self.dim)])))\n            \n            # Rank-one update of C\n            self.C = (1 - self.c_cov - self.c_mu) * self.C + self.c_cov * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :])\n            \n            # Rank-mu update of C\n            for i in range(self.mu):\n                y_diff = (xmu[:, i] - m_old) / self.sigma\n                self.C += self.c_mu * self.weights[i] * (y_diff[:, np.newaxis] @ y_diff[np.newaxis, :])\n            \n            # Eigenvalue clipping to ensure C remains positive definite and well-conditioned\n            try:\n                eigenvalues, eigenvectors = np.linalg.eigh(self.C)\n                eigenvalues = np.maximum(eigenvalues, self.eigenvalue_floor)  # Clip eigenvalues\n                self.C = eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)\n\n            # Adaptive sigma based on success rate using success history\n            self.success_history[self.success_history_idx] = np.mean(fitness < np.median(fitness))\n            self.success_history_idx = (self.success_history_idx + 1) % self.success_history_length\n            success_rate = np.mean(self.success_history)\n\n            if success_rate > 0.7:\n                self.sigma *= 1.1  # Increase step size, reduced multiplier\n            elif success_rate < 0.3:\n                self.sigma *= 0.9  # Decrease step size, reduced multiplier\n\n            # Parameter-based step size control\n            self.sigma *= np.exp((self.cs / self.damp) * (norm_ps / self.chiN - 1))\n            \n            self.sigma = min(self.sigma, 5.0) # Limit the maximum step size\n            \n            # Ensure C remains positive definite\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)  # Reset covariance matrix if it becomes non-positive definite\n\n            # Stagnation detection based on parameter change and objective change\n            param_change = np.linalg.norm(self.m - self.prev_m)\n            obj_change = np.abs(self.f_opt - np.min(fitness))\n\n            if param_change < self.stagnation_threshold_param or obj_change < self.stagnation_threshold_obj:\n                 if generation - self.last_improvement > self.restart_trigger: # Less strict restart trigger\n                    self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                    self.C = np.eye(self.dim)\n                    self.pc = np.zeros(self.dim)\n                    self.ps = np.zeros(self.dim)\n                    self.sigma = 0.3  # Reset sigma to initial value\n                    self.last_improvement = generation\n                    self.success_history = np.zeros(self.success_history_length) # Reset success history\n\n            # Restart mechanism\n            if generation - self.last_improvement > self.max_no_improvement:\n                self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.sigma = 0.3  # Reset sigma to initial value\n                self.last_improvement = generation # reset last improvement\n                self.success_history = np.zeros(self.success_history_length) # Reset success history\n                \n            self.prev_m = self.m.copy()\n            generation += 1\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "An exception occurred: name 'qmc' is not defined.", "error": "", "parent_ids": ["911ab917-2aa7-47cb-a043-66b9d04a7792"], "operator": null, "metadata": {}}
{"id": "2f45f6c3-11dc-4e8a-a871-74a7ff844731", "fitness": 0.394112464648363, "name": "CMAES_MirroredMomentum", "description": "Adaptive CMA-ES with mirrored sampling, orthogonal initialization, step-size adaptation, and a momentum-based mean update to accelerate convergence.", "code": "import numpy as np\n\nclass CMAES_MirroredMomentum:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damp=None, c_mean=0.1, ortho_init=True, momentum=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.m = None\n        self.sigma = 0.3\n        self.C = None  # covariance matrix (not explicitly used in full form for updates)\n        self.ps = None # evolution path for sigma\n        self.chiN = None\n        self.cs = cs\n        self.damp = damp if damp is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1]) + self.cs\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.c_mean = c_mean # learning rate for mean update\n        self.generation = 0\n        self.success_rate = 0.2\n        self.ortho_init = ortho_init\n        self.momentum = momentum\n        self.mean_velocity = np.zeros(self.dim)  # Initialize velocity for momentum\n\n    def orthogonal_initialization(self, func):\n        # Latin Hypercube Sampling for initial population\n        points = np.zeros((self.popsize, self.dim))\n        for i in range(self.dim):\n            points[:, i] = np.random.permutation(self.popsize)\n        points = (points + np.random.rand(self.popsize, self.dim)) / self.popsize\n        points = func.bounds.lb + points * (func.bounds.ub - func.bounds.lb)\n        return points\n\n    def __call__(self, func):\n        if self.ortho_init:\n            initial_pop = self.orthogonal_initialization(func)\n            self.m = np.mean(initial_pop, axis=0)\n        else:\n            self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        \n        self.ps = np.zeros(self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1 / (4 * self.dim) + 1 / (21 * self.dim**2))\n\n        used_budget = 0\n\n        while used_budget < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize // 2))\n            z = np.concatenate((z, -z), axis=1)  # Mirrored sampling\n            x = self.m[:, np.newaxis] + self.sigma * z\n\n            # Repair mechanism to ensure solutions are within bounds\n            x = np.clip(x, func.bounds.lb[:, np.newaxis], func.bounds.ub[:, np.newaxis])\n\n            f = np.array([func(x[:, i]) if used_budget < self.budget else np.inf for i in range(self.popsize)])\n            used_budget += self.popsize\n\n            arindex = np.argsort(f)\n            xmu = x[:, arindex[:self.mu]]\n            zmu = z[:, arindex[:self.mu]]\n\n            if f[arindex[0]] < self.f_opt:\n                self.f_opt = f[arindex[0]]\n                self.x_opt = x[:, arindex[0]].copy()\n\n            # Mean update with momentum\n            m_old = self.m.copy()\n            grad = np.sum(xmu * self.weights[np.newaxis, :], axis=1) - self.m #Calculate the gradient direction\n            self.mean_velocity = self.momentum * self.mean_velocity + self.c_mean * grad\n            self.m = self.m + self.mean_velocity\n\n            # Step-size adaptation\n            ps_old = self.ps\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs)) * np.sum(zmu * self.weights[np.newaxis, :], axis=1)\n\n            # Success rate adaptation\n            self.success_rate = (1 - 0.1) * self.success_rate + 0.1 * (np.linalg.norm(self.ps) < np.linalg.norm(ps_old)) # smoothed indicator\n            \n            # Adjust step size based on success rate\n            if self.success_rate > 0.3:\n                self.sigma *= np.exp(0.05 + self.cs / self.damp * (np.linalg.norm(self.ps) / self.chiN - 1))\n            elif self.success_rate < 0.1:\n                self.sigma *= np.exp(-0.25 + self.cs / self.damp * (np.linalg.norm(self.ps) / self.chiN - 1))\n            else:\n                 self.sigma *= np.exp(self.cs / self.damp * (np.linalg.norm(self.ps) / self.chiN - 1))\n\n            self.generation += 1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm CMAES_MirroredMomentum scored 0.394 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d276a738-6b34-4f45-bd97-98bfaab32cf3"], "operator": null, "metadata": {"aucs": [0.2866813843123288, 0.18513675493290405, 0.7299512446025981, 0.8783454685573923, 0.16639568518373238, 0.15426844306415333, 0.2401776179032853, 0.2320897571319167, 0.890960254360479, 0.15663847381032925, 0.7941134595701976, 0.9531701053990508, 0.24982661778591486, 0.1723509577597362, 0.5378706803004139, 0.3299627898840236, 0.3424393889495553, 0.2197489707859076, 0.20856574695450814, 0.1535554917188343]}}
{"id": "1a7bd804-43d2-4da0-85a1-8d0f62bd25ee", "fitness": -Infinity, "name": "PSO_DE", "description": "A population-based algorithm that combines the strengths of particle swarm optimization and differential evolution by using the swarm's best positions to guide differential evolution's mutation and crossover operators.", "code": "import numpy as np\n\nclass PSO_DE:\n    def __init__(self, budget=10000, dim=10, popsize=20, w=0.7, c1=1.5, c2=1.5, F=0.8, CR=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize\n        self.w = w  # Inertia weight\n        self.c1 = c1  # Cognitive coefficient\n        self.c2 = c2  # Social coefficient\n        self.F = F  # Differential evolution scaling factor\n        self.CR = CR  # Crossover rate\n\n        self.X = None  # Particle positions\n        self.V = None  # Particle velocities\n        self.pbest = None  # Personal best positions\n        self.pbest_fitness = None  # Personal best fitness values\n        self.gbest = None  # Global best position\n        self.gbest_fitness = np.inf  # Global best fitness value\n\n    def __call__(self, func):\n        # Initialize population\n        self.X = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.dim, self.popsize))\n        self.V = np.random.uniform(-1, 1, size=(self.dim, self.popsize))  # Initialize velocities randomly\n        self.pbest = self.X.copy()\n        self.pbest_fitness = np.array([func(self.X[:, i]) for i in range(self.popsize)])\n        self.gbest = self.pbest[:, np.argmin(self.pbest_fitness)].copy()\n        self.gbest_fitness = np.min(self.pbest_fitness)\n\n        used_budget = self.popsize  # Account for initial population evaluation\n\n        while used_budget < self.budget:\n            # Update velocities and positions\n            for i in range(self.popsize):\n                # PSO update\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.V[:, i] = self.w * self.V[:, i] + \\\n                                self.c1 * r1 * (self.pbest[:, i] - self.X[:, i]) + \\\n                                self.c2 * r2 * (self.gbest - self.X[:, i])\n                self.X[:, i] = self.X[:, i] + self.V[:, i]\n\n                # Ensure solutions are within bounds\n                self.X[:, i] = np.clip(self.X[:, i], func.bounds.lb, func.bounds.ub)\n\n                # Differential Evolution Mutation using gbest and pbest\n                idx = np.random.choice(self.popsize, 3, replace=False)\n                x1, x2, x3 = self.X[:, idx[0]], self.X[:, idx[1]], self.X[:, idx[2]]\n\n                v_trial = self.gbest + self.F * (x1 - x2) # Using gbest as base for mutation\n\n                # Crossover\n                jrand = np.random.randint(self.dim)\n                u = np.random.rand(self.dim)\n                mask = (u < self.CR) | (np.arange(self.dim) == jrand)\n                x_trial = np.where(mask, v_trial, self.X[:, i])\n\n                # Ensure solutions are within bounds\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n                \n\n                # Evaluate trial vector\n                f_trial = func(x_trial)\n                used_budget += 1\n                if used_budget >= self.budget:\n                    break\n\n                # Selection\n                if f_trial < self.pbest_fitness[i]:\n                    self.pbest[:, i] = x_trial\n                    self.pbest_fitness[i] = f_trial\n\n                    if f_trial < self.gbest_fitness:\n                        self.gbest = x_trial.copy()\n                        self.gbest_fitness = f_trial\n\n        return self.gbest_fitness, self.gbest", "configspace": "", "generation": 7, "feedback": "An exception occurred: shape mismatch: objects cannot be broadcast to a single shape.  Mismatch is between arg 0 with shape (2, 20) and arg 1 with shape (2,)..", "error": "", "parent_ids": ["6f5f77b9-c0eb-44a2-a1b4-133850e6c1af"], "operator": null, "metadata": {}}
{"id": "b8bd652b-25f0-4ac5-a1b7-0f3c79654f35", "fitness": -Infinity, "name": "DE_CMAES_Enhanced", "description": "Enhanced hybrid DE-CMAES with adaptive parameter control, orthogonal initialization, and a repair mechanism to maintain population diversity and accelerate convergence.", "code": "import numpy as np\n\nclass DE_CMAES_Enhanced:\n    def __init__(self, budget=10000, dim=10, popsize=None, F=0.5, CR=0.7, cma_sigma=0.1, adaptive_F_CR=True, orthogonal_init=True):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 2 * self.dim\n        self.F = F\n        self.CR = CR\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.cma_sigma = cma_sigma\n        self.cma_C = np.eye(dim)\n        self.cma_D = np.ones(dim)\n        self.cma_mu = self.popsize // 2\n        self.cma_weights = np.log(self.cma_mu + 1/2) - np.log(np.arange(1, self.cma_mu + 1))\n        self.cma_weights /= np.sum(self.cma_weights)\n        self.cma_mueff = np.sum(self.cma_weights)**2 / np.sum(self.cma_weights**2)\n        self.cma_cs = (self.cma_mueff + 2) / (self.dim + self.cma_mueff + 5)\n        self.cma_damps = 1 + 2 * max(0, np.sqrt((self.cma_mueff - 1)/(self.dim + 1)) - 1) + self.cma_cs\n\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1/(4 * self.dim)) + 1/(21 * self.dim**2))\n\n        self.adaptive_F_CR = adaptive_F_CR\n        self.memory_F = np.ones(self.popsize) * 0.5\n        self.memory_CR = np.ones(self.popsize) * 0.7\n        self.memory_pos = 0\n        self.S_CR = np.ones(self.popsize) * 0.7\n        self.orthogonal_init = orthogonal_init\n\n\n    def __call__(self, func):\n        bounds_lb = func.bounds.lb\n        bounds_ub = func.bounds.ub\n\n        if self.orthogonal_init:\n            # Orthogonal initialization\n            initial_population = self._latin_hypercube_sampling(self.popsize, self.dim, bounds_lb, bounds_ub)\n            self.population = initial_population\n        else:\n            self.population = np.random.uniform(bounds_lb, bounds_ub, size=(self.popsize, self.dim))\n\n        fitness = np.array([func(x) for x in self.population])\n        eval_count = self.popsize\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = self.population[np.argmin(fitness)].copy()\n\n        while eval_count < self.budget:\n            for i in range(self.popsize):\n                # Adaptive F and CR\n                if self.adaptive_F_CR:\n                    self.F = np.random.choice(self.memory_F)\n                    self.CR = np.random.choice(self.memory_CR)\n                \n                # DE mutation\n                idxs = np.random.choice(self.popsize, 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                mutant_de = self.population[i] + self.F * (x2 - x3)\n                mutant_de = self._repair(mutant_de, bounds_lb, bounds_ub)\n\n                # CMA-ES-like mutation\n                z = np.random.normal(0, 1, self.dim)\n                mutant_cmaes = self.population[i] + self.cma_sigma * self.cma_D * z\n                mutant_cmaes = self._repair(mutant_cmaes, bounds_lb, bounds_ub)\n                \n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.CR\n                trial_vector = np.where(crossover_mask, mutant_de, mutant_cmaes)\n                trial_vector = self._repair(trial_vector, bounds_lb, bounds_ub)\n\n                # Selection\n                f_trial = func(trial_vector)\n                eval_count += 1\n\n                if f_trial < fitness[i]:\n                    delta = np.abs(fitness[i] - f_trial)\n                    fitness[i] = f_trial\n                    self.population[i] = trial_vector.copy()\n\n                    if self.adaptive_F_CR:\n                        self.memory_F[self.memory_pos] = self.F\n                        self.memory_CR[self.memory_pos] = self.CR\n                        self.memory_pos = (self.memory_pos + 1) % self.popsize\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial_vector.copy()\n\n            #Adapt CMA-ES parameters\n            indices = np.argsort(fitness)\n            xmean = np.sum(self.population[indices[:self.cma_mu]].T * self.cma_weights, axis=1)\n\n            self.ps = (1 - self.cma_cs) * self.ps + np.sqrt(self.cma_cs * (2 - self.cma_cs) * self.cma_mueff) * (xmean - np.mean(self.population, axis=0)) / self.cma_sigma\n            self.cma_sigma *= np.exp((self.cma_cs / self.cma_damps) * (np.linalg.norm(self.ps)/ self.chiN -1))\n            if eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt\n\n    def _repair(self, x, lower_bound, upper_bound):\n        \"\"\"Repair solution to lie within the bounds.\"\"\"\n        return np.clip(x, lower_bound, upper_bound)\n\n    def _latin_hypercube_sampling(self, n_samples, dim, lower_bound, upper_bound):\n        \"\"\"Generate a Latin Hypercube Sample.\"\"\"\n        # Generate the intervals\n        intervals = np.linspace(lower_bound, upper_bound, n_samples + 1)\n        \n        # Generate the sample\n        sample = np.zeros((n_samples, dim))\n        for i in range(dim):\n            idxs = np.random.permutation(n_samples)\n            sample[:, i] = np.random.uniform(intervals[idxs], intervals[idxs + 1], size=n_samples)\n        return sample", "configspace": "", "generation": 7, "feedback": "An exception occurred: shape mismatch: objects cannot be broadcast to a single shape.  Mismatch is between arg 0 with shape (4,) and arg 1 with shape (4, 2)..", "error": "", "parent_ids": ["b6ff32f5-f85f-4aeb-878e-69998de8c5de"], "operator": null, "metadata": {}}
{"id": "8d0c562a-dc41-4df8-b001-018a1cd356ae", "fitness": -Infinity, "name": "EnsembleCMAES", "description": "A CMA-ES variant with ensemble-based mutation combining information from multiple covariance matrices to enhance exploration and exploitation.", "code": "import numpy as np\n\nclass EnsembleCMAES:\n    def __init__(self, budget=10000, dim=10, popsize_multiplier=4, num_ensembles=3, cs=0.3, damp=None, c_cov=None, c_mu=None, mirror_factor=0.5, stagnation_threshold_param=1e-6, stagnation_threshold_obj=1e-9, eigenvalue_floor=1e-12, success_history_length=5, restart_trigger=50):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = int(popsize_multiplier + np.floor(3 * np.log(self.dim)))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.m = None\n        self.sigma = 0.3\n        self.ensembles = []\n        self.num_ensembles = num_ensembles\n        for _ in range(self.num_ensembles):\n            self.ensembles.append({\n                'C': np.eye(self.dim),\n                'pc': np.zeros(self.dim),\n                'ps': np.zeros(self.dim),\n                'sigma': self.sigma\n            })\n        self.chiN = self.dim**0.5 * (1 - 1 / (4 * self.dim) + 1 / (21 * self.dim**2))\n\n        self.cs = cs\n        self.damp = damp if damp is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1]) + self.cs\n        self.c_cov = c_cov if c_cov is not None else 2 / ((self.dim + np.sqrt(2))**2)\n        self.c_mu = c_mu if c_mu is not None else min(1 - self.c_cov, 2 * (self.weights[0] - 1 + self.weights[0]**2) / ((self.mu + 6 + 6*self.dim**0.5)))\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.mirror_factor = mirror_factor\n        self.stagnation_threshold_param = stagnation_threshold_param\n        self.stagnation_threshold_obj = stagnation_threshold_obj\n        self.last_improvement = 0\n        self.restart_trigger = restart_trigger\n        self.eigenvalue_floor = eigenvalue_floor\n        self.success_history_length = success_history_length\n        self.success_history = np.zeros(self.success_history_length)\n        self.success_history_idx = 0\n\n    def __call__(self, func):\n        self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n\n        used_budget = 0\n        generation = 0\n\n        while used_budget < self.budget:\n            x = np.zeros((self.dim, self.popsize))\n            f = np.zeros(self.popsize)\n            xmir = np.zeros((self.dim, self.popsize))\n            fmir = np.zeros(self.popsize)\n\n            for i in range(self.popsize):\n                # Ensemble-based mutation: sample from each ensemble\n                ensemble_idx = i % self.num_ensembles\n                C = self.ensembles[ensemble_idx]['C']\n                sigma = self.ensembles[ensemble_idx]['sigma']\n                C_sqrt = np.linalg.cholesky(C)\n                z = np.random.normal(0, 1, size=self.dim)\n                x[:, i] = self.m + sigma * C_sqrt @ z\n                xmir[:, i] = self.m - self.mirror_factor * sigma * C_sqrt @ z\n\n                # Repair mechanism for bounds\n                if not func.bounds.lb[0] <= x[0, i] <= func.bounds.ub[0]:\n                    x[:, i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                if not func.bounds.lb[0] <= xmir[0, i] <= func.bounds.ub[0]:\n                    xmir[:, i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                    \n                f[i] = func(x[:, i]) if used_budget < self.budget else np.inf\n                fmir[i] = func(xmir[:, i]) if used_budget < self.budget else np.inf\n                used_budget += 2\n            \n            if np.min(f) < np.min(fmir):\n                y = x\n                fitness = f\n                best_index = np.argmin(f)\n                if f[best_index] < self.f_opt:\n                    self.f_opt = f[best_index]\n                    self.x_opt = x[:, best_index].copy()\n                    self.last_improvement = generation\n            else:\n                y = xmir\n                fitness = fmir\n                best_index = np.argmin(fmir)\n                if fmir[best_index] < self.f_opt:\n                    self.f_opt = fmir[best_index]\n                    self.x_opt = xmir[:, best_index].copy()\n                    self.last_improvement = generation\n\n            arindex = np.argsort(fitness)\n            xmu = y[:, arindex[:self.mu]]\n            \n            # Update each ensemble\n            for ensemble_idx in range(self.num_ensembles):\n                ensemble = self.ensembles[ensemble_idx]\n                C = ensemble['C']\n                pc = ensemble['pc']\n                ps = ensemble['ps']\n                sigma = ensemble['sigma']\n                \n                zmu = np.zeros((self.dim, self.mu))\n                for i in range(self.mu):\n                    z_idx = arindex[i]\n                    if z_idx % self.num_ensembles == ensemble_idx:\n                        C_sqrt = np.linalg.cholesky(C)\n                        zmu[:,i] = np.linalg.solve(C_sqrt, (xmu[:, i] - self.m) / sigma)\n                \n                m_old = self.m.copy()\n                self.m = np.sum(xmu * self.weights[np.newaxis, :], axis=1)\n\n                ps = (1 - self.cs) * ps + np.sqrt(self.cs * (2 - self.cs)) * (np.sum(zmu * self.weights[np.newaxis, :], axis=1))\n                ensemble['ps'] = ps\n\n                pc = (1 - self.c_cov) * pc + np.sqrt(self.c_cov * (2 - self.c_cov)) * (self.m - m_old) / sigma\n                ensemble['pc'] = pc\n\n                norm_ps = np.linalg.norm(ps)\n                factor = self.c_cov * (1 / (np.min([1, (1000000 * norm_ps**4 / self.dim)])))\n\n                C = (1 - self.c_cov - self.c_mu) * C + self.c_cov * (pc[:, np.newaxis] @ pc[np.newaxis, :])\n\n                for i in range(self.mu):\n                   y_diff = (xmu[:, i] - m_old) / sigma\n                   C += self.c_mu * self.weights[i] * (y_diff[:, np.newaxis] @ y_diff[np.newaxis, :])\n\n                try:\n                    eigenvalues, eigenvectors = np.linalg.eigh(C)\n                    eigenvalues = np.maximum(eigenvalues, self.eigenvalue_floor)\n                    C = eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T\n                except np.linalg.LinAlgError:\n                    C = np.eye(self.dim)\n\n                ensemble['C'] = C\n\n                sigma *= np.exp((self.cs / self.damp) * (norm_ps / self.chiN - 1))\n                ensemble['sigma'] = sigma\n            \n            # Stagnation and restart\n            if generation - self.last_improvement > self.restart_trigger:\n                self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                for ensemble in self.ensembles:\n                    ensemble['C'] = np.eye(self.dim)\n                    ensemble['pc'] = np.zeros(self.dim)\n                    ensemble['ps'] = np.zeros(self.dim)\n                    ensemble['sigma'] = 0.3\n                self.last_improvement = generation\n\n            generation += 1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "An exception occurred: Matrix is not positive definite.", "error": "", "parent_ids": ["6f5f77b9-c0eb-44a2-a1b4-133850e6c1af"], "operator": null, "metadata": {}}
{"id": "93954f71-a330-4efa-aa50-c2e82da6c0ea", "fitness": 0.20123666314619698, "name": "CMAES_MirroredMomentum_Improved", "description": "Improved CMA-ES with mirrored sampling, orthogonal initialization, step-size adaptation, a momentum-based mean update, and a simplified covariance matrix adaptation using a rank-one update strategy.", "code": "import numpy as np\n\nclass CMAES_MirroredMomentum_Improved:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damp=None, c_mean=0.1, ortho_init=True, momentum=0.1, c_rank_one=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.m = None\n        self.sigma = 0.3\n        self.C = np.eye(self.dim)  # Initialize covariance matrix\n        self.ps = None # evolution path for sigma\n        self.pc = None # evolution path for covariance matrix\n        self.chiN = None\n        self.cs = cs\n        self.damp = damp if damp is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1]) + self.cs\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.c_mean = c_mean # learning rate for mean update\n        self.generation = 0\n        self.success_rate = 0.2\n        self.ortho_init = ortho_init\n        self.momentum = momentum\n        self.mean_velocity = np.zeros(self.dim)  # Initialize velocity for momentum\n        self.c_rank_one = c_rank_one # learning rate for rank-one update of covariance matrix\n\n    def orthogonal_initialization(self, func):\n        # Latin Hypercube Sampling for initial population\n        points = np.zeros((self.popsize, self.dim))\n        for i in range(self.dim):\n            points[:, i] = np.random.permutation(self.popsize)\n        points = (points + np.random.rand(self.popsize, self.dim)) / self.popsize\n        points = func.bounds.lb + points * (func.bounds.ub - func.bounds.lb)\n        return points\n\n    def __call__(self, func):\n        if self.ortho_init:\n            initial_pop = self.orthogonal_initialization(func)\n            self.m = np.mean(initial_pop, axis=0)\n        else:\n            self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        \n        self.ps = np.zeros(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1 / (4 * self.dim) + 1 / (21 * self.dim**2))\n\n        used_budget = 0\n\n        while used_budget < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize // 2))\n            z = np.concatenate((z, -z), axis=1)  # Mirrored sampling\n            y = np.dot(np.linalg.cholesky(self.C), z) # Apply the (current) covariance matrix\n            x = self.m[:, np.newaxis] + self.sigma * y\n\n            # Repair mechanism to ensure solutions are within bounds\n            x = np.clip(x, func.bounds.lb[:, np.newaxis], func.bounds.ub[:, np.newaxis])\n\n            f = np.array([func(x[:, i]) if used_budget < self.budget else np.inf for i in range(self.popsize)])\n            used_budget += self.popsize\n\n            arindex = np.argsort(f)\n            xmu = x[:, arindex[:self.mu]]\n            ymu = y[:, arindex[:self.mu]]\n\n            if f[arindex[0]] < self.f_opt:\n                self.f_opt = f[arindex[0]]\n                self.x_opt = x[:, arindex[0]].copy()\n\n            # Mean update with momentum\n            m_old = self.m.copy()\n            grad = np.sum(xmu * self.weights[np.newaxis, :], axis=1) - self.m #Calculate the gradient direction\n            self.mean_velocity = self.momentum * self.mean_velocity + self.c_mean * grad\n            self.m = self.m + self.mean_velocity\n\n            # Step-size adaptation\n            ps_old = self.ps\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs)) * np.sum(ymu * self.weights[np.newaxis, :], axis=1)\n            \n            # Covariance matrix adaptation (rank-one update)\n            y_mean = np.sum(ymu * self.weights[np.newaxis, :], axis=1)\n            self.pc = (1 - self.cs) * self.pc + np.sqrt(self.c_rank_one * (2 - self.c_rank_one)) * y_mean\n            self.C = (1 - self.c_rank_one) * self.C + self.c_rank_one * np.outer(self.pc, self.pc)\n            \n            # Ensure that the covariance matrix stays positive definite\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)  # Reset to identity matrix if not positive definite\n\n            # Success rate adaptation\n            self.success_rate = (1 - 0.1) * self.success_rate + 0.1 * (np.linalg.norm(self.ps) < np.linalg.norm(ps_old)) # smoothed indicator\n            \n            # Adjust step size based on success rate\n            if self.success_rate > 0.3:\n                self.sigma *= np.exp(0.05 + self.cs / self.damp * (np.linalg.norm(self.ps) / self.chiN - 1))\n            elif self.success_rate < 0.1:\n                self.sigma *= np.exp(-0.25 + self.cs / self.damp * (np.linalg.norm(self.ps) / self.chiN - 1))\n            else:\n                 self.sigma *= np.exp(self.cs / self.damp * (np.linalg.norm(self.ps) / self.chiN - 1))\n\n            self.generation += 1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm CMAES_MirroredMomentum_Improved scored 0.201 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2f45f6c3-11dc-4e8a-a871-74a7ff844731"], "operator": null, "metadata": {"aucs": [0.05179593601198773, 0.17186745437216266, 0.2936725969561764, 0.1906487895519039, 0.16460352317622673, 0.1492199169287276, 0.20734304135386128, 0.2714768145173224, 0.2547526652188169, 0.1653090014684503, 0.20068602709762895, 0.1649018480969353, 0.2515723180657138, 0.17852533736351084, 0.3756925760186065, 0.24722878518832025, 0.23361316587893333, 0.19573128955850294, 0.11294118475743053, 0.14315099134272202]}}
{"id": "730a8a9c-80ea-4f87-a25e-7de0eb80cf82", "fitness": 0.42250864236689, "name": "CMAES_SimplifiedRankOne", "description": "CMA-ES with a Simplified Rank-One Update, mirrored sampling and a dynamic damping factor based on population diversity.", "code": "import numpy as np\n\nclass CMAES_SimplifiedRankOne:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damp=None, c_mean=0.1, ortho_init=True, diversity_factor=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.m = None\n        self.sigma = 0.3\n        self.C = np.eye(self.dim)  # covariance matrix (initialized as identity)\n        self.ps = None # evolution path for sigma\n        self.chiN = None\n        self.cs = cs\n        self.damp = damp if damp is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1]) + self.cs\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.c_mean = c_mean # learning rate for mean update\n        self.generation = 0\n        self.ortho_init = ortho_init\n        self.diversity_factor = diversity_factor # Factor influencing damping based on diversity\n        self.c_rank_one = 0.1 # Learning rate for rank-one update\n        self.mean_old = None\n\n\n    def orthogonal_initialization(self, func):\n        # Latin Hypercube Sampling for initial population\n        points = np.zeros((self.popsize, self.dim))\n        for i in range(self.dim):\n            points[:, i] = np.random.permutation(self.popsize)\n        points = (points + np.random.rand(self.popsize, self.dim)) / self.popsize\n        points = func.bounds.lb + points * (func.bounds.ub - func.bounds.lb)\n        return points\n\n    def __call__(self, func):\n        if self.ortho_init:\n            initial_pop = self.orthogonal_initialization(func)\n            self.m = np.mean(initial_pop, axis=0)\n        else:\n            self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        \n        self.ps = np.zeros(self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1 / (4 * self.dim) + 1 / (21 * self.dim**2))\n        self.mean_old = self.m.copy()\n\n\n        used_budget = 0\n\n        while used_budget < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize // 2))\n            z = np.concatenate((z, -z), axis=1)  # Mirrored sampling\n            x = self.m[:, np.newaxis] + self.sigma * np.dot(np.linalg.cholesky(self.C), z)\n\n            # Repair mechanism to ensure solutions are within bounds\n            x = np.clip(x, func.bounds.lb[:, np.newaxis], func.bounds.ub[:, np.newaxis])\n\n            f = np.array([func(x[:, i]) if used_budget < self.budget else np.inf for i in range(self.popsize)])\n            used_budget += self.popsize\n\n            arindex = np.argsort(f)\n            xmu = x[:, arindex[:self.mu]]\n            zmu = z[:, arindex[:self.mu]]\n\n            if f[arindex[0]] < self.f_opt:\n                self.f_opt = f[arindex[0]]\n                self.x_opt = x[:, arindex[0]].copy()\n\n            # Mean update\n            m_old = self.m.copy()\n            self.m = np.sum(xmu * self.weights[np.newaxis, :], axis=1)\n\n            # Simplified Rank-One Update\n            y = self.m - m_old\n            self.C = (1 - self.c_rank_one) * self.C + self.c_rank_one * np.outer(y, y) / (self.sigma**2)\n\n            # Step-size adaptation\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs)) * np.sum(zmu * self.weights[np.newaxis, :], axis=1)\n            self.sigma *= np.exp(self.cs / self.damp * (np.linalg.norm(self.ps) / self.chiN - 1))\n            \n            # Dynamic damping based on population diversity\n            diversity = np.std(xmu, axis=1).mean()  # Average standard deviation across dimensions\n            self.damp = 1 + 2 * np.max([0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1]) + self.cs + self.diversity_factor * diversity\n\n            self.generation += 1\n            self.mean_old = m_old\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm CMAES_SimplifiedRankOne scored 0.423 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2f45f6c3-11dc-4e8a-a871-74a7ff844731"], "operator": null, "metadata": {"aucs": [0.10823197757900505, 0.18116284708842367, 0.9698762935659128, 0.17800142078461612, 0.9653261798299818, 0.1527719523080041, 0.1840799293921821, 0.2501422219818489, 0.3160412532932819, 0.15477067929021127, 0.8746830857928993, 0.20270274817158596, 0.5967899778917416, 0.37149227421087305, 0.8979962875868193, 0.34742989194712415, 0.3870013030539827, 0.9705625317838218, 0.15024615563443844, 0.19086383615104652]}}
{"id": "aec739ab-89b6-401b-8c21-8992fd65ad75", "fitness": 0.28687104666344065, "name": "CMAES_AdaptiveRestart_Improved2", "description": "Improved CMA-ES with adaptive step size, covariance matrix adaptation, mirroring, and a new restart strategy based on population diversity.", "code": "import numpy as np\n\nclass CMAES_AdaptiveRestart_Improved2:\n    def __init__(self, budget=10000, dim=10, popsize_multiplier=4, cs=0.3, damp=None, c_cov=None, c_mu=None, mirror_factor=1.0, stagnation_threshold_param=1e-6, stagnation_threshold_obj=1e-9, eigenvalue_floor=1e-12, success_history_length=5, curvature_adaptation_rate=0.1, diversity_threshold=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = int(popsize_multiplier + np.floor(3 * np.log(self.dim)))  # Dynamic popsize based on dimension\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.m = None\n        self.sigma = 0.3  # exploration parameter\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.chiN = None\n\n        self.cs = cs\n        self.damp = damp if damp is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1]) + self.cs\n        self.c_cov = c_cov if c_cov is not None else 2 / ((self.dim + np.sqrt(2))**2)\n        self.c_mu = c_mu if c_mu is not None else min(1 - self.c_cov, 2 * (self.weights[0] - 1 + self.weights[0]**2) / ((self.mu + 6 + 6*self.dim**0.5)))\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.mirror_factor = mirror_factor\n        self.stagnation_threshold_param = stagnation_threshold_param\n        self.stagnation_threshold_obj = stagnation_threshold_obj\n        self.last_improvement = 0\n        self.max_no_improvement = 100  # generations before restart\n        self.eigenvalue_floor = eigenvalue_floor  # Minimum eigenvalue for covariance matrix\n        self.success_history_length = success_history_length\n        self.success_history = np.zeros(self.success_history_length)\n        self.success_history_idx = 0\n        self.prev_m = None\n        self.curvature_adaptation_rate = curvature_adaptation_rate\n        self.prev_fitness = np.inf\n        self.diversity_threshold = diversity_threshold\n\n    def __call__(self, func):\n        self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1 / (4 * self.dim) + 1 / (21 * self.dim**2))\n        self.prev_m = self.m.copy()\n        \n        used_budget = 0\n        generation = 0\n\n        while used_budget < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.m[:, np.newaxis] + self.sigma * C_sqrt @ z\n            \n            # Mirroring to handle bounds - more aggressive mirroring\n            xmir = self.m[:, np.newaxis] - self.mirror_factor * self.sigma * C_sqrt @ z\n            \n            # Repair mechanism to ensure solutions are within bounds and resample if necessary\n            for i in range(self.popsize):\n                if not func.bounds.lb[0] <= x[0, i] <= func.bounds.ub[0]:\n                    x[:, i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                if not func.bounds.lb[0] <= xmir[0, i] <= func.bounds.ub[0]:\n                    xmir[:, i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                    \n            f = np.array([func(x[:, i]) if used_budget < self.budget else np.inf for i in range(self.popsize)])\n            fmir = np.array([func(xmir[:, i]) if used_budget + self.popsize < self.budget else np.inf for i in range(self.popsize)])\n            \n            used_budget += 2 * self.popsize\n            \n            if np.min(f) < np.min(fmir):\n                y = x\n                fitness = f\n                best_index = np.argmin(f)\n                if f[best_index] < self.f_opt:\n                    self.f_opt = f[best_index]\n                    self.x_opt = x[:, best_index].copy()\n                    self.last_improvement = generation\n            else:\n                y = xmir\n                fitness = fmir\n                best_index = np.argmin(fmir)\n                if fmir[best_index] < self.f_opt:\n                    self.f_opt = fmir[best_index]\n                    self.x_opt = xmir[:, best_index].copy()\n                    self.last_improvement = generation\n                    \n            \n            arindex = np.argsort(fitness)\n            xmu = y[:, arindex[:self.mu]]\n            zmu = z[:, arindex[:self.mu]]\n            \n            m_old = self.m.copy()\n            self.m = np.sum(xmu * self.weights[np.newaxis, :], axis=1)\n            \n            ps_new = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs)) * (C_sqrt @ np.sum(zmu * self.weights[np.newaxis, :], axis=1))\n            self.ps = ps_new\n            \n            pc_new = (1 - self.c_cov) * self.pc + np.sqrt(self.c_cov * (2 - self.c_cov)) * (self.m - m_old) / self.sigma\n            self.pc = pc_new\n            \n            norm_ps = np.linalg.norm(self.ps)\n            factor = self.c_cov * (1 / (np.min([1, (1000000 * norm_ps**4 / self.dim)])))\n            \n            # Rank-one update of C\n            self.C = (1 - self.c_cov - self.c_mu) * self.C + self.c_cov * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :])\n            \n            # Rank-mu update of C\n            for i in range(self.mu):\n                y_diff = (xmu[:, i] - m_old) / self.sigma\n                self.C += self.c_mu * self.weights[i] * (y_diff[:, np.newaxis] @ y_diff[np.newaxis, :])\n            \n            # Eigenvalue clipping to ensure C remains positive definite and well-conditioned\n            try:\n                eigenvalues, eigenvectors = np.linalg.eigh(self.C)\n                eigenvalues = np.maximum(eigenvalues, self.eigenvalue_floor)  # Clip eigenvalues\n                self.C = eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)\n\n            # Adaptive sigma based on success rate using success history\n            self.success_history[self.success_history_idx] = np.mean(fitness < np.median(fitness))\n            self.success_history_idx = (self.success_history_idx + 1) % self.success_history_length\n            success_rate = np.mean(self.success_history)\n\n            # Curvature-based step size adaptation\n            fitness_improvement = self.prev_fitness - np.min(fitness)\n            if fitness_improvement > 0:\n                self.sigma *= (1 + self.curvature_adaptation_rate * fitness_improvement / (np.abs(self.prev_fitness) + 1e-9))  # Increase step size if improvement\n            else:\n                self.sigma *= (1 - self.curvature_adaptation_rate)  # Decrease step size if no improvement\n            self.prev_fitness = np.min(fitness)\n\n            if success_rate > 0.7:\n                self.sigma *= 1.1  # Increase step size, reduced multiplier\n            elif success_rate < 0.3:\n                self.sigma *= 0.9  # Decrease step size, reduced multiplier\n\n\n            self.sigma = min(self.sigma, 5.0) # Limit the maximum step size\n\n            self.sigma *= np.exp((self.cs / self.damp) * (norm_ps / self.chiN - 1))\n            \n            # Ensure C remains positive definite\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)  # Reset covariance matrix if it becomes non-positive definite\n\n            # Stagnation detection based on parameter change and objective change\n            param_change = np.linalg.norm(self.m - self.prev_m)\n            obj_change = np.abs(self.f_opt - np.min(fitness))\n\n            # Population diversity check\n            diversity = np.std(y, axis=1).mean()\n\n            if param_change < self.stagnation_threshold_param or obj_change < self.stagnation_threshold_obj or diversity < self.diversity_threshold:\n                 if generation - self.last_improvement > self.max_no_improvement/2: # Less strict restart trigger\n                    self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                    self.C = np.eye(self.dim)\n                    self.pc = np.zeros(self.dim)\n                    self.ps = np.zeros(self.dim)\n                    self.sigma = 0.3  # Reset sigma to initial value\n                    self.last_improvement = generation\n                    self.success_history = np.zeros(self.success_history_length) # Reset success history\n                    self.prev_fitness = np.inf\n\n            # Restart mechanism\n            if generation - self.last_improvement > self.max_no_improvement:\n                self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.sigma = 0.3  # Reset sigma to initial value\n                self.last_improvement = generation # reset last improvement\n                self.success_history = np.zeros(self.success_history_length) # Reset success history\n                self.prev_fitness = np.inf\n                \n            self.prev_m = self.m.copy()\n            generation += 1\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm CMAES_AdaptiveRestart_Improved2 scored 0.287 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6f5f77b9-c0eb-44a2-a1b4-133850e6c1af"], "operator": null, "metadata": {"aucs": [0.11627390703240781, 0.1942781898890925, 0.2808019168465268, 0.21928635220838277, 0.1980966101930609, 0.23244238195626388, 0.23134887319688346, 0.23172972106266343, 0.2069319436566428, 0.15949281429491824, 0.2527643258230342, 0.9851285260883808, 0.2633728969590099, 0.21093447179845548, 0.5572427298967102, 0.2810949669331454, 0.23432100080699547, 0.25668449681012484, 0.1598547011527751, 0.46534010666333947]}}
{"id": "9a4b8561-2d1a-4f51-b8a4-8c86afee61e7", "fitness": 0.45386201205966925, "name": "SelfOrganizingDE", "description": "A self-organizing differential evolution algorithm that dynamically adjusts mutation and crossover parameters based on individual agent success rates to enhance exploration and exploitation.", "code": "import numpy as np\n\nclass SelfOrganizingDE:\n    def __init__(self, budget=10000, dim=10, pop_size=None, F_init=0.5, Cr_init=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 * dim if pop_size is None else pop_size\n        self.F_init = F_init  # Initial mutation factor\n        self.Cr_init = Cr_init  # Initial crossover rate\n        self.pop = None\n        self.fitness = None\n        self.F = None\n        self.Cr = None\n        self.success_F = None\n        self.success_Cr = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.learning_rate = 0.1\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.pop[best_idx].copy()\n        self.F = np.full(self.pop_size, self.F_init)\n        self.Cr = np.full(self.pop_size, self.Cr_init)\n        self.success_F = np.zeros(self.pop_size)\n        self.success_Cr = np.zeros(self.pop_size)\n\n    def mutate(self, pop, i):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n        return a + self.F[i] * (b - c)\n\n    def crossover(self, target, mutant, i):\n        cross_points = np.random.rand(self.dim) < self.Cr[i]\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def update_parameters(self, i, success):\n         self.success_F[i] = (1 - self.learning_rate) * self.success_F[i] + self.learning_rate * success\n         self.success_Cr[i] = (1 - self.learning_rate) * self.success_Cr[i] + self.learning_rate * success\n\n         # Adjust F and Cr based on individual success\n         if self.success_F[i] > 0.5:\n            self.F[i] = min(self.F[i] + self.learning_rate, 1.0)\n         else:\n            self.F[i] = max(self.F[i] - self.learning_rate, 0.1)\n\n         if self.success_Cr[i] > 0.5:\n            self.Cr[i] = min(self.Cr[i] + self.learning_rate, 1.0)\n         else:\n            self.Cr[i] = max(self.Cr[i] - self.learning_rate, 0.1)\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            for i in range(len(self.pop)):\n                target = self.pop[i]\n                mutant = self.mutate(self.pop, i)\n                trial = self.crossover(target, mutant, i)\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < self.fitness[i]:\n                    self.update_parameters(i, 1)\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial.copy()\n                else:\n                    self.update_parameters(i, 0)\n\n                if self.eval_count >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm SelfOrganizingDE scored 0.454 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["7bcb5016-e6b2-445d-8ff4-c03cfe2c96a5"], "operator": null, "metadata": {"aucs": [0.17415326418881327, 0.27881229993815393, 0.4221080414186248, 0.3747914327540086, 0.3760568639613613, 0.6463475649833996, 0.36664432205951114, 0.4578959242906795, 0.4934446973183071, 0.277940412939104, 0.39811087311683946, 0.9982793471743651, 0.26189751444298026, 0.32813827890746583, 0.8099212449929407, 0.7125569798554097, 0.3986021545691416, 0.5694519896837231, 0.22903991606663698, 0.5030471185319187]}}
{"id": "a68926ad-c63c-47a4-a3ac-f4de3adc2785", "fitness": 0.47224485541010675, "name": "DE_CMAES_Improved", "description": "Improved hybrid DE-CMA-ES with adaptive step size and covariance matrix adaptation for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass DE_CMAES_Improved:\n    def __init__(self, budget=10000, dim=10, popsize=None, F=0.5, CR=0.7, cma_sigma=0.1, cma_lr_sigma=0.1, cma_lr_C=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 * self.dim  # Increased population size\n        self.F = F\n        self.CR = CR\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.cma_sigma = cma_sigma  # Step size for CMA-ES-like mutation\n        self.cma_C = np.eye(dim)     # Covariance matrix for CMA-ES-like mutation\n        self.cma_D = np.ones(dim)    # Diagonal of the covariance matrix (eigenvalues of C)\n        self.cma_mu = self.popsize // 2 # Number of parents\n        self.cma_weights = np.log(self.cma_mu + 1/2) - np.log(np.arange(1, self.cma_mu + 1))\n        self.cma_weights /= np.sum(self.cma_weights)\n        self.cma_mueff = np.sum(self.cma_weights)**2 / np.sum(self.cma_weights**2)\n        self.cma_cs = (self.cma_mueff + 2) / (self.dim + self.cma_mueff + 5)\n        self.cma_damps = 1 + 2 * max(0, np.sqrt((self.cma_mueff - 1)/(self.dim + 1)) - 1) + self.cma_cs\n        self.cma_lr_sigma = cma_lr_sigma  # Learning rate for sigma\n        self.cma_lr_C = cma_lr_C # Learning rate for covariance matrix\n\n        self.pc = np.zeros(self.dim) # Evolution path for rank-one update of C\n        self.ps = np.zeros(self.dim)  # Evolution path for sigma control\n        self.chiN = np.sqrt(self.dim) * (1 - (1/(4 * self.dim)) + 1/(21 * self.dim**2))\n        self.eval_count = 0\n\n\n    def __call__(self, func):\n        bounds_lb = func.bounds.lb\n        bounds_ub = func.bounds.ub\n        self.population = np.random.uniform(bounds_lb, bounds_ub, size=(self.popsize, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.eval_count = self.popsize\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = self.population[np.argmin(fitness)].copy()\n\n        while self.eval_count < self.budget:\n            for i in range(self.popsize):\n                # DE mutation\n                idxs = np.random.choice(self.popsize, 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                mutant_de = self.population[i] + self.F * (x2 - x3)\n                mutant_de = np.clip(mutant_de, bounds_lb, bounds_ub)\n\n                # CMA-ES-like mutation\n                z = np.random.normal(0, 1, self.dim)\n                mutant_cmaes = self.population[i] + self.cma_sigma * np.dot(self.cma_C, z) # Use C directly\n                mutant_cmaes = np.clip(mutant_cmaes, bounds_lb, bounds_ub)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.CR\n                trial_vector = np.where(crossover_mask, mutant_de, mutant_cmaes)\n\n                # Selection\n                f_trial = func(trial_vector)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    self.population[i] = trial_vector.copy()\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial_vector.copy()\n            \n            #Adapt CMA-ES parameters\n            indices = np.argsort(fitness)\n            xmean = np.sum(self.population[indices[:self.cma_mu]].T * self.cma_weights, axis=1)\n\n            # Update evolution paths\n            self.ps = (1 - self.cma_cs) * self.ps + np.sqrt(self.cma_cs * (2 - self.cma_cs) * self.cma_mueff) * (xmean - np.mean(self.population, axis=0)) / self.cma_sigma\n            self.pc = (1 - self.cma_cs) * self.pc + np.sqrt(self.cma_cs * (2 - self.cma_cs) * self.cma_mueff) * (xmean - np.mean(self.population, axis=0))\n\n            # Update covariance matrix\n            delta = (xmean - np.mean(self.population, axis=0)) / self.cma_sigma\n            self.cma_C = (1 - self.cma_lr_C) * self.cma_C + self.cma_lr_C * (np.outer(self.pc, self.pc) + self.cma_lr_C * np.eye(self.dim) ) # Rank-one update\n\n            # Adapt step size\n            self.cma_sigma *= np.exp((self.cma_lr_sigma / self.cma_damps) * (np.linalg.norm(self.ps)/ self.chiN -1))\n\n            if self.eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm DE_CMAES_Improved scored 0.472 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b6ff32f5-f85f-4aeb-878e-69998de8c5de"], "operator": null, "metadata": {"aucs": [0.14971226913732893, 0.2554473650067344, 0.4119439298026313, 0.742431118326017, 0.3855416981649713, 0.6701306614456846, 0.3020263092039497, 0.47624828717620604, 0.33437830932807533, 0.20964128887763744, 0.7493789778013336, 0.9986142548694507, 0.3536280400562276, 0.43121896302497476, 0.6776087769087151, 0.5783934636903507, 0.3422080750855858, 0.7267003147527273, 0.18174213917476034, 0.4679028663687721]}}
{"id": "e9a891b9-7303-4c6e-a97f-5b3434ef9ac0", "fitness": 0.28879619162764064, "name": "AdaptiveNichingDE", "description": "An adaptive differential evolution algorithm with a self-adaptive mutation factor, population diversity maintenance, and a niching mechanism to promote exploration of multiple promising regions.", "code": "import numpy as np\n\nclass AdaptiveNichingDE:\n    def __init__(self, budget=10000, dim=10, pop_multiplier=15, F_init=0.5, Cr=0.9, niche_radius=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = dim * pop_multiplier\n        self.F_init = F_init  # Initial mutation factor\n        self.Cr = Cr\n        self.niche_radius = niche_radius\n        self.pop = None\n        self.fitness = None\n        self.F = None  # Self-adaptive mutation factor\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.niches = []  # List of niche centers\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.F = np.full(self.pop_size, self.F_init)  # Initialize mutation factors\n        self.eval_count += self.pop_size\n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.pop[best_idx].copy()\n        self.update_niches()\n\n    def mutate(self, pop, i):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n        return a + self.F[i] * (b - c)\n\n    def crossover(self, target, mutant):\n        cross_points = np.random.rand(self.dim) < self.Cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def niching_pressure(self, x):\n        # Encourage individuals to stay within their niche or explore new ones\n        penalty = 0\n        for center in self.niches:\n            dist = np.linalg.norm(x - center)\n            if dist < self.niche_radius:\n                penalty += 0  # Within niche, no penalty\n            else:\n                penalty += (dist - self.niche_radius) # Outside niche, penalty increases with distance\n\n        return penalty\n\n    def update_niches(self):\n        # Update niche centers based on top-performing individuals\n        num_niches = min(5, len(self.pop) // 5)  # Adapt number of niches\n        top_indices = np.argsort(self.fitness)[:num_niches]\n        self.niches = [self.pop[i].copy() for i in top_indices]\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            for i in range(len(self.pop)):\n                target = self.pop[i]\n                mutant = self.mutate(self.pop, i)\n                trial = self.crossover(target, mutant)\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)  # Repair\n\n                f_trial = func(trial) + self.niching_pressure(trial) # Add niching pressure\n                self.eval_count += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    # Update mutation factor based on success\n                    self.F[i] = np.clip(self.F[i] + 0.1 * np.random.randn(), 0.1, 1.0)\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial.copy()\n                        \n\n                else:\n                     # Reduce mutation factor if unsuccessful\n                     self.F[i] = np.clip(self.F[i] - 0.05 * np.random.rand(), 0.1, 1.0)\n\n                if self.eval_count >= self.budget:\n                    break\n\n            self.update_niches() # Update niches after each generation\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveNichingDE scored 0.289 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["7bcb5016-e6b2-445d-8ff4-c03cfe2c96a5"], "operator": null, "metadata": {"aucs": [0.20750831531977354, 0.23541064766169162, 0.26428205260632553, 0.1973162145197912, 0.20483942536512645, 0.20722499113768522, 0.24049525535484895, 0.2558158353381548, 0.19549311990810259, 0.15117288878457424, 0.18897312335314453, 0.9998270921799399, 0.2708553078281005, 0.21200191235987786, 0.5463830981551838, 0.25920621897521257, 0.23907000473325413, 0.24994545851625438, 0.18473761892760854, 0.465365251528163]}}
{"id": "b633dbe7-c520-4d37-b25b-9f576cf8b935", "fitness": 0.663890535191122, "name": "DE_CMAES_AdaptiveRepair", "description": "Combines differential evolution with a CMA-ES-inspired mutation strategy, incorporating adaptive parameter control and a repair mechanism using the mean of the population.", "code": "import numpy as np\n\nclass DE_CMAES_AdaptiveRepair:\n    def __init__(self, budget=10000, dim=10, popsize=None, F=0.5, CR=0.7, cma_sigma=0.1, repair_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 2 * self.dim\n        self.F = F\n        self.CR = CR\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.cma_sigma = cma_sigma  # Step size for CMA-ES-like mutation\n        self.cma_C = np.eye(dim)     # Covariance matrix for CMA-ES-like mutation\n        self.cma_D = np.ones(dim)    # Diagonal of the covariance matrix\n        self.cma_mu = self.popsize // 2 # Number of parents\n        self.cma_weights = np.log(self.cma_mu + 1/2) - np.log(np.arange(1, self.cma_mu + 1))\n        self.cma_weights /= np.sum(self.cma_weights)\n        self.cma_mueff = np.sum(self.cma_weights)**2 / np.sum(self.cma_weights**2)\n        self.cma_cs = (self.cma_mueff + 2) / (self.dim + self.cma_mueff + 5)\n        self.cma_damps = 1 + 2 * max(0, np.sqrt((self.cma_mueff - 1)/(self.dim + 1)) - 1) + self.cma_cs\n        self.repair_prob = repair_prob\n\n        self.pc = np.zeros(self.dim) # Evolution path for rank-one update of C\n        self.ps = np.zeros(self.dim)  # Evolution path for sigma control\n        self.chiN = np.sqrt(self.dim) * (1 - (1/(4 * self.dim)) + 1/(21 * self.dim**2))\n\n        self.F_adaptive = self.F * np.ones(self.popsize)\n        self.CR_adaptive = self.CR * np.ones(self.popsize)\n\n\n    def __call__(self, func):\n        bounds_lb = func.bounds.lb\n        bounds_ub = func.bounds.ub\n        self.population = np.random.uniform(bounds_lb, bounds_ub, size=(self.popsize, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        eval_count = self.popsize\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = self.population[np.argmin(fitness)].copy()\n\n        while eval_count < self.budget:\n            mean_pop = np.mean(self.population, axis=0)\n\n            for i in range(self.popsize):\n                # Adaptive F and CR\n                self.F_adaptive[i] = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n                self.CR_adaptive[i] = np.clip(np.random.normal(self.CR, 0.1), 0.1, 0.9)\n\n                # DE mutation\n                idxs = np.random.choice(self.popsize, 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                mutant_de = self.population[i] + self.F_adaptive[i] * (x2 - x3)\n                mutant_de = np.clip(mutant_de, bounds_lb, bounds_ub)\n\n                # CMA-ES-like mutation\n                z = np.random.normal(0, 1, self.dim)\n                mutant_cmaes = self.population[i] + self.cma_sigma * self.cma_D * z\n                mutant_cmaes = np.clip(mutant_cmaes, bounds_lb, bounds_ub)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.CR_adaptive[i]\n                trial_vector = np.where(crossover_mask, mutant_de, mutant_cmaes)\n                \n                # Repair Mechanism\n                if np.random.rand() < self.repair_prob:\n                    trial_vector = 0.5 * (trial_vector + mean_pop)\n\n                trial_vector = np.clip(trial_vector, bounds_lb, bounds_ub)\n\n\n                # Selection\n                f_trial = func(trial_vector)\n                eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    self.population[i] = trial_vector.copy()\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial_vector.copy()\n            \n            #Adapt CMA-ES parameters\n            indices = np.argsort(fitness)\n            xmean = np.sum(self.population[indices[:self.cma_mu]].T * self.cma_weights, axis=1)\n\n            self.ps = (1 - self.cma_cs) * self.ps + np.sqrt(self.cma_cs * (2 - self.cma_cs) * self.cma_mueff) * (xmean - np.mean(self.population, axis=0)) / self.cma_sigma\n            self.cma_sigma *= np.exp((self.cma_cs / self.cma_damps) * (np.linalg.norm(self.ps)/ self.chiN -1))\n\n            if eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm DE_CMAES_AdaptiveRepair scored 0.664 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b6ff32f5-f85f-4aeb-878e-69998de8c5de"], "operator": null, "metadata": {"aucs": [0.22403109428169954, 0.617379975713139, 0.7805646904842403, 0.9046488316732414, 0.8399690397368522, 0.8944389726038698, 0.397182128654471, 0.7964730333964425, 0.8375006902098195, 0.2147174763894092, 0.9053457860321441, 0.9940092853394473, 0.47466497301059374, 0.5964962664288042, 0.8196838854868519, 0.8225893264724206, 0.5402921766107263, 0.9142202803075105, 0.20844323437585066, 0.4951595566149053]}}
{"id": "ca7d64f9-ca3a-42c0-8922-3204ae93da9f", "fitness": -Infinity, "name": "DE_CMAES_Adaptive", "description": "Enhanced hybrid DE-CMAES with adaptive parameter control using success-history adaptation, population diversity maintenance, and landscape-aware mutation blending.", "code": "import numpy as np\n\nclass DE_CMAES_Adaptive:\n    def __init__(self, budget=10000, dim=10, popsize=None, F=0.5, CR=0.7, cma_sigma=0.1, history_length=10):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 2 * self.dim\n        self.F = F\n        self.CR = CR\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.cma_sigma = cma_sigma\n        self.cma_C = np.eye(dim)\n        self.cma_D = np.ones(dim)\n        self.cma_mu = self.popsize // 2\n        self.cma_weights = np.log(self.cma_mu + 1/2) - np.log(np.arange(1, self.cma_mu + 1))\n        self.cma_weights /= np.sum(self.cma_weights)\n        self.cma_mueff = np.sum(self.cma_weights)**2 / np.sum(self.cma_weights**2)\n        self.cma_cs = (self.cma_mueff + 2) / (self.dim + self.cma_mueff + 5)\n        self.cma_damps = 1 + 2 * max(0, np.sqrt((self.cma_mueff - 1)/(self.dim + 1)) - 1) + self.cma_cs\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1/(4 * self.dim)) + 1/(21 * self.dim**2))\n\n        # Success-History Adaptation\n        self.history_length = history_length\n        self.sf_history = []\n        self.sc_history = []\n        self.archive_f = []\n        self.archive_cr = []\n        self.memory_sf = np.ones(history_length) * 0.5\n        self.memory_sc = np.ones(history_length) * 0.5\n        self.p_selection = 0.5\n\n        # Diversity Maintenance\n        self.diversity_threshold = 0.1 * (func.bounds.ub - func.bounds.lb)  # Example: 10% of the range\n        self.diversity_count = 0\n\n    def __call__(self, func):\n        bounds_lb = func.bounds.lb\n        bounds_ub = func.bounds.ub\n        self.population = np.random.uniform(bounds_lb, bounds_ub, size=(self.popsize, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        eval_count = self.popsize\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = self.population[np.argmin(fitness)].copy()\n\n        while eval_count < self.budget:\n            new_population = np.copy(self.population)\n            new_fitness = np.copy(fitness)\n\n            for i in range(self.popsize):\n                # Adaptive F and CR\n                if len(self.memory_sf) > 0 and np.random.rand() < self.p_selection:\n                    Cr = np.random.choice(self.memory_sc)\n                    F = np.random.choice(self.memory_sf)\n                else:\n                    Cr = self.CR\n                    F = self.F\n                \n                # DE mutation\n                idxs = np.random.choice(self.popsize, 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                mutant_de = self.population[i] + F * (x2 - x3)\n                mutant_de = np.clip(mutant_de, bounds_lb, bounds_ub)\n\n                # CMA-ES-like mutation\n                z = np.random.normal(0, 1, self.dim)\n                mutant_cmaes = self.population[i] + self.cma_sigma * self.cma_D * z\n                mutant_cmaes = np.clip(mutant_cmaes, bounds_lb, bounds_ub)\n                \n                # Adaptive blending of DE and CMA-ES\n                if fitness[i] > np.mean(fitness):  # Landscape-aware blending\n                    trial_vector = mutant_de\n                else:\n                    trial_vector = mutant_cmaes\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < Cr\n                trial_vector = np.where(crossover_mask, mutant_de, mutant_cmaes)\n\n                # Selection\n                f_trial = func(trial_vector)\n                eval_count += 1\n\n                if f_trial < fitness[i]:\n                    new_fitness[i] = f_trial\n                    new_population[i] = trial_vector.copy()\n\n                    self.sf_history.append(F)\n                    self.sc_history.append(Cr)\n                    self.archive_f.append(fitness[i])\n                    self.archive_cr.append(Cr)\n                    \n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial_vector.copy()\n            \n            # Diversity maintenance\n            distances = np.linalg.norm(self.population - np.mean(self.population, axis=0), axis=1)\n            if np.any(distances < self.diversity_threshold):\n                self.diversity_count += 1\n                if self.diversity_count > 10:\n                    # Reset a portion of the population\n                    indices_to_reset = np.argsort(fitness)[-self.popsize // 4:] # Reset worst 25%\n                    self.population[indices_to_reset] = np.random.uniform(bounds_lb, bounds_ub, size=(len(indices_to_reset), self.dim))\n                    fitness[indices_to_reset] = [func(x) for x in self.population[indices_to_reset]]\n                    eval_count += len(indices_to_reset)\n                    self.diversity_count = 0\n            else:\n                self.diversity_count = 0\n\n            self.population = new_population\n            fitness = new_fitness\n\n            #Adapt CMA-ES parameters\n            indices = np.argsort(fitness)\n            xmean = np.sum(self.population[indices[:self.cma_mu]].T * self.cma_weights, axis=1)\n\n            self.ps = (1 - self.cma_cs) * self.ps + np.sqrt(self.cma_cs * (2 - self.cma_cs) * self.cma_mueff) * (xmean - np.mean(self.population, axis=0)) / self.cma_sigma\n            self.cma_sigma *= np.exp((self.cma_cs / self.cma_damps) * (np.linalg.norm(self.ps)/ self.chiN -1))\n\n            # Update memory for F and CR\n            if len(self.sf_history) > 0:\n                self.memory_sf = np.concatenate([self.memory_sf, self.sf_history])[-self.history_length:]\n                self.memory_sc = np.concatenate([self.memory_sc, self.sc_history])[-self.history_length:]\n                self.sf_history = []\n                self.sc_history = []\n\n            if eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "An exception occurred: name 'func' is not defined.", "error": "", "parent_ids": ["b6ff32f5-f85f-4aeb-878e-69998de8c5de"], "operator": null, "metadata": {}}
{"id": "7876419b-5aa0-431f-b3bd-ffbdefeeba4a", "fitness": -Infinity, "name": "CMAES_Aging_Orthogonal", "description": "Combines CMA-ES with orthogonal sampling, adaptive step size, and a population aging mechanism to promote diversity and escape local optima.", "code": "import numpy as np\n\nclass CMAES_Aging_Orthogonal:\n    def __init__(self, budget=10000, dim=10, popsize_multiplier=4, cs=0.3, damp=None, c_cov=None, c_mu=None, mirror_factor=0.5, stagnation_threshold_param=1e-6, stagnation_threshold_obj=1e-9, eigenvalue_floor=1e-12, success_history_length=5, curvature_adaptation_rate=0.1, aging_rate=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = int(popsize_multiplier + np.floor(3 * np.log(self.dim)))  # Dynamic popsize based on dimension\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.m = None\n        self.sigma = 0.3  # exploration parameter\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.chiN = None\n\n        self.cs = cs\n        self.damp = damp if damp is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1]) + self.cs\n        self.c_cov = c_cov if c_cov is not None else 2 / ((self.dim + np.sqrt(2))**2)\n        self.c_mu = c_mu if c_mu is not None else min(1 - self.c_cov, 2 * (self.weights[0] - 1 + self.weights[0]**2) / ((self.mu + 6 + 6*self.dim**0.5)))\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.mirror_factor = mirror_factor\n        self.stagnation_threshold_param = stagnation_threshold_param\n        self.stagnation_threshold_obj = stagnation_threshold_obj\n        self.last_improvement = 0\n        self.max_no_improvement = 100  # generations before restart\n        self.eigenvalue_floor = eigenvalue_floor  # Minimum eigenvalue for covariance matrix\n        self.success_history_length = success_history_length\n        self.success_history = np.zeros(self.success_history_length)\n        self.success_history_idx = 0\n        self.prev_m = None\n        self.curvature_adaptation_rate = curvature_adaptation_rate\n        self.prev_fitness = np.inf\n        self.aging_rate = aging_rate # Rate at which solutions are replaced by random ones.\n        self.population_age = np.zeros(self.popsize)\n\n    def __call__(self, func):\n        self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1 / (4 * self.dim) + 1 / (21 * self.dim**2))\n        self.prev_m = self.m.copy()\n        \n        used_budget = 0\n        generation = 0\n\n        while used_budget < self.budget:\n            # Orthogonal Sampling\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            Q, _ = np.linalg.qr(z)  # Orthogonal basis\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.m[:, np.newaxis] + self.sigma * C_sqrt @ Q\n            \n            # Mirroring to handle bounds - more aggressive mirroring\n            xmir = self.m[:, np.newaxis] - self.mirror_factor * self.sigma * C_sqrt @ Q\n            \n            # Repair mechanism to ensure solutions are within bounds and resample if necessary\n            for i in range(self.popsize):\n                if not func.bounds.lb[0] <= x[0, i] <= func.bounds.ub[0]:\n                    x[:, i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                if not func.bounds.lb[0] <= xmir[0, i] <= func.bounds.ub[0]:\n                    xmir[:, i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                    \n            f = np.array([func(x[:, i]) if used_budget < self.budget else np.inf for i in range(self.popsize)])\n            fmir = np.array([func(xmir[:, i]) if used_budget + self.popsize < self.budget else np.inf for i in range(self.popsize)])\n            \n            used_budget += 2 * self.popsize\n            \n            if np.min(f) < np.min(fmir):\n                y = x\n                fitness = f\n                best_index = np.argmin(f)\n                if f[best_index] < self.f_opt:\n                    self.f_opt = f[best_index]\n                    self.x_opt = x[:, best_index].copy()\n                    self.last_improvement = generation\n            else:\n                y = xmir\n                fitness = fmir\n                best_index = np.argmin(fmir)\n                if fmir[best_index] < self.f_opt:\n                    self.f_opt = fmir[best_index]\n                    self.x_opt = xmir[:, best_index].copy()\n                    self.last_improvement = generation\n                    \n            \n            arindex = np.argsort(fitness)\n            xmu = y[:, arindex[:self.mu]]\n            Qmu = Q[:, arindex[:self.mu]]\n            \n            m_old = self.m.copy()\n            self.m = np.sum(xmu * self.weights[np.newaxis, :], axis=1)\n            \n            ps_new = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs)) * (C_sqrt @ np.sum(Qmu * self.weights[np.newaxis, :], axis=1))\n            self.ps = ps_new\n            \n            pc_new = (1 - self.c_cov) * self.pc + np.sqrt(self.c_cov * (2 - self.c_cov)) * (self.m - m_old) / self.sigma\n            self.pc = pc_new\n            \n            norm_ps = np.linalg.norm(self.ps)\n            factor = self.c_cov * (1 / (np.min([1, (1000000 * norm_ps**4 / self.dim)])))\n            \n            # Rank-one update of C\n            self.C = (1 - self.c_cov - self.c_mu) * self.C + self.c_cov * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :])\n            \n            # Rank-mu update of C\n            for i in range(self.mu):\n                y_diff = (xmu[:, i] - m_old) / self.sigma\n                self.C += self.c_mu * self.weights[i] * (y_diff[:, np.newaxis] @ y_diff[np.newaxis, :])\n            \n            # Eigenvalue clipping to ensure C remains positive definite and well-conditioned\n            try:\n                eigenvalues, eigenvectors = np.linalg.eigh(self.C)\n                eigenvalues = np.maximum(eigenvalues, self.eigenvalue_floor)  # Clip eigenvalues\n                self.C = eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)\n\n            # Adaptive sigma based on success rate using success history\n            self.success_history[self.success_history_idx] = np.mean(fitness < np.median(fitness))\n            self.success_history_idx = (self.success_history_idx + 1) % self.success_history_length\n            success_rate = np.mean(self.success_history)\n\n            # Curvature-based step size adaptation\n            fitness_improvement = self.prev_fitness - np.min(fitness)\n            if fitness_improvement > 0:\n                self.sigma *= (1 + self.curvature_adaptation_rate * fitness_improvement / (np.abs(self.prev_fitness) + 1e-9))  # Increase step size if improvement\n            else:\n                self.sigma *= (1 - self.curvature_adaptation_rate)  # Decrease step size if no improvement\n            self.prev_fitness = np.min(fitness)\n\n            if success_rate > 0.7:\n                self.sigma *= 1.1  # Increase step size, reduced multiplier\n            elif success_rate < 0.3:\n                self.sigma *= 0.9  # Decrease step size, reduced multiplier\n\n\n            self.sigma = min(self.sigma, 5.0) # Limit the maximum step size\n\n            self.sigma *= np.exp((self.cs / self.damp) * (norm_ps / self.chiN - 1))\n            \n            # Ensure C remains positive definite\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)  # Reset covariance matrix if it becomes non-positive definite\n\n            # Stagnation detection based on parameter change and objective change\n            param_change = np.linalg.norm(self.m - self.prev_m)\n            obj_change = np.abs(self.f_opt - np.min(fitness))\n\n            if param_change < self.stagnation_threshold_param or obj_change < self.stagnation_threshold_obj:\n                 if generation - self.last_improvement > self.max_no_improvement/2: # Less strict restart trigger\n                    self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                    self.C = np.eye(self.dim)\n                    self.pc = np.zeros(self.dim)\n                    self.ps = np.zeros(self.dim)\n                    self.sigma = 0.3  # Reset sigma to initial value\n                    self.last_improvement = generation\n                    self.success_history = np.zeros(self.success_history_length) # Reset success history\n                    self.prev_fitness = np.inf\n\n            # Restart mechanism\n            if generation - self.last_improvement > self.max_no_improvement:\n                self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.sigma = 0.3  # Reset sigma to initial value\n                self.last_improvement = generation # reset last improvement\n                self.success_history = np.zeros(self.success_history_length) # Reset success history\n                self.prev_fitness = np.inf\n                \n            # Population Aging Mechanism\n            self.population_age += 1\n            for i in range(self.popsize):\n                if np.random.rand() < self.aging_rate * (self.population_age[i] / generation if generation > 0 else 1): # Make aging more probable with age\n                    y[:, i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                    fitness[i] = func(y[:, i]) if used_budget < self.budget else np.inf\n                    used_budget += 1\n                    self.population_age[i] = 0 # Reset age\n\n            self.prev_m = self.m.copy()\n            generation += 1\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "An exception occurred: index 2 is out of bounds for axis 1 with size 2.", "error": "", "parent_ids": ["6f5f77b9-c0eb-44a2-a1b4-133850e6c1af"], "operator": null, "metadata": {}}
{"id": "82ff4ca3-1476-4933-8f2b-7f937ec316ce", "fitness": -Infinity, "name": "DE_CMAES_AdaptiveLR_Orthogonal", "description": "Adaptively adjusts CMA-ES learning rates based on the success rate of recent mutations and incorporates orthogonal sampling to improve exploration.", "code": "import numpy as np\n\nclass DE_CMAES_AdaptiveLR_Orthogonal:\n    def __init__(self, budget=10000, dim=10, popsize=None, F=0.5, CR=0.7, cma_sigma=0.1, cma_lr_sigma=0.1, cma_lr_C=0.1, success_history_length=10):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 * self.dim\n        self.F = F\n        self.CR = CR\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.cma_sigma = cma_sigma\n        self.cma_C = np.eye(dim)\n        self.cma_D = np.ones(dim)\n        self.cma_mu = self.popsize // 2\n        self.cma_weights = np.log(self.cma_mu + 1/2) - np.log(np.arange(1, self.cma_mu + 1))\n        self.cma_weights /= np.sum(self.cma_weights)\n        self.cma_mueff = np.sum(self.cma_weights)**2 / np.sum(self.cma_weights**2)\n        self.cma_cs = (self.cma_mueff + 2) / (self.dim + self.cma_mueff + 5)\n        self.cma_damps = 1 + 2 * max(0, np.sqrt((self.cma_mueff - 1)/(self.dim + 1)) - 1) + self.cma_cs\n        self.cma_lr_sigma = cma_lr_sigma\n        self.cma_lr_C = cma_lr_C\n\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1/(4 * self.dim)) + 1/(21 * self.dim**2))\n        self.eval_count = 0\n        self.success_history = []\n        self.success_history_length = success_history_length\n\n    def orthogonal_sampling(self, x, num_samples=5):\n        # Generate orthogonal directions\n        H = np.random.randn(self.dim, self.dim)\n        Q, R = np.linalg.qr(H)\n\n        samples = []\n        for i in range(num_samples):\n            direction = Q[:, i % self.dim] # Cycle through available directions\n            step_size = np.random.uniform(-self.cma_sigma, self.cma_sigma) # Scale with CMA sigma\n\n            sample = x + step_size * direction\n            sample = np.clip(sample, func.bounds.lb, func.bounds.ub) # Clip to bounds\n            samples.append(sample)\n        return samples\n\n\n    def __call__(self, func):\n        bounds_lb = func.bounds.lb\n        bounds_ub = func.bounds.ub\n        self.population = np.random.uniform(bounds_lb, bounds_ub, size=(self.popsize, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.eval_count = self.popsize\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = self.population[np.argmin(fitness)].copy()\n\n        while self.eval_count < self.budget:\n            for i in range(self.popsize):\n                # DE mutation\n                idxs = np.random.choice(self.popsize, 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                mutant_de = self.population[i] + self.F * (x2 - x3)\n                mutant_de = np.clip(mutant_de, bounds_lb, bounds_ub)\n\n                # CMA-ES-like mutation\n                z = np.random.normal(0, 1, self.dim)\n                mutant_cmaes = self.population[i] + self.cma_sigma * np.dot(self.cma_C, z)\n                mutant_cmaes = np.clip(mutant_cmaes, bounds_lb, bounds_ub)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.CR\n                trial_vector = np.where(crossover_mask, mutant_de, mutant_cmaes)\n\n                # Orthogonal Sampling around trial vector\n                orthogonal_samples = self.orthogonal_sampling(trial_vector)\n                orthogonal_fitness = [func(sample) for sample in orthogonal_samples]\n                self.eval_count += len(orthogonal_samples)\n                best_orthogonal_index = np.argmin(orthogonal_fitness)\n                best_orthogonal_sample = orthogonal_samples[best_orthogonal_index]\n                f_trial = orthogonal_fitness[best_orthogonal_index]\n\n                # Selection\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    self.population[i] = best_orthogonal_sample.copy()\n                    self.success_history.append(1)\n                    if len(self.success_history) > self.success_history_length:\n                        self.success_history.pop(0)\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = best_orthogonal_sample.copy()\n                else:\n                    self.success_history.append(0)\n                    if len(self.success_history) > self.success_history_length:\n                        self.success_history.pop(0)\n            \n            #Adapt CMA-ES parameters\n            indices = np.argsort(fitness)\n            xmean = np.sum(self.population[indices[:self.cma_mu]].T * self.cma_weights, axis=1)\n\n            # Update evolution paths\n            self.ps = (1 - self.cma_cs) * self.ps + np.sqrt(self.cma_cs * (2 - self.cma_cs) * self.cma_mueff) * (xmean - np.mean(self.population, axis=0)) / self.cma_sigma\n            self.pc = (1 - self.cma_cs) * self.pc + np.sqrt(self.cma_cs * (2 - self.cma_cs) * self.cma_mueff) * (xmean - np.mean(self.population, axis=0))\n\n            # Update covariance matrix\n            delta = (xmean - np.mean(self.population, axis=0)) / self.cma_sigma\n            self.cma_C = (1 - self.cma_lr_C) * self.cma_C + self.cma_lr_C * (np.outer(self.pc, self.pc) + self.cma_lr_C * np.eye(self.dim) ) # Rank-one update\n\n            # Adapt step size\n            self.cma_sigma *= np.exp((self.cma_lr_sigma / self.cma_damps) * (np.linalg.norm(self.ps)/ self.chiN -1))\n\n            # Adaptive Learning Rate Adjustment\n            success_rate = np.mean(self.success_history) if self.success_history else 0.5  # Default to 0.5 if no history\n            self.cma_lr_sigma *= np.exp(0.1 * (success_rate - 0.2))  # Adjust lr_sigma based on success\n            self.cma_lr_C *= np.exp(0.1 * (success_rate - 0.2))\n\n            if self.eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "An exception occurred: name 'func' is not defined.", "error": "", "parent_ids": ["a68926ad-c63c-47a4-a3ac-f4de3adc2785"], "operator": null, "metadata": {}}
{"id": "b01bb34c-25a7-4314-bc48-b0518c4c4385", "fitness": 0.0, "name": "DE_CMAES_Mirrored", "description": "Integrates DE and CMA-ES with adaptive parameter control, employing a mirrored sampling technique and spectral correction of the covariance matrix to enhance exploration and exploitation.", "code": "import numpy as np\n\nclass DE_CMAES_Mirrored:\n    def __init__(self, budget=10000, dim=10, popsize=None, F=0.5, CR=0.7, cma_sigma=0.1, cma_lr_sigma=0.1, cma_lr_C=0.1, mirrored_sampling=True):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 * self.dim\n        self.F = F\n        self.CR = CR\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.cma_sigma = cma_sigma\n        self.cma_C = np.eye(dim)\n        self.cma_D = np.ones(dim)\n        self.cma_mu = self.popsize // 2\n        self.cma_weights = np.log(self.cma_mu + 1/2) - np.log(np.arange(1, self.cma_mu + 1))\n        self.cma_weights /= np.sum(self.cma_weights)\n        self.cma_mueff = np.sum(self.cma_weights)**2 / np.sum(self.cma_weights**2)\n        self.cma_cs = (self.cma_mueff + 2) / (self.dim + self.cma_mueff + 5)\n        self.cma_damps = 1 + 2 * max(0, np.sqrt((self.cma_mueff - 1)/(self.dim + 1)) - 1) + self.cma_cs\n        self.cma_lr_sigma = cma_lr_sigma\n        self.cma_lr_C = cma_lr_C\n        self.mirrored_sampling = mirrored_sampling\n\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1/(4 * self.dim)) + 1/(21 * self.dim**2))\n        self.eval_count = 0\n        self.bounds_lb = None\n        self.bounds_ub = None\n\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.popsize, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.eval_count = self.popsize\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = self.population[np.argmin(fitness)].copy()\n\n        while self.eval_count < self.budget:\n            for i in range(self.popsize):\n                # DE mutation\n                idxs = np.random.choice(self.popsize, 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                mutant_de = self.population[i] + self.F * (x2 - x3)\n                mutant_de = np.clip(mutant_de, self.bounds_lb, self.bounds_ub)\n\n                # CMA-ES-like mutation\n                z = np.random.normal(0, 1, self.dim)\n                mutant_cmaes = self.population[i] + self.cma_sigma * np.dot(self.cma_C, z)\n                mutant_cmaes = np.clip(mutant_cmaes, self.bounds_lb, self.bounds_ub)\n\n                # Mirrored Sampling\n                if self.mirrored_sampling:\n                    mutant_cmaes_mirrored = self.population[i] + self.cma_sigma * np.dot(self.cma_C, -z)\n                    mutant_cmaes_mirrored = np.clip(mutant_cmaes_mirrored, self.bounds_lb, self.bounds_ub)\n                else:\n                    mutant_cmaes_mirrored = mutant_cmaes\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.CR\n                trial_vector = np.where(crossover_mask, mutant_de, mutant_cmaes)\n                trial_vector_mirrored = np.where(crossover_mask, mutant_de, mutant_cmaes_mirrored)\n\n                # Selection\n                f_trial = func(trial_vector)\n                self.eval_count += 1\n                f_trial_mirrored = func(trial_vector_mirrored)\n                self.eval_count += 1\n\n                if f_trial < f_trial_mirrored and f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    self.population[i] = trial_vector.copy()\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial_vector.copy()\n                elif f_trial_mirrored < fitness[i]:\n                    fitness[i] = f_trial_mirrored\n                    self.population[i] = trial_vector_mirrored.copy()\n                    if f_trial_mirrored < self.f_opt:\n                        self.f_opt = f_trial_mirrored\n                        self.x_opt = trial_vector_mirrored.copy()\n\n\n            #Adapt CMA-ES parameters\n            indices = np.argsort(fitness)\n            xmean = np.sum(self.population[indices[:self.cma_mu]].T * self.cma_weights, axis=1)\n\n            # Update evolution paths\n            self.ps = (1 - self.cma_cs) * self.ps + np.sqrt(self.cma_cs * (2 - self.cma_cs) * self.cma_mueff) * (xmean - np.mean(self.population, axis=0)) / self.cma_sigma\n            self.pc = (1 - self.cma_cs) * self.pc + np.sqrt(self.cma_cs * (2 - self.cma_cs) * self.cma_mueff) * (xmean - np.mean(self.population, axis=0))\n\n            # Update covariance matrix\n            delta = (xmean - np.mean(self.population, axis=0)) / self.cma_sigma\n            self.cma_C = (1 - self.cma_lr_C) * self.cma_C + self.cma_lr_C * (np.outer(self.pc, self.pc) + self.cma_lr_C * np.eye(self.dim) ) # Rank-one update\n\n            # Adapt step size\n            self.cma_sigma *= np.exp((self.cma_lr_sigma / self.cma_damps) * (np.linalg.norm(self.ps)/ self.chiN -1))\n            # Spectral correction of covariance matrix\n            try:\n                self.cma_C = np.linalg.matrix_power(self.cma_C, 1) # Identity operation for positive-definiteness. Higher powers could be attempted but lead to numerical instability.\n            except np.linalg.LinAlgError:\n                self.cma_C = np.eye(self.dim)  # Reset to identity if it fails\n\n\n            if self.eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm DE_CMAES_Mirrored scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a68926ad-c63c-47a4-a3ac-f4de3adc2785"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "a99a15c1-c814-463a-a6d2-95c01b810d33", "fitness": 0.3692684534391685, "name": "AdaptiveMomentumCMAES", "description": "Adaptive CMA-ES with momentum and a rank-one update focused on exploiting promising regions by adjusting the step size and covariance matrix based on the gradient-like information.", "code": "import numpy as np\n\nclass AdaptiveMomentumCMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damp=None, c_mean=0.1, momentum=0.1, ortho_init=True):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.m = None\n        self.sigma = 0.3\n        self.C = np.eye(self.dim)\n        self.ps = None\n        self.chiN = None\n        self.cs = cs\n        self.damp = damp if damp is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1]) + self.cs\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.c_mean = c_mean\n        self.generation = 0\n        self.momentum = momentum\n        self.m_momentum = np.zeros(self.dim)\n        self.ortho_init = ortho_init\n\n\n    def orthogonal_initialization(self, func):\n        # Latin Hypercube Sampling for initial population\n        points = np.zeros((self.popsize, self.dim))\n        for i in range(self.dim):\n            points[:, i] = np.random.permutation(self.popsize)\n        points = (points + np.random.rand(self.popsize, self.dim)) / self.popsize\n        points = func.bounds.lb + points * (func.bounds.ub - func.bounds.lb)\n        return points\n\n    def __call__(self, func):\n        if self.ortho_init:\n            initial_pop = self.orthogonal_initialization(func)\n            self.m = np.mean(initial_pop, axis=0)\n        else:\n            self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1 / (4 * self.dim) + 1 / (21 * self.dim**2))\n\n        used_budget = 0\n        while used_budget < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize // 2))\n            z = np.concatenate((z, -z), axis=1)\n            x = self.m[:, np.newaxis] + self.sigma * np.dot(np.linalg.cholesky(self.C), z)\n            x = np.clip(x, func.bounds.lb[:, np.newaxis], func.bounds.ub[:, np.newaxis])\n\n            f = np.array([func(x[:, i]) if used_budget < self.budget else np.inf for i in range(self.popsize)])\n            used_budget += self.popsize\n\n            arindex = np.argsort(f)\n            xmu = x[:, arindex[:self.mu]]\n            zmu = z[:, arindex[:self.mu]]\n\n            if f[arindex[0]] < self.f_opt:\n                self.f_opt = f[arindex[0]]\n                self.x_opt = x[:, arindex[0]].copy()\n\n            # Momentum-based mean update\n            m_old = self.m.copy()\n            y = np.sum(xmu * self.weights[np.newaxis, :], axis=1) - m_old\n            self.m_momentum = self.momentum * self.m_momentum + (1 - self.momentum) * y\n            self.m = m_old + self.m_momentum\n\n            # Simplified Rank-One Update\n            y = self.m - m_old\n            self.C = (1 - self.c_mean) * self.C + self.c_mean * np.outer(y, y) / (self.sigma**2)\n\n            # Step-size adaptation\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs)) * np.sum(zmu * self.weights[np.newaxis, :], axis=1)\n            self.sigma *= np.exp(self.cs / self.damp * (np.linalg.norm(self.ps) / self.chiN - 1))\n            \n            self.generation += 1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveMomentumCMAES scored 0.369 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["730a8a9c-80ea-4f87-a25e-7de0eb80cf82"], "operator": null, "metadata": {"aucs": [0.0785791585129122, 0.1809672539777354, 0.9330981805436636, 0.15623640317968346, 0.22653521146376032, 0.1604472899337952, 0.23973012969149154, 0.16340806494598548, 0.3324360117270394, 0.16982652555902933, 0.8754974236018698, 0.1985693469956299, 0.5322380135538934, 0.29000954949736957, 0.955499032226951, 0.3177235040424563, 0.2643123885774159, 0.9608056838546247, 0.20234934226978174, 0.1471005546282813]}}
{"id": "273bf2c6-5533-469a-8495-50004f3f110b", "fitness": 0.35898844800136176, "name": "DE_CMAES_MirroredCauchy", "description": "Combines differential evolution with a CMA-ES-inspired mutation, and uses a mirrored sampling technique with a dynamically adjusted learning rate, incorporating a Cauchy mutation operator for enhanced exploration.", "code": "import numpy as np\n\nclass DE_CMAES_MirroredCauchy:\n    def __init__(self, budget=10000, dim=10, popsize=None, F=0.5, CR=0.7, sigma=0.1, learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 2 * self.dim\n        self.F = F\n        self.CR = CR\n        self.sigma = sigma\n        self.learning_rate = learning_rate\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = None  # Initialize population here\n        self.fitness = None\n\n    def cauchy_mutation(self, loc, scale, size):\n        return loc + scale * np.random.standard_cauchy(size=size)\n\n    def __call__(self, func):\n        bounds_lb = func.bounds.lb\n        bounds_ub = func.bounds.ub\n        self.population = np.random.uniform(bounds_lb, bounds_ub, size=(self.popsize, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        eval_count = self.popsize\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n\n        while eval_count < self.budget:\n            for i in range(self.popsize):\n                # DE mutation\n                idxs = np.random.choice(self.popsize, 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                mutant_de = self.population[i] + self.F * (x2 - x3)\n                mutant_de = np.clip(mutant_de, bounds_lb, bounds_ub)\n\n                # CMA-ES-like mutation with mirrored sampling\n                z = np.random.normal(0, 1, self.dim)\n                mutant_cmaes = self.population[i] + self.sigma * z\n                mutant_cmaes_mirrored = self.population[i] - self.sigma * z  # Mirrored sample\n\n                mutant_cmaes = np.clip(mutant_cmaes, bounds_lb, bounds_ub)\n                mutant_cmaes_mirrored = np.clip(mutant_cmaes_mirrored, bounds_lb, bounds_ub)\n\n\n                # Cauchy mutation for added exploration\n                mutant_cauchy = self.cauchy_mutation(self.population[i], self.sigma, self.dim)\n                mutant_cauchy = np.clip(mutant_cauchy, bounds_lb, bounds_ub)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.CR\n                trial_vector = np.where(crossover_mask, mutant_de, mutant_cmaes)\n                \n                # Selection between mirrored samples and Cauchy\n                f_trial = func(trial_vector)\n                eval_count += 1\n                f_mirrored = func(mutant_cmaes_mirrored)\n                eval_count += 1\n                f_cauchy = func(mutant_cauchy)\n                eval_count += 1\n\n                best_mutant = trial_vector\n                best_fitness = f_trial\n\n                if f_mirrored < best_fitness:\n                    best_mutant = mutant_cmaes_mirrored\n                    best_fitness = f_mirrored\n\n                if f_cauchy < best_fitness:\n                    best_mutant = mutant_cauchy\n                    best_fitness = f_cauchy\n\n\n                if best_fitness < self.fitness[i]:\n                    self.fitness[i] = best_fitness\n                    self.population[i] = best_mutant.copy()\n\n                    if best_fitness < self.f_opt:\n                        self.f_opt = best_fitness\n                        self.x_opt = best_mutant.copy()\n\n            #Dynamically adjust learning rate\n            self.sigma *= np.exp(self.learning_rate * (np.mean(self.fitness) - self.f_opt))\n\n            if eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm DE_CMAES_MirroredCauchy scored 0.359 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b633dbe7-c520-4d37-b25b-9f576cf8b935"], "operator": null, "metadata": {"aucs": [0.12658649828442847, 0.20661076017031577, 0.4243185483295311, 0.36480968532002267, 0.24804102007318207, 0.32466283169327725, 0.2677749099446358, 0.25970239393762096, 0.24779049233137607, 0.17357784835208645, 0.3989945103822936, 0.9952829702102836, 0.2604737094235843, 0.2715728245191933, 0.7403323603573149, 0.4386816452854778, 0.28726197535708287, 0.47603695189739137, 0.17464465413359942, 0.4926123700245366]}}
{"id": "c3efa8ef-d136-4686-b64f-3b826bf502a8", "fitness": 0.3257776238496146, "name": "CMAES_SimplifiedRankOneAnnealed", "description": "Introducing a learning rate annealing for the rank-one update and step-size adaptation, alongside a covariance matrix clipping to maintain positive definiteness and dynamic diversity damping.", "code": "import numpy as np\n\nclass CMAES_SimplifiedRankOneAnnealed:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damp=None, c_mean=0.1, ortho_init=True, diversity_factor=0.1, c_rank_one=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.m = None\n        self.sigma = 0.3\n        self.C = np.eye(self.dim)  # covariance matrix (initialized as identity)\n        self.ps = None # evolution path for sigma\n        self.chiN = None\n        self.cs = cs\n        self.damp = damp if damp is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1]) + self.cs\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.c_mean = c_mean # learning rate for mean update\n        self.generation = 0\n        self.ortho_init = ortho_init\n        self.diversity_factor = diversity_factor # Factor influencing damping based on diversity\n        self.c_rank_one = c_rank_one  # Learning rate for rank-one update\n        self.mean_old = None\n        self.c_rank_one_initial = c_rank_one\n        self.cs_initial = cs\n        self.min_eigval = 1e-10\n\n    def orthogonal_initialization(self, func):\n        # Latin Hypercube Sampling for initial population\n        points = np.zeros((self.popsize, self.dim))\n        for i in range(self.dim):\n            points[:, i] = np.random.permutation(self.popsize)\n        points = (points + np.random.rand(self.popsize, self.dim)) / self.popsize\n        points = func.bounds.lb + points * (func.bounds.ub - func.bounds.lb)\n        return points\n\n    def __call__(self, func):\n        if self.ortho_init:\n            initial_pop = self.orthogonal_initialization(func)\n            self.m = np.mean(initial_pop, axis=0)\n        else:\n            self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        \n        self.ps = np.zeros(self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1 / (4 * self.dim) + 1 / (21 * self.dim**2))\n        self.mean_old = self.m.copy()\n\n\n        used_budget = 0\n\n        while used_budget < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize // 2))\n            z = np.concatenate((z, -z), axis=1)  # Mirrored sampling\n            x = self.m[:, np.newaxis] + self.sigma * np.dot(np.linalg.cholesky(self.C), z)\n\n            # Repair mechanism to ensure solutions are within bounds\n            x = np.clip(x, func.bounds.lb[:, np.newaxis], func.bounds.ub[:, np.newaxis])\n\n            f = np.array([func(x[:, i]) if used_budget < self.budget else np.inf for i in range(self.popsize)])\n            used_budget += self.popsize\n\n            arindex = np.argsort(f)\n            xmu = x[:, arindex[:self.mu]]\n            zmu = z[:, arindex[:self.mu]]\n\n            if f[arindex[0]] < self.f_opt:\n                self.f_opt = f[arindex[0]]\n                self.x_opt = x[:, arindex[0]].copy()\n\n            # Mean update\n            m_old = self.m.copy()\n            self.m = np.sum(xmu * self.weights[np.newaxis, :], axis=1)\n\n            # Simplified Rank-One Update\n            y = self.m - m_old\n            self.C = (1 - self.c_rank_one) * self.C + self.c_rank_one * np.outer(y, y) / (self.sigma**2)\n\n            # Ensure positive definiteness by clipping eigenvalue\n            eigvals, eigvecs = np.linalg.eigh(self.C)\n            eigvals = np.clip(eigvals, self.min_eigval, None)\n            self.C = eigvecs @ np.diag(eigvals) @ eigvecs.T\n\n            # Step-size adaptation\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs)) * np.sum(zmu * self.weights[np.newaxis, :], axis=1)\n            self.sigma *= np.exp(self.cs / self.damp * (np.linalg.norm(self.ps) / self.chiN - 1))\n            \n            # Dynamic damping based on population diversity\n            diversity = np.std(xmu, axis=1).mean()  # Average standard deviation across dimensions\n            self.damp = 1 + 2 * np.max([0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1]) + self.cs + self.diversity_factor * diversity\n\n            # Annealing learning rates\n            self.c_rank_one = self.c_rank_one_initial * (1 - used_budget / self.budget)\n            self.cs = self.cs_initial * (1 - used_budget / self.budget)\n\n            self.generation += 1\n            self.mean_old = m_old\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm CMAES_SimplifiedRankOneAnnealed scored 0.326 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["730a8a9c-80ea-4f87-a25e-7de0eb80cf82"], "operator": null, "metadata": {"aucs": [0.05737606972506726, 0.17456213418503919, 0.9659357517135989, 0.194822562763601, 0.2488512872935219, 0.14992907689805757, 0.18613523125688936, 0.1890066487997738, 0.17288665498461175, 0.19467977276143067, 0.31616092835088216, 0.21842058175613155, 0.35337155957233446, 0.20996623368802103, 0.96512079050985, 0.35355233400689223, 0.2603780899103837, 0.9547729638306635, 0.18675442209088655, 0.1628693828946569]}}
{"id": "70263b77-c464-42d9-85af-b2a8f58ddf2a", "fitness": 0.5353686450445493, "name": "DE_CMAES_Simple", "description": "Hybridizes Differential Evolution with a simplified CMA-ES, using a rank-one covariance matrix update and adaptive exploration radius based on fitness improvement.", "code": "import numpy as np\n\nclass DE_CMAES_Simple:\n    def __init__(self, budget=10000, dim=10, popsize=None, F=0.5, CR=0.7, exploration_radius=0.1, radius_reduction_factor=0.99):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 * self.dim\n        self.F = F\n        self.CR = CR\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.exploration_radius = exploration_radius\n        self.radius_reduction_factor = radius_reduction_factor\n        self.eval_count = 0\n        self.mean = None\n\n    def __call__(self, func):\n        bounds_lb = func.bounds.lb\n        bounds_ub = func.bounds.ub\n        self.population = np.random.uniform(bounds_lb, bounds_ub, size=(self.popsize, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.eval_count = self.popsize\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = self.population[np.argmin(fitness)].copy()\n        self.mean = np.mean(self.population, axis=0)\n\n        C = np.eye(self.dim) # Simplified covariance matrix, initially identity\n        \n        while self.eval_count < self.budget:\n            for i in range(self.popsize):\n                # DE mutation\n                idxs = np.random.choice(self.popsize, 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                mutant_de = self.population[i] + self.F * (x2 - x3)\n                mutant_de = np.clip(mutant_de, bounds_lb, bounds_ub)\n\n                # CMA-ES-like exploration, simplified with exploration radius\n                z = np.random.normal(0, 0, self.dim)\n                mutant_cmaes = self.population[i] + self.exploration_radius * np.dot(C,z)\n\n                mutant_cmaes = np.clip(mutant_cmaes, bounds_lb, bounds_ub)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.CR\n                trial_vector = np.where(crossover_mask, mutant_de, mutant_cmaes)\n\n                # Selection\n                f_trial = func(trial_vector)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    self.population[i] = trial_vector.copy()\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial_vector.copy()\n                        self.exploration_radius *= self.radius_reduction_factor\n                \n            self.mean = np.mean(self.population, axis=0)\n            #Simplified Rank-one update: Adjust C based on the best individual's deviation from the mean\n            best_index = np.argmin(fitness)\n            diff = self.population[best_index] - self.mean\n            C = np.outer(diff, diff) / np.linalg.norm(diff)**2 #Simplified rank-one update\n\n            if self.eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm DE_CMAES_Simple scored 0.535 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a68926ad-c63c-47a4-a3ac-f4de3adc2785"], "operator": null, "metadata": {"aucs": [0.1678690042404758, 0.3422501180640828, 0.5485296334110108, 0.7920015034461437, 0.5247001335304404, 0.722071598740625, 0.33720677721967074, 0.4879922073448103, 0.6083031087742244, 0.18356686588216808, 0.7873108736144228, 0.9950639671508086, 0.37348929169914025, 0.41440960982625585, 0.835844909328156, 0.6520431354887828, 0.4254390769234827, 0.805900155585427, 0.19581266070665504, 0.5075682699142007]}}
{"id": "27b44929-a7e0-4556-b09c-02ddb2082607", "fitness": 0.5212226684391573, "name": "AdaptiveDEwithLocalSearch", "description": "Adaptive Differential Evolution with Local Search, using a success rate-based adjustment of DE parameters and a local search around the best solution to refine the optimum.", "code": "import numpy as np\n\nclass AdaptiveDEwithLocalSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=None, F_init=0.5, Cr_init=0.9, local_search_iterations=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 * dim if pop_size is None else pop_size\n        self.F_init = F_init  # Initial mutation factor\n        self.Cr_init = Cr_init  # Initial crossover rate\n        self.pop = None\n        self.fitness = None\n        self.F = None\n        self.Cr = None\n        self.success_rate = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.learning_rate = 0.1\n        self.local_search_iterations = local_search_iterations\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.pop[best_idx].copy()\n        self.F = np.full(self.pop_size, self.F_init)\n        self.Cr = np.full(self.pop_size, self.Cr_init)\n        self.success_rate = np.zeros(self.pop_size)\n\n    def mutate(self, pop, i):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n        return a + self.F[i] * (b - c)\n\n    def crossover(self, target, mutant, i):\n        cross_points = np.random.rand(self.dim) < self.Cr[i]\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def update_parameters(self, i, success):\n        self.success_rate[i] = (1 - self.learning_rate) * self.success_rate[i] + self.learning_rate * success\n\n        # Adjust F and Cr based on success rate\n        if self.success_rate[i] > 0.3:  # Increased threshold\n            self.F[i] = min(self.F[i] * (1 + self.learning_rate), 1.0)\n            self.Cr[i] = min(self.Cr[i] * (1 + self.learning_rate), 1.0)\n        else:\n            self.F[i] = max(self.F[i] * (1 - self.learning_rate), 0.1)\n            self.Cr[i] = max(self.Cr[i] * (1 - self.learning_rate), 0.1)\n\n    def local_search(self, func):\n        # Perform local search around the best solution\n        x_current = self.x_opt.copy()\n        f_current = self.f_opt\n        for _ in range(self.local_search_iterations):\n            # Generate a random perturbation\n            perturbation = np.random.normal(0, 0.05, size=self.dim)  # Smaller step size\n            x_new = x_current + perturbation\n            x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n\n            f_new = func(x_new)\n            self.eval_count += 1\n\n            if f_new < f_current:\n                f_current = f_new\n                x_current = x_new\n                self.f_opt = f_new\n                self.x_opt = x_new.copy()  # Update global best\n            if self.eval_count >= self.budget:\n                break\n\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            for i in range(len(self.pop)):\n                target = self.pop[i]\n                mutant = self.mutate(self.pop, i)\n                trial = self.crossover(target, mutant, i)\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < self.fitness[i]:\n                    self.update_parameters(i, 1)\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial.copy()\n                else:\n                    self.update_parameters(i, 0)\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Perform local search periodically\n            if self.eval_count < self.budget:\n                self.local_search(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveDEwithLocalSearch scored 0.521 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9a4b8561-2d1a-4f51-b8a4-8c86afee61e7"], "operator": null, "metadata": {"aucs": [0.15756913482058166, 0.2348732655359318, 0.4482388534589514, 0.8498646391435594, 0.47510744913686265, 0.6568597518771992, 0.3964223085563504, 0.4608079444829526, 0.5528170332641259, 0.3236789555354438, 0.6582441076162073, 0.9869875962091572, 0.28811196548433926, 0.3867926706903748, 0.8807898765653017, 0.6933606460986645, 0.4092316359922643, 0.7445461044167329, 0.30530588623934896, 0.5148435436587951]}}
{"id": "487f9ccd-5d9c-4b8a-9190-082358fad4d9", "fitness": 0.49117440256102435, "name": "DE_CMAES_Adaptive", "description": "Combines DE and CMA-ES with adaptive learning rates for both covariance matrix and step size, along with a more robust covariance matrix adaptation.", "code": "import numpy as np\n\nclass DE_CMAES_Adaptive:\n    def __init__(self, budget=10000, dim=10, popsize=None, F=0.5, CR=0.7, cma_sigma=0.1, cma_lr_sigma=0.1, cma_lr_C=0.1, initial_lambda=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 * self.dim\n        self.F = F\n        self.CR = CR\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.cma_sigma = cma_sigma\n        self.cma_C = np.eye(dim)\n        self.cma_D = np.ones(dim)\n        self.cma_mu = self.popsize // 2\n        self.cma_weights = np.log(self.cma_mu + 1/2) - np.log(np.arange(1, self.cma_mu + 1))\n        self.cma_weights /= np.sum(self.cma_weights)\n        self.cma_mueff = np.sum(self.cma_weights)**2 / np.sum(self.cma_weights**2)\n        self.cma_cs = (self.cma_mueff + 2) / (self.dim + self.cma_mueff + 5)\n        self.cma_damps = 1 + 2 * max(0, np.sqrt((self.cma_mueff - 1)/(self.dim + 1)) - 1) + self.cma_cs\n        self.cma_lr_sigma = cma_lr_sigma\n        self.cma_lr_C = cma_lr_C\n        self.initial_lambda = initial_lambda # Scaling factor for initial exploration\n\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1/(4 * self.dim)) + 1/(21 * self.dim**2))\n        self.eval_count = 0\n\n\n    def __call__(self, func):\n        bounds_lb = func.bounds.lb\n        bounds_ub = func.bounds.ub\n        self.population = np.random.uniform(bounds_lb, bounds_ub, size=(self.popsize, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.eval_count = self.popsize\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = self.population[np.argmin(fitness)].copy()\n\n        # Initial exploration with larger step size\n        for i in range(self.popsize):\n            z = np.random.normal(0, 1, self.dim)\n            mutant = self.population[i] + self.initial_lambda * self.cma_sigma * np.dot(self.cma_C, z)\n            mutant = np.clip(mutant, bounds_lb, bounds_ub)\n            f_mutant = func(mutant)\n            self.eval_count += 1\n\n            if f_mutant < fitness[i]:\n                fitness[i] = f_mutant\n                self.population[i] = mutant.copy()\n\n                if f_mutant < self.f_opt:\n                    self.f_opt = f_mutant\n                    self.x_opt = mutant.copy()\n\n        while self.eval_count < self.budget:\n            # Adapt learning rates dynamically based on progress\n            success_ratio = np.mean(fitness < np.mean(fitness))\n            self.cma_lr_sigma = min(0.4, success_ratio + 0.1) # Increase lr if more success\n            self.cma_lr_C = min(0.4, success_ratio + 0.1)\n\n            for i in range(self.popsize):\n                # DE mutation\n                idxs = np.random.choice(self.popsize, 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                mutant_de = self.population[i] + self.F * (x2 - x3)\n                mutant_de = np.clip(mutant_de, bounds_lb, bounds_ub)\n\n                # CMA-ES-like mutation\n                z = np.random.normal(0, 1, self.dim)\n                mutant_cmaes = self.population[i] + self.cma_sigma * np.dot(self.cma_C, z)\n                mutant_cmaes = np.clip(mutant_cmaes, bounds_lb, bounds_ub)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.CR\n                trial_vector = np.where(crossover_mask, mutant_de, mutant_cmaes)\n\n                # Selection\n                f_trial = func(trial_vector)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    self.population[i] = trial_vector.copy()\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial_vector.copy()\n\n            #Adapt CMA-ES parameters\n            indices = np.argsort(fitness)\n            xmean = np.sum(self.population[indices[:self.cma_mu]].T * self.cma_weights, axis=1)\n\n            # Update evolution paths\n            self.ps = (1 - self.cma_cs) * self.ps + np.sqrt(self.cma_cs * (2 - self.cma_cs) * self.cma_mueff) * (xmean - np.mean(self.population, axis=0)) / self.cma_sigma\n            self.pc = (1 - self.cma_cs) * self.pc + np.sqrt(self.cma_cs * (2 - self.cma_cs) * self.cma_mueff) * (xmean - np.mean(self.population, axis=0)) / self.cma_sigma # Corrected this line\n\n\n            # Robust covariance matrix adaptation using rank-one update and damping\n            delta = (xmean - np.mean(self.population, axis=0)) / self.cma_sigma\n            self.cma_C = (1 - self.cma_lr_C) * self.cma_C + self.cma_lr_C * (np.outer(self.pc, self.pc) + np.eye(self.dim) * 1e-8)  # Rank-one update with regularization\n\n\n            # Adapt step size\n            self.cma_sigma *= np.exp((self.cma_lr_sigma / self.cma_damps) * (np.linalg.norm(self.ps)/ self.chiN -1))\n\n            if self.eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm DE_CMAES_Adaptive scored 0.491 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a68926ad-c63c-47a4-a3ac-f4de3adc2785"], "operator": null, "metadata": {"aucs": [0.16896473873742957, 0.31200244345522765, 0.46040955464155253, 0.6694611970962618, 0.3824608385575523, 0.6991339699842058, 0.3140303045152807, 0.4614060527974375, 0.44131847429258353, 0.23174463778304133, 0.7848736527931552, 0.9942709366359904, 0.3430105599338117, 0.3502545894289202, 0.8181696166572221, 0.5573502295918447, 0.3797786542528957, 0.7659584196425389, 0.19399153749295928, 0.4948976429305769]}}
{"id": "53dc6e88-b712-43e1-8e9a-821a888d6474", "fitness": 0.6276000858047269, "name": "DE_CMAES_Improved_PopReduce", "description": "Integrates a Population Reduction strategy and adaptive learning rate adjustments within the DE-CMAES hybrid framework to dynamically balance exploration and exploitation.", "code": "import numpy as np\n\nclass DE_CMAES_Improved_PopReduce:\n    def __init__(self, budget=10000, dim=10, popsize=None, F=0.5, CR=0.7, cma_sigma=0.1, cma_lr_sigma=0.1, cma_lr_C=0.1, reduction_factor=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 * self.dim\n        self.F = F\n        self.CR = CR\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.cma_sigma = cma_sigma\n        self.cma_C = np.eye(dim)\n        self.cma_D = np.ones(dim)\n        self.cma_mu = self.popsize // 2\n        self.cma_weights = np.log(self.cma_mu + 1/2) - np.log(np.arange(1, self.cma_mu + 1))\n        self.cma_weights /= np.sum(self.cma_weights)\n        self.cma_mueff = np.sum(self.cma_weights)**2 / np.sum(self.cma_weights**2)\n        self.cma_cs = (self.cma_mueff + 2) / (self.dim + self.cma_mueff + 5)\n        self.cma_damps = 1 + 2 * max(0, np.sqrt((self.cma_mueff - 1)/(self.dim + 1)) - 1) + self.cma_cs\n        self.cma_lr_sigma = cma_lr_sigma\n        self.cma_lr_C = cma_lr_C\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1/(4 * self.dim)) + 1/(21 * self.dim**2))\n        self.eval_count = 0\n        self.reduction_factor = reduction_factor\n\n    def __call__(self, func):\n        bounds_lb = func.bounds.lb\n        bounds_ub = func.bounds.ub\n        self.population = np.random.uniform(bounds_lb, bounds_ub, size=(self.popsize, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.eval_count = self.popsize\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = self.population[np.argmin(fitness)].copy()\n\n        generation = 0\n        while self.eval_count < self.budget:\n            generation += 1\n            for i in range(self.popsize):\n                # DE mutation\n                idxs = np.random.choice(self.popsize, 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                mutant_de = self.population[i] + self.F * (x2 - x3)\n                mutant_de = np.clip(mutant_de, bounds_lb, bounds_ub)\n\n                # CMA-ES-like mutation\n                z = np.random.normal(0, 1, self.dim)\n                mutant_cmaes = self.population[i] + self.cma_sigma * np.dot(self.cma_C, z)\n                mutant_cmaes = np.clip(mutant_cmaes, bounds_lb, bounds_ub)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.CR\n                trial_vector = np.where(crossover_mask, mutant_de, mutant_cmaes)\n\n                # Selection\n                f_trial = func(trial_vector)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    self.population[i] = trial_vector.copy()\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial_vector.copy()\n            \n            #Adapt CMA-ES parameters\n            indices = np.argsort(fitness)\n            xmean = np.sum(self.population[indices[:self.cma_mu]].T * self.cma_weights, axis=1)\n\n            # Update evolution paths\n            self.ps = (1 - self.cma_cs) * self.ps + np.sqrt(self.cma_cs * (2 - self.cma_cs) * self.cma_mueff) * (xmean - np.mean(self.population, axis=0)) / self.cma_sigma\n            self.pc = (1 - self.cma_cs) * self.pc + np.sqrt(self.cma_cs * (2 - self.cma_cs) * self.cma_mueff) * (xmean - np.mean(self.population, axis=0))\n\n            # Update covariance matrix\n            delta = (xmean - np.mean(self.population, axis=0)) / self.cma_sigma\n            self.cma_C = (1 - self.cma_lr_C) * self.cma_C + self.cma_lr_C * (np.outer(self.pc, self.pc) + self.cma_lr_C * np.eye(self.dim) ) # Rank-one update\n\n            # Adapt step size\n            self.cma_sigma *= np.exp((self.cma_lr_sigma / self.cma_damps) * (np.linalg.norm(self.ps)/ self.chiN -1))\n\n            # Population Reduction\n            if generation % 10 == 0 and self.popsize > 2 * self.dim:\n                new_popsize = int(self.popsize * self.reduction_factor)\n                best_indices = np.argsort(fitness)[:new_popsize]\n                self.population = self.population[best_indices]\n                fitness = fitness[best_indices]\n                self.popsize = new_popsize\n                self.cma_mu = self.popsize // 2\n                self.cma_weights = np.log(self.cma_mu + 1/2) - np.log(np.arange(1, self.cma_mu + 1))\n                self.cma_weights /= np.sum(self.cma_weights)\n                self.cma_mueff = np.sum(self.cma_weights)**2 / np.sum(self.cma_weights**2)\n                self.cma_cs = (self.cma_mueff + 2) / (self.dim + self.cma_mueff + 5)\n                self.cma_damps = 1 + 2 * max(0, np.sqrt((self.cma_mueff - 1)/(self.dim + 1)) - 1) + self.cma_cs\n\n                # Adjust learning rates (optional, but can be helpful)\n                self.cma_lr_sigma *= 0.9  # Reduce learning rate for sigma\n                self.cma_lr_C *= 0.9     # Reduce learning rate for C\n\n\n            if self.eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm DE_CMAES_Improved_PopReduce scored 0.628 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a68926ad-c63c-47a4-a3ac-f4de3adc2785"], "operator": null, "metadata": {"aucs": [0.1745698307377339, 0.607811368297394, 0.7438727755692551, 0.8562701205948083, 0.7073599580593698, 0.8362795992350397, 0.32353475935068576, 0.668396411252418, 0.7298927761818792, 0.46076381248955167, 0.8455404980268021, 1.0, 0.4912419082070776, 0.5013336829869182, 0.7900459523399198, 0.7928539374132939, 0.5017773477676304, 0.8633943619612217, 0.1680231929406003, 0.48903942268293976]}}
{"id": "7e9856cc-2973-4f5c-b104-b40f9d91f3cd", "fitness": 0.6872982780082142, "name": "DE_CMAES_AdaptiveRepair_Enhanced", "description": "Enhanced DE-CMAES with adaptive repair, stagnation detection, and covariance matrix adaptation focusing on rank-one updates and path-based adaptation for faster convergence.", "code": "import numpy as np\n\nclass DE_CMAES_AdaptiveRepair_Enhanced:\n    def __init__(self, budget=10000, dim=10, popsize=None, F=0.5, CR=0.7, cma_sigma=0.1, repair_prob=0.1, stagnation_threshold=1e-6, stagnation_iter=50):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 2 * self.dim\n        self.F = F\n        self.CR = CR\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.cma_sigma = cma_sigma\n        self.cma_C = np.eye(dim)\n        self.cma_D = np.ones(dim)\n        self.cma_mu = self.popsize // 2\n        self.cma_weights = np.log(self.cma_mu + 1/2) - np.log(np.arange(1, self.cma_mu + 1))\n        self.cma_weights /= np.sum(self.cma_weights)\n        self.cma_mueff = np.sum(self.cma_weights)**2 / np.sum(self.cma_weights**2)\n        self.cma_cs = (self.cma_mueff + 2) / (self.dim + self.cma_mueff + 5)\n        self.cma_damps = 1 + 2 * max(0, np.sqrt((self.cma_mueff - 1)/(self.dim + 1)) - 1) + self.cma_cs\n        self.repair_prob = repair_prob\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_iter = stagnation_iter\n\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1/(4 * self.dim)) + 1/(21 * self.dim**2))\n\n        self.F_adaptive = self.F * np.ones(self.popsize)\n        self.CR_adaptive = self.CR * np.ones(self.popsize)\n\n        self.last_improvement = 0\n        self.best_fitness_history = []\n\n\n\n    def __call__(self, func):\n        bounds_lb = func.bounds.lb\n        bounds_ub = func.bounds.ub\n        self.population = np.random.uniform(bounds_lb, bounds_ub, size=(self.popsize, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        eval_count = self.popsize\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = self.population[np.argmin(fitness)].copy()\n        self.last_improvement = 0\n        self.best_fitness_history = [self.f_opt]\n\n        while eval_count < self.budget:\n            mean_pop = np.mean(self.population, axis=0)\n\n            for i in range(self.popsize):\n                # Adaptive F and CR\n                self.F_adaptive[i] = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n                self.CR_adaptive[i] = np.clip(np.random.normal(self.CR, 0.1), 0.1, 0.9)\n\n                # DE mutation\n                idxs = np.random.choice(self.popsize, 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                mutant_de = self.population[i] + self.F_adaptive[i] * (x2 - x3)\n                mutant_de = np.clip(mutant_de, bounds_lb, bounds_ub)\n\n                # CMA-ES-like mutation\n                z = np.random.normal(0, 1, self.dim)\n                mutant_cmaes = self.population[i] + self.cma_sigma * np.dot(self.cma_C, z)  # Use covariance matrix directly\n\n                mutant_cmaes = np.clip(mutant_cmaes, bounds_lb, bounds_ub)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.CR_adaptive[i]\n                trial_vector = np.where(crossover_mask, mutant_de, mutant_cmaes)\n                \n                # Repair Mechanism\n                if np.random.rand() < self.repair_prob:\n                    trial_vector = 0.5 * (trial_vector + mean_pop)\n\n                trial_vector = np.clip(trial_vector, bounds_lb, bounds_ub)\n\n\n                # Selection\n                f_trial = func(trial_vector)\n                eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    self.population[i] = trial_vector.copy()\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial_vector.copy()\n                        self.last_improvement = eval_count\n            \n            #Adapt CMA-ES parameters\n            indices = np.argsort(fitness)\n            xmean = np.sum(self.population[indices[:self.cma_mu]].T * self.cma_weights, axis=1)\n\n            y = (xmean - np.mean(self.population, axis=0)) / self.cma_sigma\n            self.ps = (1 - self.cma_cs) * self.ps + np.sqrt(self.cma_cs * (2 - self.cma_cs) * self.cma_mueff) * y\n            self.pc = (1 - self.cma_cs) * self.pc + np.sqrt(self.cma_cs * (2 - self.cma_cs) * self.cma_mueff) * y\n\n            self.cma_C = (1 - self.cma_cs) * self.cma_C + self.cma_cs * np.outer(self.pc, self.pc) # Rank-one update\n\n            self.cma_sigma *= np.exp((self.cma_cs / self.cma_damps) * (np.linalg.norm(self.ps)/ self.chiN -1))\n\n            self.best_fitness_history.append(self.f_opt)\n            if eval_count - self.last_improvement > self.stagnation_iter:\n                 self.cma_sigma *= 0.8  # Reduce step size upon stagnation\n                 self.F *= 0.9         # Reduce exploration\n                 self.CR *= 0.9         # Reduce exploration\n                 self.last_improvement = eval_count\n            if eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm DE_CMAES_AdaptiveRepair_Enhanced scored 0.687 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b633dbe7-c520-4d37-b25b-9f576cf8b935"], "operator": null, "metadata": {"aucs": [0.18504458499862453, 0.6914097772941383, 0.7242224974958144, 0.9241720129438699, 0.8325654715219882, 0.8951742490215961, 0.3370462969795409, 0.737518477534242, 0.8780757364471653, 0.8119916360603443, 0.9207876581691148, 0.998536448219818, 0.5323890025490392, 0.3775425111593629, 0.8851218945247933, 0.8669081488859027, 0.527644632245969, 0.9207207127309852, 0.19847555188384203, 0.5006182594981303]}}
{"id": "84756697-f7a9-40a9-9f0b-15ce51d3f919", "fitness": 0.3003772842411757, "name": "AdaptiveNeighborhoodDE", "description": "Adaptive Differential Evolution with Neighborhood-Based Parameter Adaptation and a Restart Mechanism to escape local optima.", "code": "import numpy as np\n\nclass AdaptiveNeighborhoodDE:\n    def __init__(self, budget=10000, dim=10, pop_size=None, F_init=0.5, Cr_init=0.9, neighborhood_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 10 * dim if pop_size is None else pop_size\n        self.F_init = F_init\n        self.Cr_init = Cr_init\n        self.neighborhood_size = neighborhood_size\n        self.pop = None\n        self.fitness = None\n        self.F = None\n        self.Cr = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.learning_rate = 0.1\n        self.restart_trigger = 100  # Trigger restart after this many iterations without improvement\n        self.no_improvement_count = 0\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.pop[best_idx].copy()\n        self.F = np.full(self.pop_size, self.F_init)\n        self.Cr = np.full(self.pop_size, self.Cr_init)\n\n    def mutate(self, pop, i):\n        # Select three distinct individuals from the neighborhood\n        neighborhood_indices = np.random.choice(self.pop_size, self.neighborhood_size, replace=False)\n        neighborhood = pop[neighborhood_indices]\n        \n        idxs = np.random.choice(self.neighborhood_size, 3, replace=False)\n        a, b, c = neighborhood[idxs] # use neighborhood\n\n        return a + self.F[i] * (b - c)\n\n    def crossover(self, target, mutant, i):\n        cross_points = np.random.rand(self.dim) < self.Cr[i]\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def update_parameters(self, i, f_trial, f_target):\n        # Adaptive F and Cr based on performance\n        if f_trial < f_target:\n            self.F[i] = max(0.1, self.F[i] - self.learning_rate)  # Reduce F if trial is better\n            self.Cr[i] = min(1.0, self.Cr[i] + self.learning_rate)  # Increase Cr if trial is better\n        else:\n            self.F[i] = min(1.0, self.F[i] + self.learning_rate)  # Increase F if trial is worse\n            self.Cr[i] = max(0.1, self.Cr[i] - self.learning_rate)  # Reduce Cr if trial is worse\n            \n    def restart_population(self, func):\n        # Reset a portion of the population and parameters if no improvement\n        num_reset = int(0.2 * self.pop_size)  # Reset 20% of the population\n        reset_indices = np.random.choice(self.pop_size, num_reset, replace=False)\n        self.pop[reset_indices] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(num_reset, self.dim))\n        self.fitness[reset_indices] = np.array([func(x) for x in self.pop[reset_indices]])\n        self.eval_count += num_reset\n        self.F[reset_indices] = self.F_init  # Reset F values\n        self.Cr[reset_indices] = self.Cr_init  # Reset Cr values\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            for i in range(len(self.pop)):\n                target = self.pop[i]\n                mutant = self.mutate(self.pop, i)\n                trial = self.crossover(target, mutant, i)\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < self.fitness[i]:\n                    self.update_parameters(i, f_trial, self.fitness[i])\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial.copy()\n                        self.no_improvement_count = 0  # Reset counter\n                    else:\n                        self.no_improvement_count += 1\n\n                else:\n                    self.update_parameters(i, f_trial, self.fitness[i])\n                    self.no_improvement_count += 1\n\n                if self.eval_count >= self.budget:\n                    break\n\n            if self.no_improvement_count > self.restart_trigger:\n                self.restart_population(func)\n                self.no_improvement_count = 0\n                best_idx = np.argmin(self.fitness)\n                self.f_opt = self.fitness[best_idx]\n                self.x_opt = self.pop[best_idx].copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveNeighborhoodDE scored 0.300 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9a4b8561-2d1a-4f51-b8a4-8c86afee61e7"], "operator": null, "metadata": {"aucs": [0.17224438466522785, 0.26605343125799963, 0.4276621999626108, 0.6359264053200404, 0]}}
{"id": "83d81c6e-17e2-4ece-a71a-dda7d24b58a9", "fitness": 0.38835907226336064, "name": "CMAES_SimplifiedRankOneAdaptiveSmoothing", "description": "CMA-ES with simplified rank-one update, mirrored sampling, dynamic damping, and adaptive covariance matrix smoothing to balance exploration and exploitation.", "code": "import numpy as np\n\nclass CMAES_SimplifiedRankOneAdaptiveSmoothing:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damp=None, c_mean=0.1, ortho_init=True, diversity_factor=0.1, c_rank_one=0.1, initial_sigma=0.3):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.m = None\n        self.sigma = initial_sigma\n        self.C = np.eye(self.dim)  # covariance matrix (initialized as identity)\n        self.ps = None # evolution path for sigma\n        self.chiN = None\n        self.cs = cs\n        self.damp = damp if damp is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1]) + self.cs\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.c_mean = c_mean # learning rate for mean update\n        self.generation = 0\n        self.ortho_init = ortho_init\n        self.diversity_factor = diversity_factor # Factor influencing damping based on diversity\n        self.c_rank_one = c_rank_one # Learning rate for rank-one update\n        self.mean_old = None\n        self.c_smoothing = 0.1 # Smoothing factor for covariance matrix\n\n\n    def orthogonal_initialization(self, func):\n        # Latin Hypercube Sampling for initial population\n        points = np.zeros((self.popsize, self.dim))\n        for i in range(self.dim):\n            points[:, i] = np.random.permutation(self.popsize)\n        points = (points + np.random.rand(self.popsize, self.dim)) / self.popsize\n        points = func.bounds.lb + points * (func.bounds.ub - func.bounds.lb)\n        return points\n\n    def __call__(self, func):\n        if self.ortho_init:\n            initial_pop = self.orthogonal_initialization(func)\n            self.m = np.mean(initial_pop, axis=0)\n        else:\n            self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        \n        self.ps = np.zeros(self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1 / (4 * self.dim) + 1 / (21 * self.dim**2))\n        self.mean_old = self.m.copy()\n\n\n        used_budget = 0\n\n        while used_budget < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize // 2))\n            z = np.concatenate((z, -z), axis=1)  # Mirrored sampling\n            x = self.m[:, np.newaxis] + self.sigma * np.dot(np.linalg.cholesky(self.C), z)\n\n            # Repair mechanism to ensure solutions are within bounds\n            x = np.clip(x, func.bounds.lb[:, np.newaxis], func.bounds.ub[:, np.newaxis])\n\n            f = np.array([func(x[:, i]) if used_budget < self.budget else np.inf for i in range(self.popsize)])\n            used_budget += self.popsize\n\n            arindex = np.argsort(f)\n            xmu = x[:, arindex[:self.mu]]\n            zmu = z[:, arindex[:self.mu]]\n\n            if f[arindex[0]] < self.f_opt:\n                self.f_opt = f[arindex[0]]\n                self.x_opt = x[:, arindex[0]].copy()\n\n            # Mean update\n            m_old = self.m.copy()\n            self.m = np.sum(xmu * self.weights[np.newaxis, :], axis=1)\n\n            # Simplified Rank-One Update\n            y = self.m - m_old\n            rank_one_update = np.outer(y, y) / (self.sigma**2)\n            self.C = (1 - self.c_rank_one) * self.C + self.c_rank_one * rank_one_update\n\n            # Covariance matrix smoothing\n            self.C = (1 - self.c_smoothing) * self.C + self.c_smoothing * np.eye(self.dim)\n\n            # Step-size adaptation\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs)) * np.sum(zmu * self.weights[np.newaxis, :], axis=1)\n            self.sigma *= np.exp(self.cs / self.damp * (np.linalg.norm(self.ps) / self.chiN - 1))\n            \n            # Dynamic damping based on population diversity\n            diversity = np.std(xmu, axis=1).mean()  # Average standard deviation across dimensions\n            self.damp = 1 + 2 * np.max([0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1]) + self.cs + self.diversity_factor * diversity\n\n            self.generation += 1\n            self.mean_old = m_old\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm CMAES_SimplifiedRankOneAdaptiveSmoothing scored 0.388 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["730a8a9c-80ea-4f87-a25e-7de0eb80cf82"], "operator": null, "metadata": {"aucs": [0.06737832297198254, 0.18867008995638568, 0.9587446570917736, 0.2571768348384652, 0.250312102992685, 0.15661256555282288, 0.3645649458407111, 0.954635937046011, 0.9595200264643114, 0.16247191575026376, 0.25825050395494487, 0.21737919143760398, 0.24462253642278242, 0.5787351554366746, 0.17725073195497953, 0.3231712819313377, 0.3058758191676798, 0.9732099989193659, 0.19158078575721715, 0.17701804177921565]}}
