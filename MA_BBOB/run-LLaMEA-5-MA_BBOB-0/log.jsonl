{"id": "961f4b37-b0e6-4e7d-a22f-de92d1a87b07", "fitness": -Infinity, "name": "CMAES", "description": "Covariance Matrix Adaptation Evolution Strategy with resampling and adaptive step size control.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs = 0.3, damps = 1,\n                 cc = 0.1, c1 = 0.1, cmu = 0.1):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = cs\n        self.damps = damps + 2 * max(0, np.sqrt((self.mueff - 1)/(self.dim + 1)) - 1) + self.cs\n        self.cc = cc\n        self.c1 = c1 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1-self.c1, cmu / ((self.dim + 1.5)**2 + self.mueff))\n        self.chiN = self.dim**0.5 * (1 - 1/(4*self.dim) + 1/(21*self.dim**2))\n\n    def __call__(self, func):\n        # Initialize variables\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = 0.3 * (func.bounds.ub - func.bounds.lb)\n        C = np.eye(self.dim)\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        f_opt = np.Inf\n        x_opt = None\n        evals = 0\n\n        while evals < self.budget:\n            # Generate samples\n            z = np.random.randn(self.dim, self.popsize)\n            C_sqrt = np.linalg.cholesky(C)\n            x = mean[:, np.newaxis] + sigma * C_sqrt @ z\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n\n            # Evaluate samples\n            f = np.array([func(x[:,i]) for i in range(self.popsize)])\n            evals += self.popsize\n\n            # Sort by fitness\n            idx = np.argsort(f)\n            f = f[idx]\n            x = x[:, idx]\n\n            # Update optimal solution\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[:, 0]\n\n            # Update mean\n            mean_old = mean.copy()\n            mean = np.sum(self.weights * x[:, :self.mu], axis=1)\n\n            # Update evolution path\n            ps = (1 - self.cs) * ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt, (mean - mean_old) / sigma)\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - self.cs)**(2 * evals / self.popsize)) < self.chiN * (1.4 + 2/(self.dim + 1))\n            pc = (1 - self.cc) * pc + hsig * np.sqrt(self.cc * (2 - self.cc) * self.mueff) * (mean - mean_old) / sigma\n\n            # Update covariance matrix\n            C = (1 - self.c1 - self.cmu) * C + self.c1 * (pc[:, np.newaxis] @ pc[np.newaxis, :]) + \\\n                self.cmu * np.sum(self.weights[i] * ((x[:, i] - mean_old)[:, np.newaxis] @ (x[:, i] - mean_old)[np.newaxis, :]) / sigma**2 for i in range(self.mu))\n            \n            # Ensure positive definiteness\n            C = np.triu(C) + np.triu(C, 1).T\n            C = np.real(C)\n\n            # Update step size\n            sigma = sigma * np.exp((self.cs/self.damps) * (np.linalg.norm(ps)/self.chiN - 1))\n            sigma = max(sigma, 1e-10 * (func.bounds.ub - func.bounds.lb)) # prevent sigma from becoming too small\n\n            # Resample if C becomes ill-conditioned\n            if not np.all(np.linalg.eigvalsh(C) > 0):\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n\n        return f_opt, x_opt", "configspace": "", "generation": 0, "feedback": "An exception occurred: operands could not be broadcast together with shapes (2,6) (2,) (2,) .", "error": "", "parent_ids": [], "operator": null, "metadata": {}}
{"id": "84b0da67-42d7-44d1-966e-b712fb525c34", "fitness": -Infinity, "name": "CMAES", "description": "Covariance Matrix Adaptation Evolution Strategy with a restart mechanism based on stagnation detection and adaptive population sizing.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damps=None):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.cs = cs\n        self.damps = damps if damps is not None else 1 + 2 * np.max([0, np.log(self.dim / 2 + 1)])\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cc = (4 + self.mueff/self.dim) / (self.dim + 4 + 2*self.mueff/self.dim)\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = np.min([1-self.c1, 2 * (self.mueff-2+1/self.mueff) / ((self.dim+2.3)**2 + self.mueff)])\n        self.chiN = self.dim**0.5 * (1 - 1/(4*self.dim) + 1/(21*self.dim**2))\n        self.count = 0\n        self.restarts = 0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 50  # Number of iterations before considering stagnation\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.m = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        B, D = np.linalg.eig(self.C)\n        D = np.sqrt(D)\n        self.B = B\n        self.D = D\n\n        while self.count < self.budget:\n            # Sample population\n            z = np.random.randn(self.dim, self.popsize)\n            x = self.m[:, np.newaxis] + self.sigma * (self.B @ np.diag(self.D) @ z)\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            f = np.array([func(x[:,i]) for i in range(self.popsize)])\n            self.count += self.popsize\n\n            # Sort by fitness\n            idx = np.argsort(f)\n            x = x[:, idx]\n            f = f[idx]\n\n            # Update optimal solution\n            if f[0] < self.f_opt:\n                self.f_opt = f[0]\n                self.x_opt = x[:, 0]\n                self.stagnation_counter = 0 # Reset stagnation counter\n            else:\n                self.stagnation_counter += 1\n\n            # Update mean\n            m_old = self.m.copy()\n            self.m = np.dot(x[:, :self.mu], self.weights)\n\n            # Update evolution path\n            ps_old = self.ps.copy()\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * (self.m - m_old) / self.sigma / np.sqrt(np.diag(self.C))\n            hsig = np.linalg.norm(self.ps)/np.sqrt(1-(1-self.cs)**(2*self.count/self.popsize))/self.chiN < 1.4 + 2/(self.dim+1)\n            self.pc = (1 - self.cc) * self.pc + hsig * np.sqrt(self.cc * (2 - self.cc) * self.mueff) * (self.m - m_old) / self.sigma\n\n            # Update covariance matrix\n            self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * (np.outer(self.pc, self.pc) + (1-hsig) * self.cc * (2-self.cc) * self.C) + self.cmu * np.dot((x[:, :self.mu] - m_old) / self.sigma, np.dot(np.diag(self.weights), ((x[:, :self.mu] - m_old) / self.sigma).T))\n\n            # Update step size\n            self.sigma *= np.exp((self.cs/self.damps) * (np.linalg.norm(self.ps)/self.chiN - 1))\n\n            # Update B and D\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            self.C = self.C / np.linalg.norm(self.C)\n            try:\n                self.D, self.B = np.linalg.eigh(self.C)\n                self.D = np.sqrt(np.maximum(self.D, 1e-16))\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)\n                self.D = np.ones(self.dim)\n                self.B = np.eye(self.dim)\n            \n\n            # Stagnation and Restart Mechanism\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.restarts += 1\n                self.sigma *= 2 # Increase step size\n                self.stagnation_counter = 0\n                self.m = np.zeros(self.dim) + np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim) # Reset mean\n                self.C = np.eye(self.dim) # Reset covariance matrix\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.D, self.B = np.linalg.eigh(self.C)\n                self.D = np.sqrt(np.maximum(self.D, 1e-16))\n                self.popsize = int(self.popsize * 1.1)  # increase the popsize\n                self.mu = self.popsize // 2\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "An exception occurred: matmul: Input operand 0 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1).", "error": "", "parent_ids": [], "operator": null, "metadata": {}}
{"id": "054f09d5-e154-4ccd-b659-7df129b39638", "fitness": 0.48674683791794476, "name": "AdaptiveDifferentialEvolutionLevy", "description": "Adaptive Differential Evolution with Lévy flight mutation and archive for improved exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolutionLevy:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.archive_size = archive_size\n        self.archive = []\n\n    def levy_flight(self, beta=1.5):\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) / (np.math.gamma((1 + beta) / 2) * beta * (2 ** ((beta - 1) / 2)))) ** (1 / beta)\n        u = np.random.normal(0, sigma, self.dim)\n        v = np.random.normal(0, 1, self.dim)\n        step = u / (np.abs(v) ** (1 / beta))\n        return step\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n        \n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Mutation\n                if np.random.rand() < 0.5:  # Apply standard DE mutation\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                    mutant = a + self.F * (b - c)\n                else:  # Apply Lévy flight mutation\n                    mutant = population[i] + self.levy_flight()\n                    \n                # Crossover\n                cross_mask = np.random.rand(self.dim) < self.CR\n                trial = np.copy(population[i])\n                trial[cross_mask] = mutant[cross_mask]\n                \n                # Repair\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n                \n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = trial\n                    \n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                    else:\n                        idx_to_replace = np.random.randint(0, self.archive_size)\n                        self.archive[idx_to_replace] = trial\n                \n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Adaptive F and CR (optional, but can improve performance)\n            self.F = 0.5 * np.random.rand()\n            self.CR = 0.1 + 0.8 * np.random.rand()\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDifferentialEvolutionLevy scored 0.487 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.184650454078508, 0.22325431365201687, 0.45567071768130707, 0.6660574285784411, 0.4826453097559401, 0.5798681910811129, 0.4131011768000531, 0.4553063070369803, 0.5061121594207039, 0.3660761727767625, 0.5369844430493387, 0.997679864576155, 0.3364995942858968, 0.43829630251183493, 0.7249343704531825, 0.5921152490413863, 0.3929703891876154, 0.6401634268858294, 0.23925388199527375, 0.503297005510558]}}
{"id": "0631bca4-e277-4bea-b894-f7f0fd25549e", "fitness": 0.7445780370205226, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with Archive and stochastic ranking for constraint handling.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, archive_size=10, F=0.5, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.CR = CR\n        self.archive = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        for i in range(self.pop_size):\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.pop[i]\n\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n\n                mutant = a + self.F * (b - c)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, self.pop[i])\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n                \n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                else:\n                     if len(self.archive) < self.archive_size:\n                         self.archive.append(self.pop[i].copy())\n                     else:\n                         self.archive[np.random.randint(0,self.archive_size)] = self.pop[i].copy()\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDE scored 0.745 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.3630979311785648, 0.6931212203409435, 0.7497398124171986, 0.8857512332804262, 0.7965299302198512, 0.8308001396656592, 0.6880670849725831, 0.7211998393150088, 0.770558533446773, 0.7150586013592993, 0.9016234137749719, 0.9979558594622555, 0.5527632877985145, 0.7543435109654897, 0.9312053455830518, 0.8311195640322815, 0.7126949703138605, 0.8846746104504345, 0.581627859695814, 0.529627992137473]}}
{"id": "b6025926-81d9-4a87-92a8-b94863af93e8", "fitness": 0.0, "name": "AdaptiveDESelfF", "description": "An adaptive differential evolution algorithm with a self-adaptive mutation factor and a dynamically updated archive based on fitness improvement.", "code": "import numpy as np\n\nclass AdaptiveDESelfF:\n    def __init__(self, budget=10000, dim=10, pop_size=40, archive_size=10, F_init=0.5, CR=0.9, F_min=0.1, F_max=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F_init = F_init\n        self.CR = CR\n        self.archive = []\n        self.F_min = F_min\n        self.F_max = F_max\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.F = np.full(self.pop_size, self.F_init)\n        self.budget -= self.pop_size\n        \n        for i in range(self.pop_size):\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.pop[i]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n\n                # Self-adaptive F\n                self.F[i] = np.clip(np.random.normal(self.F[i], 0.1), self.F_min, self.F_max)\n                mutant = a + self.F[i] * (b - c)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, self.pop[i])\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n                \n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                    #Dynamically update archive based on improvement\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.pop[i].copy())\n                    else:\n                        #Replace the worst member of the archive\n                        archive_fitness = np.array([func(x) for x in self.archive])\n                        worst_index = np.argmax(archive_fitness)\n                        self.archive[worst_index] = self.pop[i].copy()\n\n                else:\n                     if len(self.archive) < self.archive_size:\n                         self.archive.append(self.pop[i].copy())\n                     else:\n                         self.archive[np.random.randint(0,self.archive_size)] = self.pop[i].copy()\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDESelfF scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0631bca4-e277-4bea-b894-f7f0fd25549e"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "64283a4c-cbf1-4283-b0b0-4550c92e32ce", "fitness": -Infinity, "name": "CMAES", "description": "Covariance Matrix Adaptation Evolution Strategy with simplified update rules and boundary handling.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs = 0.3, damps = 1,\n                 cc = 0.1, c1 = 0.1, cmu = 0.1):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = cs\n        self.damps = damps + 2 * max(0, np.sqrt((self.mueff - 1)/(self.dim + 1)) - 1) + self.cs\n        self.cc = cc\n        self.c1 = c1 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1-self.c1, cmu / ((self.dim + 1.5)**2 + self.mueff))\n        self.chiN = self.dim**0.5 * (1 - 1/(4*self.dim) + 1/(21*self.dim**2))\n\n    def __call__(self, func):\n        # Initialize variables\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = 0.3 * (func.bounds.ub - func.bounds.lb)\n        C = np.eye(self.dim)\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        f_opt = np.Inf\n        x_opt = None\n        evals = 0\n\n        while evals < self.budget:\n            # Generate samples\n            z = np.random.randn(self.dim, self.popsize)\n            C_sqrt = np.linalg.cholesky(C)\n            x = mean[:, np.newaxis] + sigma * C_sqrt @ z\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n\n            # Evaluate samples\n            f = np.array([func(x[:,i]) for i in range(self.popsize)])\n            evals += self.popsize\n\n            # Sort by fitness\n            idx = np.argsort(f)\n            f = f[idx]\n            x = x[:, idx]\n\n            # Update optimal solution\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[:, 0].copy()\n\n            # Update mean\n            mean_old = mean.copy()\n            mean = np.sum(self.weights * x[:, :self.mu], axis=1)\n\n            # Update evolution path\n            ps = (1 - self.cs) * ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt, (mean - mean_old) / sigma)\n            hsig = (np.linalg.norm(ps)/self.chiN) < (1.4 + 2/(self.dim + 1))\n            pc = (1 - self.cc) * pc + hsig * np.sqrt(self.cc * (2 - self.cc) * self.mueff) * (mean - mean_old) / sigma\n\n            # Update covariance matrix\n            C = (1 - self.c1 - self.cmu) * C + self.c1 * np.outer(pc, pc) + \\\n                self.cmu * np.sum(self.weights[i] * np.outer((x[:, i] - mean_old) / sigma, (x[:, i] - mean_old) / sigma) for i in range(self.mu))\n            \n            C = np.triu(C) + np.triu(C, 1).T\n            C = np.real(C)\n\n\n            # Update step size\n            sigma = sigma * np.exp((self.cs/self.damps) * ((np.linalg.norm(ps)/self.chiN) - 1))\n            sigma = max(sigma, 1e-10 * (func.bounds.ub - func.bounds.lb))\n\n            if not np.all(np.linalg.eigvalsh(C) > 0):\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                \n\n            if evals >= self.budget:\n                break\n\n        return f_opt, x_opt", "configspace": "", "generation": 1, "feedback": "An exception occurred: operands could not be broadcast together with shapes (2,6) (2,) (2,) .", "error": "", "parent_ids": ["961f4b37-b0e6-4e7d-a22f-de92d1a87b07"], "operator": null, "metadata": {}}
{"id": "da917d56-fceb-426c-9f22-e577b1ac9a54", "fitness": 0.0, "name": "AdaptiveDE", "description": "Introducing a local search operator based on Gaussian perturbation, applied probabilistically to further refine promising solutions.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, archive_size=10, F=0.5, CR=0.9, local_search_prob=0.1, local_search_sigma=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.CR = CR\n        self.archive = []\n        self.local_search_prob = local_search_prob\n        self.local_search_sigma = local_search_sigma\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        for i in range(self.pop_size):\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.pop[i]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n\n                mutant = a + self.F * (b - c)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, self.pop[i])\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n                \n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                    # Local Search\n                    if np.random.rand() < self.local_search_prob:\n                        trial_local = np.clip(trial + np.random.normal(0, self.local_search_sigma, self.dim), func.bounds.lb, func.bounds.ub)\n                        f_trial_local = func(trial_local)\n                        self.budget -= 1\n                        if f_trial_local < f_trial:\n                            self.fitness[i] = f_trial_local\n                            self.pop[i] = trial_local\n                            if f_trial_local < self.f_opt:\n                                self.f_opt = f_trial_local\n                                self.x_opt = trial_local\n\n                else:\n                     if len(self.archive) < self.archive_size:\n                         self.archive.append(self.pop[i].copy())\n                     else:\n                         self.archive[np.random.randint(0,self.archive_size)] = self.pop[i].copy()\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0631bca4-e277-4bea-b894-f7f0fd25549e"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "56dc17ff-8077-41bf-84a6-638d9af43587", "fitness": -Infinity, "name": "CMAES", "description": "CMA-ES with improved numerical stability and boundary handling.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs = 0.3, damps = 1,\n                 cc = 0.1, c1 = 0.1, cmu = 0.1):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = cs\n        self.damps = damps + 2 * max(0, np.sqrt((self.mueff - 1)/(self.dim + 1)) - 1) + self.cs\n        self.cc = cc\n        self.c1 = c1 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1-self.c1, cmu / ((self.dim + 1.5)**2 + self.mueff))\n        self.chiN = self.dim**0.5 * (1 - 1/(4*self.dim) + 1/(21*self.dim**2))\n\n    def __call__(self, func):\n        # Initialize variables\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        mean = np.random.uniform(lb, ub, size=self.dim)\n        sigma = 0.3 * (ub - lb)\n        C = np.eye(self.dim)\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        f_opt = np.Inf\n        x_opt = None\n        evals = 0\n\n        while evals < self.budget:\n            # Generate samples\n            z = np.random.randn(self.dim, self.popsize)\n            try:\n                C_sqrt = np.linalg.cholesky(C)\n                x = mean[:, np.newaxis] + sigma * C_sqrt @ z\n            except np.linalg.LinAlgError:\n                # If C is not positive definite, reset it to identity\n                C = np.eye(self.dim)\n                C_sqrt = np.eye(self.dim)\n                x = mean[:, np.newaxis] + sigma * C_sqrt @ z\n                \n            x = np.clip(x, lb, ub)\n\n            # Evaluate samples\n            f = np.array([func(x[:,i]) for i in range(self.popsize)])\n            evals += self.popsize\n\n            # Sort by fitness\n            idx = np.argsort(f)\n            f = f[idx]\n            x = x[:, idx]\n\n            # Update optimal solution\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[:, 0].copy()\n\n            # Update mean\n            mean_old = mean.copy()\n            mean = np.sum(self.weights * x[:, :self.mu], axis=1)\n\n            # Update evolution path\n            ps = (1 - self.cs) * ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt, (mean - mean_old) / sigma)\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - self.cs)**(2 * evals / self.popsize)) < self.chiN * (1.4 + 2/(self.dim + 1))\n            pc = (1 - self.cc) * pc + hsig * np.sqrt(self.cc * (2 - self.cc) * self.mueff) * (mean - mean_old) / sigma\n\n            # Update covariance matrix\n            C = (1 - self.c1 - self.cmu) * C + self.c1 * (pc[:, np.newaxis] @ pc[np.newaxis, :])\n            \n            # Add rank-mu update\n            for i in range(self.mu):\n                y = (x[:, i] - mean_old) / sigma\n                C += self.cmu * self.weights[i] * (y[:, np.newaxis] @ y[np.newaxis, :])\n            \n\n            # Ensure positive definiteness and symmetry\n            C = np.triu(C) + np.triu(C, 1).T\n            C = (C + C.T) / 2  # Ensure symmetry\n            \n            mineig = np.min(np.linalg.eigvalsh(C))\n            if mineig < 1e-10:\n                C += (1e-10 - mineig) * np.eye(self.dim)\n\n            # Update step size\n            sigma = sigma * np.exp((self.cs/self.damps) * (np.linalg.norm(ps)/self.chiN - 1))\n            sigma = max(sigma, 1e-10 * (ub - lb)) # prevent sigma from becoming too small\n\n\n        return f_opt, x_opt", "configspace": "", "generation": 1, "feedback": "An exception occurred: operands could not be broadcast together with shapes (2,6) (2,) (2,) .", "error": "", "parent_ids": ["961f4b37-b0e6-4e7d-a22f-de92d1a87b07"], "operator": null, "metadata": {}}
{"id": "390f37af-ac91-4525-8e2f-8614b61b91f7", "fitness": 0.0, "name": "SelfAdaptiveDE", "description": "Self-Adaptive Differential Evolution with Neighborhood Search, dynamically adjusting parameters F and CR based on success and incorporating neighborhood search for local refinement.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, archive_size=10, F_init=0.5, CR_init=0.9, F_adapt=True, CR_adapt=True, neighborhood_size=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = np.full(pop_size, F_init)\n        self.CR = np.full(pop_size, CR_init)\n        self.archive = []\n        self.F_adapt = F_adapt\n        self.CR_adapt = CR_adapt\n        self.neighborhood_size = neighborhood_size\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        for i in range(self.pop_size):\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.pop[i]\n\n        success_F = []\n        success_CR = []\n        \n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n\n                mutant = a + self.F[i] * (b - c)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR[i]\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, self.pop[i])\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n                \n                # Neighborhood Search\n                neighborhood_trial = trial + np.random.uniform(-self.neighborhood_size, self.neighborhood_size, size=self.dim)\n                neighborhood_trial = np.clip(neighborhood_trial, func.bounds.lb, func.bounds.ub)\n\n                f_trial = func(trial)\n                f_neighborhood_trial = func(neighborhood_trial)\n                self.budget -= 2  #Two function evaluations\n\n                if f_neighborhood_trial < f_trial:\n                   f_trial = f_neighborhood_trial\n                   trial = neighborhood_trial\n\n                if f_trial < self.fitness[i]:\n                    success_F.append(self.F[i])\n                    success_CR.append(self.CR[i])\n\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                else:\n                     if len(self.archive) < self.archive_size:\n                         self.archive.append(self.pop[i].copy())\n                     else:\n                         self.archive[np.random.randint(0,self.archive_size)] = self.pop[i].copy()\n            \n            if self.F_adapt:\n                if success_F:\n                    self.F = np.clip(np.random.normal(np.mean(success_F), 0.1, self.pop_size), 0.1, 0.9)\n                success_F = []\n\n            if self.CR_adapt:\n                if success_CR:\n                   self.CR = np.clip(np.random.normal(np.mean(success_CR), 0.1, self.pop_size), 0.1, 0.9)\n                success_CR = []\n\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm SelfAdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0631bca4-e277-4bea-b894-f7f0fd25549e"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "fd45d9b2-7212-4193-b03c-94dc8748cca3", "fitness": -Infinity, "name": "CMAES", "description": "Improved CMA-ES with boundary handling, covariance matrix repair, and dynamic population size adjustment based on stagnation.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damps=None):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.cs = cs\n        self.damps = damps if damps is not None else 1 + 2 * np.max([0, np.log(self.dim / 2 + 1)])\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cc = (4 + self.mueff/self.dim) / (self.dim + 4 + 2*self.mueff/self.dim)\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = np.min([1-self.c1, 2 * (self.mueff-2+1/self.mueff) / ((self.dim+2.3)**2 + self.mueff)])\n        self.chiN = self.dim**0.5 * (1 - 1/(4*self.dim) + 1/(21*self.dim**2))\n        self.count = 0\n        self.restarts = 0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 50  # Number of iterations before considering stagnation\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.m = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n\n        while self.count < self.budget:\n            # Sample population\n            z = np.random.randn(self.dim, self.popsize)\n            x = self.m[:, np.newaxis] + self.sigma * np.real(self.B @ (self.D[:, np.newaxis] * z))\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            f = np.array([func(x[:,i]) for i in range(self.popsize)])\n            self.count += self.popsize\n\n            # Sort by fitness\n            idx = np.argsort(f)\n            x = x[:, idx]\n            f = f[idx]\n\n            # Update optimal solution\n            if f[0] < self.f_opt:\n                self.f_opt = f[0]\n                self.x_opt = x[:, 0]\n                self.stagnation_counter = 0 # Reset stagnation counter\n            else:\n                self.stagnation_counter += 1\n\n            # Update mean\n            m_old = self.m.copy()\n            self.m = np.dot(x[:, :self.mu], self.weights)\n\n            # Update evolution path\n            ps_old = self.ps.copy()\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * (self.m - m_old) / self.sigma\n            hsig = np.linalg.norm(self.ps)/np.sqrt(1-(1-self.cs)**(2*self.count/self.popsize))/self.chiN < 1.4 + 2/(self.dim+1)\n            self.pc = (1 - self.cc) * self.pc + hsig * np.sqrt(self.cc * (2 - self.cc) * self.mueff) * (self.m - m_old) / self.sigma\n\n            # Update covariance matrix\n            self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * (np.outer(self.pc, self.pc) + (1-hsig) * self.cc * (2-self.cc) * self.C) + self.cmu * np.dot((x[:, :self.mu] - m_old) / self.sigma, np.dot(np.diag(self.weights), ((x[:, :self.mu] - m_old) / self.sigma).T))\n\n            # Repair covariance matrix to ensure positive definiteness\n            try:\n                self.C = np.triu(self.C) + np.triu(self.C, 1).T\n                # Ensure symmetry\n                self.C = (self.C + self.C.T) / 2\n                \n                # Add a small positive value to the diagonal to ensure positive definiteness\n                min_eig = np.min(np.real(np.linalg.eigvals(self.C)))\n                if min_eig < 1e-8:\n                    self.C += (1e-8 - min_eig) * np.eye(self.dim)\n\n                self.D, self.B = np.linalg.eigh(self.C)\n                self.D = np.sqrt(np.maximum(self.D, 1e-16))\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)\n                self.D = np.ones(self.dim)\n                self.B = np.eye(self.dim)\n\n            # Update step size\n            self.sigma *= np.exp((self.cs/self.damps) * (np.linalg.norm(self.ps)/self.chiN - 1))\n\n            # Stagnation and Restart Mechanism\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.restarts += 1\n                self.sigma *= 2 # Increase step size\n                self.stagnation_counter = 0\n                self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim) # Reset mean\n                self.C = np.eye(self.dim) # Reset covariance matrix\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.D, self.B = np.linalg.eigh(self.C)\n                self.D = np.sqrt(np.maximum(self.D, 1e-16))\n                self.popsize = int(min(self.popsize * 1.1, self.budget/10))  # increase the popsize, but prevent it from growing too large.\n                self.mu = self.popsize // 2\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "An exception occurred: 'CMAES' object has no attribute 'B'.", "error": "", "parent_ids": ["84b0da67-42d7-44d1-966e-b712fb525c34"], "operator": null, "metadata": {}}
{"id": "17faeb93-a814-40fc-92f6-065e03c428d7", "fitness": -Infinity, "name": "AdaptiveCMAES", "description": "An adaptive CMA-ES variant that dynamically adjusts the population size and step size based on the function's landscape by monitoring the change in fitness values and covariance matrix condition number.", "code": "import numpy as np\n\nclass AdaptiveCMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damps=None, initial_sigma=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.cs = cs\n        self.damps = damps if damps is not None else 1 + 2 * np.max([0, np.log(self.dim / 2 + 1)])\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cc = (4 + self.mueff/self.dim) / (self.dim + 4 + 2*self.mueff/self.dim)\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = np.min([1-self.c1, 2 * (self.mueff-2+1/self.mueff) / ((self.dim+2.3)**2 + self.mueff)])\n        self.chiN = self.dim**0.5 * (1 - 1/(4*self.dim) + 1/(21*self.dim**2))\n        self.count = 0\n        self.restarts = 0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 50\n        self.sigma = initial_sigma\n        self.initial_popsize = self.popsize\n        self.min_popsize = 4\n        self.max_popsize = 200\n        self.fitness_history = []\n        self.condition_number_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.m = np.zeros(self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        try:\n            self.D, self.B = np.linalg.eigh(self.C)\n            self.D = np.sqrt(np.maximum(self.D, 1e-16))\n        except np.linalg.LinAlgError:\n            self.C = np.eye(self.dim)\n            self.D = np.ones(self.dim)\n            self.B = np.eye(self.dim)\n            \n        while self.count < self.budget:\n            # Sample population\n            z = np.random.randn(self.dim, self.popsize)\n            x = self.m[:, np.newaxis] + self.sigma * (self.B @ np.diag(self.D) @ z)\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            f = np.array([func(x[:,i]) for i in range(self.popsize)])\n            self.count += self.popsize\n\n            # Sort by fitness\n            idx = np.argsort(f)\n            x = x[:, idx]\n            f = f[idx]\n\n            # Update optimal solution\n            if f[0] < self.f_opt:\n                self.f_opt = f[0]\n                self.x_opt = x[:, 0]\n                self.stagnation_counter = 0\n            else:\n                self.stagnation_counter += 1\n\n            # Update mean\n            m_old = self.m.copy()\n            self.m = np.dot(x[:, :self.mu], self.weights)\n\n            # Update evolution path\n            ps_old = self.ps.copy()\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * (self.m - m_old) / self.sigma\n            hsig = np.linalg.norm(self.ps)/np.sqrt(1-(1-self.cs)**(2*self.count/self.popsize))/self.chiN < 1.4 + 2/(self.dim+1)\n            self.pc = (1 - self.cc) * self.pc + hsig * np.sqrt(self.cc * (2 - self.cc) * self.mueff) * (self.m - m_old) / self.sigma\n\n            # Update covariance matrix\n            self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * (np.outer(self.pc, self.pc) + (1-hsig) * self.cc * (2-self.cc) * self.C) + self.cmu * np.dot((x[:, :self.mu] - m_old) / self.sigma, np.dot(np.diag(self.weights), ((x[:, :self.mu] - m_old) / self.sigma).T))\n\n            # Update step size\n            self.sigma *= np.exp((self.cs/self.damps) * (np.linalg.norm(self.ps)/self.chiN - 1))\n\n            # Update B and D\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            self.C = self.C / np.linalg.norm(self.C)\n            try:\n                self.D, self.B = np.linalg.eigh(self.C)\n                self.D = np.sqrt(np.maximum(self.D, 1e-16))\n                condition_number = np.max(self.D) / np.min(self.D) if np.min(self.D) > 0 else np.inf\n                self.condition_number_history.append(condition_number)\n\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)\n                self.D = np.ones(self.dim)\n                self.B = np.eye(self.dim)\n                self.condition_number_history.append(1.0)\n\n            self.fitness_history.append(f[0])\n\n            # Adaptive Population Size Adjustment\n            if len(self.fitness_history) > 10:\n                fitness_change = np.mean(np.diff(self.fitness_history[-10:]))\n                if np.abs(fitness_change) < 1e-6:  # Stagnation detected\n                    self.popsize = min(self.popsize + 2, self.max_popsize) # Increase population size\n                    self.sigma *= 0.9 # Reduce step size to exploit\n                else:\n                    self.popsize = max(self.popsize - 1, self.min_popsize) # Decrease population size if making progress\n\n            # Adaptive Step Size based on condition number\n            if len(self.condition_number_history) > 5:\n                condition_change = np.mean(np.diff(self.condition_number_history[-5:]))\n                if condition_change > 10: # Covariance is collapsing\n                    self.sigma *= 0.5 # Reduce step size\n                elif condition_change < -10: # Covariance is expanding too fast\n                    self.sigma *= 1.1 # Increase step size\n\n            # Stagnation and Restart Mechanism\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.restarts += 1\n                self.sigma *= 2 # Increase step size\n                self.stagnation_counter = 0\n                self.m = np.zeros(self.dim) + np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim) # Reset mean\n                self.C = np.eye(self.dim) # Reset covariance matrix\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                try:\n                    self.D, self.B = np.linalg.eigh(self.C)\n                    self.D = np.sqrt(np.maximum(self.D, 1e-16))\n                except np.linalg.LinAlgError:\n                    self.C = np.eye(self.dim)\n                    self.D = np.ones(self.dim)\n                    self.B = np.eye(self.dim)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "An exception occurred: operands could not be broadcast together with shapes (2,6) (2,) (2,) .", "error": "", "parent_ids": ["84b0da67-42d7-44d1-966e-b712fb525c34"], "operator": null, "metadata": {}}
{"id": "f1608430-ef2d-4a35-97ed-85287468602f", "fitness": 0.7238504578212145, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with simplified mutation, crossover, and archive handling for improved efficiency.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, archive_size=10, F=0.5, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.CR = CR\n        self.archive = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.pop[best_idx]\n\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                a, b, c = self.pop[idxs]\n\n                mutant = a + self.F * (b - c)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR                \n                trial = np.where(cross_points, mutant, self.pop[i])\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n                \n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                elif len(self.archive) < self.archive_size:\n                    self.archive.append(self.pop[i].copy())\n                else:\n                    self.archive[np.random.randint(0, self.archive_size)] = self.pop[i].copy()\n\n            if len(self.archive) > 0:\n                archive_idx = np.random.randint(0, len(self.archive))\n                self.pop[np.random.randint(0,self.pop_size)] = self.archive[archive_idx]\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDE scored 0.724 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0631bca4-e277-4bea-b894-f7f0fd25549e"], "operator": null, "metadata": {"aucs": [0.3121681763372408, 0.6745433462230572, 0.7141860078863413, 0.8678396920877082, 0.7654405168283229, 0.8250702406499902, 0.699202842111573, 0.69664852514316, 0.7805484030616306, 0.7484169761935082, 0.8514751162544538, 0.99925132096539, 0.6901226918830211, 0.7732442528597293, 0.9165108052570974, 0.8043518107698551, 0.6608921706042267, 0.8735498590650683, 0.3048678640518063, 0.5186785381911084]}}
{"id": "b2ebded2-4892-4cb6-9981-f96bea516e20", "fitness": 0.7571260333767322, "name": "AdaptiveDE", "description": "Simplified Adaptive Differential Evolution with Archive, removing stochastic ranking elements for focused exploration.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, archive_size=10, F=0.5, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.CR = CR\n        self.archive = []\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        for i in range(self.pop_size):\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.pop[i]\n\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n\n                mutant = a + self.F * (b - c)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, self.pop[i])\n                trial = np.clip(trial, self.bounds_lb, self.bounds_ub)\n                \n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                else:\n                     if len(self.archive) < self.archive_size:\n                         self.archive.append(self.pop[i].copy())\n                     else:\n                         self.archive[np.random.randint(0,self.archive_size)] = self.pop[i].copy()\n                         \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDE scored 0.757 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0631bca4-e277-4bea-b894-f7f0fd25549e"], "operator": null, "metadata": {"aucs": [0.3840819539150586, 0.7340879626394521, 0.7065849542223237, 0.8929833799183223, 0.7997474808402577, 0.829417553656797, 0.7187202014600733, 0.732555518600108, 0.7735985117580668, 0.7671729193592666, 0.9010133274894125, 0.9928128858490589, 0.7341840541677964, 0.7765409554888105, 0.8992088341065755, 0.8274443762722707, 0.686774868318327, 0.8827502003440116, 0.5791465019761006, 0.5236942271525526]}}
{"id": "52429c41-f7f8-4b56-89d3-377f4127568d", "fitness": -Infinity, "name": "CMAES", "description": "CMA-ES with improved numerical stability, stagnation detection, adaptive population sizing, and a more robust covariance matrix update.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damps=None):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.cs = cs\n        self.damps = damps if damps is not None else 1 + 2 * np.max([0, np.log(self.dim / 2 + 1)])\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cc = (4 + self.mueff/self.dim) / (self.dim + 4 + 2*self.mueff/self.dim)\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = np.min([1-self.c1, 2 * (self.mueff-2+1/self.mueff) / ((self.dim+2.3)**2 + self.mueff)])\n        self.chiN = self.dim**0.5 * (1 - 1/(4*self.dim) + 1/(21*self.dim**2))\n        self.count = 0\n        self.restarts = 0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 50  # Number of iterations before considering stagnation\n        self.min_sigma = 1e-16 # Minimum step size to prevent premature convergence\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim) # Initialize mean within bounds\n        self.sigma = 0.5\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        \n        # Eigen decomposition outside the loop\n        self.D, self.B = np.linalg.eigh(self.C)\n        self.D = np.sqrt(np.maximum(self.D, 1e-16))\n\n        while self.count < self.budget:\n            # Sample population\n            z = np.random.randn(self.dim, self.popsize)\n            x = self.m[:, np.newaxis] + self.sigma * (self.B @ np.diag(self.D) @ z)\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            f = np.array([func(x[:,i]) for i in range(self.popsize)])\n            self.count += self.popsize\n\n            # Sort by fitness\n            idx = np.argsort(f)\n            x = x[:, idx]\n            f = f[idx]\n\n            # Update optimal solution\n            if f[0] < self.f_opt:\n                self.f_opt = f[0]\n                self.x_opt = x[:, 0]\n                self.stagnation_counter = 0 # Reset stagnation counter\n            else:\n                self.stagnation_counter += 1\n\n            # Update mean\n            m_old = self.m.copy()\n            self.m = np.dot(x[:, :self.mu], self.weights)\n\n            # Update evolution path\n            ps_old = self.ps.copy()\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * (self.m - m_old) / self.sigma\n            hsig = np.linalg.norm(self.ps)/np.sqrt(1-(1-self.cs)**(2*self.count/self.popsize))/self.chiN < 1.4 + 2/(self.dim+1)\n            self.pc = (1 - self.cc) * self.pc + hsig * np.sqrt(self.cc * (2 - self.cc) * self.mueff) * (self.m - m_old) / self.sigma\n\n            # Update covariance matrix\n            artmp = (x[:, :self.mu] - m_old) / self.sigma\n            self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * (np.outer(self.pc, self.pc) + (1-hsig) * self.cc * (2-self.cc) * self.C) + self.cmu * np.dot(artmp, np.dot(np.diag(self.weights), artmp.T))\n            \n            # Ensure positive definiteness and symmetry\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            self.C = np.real(self.C)  # Remove potential tiny imaginary parts\n            \n            # Regularize covariance matrix (add a small diagonal term)\n            self.C += np.eye(self.dim) * 1e-8  # Add a small diagonal for positive definiteness\n\n            # Update step size\n            self.sigma *= np.exp((self.cs/self.damps) * (np.linalg.norm(self.ps)/self.chiN - 1))\n            self.sigma = max(self.sigma, self.min_sigma) # Prevent sigma from becoming too small.\n\n            # Update B and D (Eigen decomposition)\n            try:\n                self.D, self.B = np.linalg.eigh(self.C)\n                self.D = np.sqrt(np.maximum(self.D, 1e-16)) # Ensure eigenvalues are positive\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)\n                self.D = np.ones(self.dim)\n                self.B = np.eye(self.dim)\n            \n\n            # Stagnation and Restart Mechanism\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.restarts += 1\n                self.sigma *= 2 # Increase step size\n                self.stagnation_counter = 0\n                self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim) # Reset mean\n                self.C = np.eye(self.dim) # Reset covariance matrix\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                try:\n                    self.D, self.B = np.linalg.eigh(self.C)\n                    self.D = np.sqrt(np.maximum(self.D, 1e-16))\n                except np.linalg.LinAlgError:\n                    self.C = np.eye(self.dim)\n                    self.D = np.ones(self.dim)\n                    self.B = np.eye(self.dim)\n                self.popsize = int(self.popsize * 1.1)  # increase the popsize\n                self.mu = self.popsize // 2\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "An exception occurred: operands could not be broadcast together with shapes (2,6) (2,) (2,) .", "error": "", "parent_ids": ["84b0da67-42d7-44d1-966e-b712fb525c34"], "operator": null, "metadata": {}}
{"id": "c3122e70-cf9b-418a-96b1-99bfc6cacd94", "fitness": -Infinity, "name": "AdaptiveDE", "description": "Simplified Adaptive Differential Evolution with dynamic F and CR, and a combined population and archive selection for mutation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.archive = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        for i in range(self.pop_size):\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.pop[i]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Dynamic F and CR\n                F = np.random.uniform(0.3, 0.9)\n                CR = np.random.uniform(0.1, 0.9)\n\n                # Mutation: Combine population and archive\n                candidates = np.vstack((self.pop, self.archive)) if len(self.archive) > 0 else self.pop\n                \n                idxs = [idx for idx in range(len(candidates)) if not np.all(candidates[idx] == self.pop[i])] # Avoid selecting same vector\n                if len(idxs) < 3:\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    candidates = self.pop\n                \n                if len(idxs) >= 3:\n\n                    a, b, c = candidates[np.random.choice(idxs, 3, replace=False)]\n\n                    mutant = a + F * (b - c)\n\n                    # Crossover\n                    cross_points = np.random.rand(self.dim) < CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    \n                    trial = np.where(cross_points, mutant, self.pop[i])\n                    trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n                    \n                    f_trial = func(trial)\n                    self.budget -= 1\n                    \n                    if f_trial < self.fitness[i]:\n                        self.fitness[i] = f_trial\n                        self.pop[i] = trial\n                        if f_trial < self.f_opt:\n                            self.f_opt = f_trial\n                            self.x_opt = trial\n                    else:\n                        if len(self.archive) < self.archive_size:\n                            self.archive.append(self.pop[i].copy())\n                        else:\n                            self.archive[np.random.randint(0,self.archive_size)] = self.pop[i].copy()\n                else:\n                    self.budget -= 1\n                    \n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "An exception occurred: Evaluation timed out after 60 seconds..", "error": "", "parent_ids": ["0631bca4-e277-4bea-b894-f7f0fd25549e"], "operator": null, "metadata": {}}
{"id": "26c643a6-f98e-42b5-ae63-043523d854ce", "fitness": 0.5570411721403318, "name": "EnhancedAdaptiveDifferentialEvolutionLevy", "description": "Enhanced Adaptive Differential Evolution with Lévy flight, archive, and self-adaptive parameters, incorporating a selection pressure mechanism to balance exploration and exploitation more effectively.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolutionLevy:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, archive_size=10, p_selection=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.archive_size = archive_size\n        self.archive = []\n        self.p_selection = p_selection # Probability of selecting from the best individuals\n\n    def levy_flight(self, beta=1.5):\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) / (np.math.gamma((1 + beta) / 2) * beta * (2 ** ((beta - 1) / 2)))) ** (1 / beta)\n        u = np.random.normal(0, sigma, self.dim)\n        v = np.random.normal(0, 1, self.dim)\n        step = u / (np.abs(v) ** (1 / beta))\n        return step\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n        \n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Selection of parents\n                if np.random.rand() < self.p_selection:\n                    # Select from the best individuals\n                    sorted_indices = np.argsort(fitness)\n                    a_idx = sorted_indices[np.random.randint(0, int(self.pop_size * self.p_selection))] # Select from top p_selection %\n                    idxs = [idx for idx in range(self.pop_size) if idx != i and idx != a_idx]\n\n                    b_idx = np.random.choice(idxs)\n                    idxs.remove(b_idx)\n                    c_idx = np.random.choice(idxs)\n\n                    a, b, c = population[a_idx], population[b_idx], population[c_idx]\n                else:\n                    # Random selection\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n\n                # Mutation\n                if np.random.rand() < 0.5:  # Apply standard DE mutation\n                    mutant = a + self.F * (b - c)\n                else:  # Apply Lévy flight mutation\n                    mutant = population[i] + self.levy_flight()\n                    \n                # Crossover\n                cross_mask = np.random.rand(self.dim) < self.CR\n                trial = np.copy(population[i])\n                trial[cross_mask] = mutant[cross_mask]\n                \n                # Repair\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n                \n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = trial\n                    \n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                    else:\n                        idx_to_replace = np.random.randint(0, self.archive_size)\n                        self.archive[idx_to_replace] = trial\n                \n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Adaptive F and CR (optional, but can improve performance)\n            self.F = 0.5 * np.random.rand()\n            self.CR = 0.1 + 0.8 * np.random.rand()\n            \n            # Adaptive p_selection\n            self.p_selection = min(0.9, self.p_selection + 0.01 * (np.random.rand() - 0.5)) # adapt it slightly\n\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolutionLevy scored 0.557 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["054f09d5-e154-4ccd-b659-7df129b39638"], "operator": null, "metadata": {"aucs": [0.18726459422020025, 0.3769413609595117, 0.4539592790389253, 0.8055368826920881, 0.5722549050906378, 0.6602741785139865, 0.4775117460675933, 0.46350451717436203, 0.5469751722404703, 0.4627707750818112, 0.7616835061374536, 0.9912692394320441, 0.31976650752345637, 0.5647899907806289, 0.866752194644677, 0.6748412977361744, 0.4541431241413424, 0.7481700646352278, 0.23045585465668617, 0.5219582520393586]}}
{"id": "b57da7fd-9de8-4520-b3de-72dd0bfa4bb2", "fitness": -Infinity, "name": "AdaptiveDE", "description": "Simplified Adaptive Differential Evolution with reduced archive interaction and parameter adaptation for faster convergence.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, archive_size=10, F=0.5, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.CR = CR\n        self.archive = []\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x)] for x in self.pop)\n        self.budget -= self.pop_size\n        \n        for i in range(self.pop_size):\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.pop[i]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n\n                mutant = a + self.F * (b - c)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, self.pop[i])\n                trial = np.clip(trial, self.bounds_lb, self.bounds_ub)\n                \n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                    self.F = np.clip(self.F + 0.01 * np.random.normal(0,1), 0.1, 0.9)\n                    self.CR = np.clip(self.CR + 0.01 * np.random.normal(0,1), 0.1, 0.9)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "An exception occurred: too many indices for array: array is 0-dimensional, but 1 were indexed.", "error": "", "parent_ids": ["b2ebded2-4892-4cb6-9981-f96bea516e20"], "operator": null, "metadata": {}}
{"id": "df81f8be-b973-4f1b-ac1d-6191142be685", "fitness": 0.0, "name": "RingTopologyAdaptiveDE", "description": "An adaptive differential evolution algorithm with a ring topology-based mutation and a self-adjusting archive, enhancing exploration and exploitation by considering neighborhood information and historical best solutions.", "code": "import numpy as np\n\nclass RingTopologyAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.archive_size = archive_size\n        self.archive = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n        \n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Ring Topology Selection: Use neighbors in the population\n                left = (i - 1) % self.pop_size\n                right = (i + 1) % self.pop_size\n\n                # Mutation using ring topology\n                mutant = population[i] + self.F * (population[left] - population[right])\n\n                # Add archive contribution with a small probability\n                if len(self.archive) > 0 and np.random.rand() < 0.1:\n                    archive_member = self.archive[np.random.randint(0, len(self.archive))]\n                    mutant += self.F * (archive_member - population[i])\n                \n                # Crossover\n                cross_mask = np.random.rand(self.dim) < self.CR\n                trial = np.copy(population[i])\n                trial[cross_mask] = mutant[cross_mask]\n                \n                # Repair\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n                \n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = trial\n                    \n                    # Adaptive Archive Update\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                    else:\n                        # Replace worst member of archive\n                        archive_fitness = [func(x) for x in self.archive]\n                        worst_archive_index = np.argmax(archive_fitness)\n                        self.archive[worst_archive_index] = trial\n                \n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Adaptive F and CR (optional, but can improve performance)\n            self.F = 0.5 * np.random.rand()\n            self.CR = 0.1 + 0.8 * np.random.rand()\n\n            # Archive maintenance: Remove duplicates\n            self.archive = [tuple(row) for row in self.archive]\n            self.archive = [np.array(row) for row in list(set(self.archive))]\n\n            # Limit archive size\n            while len(self.archive) > self.archive_size:\n                self.archive.pop(np.random.randint(0, len(self.archive)))\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm RingTopologyAdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["26c643a6-f98e-42b5-ae63-043523d854ce"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "57757d86-4cbd-4417-bd06-382dc8b9171d", "fitness": 0.0, "name": "AdaptiveDE", "description": "Enhanced Adaptive Differential Evolution with Archive and adaptive F/CR parameters, including a simple repair mechanism to handle boundary violations.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, archive_size=10, F_min=0.1, F_max=0.9, CR_min=0.1, CR_max=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F_min = F_min\n        self.F_max = F_max\n        self.CR_min = CR_min\n        self.CR_max = CR_max\n        self.archive = []\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n\n    def repair(self, x):\n        \"\"\"Repair solution to lie within the bounds.\"\"\"\n        return np.clip(x, self.bounds_lb, self.bounds_ub)\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        for i in range(self.pop_size):\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.pop[i]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                F = np.random.uniform(self.F_min, self.F_max)\n                CR = np.random.uniform(self.CR_min, self.CR_max)\n\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                if len(self.archive) > 0:\n                    idxs_archive = list(range(len(self.archive)))\n                    idx_a = np.random.choice(idxs)\n                    idx_b = np.random.choice(idxs)\n                    idx_c = np.random.choice(idxs_archive)\n                    a = self.pop[idx_a]\n                    b = self.pop[idx_b]\n                    c = self.archive[idx_c]\n                    mutant = a + F * (b - c)\n                else:\n                    a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n                    mutant = a + F * (b - c)\n\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, self.pop[i])\n                trial = self.repair(trial) # Repair mechanism\n                \n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                    \n                    # Archive update (replace worst)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.pop[i].copy())\n                    else:\n                        worst_idx = np.argmax(np.array([func(x) for x in self.archive])) # Find worst in archive\n                        self.archive[worst_idx] = self.pop[i].copy()\n                else:\n                    # Archive update (add if better than worst in archive)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.pop[i].copy())\n                    elif func(self.pop[i]) < np.max([func(x) for x in self.archive]):\n                        worst_idx = np.argmax([func(x) for x in self.archive])\n                        self.archive[worst_idx] = self.pop[i].copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b2ebded2-4892-4cb6-9981-f96bea516e20"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "d62a6aad-673b-4652-a68c-eb58e80ec828", "fitness": -Infinity, "name": "DynamicAdaptiveDE", "description": "Differential Evolution with a dynamically adjusted population size and mutation strategy based on fitness variance to balance exploration and exploitation.", "code": "import numpy as np\n\nclass DynamicAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, min_pop_size=10, max_pop_size=100, F=0.5, CR=0.9, variance_threshold=1e-3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.min_pop_size = min_pop_size\n        self.max_pop_size = max_pop_size\n        self.F = F\n        self.CR = CR\n        self.variance_threshold = variance_threshold\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self, func):\n         self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n         self.fitness = np.array([func(x) for x in self.population])\n         self.budget -= self.pop_size\n\n    def adjust_population_size(self):\n        fitness_variance = np.var(self.fitness)\n        if fitness_variance < self.variance_threshold and self.pop_size < self.max_pop_size:\n            # Low variance, increase population size to explore more\n            increase_amount = min(self.pop_size // 4, self.max_pop_size - self.pop_size) # Increase by at most 25% or to max_pop_size\n            if increase_amount > 0 and self.budget > increase_amount:\n                new_individuals = np.random.uniform(self.population.min(), self.population.max(), size=(increase_amount, self.dim))\n                new_fitness = np.array([func(x) for x in new_individuals])\n                self.budget -= increase_amount\n                self.population = np.vstack((self.population, new_individuals))\n                self.fitness = np.concatenate((self.fitness, new_fitness))\n                self.pop_size = self.population.shape[0]\n\n        elif fitness_variance > 10 * self.variance_threshold and self.pop_size > self.min_pop_size:\n             # High variance, decrease population size to exploit better solutions\n            decrease_amount = max(1, self.pop_size // 4) # Decrease by at least 1 or 25%\n            indices_to_remove = np.argsort(self.fitness)[-decrease_amount:]  # Remove worst individuals\n            mask = np.ones(self.pop_size, dtype=bool)\n            mask[indices_to_remove] = False\n            self.population = self.population[mask]\n            self.fitness = self.fitness[mask]\n            self.pop_size = self.population.shape[0]\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n        # Initialize population and fitness\n        self.initialize_population(func)\n\n        # Update optimal solution\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n\n            self.adjust_population_size()\n\n            for i in range(self.pop_size):\n                # Selection of parents\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n\n                # Mutation\n                if np.random.rand() < 0.5:  # Standard DE mutation\n                    mutant = a + self.F * (b - c)\n                else:  # Random mutation from the population\n                    mutant = self.population[np.random.choice(range(self.pop_size))] + self.F * (b-c)\n\n                # Crossover\n                cross_mask = np.random.rand(self.dim) < self.CR\n                trial = np.copy(self.population[i])\n                trial[cross_mask] = mutant[cross_mask]\n\n                # Repair\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n\n            # Adaptive F and CR (optional, but can improve performance)\n            self.F = 0.5 * np.random.rand()\n            self.CR = 0.1 + 0.8 * np.random.rand()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "An exception occurred: name 'func' is not defined.", "error": "", "parent_ids": ["26c643a6-f98e-42b5-ae63-043523d854ce"], "operator": null, "metadata": {}}
{"id": "d26f8728-4584-46ca-8366-f34983241652", "fitness": 0.0, "name": "AdaptiveDE", "description": "Differential Evolution with self-adaptive population size and mutation factor, dynamically adjusting exploration and exploitation based on fitness improvement.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=40, archive_size=10, F_initial=0.5, CR=0.9, F_adapt_rate=0.1, pop_adapt_rate=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.initial_pop_size = initial_pop_size\n        self.archive_size = archive_size\n        self.F = F_initial\n        self.CR = CR\n        self.archive = []\n        self.F_adapt_rate = F_adapt_rate\n        self.pop_adapt_rate = pop_adapt_rate\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.pop[best_idx]\n\n        eval_count = self.pop_size\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                a, b, c = self.pop[idxs]\n\n                mutant = a + self.F * (b - c)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR                \n                trial = np.where(cross_points, mutant, self.pop[i])\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n                \n                f_trial = func(trial)\n                self.budget -= 1\n                eval_count += 1\n                \n                if f_trial < self.fitness[i]:\n                    # Improvement, adapt F\n                    self.F = max(0, min(1, self.F * (1 + self.F_adapt_rate)))\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                else:\n                    # No improvement, adapt F\n                    self.F = max(0, min(1, self.F * (1 - self.F_adapt_rate)))\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.pop[i].copy())\n                    else:\n                        self.archive[np.random.randint(0, self.archive_size)] = self.pop[i].copy()\n\n            # Adapt population size\n            if eval_count > self.initial_pop_size:\n                improvement_ratio = np.sum(self.fitness < np.mean(self.fitness)) / self.pop_size\n                if improvement_ratio > 0.5:\n                    # Increase population size\n                    self.pop_size = min(self.initial_pop_size * 2, self.pop_size + int(self.pop_adapt_rate * self.initial_pop_size))\n                    new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size - len(self.pop), self.dim))\n                    self.pop = np.vstack((self.pop, new_individuals))\n                    new_fitness = np.array([func(x) for x in new_individuals])\n                    self.fitness = np.concatenate((self.fitness, new_fitness))\n                    self.budget -= len(new_individuals)\n                    eval_count += len(new_individuals)\n\n\n                elif improvement_ratio < 0.2:\n                    # Decrease population size\n                    self.pop_size = max(int(self.initial_pop_size / 4), int(self.pop_size * (1 - self.pop_adapt_rate)))\n                    idxs_to_keep = np.argsort(self.fitness)[:self.pop_size]\n                    self.pop = self.pop[idxs_to_keep]\n                    self.fitness = self.fitness[idxs_to_keep]\n                \n            if len(self.archive) > 0:\n                archive_idx = np.random.randint(0, len(self.archive))\n                self.pop[np.random.randint(0,self.pop_size)] = self.archive[archive_idx]\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f1608430-ef2d-4a35-97ed-85287468602f"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "edf6c6ab-fd69-4dd4-8793-9f1773300e10", "fitness": 0.0, "name": "AdaptiveDEOrthogonal", "description": "An adaptive differential evolution algorithm that incorporates orthogonal learning to enhance population diversity and convergence speed.", "code": "import numpy as np\n\nclass AdaptiveDEOrthogonal:\n    def __init__(self, budget=10000, dim=10, pop_size=40, archive_size=10, F_init=0.5, CR=0.9, F_min=0.1, F_max=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F_init = F_init\n        self.CR = CR\n        self.archive = []\n        self.F_min = F_min\n        self.F_max = F_max\n        self.orthogonal_matrix = self._generate_orthogonal_matrix(self.dim)\n\n    def _generate_orthogonal_matrix(self, dim):\n        if dim == 1:\n            return np.array([[1]])\n\n        k = int(np.ceil(np.log2(dim)))\n        n = 2**k\n\n        hadamard_matrix = np.ones((1, 1))\n        for _ in range(k):\n            hadamard_matrix = np.kron(hadamard_matrix, np.array([[1, 1], [1, -1]]))\n\n        return hadamard_matrix[:dim, :dim]\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.F = np.full(self.pop_size, self.F_init)\n        self.budget -= self.pop_size\n        \n        for i in range(self.pop_size):\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.pop[i]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n\n                # Self-adaptive F\n                self.F[i] = np.clip(np.random.normal(self.F[i], 0.1), self.F_min, self.F_max)\n                mutant = a + self.F[i] * (b - c)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, self.pop[i])\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n                \n                # Orthogonal Learning\n                orthogonal_sample = self._orthogonal_crossover(self.pop[i])\n                orthogonal_sample = np.clip(orthogonal_sample, func.bounds.lb, func.bounds.ub)\n\n                f_trial = func(trial)\n                self.budget -= 1\n\n                f_orthogonal = func(orthogonal_sample)\n                self.budget -=1\n                \n                if f_trial < self.fitness[i] and f_trial <= f_orthogonal:\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                    #Dynamically update archive based on improvement\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.pop[i].copy())\n                    else:\n                        #Replace the worst member of the archive\n                        archive_fitness = np.array([func(x) for x in self.archive])\n                        worst_index = np.argmax(archive_fitness)\n                        self.archive[worst_index] = self.pop[i].copy()\n\n                elif f_orthogonal < self.fitness[i] and f_orthogonal < f_trial:\n                    self.fitness[i] = f_orthogonal\n                    self.pop[i] = orthogonal_sample\n                    if f_orthogonal < self.f_opt:\n                        self.f_opt = f_orthogonal\n                        self.x_opt = orthogonal_sample\n\n                    #Dynamically update archive based on improvement\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.pop[i].copy())\n                    else:\n                        #Replace the worst member of the archive\n                        archive_fitness = np.array([func(x) for x in self.archive])\n                        worst_index = np.argmax(archive_fitness)\n                        self.archive[worst_index] = self.pop[i].copy()\n\n\n                else:\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.pop[i].copy())\n                    else:\n                        self.archive[np.random.randint(0,self.archive_size)] = self.pop[i].copy()\n\n            \n        return self.f_opt, self.x_opt\n    \n    def _orthogonal_crossover(self, x):\n        #Use orthogonal matrix for crossover\n        basis_vector = np.random.uniform(-1,1,self.dim)\n        new_x = x + 0.1*np.dot(self.orthogonal_matrix,basis_vector)\n        return new_x", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDEOrthogonal scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b6025926-81d9-4a87-92a8-b94863af93e8"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "ea1b4dfb-c1f4-405f-9ffa-9d81ab869002", "fitness": 0.25148265417886173, "name": "CMAES", "description": "Covariance matrix adaptation evolution strategy (CMA-ES) with simplified adaptation and restart mechanism for improved exploration-exploitation balance.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, sigma=0.5, cs=0.3, c_cov=0.1, mu_factor=0.25, restarts=3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.sigma = sigma\n        self.cs = cs\n        self.c_cov = c_cov\n        self.mu = int(self.pop_size * mu_factor)\n        self.weights = np.log(self.pop_size + 1) - np.log(np.arange(1, self.pop_size + 1))\n        self.weights = self.weights[:self.mu] / np.sum(self.weights[:self.mu])\n        self.mean = np.zeros(dim)\n        self.C = np.eye(dim)\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.restarts = restarts\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.restart_count = 0\n        \n        while self.budget > 0 and self.restart_count < self.restarts:\n            \n            X = self.sample_population()\n            F = np.array([func(x) for x in X])\n            self.budget -= self.pop_size\n            \n            idx = np.argsort(F)\n            X = X[idx]\n            F = F[idx]\n            \n            if F[0] < self.f_opt:\n                self.f_opt = F[0]\n                self.x_opt = X[0]\n                \n            self.update_distribution(X)\n\n            if self.is_stagnated():\n                self.restart()\n                self.restart_count += 1\n\n        return self.f_opt, self.x_opt\n    \n    def sample_population(self):\n        Z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n        A = np.linalg.cholesky(self.C)\n        X = self.mean + self.sigma * Z @ A.T\n        X = np.clip(X, self.bounds_lb, self.bounds_ub)\n        return X\n    \n    def update_distribution(self, X):\n        d = X[:self.mu] - self.mean\n        self.mean = np.sum(self.weights[:, None] * X[:self.mu], axis=0)\n        \n        C_update = np.sum(self.weights[:, None, None] * d[:, :, None] * d[:, None, :], axis=0)\n        self.C = (1 - self.c_cov) * self.C + self.c_cov * C_update\n        \n        # Ensure C remains positive definite\n        try:\n            np.linalg.cholesky(self.C)\n        except np.linalg.LinAlgError:\n            self.C = np.eye(self.dim)\n            \n        self.sigma *= np.exp(self.cs / 5 * (np.linalg.norm(d) / self.dim - 1))\n        self.sigma = np.clip(self.sigma, 1e-10, 1e10)\n\n    def is_stagnated(self):\n        return self.sigma < 1e-6\n\n    def restart(self):\n        self.sigma = 0.5\n        self.mean = np.random.uniform(self.bounds_lb, self.bounds_ub, size=self.dim)\n        self.C = np.eye(self.dim)", "configspace": "", "generation": 2, "feedback": "The algorithm CMAES scored 0.251 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b2ebded2-4892-4cb6-9981-f96bea516e20"], "operator": null, "metadata": {"aucs": [0.1006969222589087, 0.17663315823383818, 0.5731476604258781, 0.18711993264495907, 0.21203820188286648, 0.21579612825696481, 0.24361840110556565, 0.39466589087578974, 0.1983582549218631, 0.18883941576136154, 0.20314374245539135, 0.22038480790098725, 0.29510042331111275, 0.20534369702101363, 0.2090986047817487, 0.27781899963935797, 0.24801746836144067, 0.26898581826149326, 0.16658711383439007, 0.44425844164230455]}}
{"id": "91b1d7cf-7c48-464c-ae5c-dc565e4244fd", "fitness": 0.3948228978280121, "name": "AdaptiveDE", "description": "Simplified Adaptive Differential Evolution with enhanced mutation strategy and adaptive CR.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, archive_size=10, F=0.5, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.CR = CR\n        self.archive = []\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        for i in range(self.pop_size):\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.pop[i]\n\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b = self.pop[np.random.choice(idxs, 2, replace=False)]\n\n                mutant = self.pop[i] + self.F * (a - b) #Simplified mutation\n\n                # Crossover\n                CR_adaptive = np.random.uniform(0, self.CR) #Adaptive CR\n                cross_points = np.random.rand(self.dim) < CR_adaptive\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, self.pop[i])\n                trial = np.clip(trial, self.bounds_lb, self.bounds_ub)\n                \n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                #Archive is removed for simplicity\n                         \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDE scored 0.395 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b2ebded2-4892-4cb6-9981-f96bea516e20"], "operator": null, "metadata": {"aucs": [0.16820058084453993, 0.2424614350371661, 0.37828909241057807, 0.40157539198664094, 0.29852746904285365, 0.41461653559447265, 0.2963212019971587, 0.35752542030966905, 0.3315254146839002, 0.19001244843929332, 0.5053832483346736, 0.9924245386735409, 0.2925031926219124, 0.3327892245679801, 0.7453230148433161, 0.47673779170550223, 0.31446250294258915, 0.4725635370425073, 0.19365073949298306, 0.49156517598896454]}}
{"id": "39718de5-dd6e-47ad-b4ae-bf6c5d703ffc", "fitness": 0.7314786114239235, "name": "SimplifiedAdaptiveDE", "description": "Simplified Adaptive Differential Evolution with a linearly decreasing crossover rate and adaptive F, focusing on efficiency and reduced parameter tuning.", "code": "import numpy as np\n\nclass SimplifiedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, F_min=0.1, F_max=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_init = F_init\n        self.F_min = F_min\n        self.F_max = F_max\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.F = np.full(self.pop_size, self.F_init)\n        self.CR = 0.9  # Initial crossover rate\n        self.CR_decay = 0.9 / self.budget # Linear decay rate\n        self.budget -= self.pop_size\n        \n        for i in range(self.pop_size):\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.pop[i]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n\n                # Self-adaptive F\n                self.F[i] = np.clip(np.random.normal(self.F[i], 0.1), self.F_min, self.F_max)\n                mutant = a + self.F[i] * (b - c)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, self.pop[i])\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n                \n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n            self.CR -= self.CR_decay  #Linear decay of CR\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm SimplifiedAdaptiveDE scored 0.731 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b6025926-81d9-4a87-92a8-b94863af93e8"], "operator": null, "metadata": {"aucs": [0.34745782234242584, 0.6551952605753726, 0.7591584128567935, 0.8911874486355983, 0.7920336669867956, 0.8281510679588637, 0.7095811743146001, 0.7181883630076036, 0.7900302035383258, 0.7191479811753174, 0.8697930344771257, 0.9901087735311506, 0.7023207503429056, 0.7444904408410273, 0.9359751837427233, 0.824096236564077, 0.7013383316401882, 0.861973726300771, 0.2593622774531422, 0.5299820721936621]}}
{"id": "87ebf696-cf25-4d6f-86d7-1f446ef60469", "fitness": 0.34907137219515494, "name": "SimplifiedAdaptiveDELevy", "description": "Simplified Adaptive Differential Evolution with Lévy-based mutation, archive, and adaptive parameters, focusing on a streamlined mutation strategy and selection process.", "code": "import numpy as np\n\nclass SimplifiedAdaptiveDELevy:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.archive = []\n        self.F = 0.5\n        self.CR = 0.9\n\n    def levy_flight(self, beta=1.5):\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) / (np.math.gamma((1 + beta) / 2) * beta * (2 ** ((beta - 1) / 2)))) ** (1 / beta)\n        u = np.random.normal(0, sigma, self.dim)\n        v = np.random.normal(0, 1, self.dim)\n        step = u / (np.abs(v) ** (1 / beta))\n        return step\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n        \n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Selection of parents\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n\n                # Mutation: Simplified Lévy-based mutation\n                mutant = population[i] + self.levy_flight() + self.F * (a - b)\n\n                # Crossover\n                cross_mask = np.random.rand(self.dim) < self.CR\n                trial = np.copy(population[i])\n                trial[cross_mask] = mutant[cross_mask]\n                \n                # Repair\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n                \n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = trial\n                    \n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                    else:\n                        idx_to_replace = np.random.randint(0, self.archive_size)\n                        self.archive[idx_to_replace] = trial\n                \n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Adaptive F and CR\n            self.F = 0.5 * np.random.rand()\n            self.CR = 0.1 + 0.8 * np.random.rand()\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm SimplifiedAdaptiveDELevy scored 0.349 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["26c643a6-f98e-42b5-ae63-043523d854ce"], "operator": null, "metadata": {"aucs": [0.14769782631551132, 0.25650352638256857, 0.34071079495169976, 0.3732556368427914, 0.26510484336451823, 0.3305378056658097, 0.2821632682094827, 0.2872734493116458, 0.26463862856122133, 0.18807228544214227, 0.33961797202563304, 0.9861965437462514, 0.2727353950851741, 0.2839299172518138, 0.704812788109455, 0.3384749969061358, 0.2934787998279309, 0.3601637145650749, 0.18144555098817672, 0.484613700350062]}}
{"id": "48e361a2-738d-4f80-9b2c-3461b1fb2af7", "fitness": 0.0, "name": "AdaptiveDESelfF", "description": "Simplified Adaptive Differential Evolution with combined archive and population selection, using a single F value for all individuals and simplified archive update.", "code": "import numpy as np\n\nclass AdaptiveDESelfF:\n    def __init__(self, budget=10000, dim=10, pop_size=40, archive_size=10, F_init=0.5, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F_init = F_init\n        self.CR = CR\n        self.archive = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        for i in range(self.pop_size):\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.pop[i]\n\n        while self.budget > 0:\n            F = np.random.normal(self.F_init, 0.1)\n            F = np.clip(F, 0.1, 0.9)\n\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                if len(idxs) < 3:\n                    continue\n                a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n\n                mutant = a + F * (b - c)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, self.pop[i])\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n                \n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                # Combined population and archive selection\n                combined_pop = np.vstack((self.pop, np.array(self.archive))) if self.archive else self.pop\n                if combined_pop.shape[0] > self.pop_size:\n                    indices = np.argsort([func(x) for x in combined_pop])[:self.pop_size]\n\n                    self.pop = combined_pop[indices]\n                    self.fitness = np.array([func(x) for x in self.pop])\n                    self.archive = [] #Clear the archive after population update\n                else:\n                    #Update archive simply by adding if archive isn't full.\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.pop[i].copy())\n\n\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDESelfF scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b6025926-81d9-4a87-92a8-b94863af93e8"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "4773f68a-c659-4aa4-871a-ec24bd5a2946", "fitness": -Infinity, "name": "EnhancedAdaptiveDifferentialEvolutionLevyRestart", "description": "Introducing a restart mechanism when stagnation is detected, alongside adjustments to the Lévy flight and crossover strategies for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolutionLevyRestart:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, archive_size=10, p_selection=0.2, stagnation_threshold=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.archive_size = archive_size\n        self.archive = []\n        self.p_selection = p_selection  # Probability of selecting from the best individuals\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n\n    def levy_flight(self, beta=1.5):\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) / (np.math.gamma((1 + beta) / 2) * beta * (2 ** ((beta - 1) / 2)))) ** (1 / beta)\n        u = np.random.normal(0, sigma, self.dim)\n        v = np.random.normal(0, 1, self.dim)\n        step = u / (np.abs(v) ** (1 / beta))\n        return step\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n\n        self.best_fitness_history.append(self.f_opt)  # Store initial best fitness\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Selection of parents\n                if np.random.rand() < self.p_selection:\n                    # Select from the best individuals\n                    sorted_indices = np.argsort(fitness)\n                    a_idx = sorted_indices[np.random.randint(0, int(self.pop_size * self.p_selection))]  # Select from top p_selection %\n                    idxs = [idx for idx in range(self.pop_size) if idx != i and idx != a_idx]\n\n                    b_idx = np.random.choice(idxs)\n                    idxs.remove(b_idx)\n                    c_idx = np.random.choice(idxs)\n\n                    a, b, c = population[a_idx], population[b_idx], population[c_idx]\n                else:\n                    # Random selection\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n\n                # Mutation\n                if np.random.rand() < 0.5:  # Apply standard DE mutation\n                    mutant = a + self.F * (b - c)\n                else:  # Apply Lévy flight mutation with adaptive step size\n                    levy_step = self.levy_flight()\n                    mutant = population[i] + 0.1 * levy_step # Reduced step size for Levy\n\n                # Crossover - binomial crossover with at least one parameter change\n                j_rand = np.random.randint(self.dim)\n                cross_mask = np.random.rand(self.dim) < self.CR\n                cross_mask[j_rand] = True  # Ensure at least one parameter is crossed over\n                trial = np.copy(population[i])\n                trial[cross_mask] = mutant[cross_mask]\n\n                # Repair\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                    else:\n                        idx_to_replace = np.random.randint(0, self.archive_size)\n                        self.archive[idx_to_replace] = trial\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Adaptive F and CR (optional, but can improve performance)\n            self.F = 0.5 * np.random.rand()\n            self.CR = 0.1 + 0.8 * np.random.rand()\n\n            # Adaptive p_selection\n            self.p_selection = min(0.9, self.p_selection + 0.01 * (np.random.rand() - 0.5))  # adapt it slightly\n\n            # Stagnation detection and restart\n            self.best_fitness_history.append(self.f_opt)\n            if generation > self.stagnation_threshold:\n                if self.best_fitness_history[-1] >= self.best_fitness_history[-self.stagnation_threshold]:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0\n\n                if self.stagnation_counter >= self.stagnation_threshold:\n                    # Restart population\n                    population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                    fitness = np.array([func(x) for x in population])\n                    self.budget -= self.pop_size\n                    best_index = np.argmin(fitness)\n                    if fitness[best_index] < self.f_opt:\n                        self.f_opt = fitness[best_index]\n                        self.x_opt = population[best_index]\n                    self.stagnation_counter = 0\n                    self.best_fitness_history = [self.f_opt]  # Reset fitness history\n                    self.F = 0.5  # Reset F\n                    self.CR = 0.9  # Reset CR\n                    self.p_selection = 0.2 # Reset p_selection\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "An exception occurred: list index out of range.", "error": "", "parent_ids": ["26c643a6-f98e-42b5-ae63-043523d854ce"], "operator": null, "metadata": {}}
{"id": "a99c76d8-02a9-46d2-abe7-0e94746c5a22", "fitness": 0.0, "name": "DynamicVelocityDE", "description": "Differential Evolution with a dynamically adjusted population size and velocity-based mutation.", "code": "import numpy as np\n\nclass DynamicVelocityDE:\n    def __init__(self, budget=10000, dim=10, pop_size_init=40, pop_size_min=10, pop_size_max=100, F=0.5, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_init = pop_size_init\n        self.pop_size_min = pop_size_min\n        self.pop_size_max = pop_size_max\n        self.F = F\n        self.CR = CR\n        self.pop_size = pop_size_init  # Initialize population size\n        self.history = [] #store the fitness history\n        self.convergence_threshold = 1e-6 #Define a convergence threshold\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.velocities = np.zeros_like(self.pop) # Initialize velocities to zero\n        self.budget -= self.pop_size\n        self.history.append(np.mean(self.fitness)) #Storing the fitness history\n\n        for i in range(self.pop_size):\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.pop[i]\n        \n        generation = 0\n        while self.budget > 0:\n            generation += 1\n\n            for i in range(self.pop_size):\n                # Mutation with velocity update\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n                \n                # Update velocity based on current population differences\n                self.velocities[i] = self.F * (b - c)\n                mutant = self.pop[i] + self.velocities[i]  # Apply velocity to create mutant\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, self.pop[i])\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n                \n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Dynamic Population Size Adjustment (Adaptive based on convergence)\n            if generation > 10: #Start population size adjust after 10 generation to have enough history\n                 #Check Convergence\n                 if np.std(self.fitness) < self.convergence_threshold:\n                    self.pop_size = max(self.pop_size_min, int(self.pop_size * 0.8)) #reduce population size\n                    print(\"Reduce population size\")\n                 else:\n                    self.pop_size = min(self.pop_size_max, int(self.pop_size * 1.1)) #increase population size\n                    print(\"Increase population size\")\n\n                 #Resize population if population_size changed\n                 if self.pop_size != len(self.pop):\n                    self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                    self.fitness = np.array([func(x) for x in self.pop])\n                    self.velocities = np.zeros_like(self.pop) # Initialize velocities to zero\n                    self.budget -= self.pop_size - len(self.fitness) #Adjust budget\n                    for i in range(self.pop_size):\n                        if self.fitness[i] < self.f_opt:\n                            self.f_opt = self.fitness[i]\n                            self.x_opt = self.pop[i]\n            \n            self.history.append(np.mean(self.fitness))\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm DynamicVelocityDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["39718de5-dd6e-47ad-b4ae-bf6c5d703ffc"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "f7dbc471-ca4b-4af1-9840-c4057ea13520", "fitness": 0.0, "name": "CMAES", "description": "Improved CMA-ES with dynamic population size adjustment, adaptive covariance matrix smoothing, and a more robust restart strategy.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, sigma=0.5, cs=0.3, c_cov=0.1, mu_factor=0.25, restarts=3, adaptation_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.sigma = sigma\n        self.cs = cs\n        self.c_cov = c_cov\n        self.mu = int(self.pop_size * mu_factor)\n        self.weights = np.log(self.pop_size + 1) - np.log(np.arange(1, self.pop_size + 1))\n        self.weights = self.weights[:self.mu] / np.sum(self.weights[:self.mu])\n        self.mean = np.zeros(dim)\n        self.C = np.eye(dim)\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.restarts = restarts\n        self.adaptation_rate = adaptation_rate # Rate for adjusting parameters\n        self.eigenvalues = np.ones(dim)\n        self.adaptation_count = 0\n        self.min_pop_size = 4 + int(3 * np.log(dim))\n        self.max_pop_size = 2 * self.min_pop_size\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.restart_count = 0\n        \n        while self.budget > 0 and self.restart_count < self.restarts:\n            \n            X = self.sample_population()\n            F = np.array([func(x) for x in X])\n            self.budget -= self.pop_size\n            \n            idx = np.argsort(F)\n            X = X[idx]\n            F = F[idx]\n            \n            if F[0] < self.f_opt:\n                self.f_opt = F[0]\n                self.x_opt = X[0]\n                \n            self.update_distribution(X)\n            self.adapt_parameters(F)\n\n            if self.is_stagnated():\n                self.restart()\n                self.restart_count += 1\n\n        return self.f_opt, self.x_opt\n    \n    def sample_population(self):\n        Z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n        A = np.linalg.cholesky(self.C)\n        X = self.mean + self.sigma * Z @ A.T\n        X = np.clip(X, self.bounds_lb, self.bounds_ub)\n        return X\n    \n    def update_distribution(self, X):\n        d = X[:self.mu] - self.mean\n        self.mean = np.sum(self.weights[:, None] * X[:self.mu], axis=0)\n        \n        C_update = np.sum(self.weights[:, None, None] * d[:, :, None] * d[:, None, :], axis=0)\n        \n        # Adaptive covariance matrix smoothing\n        self.C = (1 - self.c_cov) * self.C + self.c_cov * C_update\n        \n        # Ensure C remains positive definite and regularize it\n        try:\n            eigenvalues, eigenvectors = np.linalg.eigh(self.C)\n            eigenvalues = np.maximum(eigenvalues, 1e-8)  # Ensure positive definiteness\n            self.C = eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T\n            self.eigenvalues = eigenvalues # Store eigenvalues for parameter adaptation\n        except np.linalg.LinAlgError:\n            self.C = np.eye(self.dim)\n            self.eigenvalues = np.ones(self.dim)\n            \n        self.sigma *= np.exp(self.cs / 5 * (np.linalg.norm(d) / self.dim - 1))\n        self.sigma = np.clip(self.sigma, 1e-10, 1e10)\n\n    def is_stagnated(self):\n        # Stagnation check considers both sigma and eigenvalue spread\n        return self.sigma < 1e-6 or np.max(self.eigenvalues) / np.min(self.eigenvalues) > 1e6\n\n    def restart(self):\n        self.sigma = 0.5\n        self.mean = np.random.uniform(self.bounds_lb, self.bounds_ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pop_size = 4 + int(3 * np.log(self.dim))\n        self.weights = np.log(self.pop_size + 1) - np.log(np.arange(1, self.pop_size + 1))\n        self.weights = self.weights[:self.mu] / np.sum(self.weights[:self.mu])\n        self.eigenvalues = np.ones(self.dim)\n\n    def adapt_parameters(self, F):\n        # Adapt population size based on performance\n        if self.adaptation_count % 10 == 0: # Adapt every 10 iterations\n            success_ratio = np.mean(F < np.mean(F)) \n            if success_ratio > 0.4:\n                self.pop_size = min(self.pop_size + 1, self.max_pop_size)\n            elif success_ratio < 0.1:\n                self.pop_size = max(self.pop_size - 1, self.min_pop_size)\n            self.mu = int(self.pop_size * 0.25)  #Update mu after changing pop_size\n            self.weights = np.log(self.pop_size + 1) - np.log(np.arange(1, self.pop_size + 1))\n            self.weights = self.weights[:self.mu] / np.sum(self.weights[:self.mu])\n\n\n        #Adapt cs based on sigma\n        if self.sigma > 0.2 :\n            self.cs = min(self.cs * (1 + self.adaptation_rate), 1.0)\n        else:\n            self.cs = max(self.cs * (1 - self.adaptation_rate), 0.1)\n\n        self.adaptation_count += 1", "configspace": "", "generation": 3, "feedback": "The algorithm CMAES scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ea1b4dfb-c1f4-405f-9ffa-9d81ab869002"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "bedf571e-f700-4bd6-a89b-52b24d92b59e", "fitness": 0.3655925327710017, "name": "AdaptiveDECauchy", "description": "Adaptive Differential Evolution with a Cauchy mutation and a learning rate decaying over time.", "code": "import numpy as np\n\nclass AdaptiveDECauchy:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, CR=0.9, learning_rate=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.learning_rate = learning_rate\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        for i in range(self.pop_size):\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.pop[i]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b = self.pop[np.random.choice(idxs, 2, replace=False)]\n                \n                # Cauchy mutation\n                cauchy_noise = np.random.standard_cauchy(size=self.dim)\n                mutant = self.pop[i] + self.F * (a - b) + self.learning_rate * cauchy_noise\n\n                # Crossover\n                CR_adaptive = np.random.uniform(0, self.CR)\n                cross_points = np.random.rand(self.dim) < CR_adaptive\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, self.pop[i])\n                trial = np.clip(trial, self.bounds_lb, self.bounds_ub)\n                \n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n            \n            # Decay learning rate\n            self.learning_rate = self.learning_rate * 0.99\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDECauchy scored 0.366 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["91b1d7cf-7c48-464c-ae5c-dc565e4244fd"], "operator": null, "metadata": {"aucs": [0.14064728950797323, 0.24518985489842116, 0.37475765700700836, 0.35202917854516247, 0.28152228402126167, 0.37563526774016, 0.28624081062110085, 0.31568233659352096, 0.28750686862023245, 0.2132478596200834, 0.33636657856637353, 0.9984239488112652, 0.2772513319740191, 0.2900076095766466, 0.7331596299005988, 0.39084498770041787, 0.29676039831021994, 0.43282307226457584, 0.1923349387106449, 0.4914187524303476]}}
{"id": "188aaffc-7194-4324-aace-ca7210ca4288", "fitness": 0.37833383683432015, "name": "SelfAdaptiveDE", "description": "Differential Evolution with a self-adaptive population size and a learning rate for F and CR based on success history.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size_init=50, archive_size=10, F_init=0.5, CR_init=0.9, F_lr=0.1, CR_lr=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_init = pop_size_init\n        self.pop_size = pop_size_init\n        self.archive_size = archive_size\n        self.F = F_init\n        self.CR = CR_init\n        self.F_lr = F_lr\n        self.CR_lr = CR_lr\n        self.archive = []\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.success_F = []\n        self.success_CR = []\n        self.success_history_size = 10\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        for i in range(self.pop_size):\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.pop[i]\n\n        while self.budget > 0:\n            # Adjust population size\n            if len(self.success_F) > 0:\n                success_rate = len(self.success_F) / self.pop_size\n                if success_rate > 0.3:  # If success rate is high, increase population\n                    self.pop_size = min(self.pop_size + 1, 2 * self.pop_size_init)\n                    self.pop = np.vstack((self.pop, np.random.uniform(self.bounds_lb, self.bounds_ub, size=(1, self.dim))))\n                    self.fitness = np.append(self.fitness, func(self.pop[-1]))\n                    self.budget -= 1\n                    if self.fitness[-1] < self.f_opt:\n                        self.f_opt = self.fitness[-1]\n                        self.x_opt = self.pop[-1]\n                elif success_rate < 0.1 and self.pop_size > self.pop_size_init/2:  # If success rate is low, decrease population\n                    worst_idx = np.argmax(self.fitness)\n                    self.pop = np.delete(self.pop, worst_idx, axis=0)\n                    self.fitness = np.delete(self.fitness, worst_idx)\n                    self.pop_size -= 1\n\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b = self.pop[np.random.choice(idxs, 2, replace=False)]\n\n                mutant = self.pop[i] + self.F * (a - b)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, self.pop[i])\n                trial = np.clip(trial, self.bounds_lb, self.bounds_ub)\n                \n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    delta_f = abs(f_trial - self.fitness[i])\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                    \n                    # Update F and CR based on success\n                    self.success_F.append(self.F)\n                    self.success_CR.append(self.CR)\n                    if len(self.success_F) > self.success_history_size:\n                        self.success_F.pop(0)\n                        self.success_CR.pop(0)\n\n                    self.F = self.F + self.F_lr * np.random.normal(0, 0.1)\n                    self.CR = self.CR + self.CR_lr * np.random.normal(0, 0.1)\n                    self.F = np.clip(self.F, 0.1, 1.0)\n                    self.CR = np.clip(self.CR, 0.1, 1.0)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm SelfAdaptiveDE scored 0.378 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["91b1d7cf-7c48-464c-ae5c-dc565e4244fd"], "operator": null, "metadata": {"aucs": [0.13380521349284513, 0.2399372132908616, 0.3631097337131679, 0.43008398194912, 0.29553319162374947, 0.41199908121363016, 0.2868413866561277, 0.3380805805829493, 0.28758080726786683, 0.1818727103185953, 0.44696309233115616, 0.9993918904884436, 0.3590683114872061, 0.29132765562049867, 0.7030638879432243, 0.3586427412091774, 0.33486980425231494, 0.44418207899946516, 0.17818068202908566, 0.4821426922169174]}}
{"id": "e2249b45-a8a0-4eec-97e2-daed671afb02", "fitness": 0.6750305313281237, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with an improved mutation strategy incorporating the best solution and dynamic parameter adaptation based on fitness improvement.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, CR_init=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.CR = CR_init\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n\n        self.F_history = []\n        self.CR_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.pop[best_idx]\n\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b = self.pop[np.random.choice(idxs, 2, replace=False)]\n\n                # Mutation strategy incorporating the best solution\n                mutant = self.pop[i] + self.F * (self.x_opt - self.pop[i]) + self.F * (a - b)\n\n                # Crossover\n                CR_adaptive = np.random.normal(self.CR, 0.1)  #Adaptive CR\n                CR_adaptive = np.clip(CR_adaptive, 0, 1)\n                cross_points = np.random.rand(self.dim) < CR_adaptive\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, self.pop[i])\n                trial = np.clip(trial, self.bounds_lb, self.bounds_ub)\n                \n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    # Dynamic parameter adaptation based on fitness improvement\n                    self.F = 0.9 * self.F + 0.1 * np.random.rand()\n                    self.CR = 0.9 * self.CR + 0.1 * np.random.rand()\n\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n            best_idx = np.argmin(self.fitness)\n            if self.fitness[best_idx] < self.f_opt:\n                self.f_opt = self.fitness[best_idx]\n                self.x_opt = self.pop[best_idx]\n\n\n                         \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDE scored 0.675 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["91b1d7cf-7c48-464c-ae5c-dc565e4244fd"], "operator": null, "metadata": {"aucs": [0.21083277924943056, 0.52920054660002, 0.7674247268403007, 0.9020948597848828, 0.8101324405480164, 0.8572820757569222, 0.5104862227474256, 0.7045881319619582, 0.8068098907554603, 0.5117725980623359, 0.8651313028437135, 0.9898759015282248, 0.3592851040094621, 0.7370319942496845, 0.9083189233606666, 0.8498070193194497, 0.5794342875974368, 0.8814093515618636, 0.2169989646923235, 0.5026935050928985]}}
{"id": "1bc1c86e-d8fa-480c-b93f-a92a8966a6d3", "fitness": 0.7235562852698927, "name": "EnhancedAdaptiveDE", "description": "Enhanced Simplified Adaptive Differential Evolution with mirrored boundary handling and a more responsive CR adaptation based on success rate.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, F_min=0.1, F_max=0.9, CR_init=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_init = F_init\n        self.F_min = F_min\n        self.F_max = F_max\n        self.CR_init = CR_init\n        self.CR = CR_init  # Initial crossover rate\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.F = np.full(self.pop_size, self.F_init)\n        self.success_history = [] # keep track of successful CR values\n        self.budget_used = 0\n        \n        for i in range(self.pop_size):\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.pop[i]\n        self.budget_used += self.pop_size\n        \n        while self.budget_used < self.budget:\n            successful_CRs = []\n            successful_Fs = []\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n\n                # Self-adaptive F\n                self.F[i] = np.clip(np.random.normal(self.F[i], 0.1), self.F_min, self.F_max)\n                mutant = a + self.F[i] * (b - c)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Mirrored boundary handling\n                trial = np.where(trial < func.bounds.lb, 2 * func.bounds.lb - trial, trial)\n                trial = np.where(trial > func.bounds.ub, 2 * func.bounds.ub - trial, trial)\n                \n                f_trial = func(trial)\n                self.budget_used += 1\n\n                if f_trial < self.fitness[i]:\n                    successful_CRs.append(self.CR)\n                    successful_Fs.append(self.F[i])\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                if self.budget_used >= self.budget:\n                    break\n\n            # CR adaptation based on success history\n            if successful_CRs:\n                self.CR = np.mean(successful_CRs)\n            else:\n                 self.CR = np.random.uniform(0, 1) # Randomize if no success\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm EnhancedAdaptiveDE scored 0.724 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["39718de5-dd6e-47ad-b4ae-bf6c5d703ffc"], "operator": null, "metadata": {"aucs": [0.41503376725542307, 0.7352083840154466, 0.7817884485831234, 0.896508603302632, 0.795047586589587, 0.8221740440719185, 0.7300923768061662, 0.746759969585398, 0.7984153312837226, 0.26819179444613706, 0.8829642362171987, 0.9971451084081825, 0.708019754639343, 0.7499703980215195, 0.9342419132222073, 0.8275571323607409, 0.7384197335977527, 0.8722032499367136, 0.2390887663916168, 0.532295106663021]}}
{"id": "4f535367-3a3e-415c-8ae9-b7cc5915960d", "fitness": 0.28867956425613917, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with improved mutation, crossover, and parameter adaptation based on successful updates, utilizing a mirrored boundary handling.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, archive_size=10, F=0.5, CR=0.9, F_decay=0.99, CR_decay=0.99):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.CR = CR\n        self.archive = []\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.F_decay = F_decay\n        self.CR_decay = CR_decay\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        for i in range(self.pop_size):\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.pop[i]\n\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n\n                mutant = self.pop[i] + self.F * (a - b) + np.random.normal(0, 0.01, self.dim) # Improved mutation and adding noise\n\n                # Crossover\n                CR_adaptive = np.random.uniform(0, self.CR) #Adaptive CR\n                cross_points = np.random.rand(self.dim) < CR_adaptive\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Mirrored boundary handling\n                trial = np.clip(trial, self.bounds_lb, self.bounds_ub)\n\n                for j in range(self.dim):\n                    if trial[j] < self.bounds_lb:\n                        trial[j] = self.bounds_lb + (self.bounds_lb - trial[j])\n                    if trial[j] > self.bounds_ub:\n                        trial[j] = self.bounds_ub - (trial[j] - self.bounds_ub)\n\n                trial = np.clip(trial, self.bounds_lb, self.bounds_ub) # Clip again just in case\n\n                \n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                    self.F *= self.F_decay # Adaptation based on success\n                    self.CR *= self.CR_decay\n                         \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDE scored 0.289 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["91b1d7cf-7c48-464c-ae5c-dc565e4244fd"], "operator": null, "metadata": {"aucs": [0.09652864525237981, 0.19273617729390136, 0.2667436450251519, 0.17458485493064146, 0.21965577052627527, 0.21440419820118928, 0.23370970624735243, 0.20797423911609525, 0.2576297334921359, 0.15806038193632377, 0.28409663420203524, 0.9949449950215691, 0.2696752419416907, 0.1720073092143355, 0.6181855580118436, 0.25121822873211386, 0.26889004109861225, 0.2104838055262035, 0.22793755805134464, 0.4541245613015884]}}
{"id": "56a00235-8c31-4e16-92e7-28ea5134f818", "fitness": 0.7025047035189063, "name": "EnhancedAdaptiveDE", "description": "Enhanced Simplified Adaptive Differential Evolution with rank-based mutation, adaptive CR and F, and a repair mechanism to keep solutions within bounds.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, F_min=0.1, F_max=0.9, CR_init=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_init = F_init\n        self.F_min = F_min\n        self.F_max = F_max\n        self.CR_init = CR_init\n        self.CR = CR_init  # Initial crossover rate\n        self.CR_decay = 0.9 / self.budget # Linear decay rate\n        self.archive_size = int(pop_size * 0.2)  # Archive size (20% of population)\n        self.archive = []\n\n    def repair(self, x, lb, ub):\n        \"\"\"Repair solution to lie within bounds.\"\"\"\n        return np.clip(x, lb, ub)\n    \n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.F = np.full(self.pop_size, self.F_init)\n        \n        self.budget -= self.pop_size\n        \n        for i in range(self.pop_size):\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.pop[i]\n\n        while self.budget > 0:\n            # Rank the population based on fitness\n            ranked_indices = np.argsort(self.fitness)\n            ranked_pop = self.pop[ranked_indices]\n\n            for i in range(self.pop_size):\n                # Mutation using rank-based selection\n                pbest = ranked_pop[0] # Best individual\n                \n                # Select two random individuals, avoid duplicates\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b = self.pop[np.random.choice(idxs, 2, replace=False)]\n                \n                # Self-adaptive F\n                self.F[i] = np.clip(np.random.normal(self.F[i], 0.1), self.F_min, self.F_max)\n                \n                mutant = self.repair(pbest + self.F[i] * (a - b), func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, self.pop[i])\n                trial = self.repair(trial, func.bounds.lb, func.bounds.ub)\n                \n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        \n            # Adaptive CR (adjust based on success)\n            success_indices = self.fitness < np.mean(self.fitness)\n            if np.any(success_indices):\n                self.CR = np.mean(self.F[success_indices])  # Adapt CR based on successful F values\n            else:\n                self.CR = self.CR_init\n                \n            self.CR = max(0, self.CR - self.CR_decay)\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm EnhancedAdaptiveDE scored 0.703 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["39718de5-dd6e-47ad-b4ae-bf6c5d703ffc"], "operator": null, "metadata": {"aucs": [0.2503118721038071, 0.611724930017642, 0.8172658000792254, 0.9323169345124142, 0.8441267575743253, 0.9054432673676683, 0.8424413174816543, 0.651266566739229, 0.8606650234426573, 0.21786436936183207, 0.9196087135447236, 0.9976433671068847, 0.34099687324428873, 0.6563988355831686, 0.9573780218538652, 0.8923056392354192, 0.7324284566246579, 0.9130230850050943, 0.2036779651765479, 0.5032062743230195]}}
{"id": "3d42d95a-333c-42ac-b998-b78995b5e82c", "fitness": 0.7450470856946689, "name": "SimplifiedAdaptiveDE", "description": "Simplified Adaptive Differential Evolution with archive, linearly decreasing CR, adaptive F, and tournament selection.", "code": "import numpy as np\n\nclass SimplifiedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, archive_size=20, F_init=0.5, F_min=0.1, F_max=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F_init = F_init\n        self.F_min = F_min\n        self.F_max = F_max\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        self.F = np.full(self.pop_size, self.F_init)\n        self.CR = 0.9  # Initial crossover rate\n        self.CR_decay = 0.9 / self.budget # Linear decay rate\n        self.budget -= self.pop_size\n        \n        for i in range(self.pop_size):\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.pop[i]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b = self.pop[np.random.choice(idxs, 2, replace=False)]\n                \n                #Choose c from population or archive\n                if np.random.rand() < 0.5:\n                  c = self.pop[np.random.choice(idxs, 1, replace=False)][0]\n                else:\n                  c = self.archive[np.random.choice(range(self.archive_size), 1, replace=False)][0]\n\n                # Self-adaptive F\n                self.F[i] = np.clip(np.random.normal(self.F[i], 0.1), self.F_min, self.F_max)\n                mutant = a + self.F[i] * (b - c)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, self.pop[i])\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n                \n                f_trial = func(trial)\n                self.budget -= 1\n                \n                #Tournament selection\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                else:\n                    # Add the old vector to the archive\n                    j = np.random.randint(0, self.archive_size)\n                    self.archive[j] = self.pop[i]\n\n            self.CR -= self.CR_decay  #Linear decay of CR\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm SimplifiedAdaptiveDE scored 0.745 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["39718de5-dd6e-47ad-b4ae-bf6c5d703ffc"], "operator": null, "metadata": {"aucs": [0.44569281040982933, 0.7384361269108033, 0.7695870564618748, 0.9014977270278581, 0.7996565168990478, 0.8338700716748256, 0.7423418059307038, 0.7195906144157205, 0.8138367523924541, 0.6724251389346545, 0.8847359969816927, 0.9979917837776728, 0.7086743619580294, 0.8096547620972453, 0.9300520348164403, 0.831680338896434, 0.6995215248960676, 0.8564610623429638, 0.24091977878947413, 0.5043154482795893]}}
{"id": "63023cce-3683-4927-a846-947e327c5c8c", "fitness": 0.40076863850872124, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with simplified mutation, adaptive CR, and jittering to escape local optima.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, CR=0.9, jitter_prob=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.jitter_prob = jitter_prob\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        for i in range(self.pop_size):\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.pop[i]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b = self.pop[np.random.choice(idxs, 2, replace=False)]\n\n                mutant = self.pop[i] + self.F * (a - b) #Simplified mutation\n\n                # Jittering\n                jitter = np.random.uniform(-0.1, 0.1, size=self.dim) * np.random.choice([0, 1], size=self.dim, p=[1-self.jitter_prob, self.jitter_prob])\n                mutant = mutant + jitter\n\n\n                # Crossover\n                CR_adaptive = np.random.uniform(0, self.CR) #Adaptive CR\n                cross_points = np.random.rand(self.dim) < CR_adaptive\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, self.pop[i])\n                trial = np.clip(trial, self.bounds_lb, self.bounds_ub)\n                \n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                         \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDE scored 0.401 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["91b1d7cf-7c48-464c-ae5c-dc565e4244fd"], "operator": null, "metadata": {"aucs": [0.15445901682429009, 0.24585197400887415, 0.38817626984303333, 0.4663765212615525, 0.3173312094080938, 0.44072271377687366, 0.30153922609001127, 0.33769974311235396, 0.3234924163116877, 0.19385202699950965, 0.5016827107668016, 0.9976466540002877, 0.35077575394021787, 0.2906303193284009, 0.7554225418663989, 0.4297687002152606, 0.3344267129350351, 0.4895819998915151, 0.2079263488897517, 0.48800991070447575]}}
{"id": "e0c5f55e-102c-4014-8937-5a0bbd1c389c", "fitness": -Infinity, "name": "HybridCMAES_NM", "description": "Hybrid optimization algorithm combining a simplified CMA-ES for global search with a Nelder-Mead local search for refinement.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridCMAES_NM:\n    def __init__(self, budget=10000, dim=10, pop_size=None, sigma=0.5, cs=0.3, c_cov=0.1, mu_factor=0.25, restarts=3, nm_max_iter=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.sigma = sigma\n        self.cs = cs\n        self.c_cov = c_cov\n        self.mu = int(self.pop_size * mu_factor)\n        self.weights = np.log(self.pop_size + 1) - np.log(np.arange(1, self.pop_size + 1))\n        self.weights = self.weights[:self.mu] / np.sum(self.weights[:self.mu])\n        self.mean = np.zeros(dim)\n        self.C = np.eye(dim)\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.restarts = restarts\n        self.nm_max_iter = nm_max_iter # Maximum iterations for Nelder-Mead\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.restart_count = 0\n        \n        while self.budget > 0 and self.restart_count < self.restarts:\n            \n            X = self.sample_population()\n            F = np.array([func(x) for x in X])\n            self.budget -= self.pop_size\n            \n            idx = np.argsort(F)\n            X = X[idx]\n            F = F[idx]\n            \n            if F[0] < self.f_opt:\n                self.f_opt = F[0]\n                self.x_opt = X[0]\n            \n            # Nelder-Mead local search around the best CMA-ES solution\n            nm_budget = min(self.nm_max_iter, self.budget // (self.restarts - self.restart_count)) # Distribute NM budget among restarts.\n            if nm_budget > 0:\n                res = minimize(func, self.x_opt, method='Nelder-Mead', \n                               bounds=list(zip([self.bounds_lb]*self.dim, [self.bounds_ub]*self.dim)),\n                               options={'maxiter': nm_budget, 'maxfev': nm_budget}) # Use maxfev to limit function evaluations\n                \n                self.budget -= res.nfev\n                if res.fun < self.f_opt:\n                    self.f_opt = res.fun\n                    self.x_opt = res.x\n            \n            self.update_distribution(X)\n\n            if self.is_stagnated():\n                self.restart()\n                self.restart_count += 1\n\n        return self.f_opt, self.x_opt\n    \n    def sample_population(self):\n        Z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n        A = np.linalg.cholesky(self.C)\n        X = self.mean + self.sigma * Z @ A.T\n        X = np.clip(X, self.bounds_lb, self.bounds_ub)\n        return X\n    \n    def update_distribution(self, X):\n        d = X[:self.mu] - self.mean\n        self.mean = np.sum(self.weights[:, None] * X[:self.mu], axis=0)\n        \n        C_update = np.sum(self.weights[:, None, None] * d[:, :, None] * d[:, None, :], axis=0)\n        self.C = (1 - self.c_cov) * self.C + self.c_cov * C_update\n        \n        # Ensure C remains positive definite\n        try:\n            np.linalg.cholesky(self.C)\n        except np.linalg.LinAlgError:\n            self.C = np.eye(self.dim)\n            \n        self.sigma *= np.exp(self.cs / 5 * (np.linalg.norm(d) / self.dim - 1))\n        self.sigma = np.clip(self.sigma, 1e-10, 1e10)\n\n    def is_stagnated(self):\n        return self.sigma < 1e-6\n\n    def restart(self):\n        self.sigma = 0.5\n        self.mean = np.random.uniform(self.bounds_lb, self.bounds_ub, size=self.dim)\n        self.C = np.eye(self.dim)", "configspace": "", "generation": 3, "feedback": "An exception occurred: name 'minimize' is not defined.", "error": "", "parent_ids": ["ea1b4dfb-c1f4-405f-9ffa-9d81ab869002"], "operator": null, "metadata": {}}
{"id": "c1703b54-c7fb-4cd7-a047-ebee6e29f761", "fitness": 0.06731935636840852, "name": "EnhancedAdaptiveDE", "description": "Enhanced Differential Evolution with Simulated Annealing-inspired acceptance, adaptive mutation, and a restart mechanism for improved exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, archive_size=10, F=0.5, CR=0.9, temp_init=1.0, temp_decay=0.99):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.CR = CR\n        self.archive = []\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.temp = temp_init\n        self.temp_decay = temp_decay\n        self.restart_patience = 500\n        self.no_improvement_counter = 0\n        self.best_fitness_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        for i in range(self.pop_size):\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.pop[i]\n\n        self.best_fitness_history.append(self.f_opt)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n\n                F_adaptive = np.random.uniform(0.1, self.F)  # Adaptive F\n                mutant = self.pop[i] + F_adaptive * (a - b)  + np.random.normal(0, 0.01, self.dim) #Enhanced mutation with noise\n\n\n                # Crossover\n                CR_adaptive = np.random.uniform(0, self.CR) #Adaptive CR\n                cross_points = np.random.rand(self.dim) < CR_adaptive\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, self.pop[i])\n                trial = np.clip(trial, self.bounds_lb, self.bounds_ub)\n                \n                f_trial = func(trial)\n                self.budget -= 1\n                \n                delta = f_trial - self.fitness[i]\n\n                # Simulated Annealing-inspired acceptance\n                if delta < 0 or np.random.rand() < np.exp(-delta / self.temp):\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.no_improvement_counter = 0  # Reset counter if improvement found\n                    else:\n                         self.no_improvement_counter += 1 # Increment the counter if no improvement is found.\n                else:\n                    self.no_improvement_counter += 1 # Increment the counter if no improvement is found.\n            \n            self.temp *= self.temp_decay #Cooling\n\n            self.best_fitness_history.append(self.f_opt)\n\n\n            # Restart mechanism\n            if self.no_improvement_counter > self.restart_patience:\n                self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.pop])\n                self.budget -= self.pop_size # Account for new evaluations\n                self.no_improvement_counter = 0\n                for i in range(self.pop_size):\n                    if self.fitness[i] < self.f_opt:\n                        self.f_opt = self.fitness[i]\n                        self.x_opt = self.pop[i]\n                self.best_fitness_history.append(self.f_opt)\n            \n                         \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm EnhancedAdaptiveDE scored 0.067 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["91b1d7cf-7c48-464c-ae5c-dc565e4244fd"], "operator": null, "metadata": {"aucs": [0.13463871273681705, 0]}}
{"id": "fe0fcbc4-6bd8-4722-b96b-1b086a91ff99", "fitness": 0.0, "name": "DE_GLS", "description": "Differential Evolution with a Gaussian Local Search to enhance exploitation around promising solutions, dynamically adjusting search intensity.", "code": "import numpy as np\n\nclass DE_GLS:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, CR=0.9, ls_prob=0.1, ls_scale=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.ls_prob = ls_prob  # Probability of performing local search\n        self.ls_scale = ls_scale # Scaling factor for local search step size\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n    def repair(self, x, lb, ub):\n        \"\"\"Repair solution to lie within bounds.\"\"\"\n        return np.clip(x, lb, ub)\n\n    def local_search(self, x, func, lb, ub):\n        \"\"\"Perform a Gaussian local search around x.\"\"\"\n        x_ls = x + np.random.normal(0, self.ls_scale * (ub - lb), size=self.dim)\n        x_ls = self.repair(x_ls, lb, ub)\n        f_ls = func(x_ls)\n        self.budget -= 1 # One function evaluation consumed by local search.\n        return x_ls, f_ls\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.pop = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n\n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.pop[best_idx]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = self.repair(a + self.F * (b - c), lb, ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n                trial = self.repair(trial, lb, ub)\n\n                # Evaluate trial vector\n                f_trial = func(trial)\n                self.budget -= 1\n\n                # Local Search\n                if np.random.rand() < self.ls_prob and self.budget > 0:\n                    trial_ls, f_trial_ls = self.local_search(trial, func, lb, ub)\n                    if f_trial_ls < f_trial:\n                        f_trial = f_trial_ls\n                        trial = trial_ls\n\n                # Selection\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm DE_GLS scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["56a00235-8c31-4e16-92e7-28ea5134f818"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "3739b936-1f8f-443b-be4f-a1f8d4aede06", "fitness": -Infinity, "name": "OrthogonalDE", "description": "Differential Evolution with orthogonal learning, where orthogonal array design selects promising crossover points to enhance exploration and exploitation.", "code": "import numpy as np\nfrom pyDOE import orthogonal\n\nclass OrthogonalDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, CR=0.9, orthogonal_level=3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.orthogonal_level = orthogonal_level\n        self.orthogonal_strength = 2  # Strength of the orthogonal array\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget_used = 0\n\n        for i in range(self.pop_size):\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.pop[i]\n        self.budget_used += self.pop_size\n\n        while self.budget_used < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = a + self.F * (b - c)\n                \n                # Boundary Handling\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Orthogonal Crossover\n                num_factors = int(np.ceil(self.dim / self.orthogonal_strength))\n                if num_factors > 1:\n                    if self.orthogonal_level > 1:\n                        try:\n                            orthogonal_array = orthogonal(self.orthogonal_level, num_factors).astype(int)\n                        except:\n                            orthogonal_array = np.random.randint(0,self.orthogonal_level, size=(self.orthogonal_level**2, num_factors)).astype(int)\n                    else:\n                         orthogonal_array = np.random.randint(0,self.orthogonal_level, size=(self.orthogonal_level**2, num_factors)).astype(int)\n\n                    for row in orthogonal_array:\n                        trial = self.pop[i].copy()\n                        for j in range(num_factors):\n                            start_index = j * self.orthogonal_strength\n                            end_index = min((j + 1) * self.orthogonal_strength, self.dim)\n                            \n                            if row[j] > 0:  # Switch values according to orthogonal array\n                                trial[start_index:end_index] = mutant[start_index:end_index] \n\n                        f_trial = func(trial)\n                        self.budget_used += 1\n\n                        if f_trial < self.fitness[i]:\n                            self.fitness[i] = f_trial\n                            self.pop[i] = trial\n                            if f_trial < self.f_opt:\n                                self.f_opt = f_trial\n                                self.x_opt = trial\n                        if self.budget_used >= self.budget:\n                            break\n                else:\n                    # Standard Crossover if orthogonal array is not applicable\n                    cross_points = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, self.pop[i])\n                    \n                    # Boundary Handling\n                    trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n                    f_trial = func(trial)\n                    self.budget_used += 1\n\n                    if f_trial < self.fitness[i]:\n                        self.fitness[i] = f_trial\n                        self.pop[i] = trial\n                        if f_trial < self.f_opt:\n                            self.f_opt = f_trial\n                            self.x_opt = trial\n                    if self.budget_used >= self.budget:\n                        break\n            if self.budget_used >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "An exception occurred: No module named 'pyDOE'.", "error": "", "parent_ids": ["1bc1c86e-d8fa-480c-b93f-a92a8966a6d3"], "operator": null, "metadata": {}}
{"id": "7c3db53c-d3d4-42e3-8298-762fd55f4c08", "fitness": 0.0, "name": "AdaptiveDE_PopSize", "description": "Adaptive Differential Evolution with a self-adaptive population size and a combined mutation strategy.", "code": "import numpy as np\n\nclass AdaptiveDE_PopSize:\n    def __init__(self, budget=10000, dim=10, pop_size_init=40, F_init=0.5, CR_init=0.9, pop_size_min=10, pop_size_max=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size_init\n        self.pop_size_init = pop_size_init\n        self.pop_size_min = pop_size_min\n        self.pop_size_max = pop_size_max\n        self.F = F_init\n        self.CR = CR_init\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.success_memory = []\n        self.success_threshold = 0.1\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.pop[best_idx]\n\n\n        while self.budget > 0:\n            \n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n\n                # Combined mutation strategy\n                if np.random.rand() < 0.5:\n                    # DE/rand/1\n                    mutant = self.pop[i] + self.F * (a - b)\n                else:\n                    # DE/current-to-best/1\n                    mutant = self.pop[i] + self.F * (self.x_opt - self.pop[i]) + self.F * (a - b)\n                \n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, self.pop[i])\n                trial = np.clip(trial, self.bounds_lb, self.bounds_ub)\n                \n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.success_memory.append(1)\n                    else:\n                        self.success_memory.append(0)\n                    \n                else:\n                    self.success_memory.append(0)\n            \n            best_idx = np.argmin(self.fitness)\n            if self.fitness[best_idx] < self.f_opt:\n                self.f_opt = self.fitness[best_idx]\n                self.x_opt = self.pop[best_idx]\n            \n            # Adaptive population size adjustment\n            success_rate = np.mean(self.success_memory[-min(len(self.success_memory), self.pop_size):]) if self.success_memory else 0\n\n            if success_rate > self.success_threshold and self.pop_size < self.pop_size_max:\n                self.pop_size = min(self.pop_size + 1, self.pop_size_max)\n                self.pop = np.vstack((self.pop, np.random.uniform(self.bounds_lb, self.bounds_ub, size=(1, self.dim))))\n                new_fitness = np.array([func(self.pop[-1])])\n                self.budget -=1\n                self.fitness = np.append(self.fitness, new_fitness)\n\n            elif success_rate < self.success_threshold and self.pop_size > self.pop_size_min:\n                worst_idx = np.argmax(self.fitness)\n                self.pop = np.delete(self.pop, worst_idx, axis=0)\n                self.fitness = np.delete(self.fitness, worst_idx)\n                self.pop_size = max(self.pop_size - 1, self.pop_size_min)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDE_PopSize scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e2249b45-a8a0-4eec-97e2-daed671afb02"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "937f5215-e524-48d2-b857-89ecccaa5aba", "fitness": -Infinity, "name": "AdaptiveDE", "description": "An enhanced Adaptive Differential Evolution strategy employing a mirroring technique for boundary handling, combined with a pool of mutation strategies adaptively selected based on their recent success.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, CR_init=0.9, mutation_pool_size=3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.CR = CR_init\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.mutation_pool_size = mutation_pool_size\n\n        self.mutation_strategies = [\n            self.mutation_current_to_best_1,\n            self.mutation_rand_1,\n            self.mutation_best_1,\n        ]\n        self.mutation_success_rates = np.ones(len(self.mutation_strategies)) / len(self.mutation_strategies)\n\n\n    def mutation_current_to_best_1(self, pop, i, best_x):\n        idxs = [idx for idx in range(self.pop_size) if idx != i]\n        a, b = pop[np.random.choice(idxs, 2, replace=False)]\n        return pop[i] + self.F * (best_x - pop[i]) + self.F * (a - b)\n\n    def mutation_rand_1(self, pop):\n        idxs = np.random.choice(self.pop_size, 5, replace=False)\n        a, b, c, d, e = pop[idxs]\n        return a + self.F * (b - c) + self.F * (d - e)\n\n    def mutation_best_1(self, pop, best_x):\n        idxs = np.random.choice(self.pop_size, 3, replace=False)\n        a, b, c = pop[idxs]\n        return best_x + self.F * (a - b)\n    \n    def boundary_handling_mirror(self, x):\n        \"\"\"Handles boundaries using mirroring.\"\"\"\n        violated_low = x < self.bounds_lb\n        violated_high = x > self.bounds_ub\n\n        x[violated_low] = 2 * self.bounds_lb - x[violated_low]\n        x[violated_high] = 2 * self.bounds_ub - x[violated_high]\n\n        return x\n    \n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.pop[best_idx]\n\n\n        while self.budget > 0:\n            mutation_indices = np.random.choice(len(self.mutation_strategies), size=self.pop_size, p=self.mutation_success_rates)\n            trial_fitnesses = np.zeros(self.pop_size)\n            trial_vectors = np.zeros((self.pop_size, self.dim))\n\n            for i in range(self.pop_size):\n                mutation_strategy = self.mutation_strategies[mutation_indices[i]]\n\n                # Mutation\n                if mutation_strategy == self.mutation_rand_1:\n                   mutant = mutation_strategy(self.pop)\n                else:\n                   mutant = mutation_strategy(self.pop, i, self.x_opt)\n                \n\n                # Crossover\n                CR_adaptive = np.random.normal(self.CR, 0.1)  #Adaptive CR\n                CR_adaptive = np.clip(CR_adaptive, 0, 1)\n                cross_points = np.random.rand(self.dim) < CR_adaptive\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, self.pop[i])\n                trial = self.boundary_handling_mirror(trial)\n                \n                f_trial = func(trial)\n                self.budget -= 1\n                trial_fitnesses[i] = f_trial\n                trial_vectors[i] = trial\n\n\n            # Selection and update mutation success rates\n            for i in range(self.pop_size):\n                if trial_fitnesses[i] < self.fitness[i]:\n                    self.fitness[i] = trial_fitnesses[i]\n                    self.pop[i] = trial_vectors[i]\n\n                    mutation_idx = mutation_indices[i]\n                    self.mutation_success_rates[mutation_idx] *= 1.1  # Increase success rate\n                    self.mutation_success_rates /= np.sum(self.mutation_success_rates)  # Normalize\n                    \n                    if trial_fitnesses[i] < self.f_opt:\n                        self.f_opt = trial_fitnesses[i]\n                        self.x_opt = trial_vectors[i]\n                else:\n                    mutation_idx = mutation_indices[i]\n                    self.mutation_success_rates[mutation_idx] *= 0.9  # Decrease success rate\n                    self.mutation_success_rates /= np.sum(self.mutation_success_rates)  # Normalize\n\n\n            best_idx = np.argmin(self.fitness)\n            if self.fitness[best_idx] < self.f_opt:\n                self.f_opt = self.fitness[best_idx]\n                self.x_opt = self.pop[best_idx]\n                         \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "An exception occurred: AdaptiveDE.mutation_best_1() takes 3 positional arguments but 4 were given.", "error": "", "parent_ids": ["e2249b45-a8a0-4eec-97e2-daed671afb02"], "operator": null, "metadata": {}}
{"id": "d3fa9860-de90-4772-81a8-6b2724130a9f", "fitness": -Infinity, "name": "CooperativeClusteredEnhancedAdaptiveDE", "description": "Cooperative Enhanced Adaptive DE with Population Clustering for Diversity Maintenance, using k-means to promote exploration in under-explored regions.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass CooperativeClusteredEnhancedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, F_min=0.1, F_max=0.9, CR_init=0.9, num_clusters=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_init = F_init\n        self.F_min = F_min\n        self.F_max = F_max\n        self.CR_init = CR_init\n        self.CR = CR_init  # Initial crossover rate\n        self.num_clusters = num_clusters  # Number of clusters for k-means\n        self.clustering_interval = 50 # Perform clustering every n iterations\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.F = np.full(self.pop_size, self.F_init)\n        self.success_history = [] # keep track of successful CR values\n        self.budget_used = 0\n        self.generation = 0\n        \n        for i in range(self.pop_size):\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.pop[i]\n        self.budget_used += self.pop_size\n        \n        while self.budget_used < self.budget:\n            successful_CRs = []\n            successful_Fs = []\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n\n                # Self-adaptive F\n                self.F[i] = np.clip(np.random.normal(self.F[i], 0.1), self.F_min, self.F_max)\n                mutant = a + self.F[i] * (b - c)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Mirrored boundary handling\n                trial = np.where(trial < func.bounds.lb, 2 * func.bounds.lb - trial, trial)\n                trial = np.where(trial > func.bounds.ub, 2 * func.bounds.ub - trial, trial)\n                \n                f_trial = func(trial)\n                self.budget_used += 1\n\n                if f_trial < self.fitness[i]:\n                    successful_CRs.append(self.CR)\n                    successful_Fs.append(self.F[i])\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                if self.budget_used >= self.budget:\n                    break\n            \n            # Population clustering for diversity\n            if self.generation % self.clustering_interval == 0:\n                kmeans = KMeans(n_clusters=self.num_clusters, n_init = 'auto')  # Explicitly set n_init\n                cluster_labels = kmeans.fit_predict(self.pop)\n                \n                # Find the least represented cluster\n                cluster_counts = np.bincount(cluster_labels)\n                least_represented_cluster = np.argmin(cluster_counts)\n\n                # Select an individual from the most represented cluster\n                most_represented_cluster = np.argmax(cluster_counts)\n                members_most_represented = np.where(cluster_labels == most_represented_cluster)[0]\n                if len(members_most_represented) > 0:\n                    replace_idx = np.random.choice(members_most_represented)\n                    \n                    # Replace it with a random solution within the bounds\n                    self.pop[replace_idx] = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                    self.fitness[replace_idx] = func(self.pop[replace_idx])\n                    self.budget_used += 1\n                    if self.fitness[replace_idx] < self.f_opt:\n                        self.f_opt = self.fitness[replace_idx]\n                        self.x_opt = self.pop[replace_idx]\n            \n\n            # CR adaptation based on success history\n            if successful_CRs:\n                self.CR = np.mean(successful_CRs)\n            else:\n                 self.CR = np.random.uniform(0, 1) # Randomize if no success\n            \n            self.generation += 1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "An exception occurred: name 'KMeans' is not defined.", "error": "", "parent_ids": ["1bc1c86e-d8fa-480c-b93f-a92a8966a6d3"], "operator": null, "metadata": {}}
{"id": "becbba05-54f8-4d12-8afe-6714a0b23543", "fitness": 0.0, "name": "AdaptiveDELocalSearch", "description": "An adaptive Differential Evolution strategy that incorporates a local search operator and dynamically adjusts its parameters based on population diversity.", "code": "import numpy as np\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=40, archive_size=20, F_init=0.5, F_min=0.1, F_max=0.9, local_search_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F_init = F_init\n        self.F_min = F_min\n        self.F_max = F_max\n        self.local_search_prob = local_search_prob  # Probability of applying local search\n        self.tau_F = 0.1\n        self.tau_CR = 0.1\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        self.F = np.full(self.pop_size, self.F_init)\n        self.CR = np.full(self.pop_size, 0.9)  # Individual crossover rates\n        self.budget -= self.pop_size\n        \n        for i in range(self.pop_size):\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.pop[i]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n\n                # Self-adaptive F and CR\n                if np.random.rand() < self.tau_F:\n                    self.F[i] = np.clip(np.random.normal(self.F_init, 0.1), self.F_min, self.F_max)\n                if np.random.rand() < self.tau_CR:\n                    self.CR[i] = np.random.rand()\n\n                mutant = a + self.F[i] * (b - c)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR[i]\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, self.pop[i])\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Local Search\n                if np.random.rand() < self.local_search_prob:\n                    trial = self.local_search(trial, func, func.bounds.lb, func.bounds.ub)\n                    \n                f_trial = func(trial)\n                self.budget -= 1\n                \n                # Selection\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                else:\n                    # Add the old vector to the archive\n                    j = np.random.randint(0, self.archive_size)\n                    self.archive[j] = self.pop[i]\n                    \n            # Adaptive Parameter Control (Example: Adjust local_search_prob based on diversity)\n            diversity = np.std(self.fitness)  # Simple measure of population diversity\n            self.local_search_prob = 0.1 + 0.4 * (1.0 / (1.0 + np.exp(-diversity)))  # Sigmoid function\n\n        return self.f_opt, self.x_opt\n\n    def local_search(self, x, func, lb, ub, radius=0.1):\n        \"\"\"Performs a simple local search around x.\"\"\"\n        x_new = x.copy()\n        for _ in range(5):  # Limited iterations for efficiency\n            delta = np.random.uniform(-radius, radius, size=self.dim)\n            x_new = x + delta\n            x_new = np.clip(x_new, lb, ub)\n            f_new = func(x_new)\n            self.budget -= 1\n            if f_new < func(x):\n                x = x_new.copy()\n                break #Improvement found, stop searching\n        return x", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDELocalSearch scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3d42d95a-333c-42ac-b998-b78995b5e82c"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "db027ac2-5f2d-40cf-815f-ed461ac51883", "fitness": -Infinity, "name": "CMAES", "description": "Covariance matrix adaptation evolution strategy with dynamic population size adjustment and adaptive step size control.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, sigma=0.5, cs=0.3, damps=None, c_cov=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.sigma = sigma\n        self.mean = None  # Initialize mean in __call__\n        self.C = np.eye(dim)  # Covariance matrix\n        self.p_sigma = np.zeros(dim)  # Evolution path for sigma\n        self.p_c = np.zeros(dim)  # Evolution path for covariance\n        self.cs = cs\n        self.damps = damps if damps is not None else 1 + 2 * max(0, np.sqrt((self.mu_eff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.c_cov = c_cov\n        self.mu = self.pop_size // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.mu_eff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.c_1 = self.c_cov / ((self.dim + 1.3)**2 + self.mu_eff)\n        self.c_mu = min(1 - self.c_1, self.c_cov * (self.mu_eff - 2 + 1/self.mu_eff) / ((self.dim + 2)**2 + self.mu_eff))\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        self.mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n\n        while self.budget > 0:\n            # Generate population\n            z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n            y = np.dot(z, np.linalg.cholesky(self.C).T)\n            x = self.mean + self.sigma * y\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n\n            # Evaluate population\n            fitness = np.array([func(xi) for xi in x])\n            self.budget -= self.pop_size\n\n            # Sort by fitness\n            idx = np.argsort(fitness)\n            fitness = fitness[idx]\n            x = x[idx]\n            y = y[idx]\n\n            # Update optimal solution\n            if fitness[0] < self.f_opt:\n                self.f_opt = fitness[0]\n                self.x_opt = x[0]\n\n            # Update mean\n            y_mean = np.sum(self.weights[:, None] * y[:self.mu], axis=0)\n            self.mean += self.sigma * y_mean\n\n            # Update evolution paths\n            self.p_sigma = (1 - self.cs) * self.p_sigma + np.sqrt(self.cs * (2 - self.cs) * self.mu_eff) * y_mean\n            hsig = np.linalg.norm(self.p_sigma) / np.sqrt(1 - (1 - self.cs)**(2 * (self.budget / self.pop_size))) / np.sqrt(self.dim) < 1.4 + 2 / (self.dim + 1)\n            self.p_c = (1 - self.c_cov) * self.p_c + hsig * np.sqrt(self.c_cov * (2 - self.c_cov) * self.mu_eff) * y_mean\n\n            # Update covariance matrix\n            self.C = (1 - self.c_1 - self.c_mu) * self.C + self.c_1 * np.outer(self.p_c, self.p_c) + self.c_mu * np.sum(self.weights[:, None, None] * y[:self.mu, :, None] * y[:self.mu, None, :], axis=0)\n\n            # Update step size\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.p_sigma) / np.sqrt(self.dim) - 1))\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T  # enforce symmetry\n            try:\n                np.linalg.cholesky(self.C)  # Ensure C is positive definite\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)  # Reset C if not positive definite\n\n            # Adjust population size dynamically (example, can be more sophisticated)\n            if self.budget < 0.2 * self.budget: # Reduce pop size\n                self.pop_size = max(4 + int(2 * np.log(self.dim)), self.pop_size // 2)\n            elif self.budget > 0.8 * self.budget: # Increase pop size slightly\n                self.pop_size = min(self.pop_size + 2, 4 + int(3 * np.log(self.dim))) # Prevent it becoming too large\n\n            self.mu = self.pop_size // 2\n            self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n            self.weights /= np.sum(self.weights)\n            self.mu_eff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n            self.c_1 = self.c_cov / ((self.dim + 1.3)**2 + self.mu_eff)\n            self.c_mu = min(1 - self.c_1, self.c_cov * (self.mu_eff - 2 + 1/self.mu_eff) / ((self.dim + 2)**2 + self.mu_eff))\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "An exception occurred: 'CMAES' object has no attribute 'mu_eff'.", "error": "", "parent_ids": ["56a00235-8c31-4e16-92e7-28ea5134f818"], "operator": null, "metadata": {}}
{"id": "e06dcf6e-042f-4536-8680-8f77907f268a", "fitness": 0.685027627020031, "name": "AdaptiveDE", "description": "Simplified Adaptive Differential Evolution with mirrored boundary handling and adaptive CR and F, prioritizing exploration with a broader CR range and simplified parameter updates.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, CR_init=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.CR = CR_init\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.pop[best_idx]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b = self.pop[np.random.choice(idxs, 2, replace=False)]\n\n                # Mutation strategy incorporating the best solution\n                mutant = self.pop[i] + self.F * (self.x_opt - self.pop[i]) + self.F * (a - b)\n                \n                # Mirrored boundary handling\n                mutant = np.where(mutant < self.bounds_lb, 2 * self.bounds_lb - mutant, mutant)\n                mutant = np.where(mutant > self.bounds_ub, 2 * self.bounds_ub - mutant, mutant)\n\n                # Crossover\n                CR_adaptive = np.random.normal(self.CR, 0.2)  # Broader CR range for exploration\n                CR_adaptive = np.clip(CR_adaptive, 0, 1)\n                cross_points = np.random.rand(self.dim) < CR_adaptive\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, self.pop[i])\n                \n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    # Simplified parameter adaptation\n                    self.F = 0.8 * self.F + 0.2 * np.random.rand()\n                    self.CR = 0.8 * self.CR + 0.2 * np.random.rand()\n\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                         \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDE scored 0.685 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e2249b45-a8a0-4eec-97e2-daed671afb02"], "operator": null, "metadata": {"aucs": [0.22222002050207157, 0.6203002642837075, 0.799388396433679, 0.9062766640419686, 0.7918065521906963, 0.8451919215878789, 0.36017457070358694, 0.742337903089918, 0.8069948633935999, 0.44588380210658385, 0.8796265176608178, 0.9978180030984216, 0.38237522457140116, 0.769622511527825, 0.9253491573977735, 0.8184797159994697, 0.6559384646043342, 0.8893527375740593, 0.3310650589599845, 0.5103501906728414]}}
{"id": "aef2baee-46ea-469d-9989-7ab97bf23ec1", "fitness": 0.38281299547577574, "name": "RankCauchyAdaptiveDE", "description": "An Adaptive Differential Evolution algorithm with a novel mutation strategy based on the ranking of individuals and a Cauchy distribution-based scaling factor, aiming for better exploration and exploitation.", "code": "import numpy as np\n\nclass RankCauchyAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, archive_size=20):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        self.CR = 0.5  # Fixed crossover rate\n        self.F = 0.5  # Fixed scaling factor\n        self.budget -= self.pop_size\n\n        for i in range(self.pop_size):\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.pop[i]\n\n        while self.budget > 0:\n            # Rank the population\n            ranked_indices = np.argsort(self.fitness)\n\n            for i in range(self.pop_size):\n                # Mutation using ranked individuals and Cauchy distribution\n                best_idx = ranked_indices[0]\n                worst_idx = ranked_indices[-1]\n                \n                #Cauchy distribution for scaling factor\n                cauchy_scale = 0.1\n                L = np.random.standard_cauchy(1)[0] * cauchy_scale #cauchy distributed random number\n                \n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b = self.pop[np.random.choice(idxs, 2, replace=False)]\n\n                mutant = self.pop[i] + L * (self.pop[best_idx] - self.pop[worst_idx]) + self.F * (a - b)  # Rank-based mutation\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, self.pop[i])\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                else:\n                    # Update archive\n                    j = np.random.randint(0, self.archive_size)\n                    self.archive[j] = self.pop[i]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm RankCauchyAdaptiveDE scored 0.383 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3d42d95a-333c-42ac-b998-b78995b5e82c"], "operator": null, "metadata": {"aucs": [0.15014746783645694, 0.2320626252300364, 0.3773488174024917, 0.3625980313465469, 0.3106837190382783, 0.4526778219397636, 0.30828283436869064, 0.3514379051999498, 0.2967534629547839, 0.1954476643913936, 0.36545883372392196, 0.9918139858661795, 0.2839422550924685, 0.3125895128260475, 0.7201609847095862, 0.4417942174080871, 0.3179728848455473, 0.49318327884588387, 0.19682062953366286, 0.49508297695573866]}}
{"id": "8f45b114-88f7-44e4-af74-40d6771e8b13", "fitness": 0.6798379288279521, "name": "AdaptiveDEPool", "description": "Adaptive Differential Evolution with a modified mutation strategy using a pool of past best solutions and adaptive CR/F based on individual success.", "code": "import numpy as np\n\nclass AdaptiveDEPool:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, CR_init=0.9, pool_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.CR = CR_init\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.pool_size = pool_size\n        self.best_pool_x = []\n        self.best_pool_f = []\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.pop[best_idx]\n\n        self.best_pool_x.append(self.x_opt)\n        self.best_pool_f.append(self.f_opt)\n        if len(self.best_pool_x) > self.pool_size:\n            self.best_pool_x.pop(0)\n            self.best_pool_f.pop(0)\n\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b = self.pop[np.random.choice(idxs, 2, replace=False)]\n\n                # Modified mutation strategy using a pool of past best solutions\n                if self.best_pool_x:\n                    best_x = self.best_pool_x[np.argmin(self.best_pool_f)]\n                    mutant = self.pop[i] + self.F * (best_x - self.pop[i]) + self.F * (a - b)\n                else:\n                    mutant = self.pop[i] + self.F * (self.x_opt - self.pop[i]) + self.F * (a - b)\n\n                # Crossover\n                CR_adaptive = np.random.normal(self.CR, 0.1)  #Adaptive CR\n                CR_adaptive = np.clip(CR_adaptive, 0, 1)\n                cross_points = np.random.rand(self.dim) < CR_adaptive\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, self.pop[i])\n                trial = np.clip(trial, self.bounds_lb, self.bounds_ub)\n                \n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    # Adaptive parameter adaptation based on individual success\n                    success_rate = np.mean(f_trial < self.fitness[:i]) if i > 0 else 0.5  # approximate success rate\n                    self.F = 0.9 * self.F + 0.1 * np.random.rand()\n                    self.CR = 0.9 * self.CR + 0.1 * success_rate  # adapt CR based on individual success rate\n\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.best_pool_x.append(self.x_opt)\n                        self.best_pool_f.append(self.f_opt)\n                        if len(self.best_pool_x) > self.pool_size:\n                            self.best_pool_x.pop(0)\n                            self.best_pool_f.pop(0)\n            best_idx = np.argmin(self.fitness)\n            if self.fitness[best_idx] < self.f_opt:\n                self.f_opt = self.fitness[best_idx]\n                self.x_opt = self.pop[best_idx]\n                self.best_pool_x.append(self.x_opt)\n                self.best_pool_f.append(self.f_opt)\n                if len(self.best_pool_x) > self.pool_size:\n                    self.best_pool_x.pop(0)\n                    self.best_pool_f.pop(0)\n\n                         \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDEPool scored 0.680 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e2249b45-a8a0-4eec-97e2-daed671afb02"], "operator": null, "metadata": {"aucs": [0.21355616862750337, 0.40450868345819213, 0.8068207721459956, 0.889892260113396, 0.7893618027620337, 0.8512946271194668, 0.5974447891106476, 0.7061872942769886, 0.8169963643101168, 0.43762514227893545, 0.8861609415932512, 0.9953956977243463, 0.41821702836762786, 0.7514521395167129, 0.9139724369599364, 0.8380393744774871, 0.6584090727107027, 0.8801822920137108, 0.21159611491713115, 0.5296455740748608]}}
{"id": "35ee5e92-dea0-4c2c-971d-eda1ff0f89f8", "fitness": 0.6609692292847894, "name": "AdaptiveDE", "description": "Simplified Adaptive Differential Evolution with self-adaptive parameters, best-guided mutation, and clipping.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, CR_init=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.CR = CR_init\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.pop = None  # Initialize population as None\n        self.fitness = None # Initialize fitness as None\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n\n    def __call__(self, func):\n        # Initialize population and fitness in the call method\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.pop[best_idx]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation: best-guided\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b = self.pop[np.random.choice(idxs, 2, replace=False)]\n                mutant = self.pop[i] + self.F * (self.x_opt - self.pop[i]) + self.F * (a - b)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Clip and Evaluate\n                trial = np.clip(trial, self.bounds_lb, self.bounds_ub)\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                # Selection and Parameter Adaptation\n                if f_trial < self.fitness[i]:\n                    self.F = 0.9 * self.F + 0.1 * np.random.rand()  # Update F\n                    self.CR = 0.9 * self.CR + 0.1 * np.random.rand()  # Update CR\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDE scored 0.661 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e2249b45-a8a0-4eec-97e2-daed671afb02"], "operator": null, "metadata": {"aucs": [0.2142645260887166, 0.4247709385769822, 0.770792684450048, 0.8950162009948409, 0.738882982735466, 0.8406947023685346, 0.5042686170452686, 0.721005086642972, 0.806973451506735, 0.47385516582572274, 0.8586503047962486, 0.9983412287346926, 0.2929023921748034, 0.6641314165036605, 0.9071117035260067, 0.848707947605095, 0.6283084562919208, 0.8821971148518841, 0.22616863642508578, 0.5223410285511052]}}
{"id": "369190cc-1faa-4e95-a037-511145c26870", "fitness": 0.58420265045553, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with velocity-based mutation, self-adaptive CR and F, and a repair mechanism to keep solutions within bounds.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, CR_init=0.9, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.CR = CR_init\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.archive_size = archive_size\n        self.archive = []\n\n        self.F_history = []\n        self.CR_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.pop[best_idx]\n\n        self.velocity = np.zeros_like(self.pop)  # Initialize velocity\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b = self.pop[np.random.choice(idxs, 2, replace=False)]\n\n                # Velocity update\n                self.velocity[i] = 0.5 * self.velocity[i] + self.F * (a - b)\n\n                # Mutation strategy incorporating the best solution and velocity\n                mutant = self.pop[i] + self.velocity[i] + self.F * (self.x_opt - self.pop[i])\n\n                # Crossover with self-adaptive CR\n                if np.random.rand() < 0.1:\n                   CR_adaptive = np.random.normal(self.CR, 0.2) #Explore\n                else:\n                   CR_adaptive = np.random.normal(self.CR, 0.05) #Exploit\n\n\n                CR_adaptive = np.clip(CR_adaptive, 0, 1)\n                cross_points = np.random.rand(self.dim) < CR_adaptive\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, self.pop[i])\n                trial = np.clip(trial, self.bounds_lb, self.bounds_ub)\n                \n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    # Self-adaptive parameter adaptation based on fitness improvement\n                    self.F = 0.8 * self.F + 0.2 * np.random.rand()\n                    self.CR = 0.8 * self.CR + 0.2 * np.random.rand()\n\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                    else:\n                        #Replace a random archive member\n                        self.archive[np.random.randint(0, self.archive_size)] = trial\n\n            best_idx = np.argmin(self.fitness)\n            if self.fitness[best_idx] < self.f_opt:\n                self.f_opt = self.fitness[best_idx]\n                self.x_opt = self.pop[best_idx]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDE scored 0.584 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e2249b45-a8a0-4eec-97e2-daed671afb02"], "operator": null, "metadata": {"aucs": [0.19110024621118404, 0.2942828937535221, 0.6799964200861202, 0.8293362236276396, 0.6498075280142757, 0.7671917939991053, 0.3605881883865518, 0.5846232540508368, 0.7059530655482256, 0.31398988286293783, 0.841120214593559, 0.9968534616221468, 0.2897046148647583, 0.47229752440436235, 0.8939461227434423, 0.7696454726615087, 0.4779643233865464, 0.8261482925700044, 0.20751897100296446, 0.53198451472091]}}
{"id": "bcbbe3f2-23cf-4778-abaf-82e18979bc31", "fitness": -Infinity, "name": "AdaptiveDEArchive", "description": "Adaptive Differential Evolution with a combined mutation strategy using both current-to-best and random differential evolution, an archive of past solutions, and self-adaptive parameters.", "code": "import numpy as np\n\nclass AdaptiveDEArchive:\n    def __init__(self, budget=10000, dim=10, pop_size=40, archive_size=10, F_init=0.5, CR_init=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F_init\n        self.CR = CR_init\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.archive = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n\n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.pop[best_idx]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation strategy: Combination of current-to-best and random DE\n                if np.random.rand() < 0.5:  # Choose mutation strategy randomly\n                    # Current-to-best\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    a, b = self.pop[np.random.choice(idxs, 2, replace=False)]\n                    mutant = self.pop[i] + self.F * (self.x_opt - self.pop[i]) + self.F * (a - b)\n                else:\n                    # Random DE with archive\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    a = self.pop[np.random.choice(idxs, 1, replace=False)][0]\n\n                    if len(self.archive) > 0:\n                      b = self.archive[np.random.choice(len(self.archive), 1, replace=False)][0]\n                    else:\n                      b = np.random.uniform(self.bounds_lb, self.bounds_ub, size=self.dim)\n\n                    c = np.random.uniform(self.bounds_lb, self.bounds_ub, size=self.dim)\n                    mutant = a + self.F * (b - c)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, self.pop[i])\n                trial = np.clip(trial, self.bounds_lb, self.bounds_ub)\n\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    # Update individual and self-adaptive parameters\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n\n                    # Update archive if trial vector is better than worst archive member\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                    else:\n                        fitness_archive = [func(x) for x in self.archive]\n                        worst_idx = np.argmax(fitness_archive)\n                        if f_trial < fitness_archive[worst_idx]:\n                            self.archive[worst_idx] = trial\n\n                    #Update CR and F\n                    self.F = 0.8 * self.F + 0.2 * np.random.rand()\n                    self.CR = 0.8 * self.CR + 0.2 * np.random.rand()\n                    \n\n                # Update global best\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "An exception occurred: only integer scalar arrays can be converted to a scalar index.", "error": "", "parent_ids": ["369190cc-1faa-4e95-a037-511145c26870"], "operator": null, "metadata": {}}
{"id": "9b1c8988-eb25-47fa-8a97-70a9533e8cdd", "fitness": 0.0, "name": "AdaptiveDEPoolOrtho", "description": "Adaptive Differential Evolution with a pool of past best solutions, adaptive CR/F based on population-wide success, and orthogonal learning to enhance exploration.", "code": "import numpy as np\n\nclass AdaptiveDEPoolOrtho:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, CR_init=0.9, pool_size=10, ortho_group_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.CR = CR_init\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.pool_size = pool_size\n        self.best_pool_x = []\n        self.best_pool_f = []\n        self.ortho_group_size = ortho_group_size\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.pop[best_idx]\n\n        self.best_pool_x.append(self.x_opt)\n        self.best_pool_f.append(self.f_opt)\n        if len(self.best_pool_x) > self.pool_size:\n            self.best_pool_x.pop(0)\n            self.best_pool_f.pop(0)\n\n        while self.budget > 0:\n            # Orthogonal Learning Phase\n            for group_start in range(0, self.pop_size, self.ortho_group_size):\n                group_end = min(group_start + self.ortho_group_size, self.pop_size)\n                group_indices = list(range(group_start, group_end))\n                if len(group_indices) > 1:\n                    # Create orthogonal array (simplified, replace with a proper OA if needed for higher dimensions)\n                    oa = np.array([[1, -1], [-1, 1]])  # Example L4 array for 2 parameters\n                    for i, idx in enumerate(group_indices):\n                        for j in range(self.dim):  # Sample around each dimension\n                            original_value = self.pop[idx, j]\n                            delta = 0.1 * (self.bounds_ub - self.bounds_lb) # Scale delta to problem bounds\n                            \n                            trial1 = np.clip(original_value + delta * oa[0, 0], self.bounds_lb, self.bounds_ub)\n                            trial2 = np.clip(original_value + delta * oa[0, 1], self.bounds_lb, self.bounds_ub)\n\n                            f_trial1 = func(self.pop[idx].copy())\n                            f_trial2 = func(self.pop[idx].copy()) # Avoid modifying original individual.\n                            self.pop[idx, j] = trial1\n                            f_trial1 = func(self.pop[idx])\n                            self.budget -= 1\n                            self.pop[idx, j] = trial2\n                            f_trial2 = func(self.pop[idx])\n                            self.budget -= 1\n                            self.pop[idx, j] = original_value # Restore original, modify in selection\n\n                            if f_trial1 < self.fitness[idx] and f_trial1 < f_trial2:\n                                self.fitness[idx] = f_trial1\n                                if f_trial1 < self.f_opt:\n                                    self.f_opt = f_trial1\n                                    self.x_opt = self.pop[idx].copy() #store a copy\n\n                            elif f_trial2 < self.fitness[idx] and f_trial2 < f_trial1:\n                                self.fitness[idx] = f_trial2\n                                if f_trial2 < self.f_opt:\n                                    self.f_opt = f_trial2\n                                    self.x_opt = self.pop[idx].copy() #store a copy\n\n                            else: #restore the original values in case the trials were bad\n                                self.pop[idx, j] = original_value\n\n            # DE Mutation and Crossover Phase\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b = self.pop[np.random.choice(idxs, 2, replace=False)]\n\n                # Modified mutation strategy using a pool of past best solutions\n                if self.best_pool_x:\n                    best_x = self.best_pool_x[np.argmin(self.best_pool_f)]\n                    mutant = self.pop[i] + self.F * (best_x - self.pop[i]) + self.F * (a - b)\n                else:\n                    mutant = self.pop[i] + self.F * (self.x_opt - self.pop[i]) + self.F * (a - b)\n\n                # Crossover\n                CR_adaptive = np.random.normal(self.CR, 0.1)  #Adaptive CR\n                CR_adaptive = np.clip(CR_adaptive, 0, 1)\n                cross_points = np.random.rand(self.dim) < CR_adaptive\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, self.pop[i])\n                trial = np.clip(trial, self.bounds_lb, self.bounds_ub)\n                \n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    # Adaptive parameter adaptation based on population-wide success\n                    success_rate = np.mean(f_trial < self.fitness) # Population wide success\n                    self.F = 0.9 * self.F + 0.1 * np.random.rand()\n                    self.CR = 0.9 * self.CR + 0.1 * success_rate  # adapt CR based on population success rate\n\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.best_pool_x.append(self.x_opt)\n                        self.best_pool_f.append(self.f_opt)\n                        if len(self.best_pool_x) > self.pool_size:\n                            self.best_pool_x.pop(0)\n                            self.best_pool_f.pop(0)\n            best_idx = np.argmin(self.fitness)\n            if self.fitness[best_idx] < self.f_opt:\n                self.f_opt = self.fitness[best_idx]\n                self.x_opt = self.pop[best_idx]\n                self.best_pool_x.append(self.x_opt)\n                self.best_pool_f.append(self.f_opt)\n                if len(self.best_pool_x) > self.pool_size:\n                    self.best_pool_x.pop(0)\n                    self.best_pool_f.pop(0)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDEPoolOrtho scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["8f45b114-88f7-44e4-af74-40d6771e8b13"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "bb7c35fd-e3bc-4392-90ca-dfd63704d46e", "fitness": 0.3675744880389906, "name": "AdaptiveDEOL", "description": "Adaptive Differential Evolution with orthogonal learning and a diversity-promoting archive.", "code": "import numpy as np\n\nclass AdaptiveDEOL:\n    def __init__(self, budget=10000, dim=10, pop_size=40, archive_size=10, orthogonal_samples=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.archive = []\n        self.orthogonal_samples = orthogonal_samples\n        self.CR = 0.5\n        self.F = 0.7\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n\n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.pop[best_idx]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = a + self.F * (b - c)\n                mutant = np.clip(mutant, self.bounds_lb, self.bounds_ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Orthogonal Learning\n                orthogonal_matrix = np.random.randn(self.orthogonal_samples, self.dim)\n                orthogonal_matrix /= np.linalg.norm(orthogonal_matrix, axis=1, keepdims=True)\n                \n                best_orthogonal_trial = trial\n                best_orthogonal_fitness = np.inf\n\n                for j in range(self.orthogonal_samples):\n                    orthogonal_direction = orthogonal_matrix[j]\n                    alpha = np.random.uniform(-1, 1)  # Random step size\n                    orthogonal_trial = trial + alpha * orthogonal_direction\n                    orthogonal_trial = np.clip(orthogonal_trial, self.bounds_lb, self.bounds_ub)\n\n                    f_orthogonal_trial = func(orthogonal_trial)\n                    self.budget -= 1\n                    \n                    if f_orthogonal_trial < best_orthogonal_fitness:\n                        best_orthogonal_fitness = f_orthogonal_trial\n                        best_orthogonal_trial = orthogonal_trial\n                    \n                    if self.budget <= 0:\n                        break\n                        \n\n                if best_orthogonal_fitness < self.fitness[i]:\n                        self.fitness[i] = best_orthogonal_fitness\n                        self.pop[i] = best_orthogonal_trial\n                        \n                        if best_orthogonal_fitness < self.f_opt:\n                            self.f_opt = best_orthogonal_fitness\n                            self.x_opt = best_orthogonal_trial\n\n                             #Update archive with orthogonal trial\n                            if len(self.archive) < self.archive_size:\n                                self.archive.append(best_orthogonal_trial)\n                            else:\n                                #Replace a random archive member\n                                self.archive[np.random.randint(0, self.archive_size)] = best_orthogonal_trial\n\n                # Adaptive CR and F\n                if np.random.rand() < 0.1:\n                    self.CR = np.random.rand()\n                    self.F = np.random.rand()\n                else:\n                    self.CR = 0.5 * self.CR + 0.5 * np.random.rand()\n                    self.F = 0.5 * self.F + 0.5 * np.random.rand()\n\n                if self.budget <= 0:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDEOL scored 0.368 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["369190cc-1faa-4e95-a037-511145c26870"], "operator": null, "metadata": {"aucs": [0.15999206308207126, 0.23645013321797392, 0.3647217255199723, 0.38030712854960247, 0.2938433141874054, 0.3710262586116281, 0.28386557555680914, 0.33789255456440637, 0.29098966262749804, 0.22766323210860462, 0.3267143038286, 0.9857808089330161, 0.25656294780496014, 0.29915204529377826, 0.7250296082341805, 0.3881198236557375, 0.2948511444607198, 0.45436057157781373, 0.1885854758681086, 0.48558138309692644]}}
{"id": "7d1804bc-b3d8-4ec7-adfa-7f2adf5d4812", "fitness": 0.7746022046995807, "name": "AdaptiveDE", "description": "Simplified Adaptive Differential Evolution with population-level CR/F adaptation based on success rate and best-guided mutation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, CR_init=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.CR = CR_init\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.success_history_F = []\n        self.success_history_CR = []\n\n\n    def __call__(self, func):\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.pop[best_idx]\n\n        while self.budget > 0:\n            successful_mutations = 0\n            successful_F = []\n            successful_CR = []\n\n            for i in range(self.pop_size):\n                # Mutation: best-guided\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b = self.pop[np.random.choice(idxs, 2, replace=False)]\n                mutant = self.pop[i] + self.F * (self.x_opt - self.pop[i]) + self.F * (a - b)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Clip and Evaluate\n                trial = np.clip(trial, self.bounds_lb, self.bounds_ub)\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                # Selection\n                if f_trial < self.fitness[i]:\n                    successful_mutations += 1\n                    successful_F.append(self.F)\n                    successful_CR.append(self.CR)\n\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Adapt F and CR based on success\n            if successful_mutations > 0:\n                self.F = np.mean(successful_F) if successful_F else 0.5\n                self.CR = np.mean(successful_CR) if successful_CR else 0.9\n            else:\n                 self.F = 0.5\n                 self.CR = 0.9  # Reset to exploration values\n                \n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDE scored 0.775 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["35ee5e92-dea0-4c2c-971d-eda1ff0f89f8"], "operator": null, "metadata": {"aucs": [0.6089638781597989, 0.8742933208465948, 0.8898812079265606, 0.9208109170899745, 0.8957217618419969, 0.9198161479308069, 0.3668577499145773, 0.8703876818550996, 0.8940847269373728, 0.1964527841368151, 0.9415023857739603, 0.9964867878255818, 0.8432375758264639, 0.8997829608842347, 0.7195495758419866, 0.9108584158670162, 0.7960549999912552, 0.9401291243175909, 0.2933600998327077, 0.7138119911912203]}}
{"id": "c1c7ad3c-9112-4849-9fc2-d52a71e8f397", "fitness": 0.6621784525455139, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with simplified self-adaptation of F and CR and a combined mutation strategy favoring exploitation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, CR_init=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.CR = CR_init\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.pop[best_idx]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b = self.pop[np.random.choice(idxs, 2, replace=False)]\n\n                # Combined mutation strategy: balance exploration and exploitation\n                mutant = self.pop[i] + self.F * (self.x_opt - self.pop[i]) + self.F * (a - b)\n                \n                # Boundary handling (clip instead of mirror)\n                mutant = np.clip(mutant, self.bounds_lb, self.bounds_ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, self.pop[i])\n                \n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    # Simplified parameter adaptation\n                    self.F = 0.9 * self.F + 0.1 * np.random.rand()  # Closer to previous value\n                    self.CR = 0.9 * self.CR + 0.1 * np.random.rand()  # Closer to previous value\n\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                         \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDE scored 0.662 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e06dcf6e-042f-4536-8680-8f77907f268a"], "operator": null, "metadata": {"aucs": [0.21487939283035062, 0.5482521789826569, 0.7590301993094588, 0.9069652929323677, 0.7608422778063204, 0.8536902160523322, 0.5006817907431642, 0.7140933885424958, 0.8019032276946246, 0.2754894026798602, 0.8668091131889823, 0.989561172804399, 0.33736170046512015, 0.6352139690676335, 0.9298957412744212, 0.843600268430261, 0.6221778945228214, 0.8949902986159076, 0.2541308291793355, 0.5340006957877652]}}
{"id": "905390d8-8714-4fb3-993d-f860591fb98f", "fitness": 0.6515310633950679, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with simplified parameter adaptation, best-guided mutation, and probability-based repair.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, CR_init=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.CR = CR_init\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.pop[best_idx]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation: best-guided\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b = self.pop[np.random.choice(idxs, 2, replace=False)]\n                mutant = self.pop[i] + self.F * (self.x_opt - self.pop[i]) + self.F * (a - b)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Probability-based Repair & Evaluate\n                repair_mask = (trial < self.bounds_lb) | (trial > self.bounds_ub)\n                trial[repair_mask] = np.random.uniform(self.bounds_lb, self.bounds_ub, size=np.sum(repair_mask))\n\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                # Selection and Parameter Adaptation\n                if f_trial < self.fitness[i]:\n                    self.F = 0.8 * self.F + 0.2 * np.random.rand()  # Simplified F update\n                    self.CR = 0.8 * self.CR + 0.2 * np.random.rand()  # Simplified CR update\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDE scored 0.652 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["35ee5e92-dea0-4c2c-971d-eda1ff0f89f8"], "operator": null, "metadata": {"aucs": [0.1359820952310038, 0.3868365304841418, 0.7837602328996287, 0.8831054282695666, 0.7840467407394902, 0.8401965562346773, 0.529244788408856, 0.709151677272688, 0.8149954961268971, 0.2271190332654316, 0.8550153107901348, 0.9981161595801185, 0.31684627332701243, 0.7380898266738066, 0.9129238088663874, 0.8350270579423014, 0.6738121763698777, 0.8741992946839617, 0.22262909585955937, 0.5095236848758169]}}
{"id": "8f7a6181-2a0d-4d62-8942-edc6baffcdca", "fitness": 0.6088933698808251, "name": "AdaptiveDE", "description": "Simplified Adaptive Differential Evolution with archive-based mutation, mirrored boundary handling, and adaptive CR, emphasizing exploitation by focusing mutation around successful individuals and using a decaying CR for convergence.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.archive = []\n        self.CR = 0.7  # Initial CR value\n        self.F = 0.5\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.pop[best_idx]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b = self.pop[np.random.choice(idxs, 2, replace=False)]\n                \n                # Archive-based mutation\n                if len(self.archive) > 0 and np.random.rand() < 0.5:\n                    arc_idx = np.random.randint(len(self.archive))\n                    mutant = self.pop[i] + self.F * (self.archive[arc_idx] - self.pop[i]) + self.F * (a - b)\n                else:\n                     mutant = self.pop[i] + self.F * (self.x_opt - self.pop[i]) + self.F * (a - b)\n\n                # Mirrored boundary handling\n                mutant = np.where(mutant < self.bounds_lb, 2 * self.bounds_lb - mutant, mutant)\n                mutant = np.where(mutant > self.bounds_ub, 2 * self.bounds_ub - mutant, mutant)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, self.pop[i])\n                \n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.pop[i].copy())\n                    else:\n                        #Replace a random element in the archive\n                        self.archive[np.random.randint(self.archive_size)] = self.pop[i].copy()\n\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n            \n            #Decay CR\n            self.CR *= 0.99\n                         \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDE scored 0.609 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e06dcf6e-042f-4536-8680-8f77907f268a"], "operator": null, "metadata": {"aucs": [0.19279921586801696, 0.34581587075649833, 0.6536927165225455, 0.9043129637843135, 0.6527651615376961, 0.835199156984309, 0.4567100469665515, 0.5396350452542888, 0.734792901612243, 0.28095225137856583, 0.8800021581998716, 0.9992753576655036, 0.25781576443468723, 0.6330233754658245, 0.7893470240829786, 0.842162393738054, 0.5126328360579088, 0.8877907798749115, 0.22076689000352012, 0.5583754874282131]}}
{"id": "6feb8ab7-a711-436a-866b-9fa05ef97493", "fitness": 0.6426891974875135, "name": "AdaptiveDEArchive", "description": "Adaptive Differential Evolution with a dynamically updated archive of successful solutions and simplified parameter adaptation.", "code": "import numpy as np\n\nclass AdaptiveDEArchive:\n    def __init__(self, budget=10000, dim=10, pop_size=40, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.archive_x = []\n        self.archive_f = []\n        self.F = 0.5\n        self.CR = 0.9\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.pop[best_idx]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b = self.pop[np.random.choice(idxs, 2, replace=False)]\n                \n                # Mutation with archive\n                if self.archive_x and np.random.rand() < 0.1:  # Use archive occasionally\n                    arc_idx = np.random.randint(len(self.archive_x))\n                    mutant = self.pop[i] + self.F * (self.archive_x[arc_idx] - self.pop[i]) + self.F * (a - b)\n                else:\n                    mutant = self.pop[i] + self.F * (self.x_opt - self.pop[i]) + self.F * (a - b)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n                trial = np.clip(trial, self.bounds_lb, self.bounds_ub)\n                \n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    # Update archive\n                    if len(self.archive_x) < self.archive_size:\n                        self.archive_x.append(self.pop[i].copy())\n                        self.archive_f.append(self.fitness[i])\n                    else:\n                        worst_arc_idx = np.argmax(self.archive_f)\n                        if self.fitness[i] < self.archive_f[worst_arc_idx]:\n                            self.archive_x[worst_arc_idx] = self.pop[i].copy()\n                            self.archive_f[worst_arc_idx] = self.fitness[i]\n                            \n                    # Adaptation (simplified)\n                    self.F = 0.5 + 0.4 * np.random.rand()\n                    self.CR = 0.9 * np.random.rand()\n                        \n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n            \n            best_idx = np.argmin(self.fitness)\n            if self.fitness[best_idx] < self.f_opt:\n                self.f_opt = self.fitness[best_idx]\n                self.x_opt = self.pop[best_idx]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDEArchive scored 0.643 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["8f45b114-88f7-44e4-af74-40d6771e8b13"], "operator": null, "metadata": {"aucs": [0.22358683905439736, 0.4290992662692036, 0.6794889813385782, 0.860174503596825, 0.7406940064812261, 0.8105762407461609, 0.46474455553205285, 0.6344476346455512, 0.7447103952058156, 0.5845077313537697, 0.821343300719964, 0.9970782258490564, 0.335023314154166, 0.6675691841872793, 0.9110311777470046, 0.8186032869365237, 0.5705484997683749, 0.8471812833930358, 0.20831468158556277, 0.505060841185722]}}
{"id": "c48bf26c-f281-4982-be80-90f19c0b733d", "fitness": 0.5526034501128672, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with a success-history based adaptation of CR and F, and a worst-member replacement strategy to maintain population diversity.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, archive_size=10, F_values=[0.1, 0.5, 0.9], CR_values=[0.1, 0.5, 0.9]):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.archive = []\n\n        # Success history for F and CR\n        self.F_values = F_values  # Candidate values for F\n        self.CR_values = CR_values  # Candidate values for CR\n        self.memory_size = 10\n        self.F_memory = np.full(self.memory_size, 0.5)\n        self.CR_memory = np.full(self.memory_size, 0.5)\n        self.memory_idx = 0\n\n        self.success_F = []\n        self.success_CR = []\n        self.success_fitness_diff = []\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.pop[best_idx]\n\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Choose F and CR from memory with probability proportional to success\n                if self.success_fitness_diff:\n                    probs = np.array(self.success_fitness_diff) / np.sum(self.success_fitness_diff)\n                    F = np.random.choice(self.success_F, p=probs)\n                    CR = np.random.choice(self.success_CR, p=probs)\n                else:\n                    F = np.random.choice(self.F_values)\n                    CR = np.random.choice(self.CR_values)\n\n\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n\n                mutant = self.pop[i] + F * (a - b)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n                trial = np.clip(trial, self.bounds_lb, self.bounds_ub)\n                \n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    # Store successful F and CR values\n                    self.success_F.append(F)\n                    self.success_CR.append(CR)\n                    self.success_fitness_diff.append(self.fitness[i] - f_trial)\n\n                    if len(self.success_F) > self.memory_size:\n                        self.success_F.pop(0)\n                        self.success_CR.pop(0)\n                        self.success_fitness_diff.pop(0)\n\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                    else:\n                        #Replace a random archive member\n                        self.archive[np.random.randint(0, self.archive_size)] = trial\n                else:\n                     # Replace worst member with trial to maintain diversity\n                    worst_idx = np.argmax(self.fitness)\n                    if f_trial < self.fitness[worst_idx]:\n                        self.pop[worst_idx] = trial\n                        self.fitness[worst_idx] = f_trial\n                        if f_trial < self.f_opt:\n                            self.f_opt = f_trial\n                            self.x_opt = trial\n\n\n            best_idx = np.argmin(self.fitness)\n            if self.fitness[best_idx] < self.f_opt:\n                self.f_opt = self.fitness[best_idx]\n                self.x_opt = self.pop[best_idx]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDE scored 0.553 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["369190cc-1faa-4e95-a037-511145c26870"], "operator": null, "metadata": {"aucs": [0.4028040021572953, 0.7184483741666612, 0.27816489934520083, 0.8793558343570356, 0.7614911723491287, 0.7616602389043162, 0.2532190696537203, 0.16831737729824015, 0.7989664302324034, 0.6718230474434579, 0.745481241447698, 0.9993997682649268, 0.21853892465144853, 0.7698291362015519, 0.8009460786461694, 0.8264511282800066, 0.18847453877778741, 0.21507586581524651, 0.1666651902362244, 0.42695668402882314]}}
{"id": "36f95ffb-e478-4f13-a066-da2e761cc211", "fitness": 0.5937634541455743, "name": "AdaptiveDEPool", "description": "Adaptive Differential Evolution with a mutation strategy incorporating the mean of the population to guide convergence and adaptive CR/F based on population diversity.", "code": "import numpy as np\n\nclass AdaptiveDEPool:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, CR_init=0.9, pool_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.CR = CR_init\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.pool_size = pool_size\n        self.best_pool_x = []\n        self.best_pool_f = []\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.pop[best_idx]\n\n        self.best_pool_x.append(self.x_opt)\n        self.best_pool_f.append(self.f_opt)\n        if len(self.best_pool_x) > self.pool_size:\n            self.best_pool_x.pop(0)\n            self.best_pool_f.pop(0)\n\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b = self.pop[np.random.choice(idxs, 2, replace=False)]\n\n                # Modified mutation strategy using a pool of past best solutions and population mean\n                if self.best_pool_x:\n                    best_x = self.best_pool_x[np.argmin(self.best_pool_f)]\n                    mean_x = np.mean(self.pop, axis=0)\n                    mutant = self.pop[i] + self.F * (best_x - self.pop[i]) + self.F * (a - b) + 0.1 * (mean_x - self.pop[i]) # add population mean\n                else:\n                    mutant = self.pop[i] + self.F * (self.x_opt - self.pop[i]) + self.F * (a - b)\n\n                # Crossover\n                CR_adaptive = np.random.normal(self.CR, 0.1)  #Adaptive CR\n                CR_adaptive = np.clip(CR_adaptive, 0, 1)\n                cross_points = np.random.rand(self.dim) < CR_adaptive\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, self.pop[i])\n                trial = np.clip(trial, self.bounds_lb, self.bounds_ub)\n                \n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    # Adaptive parameter adaptation based on population diversity\n                    diversity = np.std(self.fitness)\n                    self.F = 0.9 * self.F + 0.1 * np.random.rand()\n                    self.CR = 0.9 * self.CR + 0.1 * (1 - diversity)  # adapt CR based on population diversity\n\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.best_pool_x.append(self.x_opt)\n                        self.best_pool_f.append(self.f_opt)\n                        if len(self.best_pool_x) > self.pool_size:\n                            self.best_pool_x.pop(0)\n                            self.best_pool_f.pop(0)\n            best_idx = np.argmin(self.fitness)\n            if self.fitness[best_idx] < self.f_opt:\n                self.f_opt = self.fitness[best_idx]\n                self.x_opt = self.pop[best_idx]\n                self.best_pool_x.append(self.x_opt)\n                self.best_pool_f.append(self.f_opt)\n                if len(self.best_pool_x) > self.pool_size:\n                    self.best_pool_x.pop(0)\n                    self.best_pool_f.pop(0)\n\n                         \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDEPool scored 0.594 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["8f45b114-88f7-44e4-af74-40d6771e8b13"], "operator": null, "metadata": {"aucs": [0.1724916176225152, 0.25950753540492555, 0.8357850962550035, 0.7649857634458848, 0.8457226874342846, 0.8609548946209286, 0.33550403818406815, 0.8441861808949334, 0.6991051580379122, 0.2479986619893284, 0.6145219740973187, 0.9956347647306438, 0.42569472007101306, 0.6984781682382104, 0.9173605182682449, 0.3401150550454658, 0.4993078091655513, 0.8071098102404023, 0.20443487302869257, 0.5063697561361606]}}
{"id": "f6e45f02-14cf-4e7d-b46c-0cdfcfe1e296", "fitness": -Infinity, "name": "AdaptiveDEDynamicPop", "description": "Adaptive Differential Evolution with a dynamic population size and a tournament selection-based mutation strategy.", "code": "import numpy as np\n\nclass AdaptiveDEDynamicPop:\n    def __init__(self, budget=10000, dim=10, pop_size_init=40, F_init=0.5, CR_init=0.9, pop_size_min=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_init = pop_size_init\n        self.pop_size_min = pop_size_min\n        self.F = F_init\n        self.CR = CR_init\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n    def initialize_population(self):\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.pop[best_idx]\n\n    def adjust_population_size(self):\n        if self.budget < self.pop_size:\n            self.pop_size = max(self.pop_size_min, self.budget // 2)  # Dynamic reduction\n            if self.pop is not None: # Population exists\n                indices = np.argsort(self.fitness)[:self.pop_size]\n                self.pop = self.pop[indices]\n                self.fitness = self.fitness[indices]\n            \n\n    def __call__(self, func):\n        self.initialize_population()\n\n        while self.budget > 0:\n            self.adjust_population_size()\n\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Tournament selection for mutation\n                idxs = np.random.choice(self.pop_size, 5, replace=False)  # Tournament size 5\n                best_idx = idxs[np.argmin(self.fitness[idxs])]\n                a = self.pop[best_idx]\n\n                idxs = np.random.choice(self.pop_size, 3, replace=False)  # Tournament size 3\n                b_idx = idxs[np.argmin(self.fitness[idxs])]\n                b = self.pop[b_idx]\n\n                idxs = np.random.choice(self.pop_size, 3, replace=False)  # Tournament size 3\n                c_idx = idxs[np.argmin(self.fitness[idxs])]\n                c = self.pop[c_idx]\n                \n                # Mutation strategy: current-to-best with tournament selected individuals\n                mutant = self.pop[i] + self.F * (a - self.pop[i]) + self.F * (b - c)\n\n                # Mirrored boundary handling\n                mutant = np.where(mutant < self.bounds_lb, 2 * self.bounds_lb - mutant, mutant)\n                mutant = np.where(mutant > self.bounds_ub, 2 * self.bounds_ub - mutant, mutant)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    # Adaptation of parameters\n                    self.F = 0.8 * self.F + 0.2 * np.random.rand()\n                    self.CR = 0.8 * self.CR + 0.2 * np.random.rand()\n                    \n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "An exception occurred: name 'func' is not defined.", "error": "", "parent_ids": ["e06dcf6e-042f-4536-8680-8f77907f268a"], "operator": null, "metadata": {}}
{"id": "1df2ae28-bb8a-46b1-9e3c-f49a030d385e", "fitness": 0.0, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with a dynamically adjusted archive, a learning rate for parameter adaptation, and a more robust handling of the crossover probability.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, CR_init=0.9, archive_size=10, lr=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.CR = CR_init\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.archive_size = archive_size\n        self.archive = []\n        self.lr = lr  # Learning rate for parameter adaptation\n\n        self.F_history = []\n        self.CR_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.pop[best_idx]\n\n        self.velocity = np.zeros_like(self.pop)  # Initialize velocity\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b = self.pop[np.random.choice(idxs, 2, replace=False)]\n\n                # Velocity update\n                self.velocity[i] = 0.5 * self.velocity[i] + self.F * (a - b)\n\n                # Mutation strategy incorporating the best solution and velocity\n                mutant = self.pop[i] + self.velocity[i] + self.F * (self.x_opt - self.pop[i])\n\n                # Crossover with self-adaptive CR\n                if np.random.rand() < 0.1:\n                   CR_adaptive = np.random.normal(self.CR, 0.2) #Explore\n                else:\n                   CR_adaptive = np.random.normal(self.CR, 0.05) #Exploit\n\n\n                CR_adaptive = np.clip(CR_adaptive, 0, 1)\n                cross_points = np.random.rand(self.dim) < CR_adaptive\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, self.pop[i])\n                trial = np.clip(trial, self.bounds_lb, self.bounds_ub)\n                \n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    # Self-adaptive parameter adaptation based on fitness improvement\n                    self.F = (1 - self.lr) * self.F + self.lr * np.random.rand()\n                    self.CR = (1 - self.lr) * self.CR + self.lr * np.random.rand()\n\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                    else:\n                        # Replace the worst member of the archive\n                        archive_fitness = [func(x) for x in self.archive]\n                        worst_idx = np.argmax(archive_fitness)\n                        self.archive[worst_idx] = trial\n\n\n            best_idx = np.argmin(self.fitness)\n            if self.fitness[best_idx] < self.f_opt:\n                self.f_opt = self.fitness[best_idx]\n                self.x_opt = self.pop[best_idx]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["369190cc-1faa-4e95-a037-511145c26870"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "40114ce9-d1ed-4cfa-8031-78bb46713ac0", "fitness": -Infinity, "name": "OrthogonalDE", "description": "Differential Evolution with orthogonal learning to enhance the search directions and a local search step based on Nelder-Mead simplex.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass OrthogonalDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, orthogonal_components=5, local_search_iterations=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.orthogonal_components = orthogonal_components\n        self.local_search_iterations = local_search_iterations\n\n        self.F = 0.5\n        self.CR = 0.9\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.pop[best_idx]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n                \n                # Mutation\n                mutant = self.pop[i] + self.F * (a - b) + self.F * (self.x_opt - c) # include information of the best solution so far\n\n                # Orthogonal learning: Generate orthogonal directions\n                orthogonal_directions = []\n                for _ in range(self.orthogonal_components):\n                  direction = np.random.randn(self.dim)\n                  direction /= np.linalg.norm(direction)\n                  orthogonal_directions.append(direction)\n                \n                # Exploration along orthogonal directions (limited evaluations)\n                trial = mutant.copy()\n                best_trial_f = np.Inf\n                best_trial = None\n                \n                for direction in orthogonal_directions:\n                  step_size = 0.1 * np.random.rand()  # Adaptive step size\n                  temp_trial = mutant + step_size * direction\n                  temp_trial = np.clip(temp_trial, self.bounds_lb, self.bounds_ub)\n                  f_temp_trial = func(temp_trial)\n                  self.budget -=1\n\n                  if f_temp_trial < best_trial_f:\n                    best_trial_f = f_temp_trial\n                    best_trial = temp_trial.copy()\n\n                if best_trial is not None and best_trial_f < self.fitness[i]:\n                  trial = best_trial.copy()\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, trial, self.pop[i])\n                trial = np.clip(trial, self.bounds_lb, self.bounds_ub)\n                \n                # Local Search (Nelder-Mead)\n                local_search_bounds = [(self.bounds_lb, self.bounds_ub)] * self.dim\n\n                if self.budget > self.local_search_iterations:\n                  result = minimize(func, trial, method='Nelder-Mead', bounds=local_search_bounds, options={'maxiter': self.local_search_iterations}) # simplified bounds\n                  \n                  if result.fun < self.fitness[i]:\n                    f_trial = result.fun\n                    trial = result.x\n                    self.budget -= self.local_search_iterations\n                  else:\n                    f_trial = func(trial)\n                    self.budget -= 1\n                else:\n                  f_trial = func(trial)\n                  self.budget -= 1\n                    \n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n            \n            best_idx = np.argmin(self.fitness)\n            if self.fitness[best_idx] < self.f_opt:\n                self.f_opt = self.fitness[best_idx]\n                self.x_opt = self.pop[best_idx]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "An exception occurred: name 'minimize' is not defined.", "error": "", "parent_ids": ["6feb8ab7-a711-436a-866b-9fa05ef97493"], "operator": null, "metadata": {}}
{"id": "1da36b0d-fd05-46a3-96ce-44e1d5965d8d", "fitness": 0.0, "name": "AdaptiveDEPopSize", "description": "Differential Evolution with a self-adjusting population size and a memory of past successful F/CR values to guide current parameter selection.", "code": "import numpy as np\n\nclass AdaptiveDEPopSize:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.min_pop_size = 10\n        self.max_pop_size = 100\n        self.archive_size = archive_size\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.F_memory = [0.5] * archive_size  # Memory of successful F values\n        self.CR_memory = [0.9] * archive_size # Memory of successful CR values\n        self.success_count = 0\n        self.F = 0.5\n        self.CR = 0.9\n        self.archive_x = []\n        self.archive_f = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.pop[best_idx]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            \n            # Adjust population size based on success rate\n            success_rate = self.success_count / self.pop_size if self.pop_size > 0 else 0\n            if success_rate > 0.4 and self.pop_size < self.max_pop_size:\n                self.pop_size = min(self.pop_size + 5, self.max_pop_size)\n                new_individuals = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(5, self.dim))\n                new_fitness = np.array([func(x) for x in new_individuals])\n                self.budget -= 5\n                self.pop = np.concatenate((self.pop, new_individuals))\n                self.fitness = np.concatenate((self.fitness, new_fitness))\n            elif success_rate < 0.1 and self.pop_size > self.min_pop_size:\n                self.pop_size = max(self.pop_size - 5, self.min_pop_size)\n                idxs_to_remove = np.argsort(self.fitness)[-5:]  # Remove worst 5\n                self.pop = np.delete(self.pop, idxs_to_remove, axis=0)\n                self.fitness = np.delete(self.fitness, idxs_to_remove)\n                \n            self.success_count = 0  # Reset success count for this generation\n\n            for i in range(self.pop_size):\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b = self.pop[np.random.choice(idxs, 2, replace=False)]\n                \n                # Mutation with archive influence\n                if self.archive_x and np.random.rand() < 0.1:  # Use archive occasionally\n                    arc_idx = np.random.randint(len(self.archive_x))\n                    mutant = self.pop[i] + self.F * (self.archive_x[arc_idx] - self.pop[i]) + self.F * (a - b)\n                else:\n                    mutant = self.pop[i] + self.F * (self.x_opt - self.pop[i]) + self.F * (a - b)\n\n                # Crossover\n                # CR selection from memory\n                self.CR = self.CR_memory[np.random.randint(len(self.CR_memory))]\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n                trial = np.clip(trial, self.bounds_lb, self.bounds_ub)\n                \n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    self.success_count += 1\n                    # Update archive\n                    if len(self.archive_x) < self.archive_size:\n                        self.archive_x.append(self.pop[i].copy())\n                        self.archive_f.append(self.fitness[i])\n                    else:\n                        worst_arc_idx = np.argmax(self.archive_f)\n                        if self.fitness[i] < self.archive_f[worst_arc_idx]:\n                            self.archive_x[worst_arc_idx] = self.pop[i].copy()\n                            self.archive_f[worst_arc_idx] = self.fitness[i]\n                            \n                    # Adaptation (simplified) using memory and random\n                    self.F = 0.5 + 0.3 * np.random.rand() # Random F\n                    self.F_memory.append(self.F)\n                    self.F_memory.pop(0)\n\n                    self.CR_memory.append(self.CR)\n                    self.CR_memory.pop(0)\n                        \n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n            \n            best_idx = np.argmin(self.fitness)\n            if self.fitness[best_idx] < self.f_opt:\n                self.f_opt = self.fitness[best_idx]\n                self.x_opt = self.pop[best_idx]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveDEPopSize scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6feb8ab7-a711-436a-866b-9fa05ef97493"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "a681e420-af5d-4502-a227-8ef126f2ca33", "fitness": 0.48152371423523466, "name": "AdaptiveDEOpp", "description": "Differential Evolution with self-adaptive parameters, a combined mutation strategy including current-to-pbest and a repair mechanism using opposition-based learning.", "code": "import numpy as np\n\nclass AdaptiveDEOpp:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, CR_init=0.9, pbest_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.CR = CR_init\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.pbest_prob = pbest_prob\n\n    def __call__(self, func):\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.pop[best_idx]\n\n        while self.budget > 0:\n            # Adapt F and CR\n            self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n            self.CR = np.clip(np.random.normal(self.CR, 0.1), 0.1, 1.0)\n\n            for i in range(self.pop_size):\n                # Mutation Strategy: Combination of current-to-pbest and random\n                if np.random.rand() < self.pbest_prob:\n                    # Current-to-pbest mutation\n                    p_best_size = max(int(self.pop_size * 0.1), 1)\n                    idx_pbest = np.argpartition(self.fitness, p_best_size)[:p_best_size]\n                    pbest = self.pop[np.random.choice(idx_pbest)]\n\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    a, b = self.pop[np.random.choice(idxs, 2, replace=False)]\n                    mutant = self.pop[i] + self.F * (pbest - self.pop[i]) + self.F * (a - b)\n                else:\n                    # Random mutation\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n                    mutant = a + self.F * (b - c)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Clip\n                trial = np.clip(trial, self.bounds_lb, self.bounds_ub)\n                \n                # Opposition-based learning repair\n                trial_opposite = self.bounds_lb + self.bounds_ub - trial\n                trial_opposite = np.clip(trial_opposite, self.bounds_lb, self.bounds_ub)\n\n                # Evaluate both trial and its opposite\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if self.budget <= 0:\n                    return self.f_opt, self.x_opt\n                \n                f_trial_opposite = func(trial_opposite)\n                self.budget -= 1\n                \n                if self.budget <= 0:\n                    return self.f_opt, self.x_opt\n\n                # Select the better one\n                if f_trial_opposite < f_trial:\n                    trial = trial_opposite\n                    f_trial = f_trial_opposite\n\n                # Selection\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveDEOpp scored 0.482 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["7d1804bc-b3d8-4ec7-adfa-7f2adf5d4812"], "operator": null, "metadata": {"aucs": [0.17630873272500702, 0.456751670277869, 0.3832334060509359, 0.7357472670496781, 0.42209113364710127, 0.5876612505961707, 0.4638956301916797, 0.36402045715517894, 0.33752180408230437, 0.22694727336257925, 0.5038848855727378, 0.9971279274665618, 0.26209505453634996, 0.44956433409176755, 0.8671931463913662, 0.614960388912518, 0.3772910505073236, 0.7058908442479541, 0.19856532820792705, 0.4997226996316817]}}
{"id": "871442f7-8fc2-4f0e-918c-ad3a2082c3dd", "fitness": 0.713014477240284, "name": "AdaptiveDE", "description": "Simplified Adaptive DE with dynamic CR adjustment based on population diversity and focused repair.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.pop[best_idx]\n\n        while self.budget > 0:\n            # Dynamically adjust CR based on population diversity\n            pop_std = np.std(self.pop)\n            self.CR = 0.5 + 0.4 * np.tanh(pop_std) # CR adapts to population diversity\n\n            for i in range(self.pop_size):\n                # Mutation: best-guided with simplified parameters\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b = self.pop[np.random.choice(idxs, 2, replace=False)]\n                mutant = self.pop[i] + self.F * (self.x_opt - self.pop[i]) + self.F * (a - b)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Focused Repair: Repair only out-of-bounds values by reflecting them\n                for j in range(self.dim):\n                    if trial[j] < self.bounds_lb:\n                        trial[j] = self.bounds_lb + (self.bounds_lb - trial[j])  # Reflection\n                    elif trial[j] > self.bounds_ub:\n                        trial[j] = self.bounds_ub - (trial[j] - self.bounds_ub)  # Reflection\n                \n                # Evaluate and Select\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveDE scored 0.713 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["905390d8-8714-4fb3-993d-f860591fb98f"], "operator": null, "metadata": {"aucs": [0.15356823150133392, 0.8257424509007334, 0.8790007138293899, 0.19906874053408463, 0.9005923094207178, 0.756685458814975, 0.36092104071747344, 0.8659279660596437, 0.8985622584589922, 0.8475432010047266, 0.9476584642621521, 0.9977047238883456, 0.35696387086245374, 0.8998371642769802, 0.9653426081144324, 0.8172047357550591, 0.857141592767671, 0.9432049562279515, 0.2601695672982479, 0.5274494901103135]}}
{"id": "d2bf5116-fea6-4f61-a884-2bff3daba800", "fitness": 0.3896989763771173, "name": "OrthogonalDE", "description": "Differential Evolution with self-adaptive parameters, a diversity-enhancing mutation strategy using orthogonal learning, and a sigmoid-based repair mechanism.", "code": "import numpy as np\n\nclass OrthogonalDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, CR_init=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.CR = CR_init\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.pop[best_idx]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation: Orthogonal Learning-based\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = self.pop[i] + self.F * (a - b) + self.F * (self.x_opt - c)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Sigmoid-based Repair Mechanism\n                for j in range(self.dim):\n                    if trial[j] < self.bounds_lb:\n                        trial[j] = self.bounds_lb + (self.bounds_ub - self.bounds_lb) / (1 + np.exp(trial[j]))\n                    elif trial[j] > self.bounds_ub:\n                        trial[j] = self.bounds_ub - (self.bounds_ub - self.bounds_lb) / (1 + np.exp(-trial[j]))\n                \n                f_trial = func(trial)\n                self.budget -= 1\n                \n                # Selection and Parameter Adaptation\n                if f_trial < self.fitness[i]:\n                    self.F = 0.8 * self.F + 0.2 * np.random.rand()  # Simplified F update\n                    self.CR = 0.8 * self.CR + 0.2 * np.random.rand()  # Simplified CR update\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm OrthogonalDE scored 0.390 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["905390d8-8714-4fb3-993d-f860591fb98f"], "operator": null, "metadata": {"aucs": [0.13952305807217324, 0.20376157534899342, 0.3912921236044864, 0.4365711359791742, 0.2996027815860903, 0.43118025301320484, 0.28686725183964856, 0.34444555460521276, 0.3066112680114327, 0.19705256765249546, 0.44853980367501534, 0.9941825689917007, 0.26469326133636273, 0.30325059002944554, 0.7534889939327467, 0.4526649592860472, 0.3363984391474142, 0.49163414913738257, 0.21398561599932542, 0.4982335762939929]}}
{"id": "c2358d08-c14c-47c7-b459-745c9539e06a", "fitness": 0.6515314777940965, "name": "AdaptiveDEArchive", "description": "Simplified Adaptive Differential Evolution with archive and parameter adaptation based on fitness improvement.", "code": "import numpy as np\n\nclass AdaptiveDEArchive:\n    def __init__(self, budget=10000, dim=10, pop_size=40, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.archive_x = []\n        self.archive_f = []\n        self.F = 0.5\n        self.CR = 0.9\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.pop[best_idx]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b = self.pop[np.random.choice(idxs, 2, replace=False)]\n                \n                # Mutation with archive\n                if self.archive_x and np.random.rand() < 0.1:  # Use archive occasionally\n                    arc_idx = np.random.randint(len(self.archive_x))\n                    mutant = self.pop[i] + self.F * (self.archive_x[arc_idx] - self.pop[i]) + self.F * (a - b)\n                else:\n                    mutant = self.pop[i] + self.F * (self.x_opt - self.pop[i]) + self.F * (a - b)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n                trial = np.clip(trial, self.bounds_lb, self.bounds_ub)\n                \n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    # Update archive\n                    if len(self.archive_x) < self.archive_size:\n                        self.archive_x.append(self.pop[i].copy())\n                        self.archive_f.append(self.fitness[i])\n                    else:\n                        worst_arc_idx = np.argmax(self.archive_f)\n                        if self.fitness[i] < self.archive_f[worst_arc_idx]:\n                            self.archive_x[worst_arc_idx] = self.pop[i].copy()\n                            self.archive_f[worst_arc_idx] = self.fitness[i]\n                            \n                    # Adaptation (simplified)\n                    self.F = 0.5 + 0.4 * np.random.rand()\n                    self.CR = 0.9 * np.random.rand()\n                        \n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n            \n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveDEArchive scored 0.652 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6feb8ab7-a711-436a-866b-9fa05ef97493"], "operator": null, "metadata": {"aucs": [0.2012825061136787, 0.42109504296668643, 0.7280615154600176, 0.860177115396709, 0.7426152132346162, 0.8039486269920204, 0.5233513819900517, 0.6132892568615614, 0.7505320542518565, 0.6882208109542356, 0.8455845322131635, 0.9932322890966087, 0.3243165492540355, 0.6867372499162239, 0.9072129873430428, 0.8206821396401917, 0.5506500223840436, 0.8518701221393109, 0.2049055452971852, 0.5128645943766921]}}
{"id": "3401056a-951f-4236-9a92-2a14b41f15e0", "fitness": 0.726773756375191, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with success-history based parameter adaptation, best-guided mutation, and archive-based exploration for improved convergence and diversity.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, archive_size=10, F_init=0.5, CR_init=0.9, p_archive=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F_init\n        self.CR = CR_init\n        self.p_archive = p_archive\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.success_history_F = []\n        self.success_history_CR = []\n        self.archive = []\n\n\n    def __call__(self, func):\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.pop[best_idx]\n\n        while self.budget > 0:\n            successful_mutations = 0\n            successful_F = []\n            successful_CR = []\n\n            for i in range(self.pop_size):\n                # Mutation: best-guided with archive\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a = self.pop[np.random.choice(idxs)]\n                if np.random.rand() < self.p_archive and len(self.archive) > 0:\n                    b = self.archive[np.random.randint(len(self.archive))]\n                else:\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    b = self.pop[np.random.choice(idxs)]\n                mutant = self.pop[i] + self.F * (self.x_opt - self.pop[i]) + self.F * (a - b)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Clip and Evaluate\n                trial = np.clip(trial, self.bounds_lb, self.bounds_ub)\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                # Selection\n                if f_trial < self.fitness[i]:\n                    successful_mutations += 1\n                    successful_F.append(self.F)\n                    successful_CR.append(self.CR)\n\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        \n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.pop[i].copy())\n                    else:\n                        # Replace a random element in archive\n                        self.archive[np.random.randint(self.archive_size)] = self.pop[i].copy()\n\n            # Adapt F and CR based on success history (Leaning towards the center)\n            if successful_mutations > 0:\n                self.F = 0.5 * (self.F + np.mean(successful_F)) if successful_F else 0.5\n                self.CR = 0.5 * (self.CR + np.mean(successful_CR)) if successful_CR else 0.9\n            else:\n                self.F = 0.5  # Exploration value\n                self.CR = 0.9  # Exploration value\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveDE scored 0.727 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["7d1804bc-b3d8-4ec7-adfa-7f2adf5d4812"], "operator": null, "metadata": {"aucs": [0.24580866755808806, 0.8580951269409695, 0.6812478626411098, 0.9210398987588851, 0.9045016296064206, 0.927584769829574, 0.371580171338616, 0.8810516757337438, 0.9067207350736891, 0.3201358424802385, 0.9485442528683364, 0.996942452085541, 0.37194367510478, 0.41175181023025686, 0.9673077627505434, 0.9166290767101666, 0.8695566336078826, 0.9416931707858943, 0.5847703769262846, 0.5085695364727992]}}
{"id": "72216e43-38b4-42be-8b6a-c9b11dd3266b", "fitness": 0.6179522998894276, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with simplified success-history based F/CR adaptation and improved exploration using a larger initial population.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_init=0.5, CR_init=0.9, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.CR = CR_init\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.archive = []\n        self.archive_fitness = []\n        self.archive_size = archive_size\n\n    def __call__(self, func):\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.pop[best_idx]\n\n        while self.budget > 0:\n            \n            for i in range(self.pop_size):\n                # Mutation: best-guided with archive\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b = self.pop[np.random.choice(idxs, 2, replace=False)]\n                \n                if self.archive:\n                    arch_idx = np.random.randint(len(self.archive))\n                    mutant = self.pop[i] + self.F * (self.x_opt - self.pop[i]) + self.F * (a - b) + self.F * (self.archive[arch_idx] - self.pop[i])\n                else:\n                    mutant = self.pop[i] + self.F * (self.x_opt - self.pop[i]) + self.F * (a - b)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Clip and Evaluate\n                trial = np.clip(trial, self.bounds_lb, self.bounds_ub)\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                # Selection\n                if f_trial < self.fitness[i]:\n                    \n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.pop[i].copy())\n                        self.archive_fitness.append(self.fitness[i])\n                    else:\n                        worst_arch_idx = np.argmax(self.archive_fitness)\n                        if self.fitness[i] < self.archive_fitness[worst_arch_idx]:\n                            self.archive[worst_arch_idx] = self.pop[i].copy()\n                            self.archive_fitness[worst_arch_idx] = self.fitness[i]\n\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        \n            #Adaptation of parameters\n            self.F = 0.5 + 0.4 * np.random.rand()\n            self.CR = 0.1 + 0.9 * np.random.rand()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveDE scored 0.618 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["7d1804bc-b3d8-4ec7-adfa-7f2adf5d4812"], "operator": null, "metadata": {"aucs": [0.2128974940096965, 0.4096527355527021, 0.6396729789839152, 0.8118585127703465, 0.695557052659446, 0.7864943462297136, 0.4416841469603625, 0.5603526597201316, 0.6452272462312201, 0.5793367392330926, 0.8078938940511511, 0.9990618141928373, 0.40233855033371124, 0.636424806406114, 0.8971933536161243, 0.7440367352603581, 0.5252176917021498, 0.8273769679418543, 0.22472011940414593, 0.5120481525294787]}}
{"id": "90455991-2d1b-45b7-b1a8-56771fb66063", "fitness": 0.7463079104171924, "name": "AdaptiveDEOD", "description": "Adaptive Differential Evolution with orthogonal design for parameter tuning and a pool of mutation strategies.", "code": "import numpy as np\n\nclass AdaptiveDEOD:\n    def __init__(self, budget=10000, dim=10, pop_size=40, archive_size=10, mutation_pool_size=3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.mutation_pool_size = mutation_pool_size\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.archive = []\n        self.archive_fitness = []\n        self.F = 0.5\n        self.CR = 0.9\n\n        # Mutation strategies pool\n        self.mutation_strategies = [\n            self.mutation_current_to_best_1,\n            self.mutation_rand_1,\n            self.mutation_rand_2,\n        ]\n        self.mutation_weights = np.ones(self.mutation_pool_size) / self.mutation_pool_size\n\n        # Orthogonal Design for F and CR tuning\n        self.orthogonal_design = self.generate_orthogonal_design() # simple fixed orthogonal design\n        self.design_index = 0\n\n    def generate_orthogonal_design(self):\n       # A simple 2-level full factorial design for F and CR (2^2 = 4 runs)\n        design = np.array([[0.3, 0.7], [0.3, 0.9], [0.7, 0.7], [0.7, 0.9]])\n        return design\n\n    def mutation_current_to_best_1(self, i):\n        idxs = [idx for idx in range(self.pop_size) if idx != i]\n        a, b = self.pop[np.random.choice(idxs, 2, replace=False)]\n        return self.pop[i] + self.F * (self.x_opt - self.pop[i]) + self.F * (a - b)\n\n    def mutation_rand_1(self, i):\n        idxs = [idx for idx in range(self.pop_size) if idx != i]\n        a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n        return a + self.F * (b - c)\n\n    def mutation_rand_2(self, i):\n        idxs = [idx for idx in range(self.pop_size) if idx != i]\n        a, b, c, d, e = self.pop[np.random.choice(idxs, 5, replace=False)]\n        return a + self.F * (b - c) + self.F * (d-e)\n\n\n    def __call__(self, func):\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.pop[best_idx]\n\n        while self.budget > 0:\n            # Update F and CR using orthogonal design\n            self.F, self.CR = self.orthogonal_design[self.design_index]\n            self.design_index = (self.design_index + 1) % len(self.orthogonal_design)\n\n            for i in range(self.pop_size):\n                # Select mutation strategy based on weights\n                mutation_strategy_idx = np.random.choice(len(self.mutation_strategies), p=self.mutation_weights)\n                mutant = self.mutation_strategies[mutation_strategy_idx](i)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Clip and Evaluate\n                trial = np.clip(trial, self.bounds_lb, self.bounds_ub)\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                # Selection\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                        #Update the archive\n                        if len(self.archive) < self.archive_size:\n                            self.archive.append(trial)\n                            self.archive_fitness.append(f_trial)\n                        else:\n                            worst_arch_idx = np.argmax(self.archive_fitness)\n                            if f_trial < self.archive_fitness[worst_arch_idx]:\n                                self.archive[worst_arch_idx] = trial\n                                self.archive_fitness[worst_arch_idx] = f_trial\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveDEOD scored 0.746 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["7d1804bc-b3d8-4ec7-adfa-7f2adf5d4812"], "operator": null, "metadata": {"aucs": [0.34925546929540874, 0.71841300468659, 0.7775582144756905, 0.8878719885327233, 0.8033862063109404, 0.8463881037074599, 0.7227205927137306, 0.7471735627816112, 0.8097207362551723, 0.7735447877455459, 0.8900240676120046, 0.9979626642666954, 0.6386970921081745, 0.7874882814583808, 0.933740153650636, 0.8444158940146221, 0.7207792513084281, 0.8872827780482403, 0.26509827450759893, 0.5246370848641932]}}
{"id": "f20cf4f2-07ab-4b3c-957c-faf40ae695eb", "fitness": 0.7240073227140066, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with success-history based parameter adaptation, combined mutation strategies and improved repair mechanism.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, CR_init=0.9, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.CR = CR_init\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.archive = []\n        self.archive_size = archive_size\n        self.memory_F = np.ones(archive_size) * 0.5\n        self.memory_CR = np.ones(archive_size) * 0.9\n        self.memory_idx = 0\n\n    def __call__(self, func):\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.pop[best_idx]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Parameter Adaptation\n                F = np.random.choice(self.memory_F)\n                CR = np.random.choice(self.memory_CR)\n                \n                # Mutation Strategy Selection\n                if np.random.rand() < 0.5:\n                    # best-guided\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    a, b = self.pop[np.random.choice(idxs, 2, replace=False)]\n                    mutant = self.pop[i] + F * (self.x_opt - self.pop[i]) + F * (a - b)\n                else:\n                    # rand/1\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n                    mutant = self.pop[i] + F * (a - b) + F * (c - self.pop[i])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Improved Repair Mechanism: Clipping and Reflection\n                trial = np.clip(trial, self.bounds_lb, self.bounds_ub)\n                violated_mask = (trial < self.bounds_lb) | (trial > self.bounds_ub)\n                trial[violated_mask] = 2 * (self.bounds_lb + self.bounds_ub) / 2 - trial[violated_mask]\n                trial = np.clip(trial, self.bounds_lb, self.bounds_ub)\n\n\n                # Evaluate\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                # Selection and Archive Update\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        \n                    # Archive update (simplified, add F and CR used to archive)\n                    self.memory_F[self.memory_idx] = F\n                    self.memory_CR[self.memory_idx] = CR\n                    self.memory_idx = (self.memory_idx + 1) % self.archive_size\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveDE scored 0.724 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["905390d8-8714-4fb3-993d-f860591fb98f"], "operator": null, "metadata": {"aucs": [0.2715322665915366, 0.8485839670618482, 0.5374907071584357, 0.9208008941138879, 0.8670009482613247, 0.8821246484262668, 0.7100749977677537, 0.841561120597491, 0.8956867918121729, 0.35180949191869104, 0.9402290832210356, 0.9973976654199735, 0.8011215571354888, 0.8478468581207023, 0.8624807155440963, 0.8973385933099314, 0.30490441653739286, 0.9206332910226235, 0.24821770471272453, 0.5333107355467528]}}
{"id": "15dfae71-6173-4395-bcef-9045a180d5e3", "fitness": 0.6759818204322833, "name": "AdaptiveDE", "description": "Simplified Adaptive Differential Evolution with combined mutation based on best and random individuals, adaptive F/CR, and vectorized fitness evaluation for speed.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, CR_init=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.CR = CR_init\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.pop[best_idx]\n\n        while self.budget > 0:\n            # Mutation and Crossover\n            idxs = np.random.randint(0, self.pop_size, size=(self.pop_size, 2))\n            mutant = self.pop + self.F * (self.pop[best_idx] - self.pop) + self.F * (self.pop[idxs[:, 0]] - self.pop[idxs[:, 1]])\n            mutant = np.clip(mutant, self.bounds_lb, self.bounds_ub)\n            \n            cross_points = np.random.rand(self.pop_size, self.dim) < self.CR\n            trial = np.where(cross_points, mutant, self.pop)\n            \n            # Evaluate trials (vectorized)\n            f_trial = np.array([func(x) for x in trial])\n            self.budget -= self.pop_size\n            \n            # Selection and Adaptation\n            improved = f_trial < self.fitness\n            self.pop = np.where(improved[:, None], trial, self.pop)\n            self.fitness = np.where(improved, f_trial, self.fitness)\n            \n            # Update best solution\n            best_idx = np.argmin(self.fitness)\n            if self.fitness[best_idx] < self.f_opt:\n                self.f_opt = self.fitness[best_idx]\n                self.x_opt = self.pop[best_idx]\n            \n            # Simplified parameter adaptation\n            self.F = 0.9 * self.F + 0.1 * np.random.rand()\n            self.CR = 0.9 * self.CR + 0.1 * np.random.rand()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveDE scored 0.676 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c1c7ad3c-9112-4849-9fc2-d52a71e8f397"], "operator": null, "metadata": {"aucs": [0.2288677411154667, 0.4665375235613225, 0.7649720254333919, 0.9116612538955893, 0.7775429579013844, 0.8690541841694388, 0.6665727514972399, 0.755960874821083, 0.821617146186915, 0.35340932205982967, 0.9070266148618166, 0.9974318969204683, 0.2807584718737969, 0.6756117282457289, 0.9435702035015832, 0.8472415295288127, 0.6196388992397137, 0.897139426704579, 0.22225217766446337, 0.5127696794630409]}}
{"id": "e5b6019b-013e-4a42-aae8-36462f2cde39", "fitness": 0.6666413907105466, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with simplified parameter adaptation, greedy selection, and a combined mutation strategy focusing on global best exploitation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, CR_init=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.CR = CR_init\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.pop[best_idx]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b = self.pop[np.random.choice(idxs, 2, replace=False)]\n\n                # Combined mutation strategy with focus on exploitation\n                mutant = self.pop[i] + self.F * (self.x_opt - self.pop[i]) + self.F * (a - b)\n                \n                # Boundary handling (clip)\n                mutant = np.clip(mutant, self.bounds_lb, self.bounds_ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, self.pop[i])\n                \n                f_trial = func(trial)\n                self.budget -= 1\n                \n                # Greedy selection and simplified adaptation\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                    self.F = 0.8 * self.F + 0.2 * np.random.rand()  # Closer to previous value\n                    self.CR = 0.8 * self.CR + 0.2 * np.random.rand()  # Closer to previous value\n                         \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveDE scored 0.667 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c1c7ad3c-9112-4849-9fc2-d52a71e8f397"], "operator": null, "metadata": {"aucs": [0.2320259379757078, 0.5779175491302033, 0.7710179325481603, 0.8654391833625954, 0.7929359260311782, 0.8429297136447835, 0.5301442496981873, 0.6787522285158045, 0.7984621058590313, 0.2045437095377859, 0.8729754993753769, 0.998855266634916, 0.3214613272056037, 0.709567358324732, 0.9371120176523625, 0.8435718948597313, 0.6243912726587962, 0.8759584815164829, 0.34413564414443054, 0.5106305155350666]}}
{"id": "42d42eb7-c484-400c-84a6-312a1bb7c2ee", "fitness": 0.0, "name": "AdaptiveDE_SelfAdaptivePop", "description": "Adaptive Differential Evolution with a self-adaptive population size, combined with a tournament selection and a distance-based mutation strategy.", "code": "import numpy as np\n\nclass AdaptiveDE_SelfAdaptivePop:\n    def __init__(self, budget=10000, dim=10, pop_size_init=40, archive_size=10, F_init=0.5, CR_init=0.9, p_archive=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size_init\n        self.pop_size_min = 10\n        self.pop_size_max = 100\n        self.archive_size = archive_size\n        self.F = F_init\n        self.CR = CR_init\n        self.p_archive = p_archive\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.success_history_F = []\n        self.success_history_CR = []\n        self.archive = []\n        self.inertia = 0.1\n\n\n    def __call__(self, func):\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.pop[best_idx]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            successful_mutations = 0\n            successful_F = []\n            successful_CR = []\n\n            for i in range(self.pop_size):\n                # Mutation: distance-based with archive\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a = self.pop[np.random.choice(idxs)]\n                if np.random.rand() < self.p_archive and len(self.archive) > 0:\n                    b = self.archive[np.random.randint(len(self.archive))]\n                else:\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    b = self.pop[np.random.choice(idxs)]\n                \n                # Distance based F\n                distance = np.linalg.norm(self.pop[i] - self.x_opt)\n                adaptive_F = self.F * (1 + np.tanh(distance))  # Scale F based on distance to best\n                mutant = self.pop[i] + adaptive_F * (self.x_opt - self.pop[i]) + self.F * (a - b)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Clip and Evaluate\n                trial = np.clip(trial, self.bounds_lb, self.bounds_ub)\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                # Tournament Selection\n                opponent_idx = np.random.randint(self.pop_size)\n                if f_trial < self.fitness[i] or f_trial < self.fitness[opponent_idx]:\n                    successful_mutations += 1\n                    successful_F.append(self.F)\n                    successful_CR.append(self.CR)\n\n                    if f_trial < self.fitness[i]:\n                        self.fitness[i] = f_trial\n                        self.pop[i] = trial\n                    else:\n                        self.fitness[opponent_idx] = f_trial\n                        self.pop[opponent_idx] = trial                        \n                    \n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        \n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.pop[i].copy())\n                    else:\n                        # Replace a random element in archive\n                        self.archive[np.random.randint(self.archive_size)] = self.pop[i].copy()\n\n            # Adapt F and CR based on success history (Leaning towards the center)\n            if successful_mutations > 0:\n                self.F = 0.5 * (self.F + np.mean(successful_F)) if successful_F else 0.5\n                self.CR = 0.5 * (self.CR + np.mean(successful_CR)) if successful_CR else 0.9\n            else:\n                self.F = 0.5  # Exploration value\n                self.CR = 0.9  # Exploration value\n            \n            # Adapt population size\n            if generation % 10 == 0:\n                if successful_mutations / self.pop_size > 0.4:\n                    self.pop_size = min(self.pop_size + 1, self.pop_size_max)\n                    self.pop = np.vstack((self.pop, np.random.uniform(self.bounds_lb, self.bounds_ub, size=(1, self.dim))))\n                    new_fitness = np.array([func(self.pop[-1])])\n                    self.budget -= 1\n                    self.fitness = np.concatenate((self.fitness, new_fitness))\n                elif successful_mutations / self.pop_size < 0.1 and self.pop_size > self.pop_size_min:\n                    self.pop_size = max(self.pop_size - 1, self.pop_size_min)\n                    remove_idx = np.random.randint(self.pop_size)\n                    self.pop = np.delete(self.pop, remove_idx, axis=0)\n                    self.fitness = np.delete(self.fitness, remove_idx)\n                    \n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveDE_SelfAdaptivePop scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3401056a-951f-4236-9a92-2a14b41f15e0"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "ff412fbc-a61f-456e-a907-16f485e9de3a", "fitness": -Infinity, "name": "AdaptiveDEOD", "description": "Adaptive Differential Evolution with orthogonal design for parameter tuning, a pool of mutation strategies, and adaptive mutation weights based on success rate.", "code": "import numpy as np\n\nclass AdaptiveDEOD:\n    def __init__(self, budget=10000, dim=10, pop_size=40, archive_size=10, mutation_pool_size=3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.mutation_pool_size = mutation_pool_size\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.archive = []\n        self.archive_fitness = []\n        self.F = 0.5\n        self.CR = 0.9\n\n        # Mutation strategies pool\n        self.mutation_strategies = [\n            self.mutation_current_to_best_1,\n            self.mutation_rand_1,\n            self.mutation_rand_2,\n        ]\n        self.mutation_weights = np.ones(self.mutation_pool_size) / self.mutation_pool_size\n        self.success_counts = np.zeros(self.mutation_pool_size)\n        self.mutation_usage = np.zeros(self.mutation_pool_size)\n        self.epsilon = 1e-6\n\n        # Orthogonal Design for F and CR tuning\n        self.orthogonal_design = self.generate_orthogonal_design() # simple fixed orthogonal design\n        self.design_index = 0\n\n    def generate_orthogonal_design(self):\n       # A simple 2-level full factorial design for F and CR (2^2 = 4 runs)\n        design = np.array([[0.3, 0.7], [0.3, 0.9], [0.7, 0.7], [0.7, 0.9]])\n        return design\n\n    def mutation_current_to_best_1(self, i):\n        idxs = [idx for idx in range(self.pop_size) if idx != i]\n        a, b = self.pop[np.random.choice(idxs, 2, replace=False)]\n        return self.pop[i] + self.F * (self.x_opt - self.pop[i]) + self.F * (a - b)\n\n    def mutation_rand_1(self, i):\n        idxs = [idx for idx in range(self.pop_size) if idx != i]\n        a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n        return a + self.F * (b - c)\n\n    def mutation_rand_2(self, i):\n        idxs = [idx for idx in range(self.pop_size) if idx != i]\n        a, b, c, d, e = self.pop[np.random.choice(idxs, 5, replace=False)]\n        return a + self.F * (b - c) + self.F * (d-e)\n\n    def update_mutation_weights(self):\n        \"\"\"\n        Updates the mutation weights based on the success rate of each mutation strategy.\n        \"\"\"\n        normalized_success = self.success_counts / (self.mutation_usage + self.epsilon)\n        self.mutation_weights = normalized_success / np.sum(normalized_success)\n        self.success_counts[:] = 0\n        self.mutation_usage[:] = 0\n\n\n    def __call__(self, func):\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.pop[best_idx]\n\n        generation = 0\n        while self.budget > 0:\n            # Update F and CR using orthogonal design\n            self.F, self.CR = self.orthogonal_design[self.design_index]\n            self.design_index = (self.design_index + 1) % len(self.orthogonal_design)\n\n            for i in range(self.pop_size):\n                # Select mutation strategy based on weights\n                mutation_strategy_idx = np.random.choice(len(self.mutation_strategies), p=self.mutation_weights)\n                self.mutation_usage[mutation_strategy_idx] += 1\n                mutant = self.mutation_strategies[mutation_strategy_idx](i)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Clip and Evaluate\n                trial = np.clip(trial, self.bounds_lb, self.bounds_ub)\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                # Selection\n                if f_trial < self.fitness[i]:\n                    self.success_counts[mutation_strategy_idx] += 1\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                        #Update the archive\n                        if len(self.archive) < self.archive_size:\n                            self.archive.append(trial)\n                            self.archive_fitness.append(f_trial)\n                        else:\n                            worst_arch_idx = np.argmax(self.archive_fitness)\n                            if f_trial < self.archive_fitness[worst_arch_idx]:\n                                self.archive[worst_arch_idx] = trial\n                                self.archive_fitness[worst_arch_idx] = f_trial\n            generation += 1\n            if generation % 10 == 0:\n                self.update_mutation_weights()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "An exception occurred: probabilities contain NaN.", "error": "", "parent_ids": ["90455991-2d1b-45b7-b1a8-56771fb66063"], "operator": null, "metadata": {}}
{"id": "d46fb82f-cdf2-47ac-b40a-e61e204f63d8", "fitness": -Infinity, "name": "AdaptiveDEOD", "description": "Adaptive Differential Evolution with orthogonal design for parameter tuning, a pool of mutation strategies, and adaptive mutation weights based on success rate.", "code": "import numpy as np\n\nclass AdaptiveDEOD:\n    def __init__(self, budget=10000, dim=10, pop_size=40, archive_size=10, mutation_pool_size=3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.mutation_pool_size = mutation_pool_size\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.archive = []\n        self.archive_fitness = []\n        self.F = 0.5\n        self.CR = 0.9\n\n        # Mutation strategies pool\n        self.mutation_strategies = [\n            self.mutation_current_to_best_1,\n            self.mutation_rand_1,\n            self.mutation_rand_2,\n        ]\n        self.mutation_weights = np.ones(self.mutation_pool_size) / self.mutation_pool_size\n        self.success_counts = np.zeros(self.mutation_pool_size)\n        self.usage_counts = np.zeros(self.mutation_pool_size)\n\n\n        # Orthogonal Design for F and CR tuning\n        self.orthogonal_design = self.generate_orthogonal_design() # simple fixed orthogonal design\n        self.design_index = 0\n\n    def generate_orthogonal_design(self):\n       # A simple 2-level full factorial design for F and CR (2^2 = 4 runs)\n        design = np.array([[0.3, 0.7], [0.3, 0.9], [0.7, 0.7], [0.7, 0.9]])\n        return design\n\n    def mutation_current_to_best_1(self, i):\n        idxs = [idx for idx in range(self.pop_size) if idx != i]\n        a, b = self.pop[np.random.choice(idxs, 2, replace=False)]\n        return self.pop[i] + self.F * (self.x_opt - self.pop[i]) + self.F * (a - b)\n\n    def mutation_rand_1(self, i):\n        idxs = [idx for idx in range(self.pop_size) if idx != i]\n        a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n        return a + self.F * (b - c)\n\n    def mutation_rand_2(self, i):\n        idxs = [idx for idx in range(self.pop_size) if idx != i]\n        a, b, c, d, e = self.pop[np.random.choice(idxs, 5, replace=False)]\n        return a + self.F * (b - c) + self.F * (d-e)\n\n\n    def __call__(self, func):\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.pop[best_idx]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n\n            # Update F and CR using orthogonal design\n            self.F, self.CR = self.orthogonal_design[self.design_index]\n            self.design_index = (self.design_index + 1) % len(self.orthogonal_design)\n\n            for i in range(self.pop_size):\n                # Select mutation strategy based on weights\n                mutation_strategy_idx = np.random.choice(len(self.mutation_strategies), p=self.mutation_weights)\n                self.usage_counts[mutation_strategy_idx] += 1\n                mutant = self.mutation_strategies[mutation_strategy_idx](i)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Clip and Evaluate\n                trial = np.clip(trial, self.bounds_lb, self.bounds_ub)\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                # Selection\n                if f_trial < self.fitness[i]:\n                    self.success_counts[mutation_strategy_idx] += 1\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                        #Update the archive\n                        if len(self.archive) < self.archive_size:\n                            self.archive.append(trial)\n                            self.archive_fitness.append(f_trial)\n                        else:\n                            worst_arch_idx = np.argmax(self.archive_fitness)\n                            if f_trial < self.archive_fitness[worst_arch_idx]:\n                                self.archive[worst_arch_idx] = trial\n                                self.archive_fitness[worst_arch_idx] = f_trial\n\n            # Adapt mutation weights based on success rate every 10 generations\n            if generation % 10 == 0:\n                success_rates = self.success_counts / (self.usage_counts + 1e-6)  # Avoid division by zero\n                self.mutation_weights = success_rates / np.sum(success_rates)\n                self.success_counts = np.zeros(self.mutation_pool_size)\n                self.usage_counts = np.zeros(self.mutation_pool_size)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "An exception occurred: probabilities contain NaN.", "error": "", "parent_ids": ["90455991-2d1b-45b7-b1a8-56771fb66063"], "operator": null, "metadata": {}}
{"id": "48e4ea11-dcb9-4cd0-90f9-862141a57915", "fitness": 0.0, "name": "CooperativeAdaptiveDE", "description": "Cooperative Adaptive Differential Evolution with dynamic subpopulation allocation based on local search performance and orthogonal learning.", "code": "import numpy as np\n\nclass CooperativeAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, num_subpops=4, archive_size=10, local_search_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.num_subpops = num_subpops\n        self.archive_size = archive_size\n        self.local_search_prob = local_search_prob\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.subpops = [[] for _ in range(self.num_subpops)]\n        self.fitness = [[] for _ in range(self.num_subpops)]\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.archive = []\n        self.memory_F = [np.ones(archive_size) * 0.5 for _ in range(self.num_subpops)]\n        self.memory_CR = [np.ones(archive_size) * 0.9 for _ in range(self.num_subpops)]\n        self.memory_idx = [0] * self.num_subpops\n        self.subpop_sizes = [pop_size // num_subpops] * num_subpops\n        remaining = pop_size % num_subpops\n        for i in range(remaining):\n            self.subpop_sizes[i] += 1\n\n\n    def initialize_subpops(self, func):\n        start = 0\n        for i in range(self.num_subpops):\n            size = self.subpop_sizes[i]\n            self.subpops[i] = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(size, self.dim))\n            self.fitness[i] = np.array([func(x) for x in self.subpops[i]])\n            self.budget -= size\n\n            best_idx = np.argmin(self.fitness[i])\n            if self.fitness[i][best_idx] < self.f_opt:\n                self.f_opt = self.fitness[i][best_idx]\n                self.x_opt = self.subpops[i][best_idx]\n            start += size\n\n    def orthogonal_learning(self, pop, func, budget):\n        \"\"\"\n        Performs orthogonal learning on the population to improve diversity and convergence.\n        \"\"\"\n        if len(pop) == 0 or budget <= 0:\n            return pop, np.array([])\n\n        num_individuals = len(pop)\n        design_matrix = self.create_latin_hypercube(num_individuals, self.dim)\n        new_pop = np.copy(pop)\n\n        for i in range(num_individuals):\n            for j in range(self.dim):\n                level = design_matrix[i, j]\n                range_val = (self.bounds_ub - self.bounds_lb) / num_individuals\n                new_pop[i, j] = self.bounds_lb + level * range_val\n\n            new_pop[i] = np.clip(new_pop[i], self.bounds_lb, self.bounds_ub)\n        \n        new_fitness = np.array([func(x) for x in new_pop])\n        \n        return new_pop, new_fitness\n\n\n    def create_latin_hypercube(self, num_samples, num_variables):\n        \"\"\"\n        Creates a Latin Hypercube design.\n        \"\"\"\n        design = np.zeros((num_samples, num_variables))\n        for j in range(num_variables):\n            permutation = np.random.permutation(num_samples)\n            design[:, j] = permutation\n\n        return design\n\n\n    def __call__(self, func):\n        self.initialize_subpops(func)\n\n        while self.budget > 0:\n            subpop_performance = []\n            for i in range(self.num_subpops):\n                # Parameter Adaptation\n                F = np.random.choice(self.memory_F[i])\n                CR = np.random.choice(self.memory_CR[i])\n                \n                # Mutation\n                idxs = np.random.choice(len(self.subpops[i]), size=(len(self.subpops[i]), 3), replace=True)\n                mutant = self.subpops[i][idxs[:, 0]] + F * (self.subpops[i][idxs[:, 1]] - self.subpops[i][idxs[:, 2]])\n\n                # Crossover\n                cross_points = np.random.rand(len(self.subpops[i]), self.dim) < CR\n                if not np.any(cross_points, axis=1).all():\n                    for k in range(len(self.subpops[i])):\n                        if not np.any(cross_points[k, :]):\n                            cross_points[k, np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.subpops[i])\n\n                # Repair\n                trial = np.clip(trial, self.bounds_lb, self.bounds_ub)\n                violated_mask = (trial < self.bounds_lb) | (trial > self.bounds_ub)\n                trial[violated_mask] = 2 * (self.bounds_lb + self.bounds_ub) / 2 - trial[violated_mask]\n                trial = np.clip(trial, self.bounds_lb, self.bounds_ub)\n\n                # Evaluate\n                f_trial = np.array([func(x) for x in trial])\n                self.budget -= len(f_trial)\n\n                # Selection\n                improved = f_trial < self.fitness[i]\n                self.fitness[i][improved] = f_trial[improved]\n                self.subpops[i][improved] = trial[improved]\n\n                # Local Search\n                if np.random.rand() < self.local_search_prob:\n                   local_search_indices = np.random.choice(len(self.subpops[i]), size=len(self.subpops[i])//4, replace=False)\n                   for idx in local_search_indices:\n                       sigma = 0.1 * (self.bounds_ub - self.bounds_lb)\n                       new_x = np.clip(self.subpops[i][idx] + np.random.normal(0, sigma, self.dim), self.bounds_lb, self.bounds_ub)\n                       f_new = func(new_x)\n                       self.budget -= 1\n\n                       if f_new < self.fitness[i][idx]:\n                           self.fitness[i][idx] = f_new\n                           self.subpops[i][idx] = new_x\n                           \n                \n                # Orthogonal Learning\n                if self.budget > 0 and np.random.rand() < 0.2:  # Apply orthogonal learning occasionally\n                    self.subpops[i], new_fitness = self.orthogonal_learning(self.subpops[i], func, self.budget)\n                    if len(new_fitness) > 0:\n                        self.fitness[i] = new_fitness\n                        best_idx = np.argmin(self.fitness[i])\n                        if self.fitness[i][best_idx] < self.f_opt:\n                            self.f_opt = self.fitness[i][best_idx]\n                            self.x_opt = self.subpops[i][best_idx]\n\n                # Update archive\n                for j in range(len(self.subpops[i])):\n                    if improved[j]:\n                        self.memory_F[i][self.memory_idx[i]] = F\n                        self.memory_CR[i][self.memory_idx[i]] = CR\n                        self.memory_idx[i] = (self.memory_idx[i] + 1) % self.archive_size\n                            \n                # Calculate subpop performance\n                best_idx = np.argmin(self.fitness[i])\n                if self.fitness[i][best_idx] < self.f_opt:\n                    self.f_opt = self.fitness[i][best_idx]\n                    self.x_opt = self.subpops[i][best_idx]\n                subpop_performance.append(np.mean(improved))\n\n            # Dynamic Subpopulation Allocation\n            total_performance = sum(subpop_performance)\n            if total_performance > 0:\n                new_subpop_sizes = [max(1, int(p / total_performance * self.pop_size)) for p in subpop_performance]\n\n                # Adjust sizes to match total population size\n                diff = self.pop_size - sum(new_subpop_sizes)\n                if diff > 0:\n                   for k in range(diff):\n                        new_subpop_sizes[k % self.num_subpops] += 1\n                elif diff < 0:\n                    for k in range(-diff):\n                        if new_subpop_sizes[k % self.num_subpops] > 1:\n                            new_subpop_sizes[k % self.num_subpops] -= 1\n\n                # Redistribute population\n                new_subpops = [[] for _ in range(self.num_subpops)]\n                new_fitness = [[] for _ in range(self.num_subpops)]\n                \n                # Transfer individuals to new subpops\n                for i in range(self.num_subpops):\n                    current_size = len(self.subpops[i])\n                    new_size = new_subpop_sizes[i]\n\n                    if current_size <= new_size:\n                        new_subpops[i] = self.subpops[i].copy()\n                        new_fitness[i] = self.fitness[i].copy()\n\n                        # Fill remaining spots with random individuals from other subpops\n                        remaining = new_size - current_size\n                        other_subpops_indices = [j for j in range(self.num_subpops) if j != i]\n                        if len(other_subpops_indices) > 0:\n                            for _ in range(remaining):\n                                chosen_subpop = np.random.choice(other_subpops_indices)\n                                if len(self.subpops[chosen_subpop]) > 0:\n                                    chosen_idx = np.random.randint(0, len(self.subpops[chosen_subpop]))\n                                    new_individual = self.subpops[chosen_subpop][chosen_idx].copy()\n                                    new_fitness_val = self.fitness[chosen_subpop][chosen_idx]\n                                    new_subpops[i] = np.vstack([new_subpops[i], new_individual])\n                                    new_fitness[i] = np.append(new_fitness[i], new_fitness_val)\n                                else:\n                                    new_individual = np.random.uniform(self.bounds_lb, self.bounds_ub, size=self.dim)\n                                    new_fitness_val = func(new_individual)\n                                    self.budget -= 1\n                                    new_subpops[i] = np.vstack([new_subpops[i], new_individual])\n                                    new_fitness[i] = np.append(new_fitness[i], new_fitness_val)\n\n\n                    else:\n                        # Select best individuals\n                        sorted_indices = np.argsort(self.fitness[i])[:new_size]\n                        new_subpops[i] = self.subpops[i][sorted_indices].copy()\n                        new_fitness[i] = self.fitness[i][sorted_indices].copy()\n\n                self.subpops = new_subpops\n                self.fitness = new_fitness\n                self.subpop_sizes = new_subpop_sizes\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm CooperativeAdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f20cf4f2-07ab-4b3c-957c-faf40ae695eb"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "271eb37a-12e1-44b5-a38e-f137888772bf", "fitness": 0.6561473328562529, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with dynamic CR and F adaptation based on population diversity and success rate, using a combined mutation strategy and a mirrored boundary handling.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, CR=0.9, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.archive_size = archive_size\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.archive = []  # Archive for storing discarded solutions\n\n    def __call__(self, func):\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.pop[best_idx]\n\n        success_history_CR = []\n        success_history_F = []\n\n        while self.budget > 0:\n            # Dynamically adjust CR based on population diversity and success history\n            pop_std = np.std(self.pop)\n            self.CR = 0.5 + 0.4 * np.tanh(pop_std) # CR adapts to population diversity\n\n            # Adaptive F: Sample F from a Cauchy distribution.  Clip to [0.1, 1.0].\n            if success_history_F:\n                med_F = np.median(success_history_F)\n                self.F = np.clip(np.random.normal(med_F, 0.1), 0.1, 1.0)\n            else:\n                self.F = 0.5 # Default value if no history\n\n            for i in range(self.pop_size):\n                # Mutation: Combined mutation strategy\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n                \n                # \"DE/rand/1\"\n                mutant_rand = self.pop[i] + self.F * (a - b)\n\n                # \"DE/current-to-best/1\"\n                mutant_current_to_best = self.pop[i] + self.F * (self.x_opt - self.pop[i]) + self.F * (a - b)\n                \n                # Combine the two mutation strategies\n                mutant = 0.5 * mutant_rand + 0.5 * mutant_current_to_best\n\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Mirrored Boundary Handling\n                for j in range(self.dim):\n                    if trial[j] < self.bounds_lb:\n                        trial[j] = 2 * self.bounds_lb - trial[j]\n                        if trial[j] > self.bounds_ub: # if reflection goes beyond upper bound, clip\n                            trial[j] = self.bounds_ub\n                    elif trial[j] > self.bounds_ub:\n                        trial[j] = 2 * self.bounds_ub - trial[j]\n                        if trial[j] < self.bounds_lb: # if reflection goes beyond lower bound, clip\n                            trial[j] = self.bounds_lb\n                \n                # Evaluate and Select\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.archive.append(self.pop[i].copy()) # Store the old solution in archive\n                    if len(self.archive) > self.archive_size:\n                        self.archive.pop(0) # Maintain archive size\n\n                    success_history_CR.append(self.CR)\n                    success_history_F.append(self.F)\n                    \n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveDE scored 0.656 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["871442f7-8fc2-4f0e-918c-ad3a2082c3dd"], "operator": null, "metadata": {"aucs": [0.19699783883063327, 0.5956116494019823, 0.7277814572670784, 0.8831493959597982, 0.6867274881080428, 0.820485817645251, 0.3841640217391543, 0.6509791242473928, 0.7636338247424566, 0.4611486390161048, 0.9084998407425391, 0.9976177384539061, 0.715654258877946, 0.6011970259441477, 0.9148621410108193, 0.7776862537355816, 0.4716806474853291, 0.8580759397401314, 0.20382955838203676, 0.5031639957947293]}}
{"id": "27da3749-6039-44ec-ac94-2bcef8527ca5", "fitness": 0.43952731377026594, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with a self-adaptive population size and Cauchy mutation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=40, F=0.5, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size  # Initial population size\n        self.F = F\n        self.CR = CR\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.success_history_CR = []\n        self.success_history_F = []\n        self.archive = []\n\n    def __call__(self, func):\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.pop[best_idx]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            # Adjust population size based on success\n            if generation % 10 == 0:  # Adjust population size every 10 generations\n                if len(self.success_history_CR) > 0:\n                    success_rate = len(self.success_history_CR) / (generation*self.pop_size) # Fraction of successful trials\n\n                    if success_rate < 0.1:\n                        self.pop_size = max(10, int(self.pop_size * 0.9))  # Reduce population size\n                    elif success_rate > 0.3:\n                        self.pop_size = min(100, int(self.pop_size * 1.1))  # Increase population size\n                    \n                    # Resize population if needed\n                    if self.pop.shape[0] != self.pop_size:\n                        old_pop = self.pop\n                        old_fitness = self.fitness\n                        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n                        self.fitness = np.array([np.inf]*self.pop_size) # Initialize fitness to infinity\n\n                        n = min(self.pop_size,old_pop.shape[0]) # take the n best individuals\n                        idx = np.argsort(old_fitness)\n                        self.pop[:n] = old_pop[idx[:n]]\n                        self.fitness[:n] = old_fitness[idx[:n]]\n\n\n\n            for i in range(self.pop_size):\n                # Mutation: Cauchy mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b = self.pop[np.random.choice(idxs, 2, replace=False)]\n                cauchy_scale = 0.1  # Scale factor for Cauchy distribution\n                mutant = self.pop[i] + self.F * (self.x_opt - self.pop[i]) + np.random.standard_cauchy(size=self.dim) * cauchy_scale # Best guided mutation combined with cauchy\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Repair: Clipping\n                trial = np.clip(trial, self.bounds_lb, self.bounds_ub)\n                \n                # Evaluate and Select\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.success_history_CR.append(self.CR)\n                        self.success_history_F.append(self.F)\n                    self.archive.append(self.pop[i].copy())\n                \n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveDE scored 0.440 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["871442f7-8fc2-4f0e-918c-ad3a2082c3dd"], "operator": null, "metadata": {"aucs": [0.1408903693974577, 0.2153720841486716, 0.4351234306978976, 0.7436684063916743, 0.25403932056345835, 0.47384169759388517, 0.30236588994890135, 0.38923259708095026, 0.36007497365142005, 0.21169259896849135, 0.8099133348400338, 0.9998292589669243, 0.2816041752153142, 0.3572173520928702, 0.7848292059046196, 0.4242496032339006, 0.2661390630993067, 0.555005242364724, 0.28612935528650674, 0.4993283159583095]}}
{"id": "fc30bdef-184b-4d2d-9553-e8a6973371ba", "fitness": 0.7840518143013481, "name": "AdaptiveDE", "description": "Simplified Adaptive Differential Evolution with population-based mutation parameter adaptation and a combined repair strategy for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.archive = []\n        self.archive_size = archive_size\n        self.memory_F = np.ones(archive_size) * 0.5\n        self.memory_CR = np.ones(archive_size) * 0.9\n        self.memory_idx = 0\n\n    def __call__(self, func):\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.pop[best_idx]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Parameter Adaptation: Population-based\n                F = 0.5 + 0.3 * np.random.randn()\n                CR = 0.9 + 0.1 * np.random.randn()\n                F = np.clip(F, 0.1, 1.0)\n                CR = np.clip(CR, 0.1, 1.0)\n                \n                # Mutation Strategy Selection: Combined rand/1 and best/1\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n                if np.random.rand() < 0.5:\n                    mutant = self.pop[i] + F * (self.x_opt - self.pop[i]) + F * (a - b) # best/1\n                else:\n                    mutant = a + F * (b - c) # rand/1\n\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Repair Mechanism: Clipping and Reflection (Combined)\n                trial = np.clip(trial, self.bounds_lb, self.bounds_ub)\n                \n                # Evaluate\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                # Selection\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveDE scored 0.784 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f20cf4f2-07ab-4b3c-957c-faf40ae695eb"], "operator": null, "metadata": {"aucs": [0.45813004222698406, 0.8091080055347178, 0.8250287654567463, 0.9384798403281649, 0.8473573493989718, 0.8723613992532263, 0.7340006285434255, 0.7801515518230973, 0.860381735143445, 0.764726181982867, 0.9202791492315666, 0.9943422079443693, 0.789568019394719, 0.81405356638209, 0.9584910580366836, 0.863531429267984, 0.7771115076452777, 0.8975548508810861, 0.25125508803932617, 0.5251239095122113]}}
{"id": "cdedff39-36c4-4958-9023-fee09d3d0656", "fitness": 0.7485400859250375, "name": "AdaptiveDEOD", "description": "Adaptive Differential Evolution with dynamic mutation strategy adaptation based on success rate, orthogonal design for parameter control, and a more sophisticated archive update mechanism for enhanced exploration.", "code": "import numpy as np\n\nclass AdaptiveDEOD:\n    def __init__(self, budget=10000, dim=10, pop_size=40, archive_size=10, mutation_pool_size=3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.mutation_pool_size = mutation_pool_size\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.archive = []\n        self.archive_fitness = []\n        self.F = 0.5\n        self.CR = 0.9\n        self.success_mutation_counts = np.zeros(self.mutation_pool_size)\n        self.mutation_selection_counts = np.zeros(self.mutation_pool_size)\n        self.epsilon = 1e-6\n\n        # Mutation strategies pool\n        self.mutation_strategies = [\n            self.mutation_current_to_best_1,\n            self.mutation_rand_1,\n            self.mutation_rand_2,\n        ]\n        self.mutation_weights = np.ones(self.mutation_pool_size) / self.mutation_pool_size\n\n        # Orthogonal Design for F and CR tuning\n        self.orthogonal_design = self.generate_orthogonal_design()\n        self.design_index = 0\n\n    def generate_orthogonal_design(self):\n        # A simple 3-level orthogonal design for F and CR tuning\n        design = np.array([[0.3, 0.7], [0.3, 0.9], [0.5, 0.8], [0.7, 0.7], [0.7, 0.9], [0.5, 0.7]])\n        return design\n\n    def mutation_current_to_best_1(self, i):\n        idxs = [idx for idx in range(self.pop_size) if idx != i]\n        a, b = self.pop[np.random.choice(idxs, 2, replace=False)]\n        return self.pop[i] + self.F * (self.x_opt - self.pop[i]) + self.F * (a - b)\n\n    def mutation_rand_1(self, i):\n        idxs = [idx for idx in range(self.pop_size) if idx != i]\n        a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n        return a + self.F * (b - c)\n\n    def mutation_rand_2(self, i):\n        idxs = [idx for idx in range(self.pop_size) if idx != i]\n        a, b, c, d, e = self.pop[np.random.choice(idxs, 5, replace=False)]\n        return a + self.F * (b - c) + self.F * (d-e)\n\n    def update_mutation_weights(self):\n        # Update mutation weights based on success rate\n        success_rates = (self.success_mutation_counts + self.epsilon) / (self.mutation_selection_counts + self.epsilon)\n        self.mutation_weights = success_rates / np.sum(success_rates)\n        self.success_mutation_counts[:] = 0\n        self.mutation_selection_counts[:] = 0\n\n    def update_archive(self, trial, f_trial):\n        if len(self.archive) < self.archive_size:\n            self.archive.append(trial)\n            self.archive_fitness.append(f_trial)\n        else:\n            # Replace the worst individual in the archive *only* if the new individual is better\n            worst_arch_idx = np.argmax(self.archive_fitness)\n            if f_trial < self.archive_fitness[worst_arch_idx]:\n                self.archive[worst_arch_idx] = trial\n                self.archive_fitness[worst_arch_idx] = f_trial\n\n    def __call__(self, func):\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.pop[best_idx]\n        self.archive = list(self.pop) # Initialize Archive with initial population\n        self.archive_fitness = list(self.fitness)\n\n        while self.budget > 0:\n            # Update F and CR using orthogonal design\n            self.F, self.CR = self.orthogonal_design[self.design_index]\n            self.design_index = (self.design_index + 1) % len(self.orthogonal_design)\n\n            for i in range(self.pop_size):\n                # Select mutation strategy based on weights\n                mutation_strategy_idx = np.random.choice(len(self.mutation_strategies), p=self.mutation_weights)\n                self.mutation_selection_counts[mutation_strategy_idx] += 1\n                mutant = self.mutation_strategies[mutation_strategy_idx](i)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Clip and Evaluate\n                trial = np.clip(trial, self.bounds_lb, self.bounds_ub)\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                # Selection\n                if f_trial < self.fitness[i]:\n                    self.success_mutation_counts[mutation_strategy_idx] += 1\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                    self.update_archive(trial, f_trial)  # Update archive on improvement\n\n            if self.budget > 0:\n                self.update_mutation_weights() # Update weights every generation\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveDEOD scored 0.749 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["90455991-2d1b-45b7-b1a8-56771fb66063"], "operator": null, "metadata": {"aucs": [0.3518595378223047, 0.6211064929375754, 0.7991962581896945, 0.9184805887618059, 0.8361228091079751, 0.8591482233827126, 0.7296619556619044, 0.7566500753005845, 0.8232100949517214, 0.7646207648330963, 0.8666736788580577, 0.9999167340782463, 0.6013260367445753, 0.7929615930497267, 0.9428579566553724, 0.8558450719532306, 0.704931987422302, 0.9027989651554473, 0.3286915785265646, 0.5147413151078523]}}
{"id": "8bc9e938-4601-485d-913f-fe845355fbd6", "fitness": 0.6409775396694855, "name": "AdaptiveDE", "description": "Simplified Adaptive Differential Evolution with best-member weighted difference and dynamic CR adaptation based on recent success.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, archive_size=10, F_init=0.5, CR_init=0.9, p_archive=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F_init\n        self.CR = CR_init\n        self.p_archive = p_archive\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.archive = []\n        self.successful_cr = []\n\n\n    def __call__(self, func):\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.pop[best_idx]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation: best-guided with weighted difference\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                a, b, c = self.pop[idxs]\n                mutant = self.pop[i] + self.F * (self.x_opt - self.pop[i]) + self.F * (a - b)\n\n                # Crossover\n                if self.successful_cr:\n                    cr = np.clip(np.random.normal(np.mean(self.successful_cr), np.std(self.successful_cr), 1), 0, 1)[0]\n                else:\n                    cr = self.CR\n\n                cross_points = np.random.rand(self.dim) < cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Clip and Evaluate\n                trial = np.clip(trial, self.bounds_lb, self.bounds_ub)\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                # Selection\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                    self.successful_cr.append(cr)\n\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.pop[i].copy())\n                    else:\n                        # Replace a random element in archive\n                        self.archive[np.random.randint(self.archive_size)] = self.pop[i].copy()\n                if len(self.successful_cr) > 10:\n                    self.successful_cr.pop(0)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveDE scored 0.641 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3401056a-951f-4236-9a92-2a14b41f15e0"], "operator": null, "metadata": {"aucs": [0.434869567597991, 0.23559337593040486, 0.3813137544194557, 0.9527795755598565, 0.9049430965753588, 0.9194435206569866, 0.34829632469852145, 0.8802046943095743, 0.8941555246419468, 0.2091799273963756, 0.956647968718988, 0.9962942297941678, 0.3337869146032748, 0.46809333045690227, 0.9596358409693935, 0.9119286059025227, 0.28799935806623334, 0.9405943439981802, 0.29423344655214456, 0.5095573925414305]}}
{"id": "6eed164b-ba06-48b8-9d11-8986e9d57ff0", "fitness": 0.6612287188661679, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with orthogonal crossover, success-history adaptation, and a dynamic population size adjustment.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, CR_init=0.9, archive_size=10, ortho_group_size=3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.CR = CR_init\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.archive = []\n        self.archive_size = archive_size\n        self.memory_F = np.ones(archive_size) * 0.5\n        self.memory_CR = np.ones(archive_size) * 0.9\n        self.memory_idx = 0\n        self.ortho_group_size = ortho_group_size  # Size of groups for orthogonal crossover\n        self.success_F = []\n        self.success_CR = []\n        self.min_pop_size = 10\n        self.max_pop_size = 100\n\n\n    def __call__(self, func):\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.pop[best_idx]\n\n        while self.budget > 0:\n            \n            # Population Size Adjustment (simplified dynamic adjustment)\n            if len(self.success_F) > 0:\n                success_mean_F = np.mean(self.success_F)\n                if success_mean_F > 0.7:  # Empirically chosen threshold\n                    self.pop_size = max(self.min_pop_size, int(self.pop_size * 0.9))  # Reduce population if F is high\n                elif success_mean_F < 0.3:\n                    self.pop_size = min(self.max_pop_size, int(self.pop_size * 1.1)) # Increase pop if F is low\n                \n                self.pop_size = int(np.clip(self.pop_size, self.min_pop_size, self.max_pop_size)) # clip pop_size\n                if self.pop.shape[0] != self.pop_size:\n                  #resize the population to the new size\n                  if self.pop.shape[0] > self.pop_size:\n                      self.pop = self.pop[:self.pop_size, :]\n                      self.fitness = self.fitness[:self.pop_size]\n                  else:\n                      num_new = self.pop_size - self.pop.shape[0]\n                      new_pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(num_new, self.dim))\n                      new_fitness = np.array([func(x) for x in new_pop])\n                      self.budget -= num_new\n                      self.pop = np.concatenate([self.pop, new_pop], axis=0)\n                      self.fitness = np.concatenate([self.fitness, new_fitness])\n                \n            self.success_F = []\n            self.success_CR = []\n\n            for i in range(self.pop_size):\n                # Parameter Adaptation\n                F = np.random.choice(self.memory_F)\n                CR = np.random.choice(self.memory_CR)\n                \n                # Mutation Strategy Selection\n                if np.random.rand() < 0.5:\n                    # best-guided\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    a, b = self.pop[np.random.choice(idxs, 2, replace=False)]\n                    mutant = self.pop[i] + F * (self.x_opt - self.pop[i]) + F * (a - b)\n                else:\n                    # rand/1\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n                    mutant = self.pop[i] + F * (a - b) + F * (c - self.pop[i])\n\n                # Orthogonal Crossover\n                group_indices = np.random.choice(self.dim, size=min(self.ortho_group_size, self.dim), replace=False)\n                trial = self.pop[i].copy()  # Start with the original individual\n                trial[group_indices] = mutant[group_indices] # use mutant components\n                \n                # Improved Repair Mechanism: Clipping and Reflection\n                trial = np.clip(trial, self.bounds_lb, self.bounds_ub)\n                violated_mask = (trial < self.bounds_lb) | (trial > self.bounds_ub)\n                trial[violated_mask] = 2 * (self.bounds_lb + self.bounds_ub) / 2 - trial[violated_mask]\n                trial = np.clip(trial, self.bounds_lb, self.bounds_ub)\n\n\n                # Evaluate\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                # Selection and Archive Update\n                if f_trial < self.fitness[i]:\n                    self.success_F.append(F)\n                    self.success_CR.append(CR)\n\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        \n                    # Archive update (simplified, add F and CR used to archive)\n                    self.memory_F[self.memory_idx] = F\n                    self.memory_CR[self.memory_idx] = CR\n                    self.memory_idx = (self.memory_idx + 1) % self.archive_size\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveDE scored 0.661 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f20cf4f2-07ab-4b3c-957c-faf40ae695eb"], "operator": null, "metadata": {"aucs": [0.20816525240507222, 0.429133552553968, 0.7571066630241411, 0.898413622282489, 0.8045892041606216, 0.8554775572058079, 0.655708857602344, 0.6836523622567939, 0.8207087498662093, 0.4314828302055279, 0.8494827445104481, 0.9976048601144977, 0.30356404206230303, 0.6481932735616726, 0.9008467535622245, 0.8310213721692731, 0.542679678918726, 0.8850419032929157, 0.1975752169126218, 0.5241258806557003]}}
{"id": "16922287-d249-469b-8762-4f75068850dd", "fitness": -Infinity, "name": "AdaptiveDEOC", "description": "Adaptive Differential Evolution with orthogonal crossover, self-adaptive mutation scaling, and a restart mechanism for improved exploration.", "code": "import numpy as np\n\nclass AdaptiveDEOC:\n    def __init__(self, budget=10000, dim=10, pop_size=40, archive_size=10, F_init=0.5, CR_init=0.9, p_archive=0.1, restart_trigger=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F_init\n        self.CR = CR_init\n        self.p_archive = p_archive\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.success_history_F = []\n        self.success_history_CR = []\n        self.archive = []\n        self.restart_trigger = restart_trigger # Probability of restarting the population\n\n    def orthogonal_crossover(self, x, mutant):\n        # Orthogonal Crossover: Generate an orthogonal array\n        n_params = self.dim\n        if n_params <= 16:\n            import pyDOE2\n            oa = pyDOE2.fracfact('o ' + ' '.join(['1'] * (int(np.log2(n_params))))) # only works for power of 2\n            #oa = pyDOE2.fracfact('1 2 3 4')\n            oa = (oa + 1) / 2 # 0 or 1\n        else:\n            oa = np.random.randint(0, 2, size=(n_params, n_params)) # Binary Matrix\n\n        trial = np.copy(x)\n        for j in range(self.dim):\n            if oa[j, np.random.randint(self.dim)] == 1:\n                trial[j] = mutant[j]\n            else:\n                trial[j] = x[j]\n        return trial\n\n    def __call__(self, func):\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.pop[best_idx]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            successful_mutations = 0\n            successful_F = []\n            successful_CR = []\n\n            for i in range(self.pop_size):\n                # Mutation: best-guided with archive and self-adaptive scaling\n                F_adapted = self.F * (1 + 0.1 * np.random.randn()) # Self-adaptive scaling\n                F_adapted = np.clip(F_adapted, 0.1, 1.0)\n\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a = self.pop[np.random.choice(idxs)]\n                if np.random.rand() < self.p_archive and len(self.archive) > 0:\n                    b = self.archive[np.random.randint(len(self.archive))]\n                else:\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    b = self.pop[np.random.choice(idxs)]\n                mutant = self.pop[i] + F_adapted * (self.x_opt - self.pop[i]) + F_adapted * (a - b)\n\n                # Crossover (Orthogonal)\n                trial = self.orthogonal_crossover(self.pop[i], mutant)\n\n                # Clip and Evaluate\n                trial = np.clip(trial, self.bounds_lb, self.bounds_ub)\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                # Selection\n                if f_trial < self.fitness[i]:\n                    successful_mutations += 1\n                    successful_F.append(F_adapted)\n                    successful_CR.append(self.CR)\n\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        \n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.pop[i].copy())\n                    else:\n                        # Replace a random element in archive\n                        self.archive[np.random.randint(self.archive_size)] = self.pop[i].copy()\n\n            # Adapt F and CR based on success history (Leaning towards the center)\n            if successful_mutations > 0:\n                self.F = 0.5 * (self.F + np.mean(successful_F)) if successful_F else 0.5\n                self.CR = 0.5 * (self.CR + np.mean(successful_CR)) if successful_CR else 0.9\n            else:\n                self.F = 0.5  # Exploration value\n                self.CR = 0.9  # Exploration value\n                \n            # Restart mechanism\n            if np.random.rand() < self.restart_trigger:\n                self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.pop])\n                best_idx = np.argmin(self.fitness)\n                self.f_opt = self.fitness[best_idx]\n                self.x_opt = self.pop[best_idx]\n                self.archive = [] # Reset archive\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "An exception occurred: No module named 'pyDOE2'.", "error": "", "parent_ids": ["3401056a-951f-4236-9a92-2a14b41f15e0"], "operator": null, "metadata": {}}
{"id": "02cbc350-0a49-432d-9676-93d00e06ca39", "fitness": 0.730261601883306, "name": "SimplifiedAdaptiveDE", "description": "Simplified Adaptive Differential Evolution using a single, self-adaptive mutation strategy based on past success.", "code": "import numpy as np\n\nclass SimplifiedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.F = 0.5\n        self.CR = 0.9\n        self.memory_F = np.ones(5) * 0.5\n        self.memory_CR = np.ones(5) * 0.9\n        self.memory_index = 0\n\n    def __call__(self, func):\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.pop[best_idx]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Adaptation of F and CR using memory\n                self.F = np.random.choice(self.memory_F)\n                self.CR = np.random.choice(self.memory_CR)\n\n                # Mutation: current-to-best/1\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b = self.pop[np.random.choice(idxs, 2, replace=False)]\n                mutant = self.pop[i] + self.F * (self.x_opt - self.pop[i]) + self.F * (a - b)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Clip and Evaluate\n                trial = np.clip(trial, self.bounds_lb, self.bounds_ub)\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                # Selection\n                if f_trial < self.fitness[i]:\n                    # Update memory with success\n                    self.memory_F[self.memory_index] = self.F\n                    self.memory_CR[self.memory_index] = self.CR\n                    self.memory_index = (self.memory_index + 1) % len(self.memory_F)\n\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm SimplifiedAdaptiveDE scored 0.730 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["90455991-2d1b-45b7-b1a8-56771fb66063"], "operator": null, "metadata": {"aucs": [0.256944593268028, 0.8731681846305431, 0.8770566957548689, 0.1991310445592842, 0.906570955282698, 0.9155011488566209, 0.3226191715851163, 0.8812565049493413, 0.9013520345374035, 0.7367032364776732, 0.9468048605270487, 0.9954680820130632, 0.2723786262005319, 0.774185515360277, 0.9665996935435786, 0.9148027976936086, 0.8547960080707471, 0.9414193377354745, 0.5595334655753539, 0.508940081044861]}}
{"id": "fca2fe27-6595-4272-911a-149cf5ccec74", "fitness": 0.3231264504217118, "name": "CauchyLocalSearch", "description": "Population-based optimization with a Cauchy mutation operator, adaptive scaling factor, and a self-adjusting local search probability.", "code": "import numpy as np\n\nclass CauchyLocalSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=40, initial_scale=1.0, ls_prob_init=0.1, ls_decay=0.99):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.initial_scale = initial_scale\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.scale = initial_scale\n        self.ls_prob = ls_prob_init  # Probability of local search\n        self.ls_decay = ls_decay # Decay factor for local search probability\n        self.min_scale = 0.01\n\n\n    def __call__(self, func):\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.pop[best_idx]\n\n        while self.budget > 0:\n            new_pop = np.zeros_like(self.pop)\n            new_fitness = np.zeros_like(self.fitness)\n            \n            for i in range(self.pop_size):\n                # Cauchy mutation\n                z = np.random.standard_cauchy(size=self.dim)\n                mutant = self.pop[i] + self.scale * z\n\n                # Clipping\n                mutant = np.clip(mutant, self.bounds_lb, self.bounds_ub)\n                \n                # Local Search (with adaptive probability)\n                if np.random.rand() < self.ls_prob:\n                    # Perform a small perturbation around the mutant\n                    perturbation = np.random.uniform(-0.1, 0.1, size=self.dim)\n                    local_mutant = mutant + perturbation\n                    local_mutant = np.clip(local_mutant, self.bounds_lb, self.bounds_ub)\n                else:\n                    local_mutant = mutant # no change\n\n                f_mutant = func(local_mutant)\n                self.budget -= 1\n\n                if f_mutant < self.fitness[i]:\n                    new_pop[i] = local_mutant\n                    new_fitness[i] = f_mutant\n\n                    if f_mutant < self.f_opt:\n                        self.f_opt = f_mutant\n                        self.x_opt = local_mutant\n                else:\n                    new_pop[i] = self.pop[i]\n                    new_fitness[i] = self.fitness[i]\n\n                if self.budget <=0:\n                  break\n            \n            self.pop = new_pop\n            self.fitness = new_fitness\n\n            # Adaptive scaling factor\n            if len(self.fitness) > 0:\n                delta_f = np.abs(self.fitness - np.mean(self.fitness))\n                if np.sum(delta_f) > 1e-6:  # Avoid division by zero\n                    self.scale = self.initial_scale / (1 + np.mean(delta_f))\n                else:\n                    self.scale = self.initial_scale  # Reset scale if no improvement\n\n                self.scale = max(self.scale, self.min_scale) # avoid scale collapse\n\n            # Local search probability decay\n            self.ls_prob *= self.ls_decay\n            self.ls_prob = max(self.ls_prob, 0.01)  # Ensure it doesn't go to zero\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm CauchyLocalSearch scored 0.323 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6eed164b-ba06-48b8-9d11-8986e9d57ff0"], "operator": null, "metadata": {"aucs": [0.09530296855610776, 0.21767573373105742, 0.2905120063776653, 0.34604435069731965, 0.2396757085937712, 0.3091268143751936, 0.2643170096171259, 0.2589430265722792, 0.2542266298690129, 0.17855347205087857, 0.25337988565205993, 0.9991871148317227, 0.29409025459312454, 0.24516234678074567, 0.6513176935451311, 0.31281527148837207, 0.2658642795802397, 0.3291783845202877, 0.18435221252368417, 0.47280384447845647]}}
{"id": "732f87b2-2f37-485b-85a7-b6dd420e6793", "fitness": 0.2342837074319688, "name": "CMAES_AdaptiveDE", "description": "Adaptive Differential Evolution with a self-adjusting population size, covariance matrix adaptation for mutation, and orthogonal learning for parameter tuning.", "code": "import numpy as np\n\nclass CMAES_AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size_factor=4, initial_sigma=0.1, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_factor = pop_size_factor\n        self.initial_sigma = initial_sigma\n        self.archive_size = archive_size\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.archive = []\n        self.archive_fitness = []\n        self.F = 0.5\n        self.CR = 0.9\n        self.epsilon = 1e-6\n        self.pop_size = int(self.pop_size_factor * self.dim)\n\n        # CMA-ES related parameters\n        self.mu = self.pop_size // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n        self.sigma = self.initial_sigma\n        self.C = np.eye(self.dim)  # Covariance matrix\n        self.pc = np.zeros(self.dim)  # Evolution path for C\n        self.ps = np.zeros(self.dim)  # Evolution path for sigma\n        self.chiN = self.dim**0.5 * (1 - 1/(4*self.dim) + 1/(21*self.dim**2))\n\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2*max(0, np.sqrt((self.mueff - 1)/(self.dim + 1)) - 1) + self.cs\n\n        self.cc = (4 + self.mueff/self.dim) / (self.dim + 4 + 2*self.mueff/self.dim)\n        self.mu_eff_minus_one = self.mueff - 1\n        self.alpha_mu = 2\n        self.alpha_posdef = (1.5 * self.alpha_mu - 0.5) * (self.mu_eff_minus_one + 2) / (self.dim + 2)\n        self.alpha_posdef = min(1, self.alpha_posdef)\n        self.c1 = self.alpha_posdef / ((self.mu_eff_minus_one + 2) * np.prod([1]))\n        self.cmu = min(1 - self.c1, self.alpha_posdef * self.mu_eff_minus_one / ((self.mu_eff_minus_one + 2) * np.prod([1])))\n\n        # Orthogonal Design for F and CR tuning\n        self.orthogonal_design = self.generate_orthogonal_design()\n        self.design_index = 0\n        self.adaptive_pop_size = True\n\n    def generate_orthogonal_design(self):\n        # A simple 3-level orthogonal design for F and CR tuning\n        design = np.array([[0.3, 0.7], [0.3, 0.9], [0.5, 0.8], [0.7, 0.7], [0.7, 0.9], [0.5, 0.7]])\n        return design\n    \n    def sample_population(self):\n        z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n        return self.x_mean + self.sigma * z @ np.linalg.cholesky(self.C).T\n    \n    def update_archive(self, trial, f_trial):\n        if len(self.archive) < self.archive_size:\n            self.archive.append(trial)\n            self.archive_fitness.append(f_trial)\n        else:\n            # Replace the worst individual in the archive *only* if the new individual is better\n            worst_arch_idx = np.argmax(self.archive_fitness)\n            if f_trial < self.archive_fitness[worst_arch_idx]:\n                self.archive[worst_arch_idx] = trial\n                self.archive_fitness[worst_arch_idx] = f_trial\n\n    def __call__(self, func):\n        # Initialization\n        self.x_mean = np.random.uniform(self.bounds_lb, self.bounds_ub, size=self.dim)\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.pop[best_idx]\n        self.archive = list(self.pop) # Initialize Archive with initial population\n        self.archive_fitness = list(self.fitness)\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            # Generate offspring\n            offspring = self.sample_population()\n            offspring = np.clip(offspring, self.bounds_lb, self.bounds_ub)\n\n            # Evaluate offspring\n            fitness = np.array([func(x) for x in offspring])\n            self.budget -= self.pop_size\n            \n            # Sort by fitness\n            idx = np.argsort(fitness)\n            offspring = offspring[idx]\n            fitness = fitness[idx]\n\n            # Update best solution\n            if fitness[0] < self.f_opt:\n                self.f_opt = fitness[0]\n                self.x_opt = offspring[0]\n\n            # Update archive\n            for i in range(self.pop_size):\n                self.update_archive(offspring[i], fitness[i])\n\n            # Weighted recombination\n            x_mean_old = self.x_mean.copy()\n            self.x_mean = np.sum(self.weights[:, None] * offspring[:self.mu], axis=0)\n\n            # Update evolution paths\n            B = np.linalg.cholesky(self.C)\n            z = (self.x_mean - x_mean_old) / self.sigma\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * z\n            hsig = np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * generation)) < self.chiN * (1.4 + 2/(self.dim + 1))\n            self.pc = (1 - self.cc) * self.pc + hsig * np.sqrt(self.cc * (2 - self.cc) * self.mueff) * (B @ z)\n        \n            # Update covariance matrix\n            self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * (self.pc[:, None] @ self.pc[None, :]) + self.cmu * np.sum(self.weights[:, None, None] * (B @ z[:, None] @ (B @ z[:, None]).T), axis=0)\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T  # Enforce symmetry\n            \n            try:\n                np.linalg.cholesky(self.C)  # Ensure C remains positive definite\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)  # Reset to identity matrix\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm CMAES_AdaptiveDE scored 0.234 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["cdedff39-36c4-4958-9023-fee09d3d0656"], "operator": null, "metadata": {"aucs": [0.09460346540574627, 0.173603056923618, 0.23638531236040283, 0.1642244786863617, 0.18997833662370522, 0.24697743977102848, 0.21358071156024605, 0.17047615783842507, 0.19659739958682665, 0.1418237724189244, 0.2026373729750337, 0.7570318463370214, 0.20561485892326592, 0.14999381917430088, 0.39637299512396784, 0.21653287725751136, 0.21699332365916024, 0.18732431967762486, 0.10491914967005489, 0.4200034546661503]}}
{"id": "19b645e6-6e5f-4e29-8620-c041694717fa", "fitness": 0.2925508436097809, "name": "DiversityAdaptiveDE", "description": "Population-based Adaptive Differential Evolution with a restart mechanism and a learning rate based on the population diversity.", "code": "import numpy as np\n\nclass DiversityAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, CR_init=0.9, archive_size=10, restart_trigger=0.1, diversity_threshold=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.CR = CR_init\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.archive = []\n        self.archive_size = archive_size\n        self.memory_F = np.ones(archive_size) * 0.5\n        self.memory_CR = np.ones(archive_size) * 0.9\n        self.memory_idx = 0\n        self.success_F = []\n        self.success_CR = []\n        self.restart_trigger = restart_trigger # Percentage of budget to trigger restart\n        self.diversity_threshold = diversity_threshold\n\n    def calculate_diversity(self):\n        \"\"\"Calculates the diversity of the population based on the mean pairwise distance.\"\"\"\n        distances = []\n        for i in range(self.pop_size):\n            for j in range(i + 1, self.pop_size):\n                distances.append(np.linalg.norm(self.pop[i] - self.pop[j]))\n        return np.mean(distances) if distances else 0\n\n    def __call__(self, func):\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.pop[best_idx]\n\n        initial_budget = self.budget + self.pop_size\n        restart_cycle = int(self.restart_trigger * initial_budget)\n        \n        while self.budget > 0:\n            \n            diversity = self.calculate_diversity()\n            learning_rate = diversity  # Use diversity as learning rate\n\n            self.success_F = []\n            self.success_CR = []\n            \n            for i in range(self.pop_size):\n                # Parameter Adaptation\n                F = np.random.choice(self.memory_F)\n                CR = np.random.choice(self.memory_CR)\n                \n                # Mutation Strategy Selection (best-guided with diversity influence)\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b = self.pop[np.random.choice(idxs, 2, replace=False)]\n                mutant = self.pop[i] + F * (self.x_opt - self.pop[i]) + F * (a - b) * learning_rate\n\n                # Crossover\n                trial = self.pop[i].copy()\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n\n                # Repair Mechanism: Clipping and Reflection\n                trial = np.clip(trial, self.bounds_lb, self.bounds_ub)\n                violated_mask = (trial < self.bounds_lb) | (trial > self.bounds_ub)\n                trial[violated_mask] = 2 * (self.bounds_lb + self.bounds_ub) / 2 - trial[violated_mask]\n                trial = np.clip(trial, self.bounds_lb, self.bounds_ub)\n\n                # Evaluate\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                # Selection and Archive Update\n                if f_trial < self.fitness[i]:\n                    self.success_F.append(F)\n                    self.success_CR.append(CR)\n\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        \n                    # Archive update (simplified, add F and CR used to archive)\n                    self.memory_F[self.memory_idx] = F\n                    self.memory_CR[self.memory_idx] = CR\n                    self.memory_idx = (self.memory_idx + 1) % self.archive_size\n\n            # Restart mechanism if diversity is too low\n            if self.budget % restart_cycle == 0 and diversity < self.diversity_threshold:\n                self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.pop])\n                self.budget -= self.pop_size # account for new evaluations\n                best_idx = np.argmin(self.fitness)\n                self.f_opt = self.fitness[best_idx]\n                self.x_opt = self.pop[best_idx]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm DiversityAdaptiveDE scored 0.293 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6eed164b-ba06-48b8-9d11-8986e9d57ff0"], "operator": null, "metadata": {"aucs": [0.11432767879923722, 0.19071156595800598, 0.27456176071116123, 0.6041974404924879, 0.39936268238301276, 0.44134823534514767, 0.2780077544841735, 0.3287036971937136, 0.2942876207308692, 0]}}
{"id": "5212389b-1f01-4e41-954a-165811dd60fe", "fitness": 0.6890941760579163, "name": "AdaptiveDERestart", "description": "Adaptive Differential Evolution with a restart mechanism based on stagnation detection and a dynamically adjusted population size.", "code": "import numpy as np\n\nclass AdaptiveDERestart:\n    def __init__(self, budget=10000, dim=10, pop_size_init=50, stagnation_threshold=500):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_init = pop_size_init\n        self.pop_size = pop_size_init\n        self.stagnation_threshold = stagnation_threshold\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.F = 0.5\n        self.CR = 0.9\n        self.stagnation_counter = 0\n        self.success_memory_F = []\n        self.success_memory_CR = []\n        self.min_pop_size = 10\n        self.max_pop_size = 100\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Adaptive F and CR\n                if self.success_memory_F and self.success_memory_CR:\n                    self.F = np.random.choice(self.success_memory_F)\n                    self.CR = np.random.choice(self.success_memory_CR)\n                else:\n                    self.F = 0.5  # Default if no successful parameters yet\n                    self.CR = 0.9\n\n                # Mutation (DE/rand/1)\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                a, b, c = self.pop[idxs]\n                mutant = a + self.F * (b - c)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Clip and Evaluate\n                trial = np.clip(trial, self.bounds_lb, self.bounds_ub)\n                f_trial = func(trial)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < self.fitness[i]:\n                    self.success_memory_F.append(self.F)\n                    self.success_memory_CR.append(self.CR)\n                    if len(self.success_memory_F) > 10:\n                        self.success_memory_F.pop(0)\n                        self.success_memory_CR.pop(0)\n\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.stagnation_counter = 0  # Reset stagnation counter\n                else:\n                    self.stagnation_counter += 1\n\n            # Stagnation Check and Restart/Population Size Adjustment\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.adjust_population_size()\n                self.initialize_population(func)\n                self.stagnation_counter = 0\n\n        return self.f_opt, self.x_opt\n\n    def initialize_population(self, func):\n         self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n         self.fitness = np.array([func(x) for x in self.pop])\n         self.budget -= self.pop_size\n\n         best_idx = np.argmin(self.fitness)\n         self.f_opt = self.fitness[best_idx]\n         self.x_opt = self.pop[best_idx]\n\n    def adjust_population_size(self):\n        # Dynamic population size adjustment\n        if self.stagnation_counter > self.stagnation_threshold:\n            self.pop_size = int(self.pop_size * 0.8)  # Decrease pop size upon stagnation\n            if self.pop_size < self.min_pop_size:\n                self.pop_size = self.pop_size_init\n        else:\n            self.pop_size = int(self.pop_size * 1.1)  # Increase if progressing\n            if self.pop_size > self.max_pop_size:\n                self.pop_size = self.max_pop_size\n\n        self.pop_size = max(self.min_pop_size, min(self.pop_size, self.max_pop_size)) #clip\n", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveDERestart scored 0.689 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["02cbc350-0a49-432d-9676-93d00e06ca39"], "operator": null, "metadata": {"aucs": [0.2736667804746372, 0.6390316028395712, 0.6737246900387279, 0.8582590866273865, 0.7302957837818912, 0.7722878413729384, 0.5255624711708737, 0.6536254294936759, 0.7188861800692234, 0.6420537176366, 0.8415171993671162, 0.9969307023969132, 0.6038660036575186, 0.7276852261164446, 0.9308833117242687, 0.7751197598544529, 0.6086559667316203, 0.8276848643069306, 0.4355459642582954, 0.5466009392392399]}}
{"id": "930e81d1-3903-455a-a684-47d598b59cf1", "fitness": 0.7720341629410196, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with simplified parameter adaptation and population control based on success rate.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.archive = []\n        self.archive_size = archive_size\n        self.memory_F = np.ones(archive_size) * 0.5\n        self.memory_CR = np.ones(archive_size) * 0.9\n        self.memory_idx = 0\n        self.success_F = []\n        self.success_CR = []\n        self.min_pop_size = 10\n        self.max_pop_size = 100\n\n\n    def __call__(self, func):\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.pop[best_idx]\n\n        while self.budget > 0:\n            \n            # Population Size Adjustment (simplified dynamic adjustment)\n            if len(self.success_F) > 0:\n                success_mean_F = np.mean(self.success_F)\n                if success_mean_F > 0.7:\n                    self.pop_size = max(self.min_pop_size, int(self.pop_size * 0.9))\n                elif success_mean_F < 0.3:\n                    self.pop_size = min(self.max_pop_size, int(self.pop_size * 1.1))\n                \n                self.pop_size = int(np.clip(self.pop_size, self.min_pop_size, self.max_pop_size))\n                if self.pop.shape[0] != self.pop_size:\n                  if self.pop.shape[0] > self.pop_size:\n                      self.pop = self.pop[:self.pop_size, :]\n                      self.fitness = self.fitness[:self.pop_size]\n                  else:\n                      num_new = self.pop_size - self.pop.shape[0]\n                      new_pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(num_new, self.dim))\n                      new_fitness = np.array([func(x) for x in new_pop])\n                      self.budget -= num_new\n                      self.pop = np.concatenate([self.pop, new_pop], axis=0)\n                      self.fitness = np.concatenate([self.fitness, new_fitness])\n                \n            self.success_F = []\n            self.success_CR = []\n\n            for i in range(self.pop_size):\n                # Parameter Adaptation\n                F = np.random.choice(self.memory_F)\n                CR = np.random.choice(self.memory_CR)\n                \n                # Mutation: best-guided\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b = self.pop[np.random.choice(idxs, 2, replace=False)]\n                mutant = self.pop[i] + F * (self.x_opt - self.pop[i]) + F * (a - b)\n\n                # Crossover\n                trial = self.pop[i].copy()\n                mask = np.random.rand(self.dim) < CR\n                trial[mask] = mutant[mask]\n                \n                # Repair Mechanism: Clipping\n                trial = np.clip(trial, self.bounds_lb, self.bounds_ub)\n\n                # Evaluate\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                # Selection and Archive Update\n                if f_trial < self.fitness[i]:\n                    self.success_F.append(F)\n                    self.success_CR.append(CR)\n\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        \n                    self.memory_F[self.memory_idx] = F\n                    self.memory_CR[self.memory_idx] = CR\n                    self.memory_idx = (self.memory_idx + 1) % self.archive_size\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveDE scored 0.772 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6eed164b-ba06-48b8-9d11-8986e9d57ff0"], "operator": null, "metadata": {"aucs": [0.4991822667474093, 0.8896055714146658, 0.7267194626218185, 0.26309972662407277, 0.9015604179906498, 0.9185897960405885, 0.8276065405420884, 0.8679651757388125, 0.8960825633982683, 0.8500707397216586, 0.9486319320540033, 0.9981763390032597, 0.8122194811083433, 0.8785771658528676, 0.9562647264335611, 0.9160488196003082, 0.4494800761392429, 0.9349288053414533, 0.36539774622854837, 0.5404759062187705]}}
{"id": "214b2652-a1a5-4eb7-a433-e862e88dfd2b", "fitness": 0.5851841334650036, "name": "AdaptiveDEWithJitter", "description": "Introducing jitter to enhance exploration, adjusting F and CR based on success within the current generation, and adding a random component to the mutation.", "code": "import numpy as np\n\nclass AdaptiveDEWithJitter:\n    def __init__(self, budget=10000, dim=10, pop_size=40):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.F = 0.5\n        self.CR = 0.9\n        self.memory_F = np.ones(5) * 0.5\n        self.memory_CR = np.ones(5) * 0.9\n        self.memory_index = 0\n        self.success_F = []\n        self.success_CR = []\n\n    def __call__(self, func):\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.pop[best_idx]\n\n        while self.budget > 0:\n            self.success_F = []\n            self.success_CR = []\n            for i in range(self.pop_size):\n                # Adaptation of F and CR using memory\n                F = np.random.choice(self.memory_F)\n                CR = np.random.choice(self.memory_CR)\n\n                # Mutation: current-to-best/1 with jitter and random component\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b = self.pop[np.random.choice(idxs, 2, replace=False)]\n                mutant = self.pop[i] + F * (self.x_opt - self.pop[i]) + F * (a - b) + 0.01 * np.random.normal(0, 1, self.dim) # Jitter\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Clip and Evaluate\n                trial = np.clip(trial, self.bounds_lb, self.bounds_ub)\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                # Selection\n                if f_trial < self.fitness[i]:\n                    # Update memory with success\n                    self.success_F.append(F)\n                    self.success_CR.append(CR)\n\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Update memory based on successful F and CR values from the generation\n            if self.success_F:\n                self.memory_F[self.memory_index] = np.mean(self.success_F)\n                self.memory_CR[self.memory_index] = np.mean(self.success_CR)\n                self.memory_index = (self.memory_index + 1) % len(self.memory_F)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveDEWithJitter scored 0.585 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["02cbc350-0a49-432d-9676-93d00e06ca39"], "operator": null, "metadata": {"aucs": [0.2920658241779609, 0.6346872174364357, 0.5476439648367017, 0.9508184842977039, 0.5687796670219138, 0.6374359441711437, 0.4382047501742299, 0.5004806834234479, 0.5635840432717134, 0.4750359897193065, 0.950023074723403, 0.9972728827029972, 0.3507340308209226, 0.3484352262180894, 0.5836817340442786, 0.6331178112004665, 0.49415024157037046, 0.8084483012940613, 0.4296398647687677, 0.499442933426156]}}
{"id": "8cfb19b2-038e-4208-a86c-cdf55e2fd07f", "fitness": 0.6581639304089253, "name": "ReinforcementLearningDE", "description": "Adaptive Differential Evolution with a reinforcement learning-based mutation selection and a novel stochastic ranking for archive maintenance.", "code": "import numpy as np\n\nclass ReinforcementLearningDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, archive_size=10, mutation_pool_size=3, learning_rate=0.1, exploration_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.mutation_pool_size = mutation_pool_size\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.archive = []\n        self.archive_fitness = []\n        self.learning_rate = learning_rate\n        self.exploration_rate = exploration_rate\n        self.epsilon = 1e-6\n\n        # Mutation strategies pool\n        self.mutation_strategies = [\n            self.mutation_current_to_best_1,\n            self.mutation_rand_1,\n            self.mutation_rand_2,\n        ]\n        self.q_values = np.zeros(self.mutation_pool_size)  # Q-values for each mutation strategy\n\n    def mutation_current_to_best_1(self, i):\n        F = 0.7 + 0.2 * np.random.rand()\n        idxs = [idx for idx in range(self.pop_size) if idx != i]\n        a, b = self.pop[np.random.choice(idxs, 2, replace=False)]\n        return self.pop[i] + F * (self.x_opt - self.pop[i]) + F * (a - b)\n\n    def mutation_rand_1(self, i):\n        F = 0.7 + 0.2 * np.random.rand()\n        idxs = [idx for idx in range(self.pop_size) if idx != i]\n        a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n        return a + F * (b - c)\n\n    def mutation_rand_2(self, i):\n        F = 0.7 + 0.2 * np.random.rand()\n        idxs = [idx for idx in range(self.pop_size) if idx != i]\n        a, b, c, d, e = self.pop[np.random.choice(idxs, 5, replace=False)]\n        return a + F * (b - c) + F * (d-e)\n\n    def select_mutation_strategy(self):\n        if np.random.rand() < self.exploration_rate:\n            # Explore: Choose a random action\n            return np.random.randint(self.mutation_pool_size)\n        else:\n            # Exploit: Choose the action with the highest Q-value\n            return np.argmax(self.q_values)\n\n    def update_q_values(self, strategy_idx, reward):\n        self.q_values[strategy_idx] += self.learning_rate * (reward - self.q_values[strategy_idx])\n\n    def update_archive(self, trial, f_trial):\n        if len(self.archive) < self.archive_size:\n            self.archive.append(trial)\n            self.archive_fitness.append(f_trial)\n        else:\n            # Stochastic ranking: Replace a random individual with probability proportional to fitness difference\n            idx = np.random.randint(self.archive_size)\n            if np.random.rand() < (f_trial - np.min(self.archive_fitness)) / (np.max(self.archive_fitness) - np.min(self.archive_fitness) + self.epsilon):\n                self.archive[idx] = trial\n                self.archive_fitness[idx] = f_trial\n            elif f_trial < np.min(self.archive_fitness):\n                idx = np.argmax(self.archive_fitness)\n                self.archive[idx] = trial\n                self.archive_fitness[idx] = f_trial\n\n    def __call__(self, func):\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n\n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.pop[best_idx]\n        self.archive = list(self.pop)  # Initialize Archive with initial population\n        self.archive_fitness = list(self.fitness)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Select mutation strategy based on Q-values (Reinforcement Learning)\n                mutation_strategy_idx = self.select_mutation_strategy()\n                mutant = self.mutation_strategies[mutation_strategy_idx](i)\n\n                # Crossover (Binomial)\n                CR = 0.5 + 0.4 * np.random.rand()\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Clip and Evaluate\n                trial = np.clip(trial, self.bounds_lb, self.bounds_ub)\n                f_trial = func(trial)\n                self.budget -= 1\n\n                # Selection\n                reward = 0  # Initialize reward\n                if f_trial < self.fitness[i]:\n                    reward = (self.fitness[i] - f_trial) # Reward proportional to improvement\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                    self.update_archive(trial, f_trial)  # Update archive\n\n                self.update_q_values(mutation_strategy_idx, reward)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm ReinforcementLearningDE scored 0.658 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["cdedff39-36c4-4958-9023-fee09d3d0656"], "operator": null, "metadata": {"aucs": [0.2128780173381305, 0.4485417705916309, 0.6868719473912802, 0.8360803773794031, 0.7498501277846201, 0.8014554550684772, 0.6515896993453343, 0.6620206190314618, 0.7446866590804795, 0.5694384782023968, 0.8049645698718282, 0.998102618785718, 0.3780279609552488, 0.6969467275608625, 0.9040424243637135, 0.8073126527606411, 0.636581951871649, 0.8438320955242062, 0.20965683650558065, 0.5203976187658454]}}
{"id": "dbc31101-909b-4610-a911-7694a082c276", "fitness": 0.5974777071928367, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with self-adaptive parameters, dynamic population sizing, a pool of mutation strategies, and a Cauchy mutation operator to escape local optima.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, CR_init=0.9, archive_size=10, ortho_group_size=3, mutation_pool_size=3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.CR = CR_init\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.archive = []\n        self.archive_size = archive_size\n        self.memory_F = np.ones(archive_size) * 0.5\n        self.memory_CR = np.ones(archive_size) * 0.9\n        self.memory_idx = 0\n        self.ortho_group_size = ortho_group_size  # Size of groups for orthogonal crossover\n        self.success_F = []\n        self.success_CR = []\n        self.min_pop_size = 10\n        self.max_pop_size = 100\n        self.mutation_pool_size = mutation_pool_size\n        self.mutation_strategies = ['current-to-best', 'rand/1', 'current-to-rand']\n\n\n    def __call__(self, func):\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.pop[best_idx]\n\n        while self.budget > 0:\n            \n            # Population Size Adjustment (simplified dynamic adjustment)\n            if len(self.success_F) > 0:\n                success_mean_F = np.mean(self.success_F)\n                if success_mean_F > 0.7:  # Empirically chosen threshold\n                    self.pop_size = max(self.min_pop_size, int(self.pop_size * 0.9))  # Reduce population if F is high\n                elif success_mean_F < 0.3:\n                    self.pop_size = min(self.max_pop_size, int(self.pop_size * 1.1)) # Increase pop if F is low\n                \n                self.pop_size = int(np.clip(self.pop_size, self.min_pop_size, self.max_pop_size)) # clip pop_size\n                if self.pop.shape[0] != self.pop_size:\n                  #resize the population to the new size\n                  if self.pop.shape[0] > self.pop_size:\n                      self.pop = self.pop[:self.pop_size, :]\n                      self.fitness = self.fitness[:self.pop_size]\n                  else:\n                      num_new = self.pop_size - self.pop.shape[0]\n                      new_pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(num_new, self.dim))\n                      new_fitness = np.array([func(x) for x in new_pop])\n                      self.budget -= num_new\n                      self.pop = np.concatenate([self.pop, new_pop], axis=0)\n                      self.fitness = np.concatenate([self.fitness, new_fitness])\n                \n            self.success_F = []\n            self.success_CR = []\n\n            for i in range(self.pop_size):\n                # Parameter Adaptation\n                F = np.random.choice(self.memory_F)\n                CR = np.random.choice(self.memory_CR)\n                \n                # Mutation Strategy Selection\n                mutation_strategy = np.random.choice(self.mutation_strategies)\n\n                if mutation_strategy == 'current-to-best':\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    a, b = self.pop[np.random.choice(idxs, 2, replace=False)]\n                    mutant = self.pop[i] + F * (self.x_opt - self.pop[i]) + F * (a - b)\n                elif mutation_strategy == 'rand/1':\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n                    mutant = self.pop[i] + F * (a - b) + F * (c - self.pop[i])\n                elif mutation_strategy == 'current-to-rand':\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    a = self.pop[np.random.choice(idxs, 1, replace=False)][0]\n                    mutant = self.pop[i] + np.random.rand() * (a - self.pop[i])  # Current-to-rand\n\n                # Cauchy Mutation (applied with a probability)\n                if np.random.rand() < 0.05:  # Small probability for exploration\n                    mutant += 0.01 * np.random.standard_cauchy(size=self.dim)  # Cauchy noise\n\n                # Orthogonal Crossover\n                group_indices = np.random.choice(self.dim, size=min(self.ortho_group_size, self.dim), replace=False)\n                trial = self.pop[i].copy()  # Start with the original individual\n                \n                crossover_mask = np.random.rand(len(group_indices)) < CR\n                trial[group_indices[crossover_mask]] = mutant[group_indices[crossover_mask]] # use mutant components\n\n                # Improved Repair Mechanism: Clipping and Reflection\n                trial = np.clip(trial, self.bounds_lb, self.bounds_ub)\n                violated_mask = (trial < self.bounds_lb) | (trial > self.bounds_ub)\n                trial[violated_mask] = 2 * (self.bounds_lb + self.bounds_ub) / 2 - trial[violated_mask]\n                trial = np.clip(trial, self.bounds_lb, self.bounds_ub)\n\n\n                # Evaluate\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                # Selection and Archive Update\n                if f_trial < self.fitness[i]:\n                    self.success_F.append(F)\n                    self.success_CR.append(CR)\n\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        \n                    # Archive update (simplified, add F and CR used to archive)\n                    self.memory_F[self.memory_idx] = F\n                    self.memory_CR[self.memory_idx] = CR\n                    self.memory_idx = (self.memory_idx + 1) % self.archive_size\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveDE scored 0.597 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6eed164b-ba06-48b8-9d11-8986e9d57ff0"], "operator": null, "metadata": {"aucs": [0.19990517837327548, 0.41431532070972843, 0.587753293596206, 0.8673238578363887, 0.7423590407675994, 0.8355283388091823, 0.3573774866290408, 0.5910030305921723, 0.7878939326907568, 0.19084121074560922, 0.8483186244132417, 0.9988669080823583, 0.28381090544580245, 0.5221833062346488, 0.7838764661683187, 0.7619701325949932, 0.5997635002561126, 0.8681947094688505, 0.2084159991733714, 0.4998529012690749]}}
{"id": "ec88825b-a9ba-4fe3-91bb-72779429ff41", "fitness": 0.7714195712799338, "name": "EnhancedAdaptiveDE", "description": "Enhanced Adaptive Differential Evolution with archive-based exploration, adaptive population size, and improved parameter adaptation using a weighted approach.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.F = 0.5\n        self.CR = 0.9\n        self.memory_F = np.ones(5) * 0.5\n        self.memory_CR = np.ones(5) * 0.9\n        self.memory_index = 0\n        self.archive = []\n        self.archive_fitness = []\n        self.success_F = []\n        self.success_CR = []\n\n    def __call__(self, func):\n        # Initialize population with bounds\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n\n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.pop[best_idx]\n\n        while self.budget > 0:\n            # Adaptive population size\n            if self.budget > 0.75 * 10000:\n                self.pop_size = 40\n            elif self.budget > 0.5 * 10000:\n                self.pop_size = 30\n            elif self.budget > 0.25 * 10000:\n                self.pop_size = 20\n            else:\n                self.pop_size = 10\n            \n            for i in range(self.pop_size):\n                # Adaptation of F and CR using weighted memory\n                weights = np.exp(np.linspace(0, 1, len(self.memory_F))[::-1])\n                weights /= weights.sum()  # Normalize to sum to 1\n\n                self.F = np.random.choice(self.memory_F, p=weights)\n                self.CR = np.random.choice(self.memory_CR, p=weights)\n\n                # Mutation: current-to-best/1 with archive\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b = self.pop[np.random.choice(idxs, 2, replace=False)]\n                if len(self.archive) > 0 and np.random.rand() < 0.1:  # Archive usage probability\n                    arc_idx = np.random.randint(len(self.archive))\n                    mutant = self.pop[i] + self.F * (self.x_opt - self.pop[i]) + self.F * (a - self.archive[arc_idx])\n                else:\n                    mutant = self.pop[i] + self.F * (self.x_opt - self.pop[i]) + self.F * (a - b)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Clip and Evaluate\n                trial = np.clip(trial, self.bounds_lb, self.bounds_ub)\n                f_trial = func(trial)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < self.fitness[i]:\n                    # Update memory with success\n                    self.success_F.append(self.F)\n                    self.success_CR.append(self.CR)\n\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.pop[i].copy())\n                    else:\n                        # Replace a random archive member\n                        replace_idx = np.random.randint(self.archive_size)\n                        self.archive[replace_idx] = self.pop[i].copy()\n\n            # Update memory after each generation\n            if self.success_F:\n                self.memory_F[self.memory_index] = np.mean(self.success_F)\n                self.memory_CR[self.memory_index] = np.mean(self.success_CR)\n                self.memory_index = (self.memory_index + 1) % len(self.memory_F)\n                self.success_F = []\n                self.success_CR = []\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm EnhancedAdaptiveDE scored 0.771 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["02cbc350-0a49-432d-9676-93d00e06ca39"], "operator": null, "metadata": {"aucs": [0.4796429019834554, 0.8857207254493976, 0.8940006907611957, 0.9500426225102045, 0.9078004295913078, 0.9199675148400326, 0.31143290633435683, 0.8695619863282664, 0.9024731314261638, 0.8320884014377299, 0.945136587245261, 0.9916623550479156, 0.8568582841921418, 0.8900030764114788, 0.9521033248716022, 0.3340857571390671, 0.8554714154724655, 0.9357393443691492, 0.1960871100407432, 0.5185128601467418]}}
{"id": "4be8a837-0b9e-4adb-939f-3dc5f27b617d", "fitness": 0.0, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with improved parameter adaptation using a weighted historical memory, combined mutation strategies with dynamic selection, and a more robust repair mechanism integrating random opposition.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, archive_size=10, p=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.archive = []\n        self.archive_size = archive_size\n        self.memory_F = np.ones(archive_size) * 0.5\n        self.memory_CR = np.ones(archive_size) * 0.9\n        self.memory_idx = 0\n        self.p = p  # Probability for random opposition-based learning\n\n    def __call__(self, func):\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.pop[best_idx]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Parameter Adaptation: Weighted historical memory\n                weights = np.exp(np.linspace(0, 1, self.archive_size))\n                weights /= weights.sum()\n\n                F = np.sum(self.memory_F * weights)\n                CR = np.sum(self.memory_CR * weights)\n                \n                F += 0.1 * np.random.randn() # adding small noise\n                CR += 0.02 * np.random.randn()\n                \n                F = np.clip(F, 0.1, 1.0)\n                CR = np.clip(CR, 0.1, 1.0)\n                \n                # Mutation Strategy Selection: Dynamic probability\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n                \n                if np.random.rand() < 0.5:\n                    mutant = self.pop[i] + F * (self.x_opt - self.pop[i]) + F * (a - b) # best/1\n                else:\n                    mutant = a + F * (b - c) # rand/1\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Repair Mechanism: Clipping, Reflection and Random Opposition\n                trial = np.clip(trial, self.bounds_lb, self.bounds_ub)\n                \n                # Random opposition-based learning\n                if np.random.rand() < self.p:\n                    trial_opp = self.bounds_lb + self.bounds_ub - trial\n                    trial_opp = np.clip(trial_opp, self.bounds_lb, self.bounds_ub)\n                    f_trial_opp = func(trial_opp)\n                    self.budget -= 1\n                    if f_trial_opp < func(trial):\n                      trial = trial_opp\n                      \n                # Evaluate\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                # Selection and Memory update\n\n                delta = self.fitness[i] - f_trial\n                if delta > 0:\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                    self.memory_F[self.memory_idx] = F\n                    self.memory_CR[self.memory_idx] = CR\n                    self.memory_idx = (self.memory_idx + 1) % self.archive_size\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["fc30bdef-184b-4d2d-9553-e8a6973371ba"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "ec752179-a2bf-4ca4-a253-d37960395ef1", "fitness": -Infinity, "name": "AdaptiveDEOD", "description": "Adaptive Differential Evolution with orthogonal design and improved archive, using a combined archive and population for mutation, and an adaptive F/CR scheme based on success.", "code": "import numpy as np\n\nclass AdaptiveDEOD:\n    def __init__(self, budget=10000, dim=10, pop_size=40, archive_size=20, mutation_pool_size=3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.mutation_pool_size = mutation_pool_size\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.archive = []\n        self.archive_fitness = []\n        self.F = 0.5\n        self.CR = 0.9\n        self.success_mutation_counts = np.zeros(self.mutation_pool_size)\n        self.mutation_selection_counts = np.zeros(self.mutation_pool_size)\n        self.epsilon = 1e-6\n        self.success_F = np.zeros(self.mutation_pool_size)\n        self.success_CR = np.zeros(self.mutation_pool_size)\n        self.success_count = np.zeros(self.mutation_pool_size)\n\n        # Mutation strategies pool\n        self.mutation_strategies = [\n            self.mutation_current_to_best_1,\n            self.mutation_rand_1,\n            self.mutation_rand_2,\n        ]\n        self.mutation_weights = np.ones(self.mutation_pool_size) / self.mutation_pool_size\n\n        # Orthogonal Design for F and CR tuning (Removed)\n        self.memory_size = 10\n        self.F_memory = np.ones(self.memory_size) * 0.5\n        self.CR_memory = np.ones(self.memory_size) * 0.9\n        self.memory_index = 0\n\n    def mutation_current_to_best_1(self, i):\n        idxs = [idx for idx in range(self.pop_size) if idx != i]\n        a, b = self.pop[np.random.choice(idxs, 2, replace=False)]\n        return self.pop[i] + self.F * (self.x_opt - self.pop[i]) + self.F * (a - b)\n\n    def mutation_rand_1(self, i):\n        # Use combined population and archive for mutation\n        combined_pop = np.concatenate((self.pop, self.archive))\n        num_combined = combined_pop.shape[0]\n        idxs = [idx for idx in range(num_combined) if not np.all(combined_pop[idx] == self.pop[i])] # Ensure idx is not i\n        if len(idxs) < 2:\n            return self.pop[i] # If not enough distinct vectors, return original vector\n        a_idx, b_idx = np.random.choice(idxs, 2, replace=False)\n        a, b = combined_pop[a_idx], combined_pop[b_idx]\n        return self.pop[i] + self.F * (a - b)\n\n    def mutation_rand_2(self, i):\n        # Use combined population and archive for mutation\n        combined_pop = np.concatenate((self.pop, self.archive))\n        num_combined = combined_pop.shape[0]\n        idxs = [idx for idx in range(num_combined) if not np.all(combined_pop[idx] == self.pop[i])]  # Ensure idx is not i\n        if len(idxs) < 4:\n            return self.pop[i] # If not enough distinct vectors, return original vector\n        a_idx, b_idx, c_idx, d_idx = np.random.choice(idxs, 4, replace=False)\n        a, b, c, d = combined_pop[a_idx], combined_pop[b_idx], combined_pop[c_idx], combined_pop[d_idx]\n        return a + self.F * (b - c) + self.F * (d - self.pop[i])\n\n    def update_mutation_weights(self):\n        # Update mutation weights based on success rate and F/CR values\n        success_rates = (self.success_mutation_counts + self.epsilon) / (self.mutation_selection_counts + self.epsilon)\n        self.mutation_weights = success_rates / np.sum(success_rates)\n\n        for i in range(len(self.mutation_strategies)):\n             if self.success_count[i] > 0:\n                 self.F_memory[self.memory_index] = np.mean(self.success_F[i] / self.success_count[i])\n                 self.CR_memory[self.memory_index] = np.mean(self.success_CR[i] / self.success_count[i])\n\n        self.F = np.clip(np.random.choice(self.F_memory), 0.1, 1.0)\n        self.CR = np.clip(np.random.choice(self.CR_memory), 0.1, 1.0)\n        self.memory_index = (self.memory_index + 1) % self.memory_size\n\n        self.success_mutation_counts[:] = 0\n        self.mutation_selection_counts[:] = 0\n        self.success_F[:] = 0\n        self.success_CR[:] = 0\n        self.success_count[:] = 0\n\n\n    def update_archive(self, trial, f_trial):\n        if len(self.archive) < self.archive_size:\n            self.archive.append(trial)\n            self.archive_fitness.append(f_trial)\n        else:\n            # Replace the worst individual in the archive *only* if the new individual is better\n            worst_arch_idx = np.argmax(self.archive_fitness)\n            if f_trial < self.archive_fitness[worst_arch_idx]:\n                self.archive[worst_arch_idx] = trial\n                self.archive_fitness[worst_arch_idx] = f_trial\n\n    def __call__(self, func):\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.pop[best_idx]\n        self.archive = list(self.pop) # Initialize Archive with initial population\n        self.archive_fitness = list(self.fitness)\n\n        while self.budget > 0:\n\n            for i in range(self.pop_size):\n                # Select mutation strategy based on weights\n                mutation_strategy_idx = np.random.choice(len(self.mutation_strategies), p=self.mutation_weights)\n                self.mutation_selection_counts[mutation_strategy_idx] += 1\n                mutant = self.mutation_strategies[mutation_strategy_idx](i)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Clip and Evaluate\n                trial = np.clip(trial, self.bounds_lb, self.bounds_ub)\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                # Selection\n                if f_trial < self.fitness[i]:\n                    self.success_mutation_counts[mutation_strategy_idx] += 1\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                    self.update_archive(trial, f_trial)  # Update archive on improvement\n\n                    self.success_F[mutation_strategy_idx] += self.F\n                    self.success_CR[mutation_strategy_idx] += self.CR\n                    self.success_count[mutation_strategy_idx] += 1\n\n\n            if self.budget > 0:\n                self.update_mutation_weights() # Update weights every generation\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "An exception occurred: Evaluation timed out after 60 seconds..", "error": "", "parent_ids": ["cdedff39-36c4-4958-9023-fee09d3d0656"], "operator": null, "metadata": {}}
{"id": "3ce44ee7-6b03-485f-a1af-7b9f45ae66d1", "fitness": 0.6841140694463438, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with simplified parameter adaptation, best-guided mutation, and clipping-based repair for efficient exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.archive = []\n        self.archive_size = archive_size\n        self.memory_F = np.ones(archive_size) * 0.5\n        self.memory_CR = np.ones(archive_size) * 0.9\n        self.memory_idx = 0\n        self.success_F = []\n        self.success_CR = []\n        self.min_pop_size = 10\n        self.max_pop_size = 100\n\n\n    def __call__(self, func):\n        self.pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.pop[best_idx]\n\n        while self.budget > 0:\n            \n            # Simplified Population Size Adjustment\n            if len(self.success_F) > 0:\n                mean_F = np.mean(self.success_F)\n                self.pop_size = int(np.clip(self.pop_size * (1 + 0.2 * (mean_F - 0.5)), self.min_pop_size, self.max_pop_size))\n                if self.pop.shape[0] != self.pop_size:\n                    if self.pop.shape[0] > self.pop_size:\n                        self.pop = self.pop[:self.pop_size, :]\n                        self.fitness = self.fitness[:self.pop_size]\n                    else:\n                        num_new = self.pop_size - self.pop.shape[0]\n                        new_pop = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(num_new, self.dim))\n                        new_fitness = np.array([func(x) for x in new_pop])\n                        self.budget -= num_new\n                        self.pop = np.concatenate([self.pop, new_pop], axis=0)\n                        self.fitness = np.concatenate([self.fitness, new_fitness])\n\n            self.success_F = []\n            self.success_CR = []\n\n            for i in range(self.pop_size):\n                # Simplified Parameter Adaptation\n                F = np.random.choice(self.memory_F)\n                CR = np.random.choice(self.memory_CR)\n                \n                # Mutation: best-guided\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b = self.pop[np.random.choice(idxs, 2, replace=False)]\n                mutant = self.pop[i] + F * (self.x_opt - self.pop[i]) + F * (a - b)\n\n                # Crossover\n                mask = np.random.rand(self.dim) < CR\n                trial = np.where(mask, mutant, self.pop[i])\n\n                # Clipping-based Repair\n                trial = np.clip(trial, self.bounds_lb, self.bounds_ub)\n\n                # Evaluate\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                # Selection\n                if f_trial < self.fitness[i]:\n                    self.success_F.append(F)\n                    self.success_CR.append(CR)\n\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                    self.memory_F[self.memory_idx] = F\n                    self.memory_CR[self.memory_idx] = CR\n                    self.memory_idx = (self.memory_idx + 1) % self.archive_size\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveDE scored 0.684 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6eed164b-ba06-48b8-9d11-8986e9d57ff0"], "operator": null, "metadata": {"aucs": [0.27878110384605803, 0.3700700409619826, 0.7423542773168, 0.19825453085338873, 0.9015762444152898, 0.9206315495881517, 0.33200751954211805, 0.8715669688746673, 0.9087197323877678, 0.4391730912560806, 0.9549933242930531, 0.9963410029442528, 0.8288867932432552, 0.8696256744244025, 0.5832306562669126, 0.5518040224626355, 0.7998118022299914, 0.936083853704793, 0.6795655862232757, 0.5188036140920025]}}
