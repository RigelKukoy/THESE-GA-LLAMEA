{"id": "7f8b7b90-8e6f-4ccc-b647-53589b0dea0e", "fitness": -Infinity, "name": "VelocityClampedPSO", "description": "Population-based algorithm with velocity updates influenced by the best-performing particle and a randomly selected particle.", "code": "import numpy as np\n\nclass VelocityClampedPSO:\n    def __init__(self, budget=10000, dim=10, pop_size=20, c1=1.49, c2=1.49, v_max_factor=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.c1 = c1  # Cognitive coefficient\n        self.c2 = c2  # Social coefficient\n        self.v_max_factor = v_max_factor  # Max velocity as fraction of search space\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        # Initialize population and velocities\n        population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        velocities = np.random.uniform(-abs(ub - lb) * self.v_max_factor, abs(ub - lb) * self.v_max_factor, size=(self.pop_size, self.dim))\n        \n        # Initialize personal best positions and fitnesses\n        pbest_positions = population.copy()\n        pbest_fitnesses = np.array([func(x) for x in population])\n        \n        # Initialize global best position and fitness\n        global_best_index = np.argmin(pbest_fitnesses)\n        global_best_position = pbest_positions[global_best_index].copy()\n        global_best_fitness = pbest_fitnesses[global_best_index]\n        \n        eval_count = self.pop_size #initial evalutations\n        \n        # Iterate until budget is exhausted\n        while eval_count < self.budget:\n            # Update velocities and positions\n            r1 = np.random.rand(self.pop_size, self.dim)\n            r2 = np.random.rand(self.pop_size, self.dim)\n            \n            velocities = velocities + self.c1 * r1 * (pbest_positions - population) + self.c2 * r2 * (global_best_position - population)\n\n            # Clamp velocities\n            v_max = abs(ub - lb) * self.v_max_factor\n            velocities = np.clip(velocities, -v_max, v_max)\n            \n            population = population + velocities\n            \n            # Reflect particles that hit boundaries\n            for i in range(self.pop_size):\n                for j in range(self.dim):\n                    if population[i, j] < lb:\n                        population[i, j] = lb + (lb - population[i, j])\n                        velocities[i,j] = -velocities[i,j]\n                    elif population[i, j] > ub:\n                        population[i, j] = ub - (population[i, j] - ub)\n                        velocities[i,j] = -velocities[i,j]\n\n            # Evaluate new positions\n            fitnesses = np.array([func(x) for x in population])\n            eval_count += self.pop_size\n\n            # Update personal best positions and fitnesses\n            for i in range(self.pop_size):\n                if fitnesses[i] < pbest_fitnesses[i]:\n                    pbest_fitnesses[i] = fitnesses[i]\n                    pbest_positions[i] = population[i].copy()\n                    \n                    # Update global best position and fitness\n                    if fitnesses[i] < global_best_fitness:\n                        global_best_fitness = fitnesses[i]\n                        global_best_position = population[i].copy()\n                        \n        return global_best_fitness, global_best_position", "configspace": "", "generation": 0, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 48, in __call__\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n.", "error": "", "parent_ids": [], "operator": null, "metadata": {}}
{"id": "538ec5b8-356e-4c69-bf41-ffd15353ca2b", "fitness": -Infinity, "name": "CMAES", "description": "Covariance matrix adaptation evolution strategy with orthogonal sampling and adaptive step size control.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damp=None, c_cov=None, mu_factor=0.25):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = int(self.popsize * mu_factor)\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = cs\n        self.damp = damp if damp is not None else 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.c_cov = c_cov if c_cov is not None else (1 / (self.mueff * self.dim**2)) * (self.mueff - 1 + 2 / self.mueff)\n        self.chiN = self.dim**0.5 * (1 - 1 / (4 * self.dim) + 1 / (21 * self.dim**2))\n\n    def __call__(self, func):\n        mean = np.zeros(self.dim)\n        sigma = 0.5\n        C = np.eye(self.dim)\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        f_opt = np.Inf\n        x_opt = None\n        used_budget = 0\n\n        while used_budget < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            x = mean[:, np.newaxis] + sigma * np.dot(np.linalg.cholesky(C), z)\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            used_budget += self.popsize\n            \n            if np.min(f) < f_opt:\n                f_opt = np.min(f)\n                x_opt = x[:, np.argmin(f)].copy()\n            \n            idx = np.argsort(f)\n            x_mu = x[:, idx[:self.mu]]\n            z_mu = z[:, idx[:self.mu]]\n            mean_new = np.dot(x_mu, self.weights)\n\n            ps = (1 - self.cs) * ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.dot(np.linalg.inv(np.linalg.cholesky(C)), (mean_new - mean) / sigma)\n            \n            if np.linalg.norm(ps) / np.sqrt(1 - (1 - self.cs)**(2 * used_budget / self.popsize)) < (1.4 + 2 / (self.dim + 1)) * self.chiN:\n                hsig = 1\n            else:\n                hsig = 0\n\n            mean = mean_new\n\n            sigma *= np.exp((self.cs / self.damp) * (np.linalg.norm(ps) / self.chiN - 1))\n\n            C = (1 - self.c_cov) * C + self.c_cov * (1 / self.mueff) * (np.outer(ps, ps) + (1 - hsig) * self.c_cov * (2 - self.c_cov) * C)\n            C += self.c_cov * np.dot(z_mu * self.weights, z_mu.T)\n\n            C = np.triu(C) + np.triu(C, 1).T\n            C = (C + C.T) / 2\n            \n            try:\n                np.linalg.cholesky(C)\n            except np.linalg.LinAlgError:\n                C = np.eye(self.dim)  # Reset covariance matrix if it's not positive definite\n        \n        return f_opt, x_opt", "configspace": "", "generation": 0, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 30, in __call__\n  File \"<__array_function__ internals>\", line 200, in clip\n  File \"/home/neocortex/.cache/pypoetry/virtualenvs/blade-xXF9vABH-py3.10/lib/python3.10/site-packages/numpy/core/fromnumeric.py\", line 2180, in clip\n    return _wrapfunc(a, 'clip', a_min, a_max, out=out, **kwargs)\n  File \"/home/neocortex/.cache/pypoetry/virtualenvs/blade-xXF9vABH-py3.10/lib/python3.10/site-packages/numpy/core/fromnumeric.py\", line 57, in _wrapfunc\n    return bound(*args, **kwds)\n  File \"/home/neocortex/.cache/pypoetry/virtualenvs/blade-xXF9vABH-py3.10/lib/python3.10/site-packages/numpy/core/_methods.py\", line 161, in _clip\n    return _clip_dep_invoke_with_casting(\n  File \"/home/neocortex/.cache/pypoetry/virtualenvs/blade-xXF9vABH-py3.10/lib/python3.10/site-packages/numpy/core/_methods.py\", line 115, in _clip_dep_invoke_with_casting\n    return ufunc(*args, out=out, **kwargs)\nValueError: operands could not be broadcast together with shapes (2,6) (2,) (2,) \n.", "error": "", "parent_ids": [], "operator": null, "metadata": {}}
{"id": "de3d9a52-7808-4788-bcf1-aad5e5398501", "fitness": 0.32840748480053333, "name": "AdaptiveGaussianSearch", "description": "A population-based algorithm with Gaussian mutation and a shrinking search space to converge towards the optimum.", "code": "import numpy as np\n\nclass AdaptiveGaussianSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_std=1.0, shrink_factor=0.99):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.initial_std = initial_std\n        self.shrink_factor = shrink_factor\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        \n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.eval_count += self.pop_size\n        \n        # Find best initial solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n            \n        std = self.initial_std\n        \n        while self.eval_count < self.budget:\n            # Generate offspring using Gaussian mutation\n            offspring = population + np.random.normal(0, std, size=(self.pop_size, self.dim))\n            \n            # Clip offspring to stay within bounds\n            offspring = np.clip(offspring, self.lb, self.ub)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(x) for x in offspring])\n            self.eval_count += self.pop_size\n            \n            # Update best solution\n            best_offspring_index = np.argmin(offspring_fitness)\n            if offspring_fitness[best_offspring_index] < self.f_opt:\n                self.f_opt = offspring_fitness[best_offspring_index]\n                self.x_opt = offspring[best_offspring_index]\n            \n            # Select survivors (replace the worst individuals with the best offspring)\n            worst_index = np.argmax(fitness)\n            best_offspring_index = np.argmin(offspring_fitness)\n            if offspring_fitness[best_offspring_index] < fitness[worst_index]:\n                population[worst_index] = offspring[best_offspring_index]\n                fitness[worst_index] = offspring_fitness[best_offspring_index]\n            \n            # Shrink the search space (reduce standard deviation)\n            std *= self.shrink_factor\n            \n            # Adjust bounds (shrink towards the best solution)\n            self.lb = np.maximum(self.lb, self.x_opt - 2.5 * std)\n            self.ub = np.minimum(self.ub, self.x_opt + 2.5 * std)\n            \n            population = np.clip(population, self.lb, self.ub)\n            \n            if self.eval_count >= self.budget:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveGaussianSearch scored 0.328 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1777524876169868, 0.2228169314335624, 0.2961382176053995, 0.18761810622667185, 0.22961958770128976, 0.3316015434277235, 0.29275083001268554, 0.2649410190168052, 0.21126184428053407, 0.23135936754594444, 0.18449339798693365, 0.9990201925155547, 0.2558206091341384, 0.23433370936468245, 0.7173373013440492, 0.27267948589912616, 0.20935267133615143, 0.5644925889088603, 0.23166557321436598, 0.45309423143920136]}}
{"id": "ed7ab72d-d902-4a89-867a-57f1daf2d604", "fitness": 0.6043658940976911, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with population restart and archive.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, archive_size=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.archive_size = archive_size\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.archive = []\n        self.archive_fitness = []\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = x1 + self.F * (x2 - x3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n\n                    # Add replaced vector to archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                        #Replace oldest in archive\n                        self.archive[generation % self.archive_size] = population[i]\n                        self.archive_fitness[generation % self.archive_size] = fitness[i]\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        \n                else:\n                    # Add trial vector to archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        #Replace oldest in archive\n                        self.archive[generation % self.archive_size] = trial\n                        self.archive_fitness[generation % self.archive_size] = f_trial\n                        \n\n            population = new_population\n            fitness = new_fitness\n\n            # Restart population if stagnating\n            if generation % 50 == 0:\n                if np.std(fitness) < 1e-6:  # Stagnation criterion\n                    population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                    fitness = np.array([func(x) for x in population])\n                    self.budget -= self.pop_size\n                    best_index = np.argmin(fitness)\n                    if fitness[best_index] < self.f_opt:\n                        self.f_opt = fitness[best_index]\n                        self.x_opt = population[best_index]\n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDE scored 0.604 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.20454599792942052, 0.43801651206690184, 0.5982450436721172, 0.8221922932884129, 0.6982564312273354, 0.6658380483977131, 0.5162144177479973, 0.5492264493979272, 0.6263940305641182, 0.5964948414720102, 0.7843649792472173, 0.9996780097898138, 0.3501823561913091, 0.597173775727848, 0.8744403900798576, 0.7245819564910896, 0.5066394542471195, 0.7771698933466715, 0.23667938498444363, 0.5209836160844991]}}
{"id": "c12f7d19-d981-4b2b-85b0-bb6338ad80c6", "fitness": -Infinity, "name": "CMAES", "description": "Improved CMA-ES with corrected broadcasting issues and covariance matrix adaptation.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damp=None, c_cov=None, mu_factor=0.25):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = int(self.popsize * mu_factor)\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = cs\n        self.damp = damp if damp is not None else 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.c_cov = c_cov if c_cov is not None else (1 / (self.mueff * self.dim**2)) * (self.mueff - 1 + 2 / self.mueff)\n        self.chiN = self.dim**0.5 * (1 - 1 / (4 * self.dim) + 1 / (21 * self.dim**2))\n\n    def __call__(self, func):\n        mean = np.zeros(self.dim)\n        sigma = 0.5\n        C = np.eye(self.dim)\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        f_opt = np.Inf\n        x_opt = None\n        used_budget = 0\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n\n        while used_budget < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            A = np.linalg.cholesky(C)\n            x = mean[:, np.newaxis] + sigma * np.dot(A, z)\n            \n            # Clip each dimension of each individual\n            x = np.clip(x, lb, ub)\n\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            used_budget += self.popsize\n            \n            if np.min(f) < f_opt:\n                f_opt = np.min(f)\n                x_opt = x[:, np.argmin(f)].copy()\n            \n            idx = np.argsort(f)\n            x_mu = x[:, idx[:self.mu]]\n            z_mu = z[:, idx[:self.mu]]\n            mean_new = np.dot(x_mu, self.weights)\n\n            ps = (1 - self.cs) * ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.dot(np.linalg.inv(A), (mean_new - mean) / sigma)\n            \n            if np.linalg.norm(ps) / np.sqrt(1 - (1 - self.cs)**(2 * used_budget / self.popsize)) < (1.4 + 2 / (self.dim + 1)) * self.chiN:\n                hsig = 1\n            else:\n                hsig = 0\n\n            mean = mean_new\n\n            sigma *= np.exp((self.cs / self.damp) * (np.linalg.norm(ps) / self.chiN - 1))\n\n            C = (1 - self.c_cov) * C + self.c_cov * (1 / self.mueff) * (np.outer(ps, ps) + (1 - hsig) * self.c_cov * (2 - self.c_cov) * C)\n            C += self.c_cov * np.dot(z_mu * self.weights, z_mu.T)\n\n            C = np.triu(C) + np.triu(C, 1).T\n            C = (C + C.T) / 2\n            \n            try:\n                np.linalg.cholesky(C)\n            except np.linalg.LinAlgError:\n                C = np.eye(self.dim)  # Reset covariance matrix if it's not positive definite\n        \n        return f_opt, x_opt", "configspace": "", "generation": 1, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 35, in __call__\n  File \"<__array_function__ internals>\", line 200, in clip\n  File \"/home/neocortex/.cache/pypoetry/virtualenvs/blade-xXF9vABH-py3.10/lib/python3.10/site-packages/numpy/core/fromnumeric.py\", line 2180, in clip\n    return _wrapfunc(a, 'clip', a_min, a_max, out=out, **kwargs)\n  File \"/home/neocortex/.cache/pypoetry/virtualenvs/blade-xXF9vABH-py3.10/lib/python3.10/site-packages/numpy/core/fromnumeric.py\", line 57, in _wrapfunc\n    return bound(*args, **kwds)\n  File \"/home/neocortex/.cache/pypoetry/virtualenvs/blade-xXF9vABH-py3.10/lib/python3.10/site-packages/numpy/core/_methods.py\", line 161, in _clip\n    return _clip_dep_invoke_with_casting(\n  File \"/home/neocortex/.cache/pypoetry/virtualenvs/blade-xXF9vABH-py3.10/lib/python3.10/site-packages/numpy/core/_methods.py\", line 115, in _clip_dep_invoke_with_casting\n    return ufunc(*args, out=out, **kwargs)\nValueError: operands could not be broadcast together with shapes (2,6) (2,) (2,) \n.", "error": "", "parent_ids": ["538ec5b8-356e-4c69-bf41-ffd15353ca2b"], "operator": null, "metadata": {}}
{"id": "0ec7cceb-b768-4c77-a2c8-950aff584e36", "fitness": -Infinity, "name": "CMAES", "description": "Corrects CMA-ES implementation to properly handle bounds and covariance matrix updates, and adds a try-except block for Cholesky decomposition.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damp=None, c_cov=None, mu_factor=0.25):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = int(self.popsize * mu_factor)\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = cs\n        self.damp = damp if damp is not None else 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.c_cov = c_cov if c_cov is not None else (1 / (self.mueff * (self.dim + 1.3)**2)) * (self.mueff - 1 + 2 / self.mueff)\n        self.chiN = self.dim**0.5 * (1 - 1 / (4 * self.dim) + 1 / (21 * self.dim**2))\n\n    def __call__(self, func):\n        mean = np.zeros(self.dim)\n        sigma = 0.5\n        C = np.eye(self.dim)\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        f_opt = np.Inf\n        x_opt = None\n        used_budget = 0\n\n        while used_budget < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            A = np.linalg.cholesky(C)\n            x = mean[:, np.newaxis] + sigma * np.dot(A, z)\n            \n            lb = func.bounds.lb\n            ub = func.bounds.ub\n            x = np.clip(x, lb, ub)\n\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            used_budget += self.popsize\n            \n            if np.min(f) < f_opt:\n                f_opt = np.min(f)\n                x_opt = x[:, np.argmin(f)].copy()\n            \n            idx = np.argsort(f)\n            x_mu = x[:, idx[:self.mu]]\n            z_mu = z[:, idx[:self.mu]]\n            mean_new = np.dot(x_mu, self.weights)\n\n            ps = (1 - self.cs) * ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * (mean_new - mean) / sigma\n            pc = (1 - self.c_cov) * pc + np.sqrt(self.c_cov * (2 - self.c_cov) * self.mueff) * np.dot(A, np.dot(z_mu, self.weights))\n\n            mean = mean_new\n\n            sigma *= np.exp((self.cs / self.damp) * (np.linalg.norm(ps) / self.chiN - 1))\n\n            C = (1 - self.c_cov) * C + self.c_cov * np.outer(pc, pc) + self.c_cov * (1 / self.mueff) * np.dot(z_mu * self.weights, z_mu.T)\n            C = np.triu(C) + np.triu(C, 1).T\n            C = (C + C.T) / 2\n            \n            try:\n                np.linalg.cholesky(C)\n            except np.linalg.LinAlgError:\n                C = np.eye(self.dim)  # Reset covariance matrix if it's not positive definite\n        \n        return f_opt, x_opt", "configspace": "", "generation": 1, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 34, in __call__\n  File \"<__array_function__ internals>\", line 200, in clip\n  File \"/home/neocortex/.cache/pypoetry/virtualenvs/blade-xXF9vABH-py3.10/lib/python3.10/site-packages/numpy/core/fromnumeric.py\", line 2180, in clip\n    return _wrapfunc(a, 'clip', a_min, a_max, out=out, **kwargs)\n  File \"/home/neocortex/.cache/pypoetry/virtualenvs/blade-xXF9vABH-py3.10/lib/python3.10/site-packages/numpy/core/fromnumeric.py\", line 57, in _wrapfunc\n    return bound(*args, **kwds)\n  File \"/home/neocortex/.cache/pypoetry/virtualenvs/blade-xXF9vABH-py3.10/lib/python3.10/site-packages/numpy/core/_methods.py\", line 161, in _clip\n    return _clip_dep_invoke_with_casting(\n  File \"/home/neocortex/.cache/pypoetry/virtualenvs/blade-xXF9vABH-py3.10/lib/python3.10/site-packages/numpy/core/_methods.py\", line 115, in _clip_dep_invoke_with_casting\n    return ufunc(*args, out=out, **kwargs)\nValueError: operands could not be broadcast together with shapes (2,6) (2,) (2,) \n.", "error": "", "parent_ids": ["538ec5b8-356e-4c69-bf41-ffd15353ca2b"], "operator": null, "metadata": {}}
{"id": "bf4470c5-9522-4179-8d53-e107a00ab355", "fitness": -Infinity, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with archive, adaptive F and CR parameters, and a local search to refine solutions.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=100, F_init=0.5, CR_init=0.7, local_search_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F_init = F_init\n        self.CR_init = CR_init\n        self.local_search_prob = local_search_prob\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.archive = []\n        self.archive_fitness = []\n        self.F = np.full(self.pop_size, self.F_init)\n        self.CR = np.full(self.pop_size, self.CR_init)\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                self.F[i] = np.clip(np.random.normal(self.F_init, 0.1), 0.1, 1.0)\n                self.CR[i] = np.clip(np.random.normal(self.CR_init, 0.1), 0.1, 1.0)\n\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = x1 + self.F[i] * (x2 - x3)\n\n                # Use archive\n                if np.random.rand() < 0.1 and len(self.archive) > 0:\n                    x4 = self.archive[np.random.randint(len(self.archive))]\n                    mutant = x1 + self.F[i] * (x2 - x4)\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR[i] or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Local Search\n                if np.random.rand() < self.local_search_prob:\n                    trial_copy = np.copy(trial)\n                    step_size = 0.01 * (func.bounds.ub - func.bounds.lb)\n                    for j in range(self.dim):\n                        trial_copy[j] = np.clip(trial[j] + np.random.uniform(-step_size, step_size), func.bounds.lb, func.bounds.ub)\n                    f_trial = func(trial_copy)\n                    self.budget -= 1\n                    if f_trial < fitness[i]:\n                        trial = trial_copy\n                    else:\n                         f_trial = func(trial)\n                         self.budget -= 1\n                else:\n                   f_trial = func(trial)\n                   self.budget -= 1\n               \n                # Selection\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n\n                    # Add replaced vector to archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                        #Replace oldest in archive\n                        self.archive[generation % self.archive_size] = population[i]\n                        self.archive_fitness[generation % self.archive_size] = fitness[i]\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        \n                else:\n                    # Add trial vector to archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        #Replace oldest in archive\n                        self.archive[generation % self.archive_size] = trial\n                        self.archive_fitness[generation % self.archive_size] = f_trial\n                        \n\n            population = new_population\n            fitness = new_fitness\n\n            # Restart population if stagnating\n            if generation % 50 == 0:\n                if np.std(fitness) < 1e-6:  # Stagnation criterion\n                    population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                    fitness = np.array([func(x) for x in population])\n                    self.budget -= self.pop_size\n                    best_index = np.argmin(fitness)\n                    if fitness[best_index] < self.f_opt:\n                        self.f_opt = fitness[best_index]\n                        self.x_opt = population[best_index]\n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "An exception occured: TypeError: only size-1 arrays can be converted to Python scalars\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 67, in __call__\nValueError: setting an array element with a sequence.\n.", "error": "", "parent_ids": ["ed7ab72d-d902-4a89-867a-57f1daf2d604"], "operator": null, "metadata": {}}
{"id": "f6ab5867-f49f-4ded-a32e-a9d681a45728", "fitness": 0.35702007516185424, "name": "AdaptiveGaussianSearch", "description": "Adaptive Gaussian Search with momentum-based population update and dynamic standard deviation adjustment.", "code": "import numpy as np\n\nclass AdaptiveGaussianSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_std=1.0, shrink_factor=0.99, momentum=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.initial_std = initial_std\n        self.shrink_factor = shrink_factor\n        self.momentum = momentum\n        self.lb = -5.0\n        self.ub = 5.0\n        self.velocity = np.zeros((pop_size, dim))\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        \n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.eval_count += self.pop_size\n        \n        # Find best initial solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n            \n        std = self.initial_std\n        \n        while self.eval_count < self.budget:\n            # Generate offspring using Gaussian mutation and momentum\n            mutation = np.random.normal(0, std, size=(self.pop_size, self.dim))\n            self.velocity = self.momentum * self.velocity + (1 - self.momentum) * mutation\n            offspring = population + self.velocity\n            \n            # Clip offspring to stay within bounds\n            offspring = np.clip(offspring, self.lb, self.ub)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(x) for x in offspring])\n            self.eval_count += self.pop_size\n            \n            # Update best solution\n            best_offspring_index = np.argmin(offspring_fitness)\n            if offspring_fitness[best_offspring_index] < self.f_opt:\n                self.f_opt = offspring_fitness[best_offspring_index]\n                self.x_opt = offspring[best_offspring_index]\n            \n            # Select survivors (replace the worst individuals with the best offspring)\n            for i in range(self.pop_size):\n                if offspring_fitness[i] < fitness[i]:\n                    population[i] = offspring[i].copy()\n                    fitness[i] = offspring_fitness[i]\n\n            # Shrink the search space (reduce standard deviation)\n            std *= self.shrink_factor\n            \n            # Dynamic std adjustment based on fitness variance\n            fitness_std = np.std(fitness)\n            if fitness_std < 0.01:  # If fitness is concentrated, shrink std more aggressively\n                std *= self.shrink_factor * 0.9\n            elif fitness_std > 1.0: # if fitness is spread out, expand std a bit\n                std *= min(1.0 / self.shrink_factor, 1.05) # avoid unbounded growth\n\n            # Adjust bounds (shrink towards the best solution)\n            self.lb = np.maximum(self.lb, self.x_opt - 2.5 * std)\n            self.ub = np.minimum(self.ub, self.x_opt + 2.5 * std)\n            \n            population = np.clip(population, self.lb, self.ub)\n            \n            if self.eval_count >= self.budget:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveGaussianSearch scored 0.357 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["de3d9a52-7808-4788-bcf1-aad5e5398501"], "operator": null, "metadata": {"aucs": [0.14302017744142592, 0.2280486438507825, 0.47996109287375044, 0.2086365582733758, 0.29328723892736785, 0.1879977786577376, 0.2657797348198747, 0.5009456799120097, 0.46398263010145724, 0.1743493025222066, 0.24996709938769923, 0.9963592666058502, 0.269025096306995, 0.2914282450825697, 0.7039844624297695, 0.3781409044514883, 0.25465156183080684, 0.41713067358664135, 0.1742160659360632, 0.45948929023921337]}}
{"id": "20021b4b-bdd4-4e0b-8fbd-2297f0de6202", "fitness": 0.3271028259015795, "name": "AdaptiveGaussianSearch", "description": "Adaptive Gaussian Search with momentum-based population update and adaptive shrinking factor.", "code": "import numpy as np\n\nclass AdaptiveGaussianSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_std=1.0, shrink_factor=0.99, momentum=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.initial_std = initial_std\n        self.shrink_factor = shrink_factor\n        self.momentum = momentum\n        self.lb = -5.0\n        self.ub = 5.0\n        self.velocity = np.zeros((pop_size, dim))  # Initialize velocity for momentum\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        \n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.eval_count += self.pop_size\n        \n        # Find best initial solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n            \n        std = self.initial_std\n        \n        while self.eval_count < self.budget:\n            # Generate offspring using Gaussian mutation and momentum\n            mutation = np.random.normal(0, std, size=(self.pop_size, self.dim))\n            offspring = population + mutation + self.momentum * self.velocity\n            \n            # Clip offspring to stay within bounds\n            offspring = np.clip(offspring, self.lb, self.ub)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(x) for x in offspring])\n            self.eval_count += self.pop_size\n            \n            # Update best solution\n            best_offspring_index = np.argmin(offspring_fitness)\n            if offspring_fitness[best_offspring_index] < self.f_opt:\n                self.f_opt = offspring_fitness[best_offspring_index]\n                self.x_opt = offspring[best_offspring_index]\n            \n            # Select survivors (replace the worst individuals with the best offspring)\n            worst_index = np.argmax(fitness)\n            best_offspring_index = np.argmin(offspring_fitness)\n            if offspring_fitness[best_offspring_index] < fitness[worst_index]:\n                population[worst_index] = offspring[best_offspring_index]\n                fitness[worst_index] = offspring_fitness[best_offspring_index]\n                self.velocity[worst_index] = offspring[best_offspring_index] - population[worst_index] #update velocity\n\n            else:\n                 self.velocity[worst_index] *= self.momentum # Dampen velocity if not replaced\n            \n            # Shrink the search space (reduce standard deviation)\n            std *= self.shrink_factor\n            \n            # Adaptive shrink factor\n            if self.eval_count % 1000 == 0:\n                 if np.std(fitness) < 0.01:  # If fitness is converging\n                      self.shrink_factor = min(0.999, self.shrink_factor * 1.01) # Reduce shrink factor\n                 else:\n                      self.shrink_factor = max(0.95, self.shrink_factor * 0.99) # Increase shrink factor if diversity is still high\n            \n            # Adjust bounds (shrink towards the best solution)\n            self.lb = np.maximum(self.lb, self.x_opt - 2.5 * std)\n            self.ub = np.minimum(self.ub, self.x_opt + 2.5 * std)\n            \n            population = np.clip(population, self.lb, self.ub)\n            \n            if self.eval_count >= self.budget:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveGaussianSearch scored 0.327 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["de3d9a52-7808-4788-bcf1-aad5e5398501"], "operator": null, "metadata": {"aucs": [0.14360726984876948, 0.19074479823311596, 0.41651011845143093, 0.1933621593113629, 0.19543606298946525, 0.2761978628558299, 0.28006929716978735, 0.42317009497234437, 0.5771504539746636, 0.24358125565707434, 0.21391020105371206, 0.9964419979772272, 0.25425005062299255, 0.21112044685983877, 0.19633109308366048, 0.35298511557483303, 0.4296562695371998, 0.20725802088043233, 0.24308534565858742, 0.4971886033192622]}}
{"id": "5b02c097-5028-4d05-81c6-f566526b213d", "fitness": 0.38013762719067373, "name": "AdaptiveGaussianSearch", "description": "Adaptive Gaussian Search with momentum and adaptive step size adjustment based on success rate.", "code": "import numpy as np\n\nclass AdaptiveGaussianSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_std=1.0, shrink_factor=0.99, success_history_length=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.initial_std = initial_std\n        self.shrink_factor = shrink_factor\n        self.lb = -5.0\n        self.ub = 5.0\n        self.success_history_length = success_history_length\n        self.success_history = []\n        self.momentum = 0.1  # Momentum for std update\n        self.std = self.initial_std\n        self.v = 0 # velocity of std\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        \n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.eval_count += self.pop_size\n        \n        # Find best initial solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n        \n        \n        while self.eval_count < self.budget:\n            # Generate offspring using Gaussian mutation\n            offspring = population + np.random.normal(0, self.std, size=(self.pop_size, self.dim))\n            \n            # Clip offspring to stay within bounds\n            offspring = np.clip(offspring, self.lb, self.ub)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(x) for x in offspring])\n            self.eval_count += self.pop_size\n            \n            # Update best solution\n            best_offspring_index = np.argmin(offspring_fitness)\n            if offspring_fitness[best_offspring_index] < self.f_opt:\n                self.f_opt = offspring_fitness[best_offspring_index]\n                self.x_opt = offspring[best_offspring_index]\n                success = True\n            else:\n                success = False\n            \n            # Select survivors (replace the worst individuals with the best offspring)\n            worst_index = np.argmax(fitness)\n            best_offspring_index = np.argmin(offspring_fitness)\n            if offspring_fitness[best_offspring_index] < fitness[worst_index]:\n                population[worst_index] = offspring[best_offspring_index]\n                fitness[worst_index] = offspring_fitness[best_offspring_index]\n            \n            # Update success history\n            self.success_history.append(int(success))\n            if len(self.success_history) > self.success_history_length:\n                self.success_history.pop(0)\n            \n            # Adjust step size based on success rate\n            if len(self.success_history) == self.success_history_length:\n                success_rate = np.mean(self.success_history)\n                if success_rate > 0.4:\n                    self.v = self.momentum * self.v + (1 - self.momentum) * 0.1\n                elif success_rate < 0.2:\n                    self.v = self.momentum * self.v - (1 - self.momentum) * 0.1\n                \n                self.std *= np.exp(self.v)\n                self.std = max(self.std, 1e-6) # Minimum std\n\n            # Adjust bounds (shrink towards the best solution)\n            self.lb = np.maximum(self.lb, self.x_opt - 2.5 * self.std)\n            self.ub = np.minimum(self.ub, self.x_opt + 2.5 * self.std)\n            \n            population = np.clip(population, self.lb, self.ub)\n            \n            if self.eval_count >= self.budget:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveGaussianSearch scored 0.380 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["de3d9a52-7808-4788-bcf1-aad5e5398501"], "operator": null, "metadata": {"aucs": [0.1672152667450697, 0.25074710474277784, 0.43088047559038134, 0.21052949028451995, 0.34049671604099785, 0.7745309635839546, 0.30657142890767053, 0.34627873969363787, 0.22632626354809682, 0.20456885343125997, 0.18520631509082974, 0.9946179185434827, 0.23374256098044177, 0.582590209640067, 0.7249012925347671, 0.34073987687616913, 0.3051579821166982, 0.2369516877993728, 0.2412615185895136, 0.4994378790737666]}}
{"id": "fef8fcd6-1df5-4c17-900f-917e2c09c654", "fitness": 0.3121839138048256, "name": "AdaptiveGaussianSearch", "description": "Adaptive Gaussian Search with momentum and adaptive shrinking factor based on fitness improvement.", "code": "import numpy as np\n\nclass AdaptiveGaussianSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_std=1.0, shrink_factor=0.99, momentum=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.initial_std = initial_std\n        self.shrink_factor = shrink_factor\n        self.momentum = momentum\n        self.lb = -5.0\n        self.ub = 5.0\n        self.velocity = np.zeros((pop_size, dim))  # Initialize velocity\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        \n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.eval_count += self.pop_size\n        \n        # Find best initial solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n            \n        std = self.initial_std\n        \n        while self.eval_count < self.budget:\n            # Generate offspring using Gaussian mutation with momentum\n            noise = np.random.normal(0, std, size=(self.pop_size, self.dim))\n            self.velocity = self.momentum * self.velocity + (1 - self.momentum) * noise\n            offspring = population + self.velocity\n            \n            # Clip offspring to stay within bounds\n            offspring = np.clip(offspring, self.lb, self.ub)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(x) for x in offspring])\n            self.eval_count += self.pop_size\n            \n            # Update best solution\n            best_offspring_index = np.argmin(offspring_fitness)\n            if offspring_fitness[best_offspring_index] < self.f_opt:\n                self.f_opt = offspring_fitness[best_offspring_index]\n                self.x_opt = offspring[best_offspring_index]\n                \n                # Adaptive shrink factor adjustment: Reduce shrink factor if improvement is good\n                if self.f_opt < np.min(fitness):\n                    self.shrink_factor = min(0.999, self.shrink_factor + 0.001) # increase shrink factor\n                else:\n                    self.shrink_factor = max(0.9, self.shrink_factor - 0.001) # decrease shrink factor\n            \n            # Select survivors (replace the worst individuals with the best offspring)\n            worst_index = np.argmax(fitness)\n            best_offspring_index = np.argmin(offspring_fitness)\n            if offspring_fitness[best_offspring_index] < fitness[worst_index]:\n                population[worst_index] = offspring[best_offspring_index]\n                fitness[worst_index] = offspring_fitness[best_offspring_index]\n            \n            # Shrink the search space (reduce standard deviation)\n            std *= self.shrink_factor\n            \n            # Adjust bounds (shrink towards the best solution)\n            self.lb = np.maximum(self.lb, self.x_opt - 2.5 * std)\n            self.ub = np.minimum(self.ub, self.x_opt + 2.5 * std)\n            \n            population = np.clip(population, self.lb, self.ub)\n            \n            if self.eval_count >= self.budget:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveGaussianSearch scored 0.312 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["de3d9a52-7808-4788-bcf1-aad5e5398501"], "operator": null, "metadata": {"aucs": [0.13355377207431574, 0.2918776941347554, 0.35851085720242426, 0.40453299037471113, 0.24896884822762233, 0.3538388578048145, 0.2681154772204738, 0.23856862300898807, 0.29870074569530447, 0.17626058209222206, 0.27243731549998007, 0.29804059267069405, 0.2597708784778141, 0.2427178868495692, 0.7207198962317352, 0.2827462983024954, 0.2915576934963561, 0.41195210728291887, 0.24301791873417744, 0.4477892407151396]}}
{"id": "655ff427-c48f-43f4-829f-c48df112ed66", "fitness": 0.36771150826767673, "name": "AdaptiveGaussianSearch", "description": "Adaptive Gaussian Search with momentum and adaptive step size based on success rate.", "code": "import numpy as np\n\nclass AdaptiveGaussianSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_std=1.0, shrink_factor=0.99, momentum=0.1, success_rate_alpha=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.initial_std = initial_std\n        self.shrink_factor = shrink_factor\n        self.lb = -5.0\n        self.ub = 5.0\n        self.momentum = momentum\n        self.success_rate_alpha = success_rate_alpha\n        self.std = initial_std  # Current std, allows adaptation\n        self.velocity = np.zeros((pop_size, dim)) #Initialize velocity\n        self.success_rate = 0.5  # Initialize success rate for step size adaptation\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        \n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.eval_count += self.pop_size\n        \n        # Find best initial solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n        \n        \n        while self.eval_count < self.budget:\n            # Generate offspring using Gaussian mutation with momentum\n            # Update velocity based on previous velocity and a Gaussian random variable\n            self.velocity = self.momentum * self.velocity + np.sqrt(1 - self.momentum**2) * np.random.normal(0, self.std, size=(self.pop_size, self.dim))\n            offspring = population + self.velocity\n\n            # Clip offspring to stay within bounds\n            offspring = np.clip(offspring, self.lb, self.ub)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(x) for x in offspring])\n            self.eval_count += self.pop_size\n            \n            # Update best solution\n            best_offspring_index = np.argmin(offspring_fitness)\n            if offspring_fitness[best_offspring_index] < self.f_opt:\n                self.f_opt = offspring_fitness[best_offspring_index]\n                self.x_opt = offspring[best_offspring_index]\n            \n            # Select survivors (replace the worst individuals with the best offspring)\n            num_improved = 0\n            for i in range(self.pop_size):\n                if offspring_fitness[i] < fitness[i]:\n                    population[i] = offspring[i].copy()\n                    fitness[i] = offspring_fitness[i]\n                    num_improved += 1\n\n            # Update success rate\n            self.success_rate = (1 - self.success_rate_alpha) * self.success_rate + self.success_rate_alpha * (num_improved / self.pop_size)\n\n            # Adjust step size based on success rate\n            if self.success_rate > 0.2:\n                self.std *= 1.1  # Increase step size\n            elif self.success_rate < 0.1:\n                self.std *= 0.9  # Decrease step size\n\n            self.std = min(self.std, self.initial_std) #Cap to avoid excessive std\n            \n            # Shrink the search space (reduce standard deviation)\n            #self.std *= self.shrink_factor #Moved to adaptation based on success\n\n            # Adjust bounds (shrink towards the best solution)\n            self.lb = np.maximum(self.lb, self.x_opt - 2.5 * self.std)\n            self.ub = np.minimum(self.ub, self.x_opt + 2.5 * self.std)\n            \n            population = np.clip(population, self.lb, self.ub)\n            \n            if self.eval_count >= self.budget:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveGaussianSearch scored 0.368 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["de3d9a52-7808-4788-bcf1-aad5e5398501"], "operator": null, "metadata": {"aucs": [0.1651156586284962, 0.2955696444953265, 0.26033558666230383, 0.8328354694110895, 0.20456359284123338, 0.6271099714541346, 0.2554596881900273, 0.4846265072343088, 0.17433302871411216, 0.2077957942958325, 0.7895013489587779, 0.9965011450892143, 0.2333034638881264, 0.27823528330594316, 0.15506797880259182, 0.2979098629409894, 0.22779117440608188, 0.19060973491830724, 0.1812756263395091, 0.4962896047771276]}}
{"id": "aca974f2-dbab-4a17-bb44-721f35809d5f", "fitness": 0.35130919266619737, "name": "VelocityClampedPSO", "description": "Improved PSO with boundary handling and checks to ensure positions stay within bounds.", "code": "import numpy as np\n\nclass VelocityClampedPSO:\n    def __init__(self, budget=10000, dim=10, pop_size=20, c1=1.49, c2=1.49, v_max_factor=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.c1 = c1  # Cognitive coefficient\n        self.c2 = c2  # Social coefficient\n        self.v_max_factor = v_max_factor  # Max velocity as fraction of search space\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        # Initialize population and velocities\n        population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        velocities = np.random.uniform(-abs(ub - lb) * self.v_max_factor, abs(ub - lb) * self.v_max_factor, size=(self.pop_size, self.dim))\n        \n        # Initialize personal best positions and fitnesses\n        pbest_positions = population.copy()\n        pbest_fitnesses = np.array([func(x) for x in population])\n        \n        # Initialize global best position and fitness\n        global_best_index = np.argmin(pbest_fitnesses)\n        global_best_position = pbest_positions[global_best_index].copy()\n        global_best_fitness = pbest_fitnesses[global_best_index]\n        \n        eval_count = self.pop_size #initial evalutations\n        \n        # Iterate until budget is exhausted\n        while eval_count < self.budget:\n            # Update velocities and positions\n            r1 = np.random.rand(self.pop_size, self.dim)\n            r2 = np.random.rand(self.pop_size, self.dim)\n            \n            velocities = velocities + self.c1 * r1 * (pbest_positions - population) + self.c2 * r2 * (global_best_position - population)\n\n            # Clamp velocities\n            v_max = abs(ub - lb) * self.v_max_factor\n            velocities = np.clip(velocities, -v_max, v_max)\n            \n            population = population + velocities\n            \n            # Reflect particles that hit boundaries - vectorized\n            population = np.where(population < lb, lb + (lb - population), population)\n            population = np.where(population > ub, ub - (population - ub), population)\n            \n            velocities = np.where(population < lb, -velocities, velocities)\n            velocities = np.where(population > ub, -velocities, velocities)\n            \n\n            # Evaluate new positions\n            fitnesses = np.array([func(x) for x in population])\n            eval_count += self.pop_size\n\n            # Update personal best positions and fitnesses\n            for i in range(self.pop_size):\n                if fitnesses[i] < pbest_fitnesses[i]:\n                    pbest_fitnesses[i] = fitnesses[i]\n                    pbest_positions[i] = population[i].copy()\n                    \n                    # Update global best position and fitness\n                    if fitnesses[i] < global_best_fitness:\n                        global_best_fitness = fitnesses[i]\n                        global_best_position = population[i].copy()\n                        \n        return global_best_fitness, global_best_position", "configspace": "", "generation": 1, "feedback": "The algorithm VelocityClampedPSO scored 0.351 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["7f8b7b90-8e6f-4ccc-b647-53589b0dea0e"], "operator": null, "metadata": {"aucs": [0.15502363269637442, 0.20375514557870933, 0.3206493877978567, 0.381724652148582, 0.24728190909705172, 0.3218774139115115, 0.29290757634072295, 0.2939422122192289, 0.28999868383856187, 0.19303466276658188, 0.3680256082165184, 0.9950841447093006, 0.2787900350379978, 0.27521670714292346, 0.7082996639330412, 0.3349809399026421, 0.28309075030468056, 0.37751001966150444, 0.2050301155162002, 0.4999605925039575]}}
{"id": "34a9ba06-ecfb-4171-98e8-13ee9f706baa", "fitness": 0.337049210900553, "name": "AdaptiveGaussianSearch", "description": "Adaptive Gaussian Search with momentum, adaptive learning rate, and selective pressure.", "code": "import numpy as np\n\nclass AdaptiveGaussianSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_std=1.0, shrink_factor=0.99, momentum=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.initial_std = initial_std\n        self.shrink_factor = shrink_factor\n        self.lb = -5.0\n        self.ub = 5.0\n        self.momentum = momentum\n        self.velocity = np.zeros((pop_size, dim))\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        \n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.eval_count += self.pop_size\n        \n        # Find best initial solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n            \n        std = self.initial_std\n        learning_rate = 1.0\n        \n        while self.eval_count < self.budget:\n            # Generate offspring using Gaussian mutation and momentum\n            mutation = np.random.normal(0, std, size=(self.pop_size, self.dim))\n            self.velocity = self.momentum * self.velocity + learning_rate * mutation\n            offspring = population + self.velocity\n            \n            # Clip offspring to stay within bounds\n            offspring = np.clip(offspring, self.lb, self.ub)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(x) for x in offspring])\n            self.eval_count += self.pop_size\n            \n            # Update best solution\n            best_offspring_index = np.argmin(offspring_fitness)\n            if offspring_fitness[best_offspring_index] < self.f_opt:\n                self.f_opt = offspring_fitness[best_offspring_index]\n                self.x_opt = offspring[best_offspring_index]\n            \n            # Selection: Tournament selection\n            selected_indices = np.random.choice(self.pop_size, size=self.pop_size, replace=True)\n            for i in range(self.pop_size):\n                if offspring_fitness[i] < fitness[selected_indices[i]]:\n                    population[selected_indices[i]] = offspring[i]\n                    fitness[selected_indices[i]] = offspring_fitness[i]\n            \n            # Shrink the search space (reduce standard deviation)\n            std *= self.shrink_factor\n            learning_rate *= self.shrink_factor\n            \n            # Adjust bounds (shrink towards the best solution)\n            self.lb = np.maximum(self.lb, self.x_opt - 2.5 * std)\n            self.ub = np.minimum(self.ub, self.x_opt + 2.5 * std)\n            \n            population = np.clip(population, self.lb, self.ub)\n            \n            if self.eval_count >= self.budget:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveGaussianSearch scored 0.337 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["de3d9a52-7808-4788-bcf1-aad5e5398501"], "operator": null, "metadata": {"aucs": [0.33693190618027036, 0.2532393334184978, 0.2589765986932231, 0.25145817996750786, 0.2273144408807476, 0.25089787657619644, 0.282111392266409, 0.2398583470176443, 0.21313332892734638, 0.5196657635968072, 0.8413037306739699, 0.9915696932695501, 0.243130581839889, 0.21938983320797067, 0.14901549905010447, 0.33855025105794545, 0.23108819842728123, 0.2184624712820633, 0.19769768520331832, 0.47718910647431745]}}
{"id": "d5ac6481-85f7-4ee8-b4a0-62c674969097", "fitness": 0.543745682188564, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with self-adaptive parameters, archive utilization in mutation, and a refined stagnation check.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, archive_size=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.archive_size = archive_size\n        self.F_memory = np.ones(self.pop_size) * 0.5\n        self.CR_memory = np.ones(self.pop_size) * 0.7\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.archive = []\n        self.archive_fitness = []\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n\n        generation = 0\n        stagnation_counter = 0\n        prev_best_fitness = self.f_opt\n\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                self.F_memory[i] = np.clip(np.random.normal(0.5, 0.3), 0.1, 1.0)\n                self.CR_memory[i] = np.clip(np.random.normal(0.7, 0.1), 0.0, 1.0)\n\n                # Mutation\n                indices = np.random.choice(self.pop_size, 2, replace=False)\n                x1, x2 = population[indices]\n\n                # Utilize archive\n                if len(self.archive) > 0 and np.random.rand() < 0.2:\n                     archive_index = np.random.randint(len(self.archive))\n                     x3 = self.archive[archive_index]\n                else:\n                    indices = np.random.choice(self.pop_size, 1, replace=False)\n                    x3 = population[indices[0]]\n\n                mutant = population[i] + self.F_memory[i] * (x1 - x2) + self.F_memory[i] * (x3 - population[i]) # Added extra term to pull to archive\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR_memory[i] or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n\n                    # Add replaced vector to archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                    else:\n                        #Replace oldest in archive\n                        self.archive[generation % self.archive_size] = population[i]\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        stagnation_counter = 0 # Reset stagnation counter\n                else:\n                    # Add trial vector to archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                    else:\n                        #Replace oldest in archive\n                        self.archive[generation % self.archive_size] = trial\n                        \n\n            population = new_population\n            fitness = new_fitness\n\n            # Restart population if stagnating\n            if abs(self.f_opt - prev_best_fitness) < 1e-8: #More robust stagnation detection\n                stagnation_counter += 1\n            else:\n                stagnation_counter = 0\n            prev_best_fitness = self.f_opt\n\n            if stagnation_counter > 100: #Increased stagnation threshold\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                stagnation_counter = 0  # Reset after restart\n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDE scored 0.544 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ed7ab72d-d902-4a89-867a-57f1daf2d604"], "operator": null, "metadata": {"aucs": [0.17626627881104473, 0.33980547474499867, 0.46576117633600755, 0.7427912186800527, 0.5922824557979532, 0.6887240824251595, 0.4055257515414734, 0.4951419394251144, 0.6324769010504943, 0.2394776609473982, 0.7941079479489376, 0.998161921264494, 0.4043470475273424, 0.5337636614289112, 0.7949772282860624, 0.6611367996288557, 0.4716573362067412, 0.7505722484330112, 0.1976986701046659, 0.4902378431825607]}}
{"id": "03f49467-3003-4488-ab57-7dfaacfe2818", "fitness": 0.6148427560596922, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with dynamic F and CR parameters and a combined archive strategy.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_base=0.5, CR_base=0.7, archive_size=100, F_range=0.3, CR_range=0.3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.archive_size = archive_size\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.archive = []\n        self.archive_fitness = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                F = self.F_base + np.random.uniform(-self.F_range, self.F_range)\n                CR = self.CR_base + np.random.uniform(-self.CR_range, self.CR_range)\n                F = np.clip(F, 0.1, 1.0)\n                CR = np.clip(CR, 0.1, 1.0)\n\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = x1 + F * (x2 - x3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n\n                    # Add replaced vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                         # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = population[i]\n                        self.archive_fitness[worst_archive_index] = fitness[i]\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                else:\n                     # Add trial vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = trial\n                        self.archive_fitness[worst_archive_index] = f_trial\n                        \n\n            population = new_population\n            fitness = new_fitness\n\n            # Restart population if stagnating\n            if generation % 50 == 0:\n                if np.std(fitness) < 1e-6:  # Stagnation criterion\n                    population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                    fitness = np.array([func(x) for x in population])\n                    self.budget -= self.pop_size\n                    best_index = np.argmin(fitness)\n                    if fitness[best_index] < self.f_opt:\n                        self.f_opt = fitness[best_index]\n                        self.x_opt = population[best_index]\n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDE scored 0.615 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ed7ab72d-d902-4a89-867a-57f1daf2d604"], "operator": null, "metadata": {"aucs": [0.2104212805390424, 0.44249349939760796, 0.5548629727208898, 0.8342615221979883, 0.6690922774438541, 0.7292642582733456, 0.5453855365189149, 0.5726053152514546, 0.6828721037119856, 0.5556774961851751, 0.7880022764911547, 0.9902729206634181, 0.43686015573965764, 0.59003663874051, 0.8931226392959521, 0.6885182663838639, 0.547535704820204, 0.7728341054851233, 0.27115024444349534, 0.5215859068902066]}}
{"id": "6851ee47-01e2-411d-9b44-7c6464541a03", "fitness": -Infinity, "name": "AdaptiveGaussianSearch", "description": "Adaptive Gaussian Search with orthogonal initialization, covariance matrix adaptation, and dynamic bounds adjustment.", "code": "import numpy as np\n\nclass AdaptiveGaussianSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_std=1.0, learning_rate=0.1, success_rate_alpha=0.1, bound_shrink_factor=0.999):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.initial_std = initial_std\n        self.learning_rate = learning_rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.success_rate_alpha = success_rate_alpha\n        self.bound_shrink_factor = bound_shrink_factor\n        self.std = initial_std\n        self.success_rate = 0.5\n        self.eval_count = 0\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim) * initial_std**2 # Initialize covariance matrix\n        self.eigenvalues = None\n        self.eigenvectors = None\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n\n        # Orthogonal initialization\n        population = self.orthogonal_initialization()\n        fitness = np.array([func(x) for x in population])\n        self.eval_count += self.pop_size\n\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n            self.mean = self.x_opt.copy()\n\n        while self.eval_count < self.budget:\n            # Generate offspring using MVN with covariance matrix\n            offspring = np.random.multivariate_normal(self.mean, self.covariance, size=self.pop_size)\n\n            # Clip offspring to stay within bounds\n            offspring = np.clip(offspring, self.lb, self.ub)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(x) for x in offspring])\n            self.eval_count += self.pop_size\n\n            # Update best solution\n            best_offspring_index = np.argmin(offspring_fitness)\n            if offspring_fitness[best_offspring_index] < self.f_opt:\n                self.f_opt = offspring_fitness[best_offspring_index]\n                self.x_opt = offspring[best_offspring_index]\n                self.mean = self.x_opt.copy()\n\n            # Selection and update mean\n            num_improved = 0\n            for i in range(self.pop_size):\n                if offspring_fitness[i] < fitness[i]:\n                    population[i] = offspring[i].copy()\n                    fitness[i] = offspring_fitness[i]\n                    num_improved += 1\n\n            # Update success rate\n            self.success_rate = (1 - self.success_rate_alpha) * self.success_rate + self.success_rate_alpha * (num_improved / self.pop_size)\n\n            # Covariance matrix adaptation (CMA) inspired update\n            diff = population - self.mean\n            weighted_diff = (fitness - np.mean(fitness)) * diff\n            delta_mean = self.learning_rate * np.mean(diff, axis=0)\n            self.mean += delta_mean\n\n            C = np.cov(population.T)\n            self.covariance = (1 - self.learning_rate) * self.covariance + self.learning_rate * C\n\n            # Bound adaptation (shrinking)\n            self.lb = self.bound_shrink_factor * self.lb + (1 - self.bound_shrink_factor) * self.x_opt\n            self.ub = self.bound_shrink_factor * self.ub + (1 - self.bound_shrink_factor) * self.x_opt\n\n            population = np.clip(population, self.lb, self.ub)\n\n            if self.eval_count >= self.budget:\n                break\n        return self.f_opt, self.x_opt\n\n    def orthogonal_initialization(self):\n        # Generate an orthogonal matrix using QR decomposition\n        H = np.random.randn(self.pop_size, self.dim)\n        Q, R = np.linalg.qr(H)\n        \n        # Scale each row randomly within bounds\n        population = np.zeros((self.pop_size, self.dim))\n        for i in range(self.pop_size):\n            random_scale = np.random.uniform(0, 1)\n            population[i] = self.lb + (self.ub - self.lb) * random_scale * Q[i] #Using a random scale and then Q\n            \n            #Alternative: sample normally with covariance\n            #population = np.random.multivariate_normal(mean=np.zeros(self.dim), cov=np.eye(self.dim), size=self.pop_size)\n            #scale appropriately\n            \n        return np.clip(population, self.lb, self.ub)", "configspace": "", "generation": 2, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 69, in __call__\nValueError: operands could not be broadcast together with shapes (20,) (20,2) \n.", "error": "", "parent_ids": ["655ff427-c48f-43f4-829f-c48df112ed66"], "operator": null, "metadata": {}}
{"id": "1edffa81-50b4-485b-8a51-0a67ef4b2ae9", "fitness": -Infinity, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with orthogonal design for parameter tuning, combined archive strategy, and dynamic population sizing.", "code": "import numpy as np\nfrom scipy.stats import norm\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_base=0.5, CR_base=0.7, archive_size=100, F_range=0.3, CR_range=0.3, orthogonal_design_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.min_pop_size = 10  # Minimum population size\n        self.max_pop_size = 100 # Maximum population size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.archive_size = archive_size\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.archive = []\n        self.archive_fitness = []\n        self.orthogonal_design_size = orthogonal_design_size\n        self.success_memory_F = []  # Store successful F values\n        self.success_memory_CR = []  # Store successful CR values\n        self.levy_exponent = 1.5  # Exponent for Levy flight\n\n    def levy_flight(self, size):\n        \"\"\"Generate Levy flight steps.\"\"\"\n        u = np.random.randn(size)\n        v = np.random.randn(size)\n        step = (u * np.sqrt(np.pi / 2) * np.gamma(1 + self.levy_exponent) * np.sin(np.pi * self.levy_exponent / 2)) / \\\n               (np.abs(v) ** (1 / self.levy_exponent) * np.gamma((1 + self.levy_exponent) / 2) * self.levy_exponent * 2 ** ((self.levy_exponent - 1) / 2))\n        return step\n\n    def orthogonal_design(self):\n        \"\"\"Generate orthogonal design for F and CR parameters.\"\"\"\n        design = np.zeros((self.orthogonal_design_size, 2))\n        for i in range(self.orthogonal_design_size):\n            design[i, 0] = np.random.uniform(self.F_base - self.F_range, self.F_base + self.F_range)\n            design[i, 1] = np.random.uniform(self.CR_base - self.CR_range, self.CR_base + self.CR_range)\n            design[i, 0] = np.clip(design[i, 0], 0.1, 1.0)\n            design[i, 1] = np.clip(design[i, 1], 0.1, 1.0)\n        return design\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n\n            orthogonal_params = self.orthogonal_design()\n\n            for i in range(self.pop_size):\n                # Orthogonal design parameter selection\n                design_index = i % self.orthogonal_design_size\n                F = orthogonal_params[design_index, 0]\n                CR = orthogonal_params[design_index, 1]\n\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = x1 + F * (x2 - x3)\n\n                # Levy flight to enhance exploration\n                levy_steps = self.levy_flight(self.dim) * 0.01  # Scale Levy steps\n                mutant += levy_steps\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n\n                    # Add replaced vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                         # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = population[i]\n                        self.archive_fitness[worst_archive_index] = fitness[i]\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                    # Store successful F and CR values\n                    self.success_memory_F.append(F)\n                    self.success_memory_CR.append(CR)\n\n                else:\n                     # Add trial vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = trial\n                        self.archive_fitness[worst_archive_index] = f_trial\n                        \n\n            population = new_population\n            fitness = new_fitness\n\n            # Dynamic population size adjustment\n            if generation % 20 == 0:\n                if len(self.success_memory_F) > 10:\n                    mean_success_F = np.mean(self.success_memory_F)\n                    mean_success_CR = np.mean(self.success_memory_CR)\n                    \n                    # Adjust population size based on success\n                    if mean_success_F > self.F_base and mean_success_CR > self.CR_base:\n                        self.pop_size = min(self.pop_size + 5, self.max_pop_size)\n                    elif mean_success_F < self.F_base and mean_success_CR < self.CR_base:\n                        self.pop_size = max(self.pop_size - 5, self.min_pop_size)\n                    \n                    # Reinitialize population if size changed\n                    if population.shape[0] != self.pop_size:\n                         population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                         fitness = np.array([func(x) for x in population])\n                         self.budget -= self.pop_size\n\n                    self.success_memory_F = []  # Clear memory\n                    self.success_memory_CR = []\n\n            # Restart population if stagnating\n            if generation % 50 == 0:\n                if np.std(fitness) < 1e-6:  # Stagnation criterion\n                    population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                    fitness = np.array([func(x) for x in population])\n                    self.budget -= self.pop_size\n                    best_index = np.argmin(fitness)\n                    if fitness[best_index] < self.f_opt:\n                        self.f_opt = fitness[best_index]\n                        self.x_opt = population[best_index]\n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 76, in __call__\n  File \"<string>\", line 27, in levy_flight\n  File \"/home/neocortex/.cache/pypoetry/virtualenvs/blade-xXF9vABH-py3.10/lib/python3.10/site-packages/numpy/__init__.py\", line 320, in __getattr__\n    raise AttributeError(\"module {!r} has no attribute \"\nAttributeError: module 'numpy' has no attribute 'gamma'\n.", "error": "", "parent_ids": ["03f49467-3003-4488-ab57-7dfaacfe2818"], "operator": null, "metadata": {}}
{"id": "2cd2c207-49b8-4bee-b451-ccda726dfe4c", "fitness": -Infinity, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with dynamic F and CR, combined archive, stagnation handling using L-BFGS-B local search, and adaptive population size.", "code": "import numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size_base=50, F_base=0.5, CR_base=0.7, archive_size=100, F_range=0.3, CR_range=0.3, local_search_prob=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_base = pop_size_base\n        self.pop_size = pop_size_base  # Initial population size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.archive_size = archive_size\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.archive = []\n        self.archive_fitness = []\n        self.local_search_prob = local_search_prob\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        bounds = func.bounds\n        lb, ub = bounds.lb, bounds.ub\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                F = self.F_base + np.random.uniform(-self.F_range, self.F_range)\n                CR = self.CR_base + np.random.uniform(-self.CR_range, self.CR_range)\n                F = np.clip(F, 0.1, 1.0)\n                CR = np.clip(CR, 0.1, 1.0)\n\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n\n                # Add archive vector with small probability\n                if np.random.rand() < 0.1 and len(self.archive) > 0:\n                    x4 = self.archive[np.random.randint(len(self.archive))]\n                    mutant = x1 + F * (x2 - x3) + F * (x4 - population[i]) # Including archive\n                else:\n                    mutant = x1 + F * (x2 - x3)\n                mutant = np.clip(mutant, lb, ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n\n                    # Add replaced vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                         # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = population[i]\n                        self.archive_fitness[worst_archive_index] = fitness[i]\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                else:\n                     # Add trial vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = trial\n                        self.archive_fitness[worst_archive_index] = f_trial\n                        \n\n            population = new_population\n            fitness = new_fitness\n\n            # Local Search with L-BFGS-B\n            if np.random.rand() < self.local_search_prob:\n                index = np.random.randint(self.pop_size)\n                x_local, f_local, d = fmin_l_bfgs_b(func, population[index], bounds=[(lb, ub)] * self.dim, approx_grad=True, maxfun=self.budget // 100) # Reduce maxfun\n                self.budget -= d['funcalls']\n\n                if f_local < fitness[index]:\n                    population[index] = x_local\n                    fitness[index] = f_local\n                    if f_local < self.f_opt:\n                        self.f_opt = f_local\n                        self.x_opt = x_local\n\n            # Adaptive Population Size\n            if generation % 20 == 0:\n                if np.std(fitness) < 1e-6:\n                    self.pop_size = int(self.pop_size * 0.8)\n                else:\n                    self.pop_size = min(self.pop_size_base, self.budget // (2 * self.dim))\n                self.pop_size = max(10, self.pop_size)\n\n            # Restart population if stagnating\n            if generation % 50 == 0:\n                if np.std(fitness) < 1e-6:  # Stagnation criterion\n                    population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n                    fitness = np.array([func(x) for x in population])\n                    self.budget -= self.pop_size\n                    best_index = np.argmin(fitness)\n                    if fitness[best_index] < self.f_opt:\n                        self.f_opt = fitness[best_index]\n                        self.x_opt = population[best_index]\n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 144, in evaluate\n    algorithm(f_new)\n  File \"<string>\", line 107, in __call__\nNameError: name 'fmin_l_bfgs_b' is not defined\n.", "error": "", "parent_ids": ["03f49467-3003-4488-ab57-7dfaacfe2818"], "operator": null, "metadata": {}}
{"id": "a555a095-7114-4ea4-96b9-28ae21f5ffea", "fitness": 0.3802569200262066, "name": "AdaptiveGaussianSearchCMA", "description": "Adaptive Gaussian Search with covariance matrix adaptation (CMA) inspired step size control and dynamic bound adjustment.", "code": "import numpy as np\n\nclass AdaptiveGaussianSearchCMA:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_std=1.0, momentum=0.1, success_rate_alpha=0.1, cma_learning_rate = 0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.initial_std = initial_std\n        self.lb = -5.0\n        self.ub = 5.0\n        self.momentum = momentum\n        self.success_rate_alpha = success_rate_alpha\n        self.std = initial_std  # Initial step size\n        self.velocity = np.zeros((pop_size, dim)) #Initialize velocity\n        self.success_rate = 0.5  # Initialize success rate for step size adaptation\n        self.cma_learning_rate = cma_learning_rate\n        self.C = np.eye(dim) #Covariance Matrix\n        self.eval_count = 0\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.eval_count += self.pop_size\n        \n        # Find best initial solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n        \n        \n        while self.eval_count < self.budget:\n            # Generate offspring using Gaussian mutation with momentum and CMA\n            z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n            offspring = population + self.std * (z @ np.linalg.cholesky(self.C).T)\n\n            # Clip offspring to stay within bounds\n            offspring = np.clip(offspring, self.lb, self.ub)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(x) for x in offspring])\n            self.eval_count += self.pop_size\n            \n            # Update best solution\n            best_offspring_index = np.argmin(offspring_fitness)\n            if offspring_fitness[best_offspring_index] < self.f_opt:\n                self.f_opt = offspring_fitness[best_offspring_index]\n                self.x_opt = offspring[best_offspring_index]\n            \n            # Select survivors (replace the worst individuals with the best offspring)\n            num_improved = 0\n            for i in range(self.pop_size):\n                if offspring_fitness[i] < fitness[i]:\n                    population[i] = offspring[i].copy()\n                    fitness[i] = offspring_fitness[i]\n                    num_improved += 1\n\n            # Update success rate\n            self.success_rate = (1 - self.success_rate_alpha) * self.success_rate + self.success_rate_alpha * (num_improved / self.pop_size)\n\n            # Adjust step size based on success rate (CMA-like adaptation)\n            if self.success_rate > 0.2:\n                self.std *= np.exp(self.cma_learning_rate * self.success_rate)  # Increase step size\n            elif self.success_rate < 0.1:\n                self.std *= np.exp(-self.cma_learning_rate * (1-self.success_rate))  # Decrease step size\n\n            self.std = min(self.std, self.initial_std) #Cap to avoid excessive std\n\n            #Update Covariance Matrix (simplified CMA-ES update)\n            weights = np.zeros(self.pop_size)\n            ranked_indices = np.argsort(offspring_fitness)\n            mu = self.pop_size // 4 #Selection pressure\n            weights[:mu] = np.log(mu+1) - np.log(np.arange(1, mu+1))\n            weights = weights / np.sum(weights)\n\n            offspring_centered = offspring - np.mean(offspring, axis=0)\n            delta = offspring_centered[ranked_indices[:mu]]\n            self.C = (1 - self.cma_learning_rate) * self.C + self.cma_learning_rate * (delta.T @ np.diag(weights[:mu]) @ delta) / (self.std**2)\n            \n            # Adjust bounds (shrink towards the best solution)\n            self.lb = np.maximum(self.lb, self.x_opt - 2.5 * self.std)\n            self.ub = np.minimum(self.ub, self.x_opt + 2.5 * self.std)\n            \n            population = np.clip(population, self.lb, self.ub)\n            \n            if self.eval_count >= self.budget:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveGaussianSearchCMA scored 0.380 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["655ff427-c48f-43f4-829f-c48df112ed66"], "operator": null, "metadata": {"aucs": [0.23326383021208663, 0.2567261920373679, 0.3501440715823504, 0.8718314543563428, 0.22239639042406156, 0.6963794993164538, 0.2794767842205631, 0.5147661496930696, 0.5505073589744249, 0.17700512689316406, 0.2347725876198672, 0.2427377851276029, 0.278860734652394, 0.31879482664899894, 0.5772184681378245, 0.4121141769298414, 0.2921705181696349, 0.37595257444564345, 0.21854654861960476, 0.5014733224628343]}}
{"id": "8ed84344-dbb2-492c-8c22-d468a5d22b3e", "fitness": 0.4421036550959627, "name": "AdaptiveGaussianSearch", "description": "Adaptive Gaussian Search with momentum-based step size adaptation and a dynamic population size adjustment based on the success rate.", "code": "import numpy as np\n\nclass AdaptiveGaussianSearch:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=20, initial_std=1.0, shrink_factor=0.99, success_history_length=10, pop_size_adjust_freq=50):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = initial_pop_size\n        self.pop_size = initial_pop_size\n        self.initial_std = initial_std\n        self.shrink_factor = shrink_factor\n        self.lb = -5.0\n        self.ub = 5.0\n        self.success_history_length = success_history_length\n        self.success_history = []\n        self.momentum = 0.1  # Momentum for std update\n        self.std = self.initial_std\n        self.v = 0 # velocity of std\n        self.eval_count = 0\n        self.pop_size_adjust_freq = pop_size_adjust_freq\n        self.pop_size_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.eval_count += self.pop_size\n\n        # Find best initial solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n\n        generation = 0\n        while self.eval_count < self.budget:\n            # Generate offspring using Gaussian mutation\n            offspring = population + np.random.normal(0, self.std, size=(self.pop_size, self.dim))\n\n            # Clip offspring to stay within bounds\n            offspring = np.clip(offspring, self.lb, self.ub)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(x) for x in offspring])\n            self.eval_count += self.pop_size\n\n            # Update best solution\n            best_offspring_index = np.argmin(offspring_fitness)\n            if offspring_fitness[best_offspring_index] < self.f_opt:\n                self.f_opt = offspring_fitness[best_offspring_index]\n                self.x_opt = offspring[best_offspring_index]\n                success = True\n            else:\n                success = False\n\n            # Select survivors (replace the worst individuals with the best offspring)\n            worst_index = np.argmax(fitness)\n            best_offspring_index = np.argmin(offspring_fitness)\n            if offspring_fitness[best_offspring_index] < fitness[worst_index]:\n                population[worst_index] = offspring[best_offspring_index]\n                fitness[worst_index] = offspring_fitness[best_offspring_index]\n\n            # Update success history\n            self.success_history.append(int(success))\n            if len(self.success_history) > self.success_history_length:\n                self.success_history.pop(0)\n\n            # Adjust step size based on success rate\n            if len(self.success_history) == self.success_history_length:\n                success_rate = np.mean(self.success_history)\n                if success_rate > 0.4:\n                    self.v = self.momentum * self.v + (1 - self.momentum) * 0.1\n                elif success_rate < 0.2:\n                    self.v = self.momentum * self.v - (1 - self.momentum) * 0.1\n\n                self.std *= np.exp(self.v)\n                self.std = max(self.std, 1e-6) # Minimum std\n\n            # Adjust bounds (shrink towards the best solution)\n            self.lb = np.maximum(self.lb, self.x_opt - 2.5 * self.std)\n            self.ub = np.minimum(self.ub, self.x_opt + 2.5 * self.std)\n\n            population = np.clip(population, self.lb, self.ub)\n\n            # Adjust population size\n            if generation % self.pop_size_adjust_freq == 0 and len(self.success_history) == self.success_history_length:\n                success_rate = np.mean(self.success_history)\n                if success_rate > 0.6 and self.pop_size < 2 * self.initial_pop_size:\n                    self.pop_size = min(self.pop_size + 5, 2 * self.initial_pop_size)\n                    #resize population\n                    new_population = np.random.uniform(self.lb, self.ub, size=(5, self.dim))\n                    new_fitness = np.array([func(x) for x in new_population])\n                    self.eval_count += 5\n                    population = np.concatenate((population, new_population))\n                    fitness = np.concatenate((fitness, new_fitness))\n                elif success_rate < 0.1 and self.pop_size > self.initial_pop_size // 2:\n                    self.pop_size = max(self.pop_size - 5, self.initial_pop_size // 2)\n                    #resize population\n                    indices = np.argsort(fitness)[-5:]\n                    population = np.delete(population, indices, axis = 0)\n                    fitness = np.delete(fitness, indices)\n            \n            self.pop_size_history.append(self.pop_size)\n            generation += 1\n\n            if self.eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveGaussianSearch scored 0.442 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5b02c097-5028-4d05-81c6-f566526b213d"], "operator": null, "metadata": {"aucs": [0.19893011736544086, 0.5610307816129108, 0.36486240024140504, 0.18403815209598595, 0.26172851272295794, 0.8524581533278148, 0.2964039995243455, 0.697068062350886, 0.3102683123376362, 0.18256024238525037, 0.32712406128427374, 0.9965863191059554, 0.2609954493891302, 0.4479523489749767, 0.5785693796994003, 0.5928842843003497, 0.21564612957558416, 0.865663468558759, 0.19254958646928366, 0.4547533405969081]}}
{"id": "0c93f3dd-492d-4577-a55c-1daa507ccb0a", "fitness": 0.3086646941139668, "name": "AdaptiveGaussianSearch", "description": "Adaptive Gaussian Search with a dynamic population size adjustment and adaptive learning rate for step size control, enhancing exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveGaussianSearch:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=20, initial_std=1.0, shrink_factor=0.99, success_history_length=10, pop_size_adapt_freq=50):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = initial_pop_size\n        self.pop_size = initial_pop_size\n        self.initial_std = initial_std\n        self.shrink_factor = shrink_factor\n        self.lb = -5.0\n        self.ub = 5.0\n        self.success_history_length = success_history_length\n        self.success_history = []\n        self.momentum = 0.1  # Momentum for std update\n        self.std = self.initial_std\n        self.v = 0 # velocity of std\n        self.eval_count = 0\n        self.pop_size_adapt_freq = pop_size_adapt_freq\n        self.learning_rate = 0.1\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.eval_count += self.pop_size\n        \n        # Find best initial solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n        \n        \n        generation = 0\n        while self.eval_count < self.budget:\n            # Generate offspring using Gaussian mutation\n            offspring = population + np.random.normal(0, self.std, size=(self.pop_size, self.dim))\n            \n            # Clip offspring to stay within bounds\n            offspring = np.clip(offspring, self.lb, self.ub)\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(x) for x in offspring])\n            self.eval_count += self.pop_size\n            \n            # Update best solution\n            best_offspring_index = np.argmin(offspring_fitness)\n            if offspring_fitness[best_offspring_index] < self.f_opt:\n                self.f_opt = offspring_fitness[best_offspring_index]\n                self.x_opt = offspring[best_offspring_index]\n                success = True\n            else:\n                success = False\n            \n            # Select survivors (replace the worst individuals with the best offspring)\n            worst_index = np.argmax(fitness)\n            best_offspring_index = np.argmin(offspring_fitness)\n            if offspring_fitness[best_offspring_index] < fitness[worst_index]:\n                population[worst_index] = offspring[best_offspring_index]\n                fitness[worst_index] = offspring_fitness[best_offspring_index]\n            \n            # Update success history\n            self.success_history.append(int(success))\n            if len(self.success_history) > self.success_history_length:\n                self.success_history.pop(0)\n            \n            # Adjust step size based on success rate\n            if len(self.success_history) == self.success_history_length:\n                success_rate = np.mean(self.success_history)\n                \n                # Adaptive Learning Rate\n                if success_rate > 0.4:\n                    lr = self.learning_rate * 1.1  # Increase learning rate if doing well\n                elif success_rate < 0.2:\n                    lr = self.learning_rate * 0.9  # Decrease learning rate if not doing well\n                else:\n                    lr = self.learning_rate\n                \n                lr = np.clip(lr, 0.01, 0.2)  # Keep learning rate within bounds\n                self.learning_rate = lr\n                \n                self.v = self.momentum * self.v + (1 - self.momentum) * (success_rate - 0.3) * lr  # Adjusted update\n                \n                self.std *= np.exp(self.v)\n                self.std = max(self.std, 1e-6) # Minimum std\n\n            # Adjust bounds (shrink towards the best solution)\n            self.lb = np.maximum(self.lb, self.x_opt - 2.5 * self.std)\n            self.ub = np.minimum(self.ub, self.x_opt + 2.5 * self.std)\n            \n            population = np.clip(population, self.lb, self.ub)\n\n            generation += 1\n            if generation % self.pop_size_adapt_freq == 0:\n                # Dynamically adjust population size\n                if success_rate > 0.4 and self.pop_size < 2 * self.initial_pop_size:\n                    self.pop_size = min(self.pop_size + 5, 2 * self.initial_pop_size)  # Increase if successful\n                    population = np.vstack((population, np.random.uniform(self.lb, self.ub, size=(5, self.dim))))\n                    new_fitness = np.array([func(x) for x in population[-5:]])\n                    fitness = np.concatenate((fitness, new_fitness))\n                    self.eval_count += 5\n\n                elif success_rate < 0.2 and self.pop_size > self.initial_pop_size // 2:\n                    self.pop_size = max(self.pop_size - 5, self.initial_pop_size // 2)  # Decrease if unsuccessful\n                    population = population[:self.pop_size]\n                    fitness = fitness[:self.pop_size]\n                \n            if self.eval_count >= self.budget:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveGaussianSearch scored 0.309 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5b02c097-5028-4d05-81c6-f566526b213d"], "operator": null, "metadata": {"aucs": [0.12427804417558186, 0.2527163506687712, 0.2966706649496116, 0.19635352626341251, 0.2798457577489689, 0.3750232773857174, 0.2958218141908108, 0.32411695494278636, 0.31464231614576543, 0.2113487440370928, 0.20198175930915863, 0.9967062728175011, 0.26442542450527395, 0.2359910875355009, 0.337256451121388, 0.35540847967458933, 0.2393030267316303, 0.17711879849590273, 0.1971064532150003, 0.4971786783648716]}}
{"id": "51b0f552-4859-4a3a-be4b-076f91cf5ceb", "fitness": 0.0, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with aging population, where old individuals are replaced by newly generated random individuals.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_base=0.5, CR_base=0.7, archive_size=100, F_range=0.3, CR_range=0.3, age_limit=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.archive_size = archive_size\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.archive = []\n        self.archive_fitness = []\n        self.age_limit = age_limit\n        self.population_ages = np.zeros(pop_size, dtype=int)\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                F = self.F_base + np.random.uniform(-self.F_range, self.F_range)\n                CR = self.CR_base + np.random.uniform(-self.CR_range, self.CR_range)\n                F = np.clip(F, 0.1, 1.0)\n                CR = np.clip(CR, 0.1, 1.0)\n\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = x1 + F * (x2 - x3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                    self.population_ages[i] = 0  # Reset age\n\n                    # Add replaced vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                         # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = population[i]\n                        self.archive_fitness[worst_archive_index] = fitness[i]\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                else:\n                    self.population_ages[i] += 1  # Increment age\n                     # Add trial vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = trial\n                        self.archive_fitness[worst_archive_index] = f_trial\n                        \n\n            population = new_population\n            fitness = new_fitness\n\n            # Replace old individuals\n            for i in range(self.pop_size):\n                if self.population_ages[i] > self.age_limit:\n                    population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                    fitness[i] = func(population[i])\n                    self.budget -= 1\n                    self.population_ages[i] = 0  # Reset age\n                    \n                    if fitness[i] < self.f_opt:\n                        self.f_opt = fitness[i]\n                        self.x_opt = population[i]\n            \n            # Restart population if stagnating\n            if generation % 50 == 0:\n                if np.std(fitness) < 1e-6:  # Stagnation criterion\n                    population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                    fitness = np.array([func(x) for x in population])\n                    self.budget -= self.pop_size\n                    self.population_ages = np.zeros(self.pop_size, dtype=int) # Reset all ages\n                    best_index = np.argmin(fitness)\n                    if fitness[best_index] < self.f_opt:\n                        self.f_opt = fitness[best_index]\n                        self.x_opt = population[best_index]\n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["03f49467-3003-4488-ab57-7dfaacfe2818"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "28a7246b-417c-4e89-90e2-6018f50375b8", "fitness": 0.49676193564399823, "name": "AdaptiveGaussianSearch", "description": "Adaptive Gaussian Search with covariance matrix adaptation and dynamic population size.", "code": "import numpy as np\n\nclass AdaptiveGaussianSearch:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=20, initial_std=1.0, target_success_rate=0.3, adaptation_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = initial_pop_size\n        self.pop_size = initial_pop_size\n        self.initial_std = initial_std\n        self.lb = -5.0\n        self.ub = 5.0\n        self.target_success_rate = target_success_rate\n        self.adaptation_rate = adaptation_rate\n        self.eval_count = 0\n        self.mean = np.zeros(dim)\n        self.C = np.eye(dim) * self.initial_std**2  # Covariance matrix\n        self.pc = np.zeros(dim)  # Evolution path for C\n        self.ps = np.zeros(dim)  # Evolution path for step size\n        self.chiN = np.sqrt(dim) * (1 - (1 / (4 * dim)) + (1 / (21 * dim**2)))  # Expectation of ||N(0,I)||\n        self.c_sigma = (self.adaptation_rate * (self.pop_size + 2)) / (dim + 3)\n        self.c_c = (4 + dim / 3) * self.adaptation_rate / (dim + 4)\n        self.c_mu = self.adaptation_rate * (self.pop_size - 2 + 1 / self.pop_size) / (np.power(dim + 2, 2))\n        self.d_sigma = 1 + 2 * max(0, np.sqrt((self.c_sigma * (dim - 1)) / (1 - self.c_sigma)) - 1)\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n        while self.eval_count < self.budget:\n            # Generate population\n            z = np.random.multivariate_normal(np.zeros(self.dim), self.C, size=self.pop_size)\n            population = self.mean + z\n            population = np.clip(population, self.lb, self.ub)\n            \n            # Evaluate population\n            fitness = np.array([func(x) for x in population])\n            self.eval_count += self.pop_size\n\n            # Find best solution in population\n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n            \n            # Sort population and fitness\n            indices = np.argsort(fitness)\n            population = population[indices]\n            fitness = fitness[indices]\n\n            # Update mean\n            old_mean = self.mean.copy()\n            self.mean = np.mean(population[:self.pop_size // 2], axis=0)\n            \n            # Cumulation for covariance matrix\n            self.ps = (1 - self.c_sigma) * self.ps + np.sqrt(self.c_sigma * (2 - self.c_sigma)) * np.linalg.solve(self.C, (self.mean - old_mean))\n            \n            # Cumulation for step size control\n            norm_ps = np.linalg.norm(self.ps)\n            self.pc = (1 - self.c_c) * self.pc + np.sqrt(self.c_c * (2 - self.c_c)) * (self.mean - old_mean)\n\n            # Update covariance matrix\n            delta = population[:self.pop_size // 2] - old_mean\n            self.C = (1 - self.c_mu) * self.C + self.c_mu * np.sum([np.outer(delta[i], delta[i]) for i in range(self.pop_size // 2)], axis=0)\n\n            # Ensure C is positive definite (numerical stability)\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = self.C + np.eye(self.dim) * 1e-6\n                \n            # Adapt population size\n            success_rate = (fitness[0] < self.f_opt)\n            if success_rate > self.target_success_rate:\n                self.pop_size = min(int(self.pop_size / (1 - self.adaptation_rate)), self.budget // 10)  # Ensure not too large pop_size\n            elif success_rate < self.target_success_rate:\n                self.pop_size = max(int(self.pop_size * (1 - self.adaptation_rate)), 2)\n\n            if self.eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveGaussianSearch scored 0.497 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5b02c097-5028-4d05-81c6-f566526b213d"], "operator": null, "metadata": {"aucs": [0.13077829915311945, 0.5383295257518823, 0.6197562016786351, 0.7849361328799161, 0.28361443409057774, 0.6792396094964115, 0.297314715665732, 0.3304610534110225, 0.6629684580290944, 0.17910777551403667, 0.819830758591644, 0.993906429566533, 0.29321237529793287, 0.36636533069794375, 0.6331064812765432, 0.4213729784108293, 0.47735563435315054, 0.7525541506265451, 0.1927196807099949, 0.47830868767841916]}}
{"id": "4945f4a0-dde9-4212-a7ee-3110f3fcdbc9", "fitness": 0.633857212166937, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with dynamic F and CR, an archive, and a local search component triggered by stagnation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_base=0.5, CR_base=0.7, archive_size=100, F_range=0.3, CR_range=0.3, local_search_radius=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.archive_size = archive_size\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.archive = []\n        self.archive_fitness = []\n        self.local_search_radius = local_search_radius\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n\n        generation = 0\n        stagnation_counter = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                F = self.F_base + np.random.uniform(-self.F_range, self.F_range)\n                CR = self.CR_base + np.random.uniform(-self.CR_range, self.CR_range)\n                F = np.clip(F, 0.1, 1.0)\n                CR = np.clip(CR, 0.1, 1.0)\n\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = x1 + F * (x2 - x3)\n\n                # Use archive\n                if np.random.rand() < 0.1 and len(self.archive) > 0:\n                    archive_index = np.random.randint(len(self.archive))\n                    mutant = x1 + F * (x2 - self.archive[archive_index])\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Selection\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n\n                    # Add replaced vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                         # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = population[i]\n                        self.archive_fitness[worst_archive_index] = fitness[i]\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                else:\n                     # Add trial vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = trial\n                        self.archive_fitness[worst_archive_index] = f_trial\n                        \n\n            population = new_population\n            fitness = new_fitness\n\n            # Stagnation check and local search\n            if generation % 50 == 0:\n                if np.std(fitness) < 1e-6:  # Stagnation criterion\n                    stagnation_counter += 1\n                    if stagnation_counter >= 2:\n                         # Perform local search around the best solution\n                        x_local = np.copy(self.x_opt)\n                        for _ in range(min(self.budget // 10, 100)):  # Limited local search budget\n                            x_new = x_local + np.random.uniform(-self.local_search_radius, self.local_search_radius, size=self.dim)\n                            x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n                            f_new = func(x_new)\n                            self.budget -= 1\n                            if f_new < self.f_opt:\n                                self.f_opt = f_new\n                                self.x_opt = x_new\n                                x_local = np.copy(x_new)  # Move center of local search\n\n                        # Restart population if local search wasn't effective\n                        if np.std(fitness) < 1e-6:\n                            population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                            fitness = np.array([func(x) for x in population])\n                            self.budget -= self.pop_size\n                            best_index = np.argmin(fitness)\n                            if fitness[best_index] < self.f_opt:\n                                self.f_opt = fitness[best_index]\n                                self.x_opt = population[best_index]\n                            stagnation_counter = 0 # Reset stagnation\n                else:\n                    stagnation_counter = 0 # Reset stagnation\n                \n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDE scored 0.634 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["03f49467-3003-4488-ab57-7dfaacfe2818"], "operator": null, "metadata": {"aucs": [0.23218018075771873, 0.4964889153181695, 0.5695269245458142, 0.8172810558005572, 0.6818012787099885, 0.7195821587110169, 0.5607847041880448, 0.5724353085047876, 0.6644127422796264, 0.6062661848693379, 0.7757937430532551, 0.9985194405715822, 0.43954127459321746, 0.6106163685938025, 0.8971144413028898, 0.7258962155082256, 0.5518635961856225, 0.7885439238977447, 0.4488206170780179, 0.5196751688693205]}}
{"id": "25366f48-13d5-40fc-9061-8a14bfe17980", "fitness": 0.6279251667558345, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with dynamic F and CR parameters, combined archive, and orthogonal learning for enhanced exploration.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_base=0.5, CR_base=0.7, archive_size=100, F_range=0.3, CR_range=0.3, orthogonal_learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.archive_size = archive_size\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.archive = []\n        self.archive_fitness = []\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                F = self.F_base + np.random.uniform(-self.F_range, self.F_range)\n                CR = self.CR_base + np.random.uniform(-self.CR_range, self.CR_range)\n                F = np.clip(F, 0.1, 1.0)\n                CR = np.clip(CR, 0.1, 1.0)\n\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = x1 + F * (x2 - x3)\n                \n                # Incorporate archive\n                if len(self.archive) > 0 and np.random.rand() < 0.1:  # 10% chance to use archive\n                    archive_index = np.random.randint(len(self.archive))\n                    mutant = x1 + F * (x2 - self.archive[archive_index])\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_learning_rate:\n                    orthogonal_vector = self.x_opt + np.random.normal(0, 0.1, self.dim)  # Perturb best solution\n                    orthogonal_vector = np.clip(orthogonal_vector, func.bounds.lb, func.bounds.ub)\n                    trial = 0.5 * (trial + orthogonal_vector)  # Combine with trial vector\n                    trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n\n                    # Add replaced vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                         # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = population[i]\n                        self.archive_fitness[worst_archive_index] = fitness[i]\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                else:\n                     # Add trial vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = trial\n                        self.archive_fitness[worst_archive_index] = f_trial\n                        \n\n            population = new_population\n            fitness = new_fitness\n\n            # Restart population if stagnating\n            if generation % 50 == 0:\n                if np.std(fitness) < 1e-6:  # Stagnation criterion\n                    population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                    fitness = np.array([func(x) for x in population])\n                    self.budget -= self.pop_size\n                    best_index = np.argmin(fitness)\n                    if fitness[best_index] < self.f_opt:\n                        self.f_opt = fitness[best_index]\n                        self.x_opt = population[best_index]\n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDE scored 0.628 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["03f49467-3003-4488-ab57-7dfaacfe2818"], "operator": null, "metadata": {"aucs": [0.2928451803711829, 0.6381578242636059, 0.6153607692274659, 0.8829041801923556, 0.702173837133109, 0.6922830403690905, 0.5536779851398481, 0.5837164037360356, 0.6841265480493006, 0.2698186732637612, 0.8280505889476644, 0.9977248996504063, 0.4352565613779158, 0.6782047409247458, 0.8373607792988622, 0.6878340644199852, 0.5686939433081446, 0.8151367475527946, 0.2919874318883633, 0.5031891360020525]}}
{"id": "530b2e0a-d6de-4212-8f6f-37bf3d188a16", "fitness": 0.5141803295249242, "name": "AdaptiveDE", "description": "Enhanced Adaptive Differential Evolution with a weighted difference vector in mutation and dynamic archive management for improved exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=100, F_init=0.5, CR_init=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F_init = F_init\n        self.CR_init = CR_init\n        self.F_memory = np.ones(self.pop_size) * self.F_init\n        self.CR_memory = np.ones(self.pop_size) * self.CR_init\n        self.archive = []\n        self.archive_fitness = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n\n        generation = 0\n        stagnation_counter = 0\n        prev_best_fitness = self.f_opt\n\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                self.F_memory[i] = np.clip(np.random.normal(0.5, 0.3), 0.1, 1.0)\n                self.CR_memory[i] = np.clip(np.random.normal(0.7, 0.1), 0.0, 1.0)\n\n                # Mutation\n                indices = np.random.choice(self.pop_size, 2, replace=False)\n                x1, x2 = population[indices]\n\n                # Utilize archive with dynamic probability\n                archive_prob = min(0.4, generation / 500)  # Increase archive usage over time\n                if len(self.archive) > 0 and np.random.rand() < archive_prob:\n                     archive_index = np.random.randint(len(self.archive))\n                     x3 = self.archive[archive_index]\n                else:\n                    indices = np.random.choice(self.pop_size, 1, replace=False)\n                    x3 = population[indices[0]]\n\n                # Weighted difference vector\n                mutant = population[i] + self.F_memory[i] * (x1 - x2) + self.F_memory[i] * 0.5 * (x3 - population[i])  # Reduced pull to archive\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR_memory[i] or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n\n                    # Dynamic archive management: replace worst in archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        if fitness[i] < self.archive_fitness[worst_archive_index]:\n                            self.archive[worst_archive_index] = population[i]\n                            self.archive_fitness[worst_archive_index] = fitness[i]\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        stagnation_counter = 0 # Reset stagnation counter\n                else:\n                    # Dynamic archive management: replace worst in archive (trial vector)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        if f_trial < self.archive_fitness[worst_archive_index]:\n                            self.archive[worst_archive_index] = trial\n                            self.archive_fitness[worst_archive_index] = f_trial\n                        \n\n            population = new_population\n            fitness = new_fitness\n\n            # Restart population if stagnating\n            if abs(self.f_opt - prev_best_fitness) < 1e-8: #More robust stagnation detection\n                stagnation_counter += 1\n            else:\n                stagnation_counter = 0\n            prev_best_fitness = self.f_opt\n\n            if stagnation_counter > 100: #Increased stagnation threshold\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                stagnation_counter = 0  # Reset after restart\n                self.archive = [] #Clear archive after restart\n                self.archive_fitness = []\n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDE scored 0.514 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d5ac6481-85f7-4ee8-b4a0-62c674969097"], "operator": null, "metadata": {"aucs": [0.16596866054305515, 0.3614802749891729, 0.49492209039485746, 0.7903914442944106, 0.4603530538793912, 0.6481967030312658, 0.33053357487708446, 0.4689828136038817, 0.5344267457645775, 0.2077880482830633, 0.7272757240006984, 0.9983903872051347, 0.4032267930085639, 0.3900873005192218, 0.8543008200653215, 0.6024125754122598, 0.4021474627667502, 0.740087204600169, 0.20465379296754793, 0.497981120292059]}}
{"id": "ea6757e4-95a0-4efc-8485-1e4cb3011b62", "fitness": 0.49981264652746293, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with dynamically adjusted F and CR based on population diversity and successful parameter settings, incorporating a probabilistic archive and a niching mechanism to promote exploration.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_base=0.5, CR_base=0.7, archive_size=50, F_range=0.3, CR_range=0.3, p_archive=0.1, niching_radius=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.archive_size = archive_size\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.archive = []\n        self.archive_fitness = []\n        self.p_archive = p_archive  # Probability of using archive vector\n        self.niching_radius = niching_radius  # Radius for niching\n\n        self.successful_F = []\n        self.successful_CR = []\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                if self.successful_F:\n                    F = np.clip(np.random.choice(self.successful_F), 0.1, 1.0)\n                else:\n                    F = self.F_base + np.random.uniform(-self.F_range, self.F_range)\n                    F = np.clip(F, 0.1, 1.0)\n                \n                if self.successful_CR:\n                    CR = np.clip(np.random.choice(self.successful_CR), 0.1, 1.0)\n                else:\n                    CR = self.CR_base + np.random.uniform(-self.CR_range, self.CR_range)\n                    CR = np.clip(CR, 0.1, 1.0)\n\n\n                # Mutation\n                indices = np.random.choice(self.pop_size, 2, replace=False)\n                x1, x2 = population[indices]\n\n                # Use archive with probability p_archive\n                if np.random.rand() < self.p_archive and self.archive:\n                    x3 = self.archive[np.random.randint(len(self.archive))]\n                else:\n                     indices = np.random.choice(self.pop_size, 1, replace=False)\n                     x3 = population[indices[0]]\n\n                mutant = x1 + F * (x2 - x3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                # Niching: Check distance to other individuals\n                distance = np.linalg.norm(trial - population, axis=1)\n                nearby_indices = np.where(distance < self.niching_radius)[0]\n\n                # Require improvement over all nearby individuals\n                if f_trial < fitness[i] and np.all(f_trial < fitness[nearby_indices]):\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n\n                    # Update successful F and CR\n                    self.successful_F.append(F)\n                    self.successful_CR.append(CR)\n\n                    # Add replaced vector to archive (probabilistic strategy)\n                    if np.random.rand() < 0.5:\n                        if len(self.archive) < self.archive_size:\n                            self.archive.append(population[i])\n                            self.archive_fitness.append(fitness[i])\n                        else:\n                            if self.archive_fitness: #Ensure archive not empty\n                                worst_archive_index = np.argmax(self.archive_fitness)\n                                self.archive[worst_archive_index] = population[i]\n                                self.archive_fitness[worst_archive_index] = fitness[i]\n\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                else:\n                    #Add trial vector to archive (probabilistic strategy)\n                    if np.random.rand() < 0.5:\n                        if len(self.archive) < self.archive_size:\n                            self.archive.append(trial)\n                            self.archive_fitness.append(f_trial)\n                        else:\n                            if self.archive_fitness:  # Ensure archive not empty\n                                worst_archive_index = np.argmax(self.archive_fitness)\n                                self.archive[worst_archive_index] = trial\n                                self.archive_fitness[worst_archive_index] = f_trial\n\n            population = new_population\n            fitness = new_fitness\n\n            # Clear successful F and CR every few generations\n            if generation % 20 == 0:\n                self.successful_F = []\n                self.successful_CR = []\n\n            # Restart population if stagnating\n            if generation % 50 == 0:\n                if np.std(fitness) < 1e-6:  # Stagnation criterion\n                    population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                    fitness = np.array([func(x) for x in population])\n                    self.budget -= self.pop_size\n                    best_index = np.argmin(fitness)\n                    if fitness[best_index] < self.f_opt:\n                        self.f_opt = fitness[best_index]\n                        self.x_opt = population[best_index]\n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDE scored 0.500 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["03f49467-3003-4488-ab57-7dfaacfe2818"], "operator": null, "metadata": {"aucs": [0.2341215479451585, 0.44002338377989936, 0.4423221413542059, 0.8273612860388023, 0.4060113520335301, 0.5070117903513103, 0.37945333980841234, 0.3757024742738895, 0.3938222089965613, 0.3385755634152898, 0.7736188567849284, 0.9993985656495231, 0.367801410937012, 0.39012653030289723, 0.8435583157830442, 0.5370075142886355, 0.39889374571574576, 0.5740101356204936, 0.25498784159947774, 0.5124449258704398]}}
{"id": "ffcaf479-7445-4cc1-a2c7-c04e41b8f515", "fitness": -Infinity, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with self-adaptive parameters, a ring topology, a combined archive, and orthogonal learning to enhance search efficiency and diversity.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=100, F_init=0.5, CR_init=0.7, ring_neighbors=3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F_init = F_init\n        self.CR_init = CR_init\n        self.F_memory = np.ones(self.pop_size) * self.F_init\n        self.CR_memory = np.ones(self.pop_size) * self.CR_init\n        self.archive = []\n        self.archive_fitness = []\n        self.ring_neighbors = ring_neighbors\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n\n        generation = 0\n        stagnation_counter = 0\n        prev_best_fitness = self.f_opt\n\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                self.F_memory[i] = np.clip(np.random.normal(0.5, 0.3), 0.1, 1.0)\n                self.CR_memory[i] = np.clip(np.random.normal(0.7, 0.1), 0.0, 1.0)\n\n                # Mutation - Ring Topology\n                neighbors = [(i + j) % self.pop_size for j in range(1, self.ring_neighbors + 1)]\n                indices = np.random.choice(neighbors, 2, replace=False)\n                x1, x2 = population[indices]\n\n                # Utilize archive with dynamic probability\n                archive_prob = min(0.4, generation / 500)  # Increase archive usage over time\n                if len(self.archive) > 0 and np.random.rand() < archive_prob:\n                     archive_index = np.random.randint(len(self.archive))\n                     x3 = self.archive[archive_index]\n                else:\n                    indices = np.random.choice(self.pop_size, 1, replace=False)\n                    x3 = population[indices[0]]\n\n                # Weighted difference vector\n                mutant = population[i] + self.F_memory[i] * (x1 - x2) + self.F_memory[i] * 0.5 * (x3 - population[i])  # Reduced pull to archive\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Orthogonal Learning\n                if np.random.rand() < 0.1:  # Probability of applying orthogonal learning\n                    H = self._generate_orthogonal_array(self.dim)\n                    orthogonal_trials = []\n                    for h in H:\n                        trial = population[i] + 0.1 * h * (func.bounds.ub - func.bounds.lb) #Scale orthogonal adjustments\n                        trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n                        orthogonal_trials.append(trial)\n\n                    orthogonal_fitness = [func(trial) for trial in orthogonal_trials]\n                    self.budget -= len(orthogonal_trials)\n                    best_orthogonal_index = np.argmin(orthogonal_fitness)\n                    if orthogonal_fitness[best_orthogonal_index] < fitness[i]:\n                        mutant = orthogonal_trials[best_orthogonal_index]\n                        \n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR_memory[i] or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n\n                    # Dynamic archive management: replace worst in archive\n                    combined_population = np.concatenate((population, np.array(self.archive)))\n                    combined_fitness = np.concatenate((fitness, np.array(self.archive_fitness)))\n\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        if fitness[i] < self.archive_fitness[worst_archive_index]:\n                            self.archive[worst_archive_index] = population[i]\n                            self.archive_fitness[worst_archive_index] = fitness[i]\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        stagnation_counter = 0 # Reset stagnation counter\n                else:\n                    # Dynamic archive management: replace worst in archive (trial vector)\n                    combined_population = np.concatenate((population, np.array(self.archive)))\n                    combined_fitness = np.concatenate((fitness, np.array(self.archive_fitness)))\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        if f_trial < self.archive_fitness[worst_archive_index]:\n                            self.archive[worst_archive_index] = trial\n                            self.archive_fitness[worst_archive_index] = f_trial\n                        \n\n            population = new_population\n            fitness = new_fitness\n\n            # Restart population if stagnating\n            if abs(self.f_opt - prev_best_fitness) < 1e-8: #More robust stagnation detection\n                stagnation_counter += 1\n            else:\n                stagnation_counter = 0\n            prev_best_fitness = self.f_opt\n\n            if stagnation_counter > 100: #Increased stagnation threshold\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                stagnation_counter = 0  # Reset after restart\n                self.archive = [] #Clear archive after restart\n                self.archive_fitness = []\n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt\n\n    def _generate_orthogonal_array(self, dim):\n        # Simple 2-level orthogonal array for demonstration (L8)\n        if dim <= 1:\n            return [[-1], [1]]\n        if dim <= 3:\n            return [\n                [-1, -1, -1],\n                [ 1, -1, -1],\n                [-1,  1, -1],\n                [ 1,  1, -1],\n                [-1, -1,  1],\n                [ 1, -1,  1],\n                [-1,  1,  1],\n                [ 1,  1,  1]\n            ]\n        \n        #For larger dimensions we will simply generate random binary combinations \n        num_points = 2 * dim  #Generate 2x the number of dims\n        orthogonal_array = np.random.choice([-1, 1], size=(num_points, dim))\n        return orthogonal_array.tolist()", "configspace": "", "generation": 3, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 114, in __call__\n  File \"<__array_function__ internals>\", line 200, in concatenate\nValueError: all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 1 dimension(s)\n.", "error": "", "parent_ids": ["530b2e0a-d6de-4212-8f6f-37bf3d188a16"], "operator": null, "metadata": {}}
{"id": "a4b600c5-a379-492e-92c9-66523fbc3a1c", "fitness": 0.0, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with a self-adaptive mutation strategy using both current-to-best and current-to-archive mutation, coupled with an aging archive and periodic orthogonal learning to escape local optima.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=100, F_init=0.5, CR_init=0.7, orthogonal_learning_freq=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F_init = F_init\n        self.CR_init = CR_init\n        self.F_memory = np.ones(self.pop_size) * self.F_init\n        self.CR_memory = np.ones(self.pop_size) * self.CR_init\n        self.archive = []\n        self.archive_fitness = []\n        self.orthogonal_learning_freq = orthogonal_learning_freq\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n        best_individual = population[best_index].copy()\n\n\n        generation = 0\n        stagnation_counter = 0\n        prev_best_fitness = self.f_opt\n\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                self.F_memory[i] = np.clip(np.random.normal(0.5, 0.3), 0.1, 1.0)\n                self.CR_memory[i] = np.clip(np.random.normal(0.7, 0.1), 0.0, 1.0)\n\n                # Mutation strategy: Self-adaptive current-to-best or current-to-archive\n                if np.random.rand() < 0.5:\n                    # Current-to-best\n                    indices = np.random.choice(self.pop_size, 2, replace=False)\n                    x1, x2 = population[indices]\n                    mutant = population[i] + self.F_memory[i] * (best_individual - population[i]) + self.F_memory[i] * (x1 - x2)\n                else:\n                    # Current-to-archive\n                    if len(self.archive) > 0:\n                        archive_index = np.random.randint(len(self.archive))\n                        x_archive = self.archive[archive_index]\n                        indices = np.random.choice(self.pop_size, 1, replace=False)\n                        x1 = population[indices[0]]\n                        mutant = population[i] + self.F_memory[i] * (x_archive - population[i]) + self.F_memory[i] * (x1 - population[i])\n                    else:\n                        # If archive is empty, revert to current-to-best with random individuals\n                        indices = np.random.choice(self.pop_size, 2, replace=False)\n                        x1, x2 = population[indices]\n                        mutant = population[i] + self.F_memory[i] * (best_individual - population[i]) + self.F_memory[i] * (x1 - x2)\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR_memory[i] or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n\n                    # Archive management: replace worst in archive or add if not full (Aging Archive)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                        #Aging mechanism: Give advantage to newer individuals\n                        weights = np.linspace(1.0, 0.1, len(self.archive_fitness))\n                        weighted_fitnesses = weights * np.array(self.archive_fitness)\n                        worst_archive_index = np.argmax(weighted_fitnesses)\n                        if f_trial < self.archive_fitness[worst_archive_index]:\n                            self.archive[worst_archive_index] = population[i]\n                            self.archive_fitness[worst_archive_index] = fitness[i]\n                            \n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        best_individual = trial.copy()\n                        stagnation_counter = 0 # Reset stagnation counter\n                else:\n                    # Archive management (trial vector): Replace worst in archive or add if not full\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                         #Aging mechanism: Give advantage to newer individuals\n                        weights = np.linspace(1.0, 0.1, len(self.archive_fitness))\n                        weighted_fitnesses = weights * np.array(self.archive_fitness)\n                        worst_archive_index = np.argmax(weighted_fitnesses)\n                        if f_trial < self.archive_fitness[worst_archive_index]:\n                            self.archive[worst_archive_index] = trial\n                            self.archive_fitness[worst_archive_index] = f_trial\n                        \n\n            population = new_population\n            fitness = new_fitness\n            \n            # Orthogonal Learning\n            if generation % self.orthogonal_learning_freq == 0:\n                self.perform_orthogonal_learning(func, population)\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                    best_individual = population[best_index].copy()\n\n            # Stagnation check and restart\n            if abs(self.f_opt - prev_best_fitness) < 1e-8: #More robust stagnation detection\n                stagnation_counter += 1\n            else:\n                stagnation_counter = 0\n            prev_best_fitness = self.f_opt\n\n            if stagnation_counter > 100: #Increased stagnation threshold\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                best_index = np.argmin(fitness)\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n                best_individual = population[best_index].copy()\n                stagnation_counter = 0  # Reset after restart\n                self.archive = [] #Clear archive after restart\n                self.archive_fitness = []\n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt\n\n    def perform_orthogonal_learning(self, func, population):\n            # Select a subset of individuals for orthogonal learning\n            num_ol_individuals = min(10, self.pop_size)  # Reduce the number for efficiency\n            ol_indices = np.random.choice(self.pop_size, num_ol_individuals, replace=False)\n\n            for i in ol_indices:\n                # Generate orthogonal array (simplified 2-level OA)\n                oa = np.array([[1, 1], [1, -1], [-1, 1], [-1, -1]])\n\n                # Select a subset of dimensions for orthogonal learning\n                num_ol_dimensions = min(5, self.dim) # Reduce the number for efficiency\n                ol_dims = np.random.choice(self.dim, num_ol_dimensions, replace=False)\n\n                # Create temporary solutions based on orthogonal array\n                temp_solutions = np.zeros((len(oa), self.dim))\n                for j in range(len(oa)):\n                    temp_solution = population[i].copy()\n                    for k, dim_index in enumerate(ol_dims):\n                        # Map OA values (1, -1) to the upper and lower bounds of the selected dimensions\n                        if oa[j, k % 2] == 1:\n                            temp_solution[dim_index] = func.bounds.ub[dim_index]\n                        else:\n                            temp_solution[dim_index] = func.bounds.lb[dim_index]\n                    temp_solutions[j] = temp_solution\n\n                # Evaluate temporary solutions\n                temp_fitnesses = np.array([func(x) for x in temp_solutions])\n                self.budget -= len(temp_solutions)\n\n                # Select the best solution among the temporary solutions\n                best_temp_index = np.argmin(temp_fitnesses)\n\n                # Replace the original individual with the best temporary solution\n                population[i] = temp_solutions[best_temp_index]", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["530b2e0a-d6de-4212-8f6f-37bf3d188a16"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "8d7b96f3-5136-4c17-87d1-c1f550c050de", "fitness": 0.0, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with orthogonal crossover, self-adaptive F and CR via exponential moving averages, and a simplified archive update.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_init=0.5, CR_init=0.7, archive_size=50, p_archive=0.1, ortho_group_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.CR = CR_init\n        self.archive_size = archive_size\n        self.archive = []\n        self.p_archive = p_archive\n        self.ortho_group_size = ortho_group_size\n        self.F_ema_alpha = 0.1\n        self.CR_ema_alpha = 0.1\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n\n        while self.budget > 0:\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n\n                # Use archive with probability p_archive\n                if np.random.rand() < self.p_archive and self.archive:\n                    x4 = self.archive[np.random.randint(len(self.archive))]\n                    mutant = x1 + self.F * (x2 - x3) + self.F * (x4 - population[i]) #Increased exploration by incorporating current individual to avoid premature convergence\n                else:\n                    mutant = x1 + self.F * (x2 - x3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Orthogonal Crossover\n                trial = np.copy(population[i])\n                group_indices = np.random.choice(self.dim, min(self.ortho_group_size, self.dim), replace=False)  # Select a subset of dimensions\n\n                for j in group_indices:\n                    if np.random.rand() < self.CR:\n                        trial[j] = mutant[j]\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n\n                    # Update F and CR using exponential moving average\n                    self.F = (1 - self.F_ema_alpha) * self.F + self.F_ema_alpha * self.F #No new F since we dont save succesful ones.\n                    self.CR = (1 - self.CR_ema_alpha) * self.CR + self.CR_ema_alpha * self.CR #Same for CR\n\n                    # Simplified Archive Update: Add to archive if better than worst in archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                    else:\n                        fitnesses = [func(x) for x in self.archive]\n                        worst_index = np.argmax(fitnesses)\n                        if fitness[i] < fitnesses[worst_index]:\n                            self.archive[worst_index] = population[i]  # Replace worst with current\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                else:\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                    else:\n                        fitnesses = [func(x) for x in self.archive]\n                        worst_index = np.argmax(fitnesses)\n                        if f_trial < fitnesses[worst_index]:\n                            self.archive[worst_index] = trial  # Replace worst with current\n\n            population = new_population\n            fitness = new_fitness\n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ea6757e4-95a0-4efc-8485-1e4cb3011b62"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "e006ef3b-3056-499a-8594-526f0fbfb8ab", "fitness": 0.0, "name": "AdaptiveDE", "description": "Enhanced Adaptive Differential Evolution with a self-adaptive mutation strategy, dynamic F and CR adaptation based on success history, and periodic orthogonal learning to boost exploitation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=100, F_init=0.5, CR_init=0.7, ortho_group_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F_init = F_init\n        self.CR_init = CR_init\n        self.F_memory = np.ones(self.pop_size) * self.F_init\n        self.CR_memory = np.ones(self.pop_size) * self.CR_init\n        self.archive = []\n        self.archive_fitness = []\n        self.success_F = []\n        self.success_CR = []\n        self.ortho_group_size = ortho_group_size  # Number of individuals for orthogonal learning\n        self.ortho_matrix = self._create_orthogonal_matrix(self.ortho_group_size)\n\n    def _create_orthogonal_matrix(self, size):\n        \"\"\"Creates an orthogonal matrix for orthogonal learning.\"\"\"\n        if size == 2:\n            return np.array([[1, 1], [1, -1]])\n        elif size == 3:\n            return np.array([[1, 1, 1], [1, -1, 0], [1, 1, -2]])\n        elif size == 4:\n             return np.array([[1, 1, 1, 1], [1, -1, 1, -1], [1, 1, -1, -1], [1, -1, -1, 1]])\n        elif size == 5:\n            return np.array([[1, 1, 1, 1, 1], [1, -1, 1, -1, 0], [1, 1, -2, 0, -1], [1, -1, -1, 1, 2], [1, 1, 1, 1, -4]])\n        else:\n            raise ValueError(\"Orthogonal matrix size must be 2, 3, 4 or 5.\")\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n\n        generation = 0\n        stagnation_counter = 0\n        prev_best_fitness = self.f_opt\n\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                if self.success_F:\n                    self.F_memory[i] = np.clip(np.random.choice(self.success_F), 0.1, 1.0)\n                else:\n                    self.F_memory[i] = np.clip(np.random.normal(0.5, 0.3), 0.1, 1.0)\n\n                if self.success_CR:\n                    self.CR_memory[i] = np.clip(np.random.choice(self.success_CR), 0.0, 1.0)\n                else:\n                    self.CR_memory[i] = np.clip(np.random.normal(0.7, 0.1), 0.0, 1.0)\n\n                # Mutation\n                indices = np.random.choice(self.pop_size, 2, replace=False)\n                x1, x2 = population[indices]\n\n                # Utilize archive with dynamic probability\n                archive_prob = min(0.4, generation / 500)  # Increase archive usage over time\n                if len(self.archive) > 0 and np.random.rand() < archive_prob:\n                     archive_index = np.random.randint(len(self.archive))\n                     x3 = self.archive[archive_index]\n                else:\n                    indices = np.random.choice(self.pop_size, 1, replace=False)\n                    x3 = population[indices[0]]\n\n                # Self-adaptive mutation strategy\n                p = np.random.rand()\n                if p < 0.33:\n                    mutant = population[i] + self.F_memory[i] * (x1 - x2)  # DE/rand/1\n                elif p < 0.66:\n                    mutant = self.x_opt + self.F_memory[i] * (x1 - x2)  # DE/best/1\n                else:\n                    mutant = population[i] + self.F_memory[i] * (x1 - x2) + self.F_memory[i] * (x3 - population[i]) #DE/rand/2\n                \n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR_memory[i] or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n\n                    # Update success memory for F and CR\n                    self.success_F.append(self.F_memory[i])\n                    self.success_CR.append(self.CR_memory[i])\n                    if len(self.success_F) > 10:\n                        self.success_F.pop(0)\n                        self.success_CR.pop(0)\n\n\n                    # Dynamic archive management: replace worst in archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        if fitness[i] < self.archive_fitness[worst_archive_index]:\n                            self.archive[worst_archive_index] = population[i]\n                            self.archive_fitness[worst_archive_index] = fitness[i]\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        stagnation_counter = 0 # Reset stagnation counter\n                else:\n                    # Dynamic archive management: replace worst in archive (trial vector)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        if f_trial < self.archive_fitness[worst_archive_index]:\n                            self.archive[worst_archive_index] = trial\n                            self.archive_fitness[worst_archive_index] = f_trial\n                        \n\n            population = new_population\n            fitness = new_fitness\n\n            # Orthogonal learning\n            if generation % 50 == 0:\n                group_indices = np.random.choice(self.pop_size, self.ortho_group_size, replace=False)\n                group = population[group_indices]\n                group_fitness = fitness[group_indices]\n                \n                means = np.mean(group, axis=0)\n                std = np.std(group, axis=0)\n                \n                levels = np.zeros_like(group)\n                for k in range(self.ortho_group_size):\n                  levels[k, :] = means + self.ortho_matrix[k, :] * std\n\n                new_group = np.clip(levels, func.bounds.lb, func.bounds.ub)\n                new_fitness_group = np.array([func(x) for x in new_group])\n                self.budget -= self.ortho_group_size\n\n                for k in range(self.ortho_group_size):\n                  if new_fitness_group[k] < group_fitness[k]:\n                    population[group_indices[k]] = new_group[k]\n                    fitness[group_indices[k]] = new_fitness_group[k]\n                    \n                    if new_fitness_group[k] < self.f_opt:\n                      self.f_opt = new_fitness_group[k]\n                      self.x_opt = new_group[k]\n\n            # Restart population if stagnating\n            if abs(self.f_opt - prev_best_fitness) < 1e-8: #More robust stagnation detection\n                stagnation_counter += 1\n            else:\n                stagnation_counter = 0\n            prev_best_fitness = self.f_opt\n\n            if stagnation_counter > 100: #Increased stagnation threshold\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                stagnation_counter = 0  # Reset after restart\n                self.archive = [] #Clear archive after restart\n                self.archive_fitness = []\n                self.success_F = []\n                self.success_CR = []\n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["530b2e0a-d6de-4212-8f6f-37bf3d188a16"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "ed7f9c33-1b66-4292-9d45-0e68c6ef09f3", "fitness": 0.1173700298592339, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with self-adaptive population size, dynamic F and CR adaptation based on successful values, archive usage, and a restart mechanism triggered by stagnation to diversify the search.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size_init=50, F_base=0.5, CR_base=0.7, archive_size=50, F_range=0.3, CR_range=0.3, p_archive=0.1, stagnation_threshold=1e-6, stagnation_generations=50, pop_size_reduction_factor=0.9, pop_size_increase_factor=1.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_init = pop_size_init\n        self.pop_size = pop_size_init\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.archive_size = archive_size\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.archive = []\n        self.archive_fitness = []\n        self.p_archive = p_archive\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_generations = stagnation_generations\n        self.pop_size_reduction_factor = pop_size_reduction_factor\n        self.pop_size_increase_factor = pop_size_increase_factor\n        self.successful_F = []\n        self.successful_CR = []\n        self.stagnation_counter = 0\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                if self.successful_F:\n                    F = np.clip(np.random.choice(self.successful_F), 0.1, 1.0)\n                else:\n                    F = self.F_base + np.random.uniform(-self.F_range, self.F_range)\n                    F = np.clip(F, 0.1, 1.0)\n                \n                if self.successful_CR:\n                    CR = np.clip(np.random.choice(self.successful_CR), 0.1, 1.0)\n                else:\n                    CR = self.CR_base + np.random.uniform(-self.CR_range, self.CR_range)\n                    CR = np.clip(CR, 0.1, 1.0)\n\n\n                # Mutation\n                indices = np.random.choice(self.pop_size, 2, replace=False)\n                x1, x2 = population[indices]\n\n                # Use archive with probability p_archive\n                if np.random.rand() < self.p_archive and self.archive:\n                    x3 = self.archive[np.random.randint(len(self.archive))]\n                else:\n                     indices = np.random.choice(self.pop_size, 1, replace=False)\n                     x3 = population[indices[0]]\n\n                mutant = x1 + F * (x2 - x3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n\n                    # Update successful F and CR\n                    self.successful_F.append(F)\n                    self.successful_CR.append(CR)\n\n                    # Add replaced vector to archive (probabilistic strategy)\n                    if np.random.rand() < 0.5:\n                        if len(self.archive) < self.archive_size:\n                            self.archive.append(population[i])\n                            self.archive_fitness.append(fitness[i])\n                        else:\n                            if self.archive_fitness: #Ensure archive not empty\n                                worst_archive_index = np.argmax(self.archive_fitness)\n                                self.archive[worst_archive_index] = population[i]\n                                self.archive_fitness[worst_archive_index] = fitness[i]\n\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                else:\n                    #Add trial vector to archive (probabilistic strategy)\n                    if np.random.rand() < 0.5:\n                        if len(self.archive) < self.archive_size:\n                            self.archive.append(trial)\n                            self.archive_fitness.append(f_trial)\n                        else:\n                            if self.archive_fitness:  # Ensure archive not empty\n                                worst_archive_index = np.argmax(self.archive_fitness)\n                                self.archive[worst_archive_index] = trial\n                                self.archive_fitness[worst_archive_index] = f_trial\n\n            population = new_population\n            fitness = new_fitness\n\n            # Clear successful F and CR every few generations\n            if generation % 20 == 0:\n                self.successful_F = []\n                self.successful_CR = []\n\n            # Check for stagnation\n            if np.std(fitness) < self.stagnation_threshold:\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n\n            # Restart or adjust population size if stagnating\n            if self.stagnation_counter >= self.stagnation_generations:\n                # Dynamically adjust population size\n                if self.pop_size > self.pop_size_init:\n                     self.pop_size = int(self.pop_size * self.pop_size_reduction_factor)\n                else:\n                     self.pop_size = int(self.pop_size * self.pop_size_increase_factor)\n                self.pop_size = max(10, min(self.pop_size, self.pop_size_init * 2)) # Limit population size\n\n                #Restart population\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                self.stagnation_counter = 0  # Reset stagnation counter\n            \n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDE scored 0.117 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ea6757e4-95a0-4efc-8485-1e4cb3011b62"], "operator": null, "metadata": {"aucs": [0.2347400597184678, 0]}}
{"id": "73a53131-1779-419e-8941-3a251e1ee96f", "fitness": 0.3064617966749489, "name": "AdaptiveDE", "description": "Enhanced Adaptive DE with a diversity-guided mutation strategy, adaptive local search frequency, and dynamic population sizing to balance exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_base=0.5, CR_base=0.7, archive_size=100, F_range=0.3, CR_range=0.3, local_search_radius=0.1, local_search_frequency=50, diversity_threshold=1e-6, pop_size_reduction_factor = 0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.initial_pop_size = pop_size # Store the initial population size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.archive_size = archive_size\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.archive = []\n        self.archive_fitness = []\n        self.local_search_radius = local_search_radius\n        self.local_search_frequency = local_search_frequency\n        self.diversity_threshold = diversity_threshold\n        self.pop_size_reduction_factor = pop_size_reduction_factor\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n\n        generation = 0\n        stagnation_counter = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n\n            # Calculate population diversity\n            diversity = np.std(fitness)\n            \n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                F = self.F_base + np.random.uniform(-self.F_range, self.F_range)\n                CR = self.CR_base + np.random.uniform(-self.CR_range, self.CR_range)\n                F = np.clip(F, 0.1, 1.0)\n                CR = np.clip(CR, 0.1, 1.0)\n\n                # Mutation: Diversity-guided strategy\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n\n                # If diversity is low, explore more using archive\n                if diversity < self.diversity_threshold and len(self.archive) > 0:\n                    archive_index = np.random.randint(len(self.archive))\n                    mutant = x1 + F * (x2 - self.archive[archive_index])  # Exploration using archive\n                else:\n                    mutant = x1 + F * (x2 - x3)  # Standard DE mutation\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Selection\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n\n                    # Add replaced vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                         # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = population[i]\n                        self.archive_fitness[worst_archive_index] = fitness[i]\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                else:\n                     # Add trial vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = trial\n                        self.archive_fitness[worst_archive_index] = f_trial\n                        \n\n            population = new_population\n            fitness = new_fitness\n            \n            # Adaptive Local Search\n            if generation % self.local_search_frequency == 0:\n                 # Adjust frequency based on diversity\n                if diversity < self.diversity_threshold:\n                    self.local_search_frequency = max(10, self.local_search_frequency // 2) # Increase frequency if stagnating\n                else:\n                    self.local_search_frequency = min(50, self.local_search_frequency * 2)  # Reduce frequency if diverse\n                \n                x_local = np.copy(self.x_opt)\n                local_search_evals = min(self.budget // 10, 100)\n                for _ in range(local_search_evals):  # Limited local search budget\n                    x_new = x_local + np.random.uniform(-self.local_search_radius, self.local_search_radius, size=self.dim)\n                    x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n                    f_new = func(x_new)\n                    self.budget -= 1\n                    if f_new < self.f_opt:\n                        self.f_opt = f_new\n                        self.x_opt = x_new\n                        x_local = np.copy(x_new)  # Move center of local search\n                        \n\n            # Dynamic Population Sizing\n            if generation % 100 == 0 and diversity < self.diversity_threshold and self.pop_size > 10:  # Reduce population if stagnating\n                self.pop_size = int(self.pop_size * self.pop_size_reduction_factor)\n                population = population[:self.pop_size]\n                fitness = fitness[:self.pop_size]\n                print(f\"Reducing pop size to {self.pop_size}\")\n            elif diversity > 5 * self.diversity_threshold and self.pop_size < self.initial_pop_size:\n                # Dynamically increase pop size based on diversity\n                self.pop_size = min(self.initial_pop_size, self.pop_size + 10)\n                print(f\"Increasing pop size to {self.pop_size}\")\n                new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(10, self.dim))\n                new_fitnesses = np.array([func(x) for x in new_individuals])\n                self.budget -= 10\n\n                population = np.vstack((population, new_individuals))\n                fitness = np.concatenate((fitness, new_fitnesses))\n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDE scored 0.306 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["4945f4a0-dde9-4212-a7ee-3110f3fcdbc9"], "operator": null, "metadata": {"aucs": [0.2475846345116689, 0.4255938046241743, 0.5526687475639523, 0]}}
{"id": "07e7f2e3-2613-4e72-9680-a92b4a935bfd", "fitness": 0.30215418951852113, "name": "AdaptiveDE", "description": "Enhanced Adaptive Differential Evolution with orthogonal crossover, periodic archive refreshment, and adaptive population size adjustment based on stagnation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=100, F_init=0.5, CR_init=0.7, ortho_group_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F_init = F_init\n        self.CR_init = CR_init\n        self.F_memory = np.ones(self.pop_size) * self.F_init\n        self.CR_memory = np.ones(self.pop_size) * self.CR_init\n        self.archive = []\n        self.archive_fitness = []\n        self.ortho_group_size = ortho_group_size\n        self.min_pop_size = 10\n        self.max_pop_size = 100\n\n    def orthogonal_crossover(self, x, mutant):\n        group_size = min(self.ortho_group_size, self.dim)\n        num_groups = self.dim // group_size\n        remainder = self.dim % group_size\n\n        trial = np.copy(x)\n        for i in range(num_groups):\n            start_index = i * group_size\n            end_index = start_index + group_size\n            selected_indices = np.arange(start_index, end_index)\n            \n            if np.random.rand() < 0.5: #Apply crossover with 50% probability to each group\n                for j in selected_indices:\n                    trial[j] = mutant[j]\n\n        #Handle remainder dimensions\n        if remainder > 0:\n            start_index = num_groups * group_size\n            selected_indices = np.arange(start_index, self.dim)\n            if np.random.rand() < 0.5: #Apply crossover to the remaining dims\n                for j in selected_indices:\n                    trial[j] = mutant[j]\n\n        return trial\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n\n        generation = 0\n        stagnation_counter = 0\n        prev_best_fitness = self.f_opt\n        archive_refresh_counter = 0\n        \n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                self.F_memory[i] = np.clip(np.random.normal(0.5, 0.3), 0.1, 1.0)\n                self.CR_memory[i] = np.clip(np.random.normal(0.7, 0.1), 0.0, 1.0)\n\n                # Mutation\n                indices = np.random.choice(self.pop_size, 2, replace=False)\n                x1, x2 = population[indices]\n\n                # Utilize archive with dynamic probability\n                archive_prob = min(0.4, generation / 500)  # Increase archive usage over time\n                if len(self.archive) > 0 and np.random.rand() < archive_prob:\n                     archive_index = np.random.randint(len(self.archive))\n                     x3 = self.archive[archive_index]\n                else:\n                    indices = np.random.choice(self.pop_size, 1, replace=False)\n                    x3 = population[indices[0]]\n\n                # Weighted difference vector\n                mutant = population[i] + self.F_memory[i] * (x1 - x2) + self.F_memory[i] * 0.5 * (x3 - population[i])  # Reduced pull to archive\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Orthogonal Crossover\n                trial = self.orthogonal_crossover(population[i], mutant)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n\n                    # Dynamic archive management: replace worst in archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        if fitness[i] < self.archive_fitness[worst_archive_index]:\n                            self.archive[worst_archive_index] = population[i]\n                            self.archive_fitness[worst_archive_index] = fitness[i]\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        stagnation_counter = 0 # Reset stagnation counter\n                else:\n                    # Dynamic archive management: replace worst in archive (trial vector)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        if f_trial < self.archive_fitness[worst_archive_index]:\n                            self.archive[worst_archive_index] = trial\n                            self.archive_fitness[worst_archive_index] = f_trial\n                        \n\n            population = new_population\n            fitness = new_fitness\n            \n            #Adaptive Population Size adjustment\n            if abs(self.f_opt - prev_best_fitness) < 1e-8:\n                stagnation_counter += 1\n                if self.pop_size > self.min_pop_size and stagnation_counter > 200:\n                    self.pop_size = max(self.min_pop_size, int(self.pop_size * 0.9))  # Reduce population\n                    population = population[:self.pop_size]\n                    fitness = fitness[:self.pop_size]\n                    self.F_memory = self.F_memory[:self.pop_size]\n                    self.CR_memory = self.CR_memory[:self.pop_size]\n                    stagnation_counter = 0\n            else:\n                stagnation_counter = 0\n                if self.pop_size < self.max_pop_size and generation % 50 == 0:\n                     self.pop_size = min(self.max_pop_size, int(self.pop_size * 1.1)) #Increase Population\n                     new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size - len(population), self.dim))\n                     new_fitness_vals = np.array([func(x) for x in new_individuals])\n                     self.budget -= len(new_fitness_vals)\n                     population = np.vstack((population, new_individuals))\n                     fitness = np.concatenate((fitness, new_fitness_vals))\n                     self.F_memory = np.concatenate((self.F_memory, np.ones(len(new_fitness_vals))*self.F_init))\n                     self.CR_memory = np.concatenate((self.CR_memory, np.ones(len(new_fitness_vals))*self.CR_init))\n\n\n\n            prev_best_fitness = self.f_opt\n\n            # Periodic Archive Refreshment\n            archive_refresh_counter += 1\n            if archive_refresh_counter > 300:\n                self.archive = []\n                self.archive_fitness = []\n                archive_refresh_counter = 0\n\n            # Restart population if stagnating\n            if stagnation_counter > 500: #Increased stagnation threshold\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                stagnation_counter = 0  # Reset after restart\n                self.archive = [] #Clear archive after restart\n                self.archive_fitness = []\n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDE scored 0.302 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["530b2e0a-d6de-4212-8f6f-37bf3d188a16"], "operator": null, "metadata": {"aucs": [0.1722048275965744, 0.33226155168663085, 0.39776329875312144, 0.6085412695562791, 0]}}
{"id": "d370398e-cdd7-4bf9-a471-526b7563e553", "fitness": 0.44701263286840076, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with orthogonal design for crossover, self-adaptive mutation scaling, and a quality-based archive update strategy.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=100, F_init=0.5, CR_init=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F_init = F_init\n        self.CR_init = CR_init\n        self.F_memory = np.ones(self.pop_size) * self.F_init\n        self.CR_memory = np.ones(self.pop_size) * self.CR_init\n        self.archive = []\n        self.archive_fitness = []\n        self.success_F = []\n        self.success_CR = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n\n        generation = 0\n        stagnation_counter = 0\n        prev_best_fitness = self.f_opt\n\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n            successful_mutations = 0\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                F = np.clip(np.random.normal(0.5, 0.3), 0.1, 1.0)\n                CR = np.clip(np.random.normal(0.7, 0.1), 0.0, 1.0)\n\n                # Mutation\n                indices = np.random.choice(self.pop_size, 2, replace=False)\n                x1, x2 = population[indices]\n\n                # Utilize archive with dynamic probability\n                archive_prob = min(0.4, generation / 500)  # Increase archive usage over time\n                if len(self.archive) > 0 and np.random.rand() < archive_prob:\n                     archive_index = np.random.randint(len(self.archive))\n                     x3 = self.archive[archive_index]\n                else:\n                    indices = np.random.choice(self.pop_size, 1, replace=False)\n                    x3 = population[indices[0]]\n\n                # Weighted difference vector\n                mutant = population[i] + F * (x1 - x2) + F * 0.5 * (x3 - population[i])  # Reduced pull to archive\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Orthogonal Crossover\n                trial = np.copy(population[i])\n                orthogonal_matrix = self.generate_orthogonal_array(self.dim)\n                for j in range(self.dim):\n                    if orthogonal_matrix[0, j] == 1:\n                        trial[j] = mutant[j]\n                \n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                    successful_mutations += 1\n                    self.success_F.append(F)\n                    self.success_CR.append(CR)\n\n\n                    # Dynamic archive management: replace worst in archive, only if improvement is significant\n                    if len(self.archive) < self.archive_size or fitness[i] - f_trial > 1e-5:\n                        if len(self.archive) < self.archive_size:\n                            self.archive.append(population[i])\n                            self.archive_fitness.append(fitness[i])\n                        else:\n                            worst_archive_index = np.argmax(self.archive_fitness)\n                            if fitness[i] > f_trial:\n                                self.archive[worst_archive_index] = population[i]\n                                self.archive_fitness[worst_archive_index] = fitness[i]\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        stagnation_counter = 0 # Reset stagnation counter\n                else:\n                    # Dynamic archive management (trial vector)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        if f_trial < self.archive_fitness[worst_archive_index]:\n                            self.archive[worst_archive_index] = trial\n                            self.archive_fitness[worst_archive_index] = f_trial\n\n            # Update F and CR memory based on successful mutations (if any)\n            if len(self.success_F) > 0:\n                self.F_memory[i] = np.mean(self.success_F)\n                self.CR_memory[i] = np.mean(self.success_CR)\n                self.success_F = []\n                self.success_CR = []\n\n\n            population = new_population\n            fitness = new_fitness\n\n            # Restart population if stagnating\n            if abs(self.f_opt - prev_best_fitness) < 1e-8: #More robust stagnation detection\n                stagnation_counter += 1\n            else:\n                stagnation_counter = 0\n            prev_best_fitness = self.f_opt\n\n            if stagnation_counter > 100: #Increased stagnation threshold\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                stagnation_counter = 0  # Reset after restart\n                self.archive = [] #Clear archive after restart\n                self.archive_fitness = []\n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt\n\n    def generate_orthogonal_array(self, n):\n        # Use a simplified Hadamard matrix approach for demonstration.\n        # For larger n, consider using dedicated orthogonal array libraries.\n        if n <= 1:\n            return np.ones((1, n))\n        if n == 2:\n            return np.array([[1, 1], [1, -1]])\n        \n        # For dimensions > 2, creating an orthogonal array becomes more complex\n        # A simple strategy is to create a random binary array\n        orthogonal_array = np.random.randint(0, 2, size=(1, n))\n        return orthogonal_array", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDE scored 0.447 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["530b2e0a-d6de-4212-8f6f-37bf3d188a16"], "operator": null, "metadata": {"aucs": [0.16331061139462089, 0.28411504581060776, 0.4260540833235974, 0.6153712548031687, 0.3505858526886314, 0.5733966850764236, 0.3209336913797892, 0.41239106202672005, 0.4174144981483958, 0.20256702690509276, 0.6035314270812248, 0.9988082210725484, 0.2758015445743185, 0.3393538290046121, 0.7429780466940303, 0.5189313218289184, 0.346570637636911, 0.6494825177321671, 0.20618263077531618, 0.4924726694109207]}}
{"id": "9e7acf1c-bcc9-4a3f-b347-c892323d9cef", "fitness": 0.45932028006343756, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with orthogonal crossover, self-adaptive parameters, and a combined archive with periodic clearing to boost exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=100, F_init=0.5, CR_init=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F_init = F_init\n        self.CR_init = CR_init\n        self.F_memory = np.ones(self.pop_size) * self.F_init\n        self.CR_memory = np.ones(self.pop_size) * self.CR_init\n        self.archive = []\n        self.archive_fitness = []\n        self.archive_age = [] # Track age of individuals in archive\n        self.success_F = []\n        self.success_CR = []\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n\n        generation = 0\n        stagnation_counter = 0\n        prev_best_fitness = self.f_opt\n        archive_clear_interval = 200 # Clear the archive periodically\n        \n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR using success history\n                if self.success_F:\n                    F_mean = np.mean(self.success_F)\n                    self.F_memory[i] = np.clip(np.random.normal(F_mean, 0.3), 0.1, 1.0)\n                else:\n                    self.F_memory[i] = np.clip(np.random.normal(0.5, 0.3), 0.1, 1.0)\n\n                if self.success_CR:\n                    CR_mean = np.mean(self.success_CR)\n                    self.CR_memory[i] = np.clip(np.random.normal(CR_mean, 0.1), 0.0, 1.0)\n                else:\n                    self.CR_memory[i] = np.clip(np.random.normal(0.7, 0.1), 0.0, 1.0)\n\n\n                # Mutation\n                indices = np.random.choice(self.pop_size, 2, replace=False)\n                x1, x2 = population[indices]\n\n                # Utilize archive with dynamic probability\n                archive_prob = min(0.4, generation / 500)  # Increase archive usage over time\n                if len(self.archive) > 0 and np.random.rand() < archive_prob:\n                     archive_index = np.random.randint(len(self.archive))\n                     x3 = self.archive[archive_index]\n                else:\n                    indices = np.random.choice(self.pop_size, 1, replace=False)\n                    x3 = population[indices[0]]\n\n                # Weighted difference vector\n                mutant = population[i] + self.F_memory[i] * (x1 - x2) + self.F_memory[i] * 0.5 * (x3 - population[i])  # Reduced pull to archive\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Orthogonal Crossover\n                trial = np.copy(population[i])\n                num_changed_vars = 0\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR_memory[i]:\n                        trial[j] = mutant[j]\n                        num_changed_vars += 1\n                # Ensure at least one variable is changed\n                if num_changed_vars == 0:\n                    j_rand = np.random.randint(self.dim)\n                    trial[j_rand] = mutant[j_rand]\n                \n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n\n                    self.success_F.append(self.F_memory[i])\n                    self.success_CR.append(self.CR_memory[i])\n                    if len(self.success_F) > 10:\n                        self.success_F.pop(0)\n                        self.success_CR.pop(0)\n\n                    # Dynamic archive management: replace worst in archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                        self.archive_fitness.append(fitness[i])\n                        self.archive_age.append(0)\n                    else:\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        if fitness[i] < self.archive_fitness[worst_archive_index]:\n                            self.archive[worst_archive_index] = population[i]\n                            self.archive_fitness[worst_archive_index] = fitness[i]\n                            self.archive_age[worst_archive_index] = 0\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        stagnation_counter = 0 # Reset stagnation counter\n                else:\n                    # Dynamic archive management: replace worst in archive (trial vector)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                        self.archive_age.append(0)\n                    else:\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        if f_trial < self.archive_fitness[worst_archive_index]:\n                            self.archive[worst_archive_index] = trial\n                            self.archive_fitness[worst_archive_index] = f_trial\n                            self.archive_age[worst_archive_index] = 0\n\n            # Update age of archive members\n            self.archive_age = [age + 1 for age in self.archive_age]\n\n\n            population = new_population\n            fitness = new_fitness\n\n            # Restart population if stagnating\n            if abs(self.f_opt - prev_best_fitness) < 1e-8: #More robust stagnation detection\n                stagnation_counter += 1\n            else:\n                stagnation_counter = 0\n            prev_best_fitness = self.f_opt\n\n            if stagnation_counter > 100: #Increased stagnation threshold\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                stagnation_counter = 0  # Reset after restart\n                self.archive = [] #Clear archive after restart\n                self.archive_fitness = []\n                self.archive_age = []\n                self.success_F = []\n                self.success_CR = []\n\n            # Periodic archive clearing\n            if generation % archive_clear_interval == 0:\n                #Remove old or redundant entries to increase diversity\n                if len(self.archive) > 0:\n                    max_age = np.max(self.archive_age)\n                    to_remove = []\n                    for k in range(len(self.archive)):\n                        if self.archive_age[k] >= max_age:\n                            to_remove.append(k)\n\n                    for k in sorted(to_remove, reverse=True):\n                        del self.archive[k]\n                        del self.archive_fitness[k]\n                        del self.archive_age[k]\n\n\n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDE scored 0.459 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["530b2e0a-d6de-4212-8f6f-37bf3d188a16"], "operator": null, "metadata": {"aucs": [0.1550419926004586, 0.38507828723564497, 0.43177903685284114, 0.38540680980221464, 0.3579166421424237, 0.5342874304461731, 0.3168754107212475, 0.34845609292932844, 0.4388109351488032, 0.2122866655120499, 0.6659552912796265, 0.9960949097799894, 0.5329509280113219, 0.38158157577970897, 0.7675050417557449, 0.4538114237391575, 0.41206685680782784, 0.7092491016785001, 0.20062680054714888, 0.5006243684985396]}}
{"id": "fe75737f-ada7-482f-8df1-beab0b17ae44", "fitness": 0.5833307637375909, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with momentum-based mutation, self-adaptive crossover, and periodic population rejuvenation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_base=0.5, CR_base=0.7, archive_size=100, F_range=0.3, CR_range=0.3, local_search_radius=0.1, momentum=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.archive_size = archive_size\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.archive = []\n        self.archive_fitness = []\n        self.local_search_radius = local_search_radius\n        self.momentum = momentum\n        self.velocity = np.zeros((pop_size, dim)) # Initialize velocity for momentum\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n\n        generation = 0\n        stagnation_counter = 0\n        CR_history = np.full(self.pop_size, self.CR_base) #CR history for self-adaptation\n\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                F = self.F_base + np.random.uniform(-self.F_range, self.F_range)\n                F = np.clip(F, 0.1, 1.0)\n\n                # Self-adaptive CR\n                CR = CR_history[i] + np.random.normal(0, self.CR_range)\n                CR = np.clip(CR, 0.1, 0.9)\n\n                # Mutation with momentum\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                \n                # Momentum update\n                self.velocity[i] = self.momentum * self.velocity[i] + F * (x2 - x3)\n                mutant = x1 + self.velocity[i]\n\n                # Use archive\n                if np.random.rand() < 0.1 and len(self.archive) > 0:\n                    archive_index = np.random.randint(len(self.archive))\n                    mutant = x1 + F * (x2 - self.archive[archive_index])\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Selection\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                    CR_history[i] = CR # store successful CR\n\n                    # Add replaced vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                         # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = population[i]\n                        self.archive_fitness[worst_archive_index] = fitness[i]\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                else:\n                     # Add trial vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = trial\n                        self.archive_fitness[worst_archive_index] = f_trial\n                        \n\n            population = new_population\n            fitness = new_fitness\n\n            # Stagnation check and periodic population rejuvenation\n            if generation % 50 == 0:\n                if np.std(fitness) < 1e-6:  # Stagnation criterion\n                    stagnation_counter += 1\n                    if stagnation_counter >= 2:\n                         # Perform local search around the best solution\n                        x_local = np.copy(self.x_opt)\n                        for _ in range(min(self.budget // 10, 100)):  # Limited local search budget\n                            x_new = x_local + np.random.uniform(-self.local_search_radius, self.local_search_radius, size=self.dim)\n                            x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n                            f_new = func(x_new)\n                            self.budget -= 1\n                            if f_new < self.f_opt:\n                                self.f_opt = f_new\n                                self.x_opt = x_new\n                                x_local = np.copy(x_new)  # Move center of local search\n                        stagnation_counter = 0 # Reset stagnation\n\n                else:\n                    stagnation_counter = 0 # Reset stagnation\n\n                #Periodic rejuvenation\n                if generation % 200 == 0:\n                    # Replace a fraction of the population with new random solutions\n                    num_rejuvenated = int(0.2 * self.pop_size)\n                    new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(num_rejuvenated, self.dim))\n                    new_fitnesses = np.array([func(x) for x in new_individuals])\n                    self.budget -= num_rejuvenated\n\n                    worst_indices = np.argsort(fitness)[-num_rejuvenated:]  # Indices of worst individuals\n                    population[worst_indices] = new_individuals\n                    fitness[worst_indices] = new_fitnesses\n\n                    # Update optimal solution\n                    best_index = np.argmin(fitness)\n                    if fitness[best_index] < self.f_opt:\n                        self.f_opt = fitness[best_index]\n                        self.x_opt = population[best_index]\n                \n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDE scored 0.583 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["4945f4a0-dde9-4212-a7ee-3110f3fcdbc9"], "operator": null, "metadata": {"aucs": [0.1949500980207991, 0.42550940248231306, 0.536749580812252, 0.7756918050517803, 0.5856997322125513, 0.6983064780715682, 0.4983927668656942, 0.5202359016070449, 0.6235043994210673, 0.5542907235032183, 0.7578191427253063, 0.9827431542147332, 0.4474425504694961, 0.5694473075322253, 0.8519322852128717, 0.7186480508334281, 0.45755094555967357, 0.743949377469513, 0.21713374028212384, 0.5066178324041543]}}
{"id": "8d38d9e7-e3ad-441e-b2eb-654048c3c097", "fitness": 0.7056355090554378, "name": "AdaptiveDE", "description": "Enhanced Adaptive Differential Evolution with self-adaptive F and CR, a combined archive, orthogonal learning, and a diversity-based population restart strategy to balance exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_base=0.5, CR_base=0.7, archive_size=100, F_range=0.3, CR_range=0.3, orthogonal_learning_rate=0.1, restart_patience=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.archive_size = archive_size\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.archive = []\n        self.archive_fitness = []\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.restart_patience = restart_patience\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.F_history = []\n        self.CR_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n            self.stagnation_counter = 0\n            self.previous_best_fitness = self.f_opt\n        else:\n            self.stagnation_counter +=1\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n            \n            # Self-adaptive F and CR\n            if self.F_history:\n                self.F_base = np.mean(self.F_history)\n            if self.CR_history:\n                self.CR_base = np.mean(self.CR_history)\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                F = self.F_base + np.random.uniform(-self.F_range, self.F_range)\n                CR = self.CR_base + np.random.uniform(-self.CR_range, self.CR_range)\n                F = np.clip(F, 0.1, 1.0)\n                CR = np.clip(CR, 0.1, 1.0)\n\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = x1 + F * (x2 - x3)\n                \n                # Incorporate archive\n                if len(self.archive) > 0 and np.random.rand() < 0.1:  # 10% chance to use archive\n                    archive_index = np.random.randint(len(self.archive))\n                    mutant = x1 + F * (x2 - self.archive[archive_index])\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_learning_rate:\n                    orthogonal_vector = self.x_opt + np.random.normal(0, 0.1, self.dim)  # Perturb best solution\n                    orthogonal_vector = np.clip(orthogonal_vector, func.bounds.lb, func.bounds.ub)\n                    trial = 0.5 * (trial + orthogonal_vector)  # Combine with trial vector\n                    trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                    \n                    # Record successful F and CR values\n                    self.F_history.append(F)\n                    self.CR_history.append(CR)\n                    if len(self.F_history) > 50:\n                        self.F_history.pop(0)\n                        self.CR_history.pop(0)\n\n                    # Add replaced vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                         # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = population[i]\n                        self.archive_fitness[worst_archive_index] = fitness[i]\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.stagnation_counter = 0\n                        self.previous_best_fitness = self.f_opt\n                else:\n                     # Add trial vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = trial\n                        self.archive_fitness[worst_archive_index] = f_trial\n                        \n\n            population = new_population\n            fitness = new_fitness\n            \n            # Restart population if stagnating\n            if self.stagnation_counter > self.restart_patience:\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                    self.stagnation_counter = 0\n                    self.previous_best_fitness = self.f_opt\n                else:\n                    self.stagnation_counter +=1\n                    \n                # Reset F and CR history\n                self.F_history = []\n                self.CR_history = []\n                \n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDE scored 0.706 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["25366f48-13d5-40fc-9061-8a14bfe17980"], "operator": null, "metadata": {"aucs": [0.38530921472908397, 0.7694621250834512, 0.5416050356084519, 0.901878449700934, 0.756860763182652, 0.8054062025112502, 0.7016930245692362, 0.6764239356293652, 0.7510148926047618, 0.735761896331148, 0.9019817813525587, 0.9910199039115853, 0.466720331950082, 0.7448558687960114, 0.8527199633272429, 0.7991599524621978, 0.6039832966143488, 0.8448215661970941, 0.37962227974126195, 0.502409696806039]}}
{"id": "a36df3c3-5d99-4d76-9027-ab316b352bb3", "fitness": 0.7003301296421235, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with self-adaptive mutation strategies and learning rate, adaptive population size, and a restart mechanism to escape local optima.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size_min=20, pop_size_max=100, F_base=0.5, CR_base=0.7, archive_size=50, F_range=0.3, CR_range=0.3, p_archive=0.1, lr=0.1, restart_patience=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_min = pop_size_min\n        self.pop_size_max = pop_size_max\n        self.pop_size = pop_size_max  # Start with a larger population size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.archive_size = archive_size\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.archive = []\n        self.archive_fitness = []\n        self.p_archive = p_archive\n        self.lr = lr  # Learning rate for F and CR adaptation\n        self.restart_patience = restart_patience  # Patience for stagnation detection\n        self.stagnation_counter = 0\n\n        self.successful_F = []\n        self.successful_CR = []\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR with learning rate\n                if self.successful_F:\n                    F = np.clip(np.mean(self.successful_F) + self.lr * np.random.randn(), 0.1, 1.0)\n                else:\n                    F = self.F_base + np.random.uniform(-self.F_range, self.F_range)\n                    F = np.clip(F, 0.1, 1.0)\n                \n                if self.successful_CR:\n                    CR = np.clip(np.mean(self.successful_CR) + self.lr * np.random.randn(), 0.1, 1.0)\n                else:\n                    CR = self.CR_base + np.random.uniform(-self.CR_range, self.CR_range)\n                    CR = np.clip(CR, 0.1, 1.0)\n\n                # Mutation Strategy: Adaptive selection\n                rand = np.random.rand()\n                if rand < 0.33:\n                    # DE/rand/1\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x1, x2, x3 = population[indices]\n                    mutant = x1 + F * (x2 - x3)\n                elif rand < 0.66:\n                    # DE/current-to-best/1\n                    indices = np.random.choice(self.pop_size, 2, replace=False)\n                    x1, x2 = population[indices]\n                    mutant = population[i] + F * (self.x_opt - population[i]) + F * (x1 - x2)\n                else:\n                    # DE/best/1\n                    indices = np.random.choice(self.pop_size, 2, replace=False)\n                    x1, x2 = population[indices]\n                    mutant = self.x_opt + F * (x1 - x2)\n\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n\n                    # Update successful F and CR\n                    self.successful_F.append(F)\n                    self.successful_CR.append(CR)\n\n                    # Add replaced vector to archive (probabilistic strategy)\n                    if np.random.rand() < 0.5:\n                        if len(self.archive) < self.archive_size:\n                            self.archive.append(population[i])\n                            self.archive_fitness.append(fitness[i])\n                        else:\n                            if self.archive_fitness: #Ensure archive not empty\n                                worst_archive_index = np.argmax(self.archive_fitness)\n                                self.archive[worst_archive_index] = population[i]\n                                self.archive_fitness[worst_archive_index] = fitness[i]\n\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            population = new_population\n            fitness = new_fitness\n\n            # Clear successful F and CR every few generations\n            if generation % 20 == 0:\n                self.successful_F = []\n                self.successful_CR = []\n\n            # Check for stagnation and potentially restart or reduce population size\n            if np.std(fitness) < 1e-6:\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n\n            if self.stagnation_counter > self.restart_patience:\n                #Option 1: Restart\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                self.stagnation_counter = 0\n\n                #Option 2: Reduce population size (if possible)\n                #self.pop_size = max(self.pop_size_min, int(self.pop_size * 0.8)) #Reduce by 20%\n                #population = population[:self.pop_size] #Truncate\n                #fitness = fitness[:self.pop_size]\n                #print(\"Reduced population size to:\", self.pop_size)\n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDE scored 0.700 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ea6757e4-95a0-4efc-8485-1e4cb3011b62"], "operator": null, "metadata": {"aucs": [0.3136127289135292, 0.8115303160161891, 0.7201855915462965, 0.8690843850818245, 0.6893143307978085, 0.6782884752398514, 0.6027777865304876, 0.5768905841055878, 0.7828678831358299, 0.6193596057718728, 0.8696571580005575, 0.9979289413917766, 0.663181854806014, 0.7882954785526827, 0.9167611811682508, 0.7788059412587105, 0.7302686831461568, 0.7665331150984582, 0.3232961664154418, 0.5079623858651443]}}
{"id": "b6a3eb5a-1a80-4006-88a7-b926c38a0953", "fitness": -Infinity, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with a dynamically adjusted population size based on fitness diversity, combined with an improved archive handling and orthogonal learning for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size_base=50, F_base=0.5, CR_base=0.7, archive_size=100, F_range=0.3, CR_range=0.3, local_search_radius=0.1, momentum=0.1, ortho_group_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_base = pop_size_base\n        self.pop_size = pop_size_base  # Initial population size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.archive_size = archive_size\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.archive = []\n        self.archive_fitness = []\n        self.local_search_radius = local_search_radius\n        self.momentum = momentum\n        self.velocity = np.zeros((pop_size_base, dim)) # Initialize velocity for momentum\n        self.ortho_group_size = ortho_group_size  # Size of orthogonal groups\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n\n        generation = 0\n        stagnation_counter = 0\n        CR_history = np.full(self.pop_size, self.CR_base) #CR history for self-adaptation\n\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n\n            # Dynamic population size adjustment\n            fitness_std = np.std(fitness)\n            if fitness_std > 1e-3:\n                self.pop_size = min(self.pop_size_base * 2, self.budget // 10)  # Increase population if diverse\n            else:\n                self.pop_size = max(self.pop_size_base // 2, 10)  # Reduce population if stagnant\n\n            if self.pop_size != population.shape[0]:\n                 # Resize population\n                if self.pop_size > population.shape[0]:\n                    # Add new random individuals\n                    num_new = self.pop_size - population.shape[0]\n                    new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(num_new, self.dim))\n                    new_fitnesses = np.array([func(x) for x in new_individuals])\n                    self.budget -= num_new\n                    population = np.vstack((population, new_individuals))\n                    fitness = np.concatenate((fitness, new_fitnesses))\n                    CR_history = np.concatenate((CR_history, np.full(num_new, self.CR_base)))\n                    self.velocity = np.vstack((self.velocity, np.zeros((num_new, self.dim))))\n                else:\n                    # Remove worst individuals\n                    num_remove = population.shape[0] - self.pop_size\n                    worst_indices = np.argsort(fitness)[-num_remove:]\n                    remaining_indices = np.setdiff1d(np.arange(population.shape[0]), worst_indices)\n                    population = population[remaining_indices]\n                    fitness = fitness[remaining_indices]\n                    CR_history = CR_history[remaining_indices]\n                    self.velocity = self.velocity[remaining_indices]\n\n\n            for i in range(population.shape[0]): #Iterate over the current population\n                # Adaptive F and CR\n                F = self.F_base + np.random.uniform(-self.F_range, self.F_range)\n                F = np.clip(F, 0.1, 1.0)\n\n                # Self-adaptive CR\n                CR = CR_history[i] + np.random.normal(0, self.CR_range)\n                CR = np.clip(CR, 0.1, 0.9)\n\n                # Mutation with momentum\n                indices = np.random.choice(population.shape[0], 3, replace=False)\n                x1, x2, x3 = population[indices]\n                \n                # Momentum update\n                self.velocity[i] = self.momentum * self.velocity[i] + F * (x2 - x3)\n                mutant = x1 + self.velocity[i]\n\n                # Use archive\n                if np.random.rand() < 0.1 and len(self.archive) > 0:\n                    archive_index = np.random.randint(len(self.archive))\n                    mutant = x1 + F * (x2 - self.archive[archive_index])\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Orthogonal learning within a small group\n                if (i % self.ortho_group_size == 0) and (i + self.ortho_group_size <= population.shape[0]):\n                    group = population[i:i + self.ortho_group_size]\n                    group_trials = np.copy(group)\n                    for k in range(self.ortho_group_size):\n                        j_rand_ortho = np.random.randint(self.dim)\n                        for j in range(self.dim):\n                            if np.random.rand() < CR or j == j_rand_ortho:\n                                group_trials[k, j] = mutant[j]\n                                group_trials[k] = np.clip(group_trials[k], func.bounds.lb, func.bounds.ub)\n                    \n                    group_fitness = np.array([func(x) if self.budget > 0 else np.inf for x in group_trials])\n                    self.budget -= np.sum(group_fitness != np.inf) #Adjust budget\n                    best_index_group = np.argmin(group_fitness)\n\n                    if group_fitness[best_index_group] < fitness[i + best_index_group]:\n                        new_population[i + best_index_group] = group_trials[best_index_group]\n                        new_fitness[i + best_index_group] = group_fitness[best_index_group]\n                        CR_history[i + best_index_group] = CR\n                        trial = new_population[i + best_index_group]  # Update trial for archive and best solution\n\n                        # Add replaced vector to archive (combined strategy)\n                        if len(self.archive) < self.archive_size:\n                            self.archive.append(population[i + best_index_group])\n                            self.archive_fitness.append(fitness[i + best_index_group])\n                        else:\n                             # Replace worst in archive\n                            worst_archive_index = np.argmax(self.archive_fitness)\n                            self.archive[worst_archive_index] = population[i + best_index_group]\n                            self.archive_fitness[worst_archive_index] = fitness[i + best_index_group]\n                        if group_fitness[best_index_group] < self.f_opt:\n                            self.f_opt = group_fitness[best_index_group]\n                            self.x_opt = group_trials[best_index_group]\n\n                # Selection\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n                f_trial = func(trial) if self.budget > 0 else np.inf\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                    CR_history[i] = CR # store successful CR\n\n                    # Add replaced vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                         # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = population[i]\n                        self.archive_fitness[worst_archive_index] = fitness[i]\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                else:\n                     # Add trial vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = trial\n                        self.archive_fitness[worst_archive_index] = f_trial\n                        \n\n            population = new_population\n            fitness = new_fitness\n\n            # Stagnation check and periodic population rejuvenation\n            if generation % 50 == 0:\n                if np.std(fitness) < 1e-6:  # Stagnation criterion\n                    stagnation_counter += 1\n                    if stagnation_counter >= 2:\n                         # Perform local search around the best solution\n                        x_local = np.copy(self.x_opt)\n                        for _ in range(min(self.budget // 10, 100)):  # Limited local search budget\n                            x_new = x_local + np.random.uniform(-self.local_search_radius, self.local_search_radius, size=self.dim)\n                            x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n                            f_new = func(x_new) if self.budget > 0 else np.inf\n                            self.budget -= 1\n                            if f_new < self.f_opt:\n                                self.f_opt = f_new\n                                self.x_opt = x_new\n                                x_local = np.copy(x_new)  # Move center of local search\n                        stagnation_counter = 0 # Reset stagnation\n\n                else:\n                    stagnation_counter = 0 # Reset stagnation\n\n                #Periodic rejuvenation\n                if generation % 200 == 0:\n                    # Replace a fraction of the population with new random solutions\n                    num_rejuvenated = int(0.2 * population.shape[0])\n                    new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(num_rejuvenated, self.dim))\n                    new_fitnesses = np.array([func(x) if self.budget > 0 else np.inf for x in new_individuals])\n                    self.budget -= np.sum(new_fitnesses != np.inf)\n\n                    worst_indices = np.argsort(fitness)[-num_rejuvenated:]  # Indices of worst individuals\n                    population[worst_indices] = new_individuals\n                    fitness[worst_indices] = new_fitnesses\n\n                    # Update optimal solution\n                    best_index = np.argmin(fitness)\n                    if fitness[best_index] < self.f_opt:\n                        self.f_opt = fitness[best_index]\n                        self.x_opt = population[best_index]\n                \n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 71, in __call__\nIndexError: index 6 is out of bounds for axis 0 with size 5\n.", "error": "", "parent_ids": ["fe75737f-ada7-482f-8df1-beab0b17ae44"], "operator": null, "metadata": {}}
{"id": "117ef5d2-4db2-4897-a50d-fe0b33038e54", "fitness": 0.0, "name": "AdaptiveDE", "description": "Enhanced Adaptive Differential Evolution with a simplified mutation strategy, dynamic parameter adaptation based on success rates, an aging archive, and a distance-based diversity maintenance strategy with orthogonal crossover.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=100, F_init=0.5, CR_init=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F_init = F_init\n        self.CR_init = CR_init\n        self.F_memory = np.ones(self.pop_size) * self.F_init\n        self.CR_memory = np.ones(self.pop_size) * self.CR_init\n        self.archive = []\n        self.archive_fitness = []\n        self.archive_age = []\n        self.success_F = []\n        self.success_CR = []\n        self.min_F = 0.1\n        self.max_F = 0.9\n        self.archive_prob = 0.2  # Static archive probability\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n\n        generation = 0\n        stagnation_counter = 0\n        prev_best_fitness = self.f_opt\n        archive_clear_interval = 200\n        \n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR using success history (Le Cun style update)\n                if self.success_F:\n                    F_mean = np.mean(self.success_F)\n                    self.F_memory[i] = np.clip(np.random.normal(F_mean, 0.1), self.min_F, self.max_F)\n                else:\n                    self.F_memory[i] = np.clip(np.random.normal(0.5, 0.1), self.min_F, self.max_F)\n\n                if self.success_CR:\n                    CR_mean = np.mean(self.success_CR)\n                    self.CR_memory[i] = np.clip(np.random.normal(CR_mean, 0.1), 0.0, 1.0)\n                else:\n                    self.CR_memory[i] = np.clip(np.random.normal(0.7, 0.1), 0.0, 1.0)\n\n                # Mutation: Simplified DE/rand/1 strategy with archive\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n\n                if len(self.archive) > 0 and np.random.rand() < self.archive_prob:\n                    archive_index = np.random.randint(len(self.archive))\n                    x3 = self.archive[archive_index]\n\n                mutant = x1 + self.F_memory[i] * (x2 - x3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Orthogonal Crossover\n                trial = np.copy(population[i])\n                num_changed_vars = 0\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR_memory[i]:\n                        trial[j] = mutant[j]\n                        num_changed_vars += 1\n                if num_changed_vars == 0:\n                    j_rand = np.random.randint(self.dim)\n                    trial[j_rand] = mutant[j_rand]\n                \n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n\n                    self.success_F.append(self.F_memory[i])\n                    self.success_CR.append(self.CR_memory[i])\n                    if len(self.success_F) > 10:\n                        self.success_F.pop(0)\n                        self.success_CR.pop(0)\n\n                    # Archive management: replace worst in archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                        self.archive_fitness.append(fitness[i])\n                        self.archive_age.append(0)\n                    else:\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        if fitness[i] < self.archive_fitness[worst_archive_index]:\n                            self.archive[worst_archive_index] = population[i]\n                            self.archive_fitness[worst_archive_index] = fitness[i]\n                            self.archive_age[worst_archive_index] = 0\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        stagnation_counter = 0\n                else:\n                    # Archive management: replace worst in archive (trial vector)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                        self.archive_age.append(0)\n                    else:\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        if f_trial < self.archive_fitness[worst_archive_index]:\n                            self.archive[worst_archive_index] = trial\n                            self.archive_fitness[worst_archive_index] = f_trial\n                            self.archive_age[worst_archive_index] = 0\n                            \n            # Update age of archive members and clear out old ones\n            self.archive_age = [age + 1 for age in self.archive_age]\n\n            # Diversity Maintenance\n            if generation % 50 == 0:\n                distances = np.zeros((self.pop_size, self.pop_size))\n                for k in range(self.pop_size):\n                    for l in range(k + 1, self.pop_size):\n                        distances[k, l] = np.linalg.norm(population[k] - population[l])\n                        distances[l, k] = distances[k, l]\n\n                min_dist = np.min(distances + np.eye(self.pop_size) * 1e9)\n                if min_dist < 0.1:\n                    # Introduce random individuals\n                    for k in range(self.pop_size):\n                        if np.random.rand() < 0.1:\n                            population[k] = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                            fitness[k] = func(population[k])\n                            self.budget -= 1\n                            if fitness[k] < self.f_opt:\n                                self.f_opt = fitness[k]\n                                self.x_opt = population[k]\n\n            population = new_population\n            fitness = new_fitness\n\n            if abs(self.f_opt - prev_best_fitness) < 1e-8:\n                stagnation_counter += 1\n            else:\n                stagnation_counter = 0\n            prev_best_fitness = self.f_opt\n\n            if stagnation_counter > 100:\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                stagnation_counter = 0\n                self.archive = []\n                self.archive_fitness = []\n                self.archive_age = []\n                self.success_F = []\n                self.success_CR = []\n\n            if generation % archive_clear_interval == 0:\n                max_age = np.max(self.archive_age)\n                to_remove = []\n                for k in range(len(self.archive)):\n                    if self.archive_age[k] >= max_age:\n                        to_remove.append(k)\n\n                for k in sorted(to_remove, reverse=True):\n                    del self.archive[k]\n                    del self.archive_fitness[k]\n                    del self.archive_age[k]\n\n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9e7acf1c-bcc9-4a3f-b347-c892323d9cef"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "4fad32ff-4d41-4423-bfed-0d145cbd4482", "fitness": 0.0, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with a dynamically adjusted population size based on fitness diversity and an aging mechanism to promote exploration.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_base=0.5, CR_base=0.7, archive_size=100, F_range=0.3, CR_range=0.3, local_search_radius=0.1, momentum=0.1, age_limit=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.archive_size = archive_size\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.archive = []\n        self.archive_fitness = []\n        self.local_search_radius = local_search_radius\n        self.momentum = momentum\n        self.velocity = np.zeros((pop_size, dim))  # Initialize velocity for momentum\n        self.age_limit = age_limit\n        self.ages = np.zeros(pop_size, dtype=int)  # Initialize ages for each individual\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n\n        generation = 0\n        stagnation_counter = 0\n        CR_history = np.full(self.pop_size, self.CR_base)  # CR history for self-adaptation\n\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n            self.ages += 1  # Increase age for all individuals\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                F = self.F_base + np.random.uniform(-self.F_range, self.F_range)\n                F = np.clip(F, 0.1, 1.0)\n\n                # Self-adaptive CR\n                CR = CR_history[i] + np.random.normal(0, self.CR_range)\n                CR = np.clip(CR, 0.1, 0.9)\n\n                # Mutation with momentum\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n\n                # Momentum update\n                self.velocity[i] = self.momentum * self.velocity[i] + F * (x2 - x3)\n                mutant = x1 + self.velocity[i]\n\n                # Use archive\n                if np.random.rand() < 0.1 and len(self.archive) > 0:\n                    archive_index = np.random.randint(len(self.archive))\n                    mutant = x1 + F * (x2 - self.archive[archive_index])\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n\n                # Selection\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                    CR_history[i] = CR  # store successful CR\n                    self.ages[i] = 0 #reset age\n\n                    # Add replaced vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                        # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = population[i]\n                        self.archive_fitness[worst_archive_index] = fitness[i]\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                else:\n                    # Add trial vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = trial\n                        self.archive_fitness[worst_archive_index] = f_trial\n\n            population = new_population\n            fitness = new_fitness\n\n            # Dynamic population size adjustment\n            fitness_std = np.std(fitness)\n            if fitness_std < 1e-5 and self.pop_size > 10:\n                self.pop_size = max(10, int(self.pop_size * 0.9))  # Reduce population size\n                population = population[:self.pop_size]\n                fitness = fitness[:self.pop_size]\n                self.velocity = self.velocity[:self.pop_size]\n                self.ages = self.ages[:self.pop_size]\n                CR_history = CR_history[:self.pop_size]\n\n            elif fitness_std > 1e-2 and self.pop_size < 100:\n                self.pop_size = min(100, int(self.pop_size * 1.1))  # Increase population size\n                # Add new random individuals\n                new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size - len(population), self.dim))\n                new_fitnesses = np.array([func(x) for x in new_individuals])\n                self.budget -= len(new_individuals)\n\n                population = np.concatenate([population, new_individuals])\n                fitness = np.concatenate([fitness, new_fitnesses])\n                self.velocity = np.concatenate([self.velocity, np.zeros((len(new_individuals), self.dim))])\n                self.ages = np.concatenate([self.ages, np.zeros(len(new_individuals), dtype=int)])\n                CR_history = np.concatenate([CR_history, np.full(len(new_individuals), self.CR_base)])\n\n            # Aging mechanism\n            for i in range(self.pop_size):\n                if self.ages[i] > self.age_limit:\n                    # Replace old individual with a new random one\n                    population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                    fitness[i] = func(population[i])\n                    self.budget -= 1\n                    self.velocity[i] = np.zeros(self.dim)\n                    self.ages[i] = 0\n\n                    # Update optimal solution\n                    if fitness[i] < self.f_opt:\n                        self.f_opt = fitness[i]\n                        self.x_opt = population[i]\n\n            # Stagnation check and periodic population rejuvenation\n            if generation % 50 == 0:\n                if np.std(fitness) < 1e-6:  # Stagnation criterion\n                    stagnation_counter += 1\n                    if stagnation_counter >= 2:\n                        # Perform local search around the best solution\n                        x_local = np.copy(self.x_opt)\n                        for _ in range(min(self.budget // 10, 100)):  # Limited local search budget\n                            x_new = x_local + np.random.uniform(-self.local_search_radius, self.local_search_radius, size=self.dim)\n                            x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n                            f_new = func(x_new)\n                            self.budget -= 1\n                            if f_new < self.f_opt:\n                                self.f_opt = f_new\n                                self.x_opt = x_new\n                                x_local = np.copy(x_new)  # Move center of local search\n                        stagnation_counter = 0  # Reset stagnation\n\n                else:\n                    stagnation_counter = 0  # Reset stagnation\n\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["fe75737f-ada7-482f-8df1-beab0b17ae44"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "64a8fba9-6548-420d-982e-645509dc9e5f", "fitness": 0.0, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with a simplified self-adaptive strategy for CR and F, a perturbing archive, local search with dynamically adjusted radius, and a more aggressive rejuvenation mechanism, aiming to improve both convergence speed and escape from local optima.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=50, local_search_init_radius=0.1, local_search_decay=0.95, rejuvenation_rate=0.3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.archive = []\n        self.local_search_init_radius = local_search_init_radius\n        self.local_search_decay = local_search_decay\n        self.rejuvenation_rate = rejuvenation_rate\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        generation = 0\n        stagnation_counter = 0\n        local_search_radius = self.local_search_init_radius\n\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n\n            for i in range(self.pop_size):\n                # Simplified Adaptive F and CR\n                F = np.random.uniform(0.3, 0.9)\n                CR = np.random.uniform(0.1, 0.9)\n\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n\n                # Perturb archive usage\n                if len(self.archive) > 0 and np.random.rand() < 0.2:\n                    archive_idx = np.random.randint(len(self.archive))\n                    mutant = x1 + F * (x2 - self.archive[archive_idx] + 0.01 * np.random.randn(self.dim))  # Add perturbation\n                else:\n                    mutant = x1 + F * (x2 - x3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                crossover_points = np.random.rand(self.dim) < CR\n                trial[crossover_points] = mutant[crossover_points]\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n\n                    # Update archive (replace the worst)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                    else:\n                        worst_idx = np.argmax([func(x) for x in self.archive])  #Find worst using func evals, not stored fitness\n                        self.archive[worst_idx] = population[i]\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                else:\n                    #Archive update with trial if it's better than worst in archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                    elif f_trial < np.max([func(x) for x in self.archive]):\n                        worst_idx = np.argmax([func(x) for x in self.archive])\n                        self.archive[worst_idx] = trial\n                        \n\n            population = new_population\n            fitness = new_fitness\n\n            # Stagnation check and local search with dynamic radius\n            if generation % 50 == 0:\n                if np.std(fitness) < 1e-6:\n                    stagnation_counter += 1\n                    if stagnation_counter >= 2:\n                        # Local search\n                        x_local = np.copy(self.x_opt)\n                        for _ in range(min(self.budget // 10, 50)):\n                            x_new = x_local + np.random.uniform(-local_search_radius, local_search_radius, size=self.dim)\n                            x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n                            f_new = func(x_new)\n                            self.budget -= 1\n                            if f_new < self.f_opt:\n                                self.f_opt = f_new\n                                self.x_opt = x_new\n                                x_local = np.copy(x_new)  # Move center\n                        local_search_radius *= self.local_search_decay #Decay search radius\n\n                        stagnation_counter = 0  # Reset stagnation\n                else:\n                    stagnation_counter = 0\n                    local_search_radius = self.local_search_init_radius #Reset radius if not stagnating\n\n                # Population Rejuvenation (More aggressive)\n                num_rejuvenated = int(self.rejuvenation_rate * self.pop_size)\n                if num_rejuvenated > 0: #Check if any to rejuvenate\n                    new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(num_rejuvenated, self.dim))\n                    new_fitnesses = np.array([func(x) for x in new_individuals])\n                    self.budget -= num_rejuvenated\n\n                    worst_indices = np.argsort(fitness)[-num_rejuvenated:]\n                    population[worst_indices] = new_individuals\n                    fitness[worst_indices] = new_fitnesses\n\n                    best_index = np.argmin(fitness)\n                    if fitness[best_index] < self.f_opt:\n                        self.f_opt = fitness[best_index]\n                        self.x_opt = population[best_index]\n\n\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["fe75737f-ada7-482f-8df1-beab0b17ae44"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "15eb0335-e9d2-4d86-848a-e978bf3197b3", "fitness": 0.445452196560102, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with covariance matrix adaptation learning rate control, combined mutation strategies, and dynamic population size adjustment for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size_min=20, pop_size_max=100, F_base=0.5, CR_base=0.7, archive_size=50, F_range=0.3, CR_range=0.3, p_archive=0.1, lr_F=0.1, lr_CR=0.1, restart_patience=50, CMA_decay=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_min = pop_size_min\n        self.pop_size_max = pop_size_max\n        self.pop_size = pop_size_max  # Start with a larger population size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.archive_size = archive_size\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.archive = []\n        self.archive_fitness = []\n        self.p_archive = p_archive\n        self.lr_F = lr_F  # Learning rate for F adaptation\n        self.lr_CR = lr_CR  # Learning rate for CR adaptation\n        self.restart_patience = restart_patience  # Patience for stagnation detection\n        self.stagnation_counter = 0\n        self.CMA_decay = CMA_decay #Decay factor for learning rate of F and CR\n\n        self.successful_F = []\n        self.successful_CR = []\n        self.mean_F = self.F_base\n        self.mean_CR = self.CR_base\n        self.CMA_F = 1.0\n        self.CMA_CR = 1.0\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR with learning rate & CMA-ES style update\n                F = np.clip(self.mean_F + self.CMA_F * np.random.randn(), 0.1, 1.0)\n                CR = np.clip(self.mean_CR + self.CMA_CR * np.random.randn(), 0.1, 1.0)\n\n\n                # Mutation Strategy: Adaptive selection\n                rand = np.random.rand()\n                if rand < 0.33:\n                    # DE/rand/1\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x1, x2, x3 = population[indices]\n                    mutant = x1 + F * (x2 - x3)\n                elif rand < 0.66:\n                    # DE/current-to-best/1\n                    indices = np.random.choice(self.pop_size, 2, replace=False)\n                    x1, x2 = population[indices]\n                    mutant = population[i] + F * (self.x_opt - population[i]) + F * (x1 - x2)\n                else:\n                    # DE/best/1\n                    indices = np.random.choice(self.pop_size, 2, replace=False)\n                    x1, x2 = population[indices]\n                    mutant = self.x_opt + F * (x1 - x2)\n\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n\n                    # Update successful F and CR\n                    self.successful_F.append(F)\n                    self.successful_CR.append(CR)\n\n                    # Add replaced vector to archive (probabilistic strategy)\n                    if np.random.rand() < self.p_archive:\n                        if len(self.archive) < self.archive_size:\n                            self.archive.append(population[i])\n                            self.archive_fitness.append(fitness[i])\n                        else:\n                            if self.archive_fitness: #Ensure archive not empty\n                                worst_archive_index = np.argmax(self.archive_fitness)\n                                self.archive[worst_archive_index] = population[i]\n                                self.archive_fitness[worst_archive_index] = fitness[i]\n\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            population = new_population\n            fitness = new_fitness\n\n            # CMA-ES style update of mean and variance\n            if self.successful_F:\n                self.mean_F = (1 - self.lr_F) * self.mean_F + self.lr_F * np.mean(self.successful_F)\n                self.CMA_F = self.CMA_decay * self.CMA_F + (1-self.CMA_decay) * np.std(self.successful_F)\n            if self.successful_CR:\n                self.mean_CR = (1 - self.lr_CR) * self.mean_CR + self.lr_CR * np.mean(self.successful_CR)\n                self.CMA_CR = self.CMA_decay * self.CMA_CR + (1-self.CMA_decay) * np.std(self.successful_CR)\n\n            # Clear successful F and CR every few generations\n            if generation % 20 == 0:\n                self.successful_F = []\n                self.successful_CR = []\n\n            # Check for stagnation and potentially restart or reduce population size\n            if np.std(fitness) < 1e-6:\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n\n            if self.stagnation_counter > self.restart_patience:\n                #Option 1: Restart\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                self.stagnation_counter = 0\n\n                #Option 2: Reduce population size (if possible)\n                if self.pop_size > self.pop_size_min:\n                    self.pop_size = max(self.pop_size_min, int(self.pop_size * 0.8)) #Reduce by 20%\n                    population = population[:self.pop_size] #Truncate\n                    fitness = fitness[:self.pop_size]\n                    print(\"Reduced population size to:\", self.pop_size)\n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDE scored 0.445 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a36df3c3-5d99-4d76-9027-ab316b352bb3"], "operator": null, "metadata": {"aucs": [0.4189895246959475, 0.6514938031406146, 0.7113254584038458, 0]}}
{"id": "1b3c9ea7-68b9-4979-9cdb-bafb2340fc5e", "fitness": 0.4228247798553971, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with self-adaptive parameters, multiple mutation strategies with probabilities based on their recent success, a dynamically updated archive, and population size adaptation based on stagnation detection.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size_min=20, pop_size_max=100, F_base=0.5, CR_base=0.7, archive_size=50, F_range=0.3, CR_range=0.3, p_archive=0.1, lr=0.1, restart_patience=50, strategy_probs=[0.33, 0.33, 0.34]):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_min = pop_size_min\n        self.pop_size_max = pop_size_max\n        self.pop_size = pop_size_max  # Start with a larger population size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.archive_size = archive_size\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.archive = []\n        self.archive_fitness = []\n        self.p_archive = p_archive\n        self.lr = lr  # Learning rate for F and CR adaptation\n        self.restart_patience = restart_patience  # Patience for stagnation detection\n        self.stagnation_counter = 0\n\n        self.successful_F = []\n        self.successful_CR = []\n        self.strategy_probs = strategy_probs # Probabilities for each mutation strategy\n        self.strategy_successes = [0] * len(strategy_probs)\n        self.strategy_counts = [0] * len(strategy_probs)\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR with learning rate\n                if self.successful_F:\n                    F = np.clip(np.mean(self.successful_F) + self.lr * np.random.randn(), 0.1, 1.0)\n                else:\n                    F = self.F_base + np.random.uniform(-self.F_range, self.F_range)\n                    F = np.clip(F, 0.1, 1.0)\n                \n                if self.successful_CR:\n                    CR = np.clip(np.mean(self.successful_CR) + self.lr * np.random.randn(), 0.1, 1.0)\n                else:\n                    CR = self.CR_base + np.random.uniform(-self.CR_range, self.CR_range)\n                    CR = np.clip(CR, 0.1, 1.0)\n\n                # Mutation Strategy: Adaptive selection\n                rand = np.random.rand()\n                strategy_index = np.searchsorted(np.cumsum(self.strategy_probs), rand)\n\n                if strategy_index == 0:\n                    # DE/rand/1\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x1, x2, x3 = population[indices]\n                    mutant = x1 + F * (x2 - x3)\n                elif strategy_index == 1:\n                    # DE/current-to-best/1\n                    indices = np.random.choice(self.pop_size, 2, replace=False)\n                    x1, x2 = population[indices]\n                    mutant = population[i] + F * (self.x_opt - population[i]) + F * (x1 - x2)\n                else:\n                    # DE/best/1\n                    indices = np.random.choice(self.pop_size, 2, replace=False)\n                    x1, x2 = population[indices]\n                    mutant = self.x_opt + F * (x1 - x2)\n\n                self.strategy_counts[strategy_index] += 1\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n\n                    # Update successful F and CR\n                    self.successful_F.append(F)\n                    self.successful_CR.append(CR)\n                    self.strategy_successes[strategy_index] += 1\n\n\n                    # Add replaced vector to archive (probabilistic strategy) with fitness check\n                    if np.random.rand() < 0.5:\n                        if len(self.archive) < self.archive_size:\n                            self.archive.append(population[i])\n                            self.archive_fitness.append(fitness[i])\n                        else:\n                            if self.archive_fitness: #Ensure archive not empty\n                                worst_archive_index = np.argmax(self.archive_fitness)\n                                if fitness[i] < self.archive_fitness[worst_archive_index]:\n                                    self.archive[worst_archive_index] = population[i]\n                                    self.archive_fitness[worst_archive_index] = fitness[i]\n\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            population = new_population\n            fitness = new_fitness\n\n            # Update strategy probabilities based on success rates\n            for k in range(len(self.strategy_probs)):\n                if self.strategy_counts[k] > 0:\n                    success_rate = self.strategy_successes[k] / self.strategy_counts[k]\n                    self.strategy_probs[k] = 0.1 + 0.9 * success_rate # Bias towards successful strategies\n                else:\n                    self.strategy_probs[k] = 1.0 / len(self.strategy_probs) #Ensure all strategies have chance\n            self.strategy_probs /= np.sum(self.strategy_probs)  # Normalize probabilities\n\n            # Reset counts and successes\n            self.strategy_successes = [0] * len(self.strategy_probs)\n            self.strategy_counts = [0] * len(self.strategy_probs)\n\n            # Clear successful F and CR every few generations\n            if generation % 20 == 0:\n                self.successful_F = []\n                self.successful_CR = []\n\n            # Check for stagnation and potentially restart or reduce population size\n            if np.std(fitness) < 1e-6:\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n\n            if self.stagnation_counter > self.restart_patience:\n                #Option 1: Restart\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                self.stagnation_counter = 0\n\n                #Option 2: Reduce population size (if possible)\n                if self.pop_size > self.pop_size_min:\n                  self.pop_size = max(self.pop_size_min, int(self.pop_size * 0.8)) #Reduce by 20%\n                  population = population[:self.pop_size] #Truncate\n                  fitness = fitness[:self.pop_size]\n                #print(\"Reduced population size to:\", self.pop_size)\n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDE scored 0.423 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a36df3c3-5d99-4d76-9027-ab316b352bb3"], "operator": null, "metadata": {"aucs": [0.3903283937534693, 0.5875769879010444, 0.7133937377670747, 0]}}
{"id": "d136cc9c-8d6f-4dfb-833d-0ef9f7c32d39", "fitness": 0.40471809650057616, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with self-adaptive parameters, multiple mutation strategies blended based on success rate, and a dynamic population size adjustment strategy for improved exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size_min=20, pop_size_max=100, F_base=0.5, CR_base=0.7, archive_size=50, F_range=0.3, CR_range=0.3, p_archive=0.1, lr=0.1, stagnation_patience=50, pop_decay_rate=0.95, pop_increase_rate=1.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_min = pop_size_min\n        self.pop_size_max = pop_size_max\n        self.pop_size = pop_size_max  # Start with a larger population size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.archive_size = archive_size\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.archive = []\n        self.archive_fitness = []\n        self.p_archive = p_archive\n        self.lr = lr  # Learning rate for F and CR adaptation\n        self.stagnation_patience = stagnation_patience  # Patience for stagnation detection\n        self.stagnation_counter = 0\n        self.pop_decay_rate = pop_decay_rate\n        self.pop_increase_rate = pop_increase_rate\n\n        self.successful_F = []\n        self.successful_CR = []\n\n        # Mutation strategy weights (initialized equally)\n        self.mutation_weights = np.array([1/3, 1/3, 1/3])\n        self.mutation_success = np.array([0, 0, 0])\n        self.mutation_counts = np.array([0, 0, 0])\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR with learning rate\n                if self.successful_F:\n                    F = np.clip(np.mean(self.successful_F) + self.lr * np.random.randn(), 0.1, 1.0)\n                else:\n                    F = self.F_base + np.random.uniform(-self.F_range, self.F_range)\n                    F = np.clip(F, 0.1, 1.0)\n                \n                if self.successful_CR:\n                    CR = np.clip(np.mean(self.successful_CR) + self.lr * np.random.randn(), 0.1, 1.0)\n                else:\n                    CR = self.CR_base + np.random.uniform(-self.CR_range, self.CR_range)\n                    CR = np.clip(CR, 0.1, 1.0)\n\n                # Mutation Strategy: Adaptive selection based on weights\n                mutation_probs = self.mutation_weights / np.sum(self.mutation_weights)\n                mutation_choice = np.random.choice(3, p=mutation_probs)\n\n                if mutation_choice == 0:\n                    # DE/rand/1\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x1, x2, x3 = population[indices]\n                    mutant = x1 + F * (x2 - x3)\n                elif mutation_choice == 1:\n                    # DE/current-to-best/1\n                    indices = np.random.choice(self.pop_size, 2, replace=False)\n                    x1, x2 = population[indices]\n                    mutant = population[i] + F * (self.x_opt - population[i]) + F * (x1 - x2)\n                else:\n                    # DE/best/1\n                    indices = np.random.choice(self.pop_size, 2, replace=False)\n                    x1, x2 = population[indices]\n                    mutant = self.x_opt + F * (x1 - x2)\n\n                self.mutation_counts[mutation_choice] += 1\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n\n                    # Update successful F and CR\n                    self.successful_F.append(F)\n                    self.successful_CR.append(CR)\n                    self.mutation_success[mutation_choice] += 1\n\n\n                    # Add replaced vector to archive (probabilistic strategy)\n                    if np.random.rand() < self.p_archive:\n                        if len(self.archive) < self.archive_size:\n                            self.archive.append(population[i])\n                            self.archive_fitness.append(fitness[i])\n                        else:\n                            if self.archive_fitness: #Ensure archive not empty\n                                worst_archive_index = np.argmax(self.archive_fitness)\n                                self.archive[worst_archive_index] = population[i]\n                                self.archive_fitness[worst_archive_index] = fitness[i]\n\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            population = new_population\n            fitness = new_fitness\n\n            # Update mutation strategy weights based on success rate\n            for k in range(3):\n                if self.mutation_counts[k] > 0:\n                    self.mutation_weights[k] = (1-self.lr) * self.mutation_weights[k] + self.lr * (self.mutation_success[k] / self.mutation_counts[k])\n                self.mutation_success[k] = 0\n                self.mutation_counts[k] = 0\n                \n            self.mutation_weights = np.clip(self.mutation_weights, 0.1, 1.0)\n\n\n            # Clear successful F and CR every few generations\n            if generation % 20 == 0:\n                self.successful_F = []\n                self.successful_CR = []\n\n            # Check for stagnation and potentially restart or adjust population size\n            if np.std(fitness) < 1e-6:\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n\n            if self.stagnation_counter > self.stagnation_patience:\n                # Adjust population size dynamically\n                if self.pop_size > self.pop_size_min:\n                    self.pop_size = max(self.pop_size_min, int(self.pop_size * self.pop_decay_rate)) #Reduce by decay_rate\n                    population = population[:self.pop_size] #Truncate\n                    fitness = fitness[:self.pop_size]\n                else:\n                   #Restart if minimum population is reached\n                    self.pop_size = self.pop_size_max\n                    population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                    fitness = np.array([func(x) for x in population])\n                    self.budget -= self.pop_size\n                    best_index = np.argmin(fitness)\n                    if fitness[best_index] < self.f_opt:\n                        self.f_opt = fitness[best_index]\n                        self.x_opt = population[best_index]\n                \n                self.stagnation_counter = 0\n\n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDE scored 0.405 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a36df3c3-5d99-4d76-9027-ab316b352bb3"], "operator": null, "metadata": {"aucs": [0.2306067780640948, 0.71609909722846, 0.6721665107097499, 0]}}
{"id": "37d06b96-6b2a-4796-8519-b7504bc2ca46", "fitness": 0.48316989344433187, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with a dynamically sized archive based on success rate, orthogonal crossover, self-adaptive parameters, and a periodic archive maintenance strategy that considers both age and fitness diversity.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size_init=50, F_init=0.5, CR_init=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size_init = archive_size_init\n        self.archive_size = archive_size_init  # Initialize dynamic archive size\n        self.F_init = F_init\n        self.CR_init = CR_init\n        self.F_memory = np.ones(self.pop_size) * self.F_init\n        self.CR_memory = np.ones(self.pop_size) * self.CR_init\n        self.archive = []\n        self.archive_fitness = []\n        self.archive_age = [] # Track age of individuals in archive\n        self.success_F = []\n        self.success_CR = []\n        self.success_count = 0\n        self.failure_count = 0\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n\n        generation = 0\n        stagnation_counter = 0\n        prev_best_fitness = self.f_opt\n        archive_clear_interval = 200 # Clear the archive periodically\n        \n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n\n            # Adjust archive size dynamically based on success rate\n            success_rate = self.success_count / (self.success_count + self.failure_count + 1e-9)\n            self.archive_size = int(self.archive_size_init * (0.5 + success_rate))  # Increase if successful\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR using success history\n                if self.success_F:\n                    F_mean = np.mean(self.success_F)\n                    self.F_memory[i] = np.clip(np.random.normal(F_mean, 0.3), 0.1, 1.0)\n                else:\n                    self.F_memory[i] = np.clip(np.random.normal(0.5, 0.3), 0.1, 1.0)\n\n                if self.success_CR:\n                    CR_mean = np.mean(self.success_CR)\n                    self.CR_memory[i] = np.clip(np.random.normal(CR_mean, 0.1), 0.0, 1.0)\n                else:\n                    self.CR_memory[i] = np.clip(np.random.normal(0.7, 0.1), 0.0, 1.0)\n\n\n                # Mutation\n                indices = np.random.choice(self.pop_size, 2, replace=False)\n                x1, x2 = population[indices]\n\n                # Utilize archive with dynamic probability\n                archive_prob = min(0.4, generation / 500)  # Increase archive usage over time\n                if len(self.archive) > 0 and np.random.rand() < archive_prob:\n                     archive_index = np.random.randint(len(self.archive))\n                     x3 = self.archive[archive_index]\n                else:\n                    indices = np.random.choice(self.pop_size, 1, replace=False)\n                    x3 = population[indices[0]]\n\n                # Weighted difference vector\n                mutant = population[i] + self.F_memory[i] * (x1 - x2) + self.F_memory[i] * 0.5 * (x3 - population[i])  # Reduced pull to archive\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Orthogonal Crossover\n                trial = np.copy(population[i])\n                num_changed_vars = 0\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR_memory[i]:\n                        trial[j] = mutant[j]\n                        num_changed_vars += 1\n                # Ensure at least one variable is changed\n                if num_changed_vars == 0:\n                    j_rand = np.random.randint(self.dim)\n                    trial[j_rand] = mutant[j_rand]\n                \n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n\n                    self.success_F.append(self.F_memory[i])\n                    self.success_CR.append(self.CR_memory[i])\n                    if len(self.success_F) > 10:\n                        self.success_F.pop(0)\n                        self.success_CR.pop(0)\n                    self.success_count += 1\n\n                    # Dynamic archive management: replace worst in archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                        self.archive_fitness.append(fitness[i])\n                        self.archive_age.append(0)\n                    else:\n                        if len(self.archive) > 0: # Ensure archive is not empty\n                            worst_archive_index = np.argmax(self.archive_fitness)\n                            if fitness[i] < self.archive_fitness[worst_archive_index]:\n                                self.archive[worst_archive_index] = population[i]\n                                self.archive_fitness[worst_archive_index] = fitness[i]\n                                self.archive_age[worst_archive_index] = 0\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        stagnation_counter = 0 # Reset stagnation counter\n                else:\n                    self.failure_count += 1\n                    # Dynamic archive management: replace worst in archive (trial vector)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                        self.archive_age.append(0)\n                    else:\n                         if len(self.archive) > 0: # Ensure archive is not empty\n                            worst_archive_index = np.argmax(self.archive_fitness)\n                            if f_trial < self.archive_fitness[worst_archive_index]:\n                                self.archive[worst_archive_index] = trial\n                                self.archive_fitness[worst_archive_index] = f_trial\n                                self.archive_age[worst_archive_index] = 0\n\n            # Update age of archive members\n            self.archive_age = [age + 1 for age in self.archive_age]\n\n\n            population = new_population\n            fitness = new_fitness\n\n            # Restart population if stagnating\n            if abs(self.f_opt - prev_best_fitness) < 1e-8: #More robust stagnation detection\n                stagnation_counter += 1\n            else:\n                stagnation_counter = 0\n            prev_best_fitness = self.f_opt\n\n            if stagnation_counter > 100: #Increased stagnation threshold\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                stagnation_counter = 0  # Reset after restart\n                self.archive = [] #Clear archive after restart\n                self.archive_fitness = []\n                self.archive_age = []\n                self.success_F = []\n                self.success_CR = []\n                self.success_count = 0\n                self.failure_count = 0\n\n            # Periodic archive clearing\n            if generation % archive_clear_interval == 0:\n                #Remove old or redundant entries to increase diversity\n                if len(self.archive) > 0:\n                    # Calculate fitness diversity\n                    fitness_diversity = np.std(self.archive_fitness)\n\n                    # Determine threshold for age based on fitness diversity\n                    age_threshold = max(10, 50 - int(fitness_diversity * 20))  # Adjust as needed\n\n                    to_remove = []\n                    for k in range(len(self.archive)):\n                        if self.archive_age[k] >= age_threshold:\n                            to_remove.append(k)\n\n                    for k in sorted(to_remove, reverse=True):\n                        del self.archive[k]\n                        del self.archive_fitness[k]\n                        del self.archive_age[k]\n\n\n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDE scored 0.483 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9e7acf1c-bcc9-4a3f-b347-c892323d9cef"], "operator": null, "metadata": {"aucs": [0.17989730144302507, 0.4954241870391116, 0.4032185264728909, 0.5781485432721668, 0.35822348124024916, 0.5639025800513313, 0.32079423536348906, 0.3903999079838807, 0.3963581497066613, 0.2214303238512062, 0.7932169570595592, 0.9929003836291774, 0.5138757294139354, 0.35743066616815466, 0.7787178311383549, 0.5105681097226382, 0.409581034064879, 0.6564078444180367, 0.25178975106368773, 0.4911123257842003]}}
{"id": "b659a3c2-716b-47f8-a511-5ce90c271192", "fitness": 0.6459395977342762, "name": "AdaptiveDE", "description": "Enhanced Adaptive Differential Evolution with orthogonal learning, adaptive parameter control with a trend-based adjustment, a repair mechanism, and a combined archive with aging.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_base=0.5, CR_base=0.7, archive_size=100, F_range=0.3, CR_range=0.3, orthogonal_learning_rate=0.1, restart_patience=50, aging_rate=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.archive_size = archive_size\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.archive = []\n        self.archive_fitness = []\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.restart_patience = restart_patience\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.F_history = []\n        self.CR_history = []\n        self.aging_rate = aging_rate # Rate at which archive fitness degrades\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n            self.stagnation_counter = 0\n            self.previous_best_fitness = self.f_opt\n        else:\n            self.stagnation_counter +=1\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n            \n            # Adaptive F and CR parameter control with trend adjustment\n            F_trend = 0\n            CR_trend = 0\n            if len(self.F_history) > 1:\n                F_trend = np.mean(np.diff(self.F_history))\n            if len(self.CR_history) > 1:\n                CR_trend = np.mean(np.diff(self.CR_history))\n\n            self.F_base = np.clip(self.F_base + 0.1 * F_trend, 0.1, 0.9)\n            self.CR_base = np.clip(self.CR_base + 0.1 * CR_trend, 0.1, 0.9)\n\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                F = self.F_base + np.random.uniform(-self.F_range, self.F_range)\n                CR = self.CR_base + np.random.uniform(-self.CR_range, self.CR_range)\n                F = np.clip(F, 0.1, 1.0)\n                CR = np.clip(CR, 0.1, 1.0)\n\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = x1 + F * (x2 - x3)\n                \n                # Incorporate archive\n                if len(self.archive) > 0 and np.random.rand() < 0.1:  # 10% chance to use archive\n                    archive_index = np.random.randint(len(self.archive))\n                    mutant = x1 + F * (x2 - self.archive[archive_index])\n\n                # Repair mechanism\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_learning_rate:\n                    orthogonal_vector = self.x_opt + np.random.normal(0, 0.1, self.dim)  # Perturb best solution\n                    orthogonal_vector = np.clip(orthogonal_vector, func.bounds.lb, func.bounds.ub)\n                    trial = 0.5 * (trial + orthogonal_vector)  # Combine with trial vector\n                    trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                    \n                    # Record successful F and CR values\n                    self.F_history.append(F)\n                    self.CR_history.append(CR)\n                    if len(self.F_history) > 50:\n                        self.F_history.pop(0)\n                        self.CR_history.pop(0)\n\n                    # Add replaced vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                         # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = population[i]\n                        self.archive_fitness[worst_archive_index] = fitness[i]\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.stagnation_counter = 0\n                        self.previous_best_fitness = self.f_opt\n                else:\n                     # Add trial vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = trial\n                        self.archive_fitness[worst_archive_index] = f_trial\n                        \n\n            population = new_population\n            fitness = new_fitness\n            \n            # Restart population if stagnating\n            if self.stagnation_counter > self.restart_patience:\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                    self.stagnation_counter = 0\n                    self.previous_best_fitness = self.f_opt\n                else:\n                    self.stagnation_counter +=1\n                    \n                # Reset F and CR history\n                self.F_history = []\n                self.CR_history = []\n                \n            # Archive aging\n            for k in range(len(self.archive_fitness)):\n                self.archive_fitness[k] *= (1 - self.aging_rate)\n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDE scored 0.646 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["8d38d9e7-e3ad-441e-b2eb-654048c3c097"], "operator": null, "metadata": {"aucs": [0.24433617852411094, 0.6006914820464271, 0.6514962504344913, 0.8697694295896313, 0.6744446295425239, 0.7599463426372068, 0.5508962994616237, 0.6027753531233293, 0.7094770252577924, 0.31363570089141857, 0.8633447182928922, 0.998698662912699, 0.4113912535734896, 0.6715171458216991, 0.9008275262929145, 0.7397066273783517, 0.5593338774099105, 0.849637290896631, 0.3457497188336178, 0.6011164417647625]}}
{"id": "9766597b-3dad-42f6-8c14-48987b4db324", "fitness": 0.6650543891906519, "name": "AdaptiveDE", "description": "Enhanced Adaptive Differential Evolution with self-adaptive F and CR, a combined archive, orthogonal learning, a diversity-based population restart strategy, and a Cauchy mutation operator to handle high modality.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_base=0.5, CR_base=0.7, archive_size=100, F_range=0.3, CR_range=0.3, orthogonal_learning_rate=0.1, restart_patience=50, cauchy_mutation_prob=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.archive_size = archive_size\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.archive = []\n        self.archive_fitness = []\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.restart_patience = restart_patience\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.F_history = []\n        self.CR_history = []\n        self.cauchy_mutation_prob = cauchy_mutation_prob\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n            self.stagnation_counter = 0\n            self.previous_best_fitness = self.f_opt\n        else:\n            self.stagnation_counter +=1\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n            \n            # Self-adaptive F and CR\n            if self.F_history:\n                self.F_base = np.mean(self.F_history)\n            if self.CR_history:\n                self.CR_base = np.mean(self.CR_history)\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                F = self.F_base + np.random.uniform(-self.F_range, self.F_range)\n                CR = self.CR_base + np.random.uniform(-self.CR_range, self.CR_range)\n                F = np.clip(F, 0.1, 1.0)\n                CR = np.clip(CR, 0.1, 1.0)\n\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                \n                # Apply Cauchy mutation with a certain probability\n                if np.random.rand() < self.cauchy_mutation_prob:\n                    mutant = x1 + F * (x2 - x3) + 0.1 * np.random.standard_cauchy(size=self.dim)  # Cauchy mutation\n                else:\n                    mutant = x1 + F * (x2 - x3)\n\n                # Incorporate archive\n                if len(self.archive) > 0 and np.random.rand() < 0.1:  # 10% chance to use archive\n                    archive_index = np.random.randint(len(self.archive))\n                    mutant = x1 + F * (x2 - self.archive[archive_index])\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_learning_rate:\n                    orthogonal_vector = self.x_opt + np.random.normal(0, 0.1, self.dim)  # Perturb best solution\n                    orthogonal_vector = np.clip(orthogonal_vector, func.bounds.lb, func.bounds.ub)\n                    trial = 0.5 * (trial + orthogonal_vector)  # Combine with trial vector\n                    trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                    \n                    # Record successful F and CR values\n                    self.F_history.append(F)\n                    self.CR_history.append(CR)\n                    if len(self.F_history) > 50:\n                        self.F_history.pop(0)\n                        self.CR_history.pop(0)\n\n                    # Add replaced vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                         # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = population[i]\n                        self.archive_fitness[worst_archive_index] = fitness[i]\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.stagnation_counter = 0\n                        self.previous_best_fitness = self.f_opt\n                else:\n                     # Add trial vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = trial\n                        self.archive_fitness[worst_archive_index] = f_trial\n                        \n\n            population = new_population\n            fitness = new_fitness\n            \n            # Restart population if stagnating\n            if self.stagnation_counter > self.restart_patience:\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                    self.stagnation_counter = 0\n                    self.previous_best_fitness = self.f_opt\n                else:\n                    self.stagnation_counter +=1\n                    \n                # Reset F and CR history\n                self.F_history = []\n                self.CR_history = []\n                \n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDE scored 0.665 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["8d38d9e7-e3ad-441e-b2eb-654048c3c097"], "operator": null, "metadata": {"aucs": [0.42778914594274, 0.708968058476908, 0.5231378389871283, 0.8993349714897828, 0.7511860788082034, 0.8007617841377687, 0.6568488323364701, 0.6510738047373881, 0.7351298121334315, 0.7125057040925901, 0.8958667862703866, 0.9979867047450407, 0.3552540170792938, 0.7234053614761811, 0.7787124269970286, 0.752629644779532, 0.33187545568245713, 0.8230725219872674, 0.24799447688704057, 0.5275543567663994]}}
{"id": "79a5e1f9-a95f-4e28-9be6-df8b6f0252dd", "fitness": 0.7062782616113961, "name": "AdaptiveDE", "description": "Enhanced Adaptive Differential Evolution with a novel mutation strategy that combines current-to-best with random mutation, adaptive F and CR parameters, a combined archive strategy, orthogonal learning, and a diversity-based population restart.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_base=0.5, CR_base=0.7, archive_size=100, F_range=0.3, CR_range=0.3, orthogonal_learning_rate=0.1, restart_patience=50, archive_prob=0.1, current_to_best_prob=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.archive_size = archive_size\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.archive = []\n        self.archive_fitness = []\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.restart_patience = restart_patience\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.F_history = []\n        self.CR_history = []\n        self.archive_prob = archive_prob\n        self.current_to_best_prob = current_to_best_prob\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n            self.stagnation_counter = 0\n            self.previous_best_fitness = self.f_opt\n        else:\n            self.stagnation_counter +=1\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n            \n            # Self-adaptive F and CR\n            if self.F_history:\n                self.F_base = np.mean(self.F_history)\n            if self.CR_history:\n                self.CR_base = np.mean(self.CR_history)\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                F = self.F_base + np.random.uniform(-self.F_range, self.F_range)\n                CR = self.CR_base + np.random.uniform(-self.CR_range, self.CR_range)\n                F = np.clip(F, 0.1, 1.0)\n                CR = np.clip(CR, 0.1, 1.0)\n\n                # Mutation: Combining current-to-best with random mutation\n                indices = np.random.choice(self.pop_size, 2, replace=False)\n                x1, x2 = population[indices]\n                \n                if np.random.rand() < self.current_to_best_prob:\n                    mutant = population[i] + F * (self.x_opt - population[i]) + F * (x1 - x2)\n                else:\n                    x3 = population[np.random.choice(self.pop_size)]\n                    mutant = x1 + F * (x2 - x3)\n                \n                # Incorporate archive\n                if len(self.archive) > 0 and np.random.rand() < self.archive_prob:  # archive_prob chance to use archive\n                    archive_index = np.random.randint(len(self.archive))\n                    mutant = x1 + F * (x2 - self.archive[archive_index])\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_learning_rate:\n                    orthogonal_vector = self.x_opt + np.random.normal(0, 0.1, self.dim)  # Perturb best solution\n                    orthogonal_vector = np.clip(orthogonal_vector, func.bounds.lb, func.bounds.ub)\n                    trial = 0.5 * (trial + orthogonal_vector)  # Combine with trial vector\n                    trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                    \n                    # Record successful F and CR values\n                    self.F_history.append(F)\n                    self.CR_history.append(CR)\n                    if len(self.F_history) > 50:\n                        self.F_history.pop(0)\n                        self.CR_history.pop(0)\n\n                    # Add replaced vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                         # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = population[i]\n                        self.archive_fitness[worst_archive_index] = fitness[i]\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.stagnation_counter = 0\n                        self.previous_best_fitness = self.f_opt\n                else:\n                     # Add trial vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = trial\n                        self.archive_fitness[worst_archive_index] = f_trial\n                        \n\n            population = new_population\n            fitness = new_fitness\n            \n            # Restart population if stagnating\n            if self.stagnation_counter > self.restart_patience:\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                    self.stagnation_counter = 0\n                    self.previous_best_fitness = self.f_opt\n                else:\n                    self.stagnation_counter +=1\n                    \n                # Reset F and CR history\n                self.F_history = []\n                self.CR_history = []\n                \n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDE scored 0.706 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["8d38d9e7-e3ad-441e-b2eb-654048c3c097"], "operator": null, "metadata": {"aucs": [0.5254528890284277, 0.7420948144040587, 0.57987622325758, 0.9067259805068104, 0.780375470856004, 0.8367937077881846, 0.6933730565292492, 0.7178264784745594, 0.7716742493083222, 0.7373740127815273, 0.9000144715286792, 0.9988233608917971, 0.3253837848890605, 0.756893298760049, 0.8154820330172304, 0.8071690129473302, 0.6273453892517173, 0.8427310244627366, 0.22845788871739448, 0.5316980848272042]}}
{"id": "ff35fcc5-1168-4f88-9035-2bfe70cd186e", "fitness": 0.4402357104512413, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with a self-adjusting archive size, a more aggressive stagnation check, and covariance matrix adaptation for mutation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size_init=50, F_init=0.5, CR_init=0.7, stagnation_threshold=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size_init = archive_size_init\n        self.archive_size = archive_size_init #Dynamic archive size\n        self.F_init = F_init\n        self.CR_init = CR_init\n        self.F_memory = np.ones(self.pop_size) * self.F_init\n        self.CR_memory = np.ones(self.pop_size) * self.CR_init\n        self.archive = []\n        self.archive_fitness = []\n        self.success_F = []\n        self.success_CR = []\n        self.stagnation_threshold = stagnation_threshold\n        self.C = np.eye(dim) # Covariance matrix for CMA-ES-like mutation\n        self.c_learn = 0.1 #Learning rate for CMA\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n\n        generation = 0\n        stagnation_counter = 0\n        prev_best_fitness = self.f_opt\n        archive_clear_interval = 200\n        \n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR using success history\n                if self.success_F:\n                    F_mean = np.mean(self.success_F)\n                    self.F_memory[i] = np.clip(np.random.normal(F_mean, 0.3), 0.1, 1.0)\n                else:\n                    self.F_memory[i] = np.clip(np.random.normal(0.5, 0.3), 0.1, 1.0)\n\n                if self.success_CR:\n                    CR_mean = np.mean(self.success_CR)\n                    self.CR_memory[i] = np.clip(np.random.normal(CR_mean, 0.1), 0.0, 1.0)\n                else:\n                    self.CR_memory[i] = np.clip(np.random.normal(0.7, 0.1), 0.0, 1.0)\n\n\n                # Mutation\n                indices = np.random.choice(self.pop_size, 2, replace=False)\n                x1, x2 = population[indices]\n\n                # Utilize archive with dynamic probability\n                archive_prob = min(0.4, generation / 500)  # Increase archive usage over time\n                if len(self.archive) > 0 and np.random.rand() < archive_prob:\n                     archive_index = np.random.randint(len(self.archive))\n                     x3 = self.archive[archive_index]\n                else:\n                    indices = np.random.choice(self.pop_size, 1, replace=False)\n                    x3 = population[indices[0]]\n\n                # Weighted difference vector with CMA-ES-like adaptation\n                z = np.random.multivariate_normal(np.zeros(self.dim), self.C)\n                mutant = population[i] + self.F_memory[i] * (x1 - x2) + self.F_memory[i] * 0.5 * (x3 - population[i]) + 0.1 * z # CMA-ES part\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Orthogonal Crossover\n                trial = np.copy(population[i])\n                num_changed_vars = 0\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR_memory[i]:\n                        trial[j] = mutant[j]\n                        num_changed_vars += 1\n                # Ensure at least one variable is changed\n                if num_changed_vars == 0:\n                    j_rand = np.random.randint(self.dim)\n                    trial[j_rand] = mutant[j_rand]\n                \n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n\n                    self.success_F.append(self.F_memory[i])\n                    self.success_CR.append(self.CR_memory[i])\n                    if len(self.success_F) > 10:\n                        self.success_F.pop(0)\n                        self.success_CR.pop(0)\n\n                    # Dynamic archive management: replace worst in archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        if fitness[i] < self.archive_fitness[worst_archive_index]:\n                            self.archive[worst_archive_index] = population[i]\n                            self.archive_fitness[worst_archive_index] = fitness[i]\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        stagnation_counter = 0\n                        # CMA-ES Learning: Update Covariance matrix based on successful step\n                        d = trial - population[i]\n                        self.C = (1 - self.c_learn) * self.C + self.c_learn * np.outer(d, d)\n\n                else:\n                    # Dynamic archive management: replace worst in archive (trial vector)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        if f_trial < self.archive_fitness[worst_archive_index]:\n                            self.archive[worst_archive_index] = trial\n                            self.archive_fitness[worst_archive_index] = f_trial\n\n            population = new_population\n            fitness = new_fitness\n\n            # Aggressive stagnation check\n            if abs(self.f_opt - prev_best_fitness) < 1e-9:\n                stagnation_counter += 1\n            else:\n                stagnation_counter = 0\n            prev_best_fitness = self.f_opt\n\n            if stagnation_counter > self.stagnation_threshold:\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                stagnation_counter = 0\n                self.archive = []\n                self.archive_fitness = []\n                self.success_F = []\n                self.success_CR = []\n                self.C = np.eye(self.dim) #Reset CMA\n\n                #Adjust archive size upon restart\n                self.archive_size = min(self.archive_size_init + 10, self.pop_size * 2) #Increase size after stagnation\n\n            #Periodic archive clearing, less frequent and simpler\n            if generation % archive_clear_interval == 0 and len(self.archive) > self.archive_size // 2:\n                  worst_archive_index = np.argmax(self.archive_fitness)\n                  del self.archive[worst_archive_index]\n                  del self.archive_fitness[worst_archive_index]\n\n            #Dynamically adjust archive size\n            if generation % 100 == 0:\n              if len(self.archive) < self.archive_size // 2:\n                self.archive_size = max(self.archive_size - 5, 10) #Reduce archive if underutilized\n              elif len(self.archive) == self.archive_size:\n                self.archive_size = min(self.archive_size + 5, self.pop_size * 2) #Increase if full.\n\n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDE scored 0.440 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9e7acf1c-bcc9-4a3f-b347-c892323d9cef"], "operator": null, "metadata": {"aucs": [0.16200634716405782, 0.291144786627336, 0.4182728615579523, 0.6824211817643321, 0.3178587747775482, 0.509839744790886, 0.3103063502888235, 0.3735257055327793, 0.42038806489520264, 0.1884456108081074, 0.6668060273157421, 0.9980283752400907, 0.27691715248506465, 0.40192340674994953, 0.7616345641804017, 0.4621578113684278, 0.34916750099646365, 0.5162348597845153, 0.19715749065505794, 0.5004775920420883]}}
{"id": "fe4e2c24-28ed-4349-b03a-634b5556f5bd", "fitness": -Infinity, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with a self-adjusting archive based on fitness improvement, orthogonal learning using a Cauchy distribution, and a refined restart strategy based on both fitness and diversity.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_base=0.5, CR_base=0.7, archive_size=100, F_range=0.3, CR_range=0.3, orthogonal_learning_rate=0.1, restart_patience=50, archive_prob=0.1, current_to_best_prob=0.2, restart_threshold=1e-6):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.archive_size = archive_size\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.archive = []\n        self.archive_fitness = []\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.restart_patience = restart_patience\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.F_history = []\n        self.CR_history = []\n        self.archive_prob = archive_prob\n        self.current_to_best_prob = current_to_best_prob\n        self.restart_threshold = restart_threshold # Threshold for fitness improvement to avoid premature restarts\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n            self.stagnation_counter = 0\n            self.previous_best_fitness = self.f_opt\n        else:\n            self.stagnation_counter +=1\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n            \n            # Self-adaptive F and CR\n            if self.F_history:\n                self.F_base = np.mean(self.F_history)\n            if self.CR_history:\n                self.CR_base = np.mean(self.CR_history)\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                F = self.F_base + np.random.uniform(-self.F_range, self.F_range)\n                CR = self.CR_base + np.random.uniform(-self.CR_range, self.CR_range)\n                F = np.clip(F, 0.1, 1.0)\n                CR = np.clip(CR, 0.1, 1.0)\n\n                # Mutation: Combining current-to-best with random mutation\n                indices = np.random.choice(self.pop_size, 2, replace=False)\n                x1, x2 = population[indices]\n                \n                if np.random.rand() < self.current_to_best_prob:\n                    mutant = population[i] + F * (self.x_opt - population[i]) + F * (x1 - x2)\n                else:\n                    x3 = population[np.random.choice(self.pop_size)]\n                    mutant = x1 + F * (x2 - x3)\n                \n                # Incorporate archive\n                if len(self.archive) > 0 and np.random.rand() < self.archive_prob:  # archive_prob chance to use archive\n                    archive_index = np.random.randint(len(self.archive))\n                    mutant = x1 + F * (x2 - self.archive[archive_index])\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Orthogonal Learning with Cauchy distribution\n                if np.random.rand() < self.orthogonal_learning_rate:\n                    orthogonal_vector = self.x_opt + np.random.standard_cauchy(size=self.dim) * 0.1  # Cauchy perturbation\n                    orthogonal_vector = np.clip(orthogonal_vector, func.bounds.lb, func.bounds.ub)\n                    trial = 0.5 * (trial + orthogonal_vector)  # Combine with trial vector\n                    trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                    \n                    # Record successful F and CR values\n                    self.F_history.append(F)\n                    self.CR_history.append(CR)\n                    if len(self.F_history) > 50:\n                        self.F_history.pop(0)\n                        self.CR_history.pop(0)\n\n                    # Add replaced vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                         # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = population[i]\n                        self.archive_fitness[worst_archive_index] = fitness[i]\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.stagnation_counter = 0\n                        self.previous_best_fitness = self.f_opt\n                else:\n                     # Add trial vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = trial\n                        self.archive_fitness[worst_archive_index] = f_trial\n                        \n\n            population = new_population\n            fitness = new_fitness\n            \n            # Restart population if stagnating and sufficient improvement has not been achieved\n            if self.stagnation_counter > self.restart_patience and (self.previous_best_fitness - self.f_opt) < self.restart_threshold:\n                # Calculate diversity of the population\n                diversity = np.mean(np.std(population, axis=0))\n                \n                # Only restart if diversity is low enough, indicating convergence\n                if diversity < 0.1 * (func.bounds.ub - func.bounds.lb):\n                    population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                    fitness = np.array([func(x) for x in population])\n                    self.budget -= self.pop_size\n                    best_index = np.argmin(fitness)\n                    if fitness[best_index] < self.f_opt:\n                        self.f_opt = fitness[best_index]\n                        self.x_opt = population[best_index]\n                        self.stagnation_counter = 0\n                        self.previous_best_fitness = self.f_opt\n                    else:\n                        self.stagnation_counter +=1\n                        \n                    # Reset F and CR history\n                    self.F_history = []\n                    self.CR_history = []\n                else:\n                    self.stagnation_counter = 0 # Reset counter if diversity is still high\n\n            #Update stagnation counter even if no restart happened\n            if (self.previous_best_fitness - self.f_opt) < self.restart_threshold:\n                self.stagnation_counter +=1\n            else:\n                self.stagnation_counter = 0\n            self.previous_best_fitness = self.f_opt # Update previous best fitness\n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 144, in evaluate\n    algorithm(f_new)\n  File \"<string>\", line 145, in __call__\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n.", "error": "", "parent_ids": ["79a5e1f9-a95f-4e28-9be6-df8b6f0252dd"], "operator": null, "metadata": {}}
{"id": "4fe0fcf3-2ad6-4297-8cab-b3c3be732dc4", "fitness": 0.6641100827404103, "name": "AdaptiveDE", "description": "Enhanced Adaptive Differential Evolution with a new mutation strategy combining multiple DE variants, adaptive F and CR with covariance learning, dynamic archive management with fitness and diversity considerations, orthogonal learning, and a multi-faceted restart mechanism.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_base=0.5, CR_base=0.7, archive_size=100, F_range=0.3, CR_range=0.3, orthogonal_learning_rate=0.1, restart_patience=50, aging_rate=0.05, archive_decay_rate=0.99, mutation_strategy=\"rand1\"):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.archive_size = archive_size\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.archive = []\n        self.archive_fitness = []\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.restart_patience = restart_patience\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.F_history = []\n        self.CR_history = []\n        self.CR_history = []\n        self.aging_rate = aging_rate\n        self.archive_decay_rate = archive_decay_rate\n        self.mutation_strategy = mutation_strategy\n        self.F_covariance = np.eye(1) * 0.1  # Initial covariance for F adaptation\n        self.CR_covariance = np.eye(1) * 0.1  # Initial covariance for CR adaptation\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n            self.stagnation_counter = 0\n            self.previous_best_fitness = self.f_opt\n        else:\n            self.stagnation_counter +=1\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n            \n            # Adaptive F and CR parameter control with covariance learning\n            # Sample F and CR from Gaussian distributions with learned covariance\n            F = np.random.multivariate_normal([self.F_base], self.F_covariance)[0]\n            CR = np.random.multivariate_normal([self.CR_base], self.CR_covariance)[0]\n            F = np.clip(F, 0.1, 1.0)\n            CR = np.clip(CR, 0.1, 1.0)\n\n\n            for i in range(self.pop_size):\n                # Mutation\n                if self.mutation_strategy == \"rand1\":\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x1, x2, x3 = population[indices]\n                    mutant = x1 + F * (x2 - x3)\n\n                    # Incorporate archive\n                    if len(self.archive) > 0 and np.random.rand() < 0.1:  # 10% chance to use archive\n                        archive_index = np.random.randint(len(self.archive))\n                        mutant = x1 + F * (x2 - self.archive[archive_index])\n\n                elif self.mutation_strategy == \"current_to_best\":\n                     indices = np.random.choice(self.pop_size, 2, replace=False)\n                     x1, x2 = population[indices]\n                     mutant = population[i] + F * (self.x_opt - population[i]) + F*(x1-x2)\n\n                elif self.mutation_strategy == \"rand2\":\n                    indices = np.random.choice(self.pop_size, 5, replace=False)\n                    x1, x2, x3, x4, x5 = population[indices]\n                    mutant = x1 + F * (x2 - x3) + F*(x4 - x5)\n                else: # best2\n                    indices = np.random.choice(self.pop_size, 4, replace=False)\n                    x1, x2, x3, x4 = population[indices]\n                    mutant = self.x_opt + F * (x1 - x2) + F*(x3-x4)\n\n\n                # Repair mechanism\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_learning_rate:\n                    orthogonal_vector = self.x_opt + np.random.normal(0, 0.1, self.dim)  # Perturb best solution\n                    orthogonal_vector = np.clip(orthogonal_vector, func.bounds.lb, func.bounds.ub)\n                    trial = 0.5 * (trial + orthogonal_vector)  # Combine with trial vector\n                    trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                    \n                    # Record successful F and CR values\n                    self.F_history.append(F)\n                    self.CR_history.append(CR)\n                    if len(self.F_history) > 50:\n                        self.F_history.pop(0)\n                        self.CR_history.pop(0)\n\n                    # Update covariance matrices based on successful F and CR\n                    self.F_covariance = np.cov(np.array(self.F_history).flatten(), rowvar=False) + np.eye(1)*0.001 # Add small value for stability\n                    self.CR_covariance = np.cov(np.array(self.CR_history).flatten(), rowvar=False) + np.eye(1)*0.001\n\n                    # Dynamic Archive management (fitness and diversity)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                        # Replace vector with worst fitness or closest to existing archive members\n                        if np.random.rand() < 0.5:\n                            #Replace based on fitness\n                            worst_archive_index = np.argmax(self.archive_fitness)\n                            self.archive[worst_archive_index] = population[i]\n                            self.archive_fitness[worst_archive_index] = fitness[i]\n                        else:\n                            #Replace based on diversity\n                            distances = [np.linalg.norm(population[i] - archive_member) for archive_member in self.archive]\n                            closest_archive_index = np.argmin(distances)\n                            self.archive[closest_archive_index] = population[i]\n                            self.archive_fitness[closest_archive_index] = fitness[i]\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.stagnation_counter = 0\n                        self.previous_best_fitness = self.f_opt\n                else:\n                     # Add trial vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = trial\n                        self.archive_fitness[worst_archive_index] = f_trial\n                        \n\n            population = new_population\n            fitness = new_fitness\n            \n            # Multi-faceted Restart mechanism\n            if self.stagnation_counter > self.restart_patience:\n                if np.random.rand() < 0.3:\n                    #Restart from best\n                    population = self.x_opt + np.random.normal(0, 0.1, size=(self.pop_size, self.dim))\n                    population = np.clip(population, func.bounds.lb, func.bounds.ub)\n\n                elif np.random.rand() < 0.6:\n                    # Restart using Latin Hypercube Sampling\n                     population = np.random.uniform(0, 1, size=(self.pop_size, self.dim))\n                     for j in range(self.dim):\n                        idx = np.random.permutation(self.pop_size)\n                        population[:, j] = (idx + population[:, j]) / self.pop_size\n                        population[:,j] = func.bounds.lb[j] + population[:,j] * (func.bounds.ub[j] - func.bounds.lb[j])\n\n\n                else:\n\n                    # Restart with random population\n                    population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                    self.stagnation_counter = 0\n                    self.previous_best_fitness = self.f_opt\n                else:\n                    self.stagnation_counter +=1\n                    \n                # Reset F and CR history\n                self.F_history = []\n                self.CR_history = []\n                \n            # Archive aging\n            for k in range(len(self.archive_fitness)):\n                self.archive_fitness[k] *= self.archive_decay_rate # Gradual decay\n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDE scored 0.664 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b659a3c2-716b-47f8-a511-5ce90c271192"], "operator": null, "metadata": {"aucs": [0.25616812375432707, 0.6263572674440057, 0.6320485065733552, 0.8866428778721789, 0.7145343973832231, 0.7613316620064645, 0.5543750373928815, 0.5771901094347638, 0.7244504371146248, 0.5931932622149263, 0.8572757766197243, 0.9996532756635713, 0.4223528753661707, 0.671149172298348, 0.8936740757434058, 0.7663773586678676, 0.5521827632374207, 0.8217101835844063, 0.452953883071885, 0.5185806093646559]}}
{"id": "0e08a803-4838-421b-a3da-f836ecc036c0", "fitness": 0.669191707054708, "name": "AdaptiveDE", "description": "Enhanced Adaptive Differential Evolution with self-adaptive parameters, a combined archive with aging and clustering, orthogonal learning, and a multi-strategy mutation with probabilistic selection.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size_init=50, F_init=0.5, CR_init=0.7, num_clusters=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size_init = archive_size_init\n        self.archive_size = archive_size_init\n        self.F_init = F_init\n        self.CR_init = CR_init\n        self.F_memory = np.ones(self.pop_size) * self.F_init\n        self.CR_memory = np.ones(self.pop_size) * self.CR_init\n        self.archive = []\n        self.archive_fitness = []\n        self.archive_age = []\n        self.success_F = []\n        self.success_CR = []\n        self.success_count = 0\n        self.failure_count = 0\n        self.num_clusters = num_clusters # Number of clusters for archive management\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n\n        generation = 0\n        stagnation_counter = 0\n        prev_best_fitness = self.f_opt\n        archive_clear_interval = 200\n        \n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n\n            # Adjust archive size dynamically\n            success_rate = self.success_count / (self.success_count + self.failure_count + 1e-9)\n            self.archive_size = int(self.archive_size_init * (0.5 + success_rate))\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                if self.success_F:\n                    F_mean = np.mean(self.success_F)\n                    self.F_memory[i] = np.clip(np.random.normal(F_mean, 0.3), 0.1, 1.0)\n                else:\n                    self.F_memory[i] = np.clip(np.random.normal(0.5, 0.3), 0.1, 1.0)\n\n                if self.success_CR:\n                    CR_mean = np.mean(self.success_CR)\n                    self.CR_memory[i] = np.clip(np.random.normal(CR_mean, 0.1), 0.0, 1.0)\n                else:\n                    self.CR_memory[i] = np.clip(np.random.normal(0.7, 0.1), 0.0, 1.0)\n\n                # Mutation Strategies with Probabilistic Selection\n                mutation_strategy = np.random.choice([1, 2, 3], p=[0.4, 0.3, 0.3]) # Adjusted probabilities\n                \n                if mutation_strategy == 1:\n                    # DE/rand/1\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x1, x2, x3 = population[indices]\n                    mutant = x1 + self.F_memory[i] * (x2 - x3)\n                elif mutation_strategy == 2:\n                    # DE/current-to-best/1\n                    indices = np.random.choice(self.pop_size, 2, replace=False)\n                    x1, x2 = population[indices]\n                    mutant = population[i] + self.F_memory[i] * (self.x_opt - population[i]) + self.F_memory[i] * (x1 - x2)\n                else:\n                    # DE/rand/2 with archive\n                    indices = np.random.choice(self.pop_size, 2, replace=False)\n                    x1, x2 = population[indices]\n                    if len(self.archive) > 0:\n                        archive_index = np.random.randint(len(self.archive))\n                        x3 = self.archive[archive_index]\n                    else:\n                        x3 = population[np.random.choice(self.pop_size)]\n                    x4 = population[np.random.choice(self.pop_size)]\n                    mutant = x1 + self.F_memory[i] * (x2 - x3) + self.F_memory[i] * (x4 - population[i])\n\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Orthogonal Crossover\n                trial = np.copy(population[i])\n                num_changed_vars = 0\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR_memory[i]:\n                        trial[j] = mutant[j]\n                        num_changed_vars += 1\n                if num_changed_vars == 0:\n                    j_rand = np.random.randint(self.dim)\n                    trial[j_rand] = mutant[j_rand]\n                \n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n\n                    self.success_F.append(self.F_memory[i])\n                    self.success_CR.append(self.CR_memory[i])\n                    if len(self.success_F) > 10:\n                        self.success_F.pop(0)\n                        self.success_CR.pop(0)\n                    self.success_count += 1\n\n                    # Dynamic archive management: replace worst in archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                        self.archive_fitness.append(fitness[i])\n                        self.archive_age.append(0)\n                    else:\n                        if len(self.archive) > 0:\n                            worst_archive_index = np.argmax(self.archive_fitness)\n                            if fitness[i] < self.archive_fitness[worst_archive_index]:\n                                self.archive[worst_archive_index] = population[i]\n                                self.archive_fitness[worst_archive_index] = fitness[i]\n                                self.archive_age[worst_archive_index] = 0\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        stagnation_counter = 0\n                else:\n                    self.failure_count += 1\n                    # Dynamic archive management: replace worst in archive (trial vector)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                        self.archive_age.append(0)\n                    else:\n                         if len(self.archive) > 0:\n                            worst_archive_index = np.argmax(self.archive_fitness)\n                            if f_trial < self.archive_fitness[worst_archive_index]:\n                                self.archive[worst_archive_index] = trial\n                                self.archive_fitness[worst_archive_index] = f_trial\n                                self.archive_age[worst_archive_index] = 0\n\n            # Update age of archive members\n            self.archive_age = [age + 1 for age in self.archive_age]\n\n            population = new_population\n            fitness = new_fitness\n\n            # Restart population if stagnating\n            if abs(self.f_opt - prev_best_fitness) < 1e-8:\n                stagnation_counter += 1\n            else:\n                stagnation_counter = 0\n            prev_best_fitness = self.f_opt\n\n            if stagnation_counter > 100:\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                stagnation_counter = 0\n                self.archive = []\n                self.archive_fitness = []\n                self.archive_age = []\n                self.success_F = []\n                self.success_CR = []\n                self.success_count = 0\n                self.failure_count = 0\n\n            # Periodic archive clearing and clustering\n            if generation % archive_clear_interval == 0 and len(self.archive) > 0:\n                # Clustering to maintain diversity\n                try:\n                    kmeans = KMeans(n_clusters=min(self.num_clusters, len(self.archive)), random_state=0, n_init=10) # Ensure n_clusters <= n_samples\n                    kmeans.fit(self.archive)\n                    cluster_labels = kmeans.labels_\n\n                    # Keep only the best individual from each cluster\n                    new_archive = []\n                    new_archive_fitness = []\n                    new_archive_age = []\n                    for cluster_id in range(kmeans.n_clusters):\n                        cluster_indices = np.where(cluster_labels == cluster_id)[0]\n                        best_index_in_cluster = cluster_indices[np.argmin([self.archive_fitness[i] for i in cluster_indices])]\n                        new_archive.append(self.archive[best_index_in_cluster])\n                        new_archive_fitness.append(self.archive_fitness[best_index_in_cluster])\n                        new_archive_age.append(0) # Reset age\n\n                    self.archive = new_archive\n                    self.archive_fitness = new_archive_fitness\n                    self.archive_age = new_archive_age\n                except ValueError as e:\n                    print(f\"KMeans clustering failed: {e}\")\n                    self.archive = []\n                    self.archive_fitness = []\n                    self.archive_age = []\n\n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDE scored 0.669 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["37d06b96-6b2a-4796-8519-b7504bc2ca46"], "operator": null, "metadata": {"aucs": [0.4445043949191665, 0.7182749802962953, 0.5317680232094653, 0.8544725489843041, 0.6647763774238504, 0.7369013227755377, 0.43355023537799975, 0.6784253709472925, 0.7458055945493056, 0.6498412522141511, 0.8380782756767489, 0.996812609948616, 0.7518365632619614, 0.3971877260806541, 0.9205673291638355, 0.7646951618208486, 0.6412517863302254, 0.8567020012071259, 0.2511956276175972, 0.5071869592891802]}}
{"id": "410e13d1-304a-4061-8a68-eb3348501bbd", "fitness": 0.5495169439513621, "name": "AdaptiveDE", "description": "Enhanced Adaptive Differential Evolution with a success-history based adaptation of F and CR, a toroidal mutation to handle boundary constraints, orthogonal learning, a combined archive strategy, and a dynamic population size adjustment.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size_min=20, pop_size_max=100, F_base=0.5, CR_base=0.7, archive_size=100, F_range=0.3, CR_range=0.3, orthogonal_learning_rate=0.1, restart_patience=50, cauchy_mutation_prob=0.05, success_history_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_min = pop_size_min\n        self.pop_size_max = pop_size_max\n        self.pop_size = pop_size_max  # Start with maximum population size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.archive_size = archive_size\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.archive = []\n        self.archive_fitness = []\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.restart_patience = restart_patience\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.success_history_size = success_history_size\n        self.F_history = []\n        self.CR_history = []\n        self.success_count = 0  # Track successful generations for pop size adjustment\n        self.cauchy_mutation_prob = cauchy_mutation_prob\n        self.lb = -5.0\n        self.ub = 5.0\n\n\n    def toroidal_mutation(self, x1, x2, x3, F):\n        mutant = x1 + F * (x2 - x3)\n        # Toroidal handling of boundaries\n        for i in range(self.dim):\n            if mutant[i] < self.lb:\n                mutant[i] = self.ub - (self.lb - mutant[i]) % (self.ub - self.lb)\n            elif mutant[i] > self.ub:\n                mutant[i] = self.lb + (mutant[i] - self.ub) % (self.ub - self.lb)\n        return mutant\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n            self.stagnation_counter = 0\n            self.previous_best_fitness = self.f_opt\n        else:\n            self.stagnation_counter +=1\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n            \n            # Self-adaptive F and CR (Success-History Based Adaptation)\n            if self.F_history:\n                self.F_base = np.mean(self.F_history)\n            if self.CR_history:\n                self.CR_base = np.mean(self.CR_history)\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                F = self.F_base + np.random.uniform(-self.F_range, self.F_range)\n                CR = self.CR_base + np.random.uniform(-self.CR_range, self.CR_range)\n                F = np.clip(F, 0.1, 1.0)\n                CR = np.clip(CR, 0.1, 1.0)\n\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                \n                # Apply Cauchy mutation with a certain probability\n                if np.random.rand() < self.cauchy_mutation_prob:\n                    mutant = self.toroidal_mutation(x1, x2, x3, F) + 0.1 * np.random.standard_cauchy(size=self.dim)  # Cauchy mutation\n                    mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                else:\n                    mutant = self.toroidal_mutation(x1, x2, x3, F)\n\n                # Incorporate archive\n                if len(self.archive) > 0 and np.random.rand() < 0.1:  # 10% chance to use archive\n                    archive_index = np.random.randint(len(self.archive))\n                    mutant = self.toroidal_mutation(x1, x2, self.archive[archive_index], F)\n                    \n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_learning_rate:\n                    orthogonal_vector = self.x_opt + np.random.normal(0, 0.1, self.dim)  # Perturb best solution\n                    orthogonal_vector = np.clip(orthogonal_vector, func.bounds.lb, func.bounds.ub)\n                    trial = 0.5 * (trial + orthogonal_vector)  # Combine with trial vector\n                    trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                    \n                    # Record successful F and CR values\n                    self.F_history.append(F)\n                    self.CR_history.append(CR)\n                    if len(self.F_history) > self.success_history_size:\n                        self.F_history.pop(0)\n                        self.CR_history.pop(0)\n\n                    # Add replaced vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                         # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = population[i]\n                        self.archive_fitness[worst_archive_index] = fitness[i]\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.stagnation_counter = 0\n                        self.previous_best_fitness = self.f_opt\n                        self.success_count += 1\n                else:\n                     # Add trial vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = trial\n                        self.archive_fitness[worst_archive_index] = f_trial\n                        \n\n            population = new_population\n            fitness = new_fitness\n\n            # Dynamic population size adjustment\n            if self.success_count > 0.2 * self.pop_size: # Increase pop size\n                self.pop_size = min(self.pop_size + 1, self.pop_size_max)\n                self.success_count = 0\n            elif self.stagnation_counter > self.restart_patience/2: # Decrease pop size\n                 self.pop_size = max(self.pop_size - 1, self.pop_size_min)\n                 self.stagnation_counter = 0\n            \n            # Restart population if stagnating\n            if self.stagnation_counter > self.restart_patience:\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                    self.stagnation_counter = 0\n                    self.previous_best_fitness = self.f_opt\n                else:\n                    self.stagnation_counter +=1\n                    \n                # Reset F and CR history\n                self.F_history = []\n                self.CR_history = []\n                \n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDE scored 0.550 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9766597b-3dad-42f6-8c14-48987b4db324"], "operator": null, "metadata": {"aucs": [0.26825430585593835, 0.4757510938056263, 0.4878751230432795, 0.8381080015729827, 0.47479451127764305, 0.5782687518897817, 0.41344865771267614, 0.4592394722430424, 0.5462293093498171, 0.2358131625877833, 0.8295859248087819, 0.9989420654971987, 0.4625703023472254, 0.5430217987611468, 0.8480613332394561, 0.5834310108082181, 0.44698099089474674, 0.7194731159311001, 0.2865369190374599, 0.4939530283633379]}}
{"id": "0ff8f24d-a0bd-4629-bc27-2c5d5451776f", "fitness": 0.6534332668936663, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with a self-adaptive strategy, an archive, orthogonal learning, restarts, Cauchy mutation, and a novel local search mechanism triggered upon stagnation for fine-tuning.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_base=0.5, CR_base=0.7, archive_size=100, F_range=0.3, CR_range=0.3, orthogonal_learning_rate=0.1, restart_patience=50, cauchy_mutation_prob=0.05, local_search_prob=0.1, local_search_radius=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.archive_size = archive_size\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.archive = []\n        self.archive_fitness = []\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.restart_patience = restart_patience\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.F_history = []\n        self.CR_history = []\n        self.cauchy_mutation_prob = cauchy_mutation_prob\n        self.local_search_prob = local_search_prob\n        self.local_search_radius = local_search_radius\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n            self.stagnation_counter = 0\n            self.previous_best_fitness = self.f_opt\n        else:\n            self.stagnation_counter +=1\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n            \n            # Self-adaptive F and CR\n            if self.F_history:\n                self.F_base = np.mean(self.F_history)\n            if self.CR_history:\n                self.CR_base = np.mean(self.CR_history)\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                F = self.F_base + np.random.uniform(-self.F_range, self.F_range)\n                CR = self.CR_base + np.random.uniform(-self.CR_range, self.CR_range)\n                F = np.clip(F, 0.1, 1.0)\n                CR = np.clip(CR, 0.1, 1.0)\n\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                \n                # Apply Cauchy mutation with a certain probability\n                if np.random.rand() < self.cauchy_mutation_prob:\n                    mutant = x1 + F * (x2 - x3) + 0.1 * np.random.standard_cauchy(size=self.dim)  # Cauchy mutation\n                else:\n                    mutant = x1 + F * (x2 - x3)\n\n                # Incorporate archive\n                if len(self.archive) > 0 and np.random.rand() < 0.1:  # 10% chance to use archive\n                    archive_index = np.random.randint(len(self.archive))\n                    mutant = x1 + F * (x2 - self.archive[archive_index])\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_learning_rate:\n                    orthogonal_vector = self.x_opt + np.random.normal(0, 0.1, self.dim)  # Perturb best solution\n                    orthogonal_vector = np.clip(orthogonal_vector, func.bounds.lb, func.bounds.ub)\n                    trial = 0.5 * (trial + orthogonal_vector)  # Combine with trial vector\n                    trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                    \n                    # Record successful F and CR values\n                    self.F_history.append(F)\n                    self.CR_history.append(CR)\n                    if len(self.F_history) > 50:\n                        self.F_history.pop(0)\n                        self.CR_history.pop(0)\n\n                    # Add replaced vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                         # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = population[i]\n                        self.archive_fitness[worst_archive_index] = fitness[i]\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.stagnation_counter = 0\n                        self.previous_best_fitness = self.f_opt\n                else:\n                     # Add trial vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = trial\n                        self.archive_fitness[worst_archive_index] = f_trial\n                        \n\n            population = new_population\n            fitness = new_fitness\n            \n            # Local Search\n            if self.stagnation_counter > self.restart_patience / 2 and np.random.rand() < self.local_search_prob:\n                # Perform local search around the best solution\n                for _ in range(5): # Limited local search steps\n                    neighbor = self.x_opt + np.random.uniform(-self.local_search_radius, self.local_search_radius, self.dim)\n                    neighbor = np.clip(neighbor, func.bounds.lb, func.bounds.ub)\n                    f_neighbor = func(neighbor)\n                    self.budget -= 1\n\n                    if f_neighbor < self.f_opt:\n                        self.f_opt = f_neighbor\n                        self.x_opt = neighbor\n                        self.stagnation_counter = 0\n                        self.previous_best_fitness = self.f_opt\n                        break  # Exit local search if improvement found\n            \n            # Restart population if stagnating\n            if self.stagnation_counter > self.restart_patience:\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                    self.stagnation_counter = 0\n                    self.previous_best_fitness = self.f_opt\n                else:\n                    self.stagnation_counter +=1\n                    \n                # Reset F and CR history\n                self.F_history = []\n                self.CR_history = []\n                \n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDE scored 0.653 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9766597b-3dad-42f6-8c14-48987b4db324"], "operator": null, "metadata": {"aucs": [0.3518033379382247, 0.3746745817073197, 0.6055686196543788, 0.8943121227152268, 0.7647488926287719, 0.7907276349972463, 0.6902028014332708, 0.4320523435067125, 0.7394013097799499, 0.6216904788898068, 0.8954218188258235, 0.9990627624635704, 0.3147840143205879, 0.7285410053145447, 0.8111703771770539, 0.7761079535559533, 0.3961383300482715, 0.859029826717483, 0.526577348686531, 0.4966497775125971]}}
{"id": "be34ac8d-13ff-4207-ad70-bb80883ee23a", "fitness": 0.6811184175349823, "name": "AdaptiveDE_SOM", "description": "Enhanced Adaptive Differential Evolution with a self-organizing map (SOM) for population diversity, adaptive F and CR using weighted historical success, orthogonal learning, a combined archive with aging, and a dynamic restart strategy based on SOM-detected stagnation.", "code": "import numpy as np\n\nclass AdaptiveDE_SOM:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_base=0.5, CR_base=0.7, archive_size=100, F_range=0.3, CR_range=0.3, orthogonal_learning_rate=0.1, som_grid_size=10, som_learning_rate=0.1, som_sigma=1.0, aging_rate=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.archive_size = archive_size\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.archive = []\n        self.archive_fitness = []\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.aging_rate = aging_rate\n        self.F_history = []\n        self.CR_history = []\n\n        # SOM parameters\n        self.som_grid_size = som_grid_size\n        self.som_learning_rate = som_learning_rate\n        self.som_sigma = som_sigma\n        self.som_weights = np.random.rand(som_grid_size, som_grid_size, dim)  # SOM grid\n        self.som_stagnation_threshold = 0.9  # Threshold for stagnation detection\n        self.som_stagnation_patience = 3 #Number of generations to wait before restart\n\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.restart_flag = False\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n            self.previous_best_fitness = self.f_opt\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n\n            # Adaptive F and CR using weighted historical success\n            if self.F_history and self.CR_history:\n                weights_F = np.linspace(0.1, 1.0, len(self.F_history))\n                weights_CR = np.linspace(0.1, 1.0, len(self.CR_history))\n\n                F_weighted_avg = np.average(self.F_history, weights=weights_F)\n                CR_weighted_avg = np.average(self.CR_history, weights=weights_CR)\n            else:\n                F_weighted_avg = self.F_base\n                CR_weighted_avg = self.CR_base\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                F = F_weighted_avg + np.random.uniform(-self.F_range, self.F_range)\n                CR = CR_weighted_avg + np.random.uniform(-self.CR_range, self.CR_range)\n                F = np.clip(F, 0.1, 1.0)\n                CR = np.clip(CR, 0.1, 1.0)\n\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = x1 + F * (x2 - x3)\n\n                # Incorporate archive\n                if len(self.archive) > 0 and np.random.rand() < 0.1:  # 10% chance to use archive\n                    archive_index = np.random.randint(len(self.archive))\n                    mutant = x1 + F * (x2 - self.archive[archive_index])\n\n                # Repair mechanism\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_learning_rate:\n                    orthogonal_vector = self.x_opt + np.random.normal(0, 0.1, self.dim)  # Perturb best solution\n                    orthogonal_vector = np.clip(orthogonal_vector, func.bounds.lb, func.bounds.ub)\n                    trial = 0.5 * (trial + orthogonal_vector)  # Combine with trial vector\n                    trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                    \n                    # Record successful F and CR values\n                    self.F_history.append(F)\n                    self.CR_history.append(CR)\n                    if len(self.F_history) > 50:\n                        self.F_history.pop(0)\n                        self.CR_history.pop(0)\n\n                    # Add replaced vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                         # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = population[i]\n                        self.archive_fitness[worst_archive_index] = fitness[i]\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.previous_best_fitness = self.f_opt\n                else:\n                     # Add trial vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = trial\n                        self.archive_fitness[worst_archive_index] = f_trial\n\n            population = new_population\n            fitness = new_fitness\n\n            # SOM training (diversity maintenance)\n            self.train_som(population)\n            diversity_metric = self.calculate_diversity_metric(population)\n            \n            if diversity_metric > self.som_stagnation_threshold:\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n\n            # Dynamic restart based on SOM-detected stagnation\n            if self.stagnation_counter > self.som_stagnation_patience:\n                population = self.initialize_population_from_som(func.bounds.lb, func.bounds.ub)\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                \n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                    self.previous_best_fitness = self.f_opt\n                \n                self.F_history = []\n                self.CR_history = []\n                self.stagnation_counter = 0\n\n\n            # Archive aging\n            for k in range(len(self.archive_fitness)):\n                self.archive_fitness[k] *= (1 - self.aging_rate)\n            \n            if self.budget <= 0:\n                break\n        return self.f_opt, self.x_opt\n\n    def train_som(self, population):\n        \"\"\"Trains the Self-Organizing Map (SOM) with the current population.\"\"\"\n        for x in population:\n            # Find the best matching unit (BMU)\n            bmu_index = self.find_bmu(x)\n\n            # Update the SOM weights around the BMU\n            for i in range(self.som_grid_size):\n                for j in range(self.som_grid_size):\n                    distance = np.sqrt((i - bmu_index[0])**2 + (j - bmu_index[1])**2)\n                    influence = np.exp(-distance**2 / (2 * self.som_sigma**2))\n                    self.som_weights[i, j] += self.som_learning_rate * influence * (x - self.som_weights[i, j])\n\n    def find_bmu(self, individual):\n        \"\"\"Finds the best matching unit (BMU) in the SOM for a given individual.\"\"\"\n        min_distance = np.inf\n        bmu_index = None\n        for i in range(self.som_grid_size):\n            for j in range(self.som_grid_size):\n                distance = np.linalg.norm(individual - self.som_weights[i, j])\n                if distance < min_distance:\n                    min_distance = distance\n                    bmu_index = (i, j)\n        return bmu_index\n\n    def calculate_diversity_metric(self, population):\n        \"\"\"Calculates a diversity metric based on the SOM.\"\"\"\n        bmu_indices = [self.find_bmu(x) for x in population]\n        unique_bmu_count = len(set(bmu_indices))\n        return unique_bmu_count / self.pop_size\n\n    def initialize_population_from_som(self, lower_bound, upper_bound):\n        \"\"\"Initializes a new population by sampling from the SOM.\"\"\"\n        new_population = np.zeros((self.pop_size, self.dim))\n        for i in range(self.pop_size):\n            # Randomly select a node from the SOM\n            row = np.random.randint(0, self.som_grid_size)\n            col = np.random.randint(0, self.som_grid_size)\n            \n            # Perturb the SOM node's weights to create a new individual\n            new_individual = self.som_weights[row, col] + np.random.normal(0, 0.1, self.dim)\n            new_individual = np.clip(new_individual, lower_bound, upper_bound)\n            new_population[i] = new_individual\n        return new_population", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDE_SOM scored 0.681 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b659a3c2-716b-47f8-a511-5ce90c271192"], "operator": null, "metadata": {"aucs": [0.286025823811335, 0.462284915471468, 0.7117251267913238, 0.8996247555688737, 0.7726149122396334, 0.8118578245059977, 0.6794361823939661, 0.6589253189156145, 0.7557039430003075, 0.4013504347876925, 0.8986518162216167, 0.9996043358919017, 0.44867771533774636, 0.7548970523952874, 0.8511900184533637, 0.8112338708324947, 0.6130197962446557, 0.8543971409841689, 0.4164625487721413, 0.5346848180800572]}}
{"id": "39b1104e-850a-489e-b666-bc4963790aa9", "fitness": 0.7082510709564235, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with orthogonal learning, a success-history based adaptation of F and CR with separate memories for successful and unsuccessful values, and a more aggressive archive update strategy.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_base=0.5, CR_base=0.7, archive_size=100, F_range=0.3, CR_range=0.3, orthogonal_learning_rate=0.1, restart_patience=50, archive_prob=0.1, current_to_best_prob=0.2, gamma_F = 0.1, gamma_CR = 0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.archive_size = archive_size\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.archive = []\n        self.archive_fitness = []\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.restart_patience = restart_patience\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.F_success_history = []\n        self.CR_success_history = []\n        self.F_failure_history = []\n        self.CR_failure_history = []\n        self.archive_prob = archive_prob\n        self.current_to_best_prob = current_to_best_prob\n        self.gamma_F = gamma_F\n        self.gamma_CR = gamma_CR\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n            self.stagnation_counter = 0\n            self.previous_best_fitness = self.f_opt\n        else:\n            self.stagnation_counter +=1\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n            \n            # Self-adaptive F and CR\n            if self.F_success_history:\n                self.F_base = (1-self.gamma_F) * self.F_base + self.gamma_F * np.mean(self.F_success_history)\n                self.F_base = np.clip(self.F_base, 0.1, 1.0)\n            if self.CR_success_history:\n                self.CR_base = (1-self.gamma_CR) * self.CR_base + self.gamma_CR * np.mean(self.CR_success_history)\n                self.CR_base = np.clip(self.CR_base, 0.1, 1.0)\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                F = self.F_base + np.random.uniform(-self.F_range, self.F_range)\n                CR = self.CR_base + np.random.uniform(-self.CR_range, self.CR_range)\n                F = np.clip(F, 0.1, 1.0)\n                CR = np.clip(CR, 0.1, 1.0)\n\n                # Mutation: Combining current-to-best with random mutation\n                indices = np.random.choice(self.pop_size, 2, replace=False)\n                x1, x2 = population[indices]\n                \n                if np.random.rand() < self.current_to_best_prob:\n                    mutant = population[i] + F * (self.x_opt - population[i]) + F * (x1 - x2)\n                else:\n                    x3 = population[np.random.choice(self.pop_size)]\n                    mutant = x1 + F * (x2 - x3)\n                \n                # Incorporate archive\n                if len(self.archive) > 0 and np.random.rand() < self.archive_prob:  # archive_prob chance to use archive\n                    archive_index = np.random.randint(len(self.archive))\n                    mutant = x1 + F * (x2 - self.archive[archive_index])\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_learning_rate:\n                    orthogonal_vector = self.x_opt + np.random.normal(0, 0.1, self.dim)  # Perturb best solution\n                    orthogonal_vector = np.clip(orthogonal_vector, func.bounds.lb, func.bounds.ub)\n                    trial = 0.5 * (trial + orthogonal_vector)  # Combine with trial vector\n                    trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                    \n                    # Record successful F and CR values\n                    self.F_success_history.append(F)\n                    self.CR_success_history.append(CR)\n                    if len(self.F_success_history) > 50:\n                        self.F_success_history.pop(0)\n                        self.CR_success_history.pop(0)\n                    if self.F_failure_history:\n                        self.F_failure_history.pop(0)\n                    if self.CR_failure_history:\n                        self.CR_failure_history.pop(0)\n\n\n                    # Add replaced vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                         # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = population[i]\n                        self.archive_fitness[worst_archive_index] = fitness[i]\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.stagnation_counter = 0\n                        self.previous_best_fitness = self.f_opt\n                else:\n                    self.F_failure_history.append(F)\n                    self.CR_failure_history.append(CR)\n                    if len(self.F_failure_history) > 50:\n                         self.F_failure_history.pop(0)\n                    if len(self.CR_failure_history) > 50:\n                         self.CR_failure_history.pop(0)\n                    if self.F_success_history:\n                        self.F_success_history.pop(0)\n                    if self.CR_success_history:\n                        self.CR_success_history.pop(0)\n\n                     # Add trial vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        if f_trial < self.archive_fitness[worst_archive_index]:\n                            self.archive[worst_archive_index] = trial\n                            self.archive_fitness[worst_archive_index] = f_trial\n                        \n\n            population = new_population\n            fitness = new_fitness\n            \n            # Restart population if stagnating\n            if self.stagnation_counter > self.restart_patience:\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                    self.stagnation_counter = 0\n                    self.previous_best_fitness = self.f_opt\n                else:\n                    self.stagnation_counter +=1\n                    \n                # Reset F and CR history\n                self.F_success_history = []\n                self.CR_success_history = []\n                self.F_failure_history = []\n                self.CR_failure_history = []\n                \n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDE scored 0.708 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["79a5e1f9-a95f-4e28-9be6-df8b6f0252dd"], "operator": null, "metadata": {"aucs": [0.307937353002255, 0.7094978147649378, 0.698577241053998, 0.8942034107417102, 0.7371204456655744, 0.8025215495754152, 0.655896364956174, 0.7042641628337349, 0.7690155660723117, 0.7034439730276107, 0.8901961326359008, 0.9993224509490102, 0.5946501059239846, 0.7343345878188532, 0.9423519491352974, 0.7894830417764076, 0.6210213186564997, 0.8554647348725345, 0.24065330268422158, 0.5150659129820404]}}
{"id": "f7cc0011-b431-43a2-add9-323ceae9dd50", "fitness": 0.6520793698957915, "name": "AdaptiveDE", "description": "Introducing a locality-based mutation operator in Adaptive Differential Evolution, focusing search efforts around promising regions and adaptive learning based on fitness improvement.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_base=0.5, CR_base=0.7, archive_size=100, F_range=0.3, CR_range=0.3, orthogonal_learning_rate=0.1, restart_patience=50, archive_prob=0.1, current_to_best_prob=0.2, locality_prob=0.3, locality_radius=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.archive_size = archive_size\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.archive = []\n        self.archive_fitness = []\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.restart_patience = restart_patience\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.F_history = []\n        self.CR_history = []\n        self.archive_prob = archive_prob\n        self.current_to_best_prob = current_to_best_prob\n        self.locality_prob = locality_prob  # Probability of using locality-based mutation\n        self.locality_radius = locality_radius # Radius for locality-based mutation\n        self.success_memory = np.zeros(self.pop_size) # Track individual success\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n            self.stagnation_counter = 0\n            self.previous_best_fitness = self.f_opt\n        else:\n            self.stagnation_counter +=1\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n            \n            # Self-adaptive F and CR\n            if self.F_history:\n                self.F_base = np.mean(self.F_history)\n            if self.CR_history:\n                self.CR_base = np.mean(self.CR_history)\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                F = self.F_base + np.random.uniform(-self.F_range, self.F_range)\n                CR = self.CR_base + np.random.uniform(-self.CR_range, self.CR_range)\n                F = np.clip(F, 0.1, 1.0)\n                CR = np.clip(CR, 0.1, 1.0)\n\n                # Mutation: Locality-based or Combining current-to-best with random mutation\n                if np.random.rand() < self.locality_prob and self.success_memory[i] > 0: # locality based mutation\n                    mutant = population[i] + np.random.normal(0, self.locality_radius, self.dim)\n                else:\n                    indices = np.random.choice(self.pop_size, 2, replace=False)\n                    x1, x2 = population[indices]\n                    \n                    if np.random.rand() < self.current_to_best_prob:\n                        mutant = population[i] + F * (self.x_opt - population[i]) + F * (x1 - x2)\n                    else:\n                        x3 = population[np.random.choice(self.pop_size)]\n                        mutant = x1 + F * (x2 - x3)\n                    \n                    # Incorporate archive\n                    if len(self.archive) > 0 and np.random.rand() < self.archive_prob:  # archive_prob chance to use archive\n                        archive_index = np.random.randint(len(self.archive))\n                        mutant = x1 + F * (x2 - self.archive[archive_index])\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_learning_rate:\n                    orthogonal_vector = self.x_opt + np.random.normal(0, 0.1, self.dim)  # Perturb best solution\n                    orthogonal_vector = np.clip(orthogonal_vector, func.bounds.lb, func.bounds.ub)\n                    trial = 0.5 * (trial + orthogonal_vector)  # Combine with trial vector\n                    trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                    self.success_memory[i] += 1\n                    \n                    # Record successful F and CR values\n                    self.F_history.append(F)\n                    self.CR_history.append(CR)\n                    if len(self.F_history) > 50:\n                        self.F_history.pop(0)\n                        self.CR_history.pop(0)\n\n                    # Add replaced vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                         # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = population[i]\n                        self.archive_fitness[worst_archive_index] = fitness[i]\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.stagnation_counter = 0\n                        self.previous_best_fitness = self.f_opt\n                else:\n                     # Add trial vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = trial\n                        self.archive_fitness[worst_archive_index] = f_trial\n                    self.success_memory[i] = max(0, self.success_memory[i] - 0.1)\n                        \n\n            population = new_population\n            fitness = new_fitness\n            \n            # Restart population if stagnating\n            if self.stagnation_counter > self.restart_patience:\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                    self.stagnation_counter = 0\n                    self.previous_best_fitness = self.f_opt\n                else:\n                    self.stagnation_counter +=1\n                    \n                # Reset F and CR history\n                self.F_history = []\n                self.CR_history = []\n                self.success_memory = np.zeros(self.pop_size)\n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDE scored 0.652 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["79a5e1f9-a95f-4e28-9be6-df8b6f0252dd"], "operator": null, "metadata": {"aucs": [0.2547970037749082, 0.701635148217732, 0.5887850364119329, 0.9032914286052477, 0.7544256627696965, 0.7422264237894884, 0.42900791219591927, 0.5769862100382828, 0.6648308100123791, 0.650357768899551, 0.8939083889968382, 0.9986624640300421, 0.5152285610047813, 0.6846820082801839, 0.8695421362052821, 0.7092305927389606, 0.533651933327197, 0.8332139468241573, 0.2356023282145573, 0.5015216335786923]}}
{"id": "08bf3044-da5b-48da-8506-2faee1a7c2b8", "fitness": 0.6617308284607825, "name": "AdaptiveDE", "description": "Enhanced Adaptive Differential Evolution with a mirrored sampling technique to better explore the search space boundaries, adaptive parameter control, orthogonal learning, and a combined archive with aging.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_base=0.5, CR_base=0.7, archive_size=100, F_range=0.3, CR_range=0.3, orthogonal_learning_rate=0.1, restart_patience=50, aging_rate=0.05, mirror_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.archive_size = archive_size\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.archive = []\n        self.archive_fitness = []\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.restart_patience = restart_patience\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.F_history = []\n        self.CR_history = []\n        self.aging_rate = aging_rate # Rate at which archive fitness degrades\n        self.mirror_prob = mirror_prob\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n            self.stagnation_counter = 0\n            self.previous_best_fitness = self.f_opt\n        else:\n            self.stagnation_counter +=1\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n            \n            # Adaptive F and CR parameter control with trend adjustment\n            F_trend = 0\n            CR_trend = 0\n            if len(self.F_history) > 1:\n                F_trend = np.mean(np.diff(self.F_history))\n            if len(self.CR_history) > 1:\n                CR_trend = np.mean(np.diff(self.CR_history))\n\n            self.F_base = np.clip(self.F_base + 0.1 * F_trend, 0.1, 0.9)\n            self.CR_base = np.clip(self.CR_base + 0.1 * CR_trend, 0.1, 0.9)\n\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                F = self.F_base + np.random.uniform(-self.F_range, self.F_range)\n                CR = self.CR_base + np.random.uniform(-self.CR_range, self.CR_range)\n                F = np.clip(F, 0.1, 1.0)\n                CR = np.clip(CR, 0.1, 1.0)\n\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = x1 + F * (x2 - x3)\n                \n                # Incorporate archive\n                if len(self.archive) > 0 and np.random.rand() < 0.1:  # 10% chance to use archive\n                    archive_index = np.random.randint(len(self.archive))\n                    mutant = x1 + F * (x2 - self.archive[archive_index])\n\n                # Repair mechanism and Mirrored Sampling\n                for j in range(self.dim):\n                    if mutant[j] < func.bounds.lb[j]:\n                        if np.random.rand() < self.mirror_prob:\n                            mutant[j] = func.bounds.lb[j] + (func.bounds.lb[j] - mutant[j])\n                        else:\n                            mutant[j] = func.bounds.lb[j]\n                    elif mutant[j] > func.bounds.ub[j]:\n                        if np.random.rand() < self.mirror_prob:\n                            mutant[j] = func.bounds.ub[j] - (mutant[j] - func.bounds.ub[j])\n                        else:\n                            mutant[j] = func.bounds.ub[j]\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_learning_rate:\n                    orthogonal_vector = self.x_opt + np.random.normal(0, 0.1, self.dim)  # Perturb best solution\n                    orthogonal_vector = np.clip(orthogonal_vector, func.bounds.lb, func.bounds.ub)\n                    trial = 0.5 * (trial + orthogonal_vector)  # Combine with trial vector\n                    trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                    \n                    # Record successful F and CR values\n                    self.F_history.append(F)\n                    self.CR_history.append(CR)\n                    if len(self.F_history) > 50:\n                        self.F_history.pop(0)\n                        self.CR_history.pop(0)\n\n                    # Add replaced vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                         # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = population[i]\n                        self.archive_fitness[worst_archive_index] = fitness[i]\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.stagnation_counter = 0\n                        self.previous_best_fitness = self.f_opt\n                else:\n                     # Add trial vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = trial\n                        self.archive_fitness[worst_archive_index] = f_trial\n                        \n\n            population = new_population\n            fitness = new_fitness\n            \n            # Restart population if stagnating\n            if self.stagnation_counter > self.restart_patience:\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                    self.stagnation_counter = 0\n                    self.previous_best_fitness = self.f_opt\n                else:\n                    self.stagnation_counter +=1\n                    \n                # Reset F and CR history\n                self.F_history = []\n                self.CR_history = []\n                \n            # Archive aging\n            for k in range(len(self.archive_fitness)):\n                self.archive_fitness[k] *= (1 - self.aging_rate)\n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDE scored 0.662 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b659a3c2-716b-47f8-a511-5ce90c271192"], "operator": null, "metadata": {"aucs": [0.2871090942940304, 0.6593557619594481, 0.6319036328621543, 0.8899332751375151, 0.7054177700096056, 0.7810924476450487, 0.543724028534837, 0.5697209212328511, 0.7084660330240473, 0.6182362252533641, 0.8759338454708004, 0.99799149644438, 0.43762705059194884, 0.6689119583880546, 0.8884959863931676, 0.769255781223704, 0.5751073521143314, 0.8367049574895409, 0.22324992966730228, 0.5663790214795175]}}
{"id": "8ba2aafb-e9de-48f1-b000-729655be7216", "fitness": 0.724750110245268, "name": "AdaptiveDE", "description": "Introducing a dynamically adjusted archive probability based on the success rate of the archive, along with a refined restart strategy using opposition-based learning to enhance exploration.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_base=0.5, CR_base=0.7, archive_size=100, F_range=0.3, CR_range=0.3, orthogonal_learning_rate=0.1, restart_patience=50, archive_prob=0.1, current_to_best_prob=0.2, archive_success_threshold=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.archive_size = archive_size\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.archive = []\n        self.archive_fitness = []\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.restart_patience = restart_patience\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.F_history = []\n        self.CR_history = []\n        self.archive_prob = archive_prob\n        self.current_to_best_prob = current_to_best_prob\n        self.archive_success_rate = 0.0\n        self.archive_successes = 0\n        self.archive_trials = 0\n        self.archive_success_threshold = archive_success_threshold\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n            self.stagnation_counter = 0\n            self.previous_best_fitness = self.f_opt\n        else:\n            self.stagnation_counter +=1\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n            \n            # Self-adaptive F and CR\n            if self.F_history:\n                self.F_base = np.mean(self.F_history)\n            if self.CR_history:\n                self.CR_base = np.mean(self.CR_history)\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                F = self.F_base + np.random.uniform(-self.F_range, self.F_range)\n                CR = self.CR_base + np.random.uniform(-self.CR_range, self.CR_range)\n                F = np.clip(F, 0.1, 1.0)\n                CR = np.clip(CR, 0.1, 1.0)\n\n                # Mutation: Combining current-to-best with random mutation\n                indices = np.random.choice(self.pop_size, 2, replace=False)\n                x1, x2 = population[indices]\n                \n                if np.random.rand() < self.current_to_best_prob:\n                    mutant = population[i] + F * (self.x_opt - population[i]) + F * (x1 - x2)\n                else:\n                    x3 = population[np.random.choice(self.pop_size)]\n                    mutant = x1 + F * (x2 - x3)\n                \n                # Incorporate archive\n                use_archive = False\n                if len(self.archive) > 0 and np.random.rand() < self.archive_prob:  # archive_prob chance to use archive\n                    archive_index = np.random.randint(len(self.archive))\n                    mutant = x1 + F * (x2 - self.archive[archive_index])\n                    use_archive = True\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_learning_rate:\n                    orthogonal_vector = self.x_opt + np.random.normal(0, 0.1, self.dim)  # Perturb best solution\n                    orthogonal_vector = np.clip(orthogonal_vector, func.bounds.lb, func.bounds.ub)\n                    trial = 0.5 * (trial + orthogonal_vector)  # Combine with trial vector\n                    trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                    \n                    # Record successful F and CR values\n                    self.F_history.append(F)\n                    self.CR_history.append(CR)\n                    if len(self.F_history) > 50:\n                        self.F_history.pop(0)\n                        self.CR_history.pop(0)\n\n                    # Add replaced vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                         # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = population[i]\n                        self.archive_fitness[worst_archive_index] = fitness[i]\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.stagnation_counter = 0\n                        self.previous_best_fitness = self.f_opt\n                    \n                    if use_archive:\n                        self.archive_successes += 1\n\n                else:\n                     # Add trial vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = trial\n                        self.archive_fitness[worst_archive_index] = f_trial\n                        \n                if use_archive:\n                    self.archive_trials += 1\n\n            population = new_population\n            fitness = new_fitness\n            \n            # Restart population if stagnating\n            if self.stagnation_counter > self.restart_patience:\n                # Opposition-based learning for restart\n                opposition_population = func.bounds.ub + func.bounds.lb - population\n                opposition_fitness = np.array([func(x) for x in opposition_population])\n                self.budget -= self.pop_size\n                \n                combined_population = np.concatenate((population, opposition_population))\n                combined_fitness = np.concatenate((fitness, opposition_fitness))\n                \n                # Select the best individuals from the combined population\n                indices = np.argsort(combined_fitness)[:self.pop_size]\n                population = combined_population[indices]\n                fitness = combined_fitness[indices]\n\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                    self.stagnation_counter = 0\n                    self.previous_best_fitness = self.f_opt\n                else:\n                    self.stagnation_counter +=1\n                    \n                # Reset F and CR history\n                self.F_history = []\n                self.CR_history = []\n            \n            # Adjust archive probability based on success rate\n            if self.archive_trials > 10:\n                self.archive_success_rate = self.archive_successes / self.archive_trials\n                if self.archive_success_rate < self.archive_success_threshold:\n                    self.archive_prob *= 0.9  # Decrease probability if success rate is low\n                else:\n                    self.archive_prob = min(1.0, self.archive_prob * 1.1) # Increase if success is good\n                self.archive_trials = 0\n                self.archive_successes = 0\n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDE scored 0.725 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["79a5e1f9-a95f-4e28-9be6-df8b6f0252dd"], "operator": null, "metadata": {"aucs": [0.3997544812566658, 0.7501840400004702, 0.6871141928223266, 0.8875970807461965, 0.7614523177270904, 0.8118679990454154, 0.6681941631701045, 0.6911963555184288, 0.772863873135855, 0.7357318057541522, 0.8941966322425552, 0.9939501922720492, 0.34199669100642516, 0.7776657203875731, 0.9217327189359116, 0.8036813159718311, 0.6781984724097597, 0.8668054415430196, 0.541602624368245, 0.5092160865912858]}}
{"id": "f993ecdb-ce95-464a-a3ed-9919a6c2b3ef", "fitness": 0.5988122055107868, "name": "AdaptiveDE", "description": "Enhanced Adaptive Differential Evolution with a novel mutation strategy that blends current-to-pbest with a diversity-enhanced random mutation, adaptive F and CR parameters, a combined archive strategy, orthogonal learning, and a distance-based population restart mechanism.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_base=0.5, CR_base=0.7, archive_size=100, F_range=0.3, CR_range=0.3, orthogonal_learning_rate=0.1, restart_patience=50, archive_prob=0.1, pbest_proportion=0.1, diversity_threshold=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.archive_size = archive_size\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.archive = []\n        self.archive_fitness = []\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.restart_patience = restart_patience\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.F_history = []\n        self.CR_history = []\n        self.archive_prob = archive_prob\n        self.pbest_proportion = pbest_proportion\n        self.diversity_threshold = diversity_threshold\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n            self.stagnation_counter = 0\n            self.previous_best_fitness = self.f_opt\n        else:\n            self.stagnation_counter +=1\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n            \n            # Self-adaptive F and CR\n            if self.F_history:\n                self.F_base = np.mean(self.F_history)\n            if self.CR_history:\n                self.CR_base = np.mean(self.CR_history)\n\n            # Sort population based on fitness\n            sorted_indices = np.argsort(fitness)\n            \n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                F = self.F_base + np.random.uniform(-self.F_range, self.F_range)\n                CR = self.CR_base + np.random.uniform(-self.CR_range, self.CR_range)\n                F = np.clip(F, 0.1, 1.0)\n                CR = np.clip(CR, 0.1, 1.0)\n\n                # Mutation: Combining current-to-pbest with diversity-enhanced random mutation\n                pbest_count = max(1, int(self.pop_size * self.pbest_proportion))\n                pbest_indices = sorted_indices[:pbest_count]\n                pbest_individual = population[np.random.choice(pbest_indices)]\n\n                indices = np.random.choice(self.pop_size, 2, replace=False)\n                x1, x2 = population[indices]\n                x3 = population[np.random.choice(self.pop_size)]\n                \n                # Diversity enhancement for random mutation\n                mutant = x1 + F * (x2 - x3) + np.random.normal(0, 0.01, self.dim)  # Add small random perturbation\n                mutant = population[i] + F * (pbest_individual - population[i]) + F * (x1 - x2)\n\n                \n                # Incorporate archive\n                if len(self.archive) > 0 and np.random.rand() < self.archive_prob:  # archive_prob chance to use archive\n                    archive_index = np.random.randint(len(self.archive))\n                    mutant = x1 + F * (x2 - self.archive[archive_index])\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_learning_rate:\n                    orthogonal_vector = self.x_opt + np.random.normal(0, 0.1, self.dim)  # Perturb best solution\n                    orthogonal_vector = np.clip(orthogonal_vector, func.bounds.lb, func.bounds.ub)\n                    trial = 0.5 * (trial + orthogonal_vector)  # Combine with trial vector\n                    trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                    \n                    # Record successful F and CR values\n                    self.F_history.append(F)\n                    self.CR_history.append(CR)\n                    if len(self.F_history) > 50:\n                        self.F_history.pop(0)\n                        self.CR_history.pop(0)\n\n                    # Add replaced vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                         # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = population[i]\n                        self.archive_fitness[worst_archive_index] = fitness[i]\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.stagnation_counter = 0\n                        self.previous_best_fitness = self.f_opt\n                else:\n                     # Add trial vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = trial\n                        self.archive_fitness[worst_archive_index] = f_trial\n                        \n\n            population = new_population\n            fitness = new_fitness\n            \n            # Restart population if stagnating or low diversity\n            if self.stagnation_counter > self.restart_patience or self.population_diversity(population) < self.diversity_threshold:\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                    self.stagnation_counter = 0\n                    self.previous_best_fitness = self.f_opt\n                else:\n                    self.stagnation_counter +=1\n                    \n                # Reset F and CR history\n                self.F_history = []\n                self.CR_history = []\n                \n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt\n\n    def population_diversity(self, population):\n        \"\"\"Calculates the average Euclidean distance between individuals in the population.\"\"\"\n        distances = []\n        for i in range(self.pop_size):\n            for j in range(i + 1, self.pop_size):\n                distances.append(np.linalg.norm(population[i] - population[j]))\n        return np.mean(distances) if distances else 0", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDE scored 0.599 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["79a5e1f9-a95f-4e28-9be6-df8b6f0252dd"], "operator": null, "metadata": {"aucs": [0.3375036239110333, 0.639639928486175, 0.5384120480515546, 0.9211834402480933, 0.599220732582668, 0.6480183763763661, 0.5406926090585291, 0.4913576105832468, 0.6174371307966592, 0.25679632023650956, 0.9062061318992325, 0.9984117243620502, 0.5777844492487192, 0.6258368328972932, 0.7937795860801365, 0.3507802714548266, 0.5206914430732776, 0.8178592290734192, 0.2880530794022219, 0.5065795423937255]}}
{"id": "38cf688a-af5e-4fc1-a4e5-e2dbf680c60b", "fitness": 0.6437250713094036, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with a two-archive system (success-based and diversity-based), trend-based parameter adaptation, orthogonal learning, and a restarting mechanism, prioritizing high-performing and diverse solutions.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_base=0.5, CR_base=0.7, archive_size=50, diversity_archive_size=50, F_range=0.3, CR_range=0.3, orthogonal_learning_rate=0.1, restart_patience=50, aging_rate=0.05, success_threshold=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.archive_size = archive_size\n        self.diversity_archive_size = diversity_archive_size\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.success_archive = []\n        self.success_archive_fitness = []\n        self.diversity_archive = []\n        self.diversity_archive_fitness = []\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.restart_patience = restart_patience\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.F_history = []\n        self.CR_history = []\n        self.aging_rate = aging_rate # Rate at which archive fitness degrades\n        self.success_threshold = success_threshold  # Threshold for considering a solution \"successful\"\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n            self.stagnation_counter = 0\n            self.previous_best_fitness = self.f_opt\n        else:\n            self.stagnation_counter +=1\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n            \n            # Adaptive F and CR parameter control with trend adjustment\n            F_trend = 0\n            CR_trend = 0\n            if len(self.F_history) > 1:\n                F_trend = np.mean(np.diff(self.F_history))\n            if len(self.CR_history) > 1:\n                CR_trend = np.mean(np.diff(self.CR_history))\n\n            self.F_base = np.clip(self.F_base + 0.1 * F_trend, 0.1, 0.9)\n            self.CR_base = np.clip(self.CR_base + 0.1 * CR_trend, 0.1, 0.9)\n\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                F = self.F_base + np.random.uniform(-self.F_range, self.F_range)\n                CR = self.CR_base + np.random.uniform(-self.CR_range, self.CR_range)\n                F = np.clip(F, 0.1, 1.0)\n                CR = np.clip(CR, 0.1, 1.0)\n\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = x1 + F * (x2 - x3)\n                \n                # Incorporate archives\n                if len(self.success_archive) > 0 and np.random.rand() < 0.05:  # 5% chance to use success archive\n                    archive_index = np.random.randint(len(self.success_archive))\n                    mutant = x1 + F * (x2 - self.success_archive[archive_index])\n\n                if len(self.diversity_archive) > 0 and np.random.rand() < 0.05:  # 5% chance to use diversity archive\n                    archive_index = np.random.randint(len(self.diversity_archive))\n                    mutant = x1 + F * (x1 - self.diversity_archive[archive_index])  # Diversify using diversity archive\n                    \n                # Repair mechanism\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_learning_rate:\n                    orthogonal_vector = self.x_opt + np.random.normal(0, 0.1, self.dim)  # Perturb best solution\n                    orthogonal_vector = np.clip(orthogonal_vector, func.bounds.lb, func.bounds.ub)\n                    trial = 0.5 * (trial + orthogonal_vector)  # Combine with trial vector\n                    trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                    \n                    # Record successful F and CR values\n                    self.F_history.append(F)\n                    self.CR_history.append(CR)\n                    if len(self.F_history) > 50:\n                        self.F_history.pop(0)\n                        self.CR_history.pop(0)\n\n                    # Add replaced vector to success archive if improvement is significant\n                    if fitness[i] - f_trial > self.success_threshold:\n                        if len(self.success_archive) < self.archive_size:\n                            self.success_archive.append(population[i])\n                            self.success_archive_fitness.append(fitness[i])\n                        else:\n                             # Replace worst in success archive\n                            worst_archive_index = np.argmax(self.success_archive_fitness)\n                            self.success_archive[worst_archive_index] = population[i]\n                            self.success_archive_fitness[worst_archive_index] = fitness[i]\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.stagnation_counter = 0\n                        self.previous_best_fitness = self.f_opt\n                \n                # Add trial vector to diversity archive always, prioritizing diverse solutions\n                if len(self.diversity_archive) < self.diversity_archive_size:\n                    self.diversity_archive.append(trial)\n                    self.diversity_archive_fitness.append(f_trial)\n                else:\n                    # Replace closest in diversity archive (replace the most similar one)\n                    distances = [np.linalg.norm(trial - x) for x in self.diversity_archive]\n                    closest_archive_index = np.argmin(distances)\n                    self.diversity_archive[closest_archive_index] = trial\n                    self.diversity_archive_fitness[closest_archive_index] = f_trial\n\n            population = new_population\n            fitness = new_fitness\n            \n            # Restart population if stagnating\n            if self.stagnation_counter > self.restart_patience:\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                    self.stagnation_counter = 0\n                    self.previous_best_fitness = self.f_opt\n                else:\n                    self.stagnation_counter +=1\n                    \n                # Reset F and CR history\n                self.F_history = []\n                self.CR_history = []\n                \n            # Archive aging for success archive\n            for k in range(len(self.success_archive_fitness)):\n                self.success_archive_fitness[k] *= (1 - self.aging_rate)\n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDE scored 0.644 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b659a3c2-716b-47f8-a511-5ce90c271192"], "operator": null, "metadata": {"aucs": [0.2568590986259297, 0.6071290447357438, 0.6097626451494178, 0.8868486358191738, 0.6733732890604895, 0.7483681695346265, 0.5182575165682857, 0.5955987413451838, 0.6885435446516694, 0.6553614803021268, 0.844603199815571, 0.997853507800702, 0.4052542141011619, 0.6480730578185433, 0.8341390068663869, 0.7240330710854765, 0.5523111301451133, 0.8216651647919576, 0.27953457737378085, 0.5269323305967317]}}
{"id": "769f0ce5-567d-40b6-a70f-81f4aca89ca1", "fitness": -Infinity, "name": "AdaptiveDE", "description": "Introducing covariance matrix adaptation for mutation scaling, a learning rate decay, and dynamic population sizing.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_base=0.5, CR_base=0.7, archive_size=100, F_range=0.3, CR_range=0.3, orthogonal_learning_rate=0.1, restart_patience=50, archive_prob=0.1, current_to_best_prob=0.2, archive_success_threshold=0.1, initial_learning_rate=0.1, learning_rate_decay=0.99, covariance_adaptation_frequency=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.archive_size = archive_size\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.archive = []\n        self.archive_fitness = []\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.restart_patience = restart_patience\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.F_history = []\n        self.CR_history = []\n        self.archive_prob = archive_prob\n        self.current_to_best_prob = current_to_best_prob\n        self.archive_success_rate = 0.0\n        self.archive_successes = 0\n        self.archive_trials = 0\n        self.archive_success_threshold = archive_success_threshold\n        self.learning_rate = initial_learning_rate\n        self.learning_rate_decay = learning_rate_decay\n        self.covariance_adaptation_frequency = covariance_adaptation_frequency\n        self.covariance_matrix = np.eye(dim)  # Initialize covariance matrix\n        self.pop_size_min = 10\n        self.pop_size_max = 100\n        self.dynamic_pop_size = self.pop_size\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.dynamic_pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.dynamic_pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n            self.stagnation_counter = 0\n            self.previous_best_fitness = self.f_opt\n        else:\n            self.stagnation_counter +=1\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n            \n            # Self-adaptive F and CR\n            if self.F_history:\n                self.F_base = np.mean(self.F_history)\n            if self.CR_history:\n                self.CR_base = np.mean(self.CR_history)\n\n            for i in range(self.dynamic_pop_size):\n                # Adaptive F and CR\n                F = self.F_base + np.random.uniform(-self.F_range, self.F_range)\n                CR = self.CR_base + np.random.uniform(-self.CR_range, self.CR_range)\n                F = np.clip(F, 0.1, 1.0)\n                CR = np.clip(CR, 0.1, 1.0)\n\n                # Mutation: Combining current-to-best with random mutation and covariance adaptation\n                indices = np.random.choice(self.dynamic_pop_size, 2, replace=False)\n                x1, x2 = population[indices]\n\n                if np.random.rand() < self.current_to_best_prob:\n                    mutant = population[i] + F * (self.x_opt - population[i]) + F * (x1 - x2)\n                else:\n                    x3 = population[np.random.choice(self.dynamic_pop_size)]\n                    mutant = x1 + F * (x2 - x3)\n                \n                # Incorporate archive\n                use_archive = False\n                if len(self.archive) > 0 and np.random.rand() < self.archive_prob:  # archive_prob chance to use archive\n                    archive_index = np.random.randint(len(self.archive))\n                    mutant = x1 + F * (x2 - self.archive[archive_index])\n                    use_archive = True\n\n                # Covariance matrix adaptation for mutation scaling\n                mutant = population[i] + np.random.multivariate_normal(np.zeros(self.dim), self.covariance_matrix)\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_learning_rate:\n                    orthogonal_vector = self.x_opt + np.random.normal(0, 0.1, self.dim)  # Perturb best solution\n                    orthogonal_vector = np.clip(orthogonal_vector, func.bounds.lb, func.bounds.ub)\n                    trial = 0.5 * (trial + orthogonal_vector)  # Combine with trial vector\n                    trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                    \n                    # Record successful F and CR values\n                    self.F_history.append(F)\n                    self.CR_history.append(CR)\n                    if len(self.F_history) > 50:\n                        self.F_history.pop(0)\n                        self.CR_history.pop(0)\n\n                    # Add replaced vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                         # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = population[i]\n                        self.archive_fitness[worst_archive_index] = fitness[i]\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.stagnation_counter = 0\n                        self.previous_best_fitness = self.f_opt\n                    \n                    if use_archive:\n                        self.archive_successes += 1\n\n                else:\n                     # Add trial vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = trial\n                        self.archive_fitness[worst_archive_index] = f_trial\n                        \n                if use_archive:\n                    self.archive_trials += 1\n\n            population = new_population\n            fitness = new_fitness\n\n            # Adapt covariance matrix\n            if generation % self.covariance_adaptation_frequency == 0:\n                # Calculate the mean of the population\n                mean = np.mean(population, axis=0)\n\n                # Calculate the covariance matrix\n                self.covariance_matrix = np.cov(population.T)\n\n                # Add a small constant to the diagonal to ensure positive definiteness\n                self.covariance_matrix += np.eye(self.dim) * 1e-6\n\n            # Restart population if stagnating\n            if self.stagnation_counter > self.restart_patience:\n                # Opposition-based learning for restart\n                opposition_population = func.bounds.ub + func.bounds.lb - population\n                opposition_fitness = np.array([func(x) for x in opposition_population])\n                self.budget -= self.dynamic_pop_size\n                \n                combined_population = np.concatenate((population, opposition_population))\n                combined_fitness = np.concatenate((fitness, opposition_fitness))\n                \n                # Select the best individuals from the combined population\n                indices = np.argsort(combined_fitness)[:self.dynamic_pop_size]\n                population = combined_population[indices]\n                fitness = combined_fitness[indices]\n\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                    self.stagnation_counter = 0\n                    self.previous_best_fitness = self.f_opt\n                else:\n                    self.stagnation_counter +=1\n                    \n                # Reset F and CR history\n                self.F_history = []\n                self.CR_history = []\n\n            # Learning rate decay\n            self.learning_rate *= self.learning_rate_decay\n            \n            # Adjust archive probability based on success rate\n            if self.archive_trials > 10:\n                self.archive_success_rate = self.archive_successes / self.archive_trials\n                if self.archive_success_rate < self.archive_success_threshold:\n                    self.archive_prob *= 0.9  # Decrease probability if success rate is low\n                else:\n                    self.archive_prob = min(1.0, self.archive_prob * 1.1) # Increase if success is good\n                self.archive_trials = 0\n                self.archive_successes = 0\n\n            # Dynamically adjust population size\n            if self.stagnation_counter > self.restart_patience / 2:\n                 self.dynamic_pop_size = max(self.pop_size_min, int(self.dynamic_pop_size * 0.9))\n            else:\n                self.dynamic_pop_size = min(self.pop_size_max, int(self.dynamic_pop_size * 1.1))\n                \n            self.dynamic_pop_size = min(self.budget, self.dynamic_pop_size)\n            \n            if self.budget <= 0:\n                break\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 144, in evaluate\n    algorithm(f_new)\n  File \"<string>\", line 76, in __call__\nIndexError: index 53 is out of bounds for axis 0 with size 50\n.", "error": "", "parent_ids": ["8ba2aafb-e9de-48f1-b000-729655be7216"], "operator": null, "metadata": {}}
{"id": "126fe08a-2398-498c-b28f-c22788082eee", "fitness": -Infinity, "name": "AdaptiveDE", "description": "Introducing a directed exploration strategy by biasing mutation towards promising regions identified through fitness landscape analysis, combined with a self-adaptive population size adjustment based on optimization progress and a local search intensification phase.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\nfrom scipy.ndimage import gaussian_filter\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size_init=50, archive_size_init=50, F_init=0.5, CR_init=0.7, num_clusters=5, local_search_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_init = pop_size_init\n        self.pop_size = pop_size_init\n        self.archive_size_init = archive_size_init\n        self.archive_size = archive_size_init\n        self.F_init = F_init\n        self.CR_init = CR_init\n        self.F_memory = np.ones(self.pop_size) * self.F_init\n        self.CR_memory = np.ones(self.pop_size) * self.CR_init\n        self.archive = []\n        self.archive_fitness = []\n        self.archive_age = []\n        self.success_F = []\n        self.success_CR = []\n        self.success_count = 0\n        self.failure_count = 0\n        self.num_clusters = num_clusters # Number of clusters for archive management\n        self.local_search_prob = local_search_prob\n        self.landscape_analysis_interval = 50\n        self.smoothness = 2.0\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n\n        generation = 0\n        stagnation_counter = 0\n        prev_best_fitness = self.f_opt\n        archive_clear_interval = 200\n        \n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n\n            # Adjust archive size dynamically\n            success_rate = self.success_count / (self.success_count + self.failure_count + 1e-9)\n            self.archive_size = int(self.archive_size_init * (0.5 + success_rate))\n\n            # Adaptive Population Size\n            if stagnation_counter > 50:\n                self.pop_size = max(10, int(self.pop_size * 0.9))  # Reduce population if stagnating\n            elif success_rate > 0.3 and generation > 100:\n                self.pop_size = min(self.pop_size_init, int(self.pop_size * 1.1))  # Increase if successful\n\n            if population.shape[0] != self.pop_size:\n              population = population[:self.pop_size]\n              fitness = fitness[:self.pop_size]\n              self.F_memory = self.F_memory[:self.pop_size]\n              self.CR_memory = self.CR_memory[:self.pop_size]\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                if self.success_F:\n                    F_mean = np.mean(self.success_F)\n                    self.F_memory[i] = np.clip(np.random.normal(F_mean, 0.3), 0.1, 1.0)\n                else:\n                    self.F_memory[i] = np.clip(np.random.normal(0.5, 0.3), 0.1, 1.0)\n\n                if self.success_CR:\n                    CR_mean = np.mean(self.success_CR)\n                    self.CR_memory[i] = np.clip(np.random.normal(CR_mean, 0.1), 0.0, 1.0)\n                else:\n                    self.CR_memory[i] = np.clip(np.random.normal(0.7, 0.1), 0.0, 1.0)\n\n                # Mutation Strategies with Probabilistic Selection\n                mutation_strategy = np.random.choice([1, 2, 3], p=[0.4, 0.3, 0.3]) # Adjusted probabilities\n                \n                if mutation_strategy == 1:\n                    # DE/rand/1\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x1, x2, x3 = population[indices]\n                    mutant = x1 + self.F_memory[i] * (x2 - x3)\n                elif mutation_strategy == 2:\n                    # DE/current-to-best/1\n                    indices = np.random.choice(self.pop_size, 2, replace=False)\n                    x1, x2 = population[indices]\n                    mutant = population[i] + self.F_memory[i] * (self.x_opt - population[i]) + self.F_memory[i] * (x1 - x2)\n                else:\n                    # DE/rand/2 with archive\n                    indices = np.random.choice(self.pop_size, 2, replace=False)\n                    x1, x2 = population[indices]\n                    if len(self.archive) > 0:\n                        archive_index = np.random.randint(len(self.archive))\n                        x3 = self.archive[archive_index]\n                    else:\n                        x3 = population[np.random.choice(self.pop_size)]\n                    x4 = population[np.random.choice(self.pop_size)]\n                    mutant = x1 + self.F_memory[i] * (x2 - x3) + self.F_memory[i] * (x4 - population[i])\n\n                # Directed Exploration using fitness landscape analysis\n                if generation % self.landscape_analysis_interval == 0 and generation > 0:\n                    fitness_reshaped = fitness.reshape((int(np.sqrt(self.pop_size)) if int(np.sqrt(self.pop_size))**2 == self.pop_size else 1, -1))\n                    fitness_smooth = gaussian_filter(fitness_reshaped, sigma=self.smoothness)\n                    gradient_x, gradient_y = np.gradient(fitness_smooth)\n                    \n                    if mutant[0] > func.bounds.lb[0] and mutant[0] < func.bounds.ub[0]:\n                        mutant[0] = mutant[0] - np.sign(gradient_x[i % fitness_reshaped.shape[0], i // fitness_reshaped.shape[1]]) * 0.01\n                    if mutant[1] > func.bounds.lb[1] and mutant[1] < func.bounds.ub[1]:\n                        mutant[1] = mutant[1] - np.sign(gradient_y[i % fitness_reshaped.shape[0], i // fitness_reshaped.shape[1]]) * 0.01\n                    \n                    mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                else:\n                    mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Orthogonal Crossover\n                trial = np.copy(population[i])\n                num_changed_vars = 0\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR_memory[i]:\n                        trial[j] = mutant[j]\n                        num_changed_vars += 1\n                if num_changed_vars == 0:\n                    j_rand = np.random.randint(self.dim)\n                    trial[j_rand] = mutant[j_rand]\n                \n                # Local Search Intensification\n                if np.random.rand() < self.local_search_prob:\n                    step_size = 0.01 * (func.bounds.ub - func.bounds.lb)\n                    trial = trial + np.random.uniform(-step_size, step_size)\n                    trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n\n                    self.success_F.append(self.F_memory[i])\n                    self.success_CR.append(self.CR_memory[i])\n                    if len(self.success_F) > 10:\n                        self.success_F.pop(0)\n                        self.success_CR.pop(0)\n                    self.success_count += 1\n\n                    # Dynamic archive management: replace worst in archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                        self.archive_fitness.append(fitness[i])\n                        self.archive_age.append(0)\n                    else:\n                        if len(self.archive) > 0:\n                            worst_archive_index = np.argmax(self.archive_fitness)\n                            if fitness[i] < self.archive_fitness[worst_archive_index]:\n                                self.archive[worst_archive_index] = population[i]\n                                self.archive_fitness[worst_archive_index] = fitness[i]\n                                self.archive_age[worst_archive_index] = 0\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        stagnation_counter = 0\n                else:\n                    self.failure_count += 1\n                    # Dynamic archive management: replace worst in archive (trial vector)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                        self.archive_age.append(0)\n                    else:\n                         if len(self.archive) > 0:\n                            worst_archive_index = np.argmax(self.archive_fitness)\n                            if f_trial < self.archive_fitness[worst_archive_index]:\n                                self.archive[worst_archive_index] = trial\n                                self.archive_fitness[worst_archive_index] = f_trial\n                                self.archive_age[worst_archive_index] = 0\n\n            # Update age of archive members\n            self.archive_age = [age + 1 for age in self.archive_age]\n\n            population = new_population\n            fitness = new_fitness\n\n            # Restart population if stagnating\n            if abs(self.f_opt - prev_best_fitness) < 1e-8:\n                stagnation_counter += 1\n            else:\n                stagnation_counter = 0\n            prev_best_fitness = self.f_opt\n\n            if stagnation_counter > 100:\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                stagnation_counter = 0\n                self.archive = []\n                self.archive_fitness = []\n                self.archive_age = []\n                self.success_F = []\n                self.success_CR = []\n                self.success_count = 0\n                self.failure_count = 0\n\n            # Periodic archive clearing and clustering\n            if generation % archive_clear_interval == 0 and len(self.archive) > 0:\n                # Clustering to maintain diversity\n                try:\n                    kmeans = KMeans(n_clusters=min(self.num_clusters, len(self.archive)), random_state=0, n_init=10) # Ensure n_clusters <= n_samples\n                    kmeans.fit(self.archive)\n                    cluster_labels = kmeans.labels_\n\n                    # Keep only the best individual from each cluster\n                    new_archive = []\n                    new_archive_fitness = []\n                    new_archive_age = []\n                    for cluster_id in range(kmeans.n_clusters):\n                        cluster_indices = np.where(cluster_labels == cluster_id)[0]\n                        best_index_in_cluster = cluster_indices[np.argmin([self.archive_fitness[i] for i in cluster_indices])]\n                        new_archive.append(self.archive[best_index_in_cluster])\n                        new_archive_fitness.append(self.archive_fitness[best_index_in_cluster])\n                        new_archive_age.append(0) # Reset age\n\n                    self.archive = new_archive\n                    self.archive_fitness = new_archive_fitness\n                    self.archive_age = new_archive_age\n                except ValueError as e:\n                    print(f\"KMeans clustering failed: {e}\")\n                    self.archive = []\n                    self.archive_fitness = []\n                    self.archive_age = []\n\n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 144, in evaluate\n    algorithm(f_new)\n  File \"<string>\", line 112, in __call__\nNameError: name 'gaussian_filter' is not defined\n.", "error": "", "parent_ids": ["0e08a803-4838-421b-a3da-f836ecc036c0"], "operator": null, "metadata": {}}
{"id": "43d7e383-7794-4e66-8a0d-6089eae85c66", "fitness": -Infinity, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with success-history based parameter adaptation, orthogonal learning, dynamic population size adjustment, and a local search component using Nelder-Mead for intensification.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size_min=20, pop_size_max=100, F_base=0.5, CR_base=0.7, archive_size=100, F_range=0.3, CR_range=0.3, orthogonal_learning_rate=0.1, restart_patience=50, archive_prob=0.1, current_to_best_prob=0.2, gamma_F = 0.1, gamma_CR = 0.1, local_search_frequency=25):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_min = pop_size_min\n        self.pop_size_max = pop_size_max\n        self.pop_size = pop_size_max\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.archive_size = archive_size\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.archive = []\n        self.archive_fitness = []\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.restart_patience = restart_patience\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.F_success_history = []\n        self.CR_success_history = []\n        self.F_failure_history = []\n        self.CR_failure_history = []\n        self.archive_prob = archive_prob\n        self.current_to_best_prob = current_to_best_prob\n        self.gamma_F = gamma_F\n        self.gamma_CR = gamma_CR\n        self.local_search_frequency = local_search_frequency\n        self.generation = 0\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n            self.stagnation_counter = 0\n            self.previous_best_fitness = self.f_opt\n        else:\n            self.stagnation_counter +=1\n\n        while self.budget > 0:\n            self.generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n            \n            # Self-adaptive F and CR\n            if self.F_success_history:\n                self.F_base = (1-self.gamma_F) * self.F_base + self.gamma_F * np.mean(self.F_success_history)\n                self.F_base = np.clip(self.F_base, 0.1, 1.0)\n            if self.CR_success_history:\n                self.CR_base = (1-self.gamma_CR) * self.CR_base + self.gamma_CR * np.mean(self.CR_success_history)\n                self.CR_base = np.clip(self.CR_base, 0.1, 1.0)\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                F = self.F_base + np.random.uniform(-self.F_range, self.F_range)\n                CR = self.CR_base + np.random.uniform(-self.CR_range, self.CR_range)\n                F = np.clip(F, 0.1, 1.0)\n                CR = np.clip(CR, 0.1, 1.0)\n\n                # Mutation: Combining current-to-best with random mutation\n                indices = np.random.choice(self.pop_size, 2, replace=False)\n                x1, x2 = population[indices]\n                \n                if np.random.rand() < self.current_to_best_prob:\n                    mutant = population[i] + F * (self.x_opt - population[i]) + F * (x1 - x2)\n                else:\n                    x3 = population[np.random.choice(self.pop_size)]\n                    mutant = x1 + F * (x2 - x3)\n                \n                # Incorporate archive\n                if len(self.archive) > 0 and np.random.rand() < self.archive_prob:  # archive_prob chance to use archive\n                    archive_index = np.random.randint(len(self.archive))\n                    mutant = x1 + F * (x2 - self.archive[archive_index])\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_learning_rate:\n                    orthogonal_vector = self.x_opt + np.random.normal(0, 0.1, self.dim)  # Perturb best solution\n                    orthogonal_vector = np.clip(orthogonal_vector, func.bounds.lb, func.bounds.ub)\n                    trial = 0.5 * (trial + orthogonal_vector)  # Combine with trial vector\n                    trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                    \n                    # Record successful F and CR values\n                    self.F_success_history.append(F)\n                    self.CR_success_history.append(CR)\n                    if len(self.F_success_history) > 50:\n                        self.F_success_history.pop(0)\n                        self.CR_success_history.pop(0)\n                    if self.F_failure_history:\n                        self.F_failure_history.pop(0)\n                    if self.CR_failure_history:\n                        self.CR_failure_history.pop(0)\n\n\n                    # Add replaced vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                         # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = population[i]\n                        self.archive_fitness[worst_archive_index] = fitness[i]\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.stagnation_counter = 0\n                        self.previous_best_fitness = self.f_opt\n                else:\n                    self.F_failure_history.append(F)\n                    self.CR_failure_history.append(CR)\n                    if len(self.F_failure_history) > 50:\n                         self.F_failure_history.pop(0)\n                    if len(self.CR_failure_history) > 50:\n                         self.CR_failure_history.pop(0)\n                    if self.F_success_history:\n                        self.F_success_history.pop(0)\n                    if self.CR_success_history:\n                        self.CR_success_history.pop(0)\n\n                     # Add trial vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        if f_trial < self.archive_fitness[worst_archive_index]:\n                            self.archive[worst_archive_index] = trial\n                            self.archive_fitness[worst_archive_index] = f_trial\n                        \n\n            population = new_population\n            fitness = new_fitness\n            \n            # Adjust population size\n            if self.generation % 10 == 0:\n                if len(self.F_success_history) > len(self.F_failure_history):\n                    self.pop_size = min(self.pop_size + 5, self.pop_size_max)\n                else:\n                    self.pop_size = max(self.pop_size - 5, self.pop_size_min)\n                \n                # Regenerate population with new size\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n\n            # Local search intensification\n            if self.generation % self.local_search_frequency == 0:\n                bounds = [(func.bounds.lb, func.bounds.ub)] * self.dim\n                result = minimize(func, self.x_opt, method='Nelder-Mead', bounds=bounds, options={'maxfev': min(500, self.budget)})\n                if result.fun < self.f_opt:\n                    self.f_opt = result.fun\n                    self.x_opt = result.x\n                self.budget -= result.nfev\n\n\n            # Restart population if stagnating\n            if self.stagnation_counter > self.restart_patience:\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                    self.stagnation_counter = 0\n                    self.previous_best_fitness = self.f_opt\n                else:\n                    self.stagnation_counter +=1\n                    \n                # Reset F and CR history\n                self.F_success_history = []\n                self.CR_success_history = []\n                self.F_failure_history = []\n                self.CR_failure_history = []\n                \n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 144, in evaluate\n    algorithm(f_new)\n  File \"<string>\", line 184, in __call__\nNameError: name 'minimize' is not defined\n.", "error": "", "parent_ids": ["39b1104e-850a-489e-b666-bc4963790aa9"], "operator": null, "metadata": {}}
{"id": "f46ffe34-bb66-4755-9a37-4bbb4d9e5e4b", "fitness": -Infinity, "name": "AdaptiveDE", "description": "Enhanced Adaptive Differential Evolution with a dynamic population size adjustment, a more robust archive update strategy using a crowding distance, and a simplified restart mechanism based on a population diversity metric.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size_init=50, archive_size_init=50, F_init=0.5, CR_init=0.7, num_clusters=5, diversity_threshold=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_init = pop_size_init\n        self.pop_size = pop_size_init  # Dynamic population size\n        self.archive_size_init = archive_size_init\n        self.archive_size = archive_size_init\n        self.F_init = F_init\n        self.CR_init = CR_init\n        self.F_memory = np.ones(self.pop_size_init) * self.F_init\n        self.CR_memory = np.ones(self.pop_size_init) * self.CR_init\n        self.archive = []\n        self.archive_fitness = []\n        self.archive_age = []\n        self.success_F = []\n        self.success_CR = []\n        self.success_count = 0\n        self.failure_count = 0\n        self.num_clusters = num_clusters  # Number of clusters for archive management\n        self.diversity_threshold = diversity_threshold\n        self.pop_size_min = 10\n        self.pop_size_max = 100\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n\n        generation = 0\n        stagnation_counter = 0\n        prev_best_fitness = self.f_opt\n        archive_clear_interval = 200\n\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n\n            # Adjust archive size dynamically\n            success_rate = self.success_count / (self.success_count + self.failure_count + 1e-9)\n            self.archive_size = int(self.archive_size_init * (0.5 + success_rate))\n\n            # Adjust population size dynamically\n            if success_rate > 0.6:\n                self.pop_size = min(self.pop_size + 1, self.pop_size_max)\n            elif success_rate < 0.2:\n                self.pop_size = max(self.pop_size - 1, self.pop_size_min)\n\n            if self.pop_size != population.shape[0]:\n                population = population[:min(self.pop_size,population.shape[0])]\n                fitness = fitness[:min(self.pop_size,fitness.shape[0])]\n                new_population = population\n                new_fitness = fitness\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                if self.success_F:\n                    F_mean = np.mean(self.success_F)\n                    self.F_memory[i] = np.clip(np.random.normal(F_mean, 0.3), 0.1, 1.0)\n                else:\n                    self.F_memory[i] = np.clip(np.random.normal(0.5, 0.3), 0.1, 1.0)\n\n                if self.success_CR:\n                    CR_mean = np.mean(self.success_CR)\n                    self.CR_memory[i] = np.clip(np.random.normal(CR_mean, 0.1), 0.0, 1.0)\n                else:\n                    self.CR_memory[i] = np.clip(np.random.normal(0.7, 0.1), 0.0, 1.0)\n\n                # Mutation Strategies with Probabilistic Selection\n                mutation_strategy = np.random.choice([1, 2, 3], p=[0.4, 0.3, 0.3])  # Adjusted probabilities\n\n                if mutation_strategy == 1:\n                    # DE/rand/1\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x1, x2, x3 = population[indices]\n                    mutant = x1 + self.F_memory[i] * (x2 - x3)\n                elif mutation_strategy == 2:\n                    # DE/current-to-best/1\n                    indices = np.random.choice(self.pop_size, 2, replace=False)\n                    x1, x2 = population[indices]\n                    mutant = population[i] + self.F_memory[i] * (self.x_opt - population[i]) + self.F_memory[i] * (x1 - x2)\n                else:\n                    # DE/rand/2 with archive\n                    indices = np.random.choice(self.pop_size, 2, replace=False)\n                    x1, x2 = population[indices]\n                    if len(self.archive) > 0:\n                        archive_index = np.random.randint(len(self.archive))\n                        x3 = self.archive[archive_index]\n                    else:\n                        x3 = population[np.random.choice(self.pop_size)]\n                    x4 = population[np.random.choice(self.pop_size)]\n                    mutant = x1 + self.F_memory[i] * (x2 - x3) + self.F_memory[i] * (x4 - population[i])\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Orthogonal Crossover\n                trial = np.copy(population[i])\n                num_changed_vars = 0\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR_memory[i]:\n                        trial[j] = mutant[j]\n                        num_changed_vars += 1\n                if num_changed_vars == 0:\n                    j_rand = np.random.randint(self.dim)\n                    trial[j_rand] = mutant[j_rand]\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n\n                    self.success_F.append(self.F_memory[i])\n                    self.success_CR.append(self.CR_memory[i])\n                    if len(self.success_F) > 10:\n                        self.success_F.pop(0)\n                        self.success_CR.pop(0)\n                    self.success_count += 1\n\n                    # Dynamic archive management with crowding distance\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                        self.archive_fitness.append(fitness[i])\n                        self.archive_age.append(0)\n                    else:\n                        if len(self.archive) > 0:\n                            # Crowding distance calculation (simplified)\n                            distances = [np.linalg.norm(population[i] - archive_member) for archive_member in self.archive]\n                            min_distance_index = np.argmin(distances)\n\n                            if fitness[i] < self.archive_fitness[min_distance_index]:\n                                self.archive[min_distance_index] = population[i]\n                                self.archive_fitness[min_distance_index] = fitness[i]\n                                self.archive_age[min_distance_index] = 0\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        stagnation_counter = 0\n                else:\n                    self.failure_count += 1\n                    # Dynamic archive management with crowding distance (trial vector)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                        self.archive_age.append(0)\n                    else:\n                        if len(self.archive) > 0:\n                            # Crowding distance calculation (simplified)\n                            distances = [np.linalg.norm(trial - archive_member) for archive_member in self.archive]\n                            min_distance_index = np.argmin(distances)\n\n                            if f_trial < self.archive_fitness[min_distance_index]:\n                                self.archive[min_distance_index] = trial\n                                self.archive_fitness[min_distance_index] = f_trial\n                                self.archive_age[min_distance_index] = 0\n\n            # Update age of archive members\n            self.archive_age = [age + 1 for age in self.archive_age]\n\n            population = new_population\n            fitness = new_fitness\n\n            # Restart population if stagnating (simplified diversity check)\n            diversity = np.std(population)\n            if diversity < self.diversity_threshold:\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                stagnation_counter = 0\n                self.archive = []\n                self.archive_fitness = []\n                self.archive_age = []\n                self.success_F = []\n                self.success_CR = []\n                self.success_count = 0\n                self.failure_count = 0\n                self.F_memory = np.ones(self.pop_size) * self.F_init\n                self.CR_memory = np.ones(self.pop_size) * self.CR_init\n\n            # Periodic archive clearing and clustering\n            if generation % archive_clear_interval == 0 and len(self.archive) > 0:\n                # Clustering to maintain diversity\n                try:\n                    kmeans = KMeans(n_clusters=min(self.num_clusters, len(self.archive)), random_state=0, n_init=10)  # Ensure n_clusters <= n_samples\n                    kmeans.fit(self.archive)\n                    cluster_labels = kmeans.labels_\n\n                    # Keep only the best individual from each cluster\n                    new_archive = []\n                    new_archive_fitness = []\n                    new_archive_age = []\n                    for cluster_id in range(kmeans.n_clusters):\n                        cluster_indices = np.where(cluster_labels == cluster_id)[0]\n                        best_index_in_cluster = cluster_indices[np.argmin([self.archive_fitness[i] for i in cluster_indices])]\n                        new_archive.append(self.archive[best_index_in_cluster])\n                        new_archive_fitness.append(self.archive_fitness[best_index_in_cluster])\n                        new_archive_age.append(0)  # Reset age\n\n                    self.archive = new_archive\n                    self.archive_fitness = new_archive_fitness\n                    self.archive_age = new_archive_age\n                except ValueError as e:\n                    print(f\"KMeans clustering failed: {e}\")\n                    self.archive = []\n                    self.archive_fitness = []\n                    self.archive_age = []\n\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 144, in evaluate\n    algorithm(f_new)\n  File \"<string>\", line 206, in __call__\nNameError: name 'KMeans' is not defined\n.", "error": "", "parent_ids": ["0e08a803-4838-421b-a3da-f836ecc036c0"], "operator": null, "metadata": {}}
{"id": "0987b1f4-5025-4776-a2ee-79b6ba1dc5c8", "fitness": 0.0, "name": "AdaptiveDE", "description": "Introducing an adaptive population size adjustment mechanism combined with self-adaptive parameters and orthogonal learning for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size_base=50, F_base=0.5, CR_base=0.7, archive_size=100, F_range=0.3, CR_range=0.3, orthogonal_learning_rate=0.1, restart_patience=50, archive_prob=0.1, current_to_best_prob=0.2, archive_success_threshold=0.1, pop_size_adapt_freq=20, pop_size_scale=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_base = pop_size_base\n        self.pop_size = pop_size_base  # Initial population size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.archive_size = archive_size\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.archive = []\n        self.archive_fitness = []\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.restart_patience = restart_patience\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.F_history = []\n        self.CR_history = []\n        self.archive_prob = archive_prob\n        self.current_to_best_prob = current_to_best_prob\n        self.archive_success_rate = 0.0\n        self.archive_successes = 0\n        self.archive_trials = 0\n        self.archive_success_threshold = archive_success_threshold\n        self.pop_size_adapt_freq = pop_size_adapt_freq\n        self.pop_size_scale = pop_size_scale\n        self.function_evals = 0\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.function_evals += self.pop_size\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n            self.stagnation_counter = 0\n            self.previous_best_fitness = self.f_opt\n        else:\n            self.stagnation_counter +=1\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n            \n            # Self-adaptive F and CR\n            if self.F_history:\n                self.F_base = np.mean(self.F_history)\n            if self.CR_history:\n                self.CR_base = np.mean(self.CR_history)\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                F = self.F_base + np.random.uniform(-self.F_range, self.F_range)\n                CR = self.CR_base + np.random.uniform(-self.CR_range, self.CR_range)\n                F = np.clip(F, 0.1, 1.0)\n                CR = np.clip(CR, 0.1, 1.0)\n\n                # Mutation: Combining current-to-best with random mutation\n                indices = np.random.choice(self.pop_size, 2, replace=False)\n                x1, x2 = population[indices]\n                \n                if np.random.rand() < self.current_to_best_prob:\n                    mutant = population[i] + F * (self.x_opt - population[i]) + F * (x1 - x2)\n                else:\n                    x3 = population[np.random.choice(self.pop_size)]\n                    mutant = x1 + F * (x2 - x3)\n                \n                # Incorporate archive\n                use_archive = False\n                if len(self.archive) > 0 and np.random.rand() < self.archive_prob:  # archive_prob chance to use archive\n                    archive_index = np.random.randint(len(self.archive))\n                    mutant = x1 + F * (x2 - self.archive[archive_index])\n                    use_archive = True\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_learning_rate:\n                    orthogonal_vector = self.x_opt + np.random.normal(0, 0.1, self.dim)  # Perturb best solution\n                    orthogonal_vector = np.clip(orthogonal_vector, func.bounds.lb, func.bounds.ub)\n                    trial = 0.5 * (trial + orthogonal_vector)  # Combine with trial vector\n                    trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.function_evals += 1\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                    \n                    # Record successful F and CR values\n                    self.F_history.append(F)\n                    self.CR_history.append(CR)\n                    if len(self.F_history) > 50:\n                        self.F_history.pop(0)\n                        self.CR_history.pop(0)\n\n                    # Add replaced vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                         # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = population[i]\n                        self.archive_fitness[worst_archive_index] = fitness[i]\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.stagnation_counter = 0\n                        self.previous_best_fitness = self.f_opt\n                    \n                    if use_archive:\n                        self.archive_successes += 1\n\n                else:\n                     # Add trial vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = trial\n                        self.archive_fitness[worst_archive_index] = f_trial\n                        \n                if use_archive:\n                    self.archive_trials += 1\n\n            population = new_population\n            fitness = new_fitness\n            \n            # Restart population if stagnating\n            if self.stagnation_counter > self.restart_patience:\n                # Opposition-based learning for restart\n                opposition_population = func.bounds.ub + func.bounds.lb - population\n                opposition_fitness = np.array([func(x) for x in opposition_population])\n                self.function_evals += self.pop_size\n                self.budget -= self.pop_size\n                \n                combined_population = np.concatenate((population, opposition_population))\n                combined_fitness = np.concatenate((fitness, opposition_fitness))\n                \n                # Select the best individuals from the combined population\n                indices = np.argsort(combined_fitness)[:self.pop_size]\n                population = combined_population[indices]\n                fitness = combined_fitness[indices]\n\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                    self.stagnation_counter = 0\n                    self.previous_best_fitness = self.f_opt\n                else:\n                    self.stagnation_counter +=1\n                    \n                # Reset F and CR history\n                self.F_history = []\n                self.CR_history = []\n            \n            # Adjust archive probability based on success rate\n            if self.archive_trials > 10:\n                self.archive_success_rate = self.archive_successes / self.archive_trials\n                if self.archive_success_rate < self.archive_success_threshold:\n                    self.archive_prob *= 0.9  # Decrease probability if success rate is low\n                else:\n                    self.archive_prob = min(1.0, self.archive_prob * 1.1) # Increase if success is good\n                self.archive_trials = 0\n                self.archive_successes = 0\n            \n            # Adapt population size\n            if generation % self.pop_size_adapt_freq == 0:\n                if self.f_opt < self.previous_best_fitness:\n                    # Increase population size if improvement is observed\n                    self.pop_size = min(self.pop_size_base * 2, int(self.pop_size * (1 + self.pop_size_scale)))\n                else:\n                    # Decrease population size if no improvement\n                    self.pop_size = max(self.pop_size_base // 2, int(self.pop_size * (1 - self.pop_size_scale)))\n                \n                # Ensure population size stays within reasonable bounds\n                self.pop_size = max(10, min(self.pop_size, self.budget))\n                \n                # Resize population\n                if self.pop_size != population.shape[0]:\n                    old_pop_size = population.shape[0]\n                    if self.pop_size > old_pop_size:\n                        # Add random individuals if increasing population size\n                        new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size - old_pop_size, self.dim))\n                        population = np.concatenate((population, new_individuals))\n                        new_fitness = np.array([func(x) for x in new_individuals])\n                        fitness = np.concatenate((fitness, new_fitness))\n                        self.function_evals += (self.pop_size - old_pop_size)\n                        self.budget -= (self.pop_size - old_pop_size)\n                    else:\n                        # Truncate the population if decreasing population size\n                        population = population[:self.pop_size]\n                        fitness = fitness[:self.pop_size]\n            \n            self.previous_best_fitness = self.f_opt\n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["8ba2aafb-e9de-48f1-b000-729655be7216"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "7711f148-cc7d-4003-bd8a-8653c6f5f410", "fitness": 0.2990266431416407, "name": "AdaptiveDE_SOM_v2", "description": "Enhanced Adaptive Differential Evolution with a simplified SOM-based restart, self-adaptive parameters, a combined archive, orthogonal learning, and a multi-strategy mutation with probabilistic selection, with cosine similarity-based diversity metric and reduced SOM learning.", "code": "import numpy as np\n\nclass AdaptiveDE_SOM_v2:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_base=0.5, CR_base=0.7, archive_size=100, F_range=0.3, CR_range=0.3, orthogonal_learning_rate=0.1, som_grid_size=5, som_learning_rate=0.05, som_sigma=1.0, aging_rate=0.05, mutation_prob=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.archive_size = archive_size\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.archive = []\n        self.archive_fitness = []\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.aging_rate = aging_rate\n        self.F_history = []\n        self.CR_history = []\n        self.mutation_prob = mutation_prob\n\n        # SOM parameters\n        self.som_grid_size = som_grid_size\n        self.som_learning_rate = som_learning_rate\n        self.som_sigma = som_sigma\n        self.som_weights = np.random.rand(som_grid_size, som_grid_size, dim)  # SOM grid\n        self.som_stagnation_threshold = 0.9  # Threshold for stagnation detection\n        self.som_stagnation_patience = 3 #Number of generations to wait before restart\n\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.restart_flag = False\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n            self.previous_best_fitness = self.f_opt\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n\n            # Adaptive F and CR using weighted historical success\n            if self.F_history and self.CR_history:\n                weights_F = np.linspace(0.1, 1.0, len(self.F_history))\n                weights_CR = np.linspace(0.1, 1.0, len(self.CR_history))\n\n                F_weighted_avg = np.average(self.F_history, weights=weights_F)\n                CR_weighted_avg = np.average(self.CR_history, weights=weights_CR)\n            else:\n                F_weighted_avg = self.F_base\n                CR_weighted_avg = self.CR_base\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                F = F_weighted_avg + np.random.uniform(-self.F_range, self.F_range)\n                CR = CR_weighted_avg + np.random.uniform(-self.CR_range, self.CR_range)\n                F = np.clip(F, 0.1, 1.0)\n                CR = np.clip(CR, 0.1, 1.0)\n\n                # Mutation\n                if np.random.rand() < self.mutation_prob:\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x1, x2, x3 = population[indices]\n                    mutant = x1 + F * (x2 - x3)\n                else:\n                    mutant = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n\n\n                # Incorporate archive\n                if len(self.archive) > 0 and np.random.rand() < 0.1:  # 10% chance to use archive\n                    archive_index = np.random.randint(len(self.archive))\n                    mutant = population[i] + F * (self.archive[archive_index] - population[i])\n\n                # Repair mechanism\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_learning_rate:\n                    orthogonal_vector = self.x_opt + np.random.normal(0, 0.1, self.dim)  # Perturb best solution\n                    orthogonal_vector = np.clip(orthogonal_vector, func.bounds.lb, func.bounds.ub)\n                    trial = 0.5 * (trial + orthogonal_vector)  # Combine with trial vector\n                    trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                    \n                    # Record successful F and CR values\n                    self.F_history.append(F)\n                    self.CR_history.append(CR)\n                    if len(self.F_history) > 50:\n                        self.F_history.pop(0)\n                        self.CR_history.pop(0)\n\n                    # Add replaced vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                         # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = population[i]\n                        self.archive_fitness[worst_archive_index] = fitness[i]\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.previous_best_fitness = self.f_opt\n                else:\n                     # Add trial vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = trial\n                        self.archive_fitness[worst_archive_index] = f_trial\n\n            population = new_population\n            fitness = new_fitness\n\n            # SOM training (diversity maintenance)\n            self.train_som(population)\n            diversity_metric = self.calculate_diversity_metric(population)\n            \n            if diversity_metric > self.som_stagnation_threshold:\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n\n            # Dynamic restart based on SOM-detected stagnation\n            if self.stagnation_counter > self.som_stagnation_patience:\n                # Simplified restart: re-initialize from best + noise\n                population = np.random.normal(self.x_opt, 0.5, size=(self.pop_size, self.dim))\n                population = np.clip(population, func.bounds.lb, func.bounds.ub)\n\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                \n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                    self.previous_best_fitness = self.f_opt\n                \n                self.F_history = []\n                self.CR_history = []\n                self.stagnation_counter = 0\n\n\n            # Archive aging\n            for k in range(len(self.archive_fitness)):\n                self.archive_fitness[k] *= (1 - self.aging_rate)\n            \n            if self.budget <= 0:\n                break\n        return self.f_opt, self.x_opt\n\n    def train_som(self, population):\n        \"\"\"Trains the Self-Organizing Map (SOM) with the current population.\"\"\"\n        for x in population:\n            # Find the best matching unit (BMU)\n            bmu_index = self.find_bmu(x)\n\n            # Update the SOM weights around the BMU\n            for i in range(self.som_grid_size):\n                for j in range(self.som_grid_size):\n                    distance = np.sqrt((i - bmu_index[0])**2 + (j - bmu_index[1])**2)\n                    influence = np.exp(-distance**2 / (2 * self.som_sigma**2))\n                    self.som_weights[i, j] += self.som_learning_rate * influence * (x - self.som_weights[i, j])\n\n    def find_bmu(self, individual):\n        \"\"\"Finds the best matching unit (BMU) in the SOM for a given individual.\"\"\"\n        min_distance = np.inf\n        bmu_index = None\n        for i in range(self.som_grid_size):\n            for j in range(self.som_grid_size):\n                distance = np.linalg.norm(individual - self.som_weights[i, j])\n                if distance < min_distance:\n                    min_distance = distance\n                    bmu_index = (i, j)\n        return bmu_index\n\n    def calculate_diversity_metric(self, population):\n        \"\"\"Calculates a diversity metric based on the cosine similarity.\"\"\"\n        centroid = np.mean(population, axis=0)\n        similarities = [np.dot(x, centroid) / (np.linalg.norm(x) * np.linalg.norm(centroid)) for x in population]\n        avg_similarity = np.mean(similarities)\n        return avg_similarity\n\n    def initialize_population_from_som(self, lower_bound, upper_bound):\n        \"\"\"Initializes a new population by sampling from the SOM.\"\"\"\n        new_population = np.zeros((self.pop_size, self.dim))\n        for i in range(self.pop_size):\n            # Randomly select a node from the SOM\n            row = np.random.randint(0, self.som_grid_size)\n            col = np.random.randint(0, self.som_grid_size)\n            \n            # Perturb the SOM node's weights to create a new individual\n            new_individual = self.som_weights[row, col] + np.random.normal(0, 0.1, self.dim)\n            new_individual = np.clip(new_individual, lower_bound, upper_bound)\n            new_population[i] = new_individual\n        return new_population", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveDE_SOM_v2 scored 0.299 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["be34ac8d-13ff-4207-ad70-bb80883ee23a"], "operator": null, "metadata": {"aucs": [0.18936649118439464, 0.29447458634736356, 0.3801614137478655, 0.4756932641661339, 0.3520771304159087, 0.3974153147076578, 0.29127477015822056, 0.3188562576338084, 0.2925369020065878, 0.18609000868716408, 0.4103735786445837, 0]}}
{"id": "0672430f-b47e-4aa2-bffc-d3dc74bc31f7", "fitness": 0.6681175191690131, "name": "AdaptiveDE", "description": "Introducing a simplified adaptive DE with success-rate based archive probability adjustment, enhanced parameter adaptation using a decay factor, and a jitter-based restart strategy for diversity.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_base=0.5, CR_base=0.7, archive_size=50, F_range=0.2, CR_range=0.2, orthogonal_learning_rate=0.05, restart_patience=50, archive_prob=0.1, archive_success_threshold=0.1, adaptation_decay=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.archive_size = archive_size\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.archive = []\n        self.archive_fitness = []\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.restart_patience = restart_patience\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.F_history = []\n        self.CR_history = []\n        self.archive_prob = archive_prob\n        self.archive_success_rate = 0.0\n        self.archive_successes = 0\n        self.archive_trials = 0\n        self.archive_success_threshold = archive_success_threshold\n        self.adaptation_decay = adaptation_decay\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n            self.stagnation_counter = 0\n            self.previous_best_fitness = self.f_opt\n        else:\n            self.stagnation_counter +=1\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n            \n            # Self-adaptive F and CR with decay\n            if self.F_history:\n                self.F_base = self.adaptation_decay * self.F_base + (1 - self.adaptation_decay) * np.mean(self.F_history)\n            if self.CR_history:\n                self.CR_base =  self.adaptation_decay * self.CR_base + (1 - self.adaptation_decay) * np.mean(self.CR_history)\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                F = self.F_base + np.random.uniform(-self.F_range, self.F_range)\n                CR = self.CR_base + np.random.uniform(-self.CR_range, self.CR_range)\n                F = np.clip(F, 0.1, 1.0)\n                CR = np.clip(CR, 0.1, 1.0)\n\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = x1 + F * (x2 - x3)\n                \n                # Incorporate archive\n                if len(self.archive) > 0 and np.random.rand() < self.archive_prob:  # archive_prob chance to use archive\n                    archive_index = np.random.randint(len(self.archive))\n                    mutant = x1 + F * (x2 - self.archive[archive_index])\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_learning_rate:\n                    orthogonal_vector = self.x_opt + np.random.normal(0, 0.1, self.dim)  # Perturb best solution\n                    orthogonal_vector = np.clip(orthogonal_vector, func.bounds.lb, func.bounds.ub)\n                    trial = 0.5 * (trial + orthogonal_vector)  # Combine with trial vector\n                    trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                    \n                    # Record successful F and CR values\n                    self.F_history.append(F)\n                    self.CR_history.append(CR)\n                    if len(self.F_history) > 20:\n                        self.F_history.pop(0)\n                        self.CR_history.pop(0)\n\n                    # Add replaced vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                         # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = population[i]\n                        self.archive_fitness[worst_archive_index] = fitness[i]\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.stagnation_counter = 0\n                        self.previous_best_fitness = self.f_opt\n                        self.archive_successes += 1\n                    else:\n                        self.archive_trials += 1\n\n                else:\n                     # Add trial vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = trial\n                        self.archive_fitness[worst_archive_index] = f_trial\n                        self.archive_trials += 1\n\n            population = new_population\n            fitness = new_fitness\n            \n            # Restart population if stagnating\n            if self.stagnation_counter > self.restart_patience:\n                # Jitter-based restart\n                jitter = np.random.uniform(-0.1, 0.1, size=population.shape)\n                population = np.clip(population + jitter, func.bounds.lb, func.bounds.ub)\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                    self.stagnation_counter = 0\n                    self.previous_best_fitness = self.f_opt\n                else:\n                    self.stagnation_counter +=1\n                    \n                # Reset F and CR history\n                self.F_history = []\n                self.CR_history = []\n            \n            # Adjust archive probability based on success rate\n            if self.archive_trials > 10:\n                self.archive_success_rate = self.archive_successes / self.archive_trials\n                if self.archive_success_rate < self.archive_success_threshold:\n                    self.archive_prob *= 0.9  # Decrease probability if success rate is low\n                else:\n                    self.archive_prob = min(1.0, self.archive_prob * 1.1) # Increase if success is good\n                self.archive_trials = 0\n                self.archive_successes = 0\n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveDE scored 0.668 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["8ba2aafb-e9de-48f1-b000-729655be7216"], "operator": null, "metadata": {"aucs": [0.2999348635292769, 0.6111403113275815, 0.6077347959067934, 0.8563687322647012, 0.7188894062771454, 0.7481632512711599, 0.6064899892634634, 0.6127913993760326, 0.7109424170372043, 0.6498785256904047, 0.8464313226924994, 0.9920392031397011, 0.5802005254535063, 0.6842058483696206, 0.9036037015393424, 0.7664652591626032, 0.5530773562217361, 0.8416748837783492, 0.2171820836792514, 0.5551365073998881]}}
{"id": "dbed2c30-7abf-4d4f-bd25-ca72a5c65676", "fitness": 0.6986048691453903, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with dynamic population size adjustment, orthogonal learning, archive with aging, and a combined mutation strategy.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size_min=20, pop_size_max=100, F_base=0.5, CR_base=0.7, archive_size=100, F_range=0.3, CR_range=0.3, orthogonal_learning_rate=0.1, restart_patience=50, archive_prob=0.1, current_to_best_prob=0.2, archive_success_threshold=0.1, aging_factor=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_min = pop_size_min\n        self.pop_size_max = pop_size_max\n        self.pop_size = pop_size_max # Start with max population\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.archive_size = archive_size\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.archive = []\n        self.archive_fitness = []\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.restart_patience = restart_patience\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.F_history = []\n        self.CR_history = []\n        self.archive_prob = archive_prob\n        self.current_to_best_prob = current_to_best_prob\n        self.archive_success_rate = 0.0\n        self.archive_successes = 0\n        self.archive_trials = 0\n        self.archive_success_threshold = archive_success_threshold\n        self.aging_factor = aging_factor\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n            self.stagnation_counter = 0\n            self.previous_best_fitness = self.f_opt\n        else:\n            self.stagnation_counter +=1\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n            \n            # Self-adaptive F and CR\n            if self.F_history:\n                self.F_base = np.mean(self.F_history)\n            if self.CR_history:\n                self.CR_base = np.mean(self.CR_history)\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                F = self.F_base + np.random.uniform(-self.F_range, self.F_range)\n                CR = self.CR_base + np.random.uniform(-self.CR_range, self.CR_range)\n                F = np.clip(F, 0.1, 1.0)\n                CR = np.clip(CR, 0.1, 1.0)\n\n                # Mutation: Combining current-to-best with random mutation\n                indices = np.random.choice(self.pop_size, 2, replace=False)\n                x1, x2 = population[indices]\n                \n                if np.random.rand() < self.current_to_best_prob:\n                    mutant = population[i] + F * (self.x_opt - population[i]) + F * (x1 - x2)\n                else:\n                    x3 = population[np.random.choice(self.pop_size)]\n                    mutant = x1 + F * (x2 - x3)\n                \n                # Incorporate archive\n                use_archive = False\n                if len(self.archive) > 0 and np.random.rand() < self.archive_prob:  # archive_prob chance to use archive\n                    archive_index = np.random.randint(len(self.archive))\n                    mutant = x1 + F * (x2 - self.archive[archive_index])\n                    use_archive = True\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_learning_rate:\n                    orthogonal_vector = self.x_opt + np.random.normal(0, 0.1, self.dim)  # Perturb best solution\n                    orthogonal_vector = np.clip(orthogonal_vector, func.bounds.lb, func.bounds.ub)\n                    trial = 0.5 * (trial + orthogonal_vector)  # Combine with trial vector\n                    trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                    \n                    # Record successful F and CR values\n                    self.F_history.append(F)\n                    self.CR_history.append(CR)\n                    if len(self.F_history) > 50:\n                        self.F_history.pop(0)\n                        self.CR_history.pop(0)\n\n                    # Add replaced vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                         # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = population[i]\n                        self.archive_fitness[worst_archive_index] = fitness[i]\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.stagnation_counter = 0\n                        self.previous_best_fitness = self.f_opt\n                    \n                    if use_archive:\n                        self.archive_successes += 1\n\n                else:\n                     # Add trial vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = trial\n                        self.archive_fitness[worst_archive_index] = f_trial\n                        \n                if use_archive:\n                    self.archive_trials += 1\n\n            population = new_population\n            fitness = new_fitness\n            \n            # Restart population if stagnating\n            if self.stagnation_counter > self.restart_patience:\n                # Opposition-based learning for restart\n                opposition_population = func.bounds.ub + func.bounds.lb - population\n                opposition_fitness = np.array([func(x) for x in opposition_population])\n                self.budget -= self.pop_size\n                \n                combined_population = np.concatenate((population, opposition_population))\n                combined_fitness = np.concatenate((fitness, opposition_fitness))\n                \n                # Select the best individuals from the combined population\n                indices = np.argsort(combined_fitness)[:self.pop_size]\n                population = combined_population[indices]\n                fitness = combined_fitness[indices]\n\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                    self.stagnation_counter = 0\n                    self.previous_best_fitness = self.f_opt\n                else:\n                    self.stagnation_counter +=1\n                    \n                # Reset F and CR history\n                self.F_history = []\n                self.CR_history = []\n            \n            # Adjust archive probability based on success rate\n            if self.archive_trials > 10:\n                self.archive_success_rate = self.archive_successes / self.archive_trials\n                if self.archive_success_rate < self.archive_success_threshold:\n                    self.archive_prob *= 0.9  # Decrease probability if success rate is low\n                else:\n                    self.archive_prob = min(1.0, self.archive_prob * 1.1) # Increase if success is good\n                self.archive_trials = 0\n                self.archive_successes = 0\n\n            # Dynamic population size adjustment\n            if self.f_opt < self.previous_best_fitness:\n                self.pop_size = min(self.pop_size_max, self.pop_size + 1)  # Increase pop size if improvement\n                self.previous_best_fitness = self.f_opt\n            else:\n                self.pop_size = max(self.pop_size_min, self.pop_size - 1)  # Decrease pop size if no improvement\n            \n            # Age the archive\n            for k in range(len(self.archive_fitness)):\n                self.archive_fitness[k] *= self.aging_factor\n                \n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveDE scored 0.699 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["8ba2aafb-e9de-48f1-b000-729655be7216"], "operator": null, "metadata": {"aucs": [0.3796275610439075, 0.6851278760228381, 0.699967801364777, 0.8578124781638814, 0.7060847090275242, 0.7466776611149941, 0.5977977652559003, 0.6942253418299709, 0.7168354273509998, 0.6434349978300624, 0.7729477153733681, 0.9994992880269044, 0.6284577805384477, 0.6911001476601769, 0.9073517761668188, 0.7233790189659806, 0.6091357892044582, 0.7991061071668548, 0.5937661775071406, 0.5197619632928007]}}
{"id": "7ec7f99f-4bff-487e-85f7-dce44838a65b", "fitness": 0.7068887872443199, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with a decaying archive probability, self-adaptive F and CR parameters, orthogonal learning, a combined archive with aging, and a dynamic restart strategy with dynamically adjusted bounds.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_base=0.5, CR_base=0.7, archive_size=100, F_range=0.3, CR_range=0.3, orthogonal_learning_rate=0.1, restart_patience=50, archive_prob=0.1, current_to_best_prob=0.2, archive_success_threshold=0.1, archive_decay_rate=0.99):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.archive_size = archive_size\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.archive = []\n        self.archive_fitness = []\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.restart_patience = restart_patience\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.F_history = []\n        self.CR_history = []\n        self.archive_prob = archive_prob\n        self.current_to_best_prob = current_to_best_prob\n        self.archive_success_rate = 0.0\n        self.archive_successes = 0\n        self.archive_trials = 0\n        self.archive_success_threshold = archive_success_threshold\n        self.archive_decay_rate = archive_decay_rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.dynamic_bounds_factor = 0.1\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n            self.stagnation_counter = 0\n            self.previous_best_fitness = self.f_opt\n        else:\n            self.stagnation_counter +=1\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n            \n            # Self-adaptive F and CR\n            if self.F_history:\n                self.F_base = np.mean(self.F_history)\n            if self.CR_history:\n                self.CR_base = np.mean(self.CR_history)\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                F = self.F_base + np.random.uniform(-self.F_range, self.F_range)\n                CR = self.CR_base + np.random.uniform(-self.CR_range, self.CR_range)\n                F = np.clip(F, 0.1, 1.0)\n                CR = np.clip(CR, 0.1, 1.0)\n\n                # Mutation: Combining current-to-best with random mutation\n                indices = np.random.choice(self.pop_size, 2, replace=False)\n                x1, x2 = population[indices]\n                \n                if np.random.rand() < self.current_to_best_prob:\n                    mutant = population[i] + F * (self.x_opt - population[i]) + F * (x1 - x2)\n                else:\n                    x3 = population[np.random.choice(self.pop_size)]\n                    mutant = x1 + F * (x2 - x3)\n                \n                # Incorporate archive\n                use_archive = False\n                if len(self.archive) > 0 and np.random.rand() < self.archive_prob:  # archive_prob chance to use archive\n                    archive_index = np.random.randint(len(self.archive))\n                    mutant = x1 + F * (x2 - self.archive[archive_index])\n                    use_archive = True\n\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_learning_rate:\n                    orthogonal_vector = self.x_opt + np.random.normal(0, 0.1, self.dim)  # Perturb best solution\n                    orthogonal_vector = np.clip(orthogonal_vector, self.lb, self.ub)\n                    trial = 0.5 * (trial + orthogonal_vector)  # Combine with trial vector\n                    trial = np.clip(trial, self.lb, self.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                    \n                    # Record successful F and CR values\n                    self.F_history.append(F)\n                    self.CR_history.append(CR)\n                    if len(self.F_history) > 50:\n                        self.F_history.pop(0)\n                        self.CR_history.pop(0)\n\n                    # Add replaced vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                         # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = population[i]\n                        self.archive_fitness[worst_archive_index] = fitness[i]\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.stagnation_counter = 0\n                        self.previous_best_fitness = self.f_opt\n                    \n                    if use_archive:\n                        self.archive_successes += 1\n\n                else:\n                     # Add trial vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = trial\n                        self.archive_fitness[worst_archive_index] = f_trial\n                        \n                if use_archive:\n                    self.archive_trials += 1\n\n            population = new_population\n            fitness = new_fitness\n            \n            # Restart population if stagnating\n            if self.stagnation_counter > self.restart_patience:\n                # Dynamically adjust bounds for restart\n                range_x = np.max(population) - np.min(population)\n                lower_bound = self.x_opt - range_x * self.dynamic_bounds_factor\n                upper_bound = self.x_opt + range_x * self.dynamic_bounds_factor\n                lower_bound = np.clip(lower_bound, -5.0, 5.0)\n                upper_bound = np.clip(upper_bound, -5.0, 5.0)\n                \n                # Opposition-based learning for restart within adjusted bounds\n                opposition_population = upper_bound + lower_bound - population\n                opposition_population = np.clip(opposition_population, lower_bound, upper_bound)\n\n                opposition_fitness = np.array([func(x) for x in opposition_population])\n                self.budget -= self.pop_size\n                \n                combined_population = np.concatenate((population, opposition_population))\n                combined_fitness = np.concatenate((fitness, opposition_fitness))\n                \n                # Select the best individuals from the combined population\n                indices = np.argsort(combined_fitness)[:self.pop_size]\n                population = combined_population[indices]\n                fitness = combined_fitness[indices]\n\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                    self.stagnation_counter = 0\n                    self.previous_best_fitness = self.f_opt\n                else:\n                    self.stagnation_counter +=1\n                    \n                # Reset F and CR history\n                self.F_history = []\n                self.CR_history = []\n            \n            # Adjust archive probability based on success rate\n            if self.archive_trials > 10:\n                self.archive_success_rate = self.archive_successes / self.archive_trials\n                if self.archive_success_rate < self.archive_success_threshold:\n                    self.archive_prob *= 0.9  # Decrease probability if success rate is low\n                else:\n                    self.archive_prob = min(1.0, self.archive_prob * 1.1) # Increase if success is good\n                self.archive_trials = 0\n                self.archive_successes = 0\n\n            self.archive_prob *= self.archive_decay_rate  # Decay archive probability over time\n            self.archive_prob = max(0.01, self.archive_prob) # Ensure archive_prob does not go to zero\n            \n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveDE scored 0.707 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["8ba2aafb-e9de-48f1-b000-729655be7216"], "operator": null, "metadata": {"aucs": [0.40514198247702815, 0.6852590989689529, 0.5421860477478727, 0.8992651413783661, 0.7890255942640849, 0.80797169035268, 0.6924355833919591, 0.7008041597186825, 0.7736550060390461, 0.7216353398191029, 0.8911485807719115, 0.9907718546095928, 0.4206144688751078, 0.7630791062569264, 0.9241442295600951, 0.7220922819285007, 0.7079739783548753, 0.8647369330212793, 0.31979017208082217, 0.5160444952695111]}}
{"id": "c71a85b0-79cc-4ac0-8462-d189b70da42e", "fitness": 0.6978254395821104, "name": "AdaptiveDE", "description": "Enhanced Adaptive Differential Evolution with dual archives (one for successful and one for unsuccessful solutions), adaptive F and CR with success-history and failure-history based adaptation, orthogonal learning, and a dynamic restart strategy incorporating the opposition-based learning.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_base=0.5, CR_base=0.7, archive_size=50, F_range=0.3, CR_range=0.3, orthogonal_learning_rate=0.1, restart_patience=50, archive_prob=0.1, current_to_best_prob=0.2, gamma_F = 0.1, gamma_CR = 0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.archive_size = archive_size\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.success_archive = []\n        self.failure_archive = []\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.restart_patience = restart_patience\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.F_success_history = []\n        self.CR_success_history = []\n        self.F_failure_history = []\n        self.CR_failure_history = []\n        self.archive_prob = archive_prob\n        self.current_to_best_prob = current_to_best_prob\n        self.gamma_F = gamma_F\n        self.gamma_CR = gamma_CR\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n            self.stagnation_counter = 0\n            self.previous_best_fitness = self.f_opt\n        else:\n            self.stagnation_counter +=1\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n            \n            # Self-adaptive F and CR\n            if self.F_success_history:\n                self.F_base = (1-self.gamma_F) * self.F_base + self.gamma_F * np.mean(self.F_success_history)\n                self.F_base = np.clip(self.F_base, 0.1, 1.0)\n            if self.CR_success_history:\n                self.CR_base = (1-self.gamma_CR) * self.CR_base + self.gamma_CR * np.mean(self.CR_success_history)\n                self.CR_base = np.clip(self.CR_base, 0.1, 1.0)\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                F = self.F_base + np.random.uniform(-self.F_range, self.F_range)\n                CR = self.CR_base + np.random.uniform(-self.CR_range, self.CR_range)\n                F = np.clip(F, 0.1, 1.0)\n                CR = np.clip(CR, 0.1, 1.0)\n\n                # Mutation: Combining current-to-best with random mutation\n                indices = np.random.choice(self.pop_size, 2, replace=False)\n                x1, x2 = population[indices]\n                \n                if np.random.rand() < self.current_to_best_prob:\n                    mutant = population[i] + F * (self.x_opt - population[i]) + F * (x1 - x2)\n                else:\n                    x3 = population[np.random.choice(self.pop_size)]\n                    mutant = x1 + F * (x2 - x3)\n                \n                # Incorporate archive\n                if len(self.success_archive) > 0 and np.random.rand() < self.archive_prob:  # archive_prob chance to use archive\n                    archive_index = np.random.randint(len(self.success_archive))\n                    mutant = x1 + F * (x2 - self.success_archive[archive_index])\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_learning_rate:\n                    orthogonal_vector = self.x_opt + np.random.normal(0, 0.1, self.dim)  # Perturb best solution\n                    orthogonal_vector = np.clip(orthogonal_vector, func.bounds.lb, func.bounds.ub)\n                    trial = 0.5 * (trial + orthogonal_vector)  # Combine with trial vector\n                    trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                    \n                    # Record successful F and CR values\n                    self.F_success_history.append(F)\n                    self.CR_success_history.append(CR)\n                    if len(self.F_success_history) > 50:\n                        self.F_success_history.pop(0)\n                        self.CR_success_history.pop(0)\n\n                    # Add replaced vector to success archive\n                    if len(self.success_archive) < self.archive_size:\n                        self.success_archive.append(population[i])\n                    else:\n                        self.success_archive[np.random.randint(self.archive_size)] = population[i]\n\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.stagnation_counter = 0\n                        self.previous_best_fitness = self.f_opt\n                else:\n                    self.F_failure_history.append(F)\n                    self.CR_failure_history.append(CR)\n                    if len(self.F_failure_history) > 50:\n                         self.F_failure_history.pop(0)\n                    if len(self.CR_failure_history) > 50:\n                         self.CR_failure_history.pop(0)\n\n                     # Add trial vector to failure archive\n                    if len(self.failure_archive) < self.archive_size:\n                        self.failure_archive.append(trial)\n                    else:\n                        self.failure_archive[np.random.randint(self.archive_size)] = trial\n\n            population = new_population\n            fitness = new_fitness\n            \n            # Restart population if stagnating using opposition-based learning\n            if self.stagnation_counter > self.restart_patience:\n                # Generate opposition population\n                opposition_population = func.bounds.ub + func.bounds.lb - population\n                opposition_fitness = np.array([func(x) for x in opposition_population])\n                self.budget -= self.pop_size\n\n                # Combine original and opposition populations\n                combined_population = np.vstack((population, opposition_population))\n                combined_fitness = np.concatenate((fitness, opposition_fitness))\n\n                # Select the best individuals to form the new population\n                best_indices = np.argsort(combined_fitness)[:self.pop_size]\n                population = combined_population[best_indices]\n                fitness = combined_fitness[best_indices]\n\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                    self.stagnation_counter = 0\n                    self.previous_best_fitness = self.f_opt\n                else:\n                    self.stagnation_counter +=1\n                    \n                # Reset F and CR history\n                self.F_success_history = []\n                self.CR_success_history = []\n                self.F_failure_history = []\n                self.CR_failure_history = []\n                \n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveDE scored 0.698 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["39b1104e-850a-489e-b666-bc4963790aa9"], "operator": null, "metadata": {"aucs": [0.42658231385111944, 0.7626469111847518, 0.6723803699249564, 0.9060484948826649, 0.7674772731829438, 0.8057873899074274, 0.7006746117158636, 0.702667559949174, 0.7604781586083672, 0.22805635079971964, 0.8892591743311777, 0.9904340137627343, 0.6329543888547795, 0.7565220215003947, 0.9286189289585615, 0.7951375943046018, 0.6287369575430576, 0.8616767882114512, 0.2268702378041687, 0.5134992523642931]}}
{"id": "d0c5bbca-bd80-4233-8ffe-a91bcd92aa6d", "fitness": 0.7043189645205232, "name": "AdaptiveDE_SOM", "description": "Adaptive Differential Evolution with a self-organizing map (SOM) for population diversity, adaptive F and CR using weighted historical success and quadratic approximation for local search refinement, orthogonal learning, a combined archive with aging, and a dynamic restart strategy based on SOM-detected stagnation.", "code": "import numpy as np\n\nclass AdaptiveDE_SOM:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_base=0.5, CR_base=0.7, archive_size=100, F_range=0.3, CR_range=0.3, orthogonal_learning_rate=0.1, som_grid_size=10, som_learning_rate=0.1, som_sigma=1.0, aging_rate=0.05, quadratic_approx_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.archive_size = archive_size\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.archive = []\n        self.archive_fitness = []\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.aging_rate = aging_rate\n        self.F_history = []\n        self.CR_history = []\n\n        # SOM parameters\n        self.som_grid_size = som_grid_size\n        self.som_learning_rate = som_learning_rate\n        self.som_sigma = som_sigma\n        self.som_weights = np.random.rand(som_grid_size, som_grid_size, dim)  # SOM grid\n        self.som_stagnation_threshold = 0.9  # Threshold for stagnation detection\n        self.som_stagnation_patience = 3 #Number of generations to wait before restart\n\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.restart_flag = False\n        \n        #Quadratic Approximation Rate\n        self.quadratic_approx_rate = quadratic_approx_rate\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n            self.previous_best_fitness = self.f_opt\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n\n            # Adaptive F and CR using weighted historical success\n            if self.F_history and self.CR_history:\n                weights_F = np.linspace(0.1, 1.0, len(self.F_history))\n                weights_CR = np.linspace(0.1, 1.0, len(self.CR_history))\n\n                F_weighted_avg = np.average(self.F_history, weights=weights_F)\n                CR_weighted_avg = np.average(self.CR_history, weights=weights_CR)\n            else:\n                F_weighted_avg = self.F_base\n                CR_weighted_avg = self.CR_base\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                F = F_weighted_avg + np.random.uniform(-self.F_range, self.F_range)\n                CR = CR_weighted_avg + np.random.uniform(-self.CR_range, self.CR_range)\n                F = np.clip(F, 0.1, 1.0)\n                CR = np.clip(CR, 0.1, 1.0)\n\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = x1 + F * (x2 - x3)\n\n                # Incorporate archive\n                if len(self.archive) > 0 and np.random.rand() < 0.1:  # 10% chance to use archive\n                    archive_index = np.random.randint(len(self.archive))\n                    mutant = x1 + F * (x2 - self.archive[archive_index])\n\n                # Repair mechanism\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_learning_rate:\n                    orthogonal_vector = self.x_opt + np.random.normal(0, 0.1, self.dim)  # Perturb best solution\n                    orthogonal_vector = np.clip(orthogonal_vector, func.bounds.lb, func.bounds.ub)\n                    trial = 0.5 * (trial + orthogonal_vector)  # Combine with trial vector\n                    trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Quadratic Approximation\n                if np.random.rand() < self.quadratic_approx_rate:\n                    # Select three random individuals\n                    indices_qa = np.random.choice(self.pop_size, 3, replace=False)\n                    x_a, x_b, x_c = population[indices_qa]\n                    f_a, f_b, f_c = fitness[indices_qa]\n\n                    # Perform quadratic approximation in each dimension\n                    for j in range(self.dim):\n                        try:\n                            # Solve for the minimum of the quadratic approximation\n                            denom = (x_a[j] - x_b[j]) * (f_a - f_c) - (x_a[j] - x_c[j]) * (f_a - f_b)\n                            if abs(denom) > 1e-6: # Avoid division by zero\n                                numerator = (x_a[j] - x_b[j])**2 * (f_a - f_c) - (x_a[j] - x_c[j])**2 * (f_a - f_b)\n                                x_min = x_a[j] - 0.5 * numerator / denom\n                                \n                                # Clip to bounds\n                                x_min = np.clip(x_min, func.bounds.lb[j], func.bounds.ub[j])\n                                trial[j] = x_min\n                        except:\n                            pass # In case of numerical instability\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                    \n                    # Record successful F and CR values\n                    self.F_history.append(F)\n                    self.CR_history.append(CR)\n                    if len(self.F_history) > 50:\n                        self.F_history.pop(0)\n                        self.CR_history.pop(0)\n\n                    # Add replaced vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                         # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = population[i]\n                        self.archive_fitness[worst_archive_index] = fitness[i]\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.previous_best_fitness = self.f_opt\n                else:\n                     # Add trial vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = trial\n                        self.archive_fitness[worst_archive_index] = f_trial\n\n            population = new_population\n            fitness = new_fitness\n\n            # SOM training (diversity maintenance)\n            self.train_som(population)\n            diversity_metric = self.calculate_diversity_metric(population)\n            \n            if diversity_metric > self.som_stagnation_threshold:\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n\n            # Dynamic restart based on SOM-detected stagnation\n            if self.stagnation_counter > self.som_stagnation_patience:\n                population = self.initialize_population_from_som(func.bounds.lb, func.bounds.ub)\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                \n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                    self.previous_best_fitness = self.f_opt\n                \n                self.F_history = []\n                self.CR_history = []\n                self.stagnation_counter = 0\n\n\n            # Archive aging\n            for k in range(len(self.archive_fitness)):\n                self.archive_fitness[k] *= (1 - self.aging_rate)\n            \n            if self.budget <= 0:\n                break\n        return self.f_opt, self.x_opt\n\n    def train_som(self, population):\n        \"\"\"Trains the Self-Organizing Map (SOM) with the current population.\"\"\"\n        for x in population:\n            # Find the best matching unit (BMU)\n            bmu_index = self.find_bmu(x)\n\n            # Update the SOM weights around the BMU\n            for i in range(self.som_grid_size):\n                for j in range(self.som_grid_size):\n                    distance = np.sqrt((i - bmu_index[0])**2 + (j - bmu_index[1])**2)\n                    influence = np.exp(-distance**2 / (2 * self.som_sigma**2))\n                    self.som_weights[i, j] += self.som_learning_rate * influence * (x - self.som_weights[i, j])\n\n    def find_bmu(self, individual):\n        \"\"\"Finds the best matching unit (BMU) in the SOM for a given individual.\"\"\"\n        min_distance = np.inf\n        bmu_index = None\n        for i in range(self.som_grid_size):\n            for j in range(self.som_grid_size):\n                distance = np.linalg.norm(individual - self.som_weights[i, j])\n                if distance < min_distance:\n                    min_distance = distance\n                    bmu_index = (i, j)\n        return bmu_index\n\n    def calculate_diversity_metric(self, population):\n        \"\"\"Calculates a diversity metric based on the SOM.\"\"\"\n        bmu_indices = [self.find_bmu(x) for x in population]\n        unique_bmu_count = len(set(bmu_indices))\n        return unique_bmu_count / self.pop_size\n\n    def initialize_population_from_som(self, lower_bound, upper_bound):\n        \"\"\"Initializes a new population by sampling from the SOM.\"\"\"\n        new_population = np.zeros((self.pop_size, self.dim))\n        for i in range(self.pop_size):\n            # Randomly select a node from the SOM\n            row = np.random.randint(0, self.som_grid_size)\n            col = np.random.randint(0, self.som_grid_size)\n            \n            # Perturb the SOM node's weights to create a new individual\n            new_individual = self.som_weights[row, col] + np.random.normal(0, 0.1, self.dim)\n            new_individual = np.clip(new_individual, lower_bound, upper_bound)\n            new_population[i] = new_individual\n        return new_population", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveDE_SOM scored 0.704 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["be34ac8d-13ff-4207-ad70-bb80883ee23a"], "operator": null, "metadata": {"aucs": [0.39391957573803127, 0.822497253359927, 0.5713523707930155, 0.8985029091683924, 0.7980903412657161, 0.8170934429819934, 0.7028376624726641, 0.6818453921055604, 0.7881559507510476, 0.7067649829701422, 0.913646973395998, 0.997476890513111, 0.38784561223538006, 0.76817818959657, 0.8829706359247131, 0.7717992548627977, 0.5043100065468686, 0.8669549557077743, 0.2966724038723094, 0.5154644861484553]}}
{"id": "665c31ae-e3ba-4d84-b727-c3ce4c686c11", "fitness": 0.5481089726633612, "name": "AdaptiveDE_SOM", "description": "Enhanced Adaptive Differential Evolution with a self-organizing map (SOM) for population diversity, adaptive F and CR using weighted historical success, orthogonal learning, a combined archive with aging, a dynamic restart strategy based on SOM-detected stagnation, and a niching mechanism based on the SOM to promote exploration in under-explored regions.", "code": "import numpy as np\n\nclass AdaptiveDE_SOM:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_base=0.5, CR_base=0.7, archive_size=100, F_range=0.3, CR_range=0.3, orthogonal_learning_rate=0.1, som_grid_size=10, som_learning_rate=0.1, som_sigma=1.0, aging_rate=0.05, niche_radius=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.archive_size = archive_size\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.archive = []\n        self.archive_fitness = []\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.aging_rate = aging_rate\n        self.F_history = []\n        self.CR_history = []\n\n        # SOM parameters\n        self.som_grid_size = som_grid_size\n        self.som_learning_rate = som_learning_rate\n        self.som_sigma = som_sigma\n        self.som_weights = np.random.rand(som_grid_size, som_grid_size, dim)  # SOM grid\n        self.som_stagnation_threshold = 0.9  # Threshold for stagnation detection\n        self.som_stagnation_patience = 3 #Number of generations to wait before restart\n\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.restart_flag = False\n\n        #Niche Parameters\n        self.niche_radius = niche_radius\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n            self.previous_best_fitness = self.f_opt\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n\n            # Adaptive F and CR using weighted historical success\n            if self.F_history and self.CR_history:\n                weights_F = np.linspace(0.1, 1.0, len(self.F_history))\n                weights_CR = np.linspace(0.1, 1.0, len(self.CR_history))\n\n                F_weighted_avg = np.average(self.F_history, weights=weights_F)\n                CR_weighted_avg = np.average(self.CR_history, weights=weights_CR)\n            else:\n                F_weighted_avg = self.F_base\n                CR_weighted_avg = self.CR_base\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                F = F_weighted_avg + np.random.uniform(-self.F_range, self.F_range)\n                CR = CR_weighted_avg + np.random.uniform(-self.CR_range, self.CR_range)\n                F = np.clip(F, 0.1, 1.0)\n                CR = np.clip(CR, 0.1, 1.0)\n\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = x1 + F * (x2 - x3)\n\n                # Incorporate archive\n                if len(self.archive) > 0 and np.random.rand() < 0.1:  # 10% chance to use archive\n                    archive_index = np.random.randint(len(self.archive))\n                    mutant = x1 + F * (x2 - self.archive[archive_index])\n\n                # Repair mechanism\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_learning_rate:\n                    orthogonal_vector = self.x_opt + np.random.normal(0, 0.1, self.dim)  # Perturb best solution\n                    orthogonal_vector = np.clip(orthogonal_vector, func.bounds.lb, func.bounds.ub)\n                    trial = 0.5 * (trial + orthogonal_vector)  # Combine with trial vector\n                    trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Niche Mechanism - encourage exploration in under-explored SOM regions\n                bmu_index = self.find_bmu(population[i])\n                nearby_individuals = [p for p in population if np.linalg.norm(p - population[i]) < self.niche_radius]\n                if len(nearby_individuals) < self.pop_size / self.som_grid_size:  # Encourage exploration\n                    som_node = self.som_weights[bmu_index[0], bmu_index[1]]\n                    trial = 0.7 * trial + 0.3 * som_node + np.random.normal(0, 0.05, self.dim)\n                    trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                    \n                    # Record successful F and CR values\n                    self.F_history.append(F)\n                    self.CR_history.append(CR)\n                    if len(self.F_history) > 50:\n                        self.F_history.pop(0)\n                        self.CR_history.pop(0)\n\n                    # Add replaced vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                         # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = population[i]\n                        self.archive_fitness[worst_archive_index] = fitness[i]\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.previous_best_fitness = self.f_opt\n                else:\n                     # Add trial vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = trial\n                        self.archive_fitness[worst_archive_index] = f_trial\n\n            population = new_population\n            fitness = new_fitness\n\n            # SOM training (diversity maintenance)\n            self.train_som(population)\n            diversity_metric = self.calculate_diversity_metric(population)\n            \n            if diversity_metric > self.som_stagnation_threshold:\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n\n            # Dynamic restart based on SOM-detected stagnation\n            if self.stagnation_counter > self.som_stagnation_patience:\n                population = self.initialize_population_from_som(func.bounds.lb, func.bounds.ub)\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                \n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                    self.previous_best_fitness = self.f_opt\n                \n                self.F_history = []\n                self.CR_history = []\n                self.stagnation_counter = 0\n\n\n            # Archive aging\n            for k in range(len(self.archive_fitness)):\n                self.archive_fitness[k] *= (1 - self.aging_rate)\n            \n            if self.budget <= 0:\n                break\n        return self.f_opt, self.x_opt\n\n    def train_som(self, population):\n        \"\"\"Trains the Self-Organizing Map (SOM) with the current population.\"\"\"\n        for x in population:\n            # Find the best matching unit (BMU)\n            bmu_index = self.find_bmu(x)\n\n            # Update the SOM weights around the BMU\n            for i in range(self.som_grid_size):\n                for j in range(self.som_grid_size):\n                    distance = np.sqrt((i - bmu_index[0])**2 + (j - bmu_index[1])**2)\n                    influence = np.exp(-distance**2 / (2 * self.som_sigma**2))\n                    self.som_weights[i, j] += self.som_learning_rate * influence * (x - self.som_weights[i, j])\n\n    def find_bmu(self, individual):\n        \"\"\"Finds the best matching unit (BMU) in the SOM for a given individual.\"\"\"\n        min_distance = np.inf\n        bmu_index = None\n        for i in range(self.som_grid_size):\n            for j in range(self.som_grid_size):\n                distance = np.linalg.norm(individual - self.som_weights[i, j])\n                if distance < min_distance:\n                    min_distance = distance\n                    bmu_index = (i, j)\n        return bmu_index\n\n    def calculate_diversity_metric(self, population):\n        \"\"\"Calculates a diversity metric based on the SOM.\"\"\"\n        bmu_indices = [self.find_bmu(x) for x in population]\n        unique_bmu_count = len(set(bmu_indices))\n        return unique_bmu_count / self.pop_size\n\n    def initialize_population_from_som(self, lower_bound, upper_bound):\n        \"\"\"Initializes a new population by sampling from the SOM.\"\"\"\n        new_population = np.zeros((self.pop_size, self.dim))\n        for i in range(self.pop_size):\n            # Randomly select a node from the SOM\n            row = np.random.randint(0, self.som_grid_size)\n            col = np.random.randint(0, self.som_grid_size)\n            \n            # Perturb the SOM node's weights to create a new individual\n            new_individual = self.som_weights[row, col] + np.random.normal(0, 0.1, self.dim)\n            new_individual = np.clip(new_individual, lower_bound, upper_bound)\n            new_population[i] = new_individual\n        return new_population", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveDE_SOM scored 0.548 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["be34ac8d-13ff-4207-ad70-bb80883ee23a"], "operator": null, "metadata": {"aucs": [0.2010606151101495, 0.34048196033758416, 0.5857500895693335, 0.7118369818159671, 0.26875266473741855, 0.7337189731656939, 0.46542631280525326, 0.5980129931517364, 0.7236420268373802, 0.3285557286964086, 0.7385768258534151, 0.9993170199036957, 0.2758033541454249, 0.5851057324506679, 0.8765767814423692, 0.6865102001555419, 0.317001530918254, 0.7895494412019978, 0.23527768318477538, 0.5012225377841533]}}
{"id": "310f3cdc-b2ea-43ed-b8db-d7c65d57ec88", "fitness": -Infinity, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with separate adaptation of F and CR for each individual based on their success, combined with a diversity maintenance mechanism using clustering.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=50, orthogonal_learning_rate=0.1, restart_patience=50, archive_prob=0.1, current_to_best_prob=0.2, cluster_num = 5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.restart_patience = restart_patience\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.archive = []\n        self.archive_fitnesses = []\n        self.archive_F = []\n        self.archive_CR = []\n        self.archive_prob = archive_prob\n        self.current_to_best_prob = current_to_best_prob\n        self.cluster_num = cluster_num\n        self.F = np.full(pop_size, 0.5)\n        self.CR = np.full(pop_size, 0.7)\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n            self.stagnation_counter = 0\n            self.previous_best_fitness = self.f_opt\n        else:\n            self.stagnation_counter +=1\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n\n            # Clustering for diversity maintenance\n            kmeans = KMeans(n_clusters=self.cluster_num, random_state=0, n_init = 'auto')\n            clusters = kmeans.fit_predict(population)\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR, individual-based\n                F = self.F[i] + np.random.normal(0, 0.1)\n                CR = self.CR[i] + np.random.normal(0, 0.1)\n                F = np.clip(F, 0.1, 1.0)\n                CR = np.clip(CR, 0.1, 1.0)\n\n                # Mutation: Combining current-to-best with random mutation\n                indices = np.random.choice(self.pop_size, 2, replace=False)\n                x1, x2 = population[indices]\n                \n                if np.random.rand() < self.current_to_best_prob:\n                    mutant = population[i] + F * (self.x_opt - population[i]) + F * (x1 - x2)\n                else:\n                    x3 = population[np.random.choice(self.pop_size)]\n                    mutant = x1 + F * (x2 - x3)\n                \n                # Incorporate archive\n                if len(self.archive) > 0 and np.random.rand() < self.archive_prob:\n                    archive_index = np.random.randint(len(self.archive))\n                    mutant = x1 + F * (x2 - self.archive[archive_index])\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_learning_rate:\n                    orthogonal_vector = self.x_opt + np.random.normal(0, 0.1, self.dim)  # Perturb best solution\n                    orthogonal_vector = np.clip(orthogonal_vector, func.bounds.lb, func.bounds.ub)\n                    trial = 0.5 * (trial + orthogonal_vector)  # Combine with trial vector\n                    trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n\n                    # Update F and CR values\n                    self.F[i] = 0.9 * self.F[i] + 0.1 * F # Exponential smoothing\n                    self.CR[i] = 0.9 * self.CR[i] + 0.1 * CR\n\n                    # Add replaced vector to archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                    else:\n                        self.archive[np.random.randint(self.archive_size)] = population[i]\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.stagnation_counter = 0\n                        self.previous_best_fitness = self.f_opt\n                else:\n                    # If trial is worse, penalize F and CR\n                    self.F[i] = np.clip(self.F[i] - 0.1, 0.1, 1.0)\n                    self.CR[i] = np.clip(self.CR[i] - 0.1, 0.1, 1.0)\n\n            population = new_population\n            fitness = new_fitness\n            \n            # Restart population if stagnating using opposition-based learning\n            if self.stagnation_counter > self.restart_patience:\n                # Generate opposition population\n                opposition_population = func.bounds.ub + func.bounds.lb - population\n                opposition_fitness = np.array([func(x) for x in opposition_population])\n                self.budget -= self.pop_size\n\n                # Combine original and opposition populations\n                combined_population = np.vstack((population, opposition_population))\n                combined_fitness = np.concatenate((fitness, opposition_fitness))\n\n                # Select the best individuals to form the new population\n                best_indices = np.argsort(combined_fitness)[:self.pop_size]\n                population = combined_population[best_indices]\n                fitness = combined_fitness[best_indices]\n\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                    self.stagnation_counter = 0\n                    self.previous_best_fitness = self.f_opt\n                else:\n                    self.stagnation_counter +=1\n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 50, in __call__\nNameError: name 'KMeans' is not defined\n.", "error": "", "parent_ids": ["c71a85b0-79cc-4ac0-8462-d189b70da42e"], "operator": null, "metadata": {}}
{"id": "cc90bb91-b10a-4873-a572-5df1c9df7d49", "fitness": -Infinity, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with a toroidal search space to handle boundary violations, an enhanced aging mechanism for the archive, and periodic archive refreshment based on best solutions.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size_min=20, pop_size_max=100, F_base=0.5, CR_base=0.7, archive_size=100, F_range=0.3, CR_range=0.3, orthogonal_learning_rate=0.1, restart_patience=50, archive_prob=0.1, current_to_best_prob=0.2, archive_success_threshold=0.1, aging_factor=0.95, archive_refresh_interval=20):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_min = pop_size_min\n        self.pop_size_max = pop_size_max\n        self.pop_size = pop_size_max # Start with max population\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.archive_size = archive_size\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.archive = []\n        self.archive_fitness = []\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.restart_patience = restart_patience\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.F_history = []\n        self.CR_history = []\n        self.archive_prob = archive_prob\n        self.current_to_best_prob = current_to_best_prob\n        self.archive_success_rate = 0.0\n        self.archive_successes = 0\n        self.archive_trials = 0\n        self.archive_success_threshold = archive_success_threshold\n        self.aging_factor = aging_factor\n        self.archive_refresh_interval = archive_refresh_interval\n        self.generation = 0\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n            self.stagnation_counter = 0\n            self.previous_best_fitness = self.f_opt\n        else:\n            self.stagnation_counter +=1\n\n        while self.budget > 0:\n            self.generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n            \n            # Self-adaptive F and CR\n            if self.F_history:\n                self.F_base = np.mean(self.F_history)\n            if self.CR_history:\n                self.CR_base = np.mean(self.CR_history)\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                F = self.F_base + np.random.uniform(-self.F_range, self.F_range)\n                CR = self.CR_base + np.random.uniform(-self.CR_range, self.CR_range)\n                F = np.clip(F, 0.1, 1.0)\n                CR = np.clip(CR, 0.1, 1.0)\n\n                # Mutation: Combining current-to-best with random mutation\n                indices = np.random.choice(self.pop_size, 2, replace=False)\n                x1, x2 = population[indices]\n                \n                if np.random.rand() < self.current_to_best_prob:\n                    mutant = population[i] + F * (self.x_opt - population[i]) + F * (x1 - x2)\n                else:\n                    x3 = population[np.random.choice(self.pop_size)]\n                    mutant = x1 + F * (x2 - x3)\n                \n                # Incorporate archive\n                use_archive = False\n                if len(self.archive) > 0 and np.random.rand() < self.archive_prob:  # archive_prob chance to use archive\n                    archive_index = np.random.randint(len(self.archive))\n                    mutant = x1 + F * (x2 - self.archive[archive_index])\n                    use_archive = True\n\n                # Toroidal handling of boundary violations\n                for j in range(self.dim):\n                    if mutant[j] < func.bounds.lb:\n                        mutant[j] = func.bounds.ub - (func.bounds.lb - mutant[j]) % (func.bounds.ub - func.bounds.lb)\n                    elif mutant[j] > func.bounds.ub:\n                        mutant[j] = func.bounds.lb + (mutant[j] - func.bounds.ub) % (func.bounds.ub - func.bounds.lb)\n                \n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_learning_rate:\n                    orthogonal_vector = self.x_opt + np.random.normal(0, 0.1, self.dim)  # Perturb best solution\n                    orthogonal_vector = np.clip(orthogonal_vector, func.bounds.lb, func.bounds.ub)\n                    trial = 0.5 * (trial + orthogonal_vector)  # Combine with trial vector\n                    trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                    \n                    # Record successful F and CR values\n                    self.F_history.append(F)\n                    self.CR_history.append(CR)\n                    if len(self.F_history) > 50:\n                        self.F_history.pop(0)\n                        self.CR_history.pop(0)\n\n                    # Add replaced vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                         # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = population[i]\n                        self.archive_fitness[worst_archive_index] = fitness[i]\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.stagnation_counter = 0\n                        self.previous_best_fitness = self.f_opt\n                    \n                    if use_archive:\n                        self.archive_successes += 1\n\n                else:\n                     # Add trial vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = trial\n                        self.archive_fitness[worst_archive_index] = f_trial\n                        \n                if use_archive:\n                    self.archive_trials += 1\n\n            population = new_population\n            fitness = new_fitness\n            \n            # Restart population if stagnating\n            if self.stagnation_counter > self.restart_patience:\n                # Opposition-based learning for restart\n                opposition_population = func.bounds.ub + func.bounds.lb - population\n                opposition_fitness = np.array([func(x) for x in opposition_population])\n                self.budget -= self.pop_size\n                \n                combined_population = np.concatenate((population, opposition_population))\n                combined_fitness = np.concatenate((fitness, opposition_fitness))\n                \n                # Select the best individuals from the combined population\n                indices = np.argsort(combined_fitness)[:self.pop_size]\n                population = combined_population[indices]\n                fitness = combined_fitness[indices]\n\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                    self.stagnation_counter = 0\n                    self.previous_best_fitness = self.f_opt\n                else:\n                    self.stagnation_counter +=1\n                    \n                # Reset F and CR history\n                self.F_history = []\n                self.CR_history = []\n            \n            # Adjust archive probability based on success rate\n            if self.archive_trials > 10:\n                self.archive_success_rate = self.archive_successes / self.archive_trials\n                if self.archive_success_rate < self.archive_success_threshold:\n                    self.archive_prob *= 0.9  # Decrease probability if success rate is low\n                else:\n                    self.archive_prob = min(1.0, self.archive_prob * 1.1) # Increase if success is good\n                self.archive_trials = 0\n                self.archive_successes = 0\n\n            # Dynamic population size adjustment\n            if self.f_opt < self.previous_best_fitness:\n                self.pop_size = min(self.pop_size_max, self.pop_size + 1)  # Increase pop size if improvement\n                self.previous_best_fitness = self.f_opt\n            else:\n                self.pop_size = max(self.pop_size_min, self.pop_size - 1)  # Decrease pop size if no improvement\n\n            # Enhanced Aging\n            for k in range(len(self.archive_fitness)):\n                # Age based on distance to the best\n                distance_factor = np.linalg.norm(self.archive[k] - self.x_opt) / np.linalg.norm(func.bounds.ub - func.bounds.lb)\n                self.archive_fitness[k] *= self.aging_factor + (1 - self.aging_factor) * (1 - distance_factor)\n                \n            # Periodically refresh archive with best solutions\n            if self.generation % self.archive_refresh_interval == 0:\n                num_best_replace = min(len(self.archive), self.pop_size // 4)  # Replace a fraction of archive\n                best_indices = np.argsort(fitness)[:num_best_replace]\n                worst_archive_indices = np.argsort(self.archive_fitness)[-num_best_replace:]\n                \n                for k in range(num_best_replace):\n                    self.archive[worst_archive_indices[k]] = population[best_indices[k]]\n                    self.archive_fitness[worst_archive_indices[k]] = fitness[best_indices[k]]\n                \n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 144, in evaluate\n    algorithm(f_new)\n  File \"<string>\", line 90, in __call__\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n.", "error": "", "parent_ids": ["dbed2c30-7abf-4d4f-bd25-ca72a5c65676"], "operator": null, "metadata": {}}
{"id": "4f2f4fd0-919c-40cc-8b32-aa40f347c341", "fitness": -Infinity, "name": "AdaptiveDE_SOM_NM", "description": "Enhanced Adaptive Differential Evolution with SOM-guided population initialization and local search using Nelder-Mead simplex.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDE_SOM_NM:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_base=0.5, CR_base=0.7, archive_size=100, F_range=0.3, CR_range=0.3, orthogonal_learning_rate=0.1, som_grid_size=10, som_learning_rate=0.1, som_sigma=1.0, aging_rate=0.05, nm_local_search_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.archive_size = archive_size\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.archive = []\n        self.archive_fitness = []\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.aging_rate = aging_rate\n        self.F_history = []\n        self.CR_history = []\n\n        # SOM parameters\n        self.som_grid_size = som_grid_size\n        self.som_learning_rate = som_learning_rate\n        self.som_sigma = som_sigma\n        self.som_weights = np.random.rand(som_grid_size, som_grid_size, dim)  # SOM grid\n        self.som_stagnation_threshold = 0.9  # Threshold for stagnation detection\n        self.som_stagnation_patience = 3 #Number of generations to wait before restart\n\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.restart_flag = False\n        \n        # Nelder-Mead local search rate\n        self.nm_local_search_rate = nm_local_search_rate\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population from SOM\n        population = self.initialize_population_from_som(func.bounds.lb, func.bounds.ub)\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n            self.previous_best_fitness = self.f_opt\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n\n            # Adaptive F and CR using weighted historical success\n            if self.F_history and self.CR_history:\n                weights_F = np.linspace(0.1, 1.0, len(self.F_history))\n                weights_CR = np.linspace(0.1, 1.0, len(self.CR_history))\n\n                F_weighted_avg = np.average(self.F_history, weights=weights_F)\n                CR_weighted_avg = np.average(self.CR_history, weights=weights_CR)\n            else:\n                F_weighted_avg = self.F_base\n                CR_weighted_avg = self.CR_base\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                F = F_weighted_avg + np.random.uniform(-self.F_range, self.F_range)\n                CR = CR_weighted_avg + np.random.uniform(-self.CR_range, self.CR_range)\n                F = np.clip(F, 0.1, 1.0)\n                CR = np.clip(CR, 0.1, 1.0)\n\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = x1 + F * (x2 - x3)\n\n                # Incorporate archive\n                if len(self.archive) > 0 and np.random.rand() < 0.1:  # 10% chance to use archive\n                    archive_index = np.random.randint(len(self.archive))\n                    mutant = x1 + F * (x2 - self.archive[archive_index])\n\n                # Repair mechanism\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_learning_rate:\n                    orthogonal_vector = self.x_opt + np.random.normal(0, 0.1, self.dim)  # Perturb best solution\n                    orthogonal_vector = np.clip(orthogonal_vector, func.bounds.lb, func.bounds.ub)\n                    trial = 0.5 * (trial + orthogonal_vector)  # Combine with trial vector\n                    trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Local Search using Nelder-Mead\n                if np.random.rand() < self.nm_local_search_rate:\n                    bounds = [(func.bounds.lb[j], func.bounds.ub[j]) for j in range(self.dim)]\n                    result = minimize(func, trial, method='Nelder-Mead', bounds=bounds, options={'maxfev': self.budget // (self.pop_size * 5) }) #Reduce budget for NM\n                    if result.success:\n                        trial = result.x\n                        \n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                    \n                    # Record successful F and CR values\n                    self.F_history.append(F)\n                    self.CR_history.append(CR)\n                    if len(self.F_history) > 50:\n                        self.F_history.pop(0)\n                        self.CR_history.pop(0)\n\n                    # Add replaced vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                         # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = population[i]\n                        self.archive_fitness[worst_archive_index] = fitness[i]\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.previous_best_fitness = self.f_opt\n                else:\n                     # Add trial vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = trial\n                        self.archive_fitness[worst_archive_index] = f_trial\n\n            population = new_population\n            fitness = new_fitness\n\n            # SOM training (diversity maintenance)\n            self.train_som(population)\n            diversity_metric = self.calculate_diversity_metric(population)\n            \n            if diversity_metric > self.som_stagnation_threshold:\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n\n            # Dynamic restart based on SOM-detected stagnation\n            if self.stagnation_counter > self.som_stagnation_patience:\n                population = self.initialize_population_from_som(func.bounds.lb, func.bounds.ub)\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                \n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                    self.previous_best_fitness = self.f_opt\n                \n                self.F_history = []\n                self.CR_history = []\n                self.stagnation_counter = 0\n\n\n            # Archive aging\n            for k in range(len(self.archive_fitness)):\n                self.archive_fitness[k] *= (1 - self.aging_rate)\n            \n            if self.budget <= 0:\n                break\n        return self.f_opt, self.x_opt\n\n    def train_som(self, population):\n        \"\"\"Trains the Self-Organizing Map (SOM) with the current population.\"\"\"\n        for x in population:\n            # Find the best matching unit (BMU)\n            bmu_index = self.find_bmu(x)\n\n            # Update the SOM weights around the BMU\n            for i in range(self.som_grid_size):\n                for j in range(self.som_grid_size):\n                    distance = np.sqrt((i - bmu_index[0])**2 + (j - bmu_index[1])**2)\n                    influence = np.exp(-distance**2 / (2 * self.som_sigma**2))\n                    self.som_weights[i, j] += self.som_learning_rate * influence * (x - self.som_weights[i, j])\n\n    def find_bmu(self, individual):\n        \"\"\"Finds the best matching unit (BMU) in the SOM for a given individual.\"\"\"\n        min_distance = np.inf\n        bmu_index = None\n        for i in range(self.som_grid_size):\n            for j in range(self.som_grid_size):\n                distance = np.linalg.norm(individual - self.som_weights[i, j])\n                if distance < min_distance:\n                    min_distance = distance\n                    bmu_index = (i, j)\n        return bmu_index\n\n    def calculate_diversity_metric(self, population):\n        \"\"\"Calculates a diversity metric based on the SOM.\"\"\"\n        bmu_indices = [self.find_bmu(x) for x in population]\n        unique_bmu_count = len(set(bmu_indices))\n        return unique_bmu_count / self.pop_size\n\n    def initialize_population_from_som(self, lower_bound, upper_bound):\n        \"\"\"Initializes a new population by sampling from the SOM.\"\"\"\n        new_population = np.zeros((self.pop_size, self.dim))\n        for i in range(self.pop_size):\n            # Randomly select a node from the SOM\n            row = np.random.randint(0, self.som_grid_size)\n            col = np.random.randint(0, self.som_grid_size)\n            \n            # Perturb the SOM node's weights to create a new individual\n            new_individual = self.som_weights[row, col] + np.random.normal(0, 0.1, self.dim)\n            new_individual = np.clip(new_individual, lower_bound, upper_bound)\n            new_population[i] = new_individual\n        return new_population", "configspace": "", "generation": 7, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 106, in __call__\nNameError: name 'minimize' is not defined\n.", "error": "", "parent_ids": ["d0c5bbca-bd80-4233-8ffe-a91bcd92aa6d"], "operator": null, "metadata": {}}
{"id": "dff4a673-eaea-416e-8fcb-aa46394b49d9", "fitness": -Infinity, "name": "AdaptiveDE_SOM", "description": "Adaptive Differential Evolution with a self-organizing map (SOM) for population diversity, adaptive F and CR using weighted historical success and quadratic approximation for local search refinement, orthogonal learning, a combined archive with aging, and a dynamic restart strategy based on SOM-detected stagnation.", "code": "import numpy as np\nfrom minisom import MiniSom\n\nclass AdaptiveDE_SOM:\n    def __init__(self, budget=10000, dim=10, pop_size_min=20, pop_size_max=100, F_base=0.5, CR_base=0.7, archive_size=100, F_range=0.3, CR_range=0.3, orthogonal_learning_rate=0.1, restart_patience=50, archive_prob=0.1, current_to_best_prob=0.2, archive_success_threshold=0.1, aging_factor=0.95, som_grid_size=10, som_sigma=0.3, som_learning_rate=0.5, historical_memory_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_min = pop_size_min\n        self.pop_size_max = pop_size_max\n        self.pop_size = pop_size_max  # Start with max population\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.archive_size = archive_size\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.archive = []\n        self.archive_fitness = []\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.restart_patience = restart_patience\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.F_history = []\n        self.CR_history = []\n        self.archive_prob = archive_prob\n        self.current_to_best_prob = current_to_best_prob\n        self.archive_success_rate = 0.0\n        self.archive_successes = 0\n        self.archive_trials = 0\n        self.archive_success_threshold = archive_success_threshold\n        self.aging_factor = aging_factor\n        self.som_grid_size = som_grid_size\n        self.som_sigma = som_sigma\n        self.som_learning_rate = som_learning_rate\n        self.som = MiniSom(self.som_grid_size, self.som_grid_size, self.dim, sigma=self.som_sigma, learning_rate=self.som_learning_rate)\n        self.historical_memory_size = historical_memory_size\n        self.successful_F = []\n        self.successful_CR = []\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Train SOM initially\n        self.som.train_random(population, 100)\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n            self.stagnation_counter = 0\n            self.previous_best_fitness = self.f_opt\n        else:\n            self.stagnation_counter += 1\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n\n            # Self-adaptive F and CR\n            if self.successful_F:\n                weights = np.arange(len(self.successful_F), 0, -1)\n                weights = weights / np.sum(weights)\n                self.F_base = np.sum(weights * np.array(self.successful_F))\n            if self.successful_CR:\n                weights = np.arange(len(self.successful_CR), 0, -1)\n                weights = weights / np.sum(weights)\n                self.CR_base = np.sum(weights * np.array(self.successful_CR))\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                F = self.F_base + np.random.uniform(-self.F_range, self.F_range)\n                CR = self.CR_base + np.random.uniform(-self.CR_range, self.CR_range)\n                F = np.clip(F, 0.1, 1.0)\n                CR = np.clip(CR, 0.1, 1.0)\n\n                # Mutation: Combining current-to-best with random mutation\n                indices = np.random.choice(self.pop_size, 2, replace=False)\n                x1, x2 = population[indices]\n\n                if np.random.rand() < self.current_to_best_prob:\n                    mutant = population[i] + F * (self.x_opt - population[i]) + F * (x1 - x2)\n                else:\n                    x3 = population[np.random.choice(self.pop_size)]\n                    mutant = x1 + F * (x2 - x3)\n\n                # Incorporate archive\n                use_archive = False\n                if len(self.archive) > 0 and np.random.rand() < self.archive_prob:  # archive_prob chance to use archive\n                    archive_index = np.random.randint(len(self.archive))\n                    mutant = x1 + F * (x2 - self.archive[archive_index])\n                    use_archive = True\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n\n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_learning_rate:\n                    orthogonal_vector = self.x_opt + np.random.normal(0, 0.1, self.dim)  # Perturb best solution\n                    orthogonal_vector = np.clip(orthogonal_vector, func.bounds.lb, func.bounds.ub)\n                    trial = 0.5 * (trial + orthogonal_vector)  # Combine with trial vector\n                    trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Quadratic approximation for local search\n                if np.random.rand() < 0.1:\n                    # Perturb trial solution slightly\n                    trial_perturbed = trial + np.random.normal(0, 0.01, self.dim)\n                    trial_perturbed = np.clip(trial_perturbed, func.bounds.lb, func.bounds.ub)\n                    f_trial_perturbed = func(trial_perturbed)\n                    self.budget -= 1\n                    if f_trial_perturbed < func(trial):\n                         trial = trial_perturbed\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n\n                    # Record successful F and CR values\n                    self.successful_F.append(F)\n                    self.successful_CR.append(CR)\n                    if len(self.successful_F) > self.historical_memory_size:\n                        self.successful_F.pop(0)\n                        self.successful_CR.pop(0)\n\n                    # Add replaced vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                        # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = population[i]\n                        self.archive_fitness[worst_archive_index] = fitness[i]\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.stagnation_counter = 0\n                        self.previous_best_fitness = self.f_opt\n\n                    if use_archive:\n                        self.archive_successes += 1\n\n                else:\n                    # Add trial vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = trial\n                        self.archive_fitness[worst_archive_index] = f_trial\n\n                if use_archive:\n                    self.archive_trials += 1\n\n            population = new_population\n            fitness = new_fitness\n\n            # Restart population if stagnating, using SOM for stagnation detection\n            if self.stagnation_counter > self.restart_patience:\n                # Train SOM with current population\n                self.som.train_random(population, 100)\n\n                # Get quantisation error for each individual\n                quantization_errors = [self.som.quantization_error(x) for x in population]\n                \n                # If mean quantization error is low, consider the population stagnant\n                if np.mean(quantization_errors) < 0.01: # Adjust threshold as needed\n                    # Opposition-based learning for restart\n                    opposition_population = func.bounds.ub + func.bounds.lb - population\n                    opposition_fitness = np.array([func(x) for x in opposition_population])\n                    self.budget -= self.pop_size\n\n                    combined_population = np.concatenate((population, opposition_population))\n                    combined_fitness = np.concatenate((fitness, opposition_fitness))\n\n                    # Select the best individuals from the combined population\n                    indices = np.argsort(combined_fitness)[:self.pop_size]\n                    population = combined_population[indices]\n                    fitness = combined_fitness[indices]\n\n                    best_index = np.argmin(fitness)\n                    if fitness[best_index] < self.f_opt:\n                        self.f_opt = fitness[best_index]\n                        self.x_opt = population[best_index]\n                        self.stagnation_counter = 0\n                        self.previous_best_fitness = self.f_opt\n                    else:\n                        self.stagnation_counter += 1\n\n                    # Reset F and CR history\n                    self.successful_F = []\n                    self.successful_CR = []\n\n            # Adjust archive probability based on success rate\n            if self.archive_trials > 10:\n                self.archive_success_rate = self.archive_successes / self.archive_trials\n                if self.archive_success_rate < self.archive_success_threshold:\n                    self.archive_prob *= 0.9  # Decrease probability if success rate is low\n                else:\n                    self.archive_prob = min(1.0, self.archive_prob * 1.1)  # Increase if success is good\n                self.archive_trials = 0\n                self.archive_successes = 0\n\n            # Dynamic population size adjustment\n            if self.f_opt < self.previous_best_fitness:\n                self.pop_size = min(self.pop_size_max, self.pop_size + 1)  # Increase pop size if improvement\n                self.previous_best_fitness = self.f_opt\n            else:\n                self.pop_size = max(self.pop_size_min, self.pop_size - 1)  # Decrease pop size if no improvement\n\n            # Age the archive\n            for k in range(len(self.archive_fitness)):\n                self.archive_fitness[k] *= self.aging_factor\n\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 111, in evaluate\n    exec(code, safe_globals, local_env)\n  File \"<string>\", line 2, in <module>\nModuleNotFoundError: No module named 'minisom'\n.", "error": "", "parent_ids": ["dbed2c30-7abf-4d4f-bd25-ca72a5c65676"], "operator": null, "metadata": {}}
{"id": "17371efb-58e5-4b0f-b72f-6e92650a973f", "fitness": -Infinity, "name": "AdaptiveDE_SOM", "description": "Adaptive Differential Evolution with self-organizing map (SOM) for population diversity and landscape awareness, an improved adaptive mechanism for F and CR parameters using weighted historical success and fitness-based adaptation, orthogonal learning, and a dynamic restart strategy based on SOM-detected stagnation and landscape exploration.", "code": "import numpy as np\nfrom minisom import MiniSom\n\nclass AdaptiveDE_SOM:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_base=0.5, CR_base=0.7, archive_size=100, F_range=0.3, CR_range=0.3, orthogonal_learning_rate=0.1, restart_patience=50, archive_prob=0.1, current_to_best_prob=0.2, archive_success_threshold=0.1, archive_decay_rate=0.99, som_grid_size=10, som_learning_rate=0.5, som_sigma=0.3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.archive_size = archive_size\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.archive = []\n        self.archive_fitness = []\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.restart_patience = restart_patience\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.F_history = []\n        self.CR_history = []\n        self.archive_prob = archive_prob\n        self.current_to_best_prob = current_to_best_prob\n        self.archive_success_rate = 0.0\n        self.archive_successes = 0\n        self.archive_trials = 0\n        self.archive_success_threshold = archive_success_threshold\n        self.archive_decay_rate = archive_decay_rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.dynamic_bounds_factor = 0.1\n        self.som_grid_size = som_grid_size\n        self.som_learning_rate = som_learning_rate\n        self.som_sigma = som_sigma\n        self.som = MiniSom(self.som_grid_size, self.som_grid_size, self.dim, sigma=self.som_sigma, learning_rate=self.som_learning_rate)\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Train SOM\n        self.som.train_random(population, 100)\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n            self.stagnation_counter = 0\n            self.previous_best_fitness = self.f_opt\n        else:\n            self.stagnation_counter +=1\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n            \n            # Self-adaptive F and CR using weighted historical success and fitness-based adaptation\n            if self.F_history:\n                weights = np.exp(-np.abs(np.array(self.F_history)[:,1] - np.mean(fitness))/np.std(fitness)) #Weighting based on fitness improvement\n                weights = weights / np.sum(weights) #Normalized weights\n                self.F_base = np.average(np.array(self.F_history)[:,0], weights=weights)\n            if self.CR_history:\n                weights = np.exp(-np.abs(np.array(self.CR_history)[:,1] - np.mean(fitness))/np.std(fitness)) #Weighting based on fitness improvement\n                weights = weights / np.sum(weights)  #Normalized weights\n                self.CR_base = np.average(np.array(self.CR_history)[:,0], weights=weights)\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                F = self.F_base + np.random.uniform(-self.F_range, self.F_range)\n                CR = self.CR_base + np.random.uniform(-self.CR_range, self.CR_range)\n                F = np.clip(F, 0.1, 1.0)\n                CR = np.clip(CR, 0.1, 1.0)\n\n                # Mutation: Combining current-to-best with random mutation\n                indices = np.random.choice(self.pop_size, 2, replace=False)\n                x1, x2 = population[indices]\n                \n                if np.random.rand() < self.current_to_best_prob:\n                    mutant = population[i] + F * (self.x_opt - population[i]) + F * (x1 - x2)\n                else:\n                    x3 = population[np.random.choice(self.pop_size)]\n                    mutant = x1 + F * (x2 - x3)\n                \n                # Incorporate archive\n                use_archive = False\n                if len(self.archive) > 0 and np.random.rand() < self.archive_prob:  # archive_prob chance to use archive\n                    archive_index = np.random.randint(len(self.archive))\n                    mutant = x1 + F * (x2 - self.archive[archive_index])\n                    use_archive = True\n\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_learning_rate:\n                    orthogonal_vector = self.x_opt + np.random.normal(0, 0.1, self.dim)  # Perturb best solution\n                    orthogonal_vector = np.clip(orthogonal_vector, self.lb, self.ub)\n                    trial = 0.5 * (trial + orthogonal_vector)  # Combine with trial vector\n                    trial = np.clip(trial, self.lb, self.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                    \n                    # Record successful F and CR values, along with fitness improvement\n                    self.F_history.append([F, fitness[i] - f_trial])\n                    self.CR_history.append([CR, fitness[i] - f_trial])\n                    if len(self.F_history) > 50:\n                        self.F_history.pop(0)\n                        self.CR_history.pop(0)\n\n                    # Add replaced vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                         # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = population[i]\n                        self.archive_fitness[worst_archive_index] = fitness[i]\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.stagnation_counter = 0\n                        self.previous_best_fitness = self.f_opt\n                    \n                    if use_archive:\n                        self.archive_successes += 1\n\n                else:\n                     # Add trial vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = trial\n                        self.archive_fitness[worst_archive_index] = f_trial\n                        \n                if use_archive:\n                    self.archive_trials += 1\n\n            population = new_population\n            fitness = new_fitness\n\n            # Train SOM (re-train every generation)\n            self.som.train_random(population, 10)\n            \n            # Restart population if stagnating\n            if self.stagnation_counter > self.restart_patience:\n                # Detect stagnation using SOM: check if neurons are close\n                neuron_positions = np.array([self.som.winner(x) for x in population])\n                unique_neurons = np.unique(neuron_positions, axis=0)\n\n                if len(unique_neurons) < self.som_grid_size * self.som_grid_size / 4: # If population collapses into less than 1/4 of the SOM neurons\n                    # Diversify based on SOM: sample from different neurons\n                    new_population = []\n                    new_fitness = []\n                    for neuron in unique_neurons:\n                        indices = np.where((neuron_positions[:,0] == neuron[0]) & (neuron_positions[:,1] == neuron[1]))[0]\n                        selected_indices = np.random.choice(indices, min(len(indices), self.pop_size // len(unique_neurons)), replace=False)\n                        new_population.extend(population[selected_indices])\n                        new_fitness.extend(fitness[selected_indices])\n\n                    # Fill the rest with random samples\n                    remaining = self.pop_size - len(new_population)\n                    new_samples = np.random.uniform(self.lb, self.ub, size=(remaining, self.dim))\n                    new_fitness_samples = np.array([func(x) for x in new_samples])\n                    self.budget -= remaining\n\n                    new_population.extend(new_samples)\n                    new_fitness.extend(new_fitness_samples)\n\n                    population = np.array(new_population)\n                    fitness = np.array(new_fitness)\n\n                    best_index = np.argmin(fitness)\n                    if fitness[best_index] < self.f_opt:\n                        self.f_opt = fitness[best_index]\n                        self.x_opt = population[best_index]\n                        self.stagnation_counter = 0\n                        self.previous_best_fitness = self.f_opt\n                    else:\n                        self.stagnation_counter +=1\n\n                    self.F_history = []\n                    self.CR_history = []\n                else:\n                    self.stagnation_counter = 0  # Reset stagnation if SOM indicates diversity\n\n\n            # Adjust archive probability based on success rate\n            if self.archive_trials > 10:\n                self.archive_success_rate = self.archive_successes / self.archive_trials\n                if self.archive_success_rate < self.archive_success_threshold:\n                    self.archive_prob *= 0.9  # Decrease probability if success rate is low\n                else:\n                    self.archive_prob = min(1.0, self.archive_prob * 1.1) # Increase if success is good\n                self.archive_trials = 0\n                self.archive_successes = 0\n\n            self.archive_prob *= self.archive_decay_rate  # Decay archive probability over time\n            self.archive_prob = max(0.01, self.archive_prob) # Ensure archive_prob does not go to zero\n            \n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 111, in evaluate\n    exec(code, safe_globals, local_env)\n  File \"<string>\", line 2, in <module>\nModuleNotFoundError: No module named 'minisom'\n.", "error": "", "parent_ids": ["7ec7f99f-4bff-487e-85f7-dce44838a65b"], "operator": null, "metadata": {}}
{"id": "e11b4b4f-81ac-4679-906d-8332d71c060a", "fitness": 0.0, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with self-adaptive F and CR based on successful and unsuccessful trials, a ring topology for mutation, orthogonal learning, combined archive, dynamic restart with improved population diversification, and dynamic population size adjustment with sigmoid-based control.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size_min=20, pop_size_max=100, F_base=0.5, CR_base=0.7, archive_size=100, F_range=0.3, CR_range=0.3, orthogonal_learning_rate=0.1, restart_patience=50, archive_prob=0.1, current_to_best_prob=0.2, archive_success_threshold=0.1, aging_factor=0.95, F_adapt_rate=0.1, CR_adapt_rate=0.1, pop_size_adapt_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_min = pop_size_min\n        self.pop_size_max = pop_size_max\n        self.pop_size = pop_size_max # Start with max population\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.archive_size = archive_size\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.archive = []\n        self.archive_fitness = []\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.restart_patience = restart_patience\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.successful_F = []\n        self.successful_CR = []\n        self.unsuccessful_F = []\n        self.unsuccessful_CR = []\n        self.archive_prob = archive_prob\n        self.current_to_best_prob = current_to_best_prob\n        self.archive_success_rate = 0.0\n        self.archive_successes = 0\n        self.archive_trials = 0\n        self.archive_success_threshold = archive_success_threshold\n        self.aging_factor = aging_factor\n        self.F_adapt_rate = F_adapt_rate\n        self.CR_adapt_rate = CR_adapt_rate\n        self.pop_size_adapt_rate = pop_size_adapt_rate\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n            self.stagnation_counter = 0\n            self.previous_best_fitness = self.f_opt\n        else:\n            self.stagnation_counter +=1\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n\n            # Self-adaptive F and CR based on success/failure\n            if self.successful_F:\n                F_mean_succ = np.mean(self.successful_F)\n            else:\n                F_mean_succ = self.F_base\n            if self.successful_CR:\n                CR_mean_succ = np.mean(self.successful_CR)\n            else:\n                CR_mean_succ = self.CR_base\n                \n            if self.unsuccessful_F:\n                F_mean_fail = np.mean(self.unsuccessful_F)\n            else:\n                F_mean_fail = self.F_base\n\n            if self.unsuccessful_CR:\n                CR_mean_fail = np.mean(self.unsuccessful_CR)\n            else:\n                CR_mean_fail = self.CR_base\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                F = F_mean_succ + self.F_adapt_rate * (np.random.rand() - 0.5)\n                CR = CR_mean_succ + self.CR_adapt_rate * (np.random.rand() - 0.5)\n\n                F = np.clip(F, 0.1, 1.0)\n                CR = np.clip(CR, 0.1, 1.0)\n\n                # Mutation using ring topology\n                neighbor_left = (i - 1) % self.pop_size\n                neighbor_right = (i + 1) % self.pop_size\n\n                if np.random.rand() < self.current_to_best_prob:\n                    mutant = population[i] + F * (self.x_opt - population[i]) + F * (population[neighbor_left] - population[neighbor_right])\n                else:\n                    mutant = population[neighbor_left] + F * (population[neighbor_right] - population[i])\n                \n                # Incorporate archive\n                use_archive = False\n                if len(self.archive) > 0 and np.random.rand() < self.archive_prob:  # archive_prob chance to use archive\n                    archive_index = np.random.randint(len(self.archive))\n                    mutant = population[neighbor_left] + F * (population[neighbor_right] - self.archive[archive_index])\n                    use_archive = True\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_learning_rate:\n                    orthogonal_vector = self.x_opt + np.random.normal(0, 0.1, self.dim)  # Perturb best solution\n                    orthogonal_vector = np.clip(orthogonal_vector, func.bounds.lb, func.bounds.ub)\n                    trial = 0.5 * (trial + orthogonal_vector)  # Combine with trial vector\n                    trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                    \n                    # Record successful F and CR values\n                    self.successful_F.append(F)\n                    self.successful_CR.append(CR)\n                    if len(self.successful_F) > 50:\n                        self.successful_F.pop(0)\n                        self.successful_CR.pop(0)\n\n                    # Add replaced vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                         # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = population[i]\n                        self.archive_fitness[worst_archive_index] = fitness[i]\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.stagnation_counter = 0\n                        self.previous_best_fitness = self.f_opt\n                    \n                    if use_archive:\n                        self.archive_successes += 1\n\n                else:\n                     # Add trial vector to archive (combined strategy)\n                    self.unsuccessful_F.append(F)\n                    self.unsuccessful_CR.append(CR)\n                    if len(self.unsuccessful_F) > 50:\n                        self.unsuccessful_F.pop(0)\n                        self.unsuccessful_CR.pop(0)\n\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = trial\n                        self.archive_fitness[worst_archive_index] = f_trial\n                        \n                if use_archive:\n                    self.archive_trials += 1\n\n            population = new_population\n            fitness = new_fitness\n            \n            # Restart population if stagnating\n            if self.stagnation_counter > self.restart_patience:\n                # Opposition-based learning for restart with diversification\n                opposition_population = func.bounds.ub + func.bounds.lb - population + np.random.normal(0, 0.1, size=(self.pop_size, self.dim))\n                opposition_population = np.clip(opposition_population, func.bounds.lb, func.bounds.ub)\n                opposition_fitness = np.array([func(x) for x in opposition_population])\n                self.budget -= self.pop_size\n                \n                combined_population = np.concatenate((population, opposition_population))\n                combined_fitness = np.concatenate((fitness, opposition_fitness))\n                \n                # Select the best individuals from the combined population\n                indices = np.argsort(combined_fitness)[:self.pop_size]\n                population = combined_population[indices]\n                fitness = combined_fitness[indices]\n\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                    self.stagnation_counter = 0\n                    self.previous_best_fitness = self.f_opt\n                else:\n                    self.stagnation_counter +=1\n                    \n                # Reset F and CR history\n                self.successful_F = []\n                self.successful_CR = []\n                self.unsuccessful_F = []\n                self.unsuccessful_CR = []\n            \n            # Adjust archive probability based on success rate\n            if self.archive_trials > 10:\n                self.archive_success_rate = self.archive_successes / self.archive_trials\n                if self.archive_success_rate < self.archive_success_threshold:\n                    self.archive_prob *= 0.9  # Decrease probability if success rate is low\n                else:\n                    self.archive_prob = min(1.0, self.archive_prob * 1.1) # Increase if success is good\n                self.archive_trials = 0\n                self.archive_successes = 0\n\n            # Dynamic population size adjustment with sigmoid\n            sigmoid_val = 1 / (1 + np.exp(10 * (self.f_opt - self.previous_best_fitness)))\n            self.pop_size = int(self.pop_size_min + (self.pop_size_max - self.pop_size_min) * sigmoid_val)\n\n            # Age the archive\n            for k in range(len(self.archive_fitness)):\n                self.archive_fitness[k] *= self.aging_factor\n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["dbed2c30-7abf-4d4f-bd25-ca72a5c65676"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "3e569ca2-4384-491b-a648-8a283cd83646", "fitness": -Infinity, "name": "AdaptiveDE_SOM", "description": "Improved Adaptive Differential Evolution with SOM-guided restarts, adaptive F and CR based on successful individuals, and a refined quadratic approximation for enhanced local search.", "code": "import numpy as np\n\nclass AdaptiveDE_SOM:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_base=0.5, CR_base=0.7, archive_size=100, F_range=0.3, CR_range=0.3, orthogonal_learning_rate=0.1, som_grid_size=10, som_learning_rate=0.1, som_sigma=1.0, aging_rate=0.05, quadratic_approx_rate=0.1, restart_strategy=\"som\"):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.archive_size = archive_size\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.archive = []\n        self.archive_fitness = []\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.aging_rate = aging_rate\n        self.F_history = []\n        self.CR_history = []\n\n        # SOM parameters\n        self.som_grid_size = som_grid_size\n        self.som_learning_rate = som_learning_rate\n        self.som_sigma = som_sigma\n        self.som_weights = np.random.rand(som_grid_size, som_grid_size, dim)  # SOM grid\n        self.som_stagnation_threshold = 0.9  # Threshold for stagnation detection\n        self.som_stagnation_patience = 3 #Number of generations to wait before restart\n\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.restart_flag = False\n        self.restart_strategy = restart_strategy  # 'som' or 'random'\n        \n        #Quadratic Approximation Rate\n        self.quadratic_approx_rate = quadratic_approx_rate\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n            self.previous_best_fitness = self.f_opt\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n\n            # Adaptive F and CR using weighted historical success (ONLY from successful individuals)\n            successful_F = []\n            successful_CR = []\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                if self.F_history and self.CR_history:\n                    weights_F = np.linspace(0.1, 1.0, len(self.F_history))\n                    weights_CR = np.linspace(0.1, 1.0, len(self.CR_history))\n\n                    F_weighted_avg = np.average(self.F_history, weights=weights_F)\n                    CR_weighted_avg = np.average(self.CR_history, weights=weights_CR)\n                else:\n                    F_weighted_avg = self.F_base\n                    CR_weighted_avg = self.CR_base\n\n                F = F_weighted_avg + np.random.uniform(-self.F_range, self.F_range)\n                CR = CR_weighted_avg + np.random.uniform(-self.CR_range, self.CR_range)\n                F = np.clip(F, 0.1, 1.0)\n                CR = np.clip(CR, 0.1, 1.0)\n\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = x1 + F * (x2 - x3)\n\n                # Incorporate archive\n                if len(self.archive) > 0 and np.random.rand() < 0.1:  # 10% chance to use archive\n                    archive_index = np.random.randint(len(self.archive))\n                    mutant = x1 + F * (x2 - self.archive[archive_index])\n\n                # Repair mechanism\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_learning_rate:\n                    orthogonal_vector = self.x_opt + np.random.normal(0, 0.1, self.dim)  # Perturb best solution\n                    orthogonal_vector = np.clip(orthogonal_vector, func.bounds.lb, func.bounds.ub)\n                    trial = 0.5 * (trial + orthogonal_vector)  # Combine with trial vector\n                    trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Quadratic Approximation - Enhanced version\n                if np.random.rand() < self.quadratic_approx_rate:\n                    # Select three random individuals with a preference for better fitness\n                    indices_qa = np.random.choice(self.pop_size, 3, replace=False, p=np.exp(-fitness/np.std(fitness))/np.sum(np.exp(-fitness/np.std(fitness))))  #So that better individuals have higher selection probability\n                    x_a, x_b, x_c = population[indices_qa]\n                    f_a, f_b, f_c = fitness[indices_qa]\n\n                    # Perform quadratic approximation in each dimension\n                    for j in range(self.dim):\n                        try:\n                            # Solve for the minimum of the quadratic approximation\n                            denom = (x_a[j] - x_b[j]) * (f_a - f_c) - (x_a[j] - x_c[j]) * (f_a - f_b)\n                            if abs(denom) > 1e-6: # Avoid division by zero\n                                numerator = (x_a[j] - x_b[j])**2 * (f_a - f_c) - (x_a[j] - x_c[j])**2 * (f_a - f_b)\n                                x_min = x_a[j] - 0.5 * numerator / denom\n                                \n                                # Clip to bounds\n                                x_min = np.clip(x_min, func.bounds.lb[j], func.bounds.ub[j])\n                                trial[j] = x_min\n                        except:\n                            pass # In case of numerical instability\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                    \n                    # Record successful F and CR values\n                    successful_F.append(F)\n                    successful_CR.append(CR)\n\n                    # Add replaced vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                         # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = population[i]\n                        self.archive_fitness[worst_archive_index] = fitness[i]\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.previous_best_fitness = self.f_opt\n                else:\n                     # Add trial vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = trial\n                        self.archive_fitness[worst_archive_index] = f_trial\n\n            population = new_population\n            fitness = new_fitness\n            \n            # Update F and CR history with successful values\n            self.F_history.extend(successful_F)\n            self.CR_history.extend(successful_CR)\n            if len(self.F_history) > 50:\n                self.F_history = self.F_history[-50:]\n                self.CR_history = self.CR_history[-50:]\n\n            # SOM training (diversity maintenance)\n            self.train_som(population)\n            diversity_metric = self.calculate_diversity_metric(population)\n            \n            if diversity_metric > self.som_stagnation_threshold:\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n\n            # Dynamic restart based on SOM-detected stagnation\n            if self.stagnation_counter > self.som_stagnation_patience:\n                if self.restart_strategy == \"som\":\n                    population = self.initialize_population_from_som(func.bounds.lb, func.bounds.ub)\n                else:\n                    population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                    \n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                \n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                    self.previous_best_fitness = self.f_opt\n                \n                self.F_history = []\n                self.CR_history = []\n                self.stagnation_counter = 0\n\n\n            # Archive aging\n            for k in range(len(self.archive_fitness)):\n                self.archive_fitness[k] *= (1 - self.aging_rate)\n            \n            if self.budget <= 0:\n                break\n        return self.f_opt, self.x_opt\n\n    def train_som(self, population):\n        \"\"\"Trains the Self-Organizing Map (SOM) with the current population.\"\"\"\n        for x in population:\n            # Find the best matching unit (BMU)\n            bmu_index = self.find_bmu(x)\n\n            # Update the SOM weights around the BMU\n            for i in range(self.som_grid_size):\n                for j in range(self.som_grid_size):\n                    distance = np.sqrt((i - bmu_index[0])**2 + (j - bmu_index[1])**2)\n                    influence = np.exp(-distance**2 / (2 * self.som_sigma**2))\n                    self.som_weights[i, j] += self.som_learning_rate * influence * (x - self.som_weights[i, j])\n\n    def find_bmu(self, individual):\n        \"\"\"Finds the best matching unit (BMU) in the SOM for a given individual.\"\"\"\n        min_distance = np.inf\n        bmu_index = None\n        for i in range(self.som_grid_size):\n            for j in range(self.som_grid_size):\n                distance = np.linalg.norm(individual - self.som_weights[i, j])\n                if distance < min_distance:\n                    min_distance = distance\n                    bmu_index = (i, j)\n        return bmu_index\n\n    def calculate_diversity_metric(self, population):\n        \"\"\"Calculates a diversity metric based on the SOM.\"\"\"\n        bmu_indices = [self.find_bmu(x) for x in population]\n        unique_bmu_count = len(set(bmu_indices))\n        return unique_bmu_count / self.pop_size\n\n    def initialize_population_from_som(self, lower_bound, upper_bound):\n        \"\"\"Initializes a new population by sampling from the SOM.\"\"\"\n        new_population = np.zeros((self.pop_size, self.dim))\n        for i in range(self.pop_size):\n            # Randomly select a node from the SOM\n            row = np.random.randint(0, self.som_grid_size)\n            col = np.random.randint(0, self.som_grid_size)\n            \n            # Perturb the SOM node's weights to create a new individual\n            new_individual = self.som_weights[row, col] + np.random.normal(0, 0.1, self.dim)\n            new_individual = np.clip(new_individual, lower_bound, upper_bound)\n            new_population[i] = new_individual\n        return new_population", "configspace": "", "generation": 7, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 144, in evaluate\n    algorithm(f_new)\n  File \"<string>\", line 110, in __call__\n  File \"mtrand.pyx\", line 954, in numpy.random.mtrand.RandomState.choice\nValueError: probabilities contain NaN\n.", "error": "", "parent_ids": ["d0c5bbca-bd80-4233-8ffe-a91bcd92aa6d"], "operator": null, "metadata": {}}
{"id": "a88736b8-66ef-4a76-bce9-665fcef770d1", "fitness": 0.6875133362623471, "name": "AdaptiveDE", "description": "Enhanced Adaptive Differential Evolution with adaptive mutation strategies based on success/failure, orthogonal learning, dynamic restart incorporating the best historical solutions, and separate archives for successful and unsuccessful steps with probabilistic usage.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_base=0.5, CR_base=0.7, archive_size=50, F_range=0.3, CR_range=0.3, orthogonal_learning_rate=0.1, restart_patience=50, archive_prob=0.1, current_to_best_prob=0.2, gamma_F = 0.1, gamma_CR = 0.1, historical_memory_size = 10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.archive_size = archive_size\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.success_archive = []\n        self.failure_archive = []\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.restart_patience = restart_patience\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.F_success_history = []\n        self.CR_success_history = []\n        self.F_failure_history = []\n        self.CR_failure_history = []\n        self.archive_prob = archive_prob\n        self.current_to_best_prob = current_to_best_prob\n        self.gamma_F = gamma_F\n        self.gamma_CR = gamma_CR\n        self.historical_memory_size = historical_memory_size\n        self.historical_best_fitness = []\n        self.historical_best_solutions = []\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n            self.stagnation_counter = 0\n            self.previous_best_fitness = self.f_opt\n\n            self.historical_best_fitness.append(self.f_opt)\n            self.historical_best_solutions.append(np.copy(self.x_opt))\n            if len(self.historical_best_fitness) > self.historical_memory_size:\n                self.historical_best_fitness.pop(0)\n                self.historical_best_solutions.pop(0)\n\n        else:\n            self.stagnation_counter +=1\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n            \n            # Self-adaptive F and CR\n            if self.F_success_history:\n                self.F_base = (1-self.gamma_F) * self.F_base + self.gamma_F * np.mean(self.F_success_history)\n                self.F_base = np.clip(self.F_base, 0.1, 1.0)\n            if self.CR_success_history:\n                self.CR_base = (1-self.gamma_CR) * self.CR_base + self.gamma_CR * np.mean(self.CR_success_history)\n                self.CR_base = np.clip(self.CR_base, 0.1, 1.0)\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                F = self.F_base + np.random.uniform(-self.F_range, self.F_range)\n                CR = self.CR_base + np.random.uniform(-self.CR_range, self.CR_range)\n                F = np.clip(F, 0.1, 1.0)\n                CR = np.clip(CR, 0.1, 1.0)\n\n                # Mutation strategy selection based on success/failure\n                if len(self.F_success_history) > len(self.F_failure_history):\n                    # Current-to-best mutation with archive\n                    indices = np.random.choice(self.pop_size, 2, replace=False)\n                    x1, x2 = population[indices]\n\n                    if np.random.rand() < self.current_to_best_prob:\n                        mutant = population[i] + F * (self.x_opt - population[i]) + F * (x1 - x2)\n                    else:\n                        x3 = population[np.random.choice(self.pop_size)]\n                        mutant = x1 + F * (x2 - x3)\n                    \n                    # Incorporate archive\n                    if len(self.success_archive) > 0 and np.random.rand() < self.archive_prob:  # archive_prob chance to use archive\n                        archive_index = np.random.randint(len(self.success_archive))\n                        mutant = x1 + F * (x2 - self.success_archive[archive_index])\n\n                else:\n                    # Random mutation with archive\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x1, x2, x3 = population[indices]\n                    mutant = x1 + F * (x2 - x3)\n\n                    # Incorporate failure archive (exploration)\n                    if len(self.failure_archive) > 0 and np.random.rand() < self.archive_prob:\n                        archive_index = np.random.randint(len(self.failure_archive))\n                        mutant = x1 + F * (x2 - self.failure_archive[archive_index])\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_learning_rate:\n                    orthogonal_vector = self.x_opt + np.random.normal(0, 0.1, self.dim)  # Perturb best solution\n                    orthogonal_vector = np.clip(orthogonal_vector, func.bounds.lb, func.bounds.ub)\n                    trial = 0.5 * (trial + orthogonal_vector)  # Combine with trial vector\n                    trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                    \n                    # Record successful F and CR values\n                    self.F_success_history.append(F)\n                    self.CR_success_history.append(CR)\n                    if len(self.F_success_history) > 50:\n                        self.F_success_history.pop(0)\n                        self.CR_success_history.pop(0)\n\n                    # Add replaced vector to success archive\n                    if len(self.success_archive) < self.archive_size:\n                        self.success_archive.append(population[i])\n                    else:\n                        self.success_archive[np.random.randint(self.archive_size)] = population[i]\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.stagnation_counter = 0\n                        self.previous_best_fitness = self.f_opt\n\n                        self.historical_best_fitness.append(self.f_opt)\n                        self.historical_best_solutions.append(np.copy(self.x_opt))\n                        if len(self.historical_best_fitness) > self.historical_memory_size:\n                            self.historical_best_fitness.pop(0)\n                            self.historical_best_solutions.pop(0)\n                else:\n                    self.F_failure_history.append(F)\n                    self.CR_failure_history.append(CR)\n                    if len(self.F_failure_history) > 50:\n                         self.F_failure_history.pop(0)\n                    if len(self.CR_failure_history) > 50:\n                         self.CR_failure_history.pop(0)\n\n                     # Add trial vector to failure archive\n                    if len(self.failure_archive) < self.archive_size:\n                        self.failure_archive.append(trial)\n                    else:\n                        self.failure_archive[np.random.randint(self.archive_size)] = trial\n\n            population = new_population\n            fitness = new_fitness\n            \n            # Restart population if stagnating using historical best solutions\n            if self.stagnation_counter > self.restart_patience:\n                # Incorporate historical best solutions into the population\n                num_historical = min(len(self.historical_best_solutions), self.pop_size // 2)  # Use at most half the population\n                \n                if num_historical > 0:\n                    indices = np.random.choice(len(self.historical_best_solutions), num_historical, replace=False)\n                    population[:num_historical] = np.array(self.historical_best_solutions)[indices]\n                    fitness[:num_historical] = np.array(self.historical_best_fitness)[indices]\n\n                # Generate opposition population for the remaining individuals\n                opposition_population = func.bounds.ub + func.bounds.lb - population[num_historical:]\n                opposition_fitness = np.array([func(x) for x in opposition_population])\n                self.budget -= (self.pop_size - num_historical)\n\n                # Combine original and opposition populations\n                combined_population = np.vstack((population[num_historical:], opposition_population))\n                combined_fitness = np.concatenate((fitness[num_historical:], opposition_fitness))\n\n                # Select the best individuals to form the new population\n                best_indices = np.argsort(combined_fitness)[:(self.pop_size - num_historical)]\n                population[num_historical:] = combined_population[best_indices]\n                fitness[num_historical:] = combined_fitness[best_indices]\n\n\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                    self.stagnation_counter = 0\n                    self.previous_best_fitness = self.f_opt\n\n                    self.historical_best_fitness.append(self.f_opt)\n                    self.historical_best_solutions.append(np.copy(self.x_opt))\n                    if len(self.historical_best_fitness) > self.historical_memory_size:\n                        self.historical_best_fitness.pop(0)\n                        self.historical_best_solutions.pop(0)\n                else:\n                    self.stagnation_counter +=1\n                    \n                # Reset F and CR history\n                self.F_success_history = []\n                self.CR_success_history = []\n                self.F_failure_history = []\n                self.CR_failure_history = []\n                \n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveDE scored 0.688 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c71a85b0-79cc-4ac0-8462-d189b70da42e"], "operator": null, "metadata": {"aucs": [0.3536392658689027, 0.7634631486820165, 0.673798311281194, 0.8834442360903674, 0.7177384197356276, 0.793668092387728, 0.6186683307472978, 0.6657832743638873, 0.7415179361607149, 0.6639957948031999, 0.8787365781562096, 0.9893470421553369, 0.4011070568716667, 0.7274865433957103, 0.9181552403699997, 0.7534978470225475, 0.54975365474463, 0.8216963779741073, 0.32724490482558255, 0.507524669610214]}}
{"id": "f897025e-58cd-41d0-9b7d-5fa4cf67bc58", "fitness": 0.7497264272160729, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with a success-history based adaptive F and CR, combined archives with aging and adaptive size, orthogonal learning, dynamic restart with shrinking bounds, and a modified current-to-pbest mutation strategy.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=50, orthogonal_learning_rate=0.1, restart_patience=50, archive_prob=0.1, pbest_proportion=0.1, gamma_F=0.1, gamma_CR=0.1, F_init=0.5, CR_init=0.7, archive_decay_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.restart_patience = restart_patience\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.F_success_history = []\n        self.CR_success_history = []\n        self.archive_prob = archive_prob\n        self.pbest_proportion = pbest_proportion\n        self.gamma_F = gamma_F\n        self.gamma_CR = gamma_CR\n        self.F_base = F_init\n        self.CR_base = CR_init\n        self.success_archive = []\n        self.failure_archive = []\n        self.archive_decay_rate = archive_decay_rate\n        self.bounds_lb = None\n        self.bounds_ub = None\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        \n        # Initialize population\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n            self.stagnation_counter = 0\n            self.previous_best_fitness = self.f_opt\n        else:\n            self.stagnation_counter +=1\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n            \n            # Self-adaptive F and CR\n            if self.F_success_history:\n                self.F_base = (1 - self.gamma_F) * self.F_base + self.gamma_F * np.mean(self.F_success_history)\n                self.F_base = np.clip(self.F_base, 0.1, 1.0)\n            if self.CR_success_history:\n                self.CR_base = (1 - self.gamma_CR) * self.CR_base + self.gamma_CR * np.mean(self.CR_success_history)\n                self.CR_base = np.clip(self.CR_base, 0.1, 1.0)\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR: Sample from log-normal distribution\n                F = np.exp(np.random.normal(np.log(self.F_base), 0.1))\n                CR = np.exp(np.random.normal(np.log(self.CR_base), 0.1))\n                F = np.clip(F, 0.1, 1.0)\n                CR = np.clip(CR, 0.1, 1.0)\n\n                # Mutation: current-to-pbest\n                pbest_count = max(1, int(self.pbest_proportion * self.pop_size))\n                pbest_indices = np.argsort(fitness)[:pbest_count]\n                pbest = population[np.random.choice(pbest_indices)]\n                \n                indices = np.random.choice(self.pop_size, 2, replace=False)\n                x1, x2 = population[indices]\n\n                mutant = population[i] + F * (pbest - population[i]) + F * (x1 - x2)\n                \n                # Incorporate archive\n                if len(self.success_archive) > 0 and np.random.rand() < self.archive_prob:  # archive_prob chance to use archive\n                    archive_index = np.random.randint(len(self.success_archive))\n                    mutant = x1 + F * (x2 - self.success_archive[archive_index])\n\n                mutant = np.clip(mutant, self.bounds_lb, self.bounds_ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_learning_rate:\n                    orthogonal_vector = self.x_opt + np.random.normal(0, 0.1, self.dim)  # Perturb best solution\n                    orthogonal_vector = np.clip(orthogonal_vector, self.bounds_lb, self.bounds_ub)\n                    trial = 0.5 * (trial + orthogonal_vector)  # Combine with trial vector\n                    trial = np.clip(trial, self.bounds_lb, self.bounds_ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                    \n                    # Record successful F and CR values\n                    self.F_success_history.append(F)\n                    self.CR_success_history.append(CR)\n                    if len(self.F_success_history) > 50:\n                        self.F_success_history.pop(0)\n                        self.CR_success_history.pop(0)\n\n                    # Add replaced vector to success archive\n                    if len(self.success_archive) < self.archive_size:\n                        self.success_archive.append(population[i])\n                    else:\n                        self.success_archive[np.random.randint(len(self.success_archive))] = population[i]\n\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.stagnation_counter = 0\n                        self.previous_best_fitness = self.f_opt\n                else:\n                     # Add trial vector to failure archive\n                    if len(self.failure_archive) < self.archive_size:\n                        self.failure_archive.append(trial)\n                    else:\n                        self.failure_archive[np.random.randint(len(self.failure_archive))] = trial\n\n            population = new_population\n            fitness = new_fitness\n\n            # Decay archive to promote diversity and remove potentially bad solutions\n            self.success_archive = [self.success_archive[i] for i in range(len(self.success_archive)) if np.random.rand() < self.archive_decay_rate]\n            self.failure_archive = [self.failure_archive[i] for i in range(len(self.failure_archive)) if np.random.rand() < self.archive_decay_rate]\n            \n            # Restart population if stagnating using opposition-based learning and shrinking bounds\n            if self.stagnation_counter > self.restart_patience:\n                # Generate opposition population within shrunken bounds\n                center = self.x_opt\n                bound_range = (self.bounds_ub - self.bounds_lb) * 0.5  # Shrink by half\n                new_lb = np.maximum(self.bounds_lb, center - bound_range / 2)\n                new_ub = np.minimum(self.bounds_ub, center + bound_range / 2)\n\n                opposition_population = np.random.uniform(new_lb, new_ub, size=(self.pop_size, self.dim))\n                opposition_fitness = np.array([func(x) for x in opposition_population])\n                self.budget -= self.pop_size\n\n                # Combine original and opposition populations\n                combined_population = np.vstack((population, opposition_population))\n                combined_fitness = np.concatenate((fitness, opposition_fitness))\n\n                # Select the best individuals to form the new population\n                best_indices = np.argsort(combined_fitness)[:self.pop_size]\n                population = combined_population[best_indices]\n                fitness = combined_fitness[best_indices]\n\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                    self.stagnation_counter = 0\n                    self.previous_best_fitness = self.f_opt\n                else:\n                    self.stagnation_counter +=1\n                    \n                # Reset F and CR history\n                self.F_success_history = []\n                self.CR_success_history = []\n                \n                # Update bounds\n                self.bounds_lb = new_lb\n                self.bounds_ub = new_ub\n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveDE scored 0.750 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c71a85b0-79cc-4ac0-8462-d189b70da42e"], "operator": null, "metadata": {"aucs": [0.2713732541026952, 0.6873169695666386, 0.7987066238722946, 0.935041191694186, 0.8381035901774854, 0.8629389667690132, 0.6935260682434774, 0.7661651318588651, 0.832440843774964, 0.40838916180959706, 0.9156637156845142, 0.9955334736865352, 0.7254852533307127, 0.8007956467400427, 0.954618437778863, 0.8497242333862232, 0.7132273230134398, 0.9034295321553631, 0.5089186070538201, 0.5331305196227281]}}
{"id": "255b279c-239d-42bc-a173-35a24e931863", "fitness": 0.5639303224726732, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with a modified archive update strategy incorporating fitness difference and dynamic F/CR parameter adaptation using running medians and improved aging.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size_min=20, pop_size_max=100, F_base=0.5, CR_base=0.7, archive_size=100, F_range=0.3, CR_range=0.3, orthogonal_learning_rate=0.1, restart_patience=50, archive_prob=0.1, current_to_best_prob=0.2, archive_success_threshold=0.1, aging_factor=0.95, F_CR_history_length=20):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_min = pop_size_min\n        self.pop_size_max = pop_size_max\n        self.pop_size = pop_size_max # Start with max population\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.archive_size = archive_size\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.archive = []\n        self.archive_fitness = []\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.restart_patience = restart_patience\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.F_history = []\n        self.CR_history = []\n        self.archive_prob = archive_prob\n        self.current_to_best_prob = current_to_best_prob\n        self.archive_success_rate = 0.0\n        self.archive_successes = 0\n        self.archive_trials = 0\n        self.archive_success_threshold = archive_success_threshold\n        self.aging_factor = aging_factor\n        self.F_CR_history_length = F_CR_history_length\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n            self.stagnation_counter = 0\n            self.previous_best_fitness = self.f_opt\n        else:\n            self.stagnation_counter +=1\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n            \n            # Self-adaptive F and CR using running median\n            if self.F_history:\n                self.F_base = np.median(self.F_history)\n            if self.CR_history:\n                self.CR_base = np.median(self.CR_history)\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                F = self.F_base + np.random.uniform(-self.F_range, self.F_range)\n                CR = self.CR_base + np.random.uniform(-self.CR_range, self.CR_range)\n                F = np.clip(F, 0.1, 1.0)\n                CR = np.clip(CR, 0.1, 1.0)\n\n                # Mutation: Combining current-to-best with random mutation\n                indices = np.random.choice(self.pop_size, 2, replace=False)\n                x1, x2 = population[indices]\n                \n                if np.random.rand() < self.current_to_best_prob:\n                    mutant = population[i] + F * (self.x_opt - population[i]) + F * (x1 - x2)\n                else:\n                    x3 = population[np.random.choice(self.pop_size)]\n                    mutant = x1 + F * (x2 - x3)\n                \n                # Incorporate archive\n                use_archive = False\n                if len(self.archive) > 0 and np.random.rand() < self.archive_prob:  # archive_prob chance to use archive\n                    archive_index = np.random.randint(len(self.archive))\n                    mutant = x1 + F * (x2 - self.archive[archive_index])\n                    use_archive = True\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_learning_rate:\n                    orthogonal_vector = self.x_opt + np.random.normal(0, 0.1, self.dim)  # Perturb best solution\n                    orthogonal_vector = np.clip(orthogonal_vector, func.bounds.lb, func.bounds.ub)\n                    trial = 0.5 * (trial + orthogonal_vector)  # Combine with trial vector\n                    trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                    \n                    # Record successful F and CR values\n                    self.F_history.append(F)\n                    self.CR_history.append(CR)\n                    if len(self.F_history) > self.F_CR_history_length:\n                        self.F_history.pop(0)\n                        self.CR_history.pop(0)\n\n                    # Add replaced vector to archive using fitness difference\n                    fitness_diff = abs(fitness[i] - f_trial)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                        self.archive_fitness.append(fitness_diff)  # Store fitness difference\n                    else:\n                         # Replace worst in archive based on fitness difference\n                        worst_archive_index = np.argmin(self.archive_fitness) # Smaller is worse since it's a diff.\n                        if fitness_diff > self.archive_fitness[worst_archive_index]: # replace only if better\n                            self.archive[worst_archive_index] = population[i]\n                            self.archive_fitness[worst_archive_index] = fitness_diff\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.stagnation_counter = 0\n                        self.previous_best_fitness = self.f_opt\n                    \n                    if use_archive:\n                        self.archive_successes += 1\n\n                else:\n                     # Add trial vector to archive (combined strategy) - using fitness difference\n                    fitness_diff = abs(fitness[i] - f_trial)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(fitness_diff)\n                    else:\n                        # Replace worst in archive based on fitness difference\n                        worst_archive_index = np.argmin(self.archive_fitness)\n                        if fitness_diff > self.archive_fitness[worst_archive_index]:\n                            self.archive[worst_archive_index] = trial\n                            self.archive_fitness[worst_archive_index] = fitness_diff\n                        \n                if use_archive:\n                    self.archive_trials += 1\n\n            population = new_population\n            fitness = new_fitness\n            \n            # Restart population if stagnating\n            if self.stagnation_counter > self.restart_patience:\n                # Opposition-based learning for restart\n                opposition_population = func.bounds.ub + func.bounds.lb - population\n                opposition_fitness = np.array([func(x) for x in opposition_population])\n                self.budget -= self.pop_size\n                \n                combined_population = np.concatenate((population, opposition_population))\n                combined_fitness = np.concatenate((fitness, opposition_fitness))\n                \n                # Select the best individuals from the combined population\n                indices = np.argsort(combined_fitness)[:self.pop_size]\n                population = combined_population[indices]\n                fitness = combined_fitness[indices]\n\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                    self.stagnation_counter = 0\n                    self.previous_best_fitness = self.f_opt\n                else:\n                    self.stagnation_counter +=1\n                    \n                # Reset F and CR history\n                self.F_history = []\n                self.CR_history = []\n            \n            # Adjust archive probability based on success rate\n            if self.archive_trials > 10:\n                self.archive_success_rate = self.archive_successes / self.archive_trials\n                if self.archive_success_rate < self.archive_success_threshold:\n                    self.archive_prob *= 0.9  # Decrease probability if success rate is low\n                else:\n                    self.archive_prob = min(1.0, self.archive_prob * 1.1) # Increase if success is good\n                self.archive_trials = 0\n                self.archive_successes = 0\n\n            # Dynamic population size adjustment\n            if self.f_opt < self.previous_best_fitness:\n                self.pop_size = min(self.pop_size_max, self.pop_size + 1)  # Increase pop size if improvement\n                self.previous_best_fitness = self.f_opt\n            else:\n                self.pop_size = max(self.pop_size_min, self.pop_size - 1)  # Decrease pop size if no improvement\n            \n            # Age the archive\n            for k in range(len(self.archive_fitness)):\n                self.archive_fitness[k] *= self.aging_factor\n                \n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveDE scored 0.564 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["dbed2c30-7abf-4d4f-bd25-ca72a5c65676"], "operator": null, "metadata": {"aucs": [0.22071499211848356, 0.4121697835587673, 0.4623245802775239, 0.8183361681939797, 0.5351819385258387, 0.6422216182728253, 0.4980820469952346, 0.539115453954857, 0.602309402172549, 0.3979384284838243, 0.8235502770892881, 0.9990664808009896, 0.32490436612980267, 0.6707871917310448, 0.7313820178332051, 0.611377427908517, 0.468661286585649, 0.7346826473522243, 0.23212646832706307, 0.5536738731417972]}}
{"id": "1053fef0-896c-43c0-bbd7-1bdbe93cd5de", "fitness": 0.6715622563049912, "name": "AdaptiveDE", "description": "Enhanced Adaptive Differential Evolution with a success-rate based adaptation of F and CR parameters, a combined archive with aging for both successful and unsuccessful solutions, orthogonal learning, and a dynamic restart strategy with dynamically adjusted bounds.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_base=0.5, CR_base=0.7, archive_size=50, F_range=0.3, CR_range=0.3, orthogonal_learning_rate=0.1, restart_patience=50, archive_prob=0.1, current_to_best_prob=0.2, gamma_F = 0.1, gamma_CR = 0.1, success_threshold = 0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.archive_size = archive_size\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.success_archive = []\n        self.failure_archive = []\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.restart_patience = restart_patience\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.F_success_history = []\n        self.CR_success_history = []\n        self.archive_prob = archive_prob\n        self.current_to_best_prob = current_to_best_prob\n        self.gamma_F = gamma_F\n        self.gamma_CR = gamma_CR\n        self.success_threshold = success_threshold # Threshold for considering a trial successful\n        self.age_archive_rate = 0.01 # rate at which to apply \"aging\" in archive\n        self.F_CR_update_interval = 10 # update F and CR every this many iterations\n        self.generation = 0\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n            self.stagnation_counter = 0\n            self.previous_best_fitness = self.f_opt\n        else:\n            self.stagnation_counter +=1\n\n        while self.budget > 0:\n            self.generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n            \n            # Self-adaptive F and CR - update at intervals\n            if self.generation % self.F_CR_update_interval == 0:\n                if self.F_success_history:\n                    self.F_base = (1-self.gamma_F) * self.F_base + self.gamma_F * np.mean(self.F_success_history)\n                    self.F_base = np.clip(self.F_base, 0.1, 1.0)\n                if self.CR_success_history:\n                    self.CR_base = (1-self.gamma_CR) * self.CR_base + self.gamma_CR * np.mean(self.CR_success_history)\n                    self.CR_base = np.clip(self.CR_base, 0.1, 1.0)\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                F = self.F_base + np.random.uniform(-self.F_range, self.F_range)\n                CR = self.CR_base + np.random.uniform(-self.CR_range, self.CR_range)\n                F = np.clip(F, 0.1, 1.0)\n                CR = np.clip(CR, 0.1, 1.0)\n\n                # Mutation: Combining current-to-best with random mutation\n                indices = np.random.choice(self.pop_size, 2, replace=False)\n                x1, x2 = population[indices]\n                \n                if np.random.rand() < self.current_to_best_prob:\n                    mutant = population[i] + F * (self.x_opt - population[i]) + F * (x1 - x2)\n                else:\n                    x3 = population[np.random.choice(self.pop_size)]\n                    mutant = x1 + F * (x2 - x3)\n                \n                # Incorporate archive\n                if len(self.success_archive) > 0 and np.random.rand() < self.archive_prob:  # archive_prob chance to use archive\n                    archive_index = np.random.randint(len(self.success_archive))\n                    mutant = x1 + F * (x2 - self.success_archive[archive_index])\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_learning_rate:\n                    orthogonal_vector = self.x_opt + np.random.normal(0, 0.1, self.dim)  # Perturb best solution\n                    orthogonal_vector = np.clip(orthogonal_vector, func.bounds.lb, func.bounds.ub)\n                    trial = 0.5 * (trial + orthogonal_vector)  # Combine with trial vector\n                    trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                # Define success based on threshold relative to current fitness\n                success = f_trial < fitness[i] - self.success_threshold * np.abs(fitness[i])\n                \n                if success:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                    \n                    # Record successful F and CR values\n                    self.F_success_history.append(F)\n                    self.CR_success_history.append(CR)\n                    if len(self.F_success_history) > 50:\n                        self.F_success_history.pop(0)\n                        self.CR_success_history.pop(0)\n\n                    # Add replaced vector to success archive (with aging)\n                    if len(self.success_archive) < self.archive_size:\n                        self.success_archive.append(population[i])\n                    else:\n                        index_to_replace = np.random.randint(self.archive_size)\n                        self.success_archive[index_to_replace] = population[i]\n\n                        # Apply aging (perturb the archive element slightly)\n                        self.success_archive[index_to_replace] += np.random.normal(0, self.age_archive_rate, self.dim)\n                        self.success_archive[index_to_replace] = np.clip(self.success_archive[index_to_replace], func.bounds.lb, func.bounds.ub)\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.stagnation_counter = 0\n                        self.previous_best_fitness = self.f_opt\n                else:\n                    # Add trial vector to failure archive (with aging)\n                    if len(self.failure_archive) < self.archive_size:\n                        self.failure_archive.append(trial)\n                    else:\n                        index_to_replace = np.random.randint(self.archive_size)\n                        self.failure_archive[index_to_replace] = trial\n                        \n                        # Apply aging (perturb the archive element slightly)\n                        self.failure_archive[index_to_replace] += np.random.normal(0, self.age_archive_rate, self.dim)\n                        self.failure_archive[index_to_replace] = np.clip(self.failure_archive[index_to_replace], func.bounds.lb, func.bounds.ub)\n\n            population = new_population\n            fitness = new_fitness\n            \n            # Restart population if stagnating using opposition-based learning\n            if self.stagnation_counter > self.restart_patience:\n                # Generate opposition population\n                opposition_population = func.bounds.ub + func.bounds.lb - population\n                opposition_fitness = np.array([func(x) for x in opposition_population])\n                self.budget -= self.pop_size\n\n                # Combine original and opposition populations\n                combined_population = np.vstack((population, opposition_population))\n                combined_fitness = np.concatenate((fitness, opposition_fitness))\n\n                # Select the best individuals to form the new population\n                best_indices = np.argsort(combined_fitness)[:self.pop_size]\n                population = combined_population[best_indices]\n                fitness = combined_fitness[best_indices]\n\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                    self.stagnation_counter = 0\n                    self.previous_best_fitness = self.f_opt\n                else:\n                    self.stagnation_counter +=1\n                    \n                # Reset F and CR history\n                self.F_success_history = []\n                self.CR_success_history = []\n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveDE scored 0.672 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c71a85b0-79cc-4ac0-8462-d189b70da42e"], "operator": null, "metadata": {"aucs": [0.2795672462448463, 0.6166951907975431, 0.6286316722952459, 0.8948222549988382, 0.7186576755360471, 0.7794261270849816, 0.47357758078608936, 0.5938941468258391, 0.721558647673991, 0.5886823104764796, 0.8919638088445165, 0.9973206629784545, 0.522647369142616, 0.6898078782739349, 0.9209863891296324, 0.7591643840391049, 0.5348420742650388, 0.8316814083894735, 0.42039806470386654, 0.5669202336132876]}}
{"id": "e4c12b15-4983-440a-a42a-7bad7833988c", "fitness": 0.3274022160821042, "name": "AdaptiveDE_SOM", "description": "Adaptive Differential Evolution with SOM-based population initialization and restart, adaptive F and CR, orthogonal learning, a combined archive with aging, quadratic approximation, and a dynamic restart strategy based on fitness stagnation.", "code": "import numpy as np\n\nclass AdaptiveDE_SOM:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_base=0.5, CR_base=0.7, archive_size=100, F_range=0.3, CR_range=0.3, orthogonal_learning_rate=0.1, som_grid_size=10, som_learning_rate=0.1, som_sigma=1.0, aging_rate=0.05, quadratic_approx_rate=0.1, stagnation_patience=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.archive_size = archive_size\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.archive = []\n        self.archive_fitness = []\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.aging_rate = aging_rate\n        self.F_history = []\n        self.CR_history = []\n\n        # SOM parameters\n        self.som_grid_size = som_grid_size\n        self.som_learning_rate = som_learning_rate\n        self.som_sigma = som_sigma\n        self.som_weights = np.random.rand(som_grid_size, som_grid_size, dim)  # SOM grid\n        self.som_stagnation_threshold = 0.9  # Threshold for stagnation detection\n        self.stagnation_patience = stagnation_patience #Number of generations to wait before restart\n\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.restart_flag = False\n        \n        #Quadratic Approximation Rate\n        self.quadratic_approx_rate = quadratic_approx_rate\n\n        #Exploitation control parameter\n        self.exploitation_rate = 0.7 #Probability of performing exploitation\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population using SOM\n        population = self.initialize_population_from_som(func.bounds.lb, func.bounds.ub)\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n            self.previous_best_fitness = self.f_opt\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n\n            # Adaptive F and CR using weighted historical success\n            if self.F_history and self.CR_history:\n                weights_F = np.linspace(0.1, 1.0, len(self.F_history))\n                weights_CR = np.linspace(0.1, 1.0, len(self.CR_history))\n\n                F_weighted_avg = np.average(self.F_history, weights=weights_F)\n                CR_weighted_avg = np.average(self.CR_history, weights=weights_CR)\n            else:\n                F_weighted_avg = self.F_base\n                CR_weighted_avg = self.CR_base\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                F = F_weighted_avg + np.random.uniform(-self.F_range, self.F_range)\n                CR = CR_weighted_avg + np.random.uniform(-self.CR_range, self.CR_range)\n                F = np.clip(F, 0.1, 1.0)\n                CR = np.clip(CR, 0.1, 1.0)\n\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = x1 + F * (x2 - x3)\n\n                # Incorporate archive\n                if len(self.archive) > 0 and np.random.rand() < 0.1:  # 10% chance to use archive\n                    archive_index = np.random.randint(len(self.archive))\n                    mutant = x1 + F * (x2 - self.archive[archive_index])\n\n                # Repair mechanism\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_learning_rate:\n                    orthogonal_vector = self.x_opt + np.random.normal(0, 0.1, self.dim)  # Perturb best solution\n                    orthogonal_vector = np.clip(orthogonal_vector, func.bounds.lb, func.bounds.ub)\n                    trial = 0.5 * (trial + orthogonal_vector)  # Combine with trial vector\n                    trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Quadratic Approximation\n                if np.random.rand() < self.quadratic_approx_rate:\n                    # Select three random individuals\n                    indices_qa = np.random.choice(self.pop_size, 3, replace=False)\n                    x_a, x_b, x_c = population[indices_qa]\n                    f_a, f_b, f_c = fitness[indices_qa]\n\n                    # Perform quadratic approximation in each dimension\n                    for j in range(self.dim):\n                        try:\n                            # Solve for the minimum of the quadratic approximation\n                            denom = (x_a[j] - x_b[j]) * (f_a - f_c) - (x_a[j] - x_c[j]) * (f_a - f_b)\n                            if abs(denom) > 1e-6: # Avoid division by zero\n                                numerator = (x_a[j] - x_b[j])**2 * (f_a - f_c) - (x_a[j] - x_c[j])**2 * (f_a - f_b)\n                                x_min = x_a[j] - 0.5 * numerator / denom\n                                \n                                # Clip to bounds\n                                x_min = np.clip(x_min, func.bounds.lb[j], func.bounds.ub[j])\n                                trial[j] = x_min\n                        except:\n                            pass # In case of numerical instability\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                    \n                    # Record successful F and CR values\n                    self.F_history.append(F)\n                    self.CR_history.append(CR)\n                    if len(self.F_history) > 50:\n                        self.F_history.pop(0)\n                        self.CR_history.pop(0)\n\n                    # Add replaced vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                         # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = population[i]\n                        self.archive_fitness[worst_archive_index] = fitness[i]\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.previous_best_fitness = self.f_opt\n                else:\n                     # Add trial vector to archive (combined strategy)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        # Replace worst in archive\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        self.archive[worst_archive_index] = trial\n                        self.archive_fitness[worst_archive_index] = f_trial\n\n            population = new_population\n            fitness = new_fitness\n\n            # SOM training (diversity maintenance)\n            self.train_som(population)\n            \n            # Check for fitness stagnation\n            if abs(self.f_opt - self.previous_best_fitness) < 1e-6:\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n                self.previous_best_fitness = self.f_opt\n\n            # Dynamic restart based on fitness stagnation\n            if self.stagnation_counter > self.stagnation_patience:\n                population = self.initialize_population_from_som(func.bounds.lb, func.bounds.ub)\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                \n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                    self.previous_best_fitness = self.f_opt\n                \n                self.F_history = []\n                self.CR_history = []\n                self.stagnation_counter = 0\n\n\n            # Archive aging\n            for k in range(len(self.archive_fitness)):\n                self.archive_fitness[k] *= (1 - self.aging_rate)\n            \n            if self.budget <= 0:\n                break\n        return self.f_opt, self.x_opt\n\n    def train_som(self, population):\n        \"\"\"Trains the Self-Organizing Map (SOM) with the current population.\"\"\"\n        for x in population:\n            # Find the best matching unit (BMU)\n            bmu_index = self.find_bmu(x)\n\n            # Update the SOM weights around the BMU\n            for i in range(self.som_grid_size):\n                for j in range(self.som_grid_size):\n                    distance = np.sqrt((i - bmu_index[0])**2 + (j - bmu_index[1])**2)\n                    influence = np.exp(-distance**2 / (2 * self.som_sigma**2))\n                    self.som_weights[i, j] += self.som_learning_rate * influence * (x - self.som_weights[i, j])\n\n    def find_bmu(self, individual):\n        \"\"\"Finds the best matching unit (BMU) in the SOM for a given individual.\"\"\"\n        min_distance = np.inf\n        bmu_index = None\n        for i in range(self.som_grid_size):\n            for j in range(self.som_grid_size):\n                distance = np.linalg.norm(individual - self.som_weights[i, j])\n                if distance < min_distance:\n                    min_distance = distance\n                    bmu_index = (i, j)\n        return bmu_index\n\n    def initialize_population_from_som(self, lower_bound, upper_bound):\n        \"\"\"Initializes a new population by sampling from the SOM.\"\"\"\n        new_population = np.zeros((self.pop_size, self.dim))\n        for i in range(self.pop_size):\n            # Randomly select a node from the SOM\n            row = np.random.randint(0, self.som_grid_size)\n            col = np.random.randint(0, self.som_grid_size)\n            \n            # Perturb the SOM node's weights to create a new individual\n            new_individual = self.som_weights[row, col] + np.random.normal(0, 0.1, self.dim)\n            new_individual = np.clip(new_individual, lower_bound, upper_bound)\n            new_population[i] = new_individual\n        return new_population", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveDE_SOM scored 0.327 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d0c5bbca-bd80-4233-8ffe-a91bcd92aa6d"], "operator": null, "metadata": {"aucs": [0.11871342806673457, 0.1821653698387421, 0.40495871621559754, 0.2046460154917903, 0.27287446905180723, 0.3197287731679145, 0.2607976305311904, 0.3093063660932004, 0.29167841961934793, 0.16682388669442139, 0.27619135355807134, 0.9307846691355011, 0.2561492623266467, 0.3031667201312592, 0.6290419215774933, 0.2912276004444716, 0.30600218719614236, 0.4291546043445582, 0.18912631358382714, 0.4055066145733671]}}
{"id": "1ebc2a04-1928-4648-b01a-f3b07b869b51", "fitness": -Infinity, "name": "AdaptiveDE", "description": "Improved Adaptive Differential Evolution with a new archive selection mechanism favoring better solutions, a more aggressive stagnation detection, and improved restart strategy using Gaussian perturbation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_base=0.5, CR_base=0.7, archive_size=50, F_range=0.3, CR_range=0.3, orthogonal_learning_rate=0.1, restart_patience=50, archive_prob=0.1, current_to_best_prob=0.2, gamma_F = 0.1, gamma_CR = 0.1, success_threshold = 0.2, age_archive_rate = 0.01, F_CR_update_interval = 10, stagnation_fitness_threshold = 1e-6):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.archive_size = archive_size\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.success_archive = []\n        self.failure_archive = []\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.restart_patience = restart_patience\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.F_success_history = []\n        self.CR_success_history = []\n        self.archive_prob = archive_prob\n        self.current_to_best_prob = current_to_best_prob\n        self.gamma_F = gamma_F\n        self.gamma_CR = gamma_CR\n        self.success_threshold = success_threshold\n        self.age_archive_rate = age_archive_rate\n        self.F_CR_update_interval = F_CR_update_interval\n        self.generation = 0\n        self.stagnation_fitness_threshold = stagnation_fitness_threshold # relative change in fitness for stagnation\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n            self.stagnation_counter = 0\n            self.previous_best_fitness = self.f_opt\n        else:\n            self.stagnation_counter +=1\n\n        while self.budget > 0:\n            self.generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n            \n            # Self-adaptive F and CR - update at intervals\n            if self.generation % self.F_CR_update_interval == 0:\n                if self.F_success_history:\n                    self.F_base = (1-self.gamma_F) * self.F_base + self.gamma_F * np.mean(self.F_success_history)\n                    self.F_base = np.clip(self.F_base, 0.1, 1.0)\n                if self.CR_success_history:\n                    self.CR_base = (1-self.gamma_CR) * self.CR_base + self.gamma_CR * np.mean(self.CR_success_history)\n                    self.CR_base = np.clip(self.CR_base, 0.1, 1.0)\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                F = self.F_base + np.random.uniform(-self.F_range, self.F_range)\n                CR = self.CR_base + np.random.uniform(-self.CR_range, self.CR_range)\n                F = np.clip(F, 0.1, 1.0)\n                CR = np.clip(CR, 0.1, 1.0)\n\n                # Mutation: Combining current-to-best with random mutation\n                indices = np.random.choice(self.pop_size, 2, replace=False)\n                x1, x2 = population[indices]\n                \n                if np.random.rand() < self.current_to_best_prob:\n                    mutant = population[i] + F * (self.x_opt - population[i]) + F * (x1 - x2)\n                else:\n                    x3 = population[np.random.choice(self.pop_size)]\n                    mutant = x1 + F * (x2 - x3)\n                \n                # Incorporate archive, biased towards fitter archive members\n                if len(self.success_archive) > 0 and np.random.rand() < self.archive_prob:\n                    archive_fitnesses = np.array([func(x) for x in self.success_archive]) # evaluate archive members\n                    if self.budget - len(self.success_archive) < 0:\n                      archive_fitnesses = archive_fitnesses[:self.budget]\n                      archive_probabilities = np.exp(-archive_fitnesses / self.f_opt) # Boltzman selection\n                      archive_probabilities /= archive_probabilities.sum()\n                    else:\n                      archive_probabilities = np.exp(-archive_fitnesses / self.f_opt) # Boltzman selection\n                      archive_probabilities /= archive_probabilities.sum()\n                      self.budget -= len(self.success_archive)\n\n                    archive_index = np.random.choice(len(self.success_archive), p=archive_probabilities)\n                    mutant = x1 + F * (x2 - self.success_archive[archive_index])\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_learning_rate:\n                    orthogonal_vector = self.x_opt + np.random.normal(0, 0.1, self.dim)  # Perturb best solution\n                    orthogonal_vector = np.clip(orthogonal_vector, func.bounds.lb, func.bounds.ub)\n                    trial = 0.5 * (trial + orthogonal_vector)  # Combine with trial vector\n                    trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                # Define success based on threshold relative to current fitness\n                success = f_trial < fitness[i] - self.success_threshold * np.abs(fitness[i])\n                \n                if success:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                    \n                    # Record successful F and CR values\n                    self.F_success_history.append(F)\n                    self.CR_success_history.append(CR)\n                    if len(self.F_success_history) > 50:\n                        self.F_success_history.pop(0)\n                        self.CR_success_history.pop(0)\n\n                    # Add replaced vector to success archive (with aging)\n                    if len(self.success_archive) < self.archive_size:\n                        self.success_archive.append(population[i])\n                    else:\n                        index_to_replace = np.random.randint(self.archive_size)\n                        self.success_archive[index_to_replace] = population[i]\n\n                        # Apply aging (perturb the archive element slightly)\n                        self.success_archive[index_to_replace] += np.random.normal(0, self.age_archive_rate, self.dim)\n                        self.success_archive[index_to_replace] = np.clip(self.success_archive[index_to_replace], func.bounds.lb, func.bounds.ub)\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.stagnation_counter = 0\n                        self.previous_best_fitness = self.f_opt\n                else:\n                    # Add trial vector to failure archive (with aging)\n                    if len(self.failure_archive) < self.archive_size:\n                        self.failure_archive.append(trial)\n                    else:\n                        index_to_replace = np.random.randint(self.archive_size)\n                        self.failure_archive[index_to_replace] = trial\n                        \n                        # Apply aging (perturb the archive element slightly)\n                        self.failure_archive[index_to_replace] += np.random.normal(0, self.age_archive_rate, self.dim)\n                        self.failure_archive[index_to_replace] = np.clip(self.failure_archive[index_to_replace], func.bounds.lb, func.bounds.ub)\n\n            population = new_population\n            fitness = new_fitness\n            \n            # Restart population if stagnating\n            if self.stagnation_counter > self.restart_patience or np.abs(self.previous_best_fitness - self.f_opt) < self.stagnation_fitness_threshold:\n                # Perturb the current best solution with Gaussian noise\n                new_population = np.random.normal(self.x_opt, 0.1 * (func.bounds.ub - func.bounds.lb), size=(self.pop_size, self.dim))\n                new_population = np.clip(new_population, func.bounds.lb, func.bounds.ub)\n                fitness = np.array([func(x) for x in new_population])\n                self.budget -= self.pop_size\n\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = new_population[best_index]\n                    self.stagnation_counter = 0\n                    self.previous_best_fitness = self.f_opt\n                else:\n                    self.stagnation_counter +=1\n                    \n                # Reset F and CR history\n                self.F_success_history = []\n                self.CR_success_history = []\n\n                # Clear the archives\n                self.success_archive = []\n                self.failure_archive = []\n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 93, in __call__\n  File \"mtrand.pyx\", line 954, in numpy.random.mtrand.RandomState.choice\nValueError: probabilities contain NaN\n.", "error": "", "parent_ids": ["1053fef0-896c-43c0-bbd7-1bdbe93cd5de"], "operator": null, "metadata": {}}
{"id": "7fce917e-79e4-43ab-a1ec-abef9581e986", "fitness": -Infinity, "name": "AdaptiveDE", "description": "Improved Adaptive DE with dynamic population size adjustment based on stagnation, enhanced opposition-based learning, and adaptive archive usage.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=50, orthogonal_learning_rate=0.1, restart_patience=50, archive_prob=0.1, pbest_proportion=0.1, gamma_F=0.1, gamma_CR=0.1, F_init=0.5, CR_init=0.7, archive_decay_rate=0.95, pop_size_adapt_freq=50, pop_size_scale_factor=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.restart_patience = restart_patience\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.F_success_history = []\n        self.CR_success_history = []\n        self.archive_prob = archive_prob\n        self.pbest_proportion = pbest_proportion\n        self.gamma_F = gamma_F\n        self.gamma_CR = gamma_CR\n        self.F_base = F_init\n        self.CR_base = CR_init\n        self.success_archive = []\n        self.failure_archive = []\n        self.archive_decay_rate = archive_decay_rate\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.pop_size_adapt_freq = pop_size_adapt_freq\n        self.pop_size_scale_factor = pop_size_scale_factor\n        self.initial_pop_size = pop_size\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        \n        # Initialize population\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n            self.stagnation_counter = 0\n            self.previous_best_fitness = self.f_opt\n        else:\n            self.stagnation_counter +=1\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n            \n            # Self-adaptive F and CR\n            if self.F_success_history:\n                self.F_base = (1 - self.gamma_F) * self.F_base + self.gamma_F * np.mean(self.F_success_history)\n                self.F_base = np.clip(self.F_base, 0.1, 1.0)\n            if self.CR_success_history:\n                self.CR_base = (1 - self.gamma_CR) * self.CR_base + self.gamma_CR * np.mean(self.CR_success_history)\n                self.CR_base = np.clip(self.CR_base, 0.1, 1.0)\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR: Sample from log-normal distribution\n                F = np.exp(np.random.normal(np.log(self.F_base), 0.1))\n                CR = np.exp(np.random.normal(np.log(self.CR_base), 0.1))\n                F = np.clip(F, 0.1, 1.0)\n                CR = np.clip(CR, 0.1, 1.0)\n\n                # Mutation: current-to-pbest\n                pbest_count = max(1, int(self.pbest_proportion * self.pop_size))\n                pbest_indices = np.argsort(fitness)[:pbest_count]\n                pbest = population[np.random.choice(pbest_indices)]\n                \n                indices = np.random.choice(self.pop_size, 2, replace=False)\n                x1, x2 = population[indices]\n\n                mutant = population[i] + F * (pbest - population[i]) + F * (x1 - x2)\n                \n                # Incorporate archive\n                if len(self.success_archive) > 0 and np.random.rand() < self.archive_prob:  # archive_prob chance to use archive\n                    archive_index = np.random.randint(len(self.success_archive))\n                    mutant = x1 + F * (x2 - self.success_archive[archive_index])\n\n                mutant = np.clip(mutant, self.bounds_lb, self.bounds_ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_learning_rate:\n                    orthogonal_vector = self.x_opt + np.random.normal(0, 0.1, self.dim)  # Perturb best solution\n                    orthogonal_vector = np.clip(orthogonal_vector, self.bounds_lb, self.bounds_ub)\n                    trial = 0.5 * (trial + orthogonal_vector)  # Combine with trial vector\n                    trial = np.clip(trial, self.bounds_lb, self.bounds_ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                    \n                    # Record successful F and CR values\n                    self.F_success_history.append(F)\n                    self.CR_success_history.append(CR)\n                    if len(self.F_success_history) > 50:\n                        self.F_success_history.pop(0)\n                        self.CR_success_history.pop(0)\n\n                    # Add replaced vector to success archive with fitness\n                    if len(self.success_archive) < self.archive_size:\n                        self.success_archive.append((population[i], fitness[i]))\n                    else:\n                        # Replace the worst solution in the archive\n                        worst_archive_index = np.argmax([item[1] for item in self.success_archive])  # Find worst fitness\n                        self.success_archive[worst_archive_index] = (population[i], fitness[i])\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.stagnation_counter = 0\n                        self.previous_best_fitness = self.f_opt\n                else:\n                    # Add trial vector to failure archive with fitness\n                    if len(self.failure_archive) < self.archive_size:\n                        self.failure_archive.append((trial, f_trial))\n                    else:\n                         # Replace the worst solution in the failure archive\n                        worst_archive_index = np.argmax([item[1] for item in self.failure_archive])  # Find worst fitness\n                        self.failure_archive[worst_archive_index] = (trial, f_trial)\n\n            population = new_population\n            fitness = new_fitness\n\n            # Decay archive to promote diversity and remove potentially bad solutions\n            self.success_archive = [item for item in self.success_archive if np.random.rand() < self.archive_decay_rate]\n            self.failure_archive = [item for item in self.failure_archive if np.random.rand() < self.archive_decay_rate]\n            \n            # Restart population if stagnating using enhanced opposition-based learning and shrinking bounds\n            if self.stagnation_counter > self.restart_patience:\n                # Generate opposition population within shrunken bounds\n                center = self.x_opt\n                bound_range = (self.bounds_ub - self.bounds_lb) * 0.5  # Shrink by half\n                new_lb = np.maximum(self.bounds_lb, center - bound_range / 2)\n                new_ub = np.minimum(self.bounds_ub, center + bound_range / 2)\n\n                # Enhanced Opposition-Based Learning: Generate opposition points and select the better one\n                opposition_population = np.random.uniform(new_lb, new_ub, size=(self.pop_size, self.dim))\n                \n                # Evaluate and select the best between original and opposition\n                for k in range(self.pop_size):\n                    opposition_x = np.random.uniform(new_lb, new_ub, self.dim)\n                    opposition_fitness = func(opposition_x)\n                    self.budget -= 1\n\n                    if opposition_fitness < fitness[k]:\n                        population[k] = opposition_x\n                        fitness[k] = opposition_fitness\n\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                    self.stagnation_counter = 0\n                    self.previous_best_fitness = self.f_opt\n                else:\n                    self.stagnation_counter +=1\n                    \n                # Reset F and CR history\n                self.F_success_history = []\n                self.CR_success_history = []\n                \n                # Update bounds\n                self.bounds_lb = new_lb\n                self.bounds_ub = new_ub\n            \n            # Adjust population size dynamically based on stagnation\n            if generation % self.pop_size_adapt_freq == 0:\n                if self.stagnation_counter > self.restart_patience / 2:\n                    # Increase population size if stagnating to explore more\n                    self.pop_size = min(int(self.pop_size * (1 + self.pop_size_scale_factor)), self.initial_pop_size * 2)\n                else:\n                    # Decrease population size if not stagnating to exploit more\n                    self.pop_size = max(int(self.pop_size * (1 - self.pop_size_scale_factor)), self.initial_pop_size // 2)\n\n                # Ensure population size is within reasonable bounds (at least 10)\n                self.pop_size = max(self.pop_size, 10)\n                \n                # Regenerate the population with the new size\n                population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                \n                # Update optimal solution\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                    self.stagnation_counter = 0\n                    self.previous_best_fitness = self.f_opt\n                else:\n                    self.stagnation_counter +=1\n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 85, in __call__\nValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.\n.", "error": "", "parent_ids": ["f897025e-58cd-41d0-9b7d-5fa4cf67bc58"], "operator": null, "metadata": {}}
{"id": "8752034f-a884-46b2-ba56-7a4a4c1db18e", "fitness": 0.0, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with momentum-based F/CR adaptation, improved archive handling with fitness-based replacement, and a Cauchy mutation operator for increased exploration.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=50, orthogonal_learning_rate=0.1, restart_patience=50, archive_prob=0.1, pbest_proportion=0.1, gamma_F=0.1, gamma_CR=0.1, F_init=0.5, CR_init=0.7, archive_decay_rate=0.95, cauchy_mutation_prob=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.restart_patience = restart_patience\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.F_success_history = []\n        self.CR_success_history = []\n        self.archive_prob = archive_prob\n        self.pbest_proportion = pbest_proportion\n        self.gamma_F = gamma_F\n        self.gamma_CR = gamma_CR\n        self.F_base = F_init\n        self.CR_base = CR_init\n        self.success_archive = []\n        self.failure_archive = []\n        self.archive_decay_rate = archive_decay_rate\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.momentum_F = 0.0\n        self.momentum_CR = 0.0\n        self.cauchy_mutation_prob = cauchy_mutation_prob\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        \n        # Initialize population\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n            self.stagnation_counter = 0\n            self.previous_best_fitness = self.f_opt\n        else:\n            self.stagnation_counter +=1\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n            \n            # Self-adaptive F and CR with Momentum\n            if self.F_success_history:\n                mean_F = np.mean(self.F_success_history)\n                self.momentum_F = (0.9 * self.momentum_F) + (0.1 * mean_F)\n                self.F_base = (1 - self.gamma_F) * self.F_base + self.gamma_F * self.momentum_F\n                self.F_base = np.clip(self.F_base, 0.1, 1.0)\n\n            if self.CR_success_history:\n                mean_CR = np.mean(self.CR_success_history)\n                self.momentum_CR = (0.9 * self.momentum_CR) + (0.1 * mean_CR)\n                self.CR_base = (1 - self.gamma_CR) * self.CR_base + self.gamma_CR * self.momentum_CR\n                self.CR_base = np.clip(self.CR_base, 0.1, 1.0)\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR: Sample from log-normal distribution\n                F = np.exp(np.random.normal(np.log(self.F_base), 0.1))\n                CR = np.exp(np.random.normal(np.log(self.CR_base), 0.1))\n                F = np.clip(F, 0.1, 1.0)\n                CR = np.clip(CR, 0.1, 1.0)\n\n                # Mutation: current-to-pbest\n                pbest_count = max(1, int(self.pbest_proportion * self.pop_size))\n                pbest_indices = np.argsort(fitness)[:pbest_count]\n                pbest = population[np.random.choice(pbest_indices)]\n                \n                indices = np.random.choice(self.pop_size, 2, replace=False)\n                x1, x2 = population[indices]\n\n                mutant = population[i] + F * (pbest - population[i]) + F * (x1 - x2)\n                \n                # Incorporate archive\n                if len(self.success_archive) > 0 and np.random.rand() < self.archive_prob:  # archive_prob chance to use archive\n                    archive_index = np.random.randint(len(self.success_archive))\n                    mutant = x1 + F * (x2 - self.success_archive[archive_index])\n                    \n                # Cauchy Mutation\n                if np.random.rand() < self.cauchy_mutation_prob:\n                    mutant += 0.01 * np.random.standard_cauchy(size=self.dim)  # Scale Cauchy distribution\n\n                mutant = np.clip(mutant, self.bounds_lb, self.bounds_ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_learning_rate:\n                    orthogonal_vector = self.x_opt + np.random.normal(0, 0.1, self.dim)  # Perturb best solution\n                    orthogonal_vector = np.clip(orthogonal_vector, self.bounds_lb, self.bounds_ub)\n                    trial = 0.5 * (trial + orthogonal_vector)  # Combine with trial vector\n                    trial = np.clip(trial, self.bounds_lb, self.bounds_ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                    \n                    # Record successful F and CR values\n                    self.F_success_history.append(F)\n                    self.CR_success_history.append(CR)\n                    if len(self.F_success_history) > 50:\n                        self.F_success_history.pop(0)\n                        self.CR_success_history.pop(0)\n\n                    # Add replaced vector to success archive, replacing worst solution\n                    if len(self.success_archive) < self.archive_size:\n                        self.success_archive.append(population[i])\n                    else:\n                        fitness_archive = [func(x) for x in self.success_archive]\n                        worst_index = np.argmax(fitness_archive)\n                        self.success_archive[worst_index] = population[i]\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.stagnation_counter = 0\n                        self.previous_best_fitness = self.f_opt\n                else:\n                     # Add trial vector to failure archive\n                    if len(self.failure_archive) < self.archive_size:\n                        self.failure_archive.append(trial)\n                    else:\n                        fitness_archive = [func(x) for x in self.failure_archive]\n                        worst_index = np.argmax(fitness_archive)\n                        self.failure_archive[worst_index] = trial\n\n            population = new_population\n            fitness = new_fitness\n\n            # Decay archive to promote diversity and remove potentially bad solutions\n            self.success_archive = [self.success_archive[i] for i in range(len(self.success_archive)) if np.random.rand() < self.archive_decay_rate]\n            self.failure_archive = [self.failure_archive[i] for i in range(len(self.failure_archive)) if np.random.rand() < self.archive_decay_rate]\n            \n            # Restart population if stagnating using opposition-based learning and shrinking bounds\n            if self.stagnation_counter > self.restart_patience:\n                # Generate opposition population within shrunken bounds\n                center = self.x_opt\n                bound_range = (self.bounds_ub - self.bounds_lb) * 0.5  # Shrink by half\n                new_lb = np.maximum(self.bounds_lb, center - bound_range / 2)\n                new_ub = np.minimum(self.bounds_ub, center + bound_range / 2)\n\n                opposition_population = np.random.uniform(new_lb, new_ub, size=(self.pop_size, self.dim))\n                opposition_fitness = np.array([func(x) for x in opposition_population])\n                self.budget -= self.pop_size\n\n                # Combine original and opposition populations\n                combined_population = np.vstack((population, opposition_population))\n                combined_fitness = np.concatenate((fitness, opposition_fitness))\n\n                # Select the best individuals to form the new population\n                best_indices = np.argsort(combined_fitness)[:self.pop_size]\n                population = combined_population[best_indices]\n                fitness = combined_fitness[best_indices]\n\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                    self.stagnation_counter = 0\n                    self.previous_best_fitness = self.f_opt\n                else:\n                    self.stagnation_counter +=1\n                    \n                # Reset F and CR history\n                self.F_success_history = []\n                self.CR_success_history = []\n                self.momentum_F = 0.0\n                self.momentum_CR = 0.0\n                \n                # Update bounds\n                self.bounds_lb = new_lb\n                self.bounds_ub = new_ub\n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f897025e-58cd-41d0-9b7d-5fa4cf67bc58"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "79f6d38a-27f6-48d8-894d-128d9e6d74ee", "fitness": -Infinity, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with orthogonal learning, dynamic F/CR adaptation, historical memory-based restarts, and a novel region-based mutation that adaptively selects search regions.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_base=0.5, CR_base=0.7, archive_size=50, F_range=0.3, CR_range=0.3, orthogonal_learning_rate=0.1, restart_patience=50, archive_prob=0.1, current_to_best_prob=0.2, gamma_F = 0.1, gamma_CR = 0.1, historical_memory_size = 10, region_selection_prob = 0.3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.archive_size = archive_size\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.success_archive = []\n        self.failure_archive = []\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.restart_patience = restart_patience\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.F_success_history = []\n        self.CR_success_history = []\n        self.F_failure_history = []\n        self.CR_failure_history = []\n        self.archive_prob = archive_prob\n        self.current_to_best_prob = current_to_best_prob\n        self.gamma_F = gamma_F\n        self.gamma_CR = gamma_CR\n        self.historical_memory_size = historical_memory_size\n        self.historical_best_fitness = []\n        self.historical_best_solutions = []\n        self.region_selection_prob = region_selection_prob\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n            self.stagnation_counter = 0\n            self.previous_best_fitness = self.f_opt\n\n            self.historical_best_fitness.append(self.f_opt)\n            self.historical_best_solutions.append(np.copy(self.x_opt))\n            if len(self.historical_best_fitness) > self.historical_memory_size:\n                self.historical_best_fitness.pop(0)\n                self.historical_best_solutions.pop(0)\n\n        else:\n            self.stagnation_counter +=1\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n            \n            # Self-adaptive F and CR\n            if self.F_success_history:\n                self.F_base = (1-self.gamma_F) * self.F_base + self.gamma_F * np.mean(self.F_success_history)\n                self.F_base = np.clip(self.F_base, 0.1, 1.0)\n            if self.CR_success_history:\n                self.CR_base = (1-self.gamma_CR) * self.CR_base + self.gamma_CR * np.mean(self.CR_success_history)\n                self.CR_base = np.clip(self.CR_base, 0.1, 1.0)\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                F = self.F_base + np.random.uniform(-self.F_range, self.F_range)\n                CR = self.CR_base + np.random.uniform(-self.CR_range, self.CR_range)\n                F = np.clip(F, 0.1, 1.0)\n                CR = np.clip(CR, 0.1, 1.0)\n\n                # Region-based mutation\n                if np.random.rand() < self.region_selection_prob:\n                    # Select a random dimension to focus on\n                    selected_dim = np.random.randint(self.dim)\n                    \n                    # Find individuals with high and low values in the selected dimension\n                    high_indices = np.argsort(population[:, selected_dim])[-min(5, self.pop_size):]\n                    low_indices = np.argsort(population[:, selected_dim])[:min(5, self.pop_size)]\n                    \n                    # Choose two individuals from these regions\n                    x_high = population[np.random.choice(high_indices)]\n                    x_low = population[np.random.choice(low_indices)]\n                    \n                    # Mutate using the selected individuals\n                    mutant = population[i] + F * (x_high - x_low)\n\n                # Mutation strategy selection based on success/failure (fallback if region mutation is not selected)\n                else:\n                    if len(self.F_success_history) > len(self.F_failure_history):\n                        # Current-to-best mutation with archive\n                        indices = np.random.choice(self.pop_size, 2, replace=False)\n                        x1, x2 = population[indices]\n\n                        if np.random.rand() < self.current_to_best_prob:\n                            mutant = population[i] + F * (self.x_opt - population[i]) + F * (x1 - x2)\n                        else:\n                            x3 = population[np.random.choice(self.pop_size)]\n                            mutant = x1 + F * (x2 - x3)\n                        \n                        # Incorporate archive\n                        if len(self.success_archive) > 0 and np.random.rand() < self.archive_prob:  # archive_prob chance to use archive\n                            archive_index = np.random.randint(len(self.success_archive))\n                            mutant = x1 + F * (x2 - self.success_archive[archive_index])\n\n                    else:\n                        # Random mutation with archive\n                        indices = np.random.choice(self.pop_size, 3, replace=False)\n                        x1, x2, x3 = population[indices]\n                        mutant = x1 + F * (x2 - x3)\n\n                        # Incorporate failure archive (exploration)\n                        if len(self.failure_archive) > 0 and np.random.rand() < self.archive_prob:\n                            archive_index = np.random.randint(len(self.failure_archive))\n                            mutant = x1 + F * (x2 - self.failure_archive[archive_index])\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_learning_rate:\n                    orthogonal_vector = self.x_opt + np.random.normal(0, 0.1, self.dim)  # Perturb best solution\n                    orthogonal_vector = np.clip(orthogonal_vector, func.bounds.lb, func.bounds.ub)\n                    trial = 0.5 * (trial + orthogonal_vector)  # Combine with trial vector\n                    trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                    \n                    # Record successful F and CR values\n                    self.F_success_history.append(F)\n                    self.CR_success_history.append(CR)\n                    if len(self.F_success_history) > 50:\n                        self.F_success_history.pop(0)\n                        self.CR_success_history.pop(0)\n                        self.CR_success_history.pop(0)\n\n                    # Add replaced vector to success archive\n                    if len(self.success_archive) < self.archive_size:\n                        self.success_archive.append(population[i])\n                    else:\n                        self.success_archive[np.random.randint(self.archive_size)] = population[i]\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.stagnation_counter = 0\n                        self.previous_best_fitness = self.f_opt\n\n                        self.historical_best_fitness.append(self.f_opt)\n                        self.historical_best_solutions.append(np.copy(self.x_opt))\n                        if len(self.historical_best_fitness) > self.historical_memory_size:\n                            self.historical_best_fitness.pop(0)\n                            self.historical_best_solutions.pop(0)\n                else:\n                    self.F_failure_history.append(F)\n                    self.CR_failure_history.append(CR)\n                    if len(self.F_failure_history) > 50:\n                         self.F_failure_history.pop(0)\n                    if len(self.CR_failure_history) > 50:\n                         self.CR_failure_history.pop(0)\n\n                     # Add trial vector to failure archive\n                    if len(self.failure_archive) < self.archive_size:\n                        self.failure_archive.append(trial)\n                    else:\n                        self.failure_archive[np.random.randint(self.archive_size)] = trial\n\n            population = new_population\n            fitness = new_fitness\n            \n            # Restart population if stagnating using historical best solutions\n            if self.stagnation_counter > self.restart_patience:\n                # Incorporate historical best solutions into the population\n                num_historical = min(len(self.historical_best_solutions), self.pop_size // 2)  # Use at most half the population\n                \n                if num_historical > 0:\n                    indices = np.random.choice(len(self.historical_best_solutions), num_historical, replace=False)\n                    population[:num_historical] = np.array(self.historical_best_solutions)[indices]\n                    fitness[:num_historical] = np.array(self.historical_best_fitness)[indices]\n\n                # Generate opposition population for the remaining individuals\n                opposition_population = func.bounds.ub + func.bounds.lb - population[num_historical:]\n                opposition_fitness = np.array([func(x) for x in opposition_population])\n                self.budget -= (self.pop_size - num_historical)\n\n                # Combine original and opposition populations\n                combined_population = np.vstack((population[num_historical:], opposition_population))\n                combined_fitness = np.concatenate((fitness[num_historical:], opposition_fitness))\n\n                # Select the best individuals to form the new population\n                best_indices = np.argsort(combined_fitness)[:(self.pop_size - num_historical)]\n                population[num_historical:] = combined_population[best_indices]\n                fitness[num_historical:] = combined_fitness[best_indices]\n\n\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                    self.stagnation_counter = 0\n                    self.previous_best_fitness = self.f_opt\n\n                    self.historical_best_fitness.append(self.f_opt)\n                    self.historical_best_solutions.append(np.copy(self.x_opt))\n                    if len(self.historical_best_fitness) > self.historical_memory_size:\n                        self.historical_best_fitness.pop(0)\n                        self.historical_best_solutions.pop(0)\n                else:\n                    self.stagnation_counter +=1\n                    \n                # Reset F and CR history\n                self.F_success_history = []\n                self.CR_success_history = []\n                self.F_failure_history = []\n                self.CR_failure_history = []\n                \n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 144, in evaluate\n    algorithm(f_new)\n  File \"<string>\", line 153, in __call__\nIndexError: pop from empty list\n.", "error": "", "parent_ids": ["a88736b8-66ef-4a76-bce9-665fcef770d1"], "operator": null, "metadata": {}}
{"id": "4d4c1edb-bc6c-4b00-a5bb-3d822e7b8a83", "fitness": -Infinity, "name": "AdaptiveDE", "description": "Enhanced Adaptive Differential Evolution with improved stagnation detection, adaptive population size, and a more robust archive update strategy using a fitness-based replacement.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size_min=20, pop_size_max=100, F_base=0.5, CR_base=0.7, archive_size=50, F_range=0.3, CR_range=0.3, orthogonal_learning_rate=0.1, restart_patience=50, archive_prob=0.1, current_to_best_prob=0.2, gamma_F = 0.1, gamma_CR = 0.1, success_threshold = 0.2, age_archive_rate = 0.01, F_CR_update_interval = 10, stagnation_fitness_threshold = 1e-6):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_min = pop_size_min\n        self.pop_size_max = pop_size_max\n        self.pop_size = pop_size_max  # Initialize with maximum population size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.archive_size = archive_size\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.success_archive = []\n        self.failure_archive = []\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.restart_patience = restart_patience\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.F_success_history = []\n        self.CR_success_history = []\n        self.archive_prob = archive_prob\n        self.current_to_best_prob = current_to_best_prob\n        self.gamma_F = gamma_F\n        self.gamma_CR = gamma_CR\n        self.success_threshold = success_threshold\n        self.age_archive_rate = age_archive_rate\n        self.F_CR_update_interval = F_CR_update_interval\n        self.generation = 0\n        self.stagnation_fitness_threshold = stagnation_fitness_threshold  # Threshold for fitness improvement\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n            self.stagnation_counter = 0\n            self.previous_best_fitness = self.f_opt\n        else:\n            self.stagnation_counter += 1\n\n        while self.budget > 0:\n            self.generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n            \n            # Self-adaptive F and CR - update at intervals\n            if self.generation % self.F_CR_update_interval == 0:\n                if self.F_success_history:\n                    self.F_base = (1-self.gamma_F) * self.F_base + self.gamma_F * np.mean(self.F_success_history)\n                    self.F_base = np.clip(self.F_base, 0.1, 1.0)\n                if self.CR_success_history:\n                    self.CR_base = (1-self.gamma_CR) * self.CR_base + self.gamma_CR * np.mean(self.CR_success_history)\n                    self.CR_base = np.clip(self.CR_base, 0.1, 1.0)\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                F = self.F_base + np.random.uniform(-self.F_range, self.F_range)\n                CR = self.CR_base + np.random.uniform(-self.CR_range, self.CR_range)\n                F = np.clip(F, 0.1, 1.0)\n                CR = np.clip(CR, 0.1, 1.0)\n\n                # Mutation: Combining current-to-best with random mutation\n                indices = np.random.choice(self.pop_size, 2, replace=False)\n                x1, x2 = population[indices]\n                \n                if np.random.rand() < self.current_to_best_prob:\n                    mutant = population[i] + F * (self.x_opt - population[i]) + F * (x1 - x2)\n                else:\n                    x3 = population[np.random.choice(self.pop_size)]\n                    mutant = x1 + F * (x2 - x3)\n                \n                # Incorporate archive\n                if len(self.success_archive) > 0 and np.random.rand() < self.archive_prob:\n                    archive_index = np.random.randint(len(self.success_archive))\n                    mutant = x1 + F * (x2 - self.success_archive[archive_index])\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_learning_rate:\n                    orthogonal_vector = self.x_opt + np.random.normal(0, 0.1, self.dim)\n                    orthogonal_vector = np.clip(orthogonal_vector, func.bounds.lb, func.bounds.ub)\n                    trial = 0.5 * (trial + orthogonal_vector)\n                    trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                # Define success based on threshold relative to current fitness\n                success = f_trial < fitness[i] - self.success_threshold * np.abs(fitness[i])\n                \n                if success:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                    \n                    # Record successful F and CR values\n                    self.F_success_history.append(F)\n                    self.CR_success_history.append(CR)\n                    if len(self.F_success_history) > 50:\n                        self.F_success_history.pop(0)\n                        self.CR_success_history.pop(0)\n                        self.CR_success_history.pop(0)\n\n                    # Archive Update Strategy: Fitness-based replacement\n                    if len(self.success_archive) < self.archive_size:\n                        self.success_archive.append(population[i])  # Add replaced individual\n                    else:\n                        # Replace worst element in archive if new one is better\n                        archive_fitness = [func(x) for x in self.success_archive]\n                        worst_index = np.argmax(archive_fitness)\n                        if f_trial < archive_fitness[worst_index]:\n                            self.success_archive[worst_index] = population[i]  # Replace worst with successful individual\n                            \n                    # Apply aging (perturb the archive element slightly) - do this AFTER archiving the original point\n                    if len(self.success_archive) > 0:\n                         archive_index = np.random.randint(len(self.success_archive))\n                         self.success_archive[archive_index] += np.random.normal(0, self.age_archive_rate, self.dim)\n                         self.success_archive[archive_index] = np.clip(self.success_archive[archive_index], func.bounds.lb, func.bounds.ub)\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.stagnation_counter = 0\n                        self.previous_best_fitness = self.f_opt\n                else:\n                    # Archive Update Strategy: Fitness-based replacement (failure archive)\n                    if len(self.failure_archive) < self.archive_size:\n                        self.failure_archive.append(trial)  # Add the failed trial\n                    else:\n                        # Replace worst element in archive if new one is worse\n                        archive_fitness = [func(x) for x in self.failure_archive]\n                        best_index = np.argmin(archive_fitness)\n                        if f_trial > archive_fitness[best_index]:\n                            self.failure_archive[best_index] = trial\n                            \n                    # Apply aging (perturb the archive element slightly) - do this AFTER archiving the original point\n                    if len(self.failure_archive) > 0:\n                         archive_index = np.random.randint(len(self.failure_archive))\n                         self.failure_archive[archive_index] += np.random.normal(0, self.age_archive_rate, self.dim)\n                         self.failure_archive[archive_index] = np.clip(self.failure_archive[archive_index], func.bounds.lb, func.bounds.ub)\n\n            population = new_population\n            fitness = new_fitness\n            \n            # Adaptive Population Size Adjustment\n            if self.generation % 20 == 0:  # Adjust every 20 iterations\n                if np.abs(self.f_opt - self.previous_best_fitness) < self.stagnation_fitness_threshold:\n                    self.pop_size = max(self.pop_size_min, int(self.pop_size * 0.9))  # Reduce population size\n                    print(\"Reducing Popsize\")\n                else:\n                    self.pop_size = min(self.pop_size_max, int(self.pop_size * 1.1))  # Increase population size\n                    print(\"Increasing Popsize\")\n\n                self.pop_size = np.clip(self.pop_size, self.pop_size_min, self.pop_size_max)\n                self.previous_best_fitness = self.f_opt\n                \n                # Regenerate Population, keeping the best\n                best_index = np.argmin(fitness)\n                best_individual = population[best_index]\n                \n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                population[0] = best_individual # Ensure we keep the best\n                \n                fitness = np.array([func(x) for x in population])\n                self.budget -= (self.pop_size -1 )\n\n            # Restart population if stagnating using opposition-based learning\n            if self.stagnation_counter > self.restart_patience:\n                # Generate opposition population\n                opposition_population = func.bounds.ub + func.bounds.lb - population\n                opposition_fitness = np.array([func(x) for x in opposition_population])\n                self.budget -= self.pop_size\n\n                # Combine original and opposition populations\n                combined_population = np.vstack((population, opposition_population))\n                combined_fitness = np.concatenate((fitness, opposition_fitness))\n\n                # Select the best individuals to form the new population\n                best_indices = np.argsort(combined_fitness)[:self.pop_size]\n                population = combined_population[best_indices]\n                fitness = combined_fitness[best_indices]\n\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                    self.stagnation_counter = 0\n                    self.previous_best_fitness = self.f_opt\n                else:\n                    self.stagnation_counter +=1\n                    \n                # Reset F and CR history\n                self.F_success_history = []\n                self.CR_success_history = []\n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 144, in evaluate\n    algorithm(f_new)\n  File \"<string>\", line 121, in __call__\nIndexError: pop from empty list\n.", "error": "", "parent_ids": ["1053fef0-896c-43c0-bbd7-1bdbe93cd5de"], "operator": null, "metadata": {}}
{"id": "a6c8747a-9c1a-4ed8-b7de-1acee430e3c5", "fitness": 0.0, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with orthogonal learning and dynamic restart using a local search to enhance exploitation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=50, orthogonal_learning_rate=0.1, restart_patience=50, archive_prob=0.1, pbest_proportion=0.1, gamma_F=0.1, gamma_CR=0.1, F_init=0.5, CR_init=0.7, archive_decay_rate=0.95, local_search_probability=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.restart_patience = restart_patience\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.F_success_history = []\n        self.CR_success_history = []\n        self.archive_prob = archive_prob\n        self.pbest_proportion = pbest_proportion\n        self.gamma_F = gamma_F\n        self.gamma_CR = gamma_CR\n        self.F_base = F_init\n        self.CR_base = CR_init\n        self.success_archive = []\n        self.failure_archive = []\n        self.archive_decay_rate = archive_decay_rate\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.local_search_probability = local_search_probability\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        \n        # Initialize population\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n            self.stagnation_counter = 0\n            self.previous_best_fitness = self.f_opt\n        else:\n            self.stagnation_counter +=1\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n            \n            # Self-adaptive F and CR\n            if self.F_success_history:\n                self.F_base = (1 - self.gamma_F) * self.F_base + self.gamma_F * np.mean(self.F_success_history)\n                self.F_base = np.clip(self.F_base, 0.1, 1.0)\n            if self.CR_success_history:\n                self.CR_base = (1 - self.gamma_CR) * self.CR_base + self.gamma_CR * np.mean(self.CR_success_history)\n                self.CR_base = np.clip(self.CR_base, 0.1, 1.0)\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR: Sample from log-normal distribution\n                F = np.exp(np.random.normal(np.log(self.F_base), 0.1))\n                CR = np.exp(np.random.normal(np.log(self.CR_base), 0.1))\n                F = np.clip(F, 0.1, 1.0)\n                CR = np.clip(CR, 0.1, 1.0)\n\n                # Mutation: current-to-pbest\n                pbest_count = max(1, int(self.pbest_proportion * self.pop_size))\n                pbest_indices = np.argsort(fitness)[:pbest_count]\n                pbest = population[np.random.choice(pbest_indices)]\n                \n                indices = np.random.choice(self.pop_size, 2, replace=False)\n                x1, x2 = population[indices]\n\n                mutant = population[i] + F * (pbest - population[i]) + F * (x1 - x2)\n                \n                # Incorporate archive\n                if len(self.success_archive) > 0 and np.random.rand() < self.archive_prob:  # archive_prob chance to use archive\n                    archive_index = np.random.randint(len(self.success_archive))\n                    mutant = x1 + F * (x2 - self.success_archive[archive_index])\n\n                mutant = np.clip(mutant, self.bounds_lb, self.bounds_ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_learning_rate:\n                    orthogonal_vector = self.x_opt + np.random.normal(0, 0.1, self.dim)  # Perturb best solution\n                    orthogonal_vector = np.clip(orthogonal_vector, self.bounds_lb, self.bounds_ub)\n                    trial = 0.5 * (trial + orthogonal_vector)  # Combine with trial vector\n                    trial = np.clip(trial, self.bounds_lb, self.bounds_ub)\n\n                # Local Search\n                if np.random.rand() < self.local_search_probability:\n                    step_size = 0.01 * (self.bounds_ub - self.bounds_lb)  # Small step size\n                    for j in range(self.dim):\n                        # Try small steps in both directions\n                        trial_plus = np.copy(trial)\n                        trial_minus = np.copy(trial)\n                        trial_plus[j] = np.clip(trial[j] + step_size[j], self.bounds_lb[j], self.bounds_ub[j])\n                        trial_minus[j] = np.clip(trial[j] - step_size[j], self.bounds_lb[j], self.bounds_ub[j])\n\n                        f_plus = func(trial_plus) if self.budget > 0 else np.inf\n                        self.budget -= 1 if self.budget > 0 else 0\n                        f_minus = func(trial_minus) if self.budget > 0 else np.inf\n                        self.budget -= 1 if self.budget > 0 else 0\n                        \n                        if f_plus < func(trial) and f_plus < f_minus:\n                            trial = trial_plus\n                        elif f_minus < func(trial) and f_minus < f_plus:\n                            trial = trial_minus\n\n\n                # Selection\n                f_trial = func(trial) if self.budget > 0 else np.inf\n                self.budget -= 1 if self.budget > 0 else 0\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                    \n                    # Record successful F and CR values\n                    self.F_success_history.append(F)\n                    self.CR_success_history.append(CR)\n                    if len(self.F_success_history) > 50:\n                        self.F_success_history.pop(0)\n                        self.CR_success_history.pop(0)\n\n                    # Add replaced vector to success archive\n                    if len(self.success_archive) < self.archive_size:\n                        self.success_archive.append(population[i])\n                    else:\n                        self.success_archive[np.random.randint(len(self.success_archive))] = population[i]\n\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.stagnation_counter = 0\n                        self.previous_best_fitness = self.f_opt\n                else:\n                     # Add trial vector to failure archive\n                    if len(self.failure_archive) < self.archive_size:\n                        self.failure_archive.append(trial)\n                    else:\n                        self.failure_archive[np.random.randint(len(self.failure_archive))] = trial\n\n            population = new_population\n            fitness = new_fitness\n\n            # Decay archive to promote diversity and remove potentially bad solutions\n            self.success_archive = [self.success_archive[i] for i in range(len(self.success_archive)) if np.random.rand() < self.archive_decay_rate]\n            self.failure_archive = [self.failure_archive[i] for i in range(len(self.failure_archive)) if np.random.rand() < self.archive_decay_rate]\n            \n            # Restart population if stagnating using opposition-based learning and shrinking bounds\n            if self.stagnation_counter > self.restart_patience:\n                # Generate opposition population within shrunken bounds\n                center = self.x_opt\n                bound_range = (self.bounds_ub - self.bounds_lb) * 0.5  # Shrink by half\n                new_lb = np.maximum(self.bounds_lb, center - bound_range / 2)\n                new_ub = np.minimum(self.bounds_ub, center + bound_range / 2)\n\n                opposition_population = np.random.uniform(new_lb, new_ub, size=(self.pop_size, self.dim))\n                opposition_fitness = np.array([func(x) for x in opposition_population]) if self.budget > self.pop_size else np.array([np.inf]*self.pop_size)\n                self.budget -= self.pop_size if self.budget > self.pop_size else self.budget\n\n                # Combine original and opposition populations\n                combined_population = np.vstack((population, opposition_population))\n                combined_fitness = np.concatenate((fitness, opposition_fitness))\n\n                # Select the best individuals to form the new population\n                best_indices = np.argsort(combined_fitness)[:self.pop_size]\n                population = combined_population[best_indices]\n                fitness = combined_fitness[best_indices]\n\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                    self.stagnation_counter = 0\n                    self.previous_best_fitness = self.f_opt\n                else:\n                    self.stagnation_counter +=1\n                    \n                # Reset F and CR history\n                self.F_success_history = []\n                self.CR_success_history = []\n                \n                # Update bounds\n                self.bounds_lb = new_lb\n                self.bounds_ub = new_ub\n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f897025e-58cd-41d0-9b7d-5fa4cf67bc58"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "a466d6ab-5020-4867-b722-d62972e41793", "fitness": 0.0, "name": "AdaptiveDE", "description": "Improved Adaptive Differential Evolution with dynamic population size adjustment, enhanced orthogonal learning, and adaptive archive management.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=50, orthogonal_learning_rate=0.1, restart_patience=50, archive_prob=0.1, pbest_proportion=0.1, gamma_F=0.1, gamma_CR=0.1, F_init=0.5, CR_init=0.7, archive_decay_rate=0.95, pop_size_decay=0.98, orthogonal_learning_intensity=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.original_pop_size = pop_size\n        self.archive_size = archive_size\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.restart_patience = restart_patience\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.F_success_history = []\n        self.CR_success_history = []\n        self.archive_prob = archive_prob\n        self.pbest_proportion = pbest_proportion\n        self.gamma_F = gamma_F\n        self.gamma_CR = gamma_CR\n        self.F_base = F_init\n        self.CR_base = CR_init\n        self.success_archive = []\n        self.failure_archive = []\n        self.archive_decay_rate = archive_decay_rate\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.pop_size_decay = pop_size_decay\n        self.orthogonal_learning_intensity = orthogonal_learning_intensity\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        \n        # Initialize population\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n            self.stagnation_counter = 0\n            self.previous_best_fitness = self.f_opt\n        else:\n            self.stagnation_counter +=1\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n            \n            # Self-adaptive F and CR\n            if self.F_success_history:\n                self.F_base = (1 - self.gamma_F) * self.F_base + self.gamma_F * np.mean(self.F_success_history)\n                self.F_base = np.clip(self.F_base, 0.1, 1.0)\n            if self.CR_success_history:\n                self.CR_base = (1 - self.gamma_CR) * self.CR_base + self.gamma_CR * np.mean(self.CR_success_history)\n                self.CR_base = np.clip(self.CR_base, 0.1, 1.0)\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR: Sample from log-normal distribution\n                F = np.exp(np.random.normal(np.log(self.F_base), 0.1))\n                CR = np.exp(np.random.normal(np.log(self.CR_base), 0.1))\n                F = np.clip(F, 0.1, 1.0)\n                CR = np.clip(CR, 0.1, 1.0)\n\n                # Mutation: current-to-pbest\n                pbest_count = max(1, int(self.pbest_proportion * self.pop_size))\n                pbest_indices = np.argsort(fitness)[:pbest_count]\n                pbest = population[np.random.choice(pbest_indices)]\n                \n                indices = np.random.choice(self.pop_size, 2, replace=False)\n                x1, x2 = population[indices]\n\n                mutant = population[i] + F * (pbest - population[i]) + F * (x1 - x2)\n                \n                # Incorporate archive\n                if len(self.success_archive) > 0 and np.random.rand() < self.archive_prob:  # archive_prob chance to use archive\n                    archive_index = np.random.randint(len(self.success_archive))\n                    mutant = x1 + F * (x2 - self.success_archive[archive_index])\n\n                mutant = np.clip(mutant, self.bounds_lb, self.bounds_ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_learning_rate:\n                    orthogonal_vector = self.x_opt + np.random.normal(0, self.orthogonal_learning_intensity, self.dim)  # Perturb best solution\n                    orthogonal_vector = np.clip(orthogonal_vector, self.bounds_lb, self.bounds_ub)\n                    trial = 0.5 * (trial + orthogonal_vector)  # Combine with trial vector\n                    trial = np.clip(trial, self.bounds_lb, self.bounds_ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                    \n                    # Record successful F and CR values\n                    self.F_success_history.append(F)\n                    self.CR_success_history.append(CR)\n                    if len(self.F_success_history) > 50:\n                        self.F_success_history.pop(0)\n                        self.CR_success_history.pop(0)\n\n                    # Add replaced vector to success archive\n                    if len(self.success_archive) < self.archive_size:\n                        self.success_archive.append(population[i])\n                    else:\n                        self.success_archive[np.random.randint(len(self.success_archive))] = population[i]\n\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.stagnation_counter = 0\n                        self.previous_best_fitness = self.f_opt\n                else:\n                     # Add trial vector to failure archive\n                    if len(self.failure_archive) < self.archive_size:\n                        self.failure_archive.append(trial)\n                    else:\n                        self.failure_archive[np.random.randint(len(self.failure_archive))] = trial\n\n            population = new_population\n            fitness = new_fitness\n\n            # Decay archive to promote diversity and remove potentially bad solutions\n            self.success_archive = [self.success_archive[i] for i in range(len(self.success_archive)) if np.random.rand() < self.archive_decay_rate]\n            self.failure_archive = [self.failure_archive[i] for i in range(len(self.failure_archive)) if np.random.rand() < self.archive_decay_rate]\n            \n            # Restart population if stagnating using opposition-based learning and shrinking bounds\n            if self.stagnation_counter > self.restart_patience:\n                # Generate opposition population within shrunken bounds\n                center = self.x_opt\n                bound_range = (self.bounds_ub - self.bounds_lb) * 0.5  # Shrink by half\n                new_lb = np.maximum(self.bounds_lb, center - bound_range / 2)\n                new_ub = np.minimum(self.bounds_ub, center + bound_range / 2)\n\n                opposition_population = np.random.uniform(new_lb, new_ub, size=(self.pop_size, self.dim))\n                opposition_fitness = np.array([func(x) for x in opposition_population])\n                self.budget -= self.pop_size\n\n                # Combine original and opposition populations\n                combined_population = np.vstack((population, opposition_population))\n                combined_fitness = np.concatenate((fitness, opposition_fitness))\n\n                # Select the best individuals to form the new population\n                best_indices = np.argsort(combined_fitness)[:self.pop_size]\n                population = combined_population[best_indices]\n                fitness = combined_fitness[best_indices]\n\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                    self.stagnation_counter = 0\n                    self.previous_best_fitness = self.f_opt\n                else:\n                    self.stagnation_counter +=1\n                    \n                # Reset F and CR history\n                self.F_success_history = []\n                self.CR_success_history = []\n                \n                # Update bounds\n                self.bounds_lb = new_lb\n                self.bounds_ub = new_ub\n\n            # Dynamic Population Size Adjustment\n            self.pop_size = int(self.original_pop_size * (self.pop_size_decay**(generation/10)))\n            self.pop_size = max(10, self.pop_size) #Ensure a minimum population size\n            if self.pop_size != population.shape[0]:\n                # Resize the population (linear interpolation towards the best)\n                num_to_add = self.pop_size - population.shape[0]\n\n                if num_to_add > 0:\n                     new_individuals = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(num_to_add, self.dim))\n                     population = np.vstack((population, new_individuals))\n                     fitness_new = np.array([func(x) for x in population[self.original_pop_size:]])\n                     self.budget -= num_to_add\n                     fitness = np.concatenate((fitness, fitness_new))\n                    \n                elif num_to_add < 0:\n                    # Remove the worst performing individuals\n                    worst_indices = np.argsort(fitness)[num_to_add:] #num_to_add is negative\n                    population = np.delete(population, worst_indices, axis=0)\n                    fitness = np.delete(fitness, worst_indices)\n                \n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f897025e-58cd-41d0-9b7d-5fa4cf67bc58"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "39c42933-2ef7-40f4-8cad-1969fe1250f7", "fitness": 0.2956934383740024, "name": "AdaptiveDE", "description": "Enhanced Adaptive Differential Evolution with adaptive mutation strategies, orthogonal learning, dynamic restart, separate archives, probabilistic usage, and a novel neighborhood-based mutation operator for finer local search.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_base=0.5, CR_base=0.7, archive_size=50, F_range=0.3, CR_range=0.3, orthogonal_learning_rate=0.1, restart_patience=50, archive_prob=0.1, current_to_best_prob=0.2, gamma_F = 0.1, gamma_CR = 0.1, historical_memory_size = 10, neighborhood_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.archive_size = archive_size\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.success_archive = []\n        self.failure_archive = []\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.restart_patience = restart_patience\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.F_success_history = []\n        self.CR_success_history = []\n        self.F_failure_history = []\n        self.CR_failure_history = []\n        self.archive_prob = archive_prob\n        self.current_to_best_prob = current_to_best_prob\n        self.gamma_F = gamma_F\n        self.gamma_CR = gamma_CR\n        self.historical_memory_size = historical_memory_size\n        self.historical_best_fitness = []\n        self.historical_best_solutions = []\n        self.neighborhood_size = neighborhood_size\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n            self.stagnation_counter = 0\n            self.previous_best_fitness = self.f_opt\n\n            self.historical_best_fitness.append(self.f_opt)\n            self.historical_best_solutions.append(np.copy(self.x_opt))\n            if len(self.historical_best_fitness) > self.historical_memory_size:\n                self.historical_best_fitness.pop(0)\n                self.historical_best_solutions.pop(0)\n\n        else:\n            self.stagnation_counter +=1\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n            \n            # Self-adaptive F and CR\n            if self.F_success_history:\n                self.F_base = (1-self.gamma_F) * self.F_base + self.gamma_F * np.mean(self.F_success_history)\n                self.F_base = np.clip(self.F_base, 0.1, 1.0)\n            if self.CR_success_history:\n                self.CR_base = (1-self.gamma_CR) * self.CR_base + self.gamma_CR * np.mean(self.CR_success_history)\n                self.CR_base = np.clip(self.CR_base, 0.1, 1.0)\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                F = self.F_base + np.random.uniform(-self.F_range, self.F_range)\n                CR = self.CR_base + np.random.uniform(-self.CR_range, self.CR_range)\n                F = np.clip(F, 0.1, 1.0)\n                CR = np.clip(CR, 0.1, 1.0)\n\n                # Mutation strategy selection based on success/failure\n                if len(self.F_success_history) > len(self.F_failure_history):\n                    # Current-to-best mutation with archive\n                    indices = np.random.choice(self.pop_size, 2, replace=False)\n                    x1, x2 = population[indices]\n\n                    if np.random.rand() < self.current_to_best_prob:\n                        mutant = population[i] + F * (self.x_opt - population[i]) + F * (x1 - x2)\n                    else:\n                        x3 = population[np.random.choice(self.pop_size)]\n                        mutant = x1 + F * (x2 - x3)\n                    \n                    # Incorporate archive\n                    if len(self.success_archive) > 0 and np.random.rand() < self.archive_prob:  # archive_prob chance to use archive\n                        archive_index = np.random.randint(len(self.success_archive))\n                        mutant = x1 + F * (x2 - self.success_archive[archive_index])\n\n                else:\n                    # Random mutation with archive\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x1, x2, x3 = population[indices]\n                    mutant = x1 + F * (x2 - x3)\n\n                    # Incorporate failure archive (exploration)\n                    if len(self.failure_archive) > 0 and np.random.rand() < self.archive_prob:\n                        archive_index = np.random.randint(len(self.failure_archive))\n                        mutant = x1 + F * (x2 - self.failure_archive[archive_index])\n                \n                # Neighborhood-based mutation\n                neighborhood_indices = np.random.choice(self.pop_size, self.neighborhood_size, replace=False)\n                neighborhood = population[neighborhood_indices]\n                neighborhood_mean = np.mean(neighborhood, axis=0)\n                mutant = 0.5 * (mutant + neighborhood_mean)\n\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_learning_rate:\n                    orthogonal_vector = self.x_opt + np.random.normal(0, 0.1, self.dim)  # Perturb best solution\n                    orthogonal_vector = np.clip(orthogonal_vector, func.bounds.lb, func.bounds.ub)\n                    trial = 0.5 * (trial + orthogonal_vector)  # Combine with trial vector\n                    trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                    \n                    # Record successful F and CR values\n                    self.F_success_history.append(F)\n                    self.CR_success_history.append(CR)\n                    if len(self.F_success_history) > 50:\n                        self.F_success_history.pop(0)\n                        self.CR_success_history.pop(0)\n\n                    # Add replaced vector to success archive\n                    if len(self.success_archive) < self.archive_size:\n                        self.success_archive.append(population[i])\n                    else:\n                        self.success_archive[np.random.randint(self.archive_size)] = population[i]\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.stagnation_counter = 0\n                        self.previous_best_fitness = self.f_opt\n\n                        self.historical_best_fitness.append(self.f_opt)\n                        self.historical_best_solutions.append(np.copy(self.x_opt))\n                        if len(self.historical_best_fitness) > self.historical_memory_size:\n                            self.historical_best_fitness.pop(0)\n                            self.historical_best_solutions.pop(0)\n                else:\n                    self.F_failure_history.append(F)\n                    self.CR_failure_history.append(CR)\n                    if len(self.F_failure_history) > 50:\n                         self.F_failure_history.pop(0)\n                    if len(self.CR_failure_history) > 50:\n                         self.CR_failure_history.pop(0)\n\n                     # Add trial vector to failure archive\n                    if len(self.failure_archive) < self.archive_size:\n                        self.failure_archive.append(trial)\n                    else:\n                        self.failure_archive[np.random.randint(self.archive_size)] = trial\n\n            population = new_population\n            fitness = new_fitness\n            \n            # Restart population if stagnating using historical best solutions\n            if self.stagnation_counter > self.restart_patience:\n                # Incorporate historical best solutions into the population\n                num_historical = min(len(self.historical_best_solutions), self.pop_size // 2)  # Use at most half the population\n                \n                if num_historical > 0:\n                    indices = np.random.choice(len(self.historical_best_solutions), num_historical, replace=False)\n                    population[:num_historical] = np.array(self.historical_best_solutions)[indices]\n                    fitness[:num_historical] = np.array(self.historical_best_fitness)[indices]\n\n                # Generate opposition population for the remaining individuals\n                opposition_population = func.bounds.ub + func.bounds.lb - population[num_historical:]\n                opposition_fitness = np.array([func(x) for x in opposition_population])\n                self.budget -= (self.pop_size - num_historical)\n\n                # Combine original and opposition populations\n                combined_population = np.vstack((population[num_historical:], opposition_population))\n                combined_fitness = np.concatenate((fitness[num_historical:], opposition_fitness))\n\n                # Select the best individuals to form the new population\n                best_indices = np.argsort(combined_fitness)[:(self.pop_size - num_historical)]\n                population[num_historical:] = combined_population[best_indices]\n                fitness[num_historical:] = combined_fitness[best_indices]\n\n\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                    self.stagnation_counter = 0\n                    self.previous_best_fitness = self.f_opt\n\n                    self.historical_best_fitness.append(self.f_opt)\n                    self.historical_best_solutions.append(np.copy(self.x_opt))\n                    if len(self.historical_best_fitness) > self.historical_memory_size:\n                        self.historical_best_fitness.pop(0)\n                        self.historical_best_solutions.pop(0)\n                else:\n                    self.stagnation_counter +=1\n                    \n                # Reset F and CR history\n                self.F_success_history = []\n                self.CR_success_history = []\n                self.F_failure_history = []\n                self.CR_failure_history = []\n                \n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveDE scored 0.296 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a88736b8-66ef-4a76-bce9-665fcef770d1"], "operator": null, "metadata": {"aucs": [0.12526083522521603, 0.2225668253284251, 0.3588479292680854, 0.24298459237620362, 0.22520158259405232, 0.34439582318659046, 0.2635517699842267, 0.21920515836545562, 0.255397845796021, 0.2022562874012891, 0.23945134819600822, 0.9981359920186197, 0.25793267480549165, 0.2393181010571146, 0.21284162089101333, 0.3245916884020045, 0.2600183327958401, 0.25827006311893996, 0.18907271587614327, 0.4745675807933072]}}
{"id": "67dada9e-feaf-4600-b4aa-063265f1b165", "fitness": 0.6184930249227374, "name": "AdaptiveDE", "description": "Enhanced Adaptive Differential Evolution with covariance matrix adaptation for mutation and selection pressure balancing.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size_min=20, pop_size_max=100, F_base=0.5, CR_base=0.7, archive_size=100, F_range=0.3, CR_range=0.3, orthogonal_learning_rate=0.1, restart_patience=50, archive_prob=0.1, current_to_best_prob=0.2, archive_success_threshold=0.1, aging_factor=0.95, F_CR_history_length=20, CMA_learning_rate=0.1, selection_pressure_adaptation_rate = 0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_min = pop_size_min\n        self.pop_size_max = pop_size_max\n        self.pop_size = pop_size_max # Start with max population\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.archive_size = archive_size\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.archive = []\n        self.archive_fitness = []\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.restart_patience = restart_patience\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.F_history = []\n        self.CR_history = []\n        self.archive_prob = archive_prob\n        self.current_to_best_prob = current_to_best_prob\n        self.archive_success_rate = 0.0\n        self.archive_successes = 0\n        self.archive_trials = 0\n        self.archive_success_threshold = archive_success_threshold\n        self.aging_factor = aging_factor\n        self.F_CR_history_length = F_CR_history_length\n        self.CMA_learning_rate = CMA_learning_rate\n        self.selection_pressure_adaptation_rate = selection_pressure_adaptation_rate\n        \n        # CMA initialization\n        self.C = np.eye(self.dim)  # Covariance matrix\n        self.mean = np.zeros(self.dim)\n        self.cumulative_covariance = np.zeros((self.dim, self.dim))\n        self.step_size = 0.1\n        self.cumulative_step_size = 0.0\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n            self.stagnation_counter = 0\n            self.previous_best_fitness = self.f_opt\n        else:\n            self.stagnation_counter +=1\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n            \n            # Self-adaptive F and CR using running median\n            if self.F_history:\n                self.F_base = np.median(self.F_history)\n            if self.CR_history:\n                self.CR_base = np.median(self.CR_history)\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                F = self.F_base + np.random.uniform(-self.F_range, self.F_range)\n                CR = self.CR_base + np.random.uniform(-self.CR_range, self.CR_range)\n                F = np.clip(F, 0.1, 1.0)\n                CR = np.clip(CR, 0.1, 1.0)\n\n                # Mutation: Combining current-to-best with CMA\n                indices = np.random.choice(self.pop_size, 2, replace=False)\n                x1, x2 = population[indices]\n\n                # Sample mutation vector from CMA distribution\n                z = np.random.normal(0, 1, self.dim)\n                mutation_vector = self.mean + self.step_size * np.dot(np.linalg.cholesky(self.C), z)\n                \n                if np.random.rand() < self.current_to_best_prob:\n                    mutant = population[i] + F * (self.x_opt - population[i]) + F * (x1 - x2)\n                else:\n                    mutant = population[i] + F * (x1 - x2) + mutation_vector\n                \n                # Incorporate archive\n                use_archive = False\n                if len(self.archive) > 0 and np.random.rand() < self.archive_prob:  # archive_prob chance to use archive\n                    archive_index = np.random.randint(len(self.archive))\n                    mutant = x1 + F * (x2 - self.archive[archive_index])\n                    use_archive = True\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_learning_rate:\n                    orthogonal_vector = self.x_opt + np.random.normal(0, 0.1, self.dim)  # Perturb best solution\n                    orthogonal_vector = np.clip(orthogonal_vector, func.bounds.lb, func.bounds.ub)\n                    trial = 0.5 * (trial + orthogonal_vector)  # Combine with trial vector\n                    trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                    \n                    # Record successful F and CR values\n                    self.F_history.append(F)\n                    self.CR_history.append(CR)\n                    if len(self.F_history) > self.F_CR_history_length:\n                        self.F_history.pop(0)\n                        self.CR_history.pop(0)\n\n                    # Add replaced vector to archive using fitness difference\n                    fitness_diff = abs(fitness[i] - f_trial)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                        self.archive_fitness.append(fitness_diff)  # Store fitness difference\n                    else:\n                         # Replace worst in archive based on fitness difference\n                        worst_archive_index = np.argmin(self.archive_fitness) # Smaller is worse since it's a diff.\n                        if fitness_diff > self.archive_fitness[worst_archive_index]: # replace only if better\n                            self.archive[worst_archive_index] = population[i]\n                            self.archive_fitness[worst_archive_index] = fitness_diff\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.stagnation_counter = 0\n                        self.previous_best_fitness = self.f_opt\n                    \n                    if use_archive:\n                        self.archive_successes += 1\n\n                else:\n                     # Add trial vector to archive (combined strategy) - using fitness difference\n                    fitness_diff = abs(fitness[i] - f_trial)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(fitness_diff)\n                    else:\n                        # Replace worst in archive based on fitness difference\n                        worst_archive_index = np.argmin(self.archive_fitness)\n                        if fitness_diff > self.archive_fitness[worst_archive_index]:\n                            self.archive[worst_archive_index] = trial\n                            self.archive_fitness[worst_archive_index] = fitness_diff\n                        \n                if use_archive:\n                    self.archive_trials += 1\n            \n            # CMA-ES update\n            # Selection and recombination: Select top individuals\n            elite_indices = np.argsort(fitness)[:self.pop_size // 2] # Select top half\n            elite_population = population[elite_indices]\n\n            # Update mean\n            self.mean = np.mean(elite_population, axis=0)\n            \n            # Calculate step size adaptation signal\n            old_mean = np.copy(self.mean)\n            self.mean = np.mean(elite_population, axis=0)\n            step_adaptation = (np.linalg.norm(self.mean - old_mean) / self.step_size)\n\n            # Update step size\n            self.cumulative_step_size = (1 - self.CMA_learning_rate) * self.cumulative_step_size + self.CMA_learning_rate * step_adaptation\n\n            # Covariance matrix update\n            diff = elite_population - np.tile(self.mean, (elite_population.shape[0], 1))\n            self.cumulative_covariance = (1 - self.CMA_learning_rate) * self.cumulative_covariance + self.CMA_learning_rate * (diff.T @ diff) / elite_population.shape[0]\n\n            # Apply rank-one update to covariance matrix\n            self.C = (1 - self.CMA_learning_rate) * self.C + self.CMA_learning_rate * (diff.T @ diff) / elite_population.shape[0]\n\n            # Update step size\n            self.step_size *= np.exp(0.5 * (self.cumulative_step_size - 1))\n            \n            # Adapt selection pressure\n            if step_adaptation < 1:\n                self.current_to_best_prob = min(1.0, self.current_to_best_prob + self.selection_pressure_adaptation_rate)\n            else:\n                self.current_to_best_prob = max(0.0, self.current_to_best_prob - self.selection_pressure_adaptation_rate)\n                \n            population = new_population\n            fitness = new_fitness\n            \n            # Restart population if stagnating\n            if self.stagnation_counter > self.restart_patience:\n                # Opposition-based learning for restart\n                opposition_population = func.bounds.ub + func.bounds.lb - population\n                opposition_fitness = np.array([func(x) for x in opposition_population])\n                self.budget -= self.pop_size\n                \n                combined_population = np.concatenate((population, opposition_population))\n                combined_fitness = np.concatenate((fitness, opposition_fitness))\n                \n                # Select the best individuals from the combined population\n                indices = np.argsort(combined_fitness)[:self.pop_size]\n                population = combined_population[indices]\n                fitness = combined_fitness[indices]\n\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                    self.stagnation_counter = 0\n                    self.previous_best_fitness = self.f_opt\n                else:\n                    self.stagnation_counter +=1\n                    \n                # Reset F and CR history\n                self.F_history = []\n                self.CR_history = []\n            \n            # Adjust archive probability based on success rate\n            if self.archive_trials > 10:\n                self.archive_success_rate = self.archive_successes / self.archive_trials\n                if self.archive_success_rate < self.archive_success_threshold:\n                    self.archive_prob *= 0.9  # Decrease probability if success rate is low\n                else:\n                    self.archive_prob = min(1.0, self.archive_prob * 1.1) # Increase if success is good\n                self.archive_trials = 0\n                self.archive_successes = 0\n\n            # Dynamic population size adjustment\n            if self.f_opt < self.previous_best_fitness:\n                self.pop_size = min(self.pop_size_max, self.pop_size + 1)  # Increase pop size if improvement\n                self.previous_best_fitness = self.f_opt\n            else:\n                self.pop_size = max(self.pop_size_min, self.pop_size - 1)  # Decrease pop size if no improvement\n            \n            # Age the archive\n            for k in range(len(self.archive_fitness)):\n                self.archive_fitness[k] *= self.aging_factor\n                \n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveDE scored 0.618 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["255b279c-239d-42bc-a173-35a24e931863"], "operator": null, "metadata": {"aucs": [0.2120513674034108, 0.6126269182557864, 0.48337544619043915, 0.7855818701848125, 0.6873379548012404, 0.7443709725978531, 0.3896786919788058, 0.6062115618996766, 0.6819640801100401, 0.6646520261754778, 0.8266052410487728, 0.9997402866667982, 0.6586786351937031, 0.6177655658579537, 0.8120412898887207, 0.5766139717822888, 0.548114432033968, 0.7643900137308557, 0.19485598949315286, 0.5032041831609941]}}
{"id": "ea053e87-2bb7-449b-aa13-609c226fcd01", "fitness": 0.60250790368188, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with improved exploration by combining multiple mutation strategies and dynamically adjusting their probabilities based on success rates, and also enhanced local search using a Gaussian perturbation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_base=0.5, CR_base=0.7, archive_size=50, F_range=0.3, CR_range=0.3, orthogonal_learning_rate=0.1, restart_patience=50, archive_prob=0.1, current_to_best_prob=0.2, gamma_F = 0.1, gamma_CR = 0.1, success_threshold = 0.2, mutation_strategy_probs = [0.3, 0.3, 0.4]):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.archive_size = archive_size\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.success_archive = []\n        self.failure_archive = []\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.restart_patience = restart_patience\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.F_success_history = []\n        self.CR_success_history = []\n        self.archive_prob = archive_prob\n        self.current_to_best_prob = current_to_best_prob\n        self.gamma_F = gamma_F\n        self.gamma_CR = gamma_CR\n        self.success_threshold = success_threshold # Threshold for considering a trial successful\n        self.age_archive_rate = 0.01 # rate at which to apply \"aging\" in archive\n        self.F_CR_update_interval = 10 # update F and CR every this many iterations\n        self.generation = 0\n        self.mutation_strategy_probs = mutation_strategy_probs # Probabilities for mutation strategies\n        self.mutation_success_counts = [1] * len(mutation_strategy_probs) # Initialize success counts for each mutation strategy\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n            self.stagnation_counter = 0\n            self.previous_best_fitness = self.f_opt\n        else:\n            self.stagnation_counter +=1\n\n        while self.budget > 0:\n            self.generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n            \n            # Self-adaptive F and CR - update at intervals\n            if self.generation % self.F_CR_update_interval == 0:\n                if self.F_success_history:\n                    self.F_base = (1-self.gamma_F) * self.F_base + self.gamma_F * np.mean(self.F_success_history)\n                    self.F_base = np.clip(self.F_base, 0.1, 1.0)\n                if self.CR_success_history:\n                    self.CR_base = (1-self.gamma_CR) * self.CR_base + self.gamma_CR * np.mean(self.CR_success_history)\n                    self.CR_base = np.clip(self.CR_base, 0.1, 1.0)\n                    \n                # Normalize mutation strategy probabilities based on success counts\n                total_success = sum(self.mutation_success_counts)\n                self.mutation_strategy_probs = [count / total_success for count in self.mutation_success_counts]\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                F = self.F_base + np.random.uniform(-self.F_range, self.F_range)\n                CR = self.CR_base + np.random.uniform(-self.CR_range, self.CR_range)\n                F = np.clip(F, 0.1, 1.0)\n                CR = np.clip(CR, 0.1, 1.0)\n\n                # Mutation strategy selection\n                mutation_strategy = np.random.choice(len(self.mutation_strategy_probs), p=self.mutation_strategy_probs)\n                \n                # Mutation strategies\n                if mutation_strategy == 0:  # Current-to-best\n                    indices = np.random.choice(self.pop_size, 2, replace=False)\n                    x1, x2 = population[indices]\n                    mutant = population[i] + F * (self.x_opt - population[i]) + F * (x1 - x2)\n                elif mutation_strategy == 1:  # Random mutation\n                    x1, x2, x3 = population[np.random.choice(self.pop_size, 3, replace=False)]\n                    mutant = x1 + F * (x2 - x3)\n                else:  # Adding Gaussian perturbation to current best solution\n                    mutant = self.x_opt + np.random.normal(0, 0.05, self.dim) # Reduced std for finer local search\n                    \n                # Incorporate archive\n                if len(self.success_archive) > 0 and np.random.rand() < self.archive_prob:  # archive_prob chance to use archive\n                    archive_index = np.random.randint(len(self.success_archive))\n                    x1 = population[np.random.choice(self.pop_size)]\n                    mutant = x1 + F * (mutant - self.success_archive[archive_index])\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_learning_rate:\n                    orthogonal_vector = self.x_opt + np.random.normal(0, 0.1, self.dim)  # Perturb best solution\n                    orthogonal_vector = np.clip(orthogonal_vector, func.bounds.lb, func.bounds.ub)\n                    trial = 0.5 * (trial + orthogonal_vector)  # Combine with trial vector\n                    trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                # Define success based on threshold relative to current fitness\n                success = f_trial < fitness[i] - self.success_threshold * np.abs(fitness[i])\n                \n                if success:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                    \n                    # Record successful F and CR values\n                    self.F_success_history.append(F)\n                    self.CR_success_history.append(CR)\n                    if len(self.F_success_history) > 50:\n                        self.F_success_history.pop(0)\n                        self.CR_success_history.pop(0)\n\n                    # Add replaced vector to success archive (with aging)\n                    if len(self.success_archive) < self.archive_size:\n                        self.success_archive.append(population[i])\n                    else:\n                        index_to_replace = np.random.randint(self.archive_size)\n                        self.success_archive[index_to_replace] = population[i]\n\n                        # Apply aging (perturb the archive element slightly)\n                        self.success_archive[index_to_replace] += np.random.normal(0, self.age_archive_rate, self.dim)\n                        self.success_archive[index_to_replace] = np.clip(self.success_archive[index_to_replace], func.bounds.lb, func.bounds.ub)\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.stagnation_counter = 0\n                        self.previous_best_fitness = self.f_opt\n                        \n                    self.mutation_success_counts[mutation_strategy] += 1\n                else:\n                    # Add trial vector to failure archive (with aging)\n                    if len(self.failure_archive) < self.archive_size:\n                        self.failure_archive.append(trial)\n                    else:\n                        index_to_replace = np.random.randint(self.archive_size)\n                        self.failure_archive[index_to_replace] = trial\n                        \n                        # Apply aging (perturb the archive element slightly)\n                        self.failure_archive[index_to_replace] += np.random.normal(0, self.age_archive_rate, self.dim)\n                        self.failure_archive[index_to_replace] = np.clip(self.failure_archive[index_to_replace], func.bounds.lb, func.bounds.ub)\n                    self.mutation_success_counts[mutation_strategy] = max(1, self.mutation_success_counts[mutation_strategy] - 0.1) # Reduce success count for unsuccessful strategy\n\n            population = new_population\n            fitness = new_fitness\n            \n            # Restart population if stagnating using opposition-based learning\n            if self.stagnation_counter > self.restart_patience:\n                # Generate opposition population\n                opposition_population = func.bounds.ub + func.bounds.lb - population\n                opposition_fitness = np.array([func(x) for x in opposition_population])\n                self.budget -= self.pop_size\n\n                # Combine original and opposition populations\n                combined_population = np.vstack((population, opposition_population))\n                combined_fitness = np.concatenate((fitness, opposition_fitness))\n\n                # Select the best individuals to form the new population\n                best_indices = np.argsort(combined_fitness)[:self.pop_size]\n                population = combined_population[best_indices]\n                fitness = combined_fitness[best_indices]\n\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                    self.stagnation_counter = 0\n                    self.previous_best_fitness = self.f_opt\n                else:\n                    self.stagnation_counter +=1\n                    \n                # Reset F and CR history\n                self.F_success_history = []\n                self.CR_success_history = []\n                self.mutation_success_counts = [1] * len(self.mutation_success_counts) # Reset counts\n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveDE scored 0.603 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["1053fef0-896c-43c0-bbd7-1bdbe93cd5de"], "operator": null, "metadata": {"aucs": [0.3635340049162187, 0.3742082172250445, 0.6311837562321725, 0.17028458509813515, 0.7497566878901005, 0.8044666570405307, 0.3346772269649321, 0.5839858414047083, 0.7938088877207089, 0.21476157872854285, 0.9458861451584987, 0.997494357018195, 0.5043837938147939, 0.6671475317434625, 0.9588983315551319, 0.7668376806538225, 0.5882756226983572, 0.8886884944875242, 0.2168593342667352, 0.4950193390199844]}}
{"id": "a1e58717-2b37-4235-b791-6a6ed8c6b6c2", "fitness": 0.5264143884346675, "name": "AdaptiveDE", "description": "Improved Adaptive Differential Evolution with a modified mutation strategy combining current-to-best, archive, and a novel Cauchy mutation component for enhanced exploration and exploitation, alongside dynamic parameter adaptation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_base=0.5, CR_base=0.7, archive_size=50, F_range=0.3, CR_range=0.3, orthogonal_learning_rate=0.1, restart_patience=50, archive_prob=0.1, current_to_best_prob=0.2, gamma_F = 0.1, gamma_CR = 0.1, success_threshold = 0.2, cauchy_mutation_prob = 0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.archive_size = archive_size\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.success_archive = []\n        self.failure_archive = []\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.restart_patience = restart_patience\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.F_success_history = []\n        self.CR_success_history = []\n        self.archive_prob = archive_prob\n        self.current_to_best_prob = current_to_best_prob\n        self.gamma_F = gamma_F\n        self.gamma_CR = gamma_CR\n        self.success_threshold = success_threshold # Threshold for considering a trial successful\n        self.age_archive_rate = 0.01 # rate at which to apply \"aging\" in archive\n        self.F_CR_update_interval = 10 # update F and CR every this many iterations\n        self.generation = 0\n        self.cauchy_mutation_prob = cauchy_mutation_prob # Probability of applying Cauchy mutation\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n            self.stagnation_counter = 0\n            self.previous_best_fitness = self.f_opt\n        else:\n            self.stagnation_counter +=1\n\n        while self.budget > 0:\n            self.generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n            \n            # Self-adaptive F and CR - update at intervals\n            if self.generation % self.F_CR_update_interval == 0:\n                if self.F_success_history:\n                    self.F_base = (1-self.gamma_F) * self.F_base + self.gamma_F * np.mean(self.F_success_history)\n                    self.F_base = np.clip(self.F_base, 0.1, 1.0)\n                if self.CR_success_history:\n                    self.CR_base = (1-self.gamma_CR) * self.CR_base + self.gamma_CR * np.mean(self.CR_success_history)\n                    self.CR_base = np.clip(self.CR_base, 0.1, 1.0)\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                F = self.F_base + np.random.uniform(-self.F_range, self.F_range)\n                CR = self.CR_base + np.random.uniform(-self.CR_range, self.CR_range)\n                F = np.clip(F, 0.1, 1.0)\n                CR = np.clip(CR, 0.1, 1.0)\n\n                # Mutation: Combining current-to-best with random mutation and Cauchy mutation\n                indices = np.random.choice(self.pop_size, 2, replace=False)\n                x1, x2 = population[indices]\n                \n                mutant = np.copy(population[i])  # Start with the current individual\n\n                if np.random.rand() < self.current_to_best_prob:\n                    mutant += F * (self.x_opt - population[i])  # Current-to-best component\n                \n                mutant += F * (x1 - x2)  # Add differential evolution component\n\n                # Incorporate archive\n                if len(self.success_archive) > 0 and np.random.rand() < self.archive_prob:  # archive_prob chance to use archive\n                    archive_index = np.random.randint(len(self.success_archive))\n                    mutant += F * (self.success_archive[archive_index] - population[i])\n\n                # Apply Cauchy mutation\n                if np.random.rand() < self.cauchy_mutation_prob:\n                    mutant += F * np.random.standard_cauchy(self.dim)  # Add Cauchy noise\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_learning_rate:\n                    orthogonal_vector = self.x_opt + np.random.normal(0, 0.1, self.dim)  # Perturb best solution\n                    orthogonal_vector = np.clip(orthogonal_vector, func.bounds.lb, func.bounds.ub)\n                    trial = 0.5 * (trial + orthogonal_vector)  # Combine with trial vector\n                    trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                # Define success based on threshold relative to current fitness\n                success = f_trial < fitness[i] - self.success_threshold * np.abs(fitness[i])\n                \n                if success:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                    \n                    # Record successful F and CR values\n                    self.F_success_history.append(F)\n                    self.CR_success_history.append(CR)\n                    if len(self.F_success_history) > 50:\n                        self.F_success_history.pop(0)\n                        self.CR_success_history.pop(0)\n\n                    # Add replaced vector to success archive (with aging)\n                    if len(self.success_archive) < self.archive_size:\n                        self.success_archive.append(population[i])\n                    else:\n                        index_to_replace = np.random.randint(self.archive_size)\n                        self.success_archive[index_to_replace] = population[i]\n\n                        # Apply aging (perturb the archive element slightly)\n                        self.success_archive[index_to_replace] += np.random.normal(0, self.age_archive_rate, self.dim)\n                        self.success_archive[index_to_replace] = np.clip(self.success_archive[index_to_replace], func.bounds.lb, func.bounds.ub)\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.stagnation_counter = 0\n                        self.previous_best_fitness = self.f_opt\n                else:\n                    # Add trial vector to failure archive (with aging)\n                    if len(self.failure_archive) < self.archive_size:\n                        self.failure_archive.append(trial)\n                    else:\n                        index_to_replace = np.random.randint(self.archive_size)\n                        self.failure_archive[index_to_replace] = trial\n                        \n                        # Apply aging (perturb the archive element slightly)\n                        self.failure_archive[index_to_replace] += np.random.normal(0, self.age_archive_rate, self.dim)\n                        self.failure_archive[index_to_replace] = np.clip(self.failure_archive[index_to_replace], func.bounds.lb, func.bounds.ub)\n\n            population = new_population\n            fitness = new_fitness\n            \n            # Restart population if stagnating using opposition-based learning\n            if self.stagnation_counter > self.restart_patience:\n                # Generate opposition population\n                opposition_population = func.bounds.ub + func.bounds.lb - population\n                opposition_fitness = np.array([func(x) for x in opposition_population])\n                self.budget -= self.pop_size\n\n                # Combine original and opposition populations\n                combined_population = np.vstack((population, opposition_population))\n                combined_fitness = np.concatenate((fitness, opposition_fitness))\n\n                # Select the best individuals to form the new population\n                best_indices = np.argsort(combined_fitness)[:self.pop_size]\n                population = combined_population[best_indices]\n                fitness = combined_fitness[best_indices]\n\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                    self.stagnation_counter = 0\n                    self.previous_best_fitness = self.f_opt\n                else:\n                    self.stagnation_counter +=1\n                    \n                # Reset F and CR history\n                self.F_success_history = []\n                self.CR_success_history = []\n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveDE scored 0.526 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["1053fef0-896c-43c0-bbd7-1bdbe93cd5de"], "operator": null, "metadata": {"aucs": [0.21246106782080199, 0.3948167315507354, 0.497831342214224, 0.8346180668720946, 0.4996111227071599, 0.6304800117939868, 0.3278504350267396, 0.44234864119876016, 0.4941912466153209, 0.19250813071020334, 0.8545055448913987, 0.9962147291065302, 0.3983526151233454, 0.45820909064283155, 0.8583106121277476, 0.5628095673981552, 0.42175205571573593, 0.75205544079244, 0.20503054849295954, 0.49433076789217956]}}
{"id": "f068a93a-e0d6-485c-8efc-8a439f4a7377", "fitness": 0.6712723112888777, "name": "AdaptiveDE", "description": "Enhanced Adaptive Differential Evolution with a combined F/CR adaptation based on success history, stochastic ranking for handling constraints, and a dynamically adjusted archive incorporating orthogonal learning.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_base=0.5, CR_base=0.7, archive_size=50, F_range=0.3, CR_range=0.3, orthogonal_learning_rate=0.1, restart_patience=50, archive_prob=0.1, current_to_best_prob=0.2, gamma_F = 0.1, gamma_CR = 0.1, success_threshold = 0.2, stochastic_ranking_probability = 0.45):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.archive_size = archive_size\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.success_archive = []\n        self.failure_archive = []\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.restart_patience = restart_patience\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.F_success_history = []\n        self.CR_success_history = []\n        self.archive_prob = archive_prob\n        self.current_to_best_prob = current_to_best_prob\n        self.gamma_F = gamma_F\n        self.gamma_CR = gamma_CR\n        self.success_threshold = success_threshold # Threshold for considering a trial successful\n        self.age_archive_rate = 0.01 # rate at which to apply \"aging\" in archive\n        self.F_CR_update_interval = 10 # update F and CR every this many iterations\n        self.generation = 0\n        self.stochastic_ranking_probability = stochastic_ranking_probability\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n            self.stagnation_counter = 0\n            self.previous_best_fitness = self.f_opt\n        else:\n            self.stagnation_counter +=1\n\n        while self.budget > 0:\n            self.generation += 1\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n            \n            # Self-adaptive F and CR - update at intervals\n            if self.generation % self.F_CR_update_interval == 0:\n                if self.F_success_history:\n                    # Weighted average of F and CR\n                    weights = np.arange(1, len(self.F_success_history) + 1)\n                    weights = weights / np.sum(weights)\n                    self.F_base = (1-self.gamma_F) * self.F_base + self.gamma_F * np.average(self.F_success_history, weights=weights)\n                    self.F_base = np.clip(self.F_base, 0.1, 1.0)\n                if self.CR_success_history:\n                    weights = np.arange(1, len(self.CR_success_history) + 1)\n                    weights = weights / np.sum(weights)\n                    self.CR_base = (1-self.gamma_CR) * self.CR_base + self.gamma_CR * np.average(self.CR_success_history, weights=weights)\n                    self.CR_base = np.clip(self.CR_base, 0.1, 1.0)\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                F = self.F_base + np.random.uniform(-self.F_range, self.F_range)\n                CR = self.CR_base + np.random.uniform(-self.CR_range, self.CR_range)\n                F = np.clip(F, 0.1, 1.0)\n                CR = np.clip(CR, 0.1, 1.0)\n\n                # Mutation: Combining current-to-best with random mutation\n                indices = np.random.choice(self.pop_size, 2, replace=False)\n                x1, x2 = population[indices]\n                \n                if np.random.rand() < self.current_to_best_prob:\n                    mutant = population[i] + F * (self.x_opt - population[i]) + F * (x1 - x2)\n                else:\n                    x3 = population[np.random.choice(self.pop_size)]\n                    mutant = x1 + F * (x2 - x3)\n                \n                # Incorporate archive\n                if len(self.success_archive) > 0 and np.random.rand() < self.archive_prob:  # archive_prob chance to use archive\n                    archive_index = np.random.randint(len(self.success_archive))\n                    mutant = x1 + F * (x2 - self.success_archive[archive_index])\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_learning_rate:\n                    orthogonal_vector = self.x_opt + np.random.normal(0, 0.1, self.dim)  # Perturb best solution\n                    orthogonal_vector = np.clip(orthogonal_vector, func.bounds.lb, func.bounds.ub)\n                    trial = 0.5 * (trial + orthogonal_vector)  # Combine with trial vector\n                    trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection - Stochastic Ranking\n                f_trial = func(trial)\n                self.budget -= 1\n                f_original = fitness[i]\n                \n                #Constraint violation (dummy)\n                constraint_violation_trial = 0\n                constraint_violation_original = 0\n\n                #Stochastic ranking based on probability\n                if (constraint_violation_trial == 0 and constraint_violation_original == 0) or np.random.rand() < self.stochastic_ranking_probability:\n                    if f_trial < f_original:\n                        success = True\n                    else:\n                        success = False\n                else:\n                     if constraint_violation_trial < constraint_violation_original:\n                         success = True\n                     else:\n                         success = False\n\n                # Define success based on threshold relative to current fitness\n                #success = f_trial < fitness[i] - self.success_threshold * np.abs(fitness[i])\n                \n                if success:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                    \n                    # Record successful F and CR values\n                    self.F_success_history.append(F)\n                    self.CR_success_history.append(CR)\n                    if len(self.F_success_history) > 50:\n                        self.F_success_history.pop(0)\n                        self.CR_success_history.pop(0)\n\n                    # Add replaced vector to success archive (with aging and orthogonal learning)\n                    if len(self.success_archive) < self.archive_size:\n                        self.success_archive.append(population[i])\n                    else:\n                        index_to_replace = np.random.randint(self.archive_size)\n                        self.success_archive[index_to_replace] = population[i]\n\n                        # Apply aging (perturb the archive element slightly)\n                        self.success_archive[index_to_replace] += np.random.normal(0, self.age_archive_rate, self.dim)\n                        self.success_archive[index_to_replace] = np.clip(self.success_archive[index_to_replace], func.bounds.lb, func.bounds.ub)\n\n                        # Apply orthogonal learning to the archive element\n                        orthogonal_vector = self.x_opt + np.random.normal(0, 0.1, self.dim)\n                        orthogonal_vector = np.clip(orthogonal_vector, func.bounds.lb, func.bounds.ub)\n                        self.success_archive[index_to_replace] = 0.5 * (self.success_archive[index_to_replace] + orthogonal_vector)\n                        self.success_archive[index_to_replace] = np.clip(self.success_archive[index_to_replace], func.bounds.lb, func.bounds.ub)\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.stagnation_counter = 0\n                        self.previous_best_fitness = self.f_opt\n                else:\n                    # Add trial vector to failure archive (with aging)\n                    if len(self.failure_archive) < self.archive_size:\n                        self.failure_archive.append(trial)\n                    else:\n                        index_to_replace = np.random.randint(self.archive_size)\n                        self.failure_archive[index_to_replace] = trial\n                        \n                        # Apply aging (perturb the archive element slightly)\n                        self.failure_archive[index_to_replace] += np.random.normal(0, self.age_archive_rate, self.dim)\n                        self.failure_archive[index_to_replace] = np.clip(self.failure_archive[index_to_replace], func.bounds.lb, func.bounds.ub)\n\n            population = new_population\n            fitness = new_fitness\n            \n            # Restart population if stagnating using opposition-based learning\n            if self.stagnation_counter > self.restart_patience:\n                # Generate opposition population\n                opposition_population = func.bounds.ub + func.bounds.lb - population\n                opposition_fitness = np.array([func(x) for x in opposition_population])\n                self.budget -= self.pop_size\n\n                # Combine original and opposition populations\n                combined_population = np.vstack((population, opposition_population))\n                combined_fitness = np.concatenate((fitness, opposition_fitness))\n\n                # Select the best individuals to form the new population\n                best_indices = np.argsort(combined_fitness)[:self.pop_size]\n                population = combined_population[best_indices]\n                fitness = combined_fitness[best_indices]\n\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                    self.stagnation_counter = 0\n                    self.previous_best_fitness = self.f_opt\n                else:\n                    self.stagnation_counter +=1\n                    \n                # Reset F and CR history\n                self.F_success_history = []\n                self.CR_success_history = []\n\n            if self.budget <= 0:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveDE scored 0.671 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["1053fef0-896c-43c0-bbd7-1bdbe93cd5de"], "operator": null, "metadata": {"aucs": [0.3477986278437749, 0.6657014505809475, 0.6323082804798517, 0.8889788857805726, 0.7250447611674713, 0.7702566698358491, 0.5538805005593191, 0.5893699049280947, 0.7199397059017026, 0.5999484550510894, 0.8756827999054356, 0.9965044011412038, 0.47476493987584967, 0.6741259699994189, 0.9197608316651816, 0.7657852118258384, 0.5375494688827714, 0.8539664210125104, 0.30900458981693935, 0.5250743495237324]}}
