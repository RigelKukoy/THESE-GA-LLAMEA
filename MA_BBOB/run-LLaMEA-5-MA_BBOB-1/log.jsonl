{"id": "fbed2124-84de-4d80-afc4-654394a62668", "fitness": -Infinity, "name": "CMAES", "description": "Covariance matrix adaptation evolution strategy with a dynamically adjusted population size based on function evaluation progress.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.cc = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2.3)**2 + self.mueff))\n        self.chiN = self.dim**0.5 * (1 - 1/(4*self.dim) + 1/(21*self.dim**2))\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.evals = 0\n\n    def __call__(self, func):\n        while self.evals < self.budget:\n            # Adjust population size based on remaining budget\n            remaining_evals = self.budget - self.evals\n            self.popsize = min(self.popsize, remaining_evals)\n            self.mu = self.popsize // 2\n\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            x = self.mean[:, np.newaxis] + self.sigma * np.dot(np.linalg.cholesky(self.C), z)\n            x = x.T\n            f = np.array([func(xi) for xi in x])\n            self.evals += self.popsize\n            idx = np.argsort(f)\n            x = x[idx]\n            f = f[idx]\n\n            if f[0] < self.f_opt:\n                self.f_opt = f[0]\n                self.x_opt = x[0]\n\n            xmean = np.sum(x[:self.mu].T * self.weights, axis=1)\n\n            ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.dot(np.linalg.inv(np.linalg.cholesky(self.C)), (xmean - self.mean)) / self.sigma\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.evals / self.popsize)) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.cc) * self.pc + hsig * np.sqrt(self.cc * (2 - self.cc) * self.mueff) * (xmean - self.mean) / self.sigma\n            self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * (np.outer(self.pc, self.pc) + (1 - hsig) * self.cc * (2 - self.cc) * self.C) + self.cmu * np.sum(self.weights[i] * np.outer((x[i] - self.mean) / self.sigma, (x[i] - self.mean) / self.sigma) for i in range(self.mu))\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(ps) / self.chiN - 1))\n            self.mean = xmean\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            \n            # Ensure C remains positive definite\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-8 * np.eye(self.dim)  # Add a small value to the diagonal\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "An exception occurred: operands could not be broadcast together with shapes (2,2) (3,) .", "error": "", "parent_ids": [], "operator": null, "metadata": {}}
{"id": "e0efa51a-6067-4507-8202-27f4173aecaf", "fitness": 0.2955858272700628, "name": "CMAES", "description": "Covariance Matrix Adaptation Evolution Strategy with a simplified rank-one update and adaptive step size control.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damps=1, c_mu=0.1, c_1=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.m = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.C = np.eye(self.dim)\n        self.cs = cs\n        self.damps = damps\n        self.c_mu = c_mu\n        self.c_1 = c_1\n        self.pc = np.zeros(self.dim)\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n\n    def __call__(self, func):\n        while self.evals < self.budget:\n            arz = np.random.multivariate_normal(np.zeros(self.dim), self.C, size=self.popsize)\n            arx = self.m + self.sigma * arz\n            arfitness = np.zeros(self.popsize)\n\n            for k in range(self.popsize):\n                x = arx[k]\n                \n                lb = func.bounds.lb\n                ub = func.bounds.ub\n                x = np.clip(x, lb, ub)\n                \n                arfitness[k] = func(x)\n                self.evals += 1\n                if arfitness[k] < self.f_opt:\n                    self.f_opt = arfitness[k]\n                    self.x_opt = x\n\n                if self.evals >= self.budget:\n                    break\n\n            arindex = np.argsort(arfitness)\n            arfitness = arfitness[arindex]\n            arx = arx[arindex]\n            arz = arz[arindex]\n\n            xmean = np.sum(self.weights[:, None] * arx[:self.mu], axis=0)\n            zmean = np.sum(self.weights[:, None] * arz[:self.mu], axis=0)\n\n            self.pc = (1 - self.cs) * self.pc + np.sqrt(self.cs * (2 - self.cs)) * zmean\n            self.C = (1 - self.c_1) * self.C + self.c_1 * (self.pc[:, None] @ self.pc[None, :])\n\n            self.sigma *= np.exp((self.cs/self.damps)*(np.linalg.norm(self.pc)/np.sqrt(self.dim) - 1))\n            self.m = xmean\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm CMAES scored 0.296 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.09343326816778774, 0.16939460257013483, 0.38280674875336984, 0.1935695862461977, 0.2639687795735747, 0.14854161762557339, 0.269187891719211, 0.33573767773824037, 0.25402315380088514, 0.14438958495287002, 0.2546408773698726, 0.9955865937124435, 0.26541616145142277, 0.24720830698702634, 0.5796257335342876, 0.34566233731242935, 0.2883727538592036, 0.3237933147625399, 0.20277779048998346, 0.15357976477420232]}}
{"id": "8e959804-f385-4ace-a0b5-40cd352c78da", "fitness": 0.7810463059470614, "name": "AdaptiveDifferentialEvolution", "description": "An adaptive Differential Evolution strategy that adjusts its parameters based on the function evaluations.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, CR=0.7):\n        \"\"\"\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The population size.\n            F (float): The mutation factor.\n            CR (float): The crossover rate.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.F_history = []\n        self.CR_history = []\n\n    def __call__(self, func):\n        \"\"\"\n        Optimizes the given black box function using Adaptive Differential Evolution.\n\n        Args:\n            func (callable): The black box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize population within the bounds\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update the best solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Mutation\n                indices = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = population[a] + self.F * (population[b] - population[c])\n\n                # Clip the mutant vector to the bounds\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover_mask, mutant, population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = trial\n                    \n                    # Update the best solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Adaptive parameter control (example: adjusting F and CR based on success rate)\n            success_indices = fitness < np.mean(fitness)\n            if np.any(success_indices):\n                successful_F = self.F\n                successful_CR = self.CR\n                \n                # Simple adaptation: if there are successful individuals, keep the parameters.\n                self.F_history.append(successful_F)\n                self.CR_history.append(successful_CR)\n\n                if len(self.F_history) > 10: #Keep the history relatively short\n                    self.F = np.mean(self.F_history[-10:]) \n                    self.CR = np.mean(self.CR_history[-10:])\n            else:\n                # If no improvement, perturb parameters\n                self.F = np.clip(self.F + np.random.normal(0, 0.1), 0.1, 1.0)\n                self.CR = np.clip(self.CR + np.random.normal(0, 0.1), 0.1, 1.0)\n                self.F_history.append(self.F)\n                self.CR_history.append(self.CR)\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.781 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.40418644160157224, 0.7066779889075344, 0.6453377046582734, 0.9400174036464308, 0.8724205024399635, 0.8960979355651781, 0.7584005800229672, 0.825782374147693, 0.8851511926755079, 0.8635525216055855, 0.9104490881809051, 0.9968169851235135, 0.3124520925088995, 0.863316202754252, 0.9471991313052901, 0.9092914885642593, 0.7865114036103831, 0.921593807124782, 0.45445646820296404, 0.7212148062952712]}}
{"id": "0eaaa942-aa37-452f-907d-1498312e6e84", "fitness": 0.6198348707704842, "name": "HybridDECMASolver", "description": "A population-based algorithm that combines elements of differential evolution and covariance matrix adaptation, focusing on efficient exploration and exploitation.", "code": "import numpy as np\n\nclass HybridDECMASolver:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.8, CR=0.9, mu_adapt_rate=0.1):\n        \"\"\"\n        Initialize the Hybrid DE-CMA Solver.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            F (float): Differential evolution scaling factor.\n            CR (float): Differential evolution crossover rate.\n            mu_adapt_rate (float): Adaptation rate for the mean vector.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.mu_adapt_rate = mu_adapt_rate\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using the Hybrid DE-CMA strategy.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Initialize CMA-ES-like parameters\n        mu = np.mean(population, axis=0)\n        sigma = 0.1  # Initial step size\n        C = np.eye(self.dim)  # Covariance matrix\n        pc = np.zeros(self.dim)  # Evolution path for C\n        ps = np.zeros(self.dim)  # Evolution path for sigma\n        \n        # Parameters for adaptation\n        c_sigma = 1/np.sqrt(self.dim)\n        d_sigma = 1 + 2*max(0, np.sqrt((self.dim-1)/(self.dim+1)) - 1) + c_sigma\n        c_c = 4 / (self.dim + 4)\n        c_mu = 2 / (self.dim + np.sqrt(self.dim))\n\n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            # Differential Evolution part\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                  break\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n                x_mutated = population[i] + self.F * (x_r2 - x_r3)\n\n                # Crossover\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Repair bounds\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n                \n                # Selection\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = x_trial\n\n                    if f_trial < f_opt:\n                        f_opt = f_trial\n                        x_opt = x_trial\n\n            # CMA-ES adaptation\n            old_mu = mu.copy()\n            mu = np.mean(population, axis=0)\n            \n            # Update evolution paths\n            ps = (1 - c_sigma) * ps + np.sqrt(c_sigma * (2 - c_sigma)) * (mu - old_mu) / sigma * np.linalg.inv(np.linalg.cholesky(C)) # Using cholesky decomposition for more efficient inversion of C\n\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - c_sigma)**2) < (2 + 4/(self.dim + 1))\n            dc = (1 + (2*max(0, np.linalg.norm(ps)/np.sqrt(1-(1-c_sigma)**(2*self.budget/self.pop_size))-1)) / (self.dim + 2))\n            pc = (1 - c_c) * pc + hsig * np.sqrt(c_c * (2 - c_c)) * (mu - old_mu) / sigma\n            \n            # Update covariance matrix\n            C = (1 - c_mu) * C + c_mu * (np.outer(pc, pc) + c_mu * C)\n            C = np.triu(C) + np.triu(C, 1).T # Enforce symmetry\n            \n            # Update step size\n            sigma *= np.exp((c_sigma/d_sigma) * (np.linalg.norm(ps)/np.sqrt(self.dim) - 1))\n\n        return f_opt, x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDECMASolver scored 0.620 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.22568567593954048, 0.5956924684673135, 0.570677421235738, 0.8197390331064146, 0.6623201763544673, 0.7601840271747067, 0.38003932232066784, 0.5665535273343187, 0.7142473953148684, 0.45737357943384327, 0.8392169194954086, 0.9936037828004051, 0.6124417817917162, 0.596682072851453, 0.8742763875169768, 0.7175945221759825, 0.4857808325098445, 0.7873600858470198, 0.2273498668505598, 0.5098785368884378]}}
{"id": "2bc2e259-1f9a-435d-ae7b-527e6a423701", "fitness": -Infinity, "name": "AdaptivePSO", "description": "A self-organizing particle swarm optimization (PSO) with adaptive velocity clamping and topology, adjusting exploration and exploitation during the search.", "code": "import numpy as np\n\nclass AdaptivePSO:\n    def __init__(self, budget=10000, dim=10, popsize=None, w_max=0.9, w_min=0.2, c1=2.0, c2=2.0, v_clamp_max=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 10 * self.dim\n        self.w_max = w_max\n        self.w_min = w_min\n        self.c1 = c1\n        self.c2 = c2\n        self.v_clamp_max = v_clamp_max\n        self.particles = np.random.uniform(-5, 5, size=(self.popsize, self.dim))\n        self.velocities = np.random.uniform(-1, 1, size=(self.popsize, self.dim)) * self.v_clamp_max\n        self.personal_best_positions = self.particles.copy()\n        self.personal_best_fitnesses = np.full(self.popsize, np.inf)\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.evals = 0\n\n    def __call__(self, func):\n        while self.evals < self.budget:\n            # Update inertia weight linearly\n            w = self.w_max - (self.w_max - self.w_min) * (self.evals / self.budget)\n\n            # Evaluate fitness for each particle\n            fitnesses = np.zeros(self.popsize)\n            for i in range(self.popsize):\n                x = self.particles[i]\n\n                lb = func.bounds.lb\n                ub = func.bounds.ub\n                x = np.clip(x, lb, ub)\n                \n                fitness = func(x)\n                fitnesses[i] = fitness\n                self.evals += 1\n\n                # Update personal best\n                if fitness < self.personal_best_fitnesses[i]:\n                    self.personal_best_fitnesses[i] = fitness\n                    self.personal_best_positions[i] = x.copy()\n\n                # Update global best\n                if fitness < self.global_best_fitness:\n                    self.global_best_fitness = fitness\n                    self.global_best_position = x.copy()\n\n                if self.evals >= self.budget:\n                    break\n\n            # Update velocities and positions\n            for i in range(self.popsize):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] = w * self.velocities[i] + \\\n                                     self.c1 * r1 * (self.personal_best_positions[i] - self.particles[i]) + \\\n                                     self.c2 * r2 * (self.global_best_position - self.particles[i])\n                \n                # Velocity clamping\n                self.velocities[i] = np.clip(self.velocities[i], -self.v_clamp_max, self.v_clamp_max)\n                \n                self.particles[i] += self.velocities[i]\n\n                # Boundary handling: Reflective boundaries\n                oob = (self.particles[i] < func.bounds.lb) | (self.particles[i] > func.bounds.ub)\n                self.velocities[i][oob] *= -1  # Reverse velocity\n                self.particles[i][oob] = np.clip(self.particles[i][oob], func.bounds.lb, func.bounds.ub) # Clip positions\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 1, "feedback": "An exception occurred: operands could not be broadcast together with shapes (0,) (2,) (2,) .", "error": "", "parent_ids": ["e0efa51a-6067-4507-8202-27f4173aecaf"], "operator": null, "metadata": {}}
{"id": "63bdf01f-036e-4a03-bf0f-8801d96ed455", "fitness": -Infinity, "name": "CMAES", "description": "CMA-ES with simplified updates, adaptive step size, and dynamic population size adjustment based on the remaining budget.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.cc = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2.3)**2 + self.mueff))\n        self.chiN = self.dim**0.5 * (1 - 1/(4*self.dim) + 1/(21*self.dim**2))\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.evals = 0\n\n    def __call__(self, func):\n        while self.evals < self.budget:\n            remaining_evals = self.budget - self.evals\n            self.popsize = min(self.popsize, remaining_evals)\n            self.mu = self.popsize // 2\n\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            x = self.mean[:, np.newaxis] + self.sigma * np.dot(np.linalg.cholesky(self.C), z)\n            x = x.T\n            f = np.array([func(xi) for xi in x])\n            self.evals += self.popsize\n            idx = np.argsort(f)\n            x = x[idx]\n            f = f[idx]\n\n            if f[0] < self.f_opt:\n                self.f_opt = f[0]\n                self.x_opt = x[0]\n\n            xmean = np.sum(x[:self.mu] * self.weights[:, np.newaxis], axis=0)\n\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.dot(np.linalg.inv(np.linalg.cholesky(self.C)), (xmean - self.mean)) / self.sigma\n            hsig = np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.evals / self.popsize)) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.cc) * self.pc + hsig * np.sqrt(self.cc * (2 - self.cc) * self.mueff) * (xmean - self.mean) / self.sigma\n            \n            # Simplified rank-one update\n            y = (xmean - self.mean) / self.sigma\n            self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * (np.outer(self.pc, self.pc) + (1 - hsig) * self.cc * (2 - self.cc) * self.C) + self.cmu * np.outer(y, y)\n\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            self.mean = xmean\n\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C += 1e-8 * np.eye(self.dim)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "An exception occurred: operands could not be broadcast together with shapes (2,2) (3,1) .", "error": "", "parent_ids": ["fbed2124-84de-4d80-afc4-654394a62668"], "operator": null, "metadata": {}}
{"id": "2b3e81ed-097b-48df-962f-76d0c03f4ab5", "fitness": -Infinity, "name": "CMAES", "description": "Improved CMA-ES with robust covariance matrix handling, adaptive population sizing, and boundary constraint handling.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, lb=-5, ub=5):\n        self.budget = budget\n        self.dim = dim\n        self.lb = lb\n        self.ub = ub\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.cc = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2.3)**2 + self.mueff))\n        self.chiN = self.dim**0.5 * (1 - 1/(4*self.dim) + 1/(21*self.dim**2))\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.evals = 0\n        self.tolx = 1e-12 * self.sigma  # Convergence criterion\n\n    def __call__(self, func):\n        while self.evals < self.budget:\n            # Adjust population size based on remaining budget\n            remaining_evals = self.budget - self.evals\n            self.popsize = min(self.popsize, remaining_evals)\n            self.mu = self.popsize // 2\n            if self.mu == 0:\n                self.mu = 1\n                self.popsize = 2 # ensure at least two samples\n\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            try:\n                x = self.mean[:, np.newaxis] + self.sigma * np.dot(np.linalg.cholesky(self.C), z)\n            except np.linalg.LinAlgError:\n                # Handle non-positive definite covariance matrix\n                self.C = self.C + 1e-8 * np.eye(self.dim)\n                x = self.mean[:, np.newaxis] + self.sigma * np.dot(np.linalg.cholesky(self.C), z)\n            x = x.T\n\n            # Boundary handling: clip to bounds\n            x = np.clip(x, self.lb, self.ub)\n\n            f = np.array([func(xi) for xi in x])\n            self.evals += self.popsize\n            idx = np.argsort(f)\n            x = x[idx]\n            f = f[idx]\n\n            if f[0] < self.f_opt:\n                self.f_opt = f[0]\n                self.x_opt = x[0]\n\n            xmean = np.sum(x[:self.mu].T * self.weights, axis=1)\n\n            ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.dot(np.linalg.inv(np.linalg.cholesky(self.C)), (xmean - self.mean)) / self.sigma\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.evals / self.popsize)) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.cc) * self.pc + hsig * np.sqrt(self.cc * (2 - self.cc) * self.mueff) * (xmean - self.mean) / self.sigma\n            \n            # Update covariance matrix\n            dC = self.c1 * (np.outer(self.pc, self.pc) + (1 - hsig) * self.cc * (2 - self.cc) * self.C)\n            \n            x_diff = x[:self.mu] - self.mean\n            for i in range(self.mu):\n                dC += self.cmu * self.weights[i] * np.outer(x_diff[i] / self.sigma, x_diff[i] / self.sigma)\n            \n            self.C = (1 - self.c1 - self.cmu) * self.C + dC\n            \n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(ps) / self.chiN - 1))\n            self.mean = xmean\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            \n            # Ensure C remains positive definite\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-8 * np.eye(self.dim)  # Add a small value to the diagonal\n            \n            # Check for convergence\n            if np.max(self.sigma * np.sqrt(np.diag(self.C))) < self.tolx:\n                break  # Break loop if converged\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "An exception occurred: operands could not be broadcast together with shapes (2,2) (3,) .", "error": "", "parent_ids": ["fbed2124-84de-4d80-afc4-654394a62668"], "operator": null, "metadata": {}}
{"id": "c47d930a-4659-4341-adc1-196d95d990b8", "fitness": 0.4435234111577972, "name": "AdaptiveGaussianMutation", "description": "Adaptive Gaussian mutation with step size adaptation based on success rate.", "code": "import numpy as np\n\nclass AdaptiveGaussianMutation:\n    def __init__(self, budget=10000, dim=10, initial_sigma=0.5, success_rate_threshold=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.sigma = initial_sigma\n        self.success_rate_threshold = success_rate_threshold\n        self.x_best = np.random.uniform(-5, 5, size=self.dim)\n        self.f_best = np.inf\n        self.evals = 0\n        self.successes = 0\n        self.iterations = 0\n\n    def __call__(self, func):\n        while self.evals < self.budget:\n            x_new = self.x_best + self.sigma * np.random.normal(0, 1, size=self.dim)\n            x_new = np.clip(x_new, -5, 5) \n\n            f_new = func(x_new)\n            self.evals += 1\n            self.iterations += 1\n\n            if f_new < self.f_best:\n                self.f_best = f_new\n                self.x_best = x_new\n                self.successes += 1\n\n            if self.iterations % 10 == 0:\n                success_rate = self.successes / self.iterations\n                if success_rate > self.success_rate_threshold:\n                    self.sigma *= 1.1  # Increase step size\n                else:\n                    self.sigma *= 0.9  # Decrease step size\n                self.successes = 0\n                self.iterations = 0\n\n\n        return self.f_best, self.x_best", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveGaussianMutation scored 0.444 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["fbed2124-84de-4d80-afc4-654394a62668"], "operator": null, "metadata": {"aucs": [0.12915897139896837, 0.16541345638077254, 0.34957835242461055, 0.18306178410549778, 0.2453434119045229, 0.9533969448514934, 0.24870548364438694, 0.7739780041005908, 0.163470802718792, 0.09993704232591805, 0.9776932788319204, 0.9996165426370061, 0.2299441282179413, 0.25730525295414064, 0.8908157932819787, 0.3818027309771195, 0.4181640099551319, 0.968489673458418, 0.2374830640770782, 0.19710949490965624]}}
{"id": "c0a76bf8-6612-47bf-8ac5-50cefb15fbd3", "fitness": 0.3394212374067194, "name": "CMAES", "description": "Simplified CMA-ES with rank-one update and adaptive step size control, removing unused parameters and improving efficiency.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, c_1=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.m = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.C = np.eye(self.dim)\n        self.cs = cs\n        self.c_1 = c_1\n        self.pc = np.zeros(self.dim)\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n\n    def __call__(self, func):\n        while self.evals < self.budget:\n            arz = np.random.multivariate_normal(np.zeros(self.dim), self.C, size=self.popsize)\n            arx = self.m + self.sigma * arz\n            arfitness = np.zeros(self.popsize)\n\n            for k in range(self.popsize):\n                x = arx[k]                \n                lb = func.bounds.lb\n                ub = func.bounds.ub\n                x = np.clip(x, lb, ub)\n                \n                arfitness[k] = func(x)\n                self.evals += 1\n                if arfitness[k] < self.f_opt:\n                    self.f_opt = arfitness[k]\n                    self.x_opt = x\n\n                if self.evals >= self.budget:\n                    break\n\n            arindex = np.argsort(arfitness)\n            arz = arz[arindex]\n\n            zmean = np.sum(self.weights[:, None] * arz[:self.mu], axis=0)\n\n            self.pc = (1 - self.cs) * self.pc + np.sqrt(self.cs * (2 - self.cs)) * zmean\n            self.C = (1 - self.c_1) * self.C + self.c_1 * (self.pc[:, None] @ self.pc[None, :])\n\n            self.sigma *= np.exp(self.cs/2 * (np.sum(self.pc**2) / self.dim - 1))\n            self.m += self.sigma * np.sum(self.weights[:, None] * arz[:self.mu], axis=0)\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm CMAES scored 0.339 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e0efa51a-6067-4507-8202-27f4173aecaf"], "operator": null, "metadata": {"aucs": [0.11056923659322015, 0.18240889395178095, 0.39321910009246697, 0.18177033149644073, 0.5533836509874037, 0.27936108056357567, 0.22329039624950486, 0.3591046567142061, 0.18422931222692873, 0.14601349672987873, 0.37413125416641546, 0.9952769647448534, 0.2787423548813537, 0.2422477596317385, 0.7168349056001283, 0.34629429208333906, 0.21939581676986086, 0.3808459752958643, 0.1810481261243343, 0.44025714323109233]}}
{"id": "621da0fc-9247-490a-a8e4-3da7b0b69b14", "fitness": 0.2868631339401443, "name": "CMAES", "description": "Simplified CMA-ES with rank-one update, adaptive step size, and normalized cumulative step size adaptation.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damps=1, c_1=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.m = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.C = np.eye(self.dim)\n        self.cs = cs\n        self.damps = damps\n        self.c_1 = c_1\n        self.pc = np.zeros(self.dim)\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n\n    def __call__(self, func):\n        while self.evals < self.budget:\n            arz = np.random.multivariate_normal(np.zeros(self.dim), self.C, size=self.popsize)\n            arx = self.m + self.sigma * arz\n            arfitness = np.zeros(self.popsize)\n\n            for k in range(self.popsize):\n                x = arx[k]\n                lb = func.bounds.lb\n                ub = func.bounds.ub\n                x = np.clip(x, lb, ub)\n                arfitness[k] = func(x)\n                self.evals += 1\n                if arfitness[k] < self.f_opt:\n                    self.f_opt = arfitness[k]\n                    self.x_opt = x\n\n                if self.evals >= self.budget:\n                    break\n\n            arindex = np.argsort(arfitness)\n            arz = arz[arindex]\n\n            zmean = np.sum(self.weights[:, None] * arz[:self.mu], axis=0)\n\n            self.pc = (1 - self.cs) * self.pc + np.sqrt(self.cs * (2 - self.cs)) * zmean\n            \n            norm_pc = np.linalg.norm(self.pc)\n            self.sigma *= np.exp((self.cs/self.damps)*(norm_pc/np.sqrt(self.dim) - 1))\n\n            self.C = (1 - self.c_1) * self.C + self.c_1 * (self.pc[:, None] @ self.pc[None, :])\n            self.m = self.m + self.sigma * zmean\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm CMAES scored 0.287 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e0efa51a-6067-4507-8202-27f4173aecaf"], "operator": null, "metadata": {"aucs": [0.10969915661448171, 0.18212202583783388, 0.3847399931503285, 0.17562769161289982, 0.1459374681720279, 0.2791515598344161, 0.3476282140113125, 0.4277262377204244, 0.1890597022120346, 0.15743866015442143, 0.32918863855572866, 0.2151800158583339, 0.30084187513538196, 0.2011295159921338, 0.7072720933911943, 0.3196095920837564, 0.33217736985449975, 0.2903762954012714, 0.17528017780651806, 0.4670763954038861]}}
{"id": "a362ef3b-8eda-4e4d-a15e-c51758203b64", "fitness": 0.6370977813136376, "name": "SimplifiedHybridDECMASolver", "description": "Simplified Hybrid DE-CMA, removing redundant CMA-ES parts and streamlining DE for better efficiency.", "code": "import numpy as np\n\nclass SimplifiedHybridDECMASolver:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.8, CR=0.9):\n        \"\"\"\n        Initialize the Simplified Hybrid DE-CMA Solver.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            F (float): Differential evolution scaling factor.\n            CR (float): Differential evolution crossover rate.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using a simplified Hybrid DE-CMA strategy.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            # Differential Evolution part\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                  break\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n                x_mutated = population[i] + self.F * (x_r2 - x_r3)\n\n                # Crossover\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Repair bounds\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n                \n                # Selection\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = x_trial\n\n                    if f_trial < f_opt:\n                        f_opt = f_trial\n                        x_opt = x_trial\n\n        return f_opt, x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm SimplifiedHybridDECMASolver scored 0.637 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0eaaa942-aa37-452f-907d-1498312e6e84"], "operator": null, "metadata": {"aucs": [0.21597763390374924, 0.6397010467552243, 0.5805732663445665, 0.7992204682169375, 0.7332487283777573, 0.7546315145647865, 0.5358952937939486, 0.5665291699389061, 0.6291897848510162, 0.557219578295353, 0.8310424801064993, 0.9910158306851126, 0.6300848598306066, 0.5532581996607437, 0.9222705794050272, 0.7344874874483411, 0.5447457623586527, 0.8192216709160954, 0.21292407951971726, 0.49071819129971006]}}
{"id": "b0aa3b2c-6f4d-4f22-8f82-9837b36b61fc", "fitness": 0.6284689029194793, "name": "HybridDECMASolver", "description": "Simplified Hybrid DE-CMA solver with adaptive step size and covariance matrix updates, focusing on reducing complexity and improving efficiency.", "code": "import numpy as np\n\nclass HybridDECMASolver:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.8, CR=0.9):\n        \"\"\"\n        Initialize the Hybrid DE-CMA Solver.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            F (float): Differential evolution scaling factor.\n            CR (float): Differential evolution crossover rate.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using the Hybrid DE-CMA strategy.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Initialize CMA-ES-like parameters\n        mu = np.mean(population, axis=0)\n        sigma = 0.1  # Initial step size\n        C = np.eye(self.dim)  # Covariance matrix\n        \n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            # Differential Evolution part\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n                x_mutated = population[i] + self.F * (x_r2 - x_r3)\n\n                # Crossover\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Repair bounds\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = x_trial\n\n                    if f_trial < f_opt:\n                        f_opt = f_trial\n                        x_opt = x_trial\n\n            # CMA-ES adaptation (simplified)\n            mu_old = mu.copy()\n            mu = np.mean(population, axis=0)\n            \n            # Update covariance matrix and step size\n            z = (mu - mu_old) / sigma\n            C = (1 - 0.1) * C + 0.1 * np.outer(z, z) # Simplified rank-one update\n            sigma *= np.exp(0.05 * (np.linalg.norm(z) - 1)) # Simplified step size adaptation\n            \n        return f_opt, x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm HybridDECMASolver scored 0.628 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0eaaa942-aa37-452f-907d-1498312e6e84"], "operator": null, "metadata": {"aucs": [0.17120894885950766, 0.6619080069033667, 0.6157204188658776, 0.8557877401418812, 0.6418810169509721, 0.7642799006029262, 0.4599523016283025, 0.6027237941900314, 0.6587444290967215, 0.4421544157409899, 0.8646765766923573, 0.9993968696182405, 0.6194753358852312, 0.6004014369334121, 0.921922784263981, 0.663613584836924, 0.49150451112720683, 0.8246584780991519, 0.20273920594284534, 0.5066283020096596]}}
{"id": "973560d9-714c-4930-9f5e-e8362047635b", "fitness": 0.5786426140037664, "name": "AdaptiveRestartDE", "description": "A self-adaptive parameter control scheme for differential evolution coupled with a restart strategy based on stagnation detection.", "code": "import numpy as np\n\nclass AdaptiveRestartDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F_initial=0.5, CR_initial=0.7, F_range=(0.3, 0.9), CR_range=(0.5, 1.0), stagnation_threshold=1000):\n        \"\"\"\n        Initialize the Adaptive Restart Differential Evolution.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            F_initial (float): Initial differential evolution scaling factor.\n            CR_initial (float): Initial differential evolution crossover rate.\n            F_range (tuple): Range for adaptive F.\n            CR_range (tuple): Range for adaptive CR.\n            stagnation_threshold (int): Number of iterations without improvement before restart.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_initial\n        self.CR = CR_initial\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.stagnation_threshold = stagnation_threshold\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.best_fitness = np.inf\n        self.best_individual = None\n        self.stagnation_counter = 0\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using the Adaptive Restart DE strategy.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.best_fitness = np.min(fitness)\n        self.best_individual = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            # Adaptive parameter control\n            self.F = np.random.uniform(self.F_range[0], self.F_range[1])\n            self.CR = np.random.uniform(self.CR_range[0], self.CR_range[1])\n\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n                x_mutated = population[i] + self.F * (x_r2 - x_r3)\n\n                # Crossover\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Repair bounds\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = x_trial\n\n                    if f_trial < self.best_fitness:\n                        self.best_fitness = f_trial\n                        self.best_individual = x_trial\n                        self.stagnation_counter = 0  # Reset stagnation counter\n                else:\n                    self.stagnation_counter += 1\n\n            # Restart strategy\n            if self.stagnation_counter > self.stagnation_threshold:\n                # Re-initialize population\n                population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n\n                # Update best fitness and individual\n                min_fitness = np.min(fitness)\n                if min_fitness < self.best_fitness:\n                    self.best_fitness = min_fitness\n                    self.best_individual = population[np.argmin(fitness)]\n                self.stagnation_counter = 0  # Reset stagnation counter\n\n        return self.best_fitness, self.best_individual", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveRestartDE scored 0.579 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0eaaa942-aa37-452f-907d-1498312e6e84"], "operator": null, "metadata": {"aucs": [0.16744636427306958, 0.3049519126066872, 0.5828402987422017, 0.8033614453497795, 0.5632046198779618, 0.7630222763885759, 0.39812701482309, 0.5275053092538016, 0.6634442544270144, 0.40114925380406385, 0.8278542919660994, 0.9929973735367175, 0.4914640776133937, 0.48181382294265196, 0.9017529085859646, 0.71692843385942, 0.5008308847723546, 0.8008464756207123, 0.19852989614029926, 0.48478136549147044]}}
{"id": "e5b825b0-9c88-4c50-81a8-763202e8d5a8", "fitness": 0.49544690503780703, "name": "AdaptiveHybridDECMASolver", "description": "Hybrid DE-CMA with adaptive parameter control and improved covariance matrix adaptation for faster convergence.", "code": "import numpy as np\n\nclass AdaptiveHybridDECMASolver:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.8, CR=0.9, mu_adapt_rate=0.1):\n        \"\"\"\n        Initialize the Adaptive Hybrid DE-CMA Solver.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            F (float): Differential evolution scaling factor.\n            CR (float): Differential evolution crossover rate.\n            mu_adapt_rate (float): Adaptation rate for the mean vector.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.mu_adapt_rate = mu_adapt_rate\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n\n        # Adaptive parameter control\n        self.F_history = []\n        self.CR_history = []\n        self.success_history = []\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using the Adaptive Hybrid DE-CMA strategy.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Initialize CMA-ES-like parameters\n        mu = np.mean(population, axis=0)\n        sigma = 0.1  # Initial step size\n        C = np.eye(self.dim)  # Covariance matrix\n        pc = np.zeros(self.dim)  # Evolution path for C\n        ps = np.zeros(self.dim)  # Evolution path for sigma\n\n        # Parameters for adaptation\n        c_sigma = 1/np.sqrt(self.dim)\n        d_sigma = 1 + 2*max(0, np.sqrt((self.dim-1)/(self.dim+1)) - 1) + c_sigma\n        c_c = 4 / (self.dim + 4)\n        c_mu = 2 / (self.dim + np.sqrt(self.dim))\n        c_1 = 2 / ((self.dim + 1.3)**2 + c_mu)  # Learning rate for rank-one update\n        c_mu_matrix = min(1 - c_1, c_mu)  # Learning rate for rank-mu update\n\n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n        \n        archive = [] # Archive for successful solutions\n\n        while self.budget > 0:\n            # Differential Evolution part\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n                x_mutated = population[i] + self.F * (x_r2 - x_r3)\n\n                # Crossover\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Repair bounds\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    # Store successful F and CR\n                    self.F_history.append(self.F)\n                    self.CR_history.append(self.CR)\n                    self.success_history.append(1)\n                    \n                    fitness[i] = f_trial\n                    population[i] = x_trial\n                    archive.append(x_trial) # Store successful solutions in the archive\n\n                    if f_trial < f_opt:\n                        f_opt = f_trial\n                        x_opt = x_trial\n                else:\n                    self.success_history.append(0)\n            \n            # Adaptive F and CR (using simple heuristics)\n            if len(self.success_history) > self.pop_size:  # Ensure enough data\n                success_rate = np.mean(self.success_history[-self.pop_size:])\n                if success_rate > 0.5:  # Adjust CR and F based on success\n                    self.CR *= 0.95\n                    self.F *= 1.05\n                else:\n                    self.CR *= 1.05\n                    self.F *= 0.95\n\n                self.F = np.clip(self.F, 0.1, 1.0)\n                self.CR = np.clip(self.CR, 0.1, 1.0)\n\n\n            # CMA-ES adaptation\n            old_mu = mu.copy()\n            mu = np.mean(population, axis=0)\n\n            # Update evolution paths\n            ps = (1 - c_sigma) * ps + np.sqrt(c_sigma * (2 - c_sigma)) * (mu - old_mu) / sigma\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - c_sigma)**2) < (2 + 4/(self.dim + 1))\n            dc = (1 + (2*max(0, np.linalg.norm(ps)/np.sqrt(1-(1-c_sigma)**(2*self.budget/self.pop_size))-1)) / (self.dim + 2))\n            pc = (1 - c_c) * pc + hsig * np.sqrt(c_c * (2 - c_c)) * (mu - old_mu) / sigma\n\n            # Update covariance matrix\n            C = (1 - c_1 - c_mu_matrix) * C + c_1 * np.outer(pc, pc)\n            \n            # Rank-mu update\n            population_diff = population - old_mu\n            weights = np.clip(np.log((0.5*self.pop_size + 1)/np.arange(1, self.pop_size + 1)), 0, np.inf)\n            weights /= np.sum(weights)\n            \n            for k in range(self.pop_size):\n                C += c_mu_matrix * weights[k] * np.outer(population_diff[k], population_diff[k]) / sigma**2\n            \n            C = np.triu(C) + np.triu(C, 1).T  # Enforce symmetry\n\n            # Handle near-singular covariance matrix\n            try:\n              np.linalg.cholesky(C) # Check if C is positive definite\n            except np.linalg.LinAlgError:\n              C += 1e-6 * np.eye(self.dim)  # Add a small diagonal to make it positive definite\n\n            # Update step size\n            sigma *= np.exp((c_sigma/d_sigma) * (np.linalg.norm(ps)/np.sqrt(self.dim) - 1))\n\n        return f_opt, x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveHybridDECMASolver scored 0.495 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0eaaa942-aa37-452f-907d-1498312e6e84"], "operator": null, "metadata": {"aucs": [0.17261650384797123, 0.373466932344523, 0.5127794302722124, 0.8411675644606101, 0.2771997346906705, 0.5841104651215123, 0.3067629999641541, 0.37682536368805564, 0.3703478513268931, 0.1944912095895236, 0.8345340441081917, 0.9930711606004725, 0.6058036650901373, 0.3058078001349581, 0.7664541256936739, 0.5613842052151078, 0.3690768504588856, 0.7537348142734552, 0.23447249739667098, 0.4748308824784594]}}
{"id": "c09cdd41-57cb-4b55-bae6-e1598c9c61e5", "fitness": -Infinity, "name": "CMAES", "description": "CMA-ES with boundary repair and a more robust covariance matrix update strategy.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, lb=-5.0, ub=5.0):\n        self.budget = budget\n        self.dim = dim\n        self.lb = lb\n        self.ub = ub\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.cc = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2.3)**2 + self.mueff))\n        self.chiN = self.dim**0.5 * (1 - 1/(4*self.dim) + 1/(21*self.dim**2))\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.evals = 0\n\n    def repair(self, x):\n        return np.clip(x, self.lb, self.ub)\n\n    def __call__(self, func):\n        while self.evals < self.budget:\n            # Adjust population size based on remaining budget\n            remaining_evals = self.budget - self.evals\n            self.popsize = min(self.popsize, remaining_evals)\n            self.mu = self.popsize // 2\n\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            x = self.mean[:, np.newaxis] + self.sigma * np.dot(np.linalg.cholesky(self.C), z)\n            x = x.T\n            \n            # Repair individuals that violate bounds\n            x = np.array([self.repair(xi) for xi in x])\n\n            f = np.array([func(xi) for xi in x])\n            self.evals += self.popsize\n            idx = np.argsort(f)\n            x = x[idx]\n            f = f[idx]\n\n            if f[0] < self.f_opt:\n                self.f_opt = f[0]\n                self.x_opt = x[0]\n\n            xmean = np.sum(x[:self.mu].T * self.weights, axis=1)\n\n            ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(np.linalg.cholesky(self.C), (xmean - self.mean)) / self.sigma\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.evals / self.popsize)) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.cc) * self.pc + hsig * np.sqrt(self.cc * (2 - self.cc) * self.mueff) * (xmean - self.mean) / self.sigma\n\n            # Robust covariance matrix adaptation\n            C_temp = np.zeros_like(self.C)\n            for i in range(self.mu):\n                y = (x[i] - self.mean) / self.sigma\n                C_temp += self.weights[i] * np.outer(y, y)\n\n            self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * (np.outer(self.pc, self.pc) + (1 - hsig) * self.cc * (2 - self.cc) * self.C) + self.cmu * C_temp\n\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(ps) / self.chiN - 1))\n            self.mean = xmean\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            \n            # Ensure C remains positive definite\n            try:\n                L = np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                # Add a small value to the diagonal\n                min_eig = np.min(np.real(np.linalg.eigvals(self.C)))\n                self.C += (abs(min_eig) + 1e-8) * np.eye(self.dim)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "An exception occurred: operands could not be broadcast together with shapes (2,2) (3,) .", "error": "", "parent_ids": ["fbed2124-84de-4d80-afc4-654394a62668"], "operator": null, "metadata": {}}
{"id": "7ada4d96-85cb-4d6a-a7ea-2aa2763e907e", "fitness": 0.2791171042608177, "name": "CMAES", "description": "CMA-ES with rank-one update, adaptive step size, and covariance matrix regularization to prevent premature convergence.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damps=1, c_mu=0.1, c_1=0.1, c_mueff=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.m = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.C = np.eye(self.dim)\n        self.cs = cs\n        self.damps = damps\n        self.c_mu = c_mu\n        self.c_1 = c_1\n        self.c_mueff = c_mueff\n        self.pc = np.zeros(self.dim)\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n        self.chiN = self.dim**0.5 * (1 - (1/(4*self.dim)) + 1/(21*self.dim**2))\n\n    def __call__(self, func):\n        while self.evals < self.budget:\n            arz = np.random.multivariate_normal(np.zeros(self.dim), self.C, size=self.popsize)\n            arx = self.m + self.sigma * arz\n            arfitness = np.zeros(self.popsize)\n\n            for k in range(self.popsize):\n                x = arx[k]\n                \n                lb = func.bounds.lb\n                ub = func.bounds.ub\n                x = np.clip(x, lb, ub)\n                \n                arfitness[k] = func(x)\n                self.evals += 1\n                if arfitness[k] < self.f_opt:\n                    self.f_opt = arfitness[k]\n                    self.x_opt = x\n\n                if self.evals >= self.budget:\n                    break\n\n            arindex = np.argsort(arfitness)\n            arfitness = arfitness[arindex]\n            arx = arx[arindex]\n            arz = arz[arindex]\n\n            xmean = np.sum(self.weights[:, None] * arx[:self.mu], axis=0)\n            zmean = np.sum(self.weights[:, None] * arz[:self.mu], axis=0)\n\n            ps = (1-self.cs) * self.pc + np.sqrt(self.cs * (2-self.cs) * self.mueff) * zmean\n            norm_ps = np.linalg.norm(ps)\n            self.sigma *= np.exp((self.cs/self.damps) * (norm_ps/self.chiN - 1))\n\n            self.pc = (1 - self.c_1) * self.pc + np.sqrt(self.c_1 * (2 - self.c_1) * self.mueff) * zmean\n            \n            C_temp = self.c_mu * np.sum(self.weights[:, None, None] * (arz[:self.mu, :, None] @ arz[:self.mu, None, :]), axis=0)\n            self.C = (1 - self.c_1 - self.c_mu) * self.C + self.c_1 * (self.pc[:, None] @ self.pc[None, :]) + C_temp\n\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n\n            eigval, eigvec = np.linalg.eigh(self.C)\n            eigval = np.maximum(eigval, 1e-10)\n            self.C = eigvec @ np.diag(eigval) @ eigvec.T\n\n            self.m = xmean\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm CMAES scored 0.279 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e0efa51a-6067-4507-8202-27f4173aecaf"], "operator": null, "metadata": {"aucs": [0.12218467707567704, 0.17919875319042022, 0.37284114160300463, 0.16191516354250257, 0.15773240638420305, 0.20923832062450076, 0.2223409281679657, 0.22216000197410413, 0.19857595094436709, 0.14788696383854272, 0.314897579655783, 0.9950942052296304, 0.25768614050228134, 0.163670930262115, 0.5418273253447117, 0.27449514406257947, 0.24411795756054866, 0.18505464763306345, 0.14858435870457143, 0.46283948891578197]}}
{"id": "83e055c3-864d-438d-a4f4-bb0f3aa1cd8b", "fitness": 0.0, "name": "SelfAdaptiveDE_LocalSearch", "description": "Implements a self-adaptive Differential Evolution with a local search operator triggered probabilistically based on the population's diversity.", "code": "import numpy as np\n\nclass SelfAdaptiveDE_LocalSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F_initial=0.5, CR_initial=0.7, ls_prob=0.1):\n        \"\"\"\n        Initialize the Self-Adaptive Differential Evolution with Local Search.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            F_initial (float): Initial scaling factor for differential evolution.\n            CR_initial (float): Initial crossover rate for differential evolution.\n            ls_prob (float): Probability of applying local search to a solution.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_initial = F_initial\n        self.CR_initial = CR_initial\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.ls_prob = ls_prob\n\n    def local_search(self, func, x, step_size=0.1):\n        \"\"\"\n        Performs a simple local search around the given solution.\n\n        Args:\n            func (callable): The function to optimize.\n            x (np.ndarray): The current solution.\n            step_size (float): The step size for the local search.\n\n        Returns:\n            np.ndarray: The improved solution.\n        \"\"\"\n        x_new = x.copy()\n        for i in range(self.dim):\n            # Explore in both directions\n            x_plus = x.copy()\n            x_minus = x.copy()\n            x_plus[i] += step_size\n            x_minus[i] -= step_size\n\n            # Clip to respect bounds\n            x_plus = np.clip(x_plus, self.bounds_lb, self.bounds_ub)\n            x_minus = np.clip(x_minus, self.bounds_lb, self.bounds_ub)\n\n            # Evaluate\n            f_plus = func(x_plus)\n            f_minus = func(x_minus)\n\n            # Choose the better direction\n            if f_plus < func(x_new) and f_plus < func(x):\n                x_new = x_plus\n            elif f_minus < func(x_new) and f_minus < func(x):\n                x_new = x_minus\n            self.budget -= (2 if (f_plus < func(x_new) and f_plus < func(x)) or (f_minus < func(x_new) and f_minus < func(x)) else 0)\n\n            if self.budget <= 0:\n              break\n        return x_new\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using self-adaptive DE with local search.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population, F, and CR\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        F = np.full(self.pop_size, self.F_initial)\n        CR = np.full(self.pop_size, self.CR_initial)\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                  break\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n                x_mutated = population[i] + F[i] * (x_r2 - x_r3)\n\n                # Crossover\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR[i] or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Repair bounds\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n\n                # Local Search\n                if np.random.rand() < self.ls_prob:\n                    x_trial = self.local_search(func, x_trial)\n                    if self.budget <= 0:\n                      break\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = x_trial\n                    # Update F and CR (self-adaptation)\n                    F[i] = np.random.normal(self.F_initial, 0.1)\n                    CR[i] = np.random.normal(self.CR_initial, 0.1)\n                    F[i] = np.clip(F[i], 0.1, 1.0)\n                    CR[i] = np.clip(CR[i], 0.1, 1.0)\n\n                    if f_trial < f_opt:\n                        f_opt = f_trial\n                        x_opt = x_trial\n\n        return f_opt, x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm SelfAdaptiveDE_LocalSearch scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a362ef3b-8eda-4e4d-a15e-c51758203b64"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "3c648638-035f-43e2-8950-d54dcb82b559", "fitness": -Infinity, "name": "HybridDECMASolver", "description": "Enhanced Hybrid DE-CMA with adaptive population sizing, momentum-based CMA updates, and a decay mechanism for DE parameters to balance exploration and exploitation.", "code": "import numpy as np\n\nclass HybridDECMASolver:\n    def __init__(self, budget=10000, dim=10, pop_size_initial=20, F=0.8, CR=0.9, adapt_pop_freq=50):\n        \"\"\"\n        Initialize the Enhanced Hybrid DE-CMA Solver.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size_initial (int): The initial size of the population.\n            F (float): Differential evolution scaling factor.\n            CR (float): Differential evolution crossover rate.\n            adapt_pop_freq (int): Frequency of population size adaptation.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size_initial\n        self.pop_size_history = [pop_size_initial]\n        self.F = F\n        self.CR = CR\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.generation = 0\n        self.adapt_pop_freq = adapt_pop_freq\n\n        # CMA-ES parameters\n        self.sigma = 0.1  # Initial step size\n        self.C = np.eye(self.dim)  # Covariance matrix\n        self.mu = None\n        self.momentum_C = np.zeros_like(self.C)\n        self.momentum_mu = None\n\n    def adapt_population_size(self, fitness):\n        \"\"\"\n        Adapt the population size based on the fitness variance.\n        \"\"\"\n        fitness_std = np.std(fitness)\n        if fitness_std > 1e-2:  # Dynamic threshold\n            self.pop_size = int(self.pop_size * 1.1)  # Increase pop size\n        else:\n            self.pop_size = int(self.pop_size * 0.9)  # Decrease pop size\n        \n        self.pop_size = max(10, min(self.pop_pop_size_history[-1], self.pop_size))  # Keep it within reasonable bounds, prevent oscillations\n        self.pop_size_history.append(self.pop_size)\n        \n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using the Enhanced Hybrid DE-CMA strategy.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Initialize CMA-ES-like parameters\n        self.mu = np.mean(population, axis=0)\n        if self.momentum_mu is None:\n             self.momentum_mu = np.zeros_like(self.mu)\n            \n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            self.generation += 1\n\n            # Differential Evolution part\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n                x_mutated = population[i] + self.F * (x_r2 - x_r3)\n\n                # Crossover\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Repair bounds\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = x_trial\n\n                    if f_trial < f_opt:\n                        f_opt = f_trial\n                        x_opt = x_trial\n\n            # CMA-ES adaptation (simplified)\n            mu_old = self.mu.copy()\n            self.mu = np.mean(population, axis=0)\n            \n            # Momentum for mu update\n            self.momentum_mu = 0.9 * self.momentum_mu + 0.1 * (self.mu - mu_old)\n            self.mu = mu_old + self.momentum_mu  # Apply momentum\n\n            # Update covariance matrix and step size with momentum\n            z = (self.mu - mu_old) / self.sigma\n            \n            # Momentum for C update\n            self.momentum_C = 0.9 * self.momentum_C + 0.1 * np.outer(z, z)\n            C_new = (1 - 0.1) * self.C + 0.1 * self.momentum_C  # Apply momentum\n            \n            # Ensure C remains positive definite (simplified)\n            try:\n                np.linalg.cholesky(C_new)\n                self.C = C_new\n            except np.linalg.LinAlgError:\n                self.C = self.C  # Keep the old covariance matrix\n\n            self.sigma *= np.exp(0.05 * (np.linalg.norm(z) - 1))  # Simplified step size adaptation\n\n            # Adaptive F and CR (decay over time)\n            self.F *= 0.995  # Decay F\n            self.CR *= 0.995  # Decay CR\n\n            if self.generation % self.adapt_pop_freq == 0:\n                self.adapt_population_size(fitness)\n                # Resize population:  Keep the best individuals\n                best_indices = np.argsort(fitness)[:min(self.pop_size, len(fitness))]\n                population = population[best_indices]\n                fitness = fitness[best_indices]\n                \n                # If population size increased, fill with random individuals\n                if len(population) < self.pop_size:\n                    num_new = self.pop_size - len(population)\n                    new_population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(num_new, self.dim))\n                    new_fitness = np.array([func(x) for x in new_population])\n                    self.budget -= num_new\n                    \n                    population = np.vstack((population, new_population))\n                    fitness = np.concatenate((fitness, new_fitness))\n                    \n                    \n        return f_opt, x_opt", "configspace": "", "generation": 2, "feedback": "An exception occurred: 'HybridDECMASolver' object has no attribute 'pop_pop_size_history'.", "error": "", "parent_ids": ["b0aa3b2c-6f4d-4f22-8f82-9837b36b61fc"], "operator": null, "metadata": {}}
{"id": "af9ab2de-d564-4396-b62a-ba9200e17b8d", "fitness": 0.4831305786800062, "name": "AgingArchiveDE", "description": "DE with Self-Adaptive mutation factor and an external archive to enhance exploration and exploitation, using an aging mechanism for diversity.", "code": "import numpy as np\n\nclass AgingArchiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, archive_size=10, initial_F=0.5, CR=0.9):\n        \"\"\"\n        Initialize the Aging Archive DE Solver.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            archive_size (int): The size of the external archive.\n            initial_F (float): Initial differential evolution scaling factor.\n            CR (float): Differential evolution crossover rate.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = initial_F\n        self.CR = CR\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.archive = []\n        self.archive_fitness = []\n        self.age = np.zeros(pop_size) # Age of individuals in the population\n        self.max_age = 50 # Maximum age before replacement\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using the Aging Archive DE strategy.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Mutation\n                idxs = np.random.choice(self.pop_size, 2, replace=False)\n                x_r1, x_r2 = population[idxs]\n\n                # Choose a vector from the archive with a small probability\n                if len(self.archive) > 0 and np.random.rand() < 0.1:\n                    idx_archive = np.random.randint(len(self.archive))\n                    x_r3 = self.archive[idx_archive]\n                    F = self.F # Use the self-adaptive factor from the population\n                else:\n                    x_r3 = population[np.random.choice(self.pop_size, 1, replace=False)[0]]\n                    F = self.F + 0.1 * np.random.normal() # Use a modified mutation factor\n\n                F = np.clip(F, 0.1, 1.0)\n                x_mutated = population[i] + F * (x_r1 - x_r2)\n\n                # Crossover\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Repair bounds\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = x_trial\n                    self.age[i] = 0 # Reset age\n\n                    if f_trial < f_opt:\n                        f_opt = f_trial\n                        x_opt = x_trial\n\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(x_trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        # Replace the worst element in the archive\n                        max_idx = np.argmax(self.archive_fitness)\n                        if f_trial < self.archive_fitness[max_idx]:\n                            self.archive[max_idx] = x_trial\n                            self.archive_fitness[max_idx] = f_trial\n\n                else:\n                    self.age[i] += 1 # Increase age if not improving\n\n            # Aging mechanism: Replace old individuals\n            for i in range(self.pop_size):\n                if self.age[i] > self.max_age:\n                    population[i] = np.random.uniform(self.bounds_lb, self.bounds_ub, size=self.dim)\n                    fitness[i] = func(population[i])\n                    self.budget -= 1\n                    self.age[i] = 0 # Reset age\n\n        return f_opt, x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AgingArchiveDE scored 0.483 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e5b825b0-9c88-4c50-81a8-763202e8d5a8"], "operator": null, "metadata": {"aucs": [0.1723682179746715, 0.4897502700925417, 0.5876999292128006, 0.847780906509429, 0.48327137496086914, 0.7353750993103728, 0.32039248578888035, 0.4888262208033035, 0.606590850908133, 0.20923530013627267, 0.8562762884627992, 0]}}
{"id": "3dddade3-9b36-4195-a558-0b74b3776b95", "fitness": 0.2078630940581228, "name": "SimplifiedDE", "description": "Streamlined Differential Evolution with simplified mutation and adaptive crossover probability based on fitness improvement.", "code": "import numpy as np\n\nclass SimplifiedDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, CR=0.7):\n        \"\"\"\n        Initialize the Simplified Differential Evolution optimizer.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            F (float): Differential evolution scaling factor.\n            CR (float): Initial crossover rate.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using a simplified Differential Evolution strategy.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            # Differential Evolution part\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Mutation: Simplified version\n                idxs = np.random.choice(self.pop_size, 2, replace=False)\n                x_r1, x_r2 = population[idxs]\n                x_mutated = population[i] + self.F * (x_r1 - x_r2)\n\n                # Repair bounds\n                x_mutated = np.clip(x_mutated, self.bounds_lb, self.bounds_ub)\n\n                # Crossover\n                x_trial = np.copy(population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR:\n                        x_trial[j] = x_mutated[j]\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = x_trial\n\n                    if f_trial < f_opt:\n                        f_opt = f_trial\n                        x_opt = x_trial\n                        self.CR = min(1.0, self.CR + 0.1) #Adaptive CR\n                else:\n                    self.CR = max(0.0, self.CR - 0.05) # Adaptive CR\n\n        return f_opt, x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm SimplifiedDE scored 0.208 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a362ef3b-8eda-4e4d-a15e-c51758203b64"], "operator": null, "metadata": {"aucs": [0.05338409627310159, 0.07747934353798358, 0.21628271604661709, 0.15657297128446257, 0.16005296338310215, 0.15683722636585606, 0.24474719801529066, 0.17284752172131812, 0.14738497412155482, 0.1406019400016889, 0.13036944682515583, 0.9991572732501912, 0.10391687644686354, 0.15187790057607364, 0.13484905307676365, 0.18906752217590395, 0.1618694807998361, 0.17878506484462608, 0.1356963002271968, 0.44548201218886996]}}
{"id": "b1db93dc-7b7d-472e-9087-42f62566031a", "fitness": 0.532188075609371, "name": "SimplifiedDE", "description": "Simplified DE with adaptive mutation factor and stochastic ranking for constraint handling, focusing on efficiency.", "code": "import numpy as np\n\nclass SimplifiedDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, CR=0.9):\n        \"\"\"\n        Initialize the Simplified DE.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            CR (float): Differential evolution crossover rate.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.F = 0.5  # Initial mutation factor\n        self.F_decay = 0.999 # Decay rate for F\n        self.p_selection = 0.9 # Probability of selecting better individual\n\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using a simplified DE strategy.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            # Differential Evolution part\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n                x_mutated = population[i] + self.F * (x_r2 - x_r3)\n\n                # Crossover\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Repair bounds\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Stochastic Ranking Selection\n                if np.random.rand() < self.p_selection:\n                    if f_trial < fitness[i]:\n                        fitness[i] = f_trial\n                        population[i] = x_trial\n                        if f_trial < f_opt:\n                            f_opt = f_trial\n                            x_opt = x_trial\n                else:\n                    if f_trial < fitness[i]:\n                        fitness[i] = f_trial\n                        population[i] = x_trial\n                        if f_trial < f_opt:\n                            f_opt = f_trial\n                            x_opt = x_trial\n\n                self.F *= self.F_decay  # Gradually reduce the mutation factor\n\n        return f_opt, x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm SimplifiedDE scored 0.532 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a362ef3b-8eda-4e4d-a15e-c51758203b64"], "operator": null, "metadata": {"aucs": [0.15744362370165277, 0.5959733120828768, 0.5431994292921674, 0.8389085051765091, 0.4554290226310399, 0.5960657880434554, 0.29102689274438975, 0.4680027874685636, 0.5327183344296123, 0.1943848037509721, 0.8189321282490658, 0.9899759368914458, 0.6140528133430598, 0.3538902664701573, 0.7413971172570525, 0.6223296470563281, 0.37757954264549, 0.7427679265690887, 0.21396134063546512, 0.4957222937490262]}}
{"id": "ed5f6e29-f421-47b6-9faa-fd98ed21bfcf", "fitness": 0.5806187086793904, "name": "AdaptiveRestartDE", "description": "Adaptive Restart DE with archive and improved parameter adaptation based on success rate.", "code": "import numpy as np\n\nclass AdaptiveRestartDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F_initial=0.5, CR_initial=0.7, F_range=(0.3, 0.9), CR_range=(0.5, 1.0), stagnation_threshold=1000, archive_size=10):\n        \"\"\"\n        Initialize the Adaptive Restart Differential Evolution with archive and success-based parameter adaptation.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            F_initial (float): Initial differential evolution scaling factor.\n            CR_initial (float): Initial differential evolution crossover rate.\n            F_range (tuple): Range for adaptive F.\n            CR_range (tuple): Range for adaptive CR.\n            stagnation_threshold (int): Number of iterations without improvement before restart.\n            archive_size (int): Size of the archive to store successful F and CR values.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_initial\n        self.CR = CR_initial\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.stagnation_threshold = stagnation_threshold\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.best_fitness = np.inf\n        self.best_individual = None\n        self.stagnation_counter = 0\n        self.archive_F = []\n        self.archive_CR = []\n        self.archive_size = archive_size\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using the Adaptive Restart DE strategy with archive and success-based parameter adaptation.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.best_fitness = np.min(fitness)\n        self.best_individual = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            # Adaptive parameter control\n            # Sample F and CR from archive or random if archive is empty\n            if self.archive_F and self.archive_CR:\n                self.F = np.random.choice(self.archive_F)\n                self.CR = np.random.choice(self.archive_CR)\n            else:\n                self.F = np.random.uniform(self.F_range[0], self.F_range[1])\n                self.CR = np.random.uniform(self.CR_range[0], self.CR_range[1])\n                \n            successful_F = []\n            successful_CR = []\n\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n                x_mutated = population[i] + self.F * (x_r2 - x_r3)\n\n                # Crossover\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Repair bounds\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = x_trial\n\n                    if f_trial < self.best_fitness:\n                        self.best_fitness = f_trial\n                        self.best_individual = x_trial\n                        self.stagnation_counter = 0  # Reset stagnation counter\n                    \n                    successful_F.append(self.F)\n                    successful_CR.append(self.CR)\n                else:\n                    self.stagnation_counter += 1\n            \n            # Update archive with successful F and CR values\n            self.archive_F.extend(successful_F)\n            self.archive_CR.extend(successful_CR)\n\n            # Keep archive size limited\n            self.archive_F = self.archive_F[-self.archive_size:]\n            self.archive_CR = self.archive_CR[-self.archive_size:]\n\n            # Restart strategy\n            if self.stagnation_counter > self.stagnation_threshold:\n                # Re-initialize population\n                population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n\n                # Update best fitness and individual\n                min_fitness = np.min(fitness)\n                if min_fitness < self.best_fitness:\n                    self.best_fitness = min_fitness\n                    self.best_individual = population[np.argmin(fitness)]\n                self.stagnation_counter = 0  # Reset stagnation counter\n                self.archive_F = [] # Reset archive on restart\n                self.archive_CR = []\n\n        return self.best_fitness, self.best_individual", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveRestartDE scored 0.581 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["973560d9-714c-4930-9f5e-e8362047635b"], "operator": null, "metadata": {"aucs": [0.16450042188785086, 0.49651445617602263, 0.6163092147819738, 0.8171727288093326, 0.6206130961222287, 0.731091127380708, 0.3346678131630151, 0.5551735904754429, 0.5839517284661366, 0.30477723419997493, 0.828414222389743, 0.9981215213437887, 0.39413353282510066, 0.5805500551158965, 0.8723079076141584, 0.7252491143476155, 0.48529716967050407, 0.8168057349623741, 0.19535624272296748, 0.491367261132972]}}
{"id": "fe9d0eb0-6b88-44bf-8cd4-a850630fb3d4", "fitness": 0.28240563100943916, "name": "OBLAdaptiveDE", "description": "A differential evolution strategy with a novel mutation operator based on opposition-based learning and a self-adaptive mechanism for population size reduction.", "code": "import numpy as np\n\nclass OBLAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size_initial=50, F_initial=0.5, CR_initial=0.7, F_range=(0.3, 0.9), CR_range=(0.5, 1.0), pop_reduce_factor=0.95):\n        \"\"\"\n        Initialize the Opposition-Based Learning Adaptive Differential Evolution.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size_initial (int): Initial population size.\n            F_initial (float): Initial differential evolution scaling factor.\n            CR_initial (float): Initial differential evolution crossover rate.\n            F_range (tuple): Range for adaptive F.\n            CR_range (tuple): Range for adaptive CR.\n            pop_reduce_factor (float): Factor to reduce population size by when stagnant.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size_initial\n        self.pop_size_initial = pop_size_initial\n        self.F = F_initial\n        self.CR = CR_initial\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.pop_reduce_factor = pop_reduce_factor\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.best_fitness = np.inf\n        self.best_individual = None\n        self.stagnation_counter = 0\n        self.stagnation_threshold = int(budget / 20) # Dynamically adjusted stagnation threshold\n\n    def opposition_based_point(self, x):\n        \"\"\"\n        Generate an opposition-based point for a given point x.\n        \"\"\"\n        return self.bounds_lb + self.bounds_ub - x\n\n    def initialize_population(self, func):\n        \"\"\"\n        Initialize the population with opposition-based learning.\n        \"\"\"\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        opposition_population = np.array([self.opposition_based_point(x) for x in population])\n\n        fitness = np.array([func(x) for x in population])\n        opposition_fitness = np.array([func(x) for x in opposition_population])\n        self.budget -= 2 * self.pop_size\n\n        combined_population = np.concatenate((population, opposition_population))\n        combined_fitness = np.concatenate((fitness, opposition_fitness))\n\n        # Select the best individuals from the combined population\n        indices = np.argsort(combined_fitness)[:self.pop_size]\n        population = combined_population[indices]\n        fitness = combined_fitness[indices]\n\n        self.best_fitness = np.min(fitness)\n        self.best_individual = population[np.argmin(fitness)]\n        return population, fitness\n\n    def mutate(self, population, i):\n        \"\"\"\n        Perform a novel mutation operation using opposition-based learning.\n        \"\"\"\n        idxs = np.random.choice(self.pop_size, 3, replace=False)\n        x_r1, x_r2, x_r3 = population[idxs]\n        x_mutated = population[i] + self.F * (x_r2 - x_r3)\n        x_opposed = self.opposition_based_point(x_mutated) # OBL point of mutated vector\n        return x_opposed\n\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using the OBL Adaptive DE strategy.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n\n        self.stagnation_threshold = int(self.budget / 20)\n        population, fitness = self.initialize_population(func)\n\n        while self.budget > 0:\n            # Adaptive parameter control\n            self.F = np.random.uniform(self.F_range[0], self.F_range[1])\n            self.CR = np.random.uniform(self.CR_range[0], self.CR_range[1])\n\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Mutation (using opposition-based mutated vector)\n                x_mutated = self.mutate(population, i)\n\n                # Crossover\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Repair bounds\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = x_trial\n\n                    if f_trial < self.best_fitness:\n                        self.best_fitness = f_trial\n                        self.best_individual = x_trial\n                        self.stagnation_counter = 0  # Reset stagnation counter\n                else:\n                    self.stagnation_counter += 1\n\n            # Population size reduction\n            if self.stagnation_counter > self.stagnation_threshold and self.pop_size > 10: #Min pop size is 10\n                self.pop_size = int(self.pop_size * self.pop_reduce_factor)\n                # Select the best individuals\n                indices = np.argsort(fitness)[:self.pop_size]\n                population = population[indices]\n                fitness = fitness[indices]\n                print(f\"Population reduced to {self.pop_size}\")\n                self.stagnation_counter = 0  # Reset stagnation counter\n                self.stagnation_threshold = int(self.budget / 20)\n        return self.best_fitness, self.best_individual", "configspace": "", "generation": 2, "feedback": "The algorithm OBLAdaptiveDE scored 0.282 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["973560d9-714c-4930-9f5e-e8362047635b"], "operator": null, "metadata": {"aucs": [0.10877681146089324, 0.19389970710437587, 0.30577888004801757, 0.176907880661072, 0.19026856478500853, 0.19463275008563008, 0.21922801810967607, 0.2110732468867863, 0.18956493610914227, 0.15067517544034348, 0.22742669683922268, 0.9972992175795758, 0.3103750400093892, 0.18102127890674014, 0.6166131744954464, 0.2787100337805204, 0.2319283595677577, 0.24864294641204177, 0.16785941196075893, 0.44743048994638435]}}
{"id": "73a62839-57c4-4008-9c02-0076e86f67f3", "fitness": 0.3452154407576451, "name": "VelocityDE", "description": "Population-based algorithm using a velocity-based update rule inspired by Particle Swarm Optimization (PSO) combined with Differential Evolution (DE) mutation and crossover for exploration and exploitation.", "code": "import numpy as np\n\nclass VelocityDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, w=0.7, c1=1.4, c2=1.4, F=0.8, CR=0.9):\n        \"\"\"\n        Initialize the Velocity-based Differential Evolution algorithm.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            w (float): Inertia weight for velocity update.\n            c1 (float): Cognitive coefficient for personal best attraction.\n            c2 (float): Social coefficient for global best attraction.\n            F (float): Differential evolution scaling factor.\n            CR (float): Differential evolution crossover rate.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w = w\n        self.c1 = c1\n        self.c2 = c2\n        self.F = F\n        self.CR = CR\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using the Velocity-based DE strategy.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Initialize velocities\n        velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))\n\n        # Initialize personal best positions and values\n        personal_best_positions = population.copy()\n        personal_best_fitness = fitness.copy()\n\n        # Initialize global best position and value\n        global_best_index = np.argmin(fitness)\n        global_best_position = population[global_best_index].copy()\n        global_best_fitness = fitness[global_best_index]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Update velocity\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n\n                # Mutation (DE part)\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n                x_mutated = population[i] + self.F * (x_r2 - x_r3)\n\n                # Crossover (DE part)\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n                        \n                # Update position based on velocity\n                x_trial = x_trial + velocities[i]\n\n                # Repair bounds\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = x_trial\n\n                    # Update personal best\n                    if f_trial < personal_best_fitness[i]:\n                        personal_best_fitness[i] = f_trial\n                        personal_best_positions[i] = x_trial.copy()\n\n                        # Update global best\n                        if f_trial < global_best_fitness:\n                            global_best_fitness = f_trial\n                            global_best_position = x_trial.copy()\n                            \n        return global_best_fitness, global_best_position", "configspace": "", "generation": 2, "feedback": "The algorithm VelocityDE scored 0.345 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b0aa3b2c-6f4d-4f22-8f82-9837b36b61fc"], "operator": null, "metadata": {"aucs": [0.12616219007399287, 0.1950355806849421, 0.3305965933651698, 0.2234190455092695, 0.27075839662580403, 0.24269629624335798, 0.2918320622785785, 0.3321645721544574, 0.3705448277059322, 0.21319205226003468, 0.4194541733495466, 0.9979913957366953, 0.3008994998780157, 0.27323777480082034, 0.699427356096429, 0.41617708453493796, 0.2091639203033514, 0.3780821438635057, 0.16490815290191008, 0.4485656967861509]}}
{"id": "c297fd18-176c-49e1-b609-36eec09d8a8d", "fitness": 0.49459642268462806, "name": "AdaptiveRestartDE", "description": "Simplified Adaptive Restart DE with dynamic F/CR adaptation and a more aggressive restart.", "code": "import numpy as np\n\nclass AdaptiveRestartDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F_initial=0.5, CR_initial=0.7, stagnation_threshold=500):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_initial\n        self.CR = CR_initial\n        self.stagnation_threshold = stagnation_threshold\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.best_fitness = np.inf\n        self.best_individual = None\n        self.stagnation_counter = 0\n\n    def __call__(self, func):\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.best_fitness = np.min(fitness)\n        self.best_individual = population[np.argmin(fitness)]\n\n        archive = np.copy(population) # Archive for past individuals\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Adaptive F and CR based on population success\n                success_indices = np.where(fitness < np.mean(fitness))[0] # Indices of successful individuals\n                if len(success_indices) > 0:\n                    self.F = np.mean(np.random.uniform(0.3, 0.9, size=len(success_indices)))\n                    self.CR = np.mean(np.random.uniform(0.5, 1.0, size=len(success_indices)))\n                else:\n                    self.F = np.random.uniform(0.3, 0.9)\n                    self.CR = np.random.uniform(0.5, 1.0)\n\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n                x_mutated = population[i] + self.F * (x_r2 - x_r3)\n\n                # Crossover\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = x_trial\n                    archive[i] = x_trial # Update the archive\n\n                    if f_trial < self.best_fitness:\n                        self.best_fitness = f_trial\n                        self.best_individual = x_trial\n                        self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                # Restart: Re-initialize a portion of the population (more aggressive restart)\n                num_reinit = self.pop_size // 2  # Re-initialize half the population\n                reinit_indices = np.random.choice(self.pop_size, num_reinit, replace=False)\n                \n                population[reinit_indices] = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(num_reinit, self.dim))\n                fitness[reinit_indices] = [func(x) for x in population[reinit_indices]]\n                self.budget -= num_reinit\n\n                min_fitness = np.min(fitness)\n                if min_fitness < self.best_fitness:\n                    self.best_fitness = min_fitness\n                    self.best_individual = population[np.argmin(fitness)]\n                self.stagnation_counter = 0\n\n        return self.best_fitness, self.best_individual", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveRestartDE scored 0.495 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["973560d9-714c-4930-9f5e-e8362047635b"], "operator": null, "metadata": {"aucs": [0.15539089479334345, 0.2925540329119246, 0.4263335119167435, 0.8060748523297585, 0.6065628535179369, 0.762830984931307, 0.32709960406588423, 0.5242152058170568, 0.43555071708683624, 0.18991000453402496, 0.5793002996425515, 0.994329982823053, 0.3023241072961922, 0.31099028196409795, 0.7919374320073491, 0.5185503955991457, 0.392841199705665, 0.7990619147922753, 0.18610283073622136, 0.4899673472211936]}}
{"id": "46c27bf9-a91c-4045-93e0-341212bb0717", "fitness": 0.5972968748330817, "name": "AdaptiveHybridDECMASolver", "description": "Enhanced Hybrid DE-CMA with adaptive parameter control using success history adaptation for improved exploration and exploitation balance.", "code": "import numpy as np\n\nclass AdaptiveHybridDECMASolver:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, CR=0.9, memory_size=10):\n        \"\"\"\n        Initialize the Adaptive Hybrid DE-CMA Solver.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            F (float): Initial differential evolution scaling factor.\n            CR (float): Initial differential evolution crossover rate.\n            memory_size (int): Size of the memory for storing successful F and CR values.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.memory_size = memory_size\n        self.F_memory = np.full(memory_size, F)\n        self.CR_memory = np.full(memory_size, CR)\n        self.memory_idx = 0\n        self.success_F = []\n        self.success_CR = []\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using the Adaptive Hybrid DE-CMA strategy.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Initialize CMA-ES-like parameters\n        mu = np.mean(population, axis=0)\n        sigma = 0.1  # Initial step size\n        C = np.eye(self.dim)  # Covariance matrix\n        \n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            # Differential Evolution part\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Adaptive F and CR\n                F = np.random.choice(self.F_memory)\n                CR = np.random.choice(self.CR_memory)\n                \n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n                x_mutated = population[i] + F * (x_r2 - x_r3)\n\n                # Crossover\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Repair bounds\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    self.success_F.append(F)\n                    self.success_CR.append(CR)\n                    fitness[i] = f_trial\n                    population[i] = x_trial\n\n                    if f_trial < f_opt:\n                        f_opt = f_trial\n                        x_opt = x_trial\n\n            # CMA-ES adaptation (simplified)\n            mu_old = mu.copy()\n            mu = np.mean(population, axis=0)\n            \n            # Update covariance matrix and step size\n            z = (mu - mu_old) / sigma\n            C = (1 - 0.1) * C + 0.1 * np.outer(z, z) # Simplified rank-one update\n            sigma *= np.exp(0.05 * (np.linalg.norm(z) - 1)) # Simplified step size adaptation\n            \n            # Update F and CR memories\n            if self.success_F:\n                self.F_memory[self.memory_idx] = np.mean(self.success_F)\n                self.CR_memory[self.memory_idx] = np.mean(self.success_CR)\n                self.memory_idx = (self.memory_idx + 1) % self.memory_size\n                self.success_F = []\n                self.success_CR = []\n\n        return f_opt, x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveHybridDECMASolver scored 0.597 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b0aa3b2c-6f4d-4f22-8f82-9837b36b61fc"], "operator": null, "metadata": {"aucs": [0.19324304234601675, 0.5101795232063282, 0.5828095384825571, 0.8460383838328094, 0.5530876275954073, 0.7490622214296946, 0.347471572573658, 0.5935298159440916, 0.6739046311750458, 0.26260580297423797, 0.8581124187012261, 0.9899745183768855, 0.7089012838389146, 0.47531644666171435, 0.8954527301695043, 0.7033336223526685, 0.4932263213713689, 0.8078277616913107, 0.19662720729811012, 0.5052330266400836]}}
{"id": "a23bdcc7-e46e-4049-bd53-475b7399b517", "fitness": -Infinity, "name": "DEwithGPSurrogate", "description": "Combines Differential Evolution with a local search using a Gaussian process surrogate model to refine promising solutions and adaptively adjusts exploration and exploitation.", "code": "import numpy as np\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel\n\nclass DEwithGPSurrogate:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F_initial=0.5, CR_initial=0.7, F_range=(0.3, 0.9), CR_range=(0.5, 1.0), local_search_iterations=5, gp_noise_level=0.1):\n        \"\"\"\n        Initialize the DE with Gaussian Process Surrogate local search.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            F_initial (float): Initial differential evolution scaling factor.\n            CR_initial (float): Initial differential evolution crossover rate.\n            F_range (tuple): Range for adaptive F.\n            CR_range (tuple): Range for adaptive CR.\n            local_search_iterations (int): Number of local search iterations with GP.\n            gp_noise_level (float): Noise level for the Gaussian Process.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_initial\n        self.CR = CR_initial\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.local_search_iterations = local_search_iterations\n        self.gp_noise_level = gp_noise_level\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.best_fitness = np.inf\n        self.best_individual = None\n        self.population = None\n        self.fitness = None\n\n    def local_search(self, func, x_start):\n        \"\"\"\n        Perform local search using a Gaussian Process surrogate model.\n\n        Args:\n            func (callable): The black-box function to optimize.\n            x_start (np.ndarray): Starting point for the local search.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        x_current = x_start.copy()\n        f_current = func(x_current)\n        self.budget -= 1\n\n        X_train = x_current.reshape(1, -1)\n        y_train = np.array([f_current])\n\n        kernel = ConstantKernel(1.0, constant_value_bounds=\"fixed\") * RBF(length_scale=1.0, length_scale_bounds=\"fixed\")\n        gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=0, alpha=self.gp_noise_level**2)  # alpha is equivalent to noise variance\n        gp.fit(X_train, y_train)\n\n        for _ in range(self.local_search_iterations):\n            # Generate candidate points around the current best\n            x_candidates = np.random.normal(x_current, scale=0.5, size=(10, self.dim))\n            x_candidates = np.clip(x_candidates, self.bounds_lb, self.bounds_ub)\n\n            # Predict fitness using the Gaussian Process\n            y_pred, sigma = gp.predict(x_candidates, return_std=True)\n\n            # Select the best candidate based on expected improvement\n            improvement = -y_pred # We want to *minimize* the objective\n            best_idx = np.argmax(improvement)\n\n            x_next = x_candidates[best_idx]\n            f_next = func(x_next)\n            self.budget -= 1\n\n            if f_next < f_current:\n                x_current = x_next\n                f_current = f_next\n\n            # Update the Gaussian Process with the new data\n            X_train = np.vstack((X_train, x_next.reshape(1, -1)))\n            y_train = np.append(y_train, f_next)\n            gp.fit(X_train, y_train)\n\n            if self.budget <= 0:\n                break\n\n        return f_current, x_current\n\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using the DE with Gaussian Process Surrogate local search strategy.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        self.population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        self.best_fitness = np.min(self.fitness)\n        self.best_individual = self.population[np.argmin(self.fitness)]\n\n        while self.budget > 0:\n            # Adaptive parameter control\n            self.F = np.random.uniform(self.F_range[0], self.F_range[1])\n            self.CR = np.random.uniform(self.CR_range[0], self.CR_range[1])\n\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[idxs]\n                x_mutated = self.population[i] + self.F * (x_r2 - x_r3)\n\n                # Crossover\n                x_trial = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Repair bounds\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n\n                # Evaluation - replace with local search if promising\n                f_trial = np.inf\n                if np.random.rand() < 0.2: # Apply local search with probability 0.2\n                    f_trial, x_trial = self.local_search(func, x_trial)\n                else:\n                    f_trial = func(x_trial)\n                    self.budget -= 1\n\n\n\n                # Selection\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = x_trial\n\n                    if f_trial < self.best_fitness:\n                        self.best_fitness = f_trial\n                        self.best_individual = x_trial\n\n        return self.best_fitness, self.best_individual", "configspace": "", "generation": 2, "feedback": "An exception occurred: name 'ConstantKernel' is not defined.", "error": "", "parent_ids": ["973560d9-714c-4930-9f5e-e8362047635b"], "operator": null, "metadata": {}}
{"id": "7d3bde82-6dfe-4fc6-b208-dcce9b8467ce", "fitness": 0.4605704196808588, "name": "AdaptiveSimplifiedHybridDECMASolver", "description": "Introduces a learning rate for the scaling factor F in DE, adapting it based on the success of mutations, and dynamically adjusts CR based on population diversity.", "code": "import numpy as np\n\nclass AdaptiveSimplifiedHybridDECMASolver:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.8, CR=0.9, F_lr=0.1):\n        \"\"\"\n        Initialize the Adaptive Simplified Hybrid DE-CMA Solver.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            F (float): Initial differential evolution scaling factor.\n            CR (float): Initial differential evolution crossover rate.\n            F_lr (float): Learning rate for adapting the scaling factor F.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.F_lr = F_lr\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using an adaptive simplified Hybrid DE-CMA strategy.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n\n        success_history_F = []\n\n        while self.budget > 0:\n            # Differential Evolution part\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                  break\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n                x_mutated = population[i] + self.F * (x_r2 - x_r3)\n\n                # Crossover\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Repair bounds\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n                \n                # Selection\n                if f_trial < fitness[i]:\n                    success_history_F.append(self.F)\n                    fitness[i] = f_trial\n                    population[i] = x_trial\n\n                    if f_trial < f_opt:\n                        f_opt = f_trial\n                        x_opt = x_trial\n\n                    # Adapt F\n                    if success_history_F:\n                        self.F = self.F + self.F_lr * (np.mean(success_history_F) - self.F)\n                        self.F = np.clip(self.F, 0.1, 1.0)\n                    success_history_F = []  # Clear history after adaptation\n\n            # Adapt CR based on population diversity\n            distances = np.linalg.norm(population - np.mean(population, axis=0), axis=1)\n            diversity = np.mean(distances)\n            self.CR = np.clip(diversity / (self.bounds_ub - self.bounds_lb), 0.1, 0.99)\n\n        return f_opt, x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveSimplifiedHybridDECMASolver scored 0.461 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a362ef3b-8eda-4e4d-a15e-c51758203b64"], "operator": null, "metadata": {"aucs": [0.1608511318144008, 0.2282611220602726, 0.4352766714980828, 0.657928303613079, 0.3872016423777739, 0.5979917862891082, 0.3301598160638649, 0.4228595991246953, 0.36277826818695724, 0.26912086270121294, 0.5630588116468371, 0.9921425821017771, 0.3397937958480427, 0.3264948910644767, 0.7201872739372368, 0.6110615095403173, 0.37607860378371616, 0.7200189954803529, 0.19833921551308675, 0.5118035109718845]}}
{"id": "277d0807-b519-4d72-804e-6bd50f0705bd", "fitness": 0.0, "name": "AdaptiveRestartDE", "description": "Simplified Adaptive Restart DE with periodic orthogonal learning and reduced parameter adaptation overhead.", "code": "import numpy as np\n\nclass AdaptiveRestartDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, CR=0.7, stagnation_threshold=500, orthogonal_every=500):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.stagnation_threshold = stagnation_threshold\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.best_fitness = np.inf\n        self.best_individual = None\n        self.stagnation_counter = 0\n        self.orthogonal_every = orthogonal_every\n        self.eval_count = 0\n\n    def __orthogonal_design(self, population, fitness, num_samples=5):\n        \"\"\"Orthogonal design to generate diverse solutions around the best.\"\"\"\n        best_idx = np.argmin(fitness)\n        best_individual = population[best_idx]\n        \n        # Generate samples using orthogonal design (simplified)\n        directions = np.random.randn(num_samples, self.dim)\n        directions /= np.linalg.norm(directions, axis=1, keepdims=True)\n        amplitudes = np.random.uniform(-0.1, 0.1, size=(num_samples, 1)) # Reduced amplitude\n        \n        new_samples = best_individual + amplitudes * directions\n        new_samples = np.clip(new_samples, self.bounds_lb, self.bounds_ub)\n        return new_samples\n\n\n    def __call__(self, func):\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.eval_count += self.pop_size\n        self.budget -= self.pop_size\n\n        self.best_fitness = np.min(fitness)\n        self.best_individual = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n                x_mutated = population[i] + self.F * (x_r2 - x_r3)\n\n                # Crossover\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n                f_trial = func(x_trial)\n                self.eval_count += 1\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = x_trial\n\n                    if f_trial < self.best_fitness:\n                        self.best_fitness = f_trial\n                        self.best_individual = x_trial\n                        self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                # Restart: Re-initialize a portion of the population (more aggressive restart)\n                num_reinit = self.pop_size // 2  # Re-initialize half the population\n                reinit_indices = np.random.choice(self.pop_size, num_reinit, replace=False)\n                \n                population[reinit_indices] = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(num_reinit, self.dim))\n                fitness[reinit_indices] = [func(x) for x in population[reinit_indices]]\n                self.eval_count += num_reinit\n                self.budget -= num_reinit\n\n                min_fitness = np.min(fitness)\n                if min_fitness < self.best_fitness:\n                    self.best_fitness = min_fitness\n                    self.best_individual = population[np.argmin(fitness)]\n                self.stagnation_counter = 0\n\n            if self.eval_count % self.orthogonal_every == 0:\n                # Apply orthogonal design\n                new_samples = self.__orthogonal_design(population, fitness)\n                new_fitness = [func(x) for x in new_samples]\n                self.eval_count += len(new_fitness)\n                self.budget -= len(new_fitness)\n\n                for j in range(len(new_fitness)):\n                    if new_fitness[j] < self.best_fitness:\n                        self.best_fitness = new_fitness[j]\n                        self.best_individual = new_samples[j]\n                \n                # Replace worst individuals with orthogonal samples\n                worst_indices = np.argsort(fitness)[-len(new_samples):]\n                population[worst_indices] = new_samples\n                fitness[worst_indices] = new_fitness\n\n        return self.best_fitness, self.best_individual", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveRestartDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c297fd18-176c-49e1-b609-36eec09d8a8d"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "2a6f4b58-98c9-4211-b2ec-1a5ac2f4fa09", "fitness": 0.0, "name": "AdaptiveRestartDE", "description": "Simplified Adaptive Restart DE with periodic archive resets and adaptive F/CR, emphasizing exploration through wider parameter ranges and frequent restarts.", "code": "import numpy as np\n\nclass AdaptiveRestartDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F_initial=0.5, CR_initial=0.7, F_range=(0.1, 0.9), CR_range=(0.3, 0.9), stagnation_threshold=500, archive_size=5, restart_period=2000):\n        \"\"\"\n        Initialize the Adaptive Restart Differential Evolution with archive and success-based parameter adaptation.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            F_initial (float): Initial differential evolution scaling factor.\n            CR_initial (float): Initial differential evolution crossover rate.\n            F_range (tuple): Range for adaptive F.\n            CR_range (tuple): Range for adaptive CR.\n            stagnation_threshold (int): Number of iterations without improvement before restart.\n            archive_size (int): Size of the archive to store successful F and CR values.\n            restart_period (int): How often to force a restart\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_initial\n        self.CR = CR_initial\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.stagnation_threshold = stagnation_threshold\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.best_fitness = np.inf\n        self.best_individual = None\n        self.stagnation_counter = 0\n        self.archive_F = []\n        self.archive_CR = []\n        self.archive_size = archive_size\n        self.restart_period = restart_period\n        self.eval_count = 0\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using the Adaptive Restart DE strategy with archive and success-based parameter adaptation.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        self.eval_count += self.pop_size\n\n        self.best_fitness = np.min(fitness)\n        self.best_individual = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            successful_F = []\n            successful_CR = []\n\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Adaptive parameter control: simplified sampling\n                self.F = np.random.uniform(self.F_range[0], self.F_range[1])\n                self.CR = np.random.uniform(self.CR_range[0], self.CR_range[1])\n\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n                x_mutated = population[i] + self.F * (x_r2 - x_r3)\n\n                # Crossover\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Repair bounds\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n                self.eval_count += 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = x_trial\n\n                    if f_trial < self.best_fitness:\n                        self.best_fitness = f_trial\n                        self.best_individual = x_trial\n                        self.stagnation_counter = 0  # Reset stagnation counter\n                        successful_F.append(self.F)\n                        successful_CR.append(self.CR)\n                    else:\n                        self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter += 1\n\n            # Update archive with successful F and CR values\n            self.archive_F.extend(successful_F)\n            self.archive_CR.extend(successful_CR)\n\n            # Keep archive size limited\n            self.archive_F = self.archive_F[-self.archive_size:]\n            self.archive_CR = self.archive_CR[-self.archive_size:]\n\n            # Restart strategy: Periodic and stagnation-based\n            if self.stagnation_counter > self.stagnation_threshold or (self.eval_count % self.restart_period == 0):\n                # Re-initialize population\n                population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                self.eval_count += self.pop_size\n\n                # Update best fitness and individual\n                min_fitness = np.min(fitness)\n                if min_fitness < self.best_fitness:\n                    self.best_fitness = min_fitness\n                    self.best_individual = population[np.argmin(fitness)]\n                self.stagnation_counter = 0  # Reset stagnation counter\n                self.archive_F = [] # Reset archive on restart\n                self.archive_CR = []\n\n        return self.best_fitness, self.best_individual", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveRestartDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ed5f6e29-f421-47b6-9faa-fd98ed21bfcf"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "47806f7b-00c8-483c-b1ff-477b341f10c7", "fitness": 0.0, "name": "AdaptiveHybridDECMASolver", "description": "Adaptive Hybrid DE-CMA with orthogonal learning, success history adaptation, and covariance matrix adaptation for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveHybridDECMASolver:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, CR=0.9, memory_size=10):\n        \"\"\"\n        Initialize the Adaptive Hybrid DE-CMA Solver.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            F (float): Initial differential evolution scaling factor.\n            CR (float): Initial differential evolution crossover rate.\n            memory_size (int): Size of the memory for storing successful F and CR values.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.memory_size = memory_size\n        self.F_memory = np.full(memory_size, F)\n        self.CR_memory = np.full(memory_size, CR)\n        self.memory_idx = 0\n        self.success_F = []\n        self.success_CR = []\n        self.archive = []\n        self.archive_fitness = []\n        self.archive_size = pop_size  # Limit archive size\n        self.mu = None # Initialize mu as None\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using the Adaptive Hybrid DE-CMA strategy.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Initialize CMA-ES parameters\n        self.mu = np.mean(population, axis=0)  # Initialize mean\n        sigma = 0.1  # Initial step size\n        C = np.eye(self.dim)  # Covariance matrix\n        pc = np.zeros(self.dim)  # Evolution path for covariance matrix\n        ps = np.zeros(self.dim)  # Evolution path for step size\n        \n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            # Differential Evolution part\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Adaptive F and CR\n                F = np.random.choice(self.F_memory)\n                CR = np.random.choice(self.CR_memory)\n                \n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n                x_mutated = population[i] + F * (x_r2 - x_r3)\n\n                # Crossover\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Repair bounds\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    self.success_F.append(F)\n                    self.success_CR.append(CR)\n                    \n                    # Archive successful solutions\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i].copy())  # Archive the old one\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                        # Replace worst in archive if new one is better\n                        max_archive_index = np.argmax(self.archive_fitness)\n                        if fitness[i] < self.archive_fitness[max_archive_index]:\n                            self.archive[max_archive_index] = population[i].copy()\n                            self.archive_fitness[max_archive_index] = fitness[i]\n\n                    fitness[i] = f_trial\n                    population[i] = x_trial\n\n                    if f_trial < f_opt:\n                        f_opt = f_trial\n                        x_opt = x_trial\n\n            # Orthogonal learning (using archive)\n            if len(self.archive) > 0:\n                idx = np.random.randint(len(self.archive))\n                x_rand_archive = self.archive[idx]\n                x_orth = self.mu + np.random.rand() * (x_rand_archive - population[np.random.randint(self.pop_size)]) # Perturb from archive\n\n                x_orth = np.clip(x_orth, self.bounds_lb, self.bounds_ub)\n                f_orth = func(x_orth)\n                self.budget -= 1\n\n                if f_orth < f_opt:\n                    f_opt = f_orth\n                    x_opt = x_orth\n            \n            # CMA-ES adaptation (more complete)\n            mu_old = self.mu.copy()\n            self.mu = np.mean(population, axis=0)\n\n            # Step size adaptation\n            ps = (1 - 0.1) * ps + np.sqrt(0.1 * (2 - 0.1)) * (self.mu - mu_old) / sigma\n            sigma *= np.exp((0.4 / 0.3) * (np.linalg.norm(ps) / np.sqrt(self.dim) - 1))\n\n            # Covariance matrix adaptation\n            pc = (1 - 0.1) * pc + np.sqrt(0.1 * (2 - 0.1)) * (self.mu - mu_old) / sigma\n            C = (1 - 0.01) * C + 0.01 * (np.outer(pc, pc) + 0.05 * np.eye(self.dim))\n\n            # Ensure positive definiteness\n            try:\n                np.linalg.cholesky(C)\n            except np.linalg.LinAlgError:\n                C = np.eye(self.dim)\n\n            # Update F and CR memories\n            if self.success_F:\n                self.F_memory[self.memory_idx] = np.mean(self.success_F)\n                self.CR_memory[self.memory_idx] = np.mean(self.success_CR)\n                self.memory_idx = (self.memory_idx + 1) % self.memory_size\n                self.success_F = []\n                self.success_CR = []\n\n        return f_opt, x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveHybridDECMASolver scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["46c27bf9-a91c-4045-93e0-341212bb0717"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "9b364735-ddb4-48ce-8411-9b35b6062912", "fitness": 0.0, "name": "AdaptiveHybridDECMASolver", "description": "Adaptive Hybrid DE-CMA with dynamic population sizing, adaptive parameter control with success history, and orthogonal learning to improve exploration.", "code": "import numpy as np\n\nclass AdaptiveHybridDECMASolver:\n    def __init__(self, budget=10000, dim=10, pop_size=None, F=0.5, CR=0.9, memory_size=10, ortho_group_size=3):\n        \"\"\"\n        Initialize the Adaptive Hybrid DE-CMA Solver.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int, optional): The initial size of the population. If None, it's dynamically determined.\n            F (float): Initial differential evolution scaling factor.\n            CR (float): Initial differential evolution crossover rate.\n            memory_size (int): Size of the memory for storing successful F and CR values.\n            ortho_group_size (int): Size of the groups for orthogonal learning.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_initial = min(100, 4 + int(3 * np.log(dim))) if pop_size is None else pop_size  # Dynamic pop size\n        self.pop_size = self.pop_size_initial\n        self.F = F\n        self.CR = CR\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.memory_size = memory_size\n        self.F_memory = np.full(memory_size, F)\n        self.CR_memory = np.full(memory_size, CR)\n        self.memory_idx = 0\n        self.success_F = []\n        self.success_CR = []\n        self.ortho_group_size = ortho_group_size\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using the Adaptive Hybrid DE-CMA strategy.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Initialize CMA-ES-like parameters\n        mu = np.mean(population, axis=0)\n        sigma = 0.1  # Initial step size\n        C = np.eye(self.dim)  # Covariance matrix\n        \n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n\n            # Differential Evolution part\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # Adaptive F and CR\n                if self.success_F and self.success_CR:\n                    F = np.random.choice(self.F_memory)\n                    CR = np.random.choice(self.CR_memory)\n                else:\n                    F = self.F # Fallback to initial values if no success\n                    CR = self.CR\n\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n                x_mutated = population[i] + F * (x_r2 - x_r3)\n\n                # Crossover\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Repair bounds\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    self.success_F.append(F)\n                    self.success_CR.append(CR)\n                    fitness[i] = f_trial\n                    population[i] = x_trial\n\n                    if f_trial < f_opt:\n                        f_opt = f_trial\n                        x_opt = x_trial\n            \n            # Orthogonal Learning\n            for _ in range(2): # repeat a few times\n                group_indices = np.random.choice(self.pop_size, size=min(self.ortho_group_size, self.pop_size), replace=False)\n                group = population[group_indices]\n                \n                if len(group) > 1:\n                    centroid = np.mean(group, axis=0)\n                    directions = group - centroid\n                    \n                    # Select best individual's direction\n                    best_index = np.argmin(fitness[group_indices])\n                    best_direction = directions[best_index]\n                    \n                    # Explore along that direction\n                    step_size = np.random.uniform(-sigma, sigma) # adjust step size range\n                    new_point = centroid + step_size * best_direction\n                    new_point = np.clip(new_point, self.bounds_lb, self.bounds_ub)\n                    \n                    f_new = func(new_point)\n                    self.budget -= 1\n\n                    if f_new < f_opt:\n                        f_opt = f_new\n                        x_opt = new_point\n                        \n                    # Possibly replace a member of the group with the new point\n                    worst_index = np.argmax(fitness[group_indices])\n                    if f_new < fitness[group_indices][worst_index]:\n                        population[group_indices[worst_index]] = new_point\n                        fitness[group_indices[worst_index]] = f_new\n\n            # CMA-ES adaptation (simplified)\n            mu_old = mu.copy()\n            mu = np.mean(population, axis=0)\n            \n            # Update covariance matrix and step size\n            z = (mu - mu_old) / sigma\n            C = (1 - 0.1) * C + 0.1 * np.outer(z, z) # Simplified rank-one update\n            sigma *= np.exp(0.05 * (np.linalg.norm(z) - 1)) # Simplified step size adaptation\n\n            # Update F and CR memories\n            if self.success_F:\n                self.F_memory[self.memory_idx] = np.mean(self.success_F)\n                self.CR_memory[self.memory_idx] = np.mean(self.success_CR)\n                self.memory_idx = (self.memory_idx + 1) % self.memory_size\n                self.success_F = []\n                self.success_CR = []\n            \n            # Dynamic Population Size Adjustment (simplified)\n            if generation % 10 == 0: # Adjust every so often\n                if np.std(fitness) < 1e-4:  # If converged\n                    self.pop_size = min(self.pop_size * 1.2, 2 * self.pop_size_initial)  # Increase pop size\n                else:\n                    self.pop_size = max(int(self.pop_size * 0.9), self.pop_size_initial)  # Decrease pop size\n                \n                # Re-initialize if pop_size changed significantly\n                if self.pop_size != population.shape[0]:\n                    population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(int(self.pop_size), self.dim))\n                    fitness = np.array([func(x) for x in population])\n                    self.budget -= population.shape[0]\n                    mu = np.mean(population, axis=0)\n\n        return f_opt, x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveHybridDECMASolver scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["46c27bf9-a91c-4045-93e0-341212bb0717"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "5b94adf4-3b98-490e-a46b-ee07402dad14", "fitness": 0.35269794980677593, "name": "PSO_SA", "description": "Population-based algorithm combining aspects of Particle Swarm Optimization (PSO) and Simulated Annealing (SA) with adaptive temperature and inertia control.", "code": "import numpy as np\n\nclass PSO_SA:\n    def __init__(self, budget=10000, dim=10, pop_size=20, inertia=0.7, c1=1.5, c2=1.5, initial_temp=1.0, temp_decay=0.99):\n        \"\"\"\n        Initialize the PSO_SA algorithm.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the swarm/population.\n            inertia (float): Inertia weight for PSO.\n            c1 (float): Cognitive coefficient for PSO.\n            c2 (float): Social coefficient for PSO.\n            initial_temp (float): Initial temperature for SA.\n            temp_decay (float): Temperature decay rate for SA.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.inertia = inertia\n        self.c1 = c1\n        self.c2 = c2\n        self.initial_temp = initial_temp\n        self.temp_decay = temp_decay\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.temperature = initial_temp\n\n        self.best_fitness = np.inf\n        self.best_position = None\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using the PSO_SA algorithm.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population and velocities\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))  # Random initial velocities\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Initialize personal best positions and fitnesses\n        personal_best_positions = population.copy()\n        personal_best_fitness = fitness.copy()\n\n        # Initialize global best position and fitness\n        global_best_idx = np.argmin(fitness)\n        global_best_position = population[global_best_idx].copy()\n        global_best_fitness = fitness[global_best_idx]\n\n        self.best_fitness = global_best_fitness\n        self.best_position = global_best_position\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Update velocity\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n\n                # Update position\n                new_position = population[i] + velocities[i]\n\n                # Clip to bounds\n                new_position = np.clip(new_position, self.bounds_lb, self.bounds_ub)\n\n                # Evaluate new position\n                new_fitness = func(new_position)\n                self.budget -= 1\n\n                # Simulated Annealing acceptance criterion\n                delta_e = new_fitness - fitness[i]\n                if delta_e < 0 or np.random.rand() < np.exp(-delta_e / self.temperature):\n                    population[i] = new_position\n                    fitness[i] = new_fitness\n\n                    # Update personal best\n                    if new_fitness < personal_best_fitness[i]:\n                        personal_best_fitness[i] = new_fitness\n                        personal_best_positions[i] = new_position.copy()\n\n                        # Update global best\n                        if new_fitness < global_best_fitness:\n                            global_best_fitness = new_fitness\n                            global_best_position = new_position.copy()\n                            self.best_fitness = global_best_fitness\n                            self.best_position = global_best_position\n            \n            #Cooling schedule\n            self.temperature *= self.temp_decay\n\n            # Adaptive inertia (linearly decrease)\n            self.inertia = 0.9 - (0.9-0.4)*(1-(self.budget/10000))\n        return self.best_fitness, self.best_position", "configspace": "", "generation": 3, "feedback": "The algorithm PSO_SA scored 0.353 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["46c27bf9-a91c-4045-93e0-341212bb0717"], "operator": null, "metadata": {"aucs": [0.1449022010664046, 0.18486666615285485, 0.35843395948664925, 0.1717283446458594, 0.25302559562760973, 0.24513492623273425, 0.2474449589077682, 0.354990685247777, 0.32440132184483095, 0.192586954735532, 0.4067280507136879, 0.9979675907055519, 0.19848525737767597, 0.24734486878276907, 0.6489665624612523, 0.7461398361684219, 0.3024018840571838, 0.3768131367963896, 0.16171665411813652, 0.48987954100642894]}}
{"id": "2e866325-0b4f-4357-b12b-eb3fd19e01e1", "fitness": 0.5393755512687051, "name": "SimplifiedDE", "description": "Simplified DE with decaying mutation factor, clipped trial vectors, and direct replacement for improved exploitation.", "code": "import numpy as np\n\nclass SimplifiedDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, CR=0.9, F=0.5, F_decay=0.99):\n        \"\"\"\n        Initialize the Simplified DE.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            CR (float): Differential evolution crossover rate.\n            F (float): Initial mutation factor.\n            F_decay (float): Decay rate for F.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.F = F  # Initial mutation factor\n        self.F_decay = F_decay # Decay rate for F\n\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using a simplified DE strategy.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            # Differential Evolution part\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n                x_mutated = population[i] + self.F * (x_r2 - x_r3)\n\n                # Crossover\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Repair bounds\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Selection: Direct replacement if trial is better\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = x_trial\n                    if f_trial < f_opt:\n                        f_opt = f_trial\n                        x_opt = x_trial\n\n            self.F *= self.F_decay  # Gradually reduce the mutation factor\n\n        return f_opt, x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm SimplifiedDE scored 0.539 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b1db93dc-7b7d-472e-9087-42f62566031a"], "operator": null, "metadata": {"aucs": [0.14385648219166325, 0.5321520398725751, 0.5468186518224492, 0.8204720567728347, 0.4092262328045255, 0.6413787245421709, 0.3339451419286573, 0.4354999849008643, 0.44622750407410827, 0.3094381059401847, 0.8536403840163989, 0.9987679775697408, 0.5715075322768367, 0.4290322670681831, 0.8822691478602134, 0.5456731234220449, 0.39497239812949114, 0.7569714595432959, 0.21388168652528639, 0.5217801241125761]}}
{"id": "cd2eeeab-99d0-430b-aab2-0b4769129477", "fitness": 0.5948899756637901, "name": "AdaptiveMutationDE", "description": "DE with a self-adaptive mutation strategy that adjusts mutation strength based on the success of recent mutations.", "code": "import numpy as np\n\nclass AdaptiveMutationDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, CR=0.9, F_initial=0.5, F_decay=0.99):\n        \"\"\"\n        Initialize the Adaptive Mutation DE.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            CR (float): Differential evolution crossover rate.\n            F_initial (float): Initial mutation factor.\n            F_decay (float): Decay rate for F\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.F = F_initial  # Initial mutation factor\n        self.F_decay = F_decay # Decay rate for F\n        self.p_selection = 0.9 # Probability of selecting better individual\n        self.success_history = [] # Store successful mutation sizes\n\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using a DE strategy with adaptive mutation.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            # Differential Evolution part\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n\n                # Adaptive F:  Sample F from history if available, else use current F.\n                if self.success_history:\n                    F = np.random.choice(self.success_history)\n                else:\n                    F = self.F\n\n                x_mutated = population[i] + F * (x_r2 - x_r3)\n\n                # Crossover\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Repair bounds\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Stochastic Ranking Selection\n                if np.random.rand() < self.p_selection:\n                    if f_trial < fitness[i]:\n                        # Update success history\n                        self.success_history.append(F)\n                        if len(self.success_history) > 10: # Keep history size limited\n                            self.success_history.pop(0)\n\n                        fitness[i] = f_trial\n                        population[i] = x_trial\n                        if f_trial < f_opt:\n                            f_opt = f_trial\n                            x_opt = x_trial\n                else:\n                    if f_trial < fitness[i]:\n                        # Update success history - less important in this case\n                        fitness[i] = f_trial\n                        population[i] = x_trial\n                        if f_trial < f_opt:\n                            f_opt = f_trial\n                            x_opt = x_trial\n\n                self.F *= self.F_decay  # Gradually reduce the base mutation factor\n\n        return f_opt, x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveMutationDE scored 0.595 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b1db93dc-7b7d-472e-9087-42f62566031a"], "operator": null, "metadata": {"aucs": [0.20164751208923903, 0.551227881934728, 0.6269680764625861, 0.856066503145299, 0.5827539099719499, 0.7774592704629335, 0.3461842749431794, 0.5303907685349116, 0.5942519091070613, 0.2204031722346269, 0.8466625209390952, 0.9987346483278063, 0.6410526395770755, 0.46286601660022175, 0.9064401685379241, 0.702181601288903, 0.5012262556036639, 0.8307095101971828, 0.21527071014266075, 0.5053021631747515]}}
{"id": "4ffce193-1c2c-4316-98f3-065f4845c266", "fitness": 0.37761102416459574, "name": "MirroringSelfAdaptiveDE", "description": "Self-adaptive Differential Evolution with a mirroring strategy and dynamic population size adjustment.", "code": "import numpy as np\n\nclass MirroringSelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size_init=20):\n        \"\"\"\n        Initialize the Mirroring Self-Adaptive DE.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size_init (int): Initial population size.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size_init\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.archive = []\n        self.archive_size = 10\n\n        # Hyperparameter ranges\n        self.CR_min = 0.1\n        self.CR_max = 0.9\n        self.F_min = 0.1\n        self.F_max = 1.0\n        self.adapt_rate = 0.1\n\n        self.pop_size_min = 10\n        self.pop_size_max = 100\n        self.pop_adjust_rate = 0.05 # Rate at which population size changes\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using a self-adaptive DE strategy with mirroring.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population and control parameters\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        CR = np.random.uniform(self.CR_min, self.CR_max, size=self.pop_size)\n        F = np.random.uniform(self.F_min, self.F_max, size=self.pop_size)\n\n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            # Adaptive population size adjustment\n            if np.random.rand() < self.pop_adjust_rate:\n                # Calculate fitness variance\n                fitness_var = np.var(fitness)\n                if fitness_var > 1e-6:  # Avoid division by zero\n                    # Adjust pop size based on fitness variance\n                    pop_change = int(self.pop_size * (1 - np.exp(-fitness_var)))\n                    self.pop_size = max(self.pop_size_min, min(self.pop_size + pop_change, self.pop_size_max))\n                    # Repopulate if necessary\n                    if self.pop_size > population.shape[0]:\n                        num_new = self.pop_size - population.shape[0]\n                        new_population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(num_new, self.dim))\n                        new_fitness = np.array([func(x) for x in new_population])\n                        self.budget -= num_new\n                        population = np.vstack((population, new_population))\n                        fitness = np.concatenate((fitness, new_fitness))\n                        CR = np.concatenate((CR, np.random.uniform(self.CR_min, self.CR_max, size=num_new)))\n                        F = np.concatenate((F, np.random.uniform(self.F_min, self.F_max, size=num_new)))\n\n\n            # Differential Evolution part\n            for i in range(population.shape[0]):\n                if self.budget <= 0:\n                    break\n\n                # Mutation\n                idxs = np.random.choice(population.shape[0], 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n                x_mutated = population[i] + F[i] * (x_r2 - x_r3)\n\n                # Mirroring Strategy to handle boundary violations\n                x_mutated = np.where(x_mutated < self.bounds_lb, 2 * self.bounds_lb - x_mutated, x_mutated)\n                x_mutated = np.where(x_mutated > self.bounds_ub, 2 * self.bounds_ub - x_mutated, x_mutated)\n                x_mutated = np.clip(x_mutated, self.bounds_lb, self.bounds_ub)\n\n\n                # Crossover\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR[i] or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    # Success: adapt parameters\n                    CR[i] = self.adapt_rate * CR[i] + (1 - self.adapt_rate) * np.random.uniform(self.CR_min, self.CR_max)\n                    F[i] = self.adapt_rate * F[i] + (1 - self.adapt_rate) * np.random.uniform(self.F_min, self.F_max)\n\n                    fitness[i] = f_trial\n                    population[i] = x_trial\n                    if f_trial < f_opt:\n                        f_opt = f_trial\n                        x_opt = x_trial\n\n                else:\n                    # Failure: perturb parameters\n                    CR[i] = np.random.uniform(self.CR_min, self.CR_max)\n                    F[i] = np.random.uniform(self.F_min, self.F_max)\n\n\n        return f_opt, x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm MirroringSelfAdaptiveDE scored 0.378 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b1db93dc-7b7d-472e-9087-42f62566031a"], "operator": null, "metadata": {"aucs": [0.1447558617615028, 0.25164313832493046, 0.518468793926463, 0.3205557509768776, 0.2613794701400932, 0.35260888528241086, 0.2745780204994718, 0.2915149389143724, 0.28890299430974997, 0.2135287989007375, 0.3305825945111829, 0.9880509661075332, 0.3076781224131063, 0.251976206775098, 0.7440200443894128, 0.6283264322775011, 0.2923439414167539, 0.4187394154867383, 0.1824448209874585, 0.4901212858905192]}}
{"id": "d95c4333-a030-4edc-9d7c-a7d7274d6fa1", "fitness": 0.4923738642676139, "name": "AdaptiveRestartDE", "description": "Improved Adaptive Restart DE with dynamic population size adjustment, adaptive F/CR learning from successful individuals, and a more robust restart strategy based on fitness improvement.", "code": "import numpy as np\n\nclass AdaptiveRestartDE:\n    def __init__(self, budget=10000, dim=10, pop_size_initial=20, F_initial=0.5, CR_initial=0.7, stagnation_threshold=500, pop_size_min=10, pop_size_max=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size_initial\n        self.pop_size_initial = pop_size_initial\n        self.F = F_initial\n        self.CR = CR_initial\n        self.stagnation_threshold = stagnation_threshold\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.best_fitness = np.inf\n        self.best_individual = None\n        self.stagnation_counter = 0\n        self.pop_size_min = pop_size_min\n        self.pop_size_max = pop_size_max\n        self.archive_f = []\n        self.archive_cr = []\n\n    def __call__(self, func):\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.best_fitness = np.min(fitness)\n        self.best_individual = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            # Population size adaptation\n            if self.stagnation_counter > self.stagnation_threshold // 2:\n                self.pop_size = max(self.pop_size_min, self.pop_size // 2)  # Reduce pop size if stagnating\n            else:\n                self.pop_size = min(self.pop_size_max, self.pop_size + 1) # Increase pop size if improving\n\n\n            if population.shape[0] != self.pop_size:\n                old_pop_size = population.shape[0]\n                if self.pop_size > old_pop_size:\n                    # Add new individuals if pop size increased\n                    num_new = self.pop_size - old_pop_size\n                    new_individuals = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(num_new, self.dim))\n                    population = np.vstack((population, new_individuals))\n                    new_fitness = np.array([func(x) for x in new_individuals])\n                    fitness = np.concatenate((fitness, new_fitness))\n                    self.budget -= num_new\n\n                elif self.pop_size < old_pop_size:\n                    # Reduce population by removing worst individuals\n                    num_remove = old_pop_size - self.pop_size\n                    worst_indices = np.argsort(fitness)[-num_remove:]\n                    population = np.delete(population, worst_indices, axis=0)\n                    fitness = np.delete(fitness, worst_indices)\n\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Adaptive F and CR learning from successful individuals\n                success_indices = np.where(fitness < np.mean(fitness))[0]\n                \n                if len(success_indices) > 0:\n                    f_vals = np.random.uniform(0.3, 0.9, size=len(success_indices))\n                    cr_vals = np.random.uniform(0.5, 1.0, size=len(success_indices))\n\n                    self.F = np.mean(f_vals)\n                    self.CR = np.mean(cr_vals)\n\n                    self.archive_f.extend(f_vals)\n                    self.archive_cr.extend(cr_vals)\n\n                    if len(self.archive_f) > 100:\n                        self.archive_f = self.archive_f[-100:]\n                        self.archive_cr = self.archive_cr[-100:]\n\n                else:\n                    self.F = np.random.uniform(0.3, 0.9)\n                    self.CR = np.random.uniform(0.5, 1.0)\n\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n                x_mutated = population[i] + self.F * (x_r2 - x_r3)\n\n                # Crossover\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = x_trial\n\n                    if f_trial < self.best_fitness:\n                        self.best_fitness = f_trial\n                        self.best_individual = x_trial\n                        self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                # Restart and population re-evaluation\n                if np.random.rand() < 0.8:\n                    # More aggressive restart focusing on diversity\n                    num_reinit = self.pop_size // 2\n                    reinit_indices = np.random.choice(self.pop_size, num_reinit, replace=False)\n                    population[reinit_indices] = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(num_reinit, self.dim))\n                    fitness[reinit_indices] = [func(x) for x in population[reinit_indices]]\n                    self.budget -= num_reinit\n                else:\n                    #Restart from the best solution with gaussian noise\n                    noise = np.random.normal(0, 0.1, size=(self.pop_size, self.dim))\n                    population = self.best_individual + noise\n                    population = np.clip(population, self.bounds_lb, self.bounds_ub)\n                    fitness = [func(x) for x in population]\n                    self.budget -= self.pop_size\n\n                min_fitness = np.min(fitness)\n                if min_fitness < self.best_fitness:\n                    self.best_fitness = min_fitness\n                    self.best_individual = population[np.argmin(fitness)]\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter = 0\n\n\n        return self.best_fitness, self.best_individual", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveRestartDE scored 0.492 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c297fd18-176c-49e1-b609-36eec09d8a8d"], "operator": null, "metadata": {"aucs": [0.1910049411490783, 0.3324022736612725, 0.4148490621130678, 0.7146713096229662, 0.4765132253285175, 0.5821852263325478, 0.29625838481284683, 0.4262593653319031, 0.4078656894775885, 0.195775096946308, 0.7026674231708541, 0.9924471643531744, 0.4390729151609537, 0.4879385766113108, 0.8595267308671837, 0.5481156864680639, 0.38462698495527203, 0.676792259287339, 0.2198587763278741, 0.4986461933741574]}}
{"id": "a75ec888-6d34-4eb0-aff1-7b677d3d0eb6", "fitness": 0.5977768319879789, "name": "AdaptiveRestartDE", "description": "Enhanced Adaptive Restart DE with per-dimension learning rates and a more robust restart strategy using opposition-based learning.", "code": "import numpy as np\n\nclass AdaptiveRestartDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F_initial=0.5, CR_initial=0.7, stagnation_threshold=500, learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_initial\n        self.CR = CR_initial\n        self.stagnation_threshold = stagnation_threshold\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.best_fitness = np.inf\n        self.best_individual = None\n        self.stagnation_counter = 0\n        self.learning_rate = learning_rate  # Learning rate for per-dimension adaptation\n        self.F_history = []\n        self.CR_history = []\n        self.archive = None\n\n    def __call__(self, func):\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.best_fitness = np.min(fitness)\n        self.best_individual = population[np.argmin(fitness)]\n\n        self.archive = np.copy(population)  # Archive for past individuals\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Adaptive F and CR based on success history\n                if self.F_history and self.CR_history:\n                    self.F = np.clip(np.mean(self.F_history) + self.learning_rate * np.random.normal(0, 1), 0.1, 0.9)\n                    self.CR = np.clip(np.mean(self.CR_history) + self.learning_rate * np.random.normal(0, 1), 0.1, 1.0)\n                else:\n                    self.F = np.random.uniform(0.3, 0.9)\n                    self.CR = np.random.uniform(0.5, 1.0)\n\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n                x_mutated = population[i] + self.F * (x_r2 - x_r3)\n\n                # Crossover\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = x_trial\n                    self.archive[i] = x_trial  # Update the archive\n\n                    self.F_history.append(self.F)\n                    self.CR_history.append(self.CR)\n\n                    if f_trial < self.best_fitness:\n                        self.best_fitness = f_trial\n                        self.best_individual = x_trial\n                        self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                # Restart with Opposition-Based Learning\n                num_reinit = self.pop_size // 2\n                reinit_indices = np.random.choice(self.pop_size, num_reinit, replace=False)\n                \n                # Generate opposition solutions\n                opposition_solutions = self.bounds_lb + self.bounds_ub - population[reinit_indices]\n                \n                # Evaluate opposition solutions\n                opposition_fitness = np.array([func(x) for x in opposition_solutions])\n                self.budget -= num_reinit\n\n                # Replace if opposition is better\n                for j, idx in enumerate(reinit_indices):\n                    if opposition_fitness[j] < fitness[idx]:\n                        population[idx] = opposition_solutions[j]\n                        fitness[idx] = opposition_fitness[j]\n                        self.archive[idx] = opposition_solutions[j]\n                \n                min_fitness = np.min(fitness)\n                if min_fitness < self.best_fitness:\n                    self.best_fitness = min_fitness\n                    self.best_individual = population[np.argmin(fitness)]\n                self.stagnation_counter = 0\n                self.F_history = []  # Reset history after restart\n                self.CR_history = []\n\n        return self.best_fitness, self.best_individual", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveRestartDE scored 0.598 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c297fd18-176c-49e1-b609-36eec09d8a8d"], "operator": null, "metadata": {"aucs": [0.17474245538092315, 0.5231377081620354, 0.5723332361998549, 0.7724215963610092, 0.6854527894092516, 0.7069242426878934, 0.3730128378929075, 0.5575274783577493, 0.716299457976472, 0.25055608865560075, 0.8533108242028143, 0.9997482020739155, 0.6450586252124706, 0.5210796633797886, 0.8753972311761135, 0.7192783121304012, 0.41467888534558717, 0.8220250582345467, 0.2217633719718347, 0.5507885749484112]}}
{"id": "413ea00d-d413-4062-8af5-e45bf67bfe20", "fitness": -Infinity, "name": "OrthogonalDE", "description": "DE with orthogonal learning and a self-adaptive mutation strategy based on population diversity, coupled with a periodic orthogonal crossover to enhance exploration.", "code": "import numpy as np\n\nclass OrthogonalDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F_initial=0.5, CR=0.7, ortho_period=50):\n        \"\"\"\n        Initialize Orthogonal Differential Evolution.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            F_initial (float): Initial differential evolution scaling factor.\n            CR (float): Crossover rate.\n            ortho_period (int): Period for performing orthogonal crossover.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_initial\n        self.CR = CR\n        self.ortho_period = ortho_period\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.best_fitness = np.inf\n        self.best_individual = None\n        self.generation = 0\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using Orthogonal DE strategy.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.best_fitness = np.min(fitness)\n        self.best_individual = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            self.generation += 1\n\n            # Self-adaptive F based on population diversity\n            diversity = np.std(fitness)\n            self.F = self.F_initial * (1 + diversity)  # Scale F by diversity\n            self.F = np.clip(self.F, 0.1, 0.9)  # Keep F within reasonable bounds\n\n\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n                x_mutated = population[i] + self.F * (x_r2 - x_r3)\n\n                # Crossover\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Repair bounds\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = x_trial\n\n                    if f_trial < self.best_fitness:\n                        self.best_fitness = f_trial\n                        self.best_individual = x_trial\n\n            # Orthogonal Crossover every ortho_period generations\n            if self.generation % self.ortho_period == 0:\n                self.orthogonal_crossover(population, fitness, func)\n\n\n        return self.best_fitness, self.best_individual\n\n    def orthogonal_crossover(self, population, fitness, func):\n        \"\"\"\n        Performs orthogonal crossover on the population.\n        \"\"\"\n        \n        for i in range(self.pop_size):\n            if self.budget <= 0:\n                break\n\n            # Select two parents randomly\n            idxs = np.random.choice(self.pop_size, 2, replace=False)\n            parent1, parent2 = population[idxs]\n\n            # Create a new individual by combining the parents using orthogonal design\n            # Simple averaging as an example; more complex designs could be used\n            x_new = 0.5 * (parent1 + parent2)\n\n            # Repair bounds\n            x_new = np.clip(x_new, self.bounds_lb, self.bounds_ub)\n\n            # Evaluation\n            f_new = func(x_new)\n            self.budget -= 1\n\n            # Selection: replace the worst individual if the new one is better\n            if f_new < np.max(fitness):\n                worst_idx = np.argmax(fitness)\n                fitness[worst_idx] = f_new\n                population[worst_idx] = x_new\n\n                if f_new < self.best_fitness:\n                    self.best_fitness = f_new\n                    self.best_individual = x_new", "configspace": "", "generation": 3, "feedback": "An exception occurred: 'OrthogonalDE' object has no attribute 'F_initial'.", "error": "", "parent_ids": ["ed5f6e29-f421-47b6-9faa-fd98ed21bfcf"], "operator": null, "metadata": {}}
{"id": "0e19b792-23d6-4474-b0ad-4b0b6cceb4c2", "fitness": 0.37293149402284465, "name": "AdaptiveRestartDE", "description": "Adaptive Restart DE with success-history adaptation, orthogonal design for population initialization, and a Cauchy mutation operator for enhanced exploration.", "code": "import numpy as np\n\nclass AdaptiveRestartDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F_initial=0.5, CR_initial=0.7, F_range=(0.3, 0.9), CR_range=(0.5, 1.0), stagnation_threshold=1000, archive_size=10, orthogonal_design_samples=10):\n        \"\"\"\n        Initialize the Adaptive Restart Differential Evolution with archive, success-based parameter adaptation, orthogonal design initialization, and Cauchy mutation.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            F_initial (float): Initial differential evolution scaling factor.\n            CR_initial (float): Initial differential evolution crossover rate.\n            F_range (tuple): Range for adaptive F.\n            CR_range (tuple): Range for adaptive CR.\n            stagnation_threshold (int): Number of iterations without improvement before restart.\n            archive_size (int): Size of the archive to store successful F and CR values.\n            orthogonal_design_samples (int): Number of samples to use for orthogonal design initialization.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_initial\n        self.CR = CR_initial\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.stagnation_threshold = stagnation_threshold\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.best_fitness = np.inf\n        self.best_individual = None\n        self.stagnation_counter = 0\n        self.archive_F = []\n        self.archive_CR = []\n        self.archive_size = archive_size\n        self.orthogonal_design_samples = orthogonal_design_samples\n\n    def generate_orthogonal_design(self, num_samples, dim):\n        \"\"\"\n        Generate an orthogonal design using Latin Hypercube Sampling.\n\n        Args:\n            num_samples (int): Number of samples to generate.\n            dim (int): Dimensionality of the space.\n\n        Returns:\n            numpy.ndarray: Orthogonal design samples.\n        \"\"\"\n        # Latin Hypercube Sampling\n        points = np.zeros((num_samples, dim))\n        for i in range(dim):\n            points[:, i] = np.random.permutation(num_samples)\n        \n        # Normalize to [0, 1]\n        points = (points + np.random.rand(num_samples, dim)) / num_samples\n        \n        # Scale to the problem bounds\n        points = self.bounds_lb + points * (self.bounds_ub - self.bounds_lb)\n        return points\n        \n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using the Adaptive Restart DE strategy with archive, success-based parameter adaptation, and orthogonal initialization.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population using orthogonal design\n        if self.pop_size > self.orthogonal_design_samples:\n            orthogonal_samples = self.generate_orthogonal_design(self.orthogonal_design_samples, self.dim)\n            remaining_samples = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size - self.orthogonal_design_samples, self.dim))\n            population = np.vstack((orthogonal_samples, remaining_samples))\n        else:\n            population = self.generate_orthogonal_design(self.pop_size, self.dim)\n            \n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.best_fitness = np.min(fitness)\n        self.best_individual = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            # Adaptive parameter control\n            if self.archive_F and self.archive_CR:\n                self.F = np.random.choice(self.archive_F)\n                self.CR = np.random.choice(self.archive_CR)\n            else:\n                self.F = np.random.uniform(self.F_range[0], self.F_range[1])\n                self.CR = np.random.uniform(self.CR_range[0], self.CR_range[1])\n                \n            successful_F = []\n            successful_CR = []\n\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Mutation (Cauchy mutation)\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n                \n                # Cauchy mutation\n                cauchy_noise = self.F * np.random.standard_cauchy(size=self.dim)\n                x_mutated = population[i] + cauchy_noise * (x_r2 - x_r3)\n\n                # Crossover\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Repair bounds\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = x_trial\n\n                    if f_trial < self.best_fitness:\n                        self.best_fitness = f_trial\n                        self.best_individual = x_trial\n                        self.stagnation_counter = 0\n                    \n                    successful_F.append(self.F)\n                    successful_CR.append(self.CR)\n                else:\n                    self.stagnation_counter += 1\n            \n            # Update archive with successful F and CR values\n            self.archive_F.extend(successful_F)\n            self.archive_CR.extend(successful_CR)\n\n            # Keep archive size limited\n            self.archive_F = self.archive_F[-self.archive_size:]\n            self.archive_CR = self.archive_CR[-self.archive_size:]\n\n            # Restart strategy\n            if self.stagnation_counter > self.stagnation_threshold:\n                # Re-initialize population using orthogonal design\n                if self.pop_size > self.orthogonal_design_samples:\n                    orthogonal_samples = self.generate_orthogonal_design(self.orthogonal_design_samples, self.dim)\n                    remaining_samples = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size - self.orthogonal_design_samples, self.dim))\n                    population = np.vstack((orthogonal_samples, remaining_samples))\n                else:\n                    population = self.generate_orthogonal_design(self.pop_size, self.dim)\n\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n\n                # Update best fitness and individual\n                min_fitness = np.min(fitness)\n                if min_fitness < self.best_fitness:\n                    self.best_fitness = min_fitness\n                    self.best_individual = population[np.argmin(fitness)]\n                self.stagnation_counter = 0\n                self.archive_F = []\n                self.archive_CR = []\n\n        return self.best_fitness, self.best_individual", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveRestartDE scored 0.373 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ed5f6e29-f421-47b6-9faa-fd98ed21bfcf"], "operator": null, "metadata": {"aucs": [0.13485548020355254, 0.20231077755593208, 0.3195215215609787, 0.33352878300954436, 0.298148865240355, 0.4976262538961017, 0.3079037959209866, 0.35732322363031854, 0.276051255284251, 0.2085692874821734, 0.29600908019412653, 0.9974931476009351, 0.271901950495515, 0.2985739542065228, 0.6724618758863994, 0.45743275669457084, 0.2776877726922182, 0.5942313745876335, 0.17728438860172013, 0.4797143357130581]}}
{"id": "c429ab11-7c54-4967-aa48-cf3bb3e14cb9", "fitness": -Infinity, "name": "AdaptiveMutationDE", "description": "Simplified DE with adaptive mutation factor sampled from a truncated Cauchy distribution and stochastic ranking selection.", "code": "import numpy as np\nfrom scipy.stats import truncnorm\n\nclass AdaptiveMutationDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, CR=0.9, F_initial=0.5):\n        \"\"\"\n        Initialize the Adaptive Mutation DE with truncated Cauchy sampling for F.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            CR (float): Differential evolution crossover rate.\n            F_initial (float): Initial median for Cauchy distribution of F.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.F_initial = F_initial  # Median for Cauchy distribution of F\n        self.p_selection = 0.9 # Probability of selecting better individual\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using a DE strategy with adaptive mutation.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            # Differential Evolution part\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n\n                # Adaptive F: Sample F from truncated Cauchy distribution.\n                F = truncnorm.rvs(a=(0 - self.F_initial) / 0.1, b=(1 - self.F_initial) / 0.1, loc=self.F_initial, scale=0.1)\n\n                x_mutated = population[i] + F * (x_r2 - x_r3)\n\n                # Crossover\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Repair bounds\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Stochastic Ranking Selection\n                if np.random.rand() < self.p_selection:\n                    if f_trial < fitness[i]:\n                        fitness[i] = f_trial\n                        population[i] = x_trial\n                        if f_trial < f_opt:\n                            f_opt = f_trial\n                            x_opt = x_trial\n                else:\n                    if f_trial < fitness[i]:\n                        fitness[i] = f_trial\n                        population[i] = x_trial\n                        if f_trial < f_opt:\n                            f_opt = f_trial\n                            x_opt = x_trial\n\n        return f_opt, x_opt", "configspace": "", "generation": 4, "feedback": "An exception occurred: name 'truncnorm' is not defined.", "error": "", "parent_ids": ["cd2eeeab-99d0-430b-aab2-0b4769129477"], "operator": null, "metadata": {}}
{"id": "aa2adefe-5993-4c1f-b624-fb1d928b590a", "fitness": -Infinity, "name": "EnhancedSimplifiedDE", "description": "Enhanced Simplified DE with adaptive mutation factor based on the success rate of updates, archive to keep promising solutions, and a local search step using Nelder-Mead simplex method to refine the best solution.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedSimplifiedDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, CR=0.9, F=0.5, F_decay=0.99, archive_size=10):\n        \"\"\"\n        Initialize the Enhanced Simplified DE.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            CR (float): Differential evolution crossover rate.\n            F (float): Initial mutation factor.\n            F_decay (float): Decay rate for F.\n            archive_size (int): Size of the archive to store promising solutions.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.F = F  # Initial mutation factor\n        self.F_decay = F_decay # Decay rate for F\n        self.archive_size = archive_size\n        self.archive = []\n        self.archive_fitness = []\n        self.success_rate = 0.5 # Initial success rate for F adaptation\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using an enhanced simplified DE strategy.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n\n        num_success = 0\n        generations = 0\n\n        while self.budget > 0:\n            generations += 1\n            num_success_gen = 0\n            # Differential Evolution part\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n                x_mutated = population[i] + self.F * (x_r2 - x_r3)\n\n                # Crossover\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Repair bounds\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Selection: Direct replacement if trial is better\n                if f_trial < fitness[i]:\n                    num_success_gen += 1\n                    fitness[i] = f_trial\n                    population[i] = x_trial\n                    if f_trial < f_opt:\n                        f_opt = f_trial\n                        x_opt = x_trial\n\n                        # Update Archive\n                        if len(self.archive) < self.archive_size:\n                            self.archive.append(x_opt.copy())\n                            self.archive_fitness.append(f_opt)\n                        else:\n                            max_archive_fitness_index = np.argmax(self.archive_fitness)\n                            if f_opt < self.archive_fitness[max_archive_fitness_index]:\n                                self.archive[max_archive_fitness_index] = x_opt.copy()\n                                self.archive_fitness[max_archive_fitness_index] = f_opt\n            \n            # Adaptive F\n            self.success_rate = 0.9 * self.success_rate + 0.1 * (num_success_gen / self.pop_size)\n            if self.success_rate > 0.3:\n                self.F *= 1.1\n            else:\n                self.F *= 0.9\n\n            self.F = np.clip(self.F, 0.1, 1.0)\n\n            # F decay\n            self.F *= self.F_decay\n\n        # Local search with Nelder-Mead on best solution\n        if len(self.archive) > 0:\n          best_archive_index = np.argmin(self.archive_fitness)\n          x_local_search = self.archive[best_archive_index]\n          \n          remaining_budget = self.budget if self.budget > 0 else 1\n          \n          res = minimize(func, x_local_search, method='Nelder-Mead', bounds=np.array([self.bounds_lb, self.bounds_ub] * self.dim).reshape((self.dim, 2)), options={'maxfev': remaining_budget})\n          \n          if res.fun < f_opt:\n              f_opt = res.fun\n              x_opt = res.x\n              \n          self.budget -= res.nfev\n            \n        return f_opt, x_opt", "configspace": "", "generation": 4, "feedback": "An exception occurred: name 'minimize' is not defined.", "error": "", "parent_ids": ["2e866325-0b4f-4357-b12b-eb3fd19e01e1"], "operator": null, "metadata": {}}
{"id": "b393c5c8-4bf9-4e8f-b2d3-4c4690e97064", "fitness": 0.0, "name": "OrthogonalAdaptiveDE", "description": "DE with a success-history based F adaptation, orthogonal crossover, and a combined selection strategy of greedy and worst-replacement.", "code": "import numpy as np\n\nclass OrthogonalAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, CR=0.9, F_initial=0.5, F_decay=0.99, orthogonal_levels=3):\n        \"\"\"\n        Initialize the Orthogonal Adaptive DE.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            CR (float): Differential evolution crossover rate.\n            F_initial (float): Initial mutation factor.\n            F_decay (float): Decay rate for F\n            orthogonal_levels (int): Number of levels for orthogonal design.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.F = F_initial  # Initial mutation factor\n        self.F_decay = F_decay # Decay rate for F\n        self.success_history = [] # Store successful mutation sizes\n        self.orthogonal_levels = orthogonal_levels # Number of levels for orthogonal design\n\n    def generate_orthogonal_array(self, n, k, l):\n        \"\"\"\n        Generates an orthogonal array of strength 2.\n        n: number of runs (trials)\n        k: number of factors (columns)\n        l: number of levels\n        \"\"\"\n        if l == 2:\n            # Use a Hadamard matrix if l=2\n            H = self.hadamard(n)\n            oa = (H + 1) // 2  # Convert -1, 1 to 0, 1\n            oa = oa[:, :k]  # Select the first k columns\n        else:\n            # For l > 2, a more general OA construction is needed.\n            # This placeholder needs to be replaced with a proper OA construction.\n            oa = np.random.randint(0, l, size=(n, k))\n        return oa\n\n    def hadamard(self, n):\n        \"\"\"\n        Constructs a Hadamard matrix of size n.\n        \"\"\"\n        if n == 1:\n            return np.array([[1]])\n        if n % 2 != 0:\n            raise ValueError(\"Size must be a power of 2.\")\n        H = np.array([[1, 1], [1, -1]])\n        while H.shape[0] < n:\n            H = np.kron(H, np.array([[1, 1], [1, -1]]))\n        return H\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using a DE strategy with adaptive mutation and orthogonal crossover.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            # Differential Evolution part\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n\n                # Adaptive F:  Sample F from history if available, else use current F.\n                if self.success_history:\n                    F = np.random.choice(self.success_history)\n                else:\n                    F = self.F\n\n                x_mutated = population[i] + F * (x_r2 - x_r3)\n\n                # Repair bounds\n                x_mutated = np.clip(x_mutated, self.bounds_lb, self.bounds_ub)\n\n                # Orthogonal Crossover\n                oa_size = self.orthogonal_levels\n                if self.dim > 1:\n                    oa = self.generate_orthogonal_array(oa_size, self.dim, self.orthogonal_levels)\n                else:\n                    oa = np.random.randint(0, self.orthogonal_levels, size=(oa_size,self.dim))\n                \n                trials = np.zeros((oa_size, self.dim))\n\n                for j in range(oa_size):\n                    trial = np.copy(population[i])\n                    for k in range(self.dim):\n                        if oa[j, k] > 0.5 : # simple threshold\n                            trial[k] = x_mutated[k]\n                    trials[j, :] = trial\n\n                fitness_trials = np.array([func(x) for x in trials])\n                self.budget -= oa_size\n\n                best_trial_index = np.argmin(fitness_trials)\n                f_trial = fitness_trials[best_trial_index]\n                x_trial = trials[best_trial_index]\n\n                # Combined Selection: Greedy + Worst-Replacement\n                if f_trial < fitness[i]:\n                    # Greedy replacement\n                    self.success_history.append(F)\n                    if len(self.success_history) > 10: # Keep history size limited\n                        self.success_history.pop(0)\n                    fitness[i] = f_trial\n                    population[i] = x_trial\n                    if f_trial < f_opt:\n                        f_opt = f_trial\n                        x_opt = x_trial\n                else:\n                    # Worst replacement: Replace the worst individual in the population\n                    worst_index = np.argmax(fitness)\n                    if f_trial < fitness[worst_index]:\n                        fitness[worst_index] = f_trial\n                        population[worst_index] = x_trial\n\n\n                self.F *= self.F_decay  # Gradually reduce the base mutation factor\n\n        return f_opt, x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm OrthogonalAdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["cd2eeeab-99d0-430b-aab2-0b4769129477"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "be88da55-a628-4662-8b0c-2f6bdb7ebf14", "fitness": 0.0, "name": "CMAES_Orthogonal", "description": "Covariance Matrix Adaptation Evolution Strategy with orthogonal sampling and adaptive population sizing to improve exploration and exploitation.", "code": "import numpy as np\n\nclass CMAES_Orthogonal:\n    def __init__(self, budget=10000, dim=10, pop_size_initial=None, sigma_initial=0.5, cs=0.3, dampson=0.3, ccov_mu=0.3, ccov_1=0.0, pop_size_factor=4, orthogonal_fraction = 0.25):\n        self.budget = budget\n        self.dim = dim\n        self.sigma = sigma_initial\n        self.cs = cs\n        self.dampson = dampson\n        self.ccov_mu = ccov_mu\n        self.ccov_1 = ccov_1\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.best_fitness = np.inf\n        self.best_individual = None\n        self.pop_size_factor = pop_size_factor\n        self.orthogonal_fraction = orthogonal_fraction\n\n\n        if pop_size_initial is None:\n           self.pop_size = int(pop_size_factor * (3 + np.log(dim)))\n        else:\n            self.pop_size = pop_size_initial\n        \n        self.mu = (self.bounds_ub + self.bounds_lb) / 2 * np.ones(self.dim)\n\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n        self.mu_eff = self.pop_size / 4\n        self.weights = np.log(self.pop_size + 1/2) - np.log(np.arange(1, self.pop_size + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.ccov_mu = min(1, self.ccov_mu * (self.pop_size / np.sum(self.weights**2)))\n        self.ccov_1 = min(1, self.ccov_1 * (self.dim / (np.linalg.norm(self.ps)**2)))\n        self.d = 1 + 2 * max(0, np.sqrt((self.mu_eff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.dampson = self.dampson / self.d\n\n\n\n    def __call__(self, func):\n        while self.budget > 0:\n            # Sample population\n            z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n\n            # Generate orthogonal samples for a fraction of the population\n            num_orthogonal = int(self.orthogonal_fraction * self.pop_size)\n            if num_orthogonal > 0:\n                z[:num_orthogonal] = self.generate_orthogonal_samples(num_orthogonal, self.dim)\n\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mu + self.sigma * z @ C_sqrt.T\n            x = np.clip(x, self.bounds_lb, self.bounds_ub)\n\n            # Evaluate fitness\n            fitness = np.array([func(xi) for xi in x])\n            self.budget -= self.pop_size\n\n            # Update best solution\n            if np.min(fitness) < self.best_fitness:\n                self.best_fitness = np.min(fitness)\n                self.best_individual = x[np.argmin(fitness)].copy()\n\n            # Sort by fitness\n            idx = np.argsort(fitness)\n            x = x[idx]\n            z = z[idx]\n\n            # Update mean\n            mu_old = self.mu.copy()\n            self.mu = np.sum(self.weights[:, None] * x[:self.pop_size], axis=0)\n\n            # Update evolution paths\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mu_eff) * (np.linalg.inv(C_sqrt) @ (self.mu - mu_old))\n            self.pc = (1 - self.ccov_1) * self.pc + np.sqrt(self.ccov_1 * (2 - self.ccov_1) * self.mu_eff) * (self.mu - mu_old)\n\n            # Update covariance matrix\n            dC = (self.ccov_mu * np.sum(self.weights[:, None, None] * (z[:self.pop_size, :, None] @ z[:self.pop_size, None, :]), axis=0)\n                  + self.ccov_1 * self.pc[:, None] @ self.pc[None, :])\n            self.C = (1 - self.ccov_mu - self.ccov_1) * self.C + dC\n\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T  #Enforce Symmetry\n            \n            try:\n                np.linalg.cholesky(self.C) #Ensure positive Definiteness by skipping the update if not the case\n            except np.linalg.LinAlgError as e:\n                self.C = np.eye(self.dim)\n\n            #Update step size\n            self.sigma = self.sigma * np.exp((self.cs / self.dampson) * ((np.linalg.norm(self.ps) / self.chiN) - 1))\n\n        return self.best_fitness, self.best_individual\n\n    def generate_orthogonal_samples(self, num_samples, dim):\n        # Generate a set of mutually orthogonal vectors\n        ortho_vectors = np.zeros((num_samples, dim))\n        for i in range(num_samples):\n            v = np.random.randn(dim)\n            for j in range(i):\n                v = v - np.dot(v, ortho_vectors[j]) * ortho_vectors[j]\n            ortho_vectors[i] = v / np.linalg.norm(v)  # Normalize\n        return ortho_vectors", "configspace": "", "generation": 4, "feedback": "The algorithm CMAES_Orthogonal scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d95c4333-a030-4edc-9d7c-a7d7274d6fa1"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "706e36e2-fb5d-4b3c-8fb9-d68f3ebc0f85", "fitness": 0.11496720125710436, "name": "AdaptiveRestartDE", "description": "Adaptive Restart DE with a more informed restart strategy using fitness-distance ratio to guide population re-initialization and a dynamically adjusted stagnation threshold.", "code": "import numpy as np\n\nclass AdaptiveRestartDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F_initial=0.5, CR_initial=0.7, stagnation_threshold=500, learning_rate=0.1, restart_probability=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_initial\n        self.CR = CR_initial\n        self.stagnation_threshold = stagnation_threshold\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.best_fitness = np.inf\n        self.best_individual = None\n        self.stagnation_counter = 0\n        self.learning_rate = learning_rate\n        self.F_history = []\n        self.CR_history = []\n        self.archive = None\n        self.restart_probability = restart_probability\n\n    def __call__(self, func):\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.best_fitness = np.min(fitness)\n        self.best_individual = population[np.argmin(fitness)]\n\n        self.archive = np.copy(population)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Adaptive F and CR based on success history\n                if self.F_history and self.CR_history:\n                    self.F = np.clip(np.mean(self.F_history) + self.learning_rate * np.random.normal(0, 1), 0.1, 0.9)\n                    self.CR = np.clip(np.mean(self.CR_history) + self.learning_rate * np.random.normal(0, 1), 0.1, 1.0)\n                else:\n                    self.F = np.random.uniform(0.3, 0.9)\n                    self.CR = np.random.uniform(0.5, 1.0)\n\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n                x_mutated = population[i] + self.F * (x_r2 - x_r3)\n\n                # Crossover\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = x_trial\n                    self.archive[i] = x_trial\n\n                    self.F_history.append(self.F)\n                    self.CR_history.append(self.CR)\n\n                    if f_trial < self.best_fitness:\n                        self.best_fitness = f_trial\n                        self.best_individual = x_trial\n                        self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n            # Dynamic stagnation threshold adjustment\n            self.stagnation_threshold = max(50, int(0.05 * self.budget))\n\n            if self.stagnation_counter > self.stagnation_threshold or np.random.rand() < self.restart_probability:\n                # Restart strategy based on fitness-distance ratio\n                fitness_distance_ratio = np.abs(fitness - np.mean(fitness)) / np.mean(np.linalg.norm(population - self.best_individual, axis=1))\n                \n                # Individuals with high fitness-distance ratio are reinitialized\n                num_reinit = self.pop_size // 2\n                reinit_indices = np.argsort(fitness_distance_ratio)[-num_reinit:]  # Select individuals with highest ratio\n\n                # Reinitialize selected individuals\n                population[reinit_indices] = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(num_reinit, self.dim))\n                fitness[reinit_indices] = np.array([func(x) for x in population[reinit_indices]])\n                self.budget -= num_reinit\n\n                min_fitness = np.min(fitness)\n                if min_fitness < self.best_fitness:\n                    self.best_fitness = min_fitness\n                    self.best_individual = population[np.argmin(fitness)]\n                self.stagnation_counter = 0\n                self.F_history = []\n                self.CR_history = []\n\n        return self.best_fitness, self.best_individual", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveRestartDE scored 0.115 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a75ec888-6d34-4eb0-aff1-7b677d3d0eb6"], "operator": null, "metadata": {"aucs": [0.14274162143696245, 0.20215998233435062, 0]}}
{"id": "7219cd39-40ad-4c05-afe9-eb25f8099935", "fitness": 0.0, "name": "AdaptiveRestartDE", "description": "Enhanced Adaptive Restart DE with orthogonal learning and adaptive population diversity control for better exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveRestartDE:\n    def __init__(self, budget=10000, dim=10, pop_size_initial=20, F_initial=0.5, CR_initial=0.7, stagnation_threshold=500, pop_size_min=10, pop_size_max=50, diversity_threshold=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size_initial\n        self.pop_size_initial = pop_size_initial\n        self.F = F_initial\n        self.CR = CR_initial\n        self.stagnation_threshold = stagnation_threshold\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.best_fitness = np.inf\n        self.best_individual = None\n        self.stagnation_counter = 0\n        self.pop_size_min = pop_size_min\n        self.pop_size_max = pop_size_max\n        self.archive_f = []\n        self.archive_cr = []\n        self.diversity_threshold = diversity_threshold # Threshold for population diversity\n        self.orthogonal_pairs = 5 #Number of orthogonal pairs\n        \n\n    def __call__(self, func):\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.best_fitness = np.min(fitness)\n        self.best_individual = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            # Population size adaptation\n            if self.stagnation_counter > self.stagnation_threshold // 2:\n                self.pop_size = max(self.pop_size_min, self.pop_size // 2)  # Reduce pop size if stagnating\n            else:\n                self.pop_size = min(self.pop_size_max, self.pop_size + 1) # Increase pop size if improving\n\n\n            if population.shape[0] != self.pop_size:\n                old_pop_size = population.shape[0]\n                if self.pop_size > old_pop_size:\n                    # Add new individuals if pop size increased\n                    num_new = self.pop_size - old_pop_size\n                    new_individuals = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(num_new, self.dim))\n                    population = np.vstack((population, new_individuals))\n                    new_fitness = np.array([func(x) for x in new_individuals])\n                    fitness = np.concatenate((fitness, new_fitness))\n                    self.budget -= num_new\n\n                elif self.pop_size < old_pop_size:\n                    # Reduce population by removing worst individuals\n                    num_remove = old_pop_size - self.pop_size\n                    worst_indices = np.argsort(fitness)[-num_remove:]\n                    population = np.delete(population, worst_indices, axis=0)\n                    fitness = np.delete(fitness, worst_indices)\n\n            # Calculate population diversity\n            diversity = np.std(population)\n\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Adaptive F and CR learning from successful individuals\n                success_indices = np.where(fitness < np.mean(fitness))[0]\n                \n                if len(success_indices) > 0:\n                    f_vals = np.random.uniform(0.3, 0.9, size=len(success_indices))\n                    cr_vals = np.random.uniform(0.5, 1.0, size=len(success_indices))\n\n                    self.F = np.mean(f_vals)\n                    self.CR = np.mean(cr_vals)\n\n                    self.archive_f.extend(f_vals)\n                    self.archive_cr.extend(cr_vals)\n\n                    if len(self.archive_f) > 100:\n                        self.archive_f = self.archive_f[-100:]\n                        self.archive_cr = self.archive_cr[-100:]\n\n                else:\n                    self.F = np.random.uniform(0.3, 0.9)\n                    self.CR = np.random.uniform(0.5, 1.0)\n\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n                x_mutated = population[i] + self.F * (x_r2 - x_r3)\n\n                # Crossover\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = x_trial\n\n                    if f_trial < self.best_fitness:\n                        self.best_fitness = f_trial\n                        self.best_individual = x_trial\n                        self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n            # Orthogonal Learning\n            if self.stagnation_counter > self.stagnation_threshold // 4:\n                for _ in range(self.orthogonal_pairs):\n                    idxs = np.random.choice(self.pop_size, 2, replace=False)\n                    x1, x2 = population[idxs]\n                    x_mean = (x1 + x2) / 2\n                    orthogonal_direction = x1 - x2\n                    \n                    # Create new solutions along the orthogonal direction\n                    alpha = np.random.uniform(-0.1, 0.1)  # Small perturbation\n                    x_new = x_mean + alpha * orthogonal_direction\n                    x_new = np.clip(x_new, self.bounds_lb, self.bounds_ub)\n                    f_new = func(x_new)\n                    self.budget -= 1\n\n                    if f_new < self.best_fitness:\n                        self.best_fitness = f_new\n                        self.best_individual = x_new\n\n                    #Replace the worst individual with the new if it's better\n                    worst_index = np.argmax(fitness)\n                    if f_new < fitness[worst_index]:\n                        fitness[worst_index] = f_new\n                        population[worst_index] = x_new\n\n\n\n            if self.stagnation_counter > self.stagnation_threshold or diversity < self.diversity_threshold:\n                # Restart and population re-evaluation\n                if np.random.rand() < 0.8:\n                    # More aggressive restart focusing on diversity\n                    num_reinit = self.pop_size // 2\n                    reinit_indices = np.random.choice(self.pop_size, num_reinit, replace=False)\n                    population[reinit_indices] = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(num_reinit, self.dim))\n                    fitness[reinit_indices] = [func(x) for x in population[reinit_indices]]\n                    self.budget -= num_reinit\n                else:\n                    #Restart from the best solution with gaussian noise\n                    noise = np.random.normal(0, 0.1, size=(self.pop_size, self.dim))\n                    population = self.best_individual + noise\n                    population = np.clip(population, self.bounds_lb, self.bounds_ub)\n                    fitness = [func(x) for x in population]\n                    self.budget -= self.pop_size\n\n                min_fitness = np.min(fitness)\n                if min_fitness < self.best_fitness:\n                    self.best_fitness = min_fitness\n                    self.best_individual = population[np.argmin(fitness)]\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter = 0\n\n\n        return self.best_fitness, self.best_individual", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveRestartDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d95c4333-a030-4edc-9d7c-a7d7274d6fa1"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "fc6ed4e2-980a-4f89-8cba-6c9e4e757d50", "fitness": 0.3649926672756859, "name": "SelfOrganizingScouts", "description": "Self-organizing scouts that explore the search space and attract other scouts to promising regions by adjusting attraction radius and step size based on fitness improvements.", "code": "import numpy as np\n\nclass SelfOrganizingScouts:\n    def __init__(self, budget=10000, dim=10, num_scouts=10, initial_step_size=1.0, attraction_radius=2.0, step_decay=0.99, radius_decay=0.99, random_move_prob=0.05):\n        \"\"\"\n        Initialize the Self-Organizing Scouts algorithm.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            num_scouts (int): The number of scouts.\n            initial_step_size (float): Initial step size for scouts.\n            attraction_radius (float): Initial attraction radius.\n            step_decay (float): Decay rate for step size.\n            radius_decay (float): Decay rate for attraction radius.\n            random_move_prob (float): Probability of making a random move.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.num_scouts = num_scouts\n        self.step_size = initial_step_size\n        self.attraction_radius = attraction_radius\n        self.step_decay = step_decay\n        self.radius_decay = radius_decay\n        self.random_move_prob = random_move_prob\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using the Self-Organizing Scouts algorithm.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize scouts within bounds\n        scouts = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.num_scouts, self.dim))\n        fitness = np.array([func(x) for x in scouts])\n        self.budget -= self.num_scouts\n\n        f_opt = np.min(fitness)\n        x_opt = scouts[np.argmin(fitness)]\n\n        while self.budget > 0:\n            for i in range(self.num_scouts):\n                if self.budget <= 0:\n                    break\n\n                # Determine move based on attraction and random exploration\n                if np.random.rand() < self.random_move_prob:\n                    # Random move\n                    move = np.random.uniform(-self.step_size, self.step_size, size=self.dim)\n                else:\n                    # Find best scout within attraction radius\n                    distances = np.linalg.norm(scouts - scouts[i], axis=1)\n                    nearby_scouts_indices = np.where(distances < self.attraction_radius)[0]\n\n                    if len(nearby_scouts_indices) > 1: # Exclude itself\n                        nearby_fitness = fitness[nearby_scouts_indices]\n                        best_nearby_scout_index = nearby_scouts_indices[np.argmin(nearby_fitness)]\n                        if best_nearby_scout_index != i:\n                            move = (scouts[best_nearby_scout_index] - scouts[i]) * self.step_size\n                        else:\n                             move = np.random.uniform(-self.step_size, self.step_size, size=self.dim) # small random move\n                    else:\n                        move = np.random.uniform(-self.step_size, self.step_size, size=self.dim) # Small random move when isolated\n\n                # Move scout\n                new_scout = scouts[i] + move\n                new_scout = np.clip(new_scout, self.bounds_lb, self.bounds_ub)\n\n                # Evaluate new position\n                new_fitness = func(new_scout)\n                self.budget -= 1\n\n                # Update if better\n                if new_fitness < fitness[i]:\n                    scouts[i] = new_scout\n                    fitness[i] = new_fitness\n\n                    # Adjust parameters if better\n                    self.step_size *= self.step_decay\n                    self.attraction_radius *= self.radius_decay\n\n                    if new_fitness < f_opt:\n                        f_opt = new_fitness\n                        x_opt = new_scout\n\n            # Periodically increase step size and radius to avoid stagnation\n            if np.random.rand() < 0.01:\n                self.step_size = min(1.0, self.step_size / self.step_decay) # Avoid step_size going to inf\n                self.attraction_radius = min(2.0, self.attraction_radius / self.radius_decay)\n\n        return f_opt, x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm SelfOrganizingScouts scored 0.365 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["cd2eeeab-99d0-430b-aab2-0b4769129477"], "operator": null, "metadata": {"aucs": [0.10501594412873783, 0.31255779772968173, 0.44244027621683746, 0.9176957028711532, 0.2330368870870988, 0.6465069885252662, 0.24893859551665887, 0.1819179735091434, 0.3458579833714319, 0.15149956798684017, 0.286458363948782, 0.9995766570362494, 0.25729108528080347, 0.20940390175266232, 0.5831186508449986, 0.2887623037819743, 0.25464829453226256, 0.19787056877216158, 0.19794725807137115, 0.43930854454960344]}}
{"id": "1bc7fe3b-bc74-44db-ab95-14709e0887a3", "fitness": 0.3001926400020984, "name": "SimplifiedDE", "description": "Simplified DE with adaptive mutation factor based on population diversity and a more aggressive direct replacement strategy.", "code": "import numpy as np\n\nclass SimplifiedDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, CR=0.9, F_base=0.5):\n        \"\"\"\n        Initialize the Simplified DE.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            CR (float): Differential evolution crossover rate.\n            F_base (float): Base mutation factor.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.F_base = F_base  # Base mutation factor\n\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using a simplified DE strategy.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            # Calculate population diversity\n            diversity = np.std(population)\n\n            # Adaptive F: Scale base F by population diversity\n            F = self.F_base * (1 + diversity)  # Increase F when diversity is high\n\n            # Differential Evolution part\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n                x_mutated = population[i] + F * (x_r2 - x_r3)\n\n                # Crossover\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Repair bounds\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Selection: Direct replacement if trial is better\n                if f_trial <= fitness[i]: # More agressive replacement\n                    fitness[i] = f_trial\n                    population[i] = x_trial\n                    if f_trial < f_opt:\n                        f_opt = f_trial\n                        x_opt = x_trial\n\n        return f_opt, x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm SimplifiedDE scored 0.300 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2e866325-0b4f-4357-b12b-eb3fd19e01e1"], "operator": null, "metadata": {"aucs": [0.12821439737660334, 0.1815087308589427, 0.35843323116319104, 0.25240511923019826, 0.23003094714040828, 0.2939366221030124, 0.25522141598682346, 0.2837515971529726, 0.20464630257826077, 0.17685858749641226, 0.2510932739424685, 0.9963562888340391, 0.26087956547143853, 0.16605456675943164, 0.555904406896061, 0.31341417766286317, 0.24490007839733674, 0.22201430464529448, 0.16540035596161373, 0.462828830384596]}}
{"id": "40bea10b-c58d-4a93-8704-89ff50f28368", "fitness": 0.5380564710991736, "name": "AdaptiveDE", "description": "Adaptive DE with simplified parameter adaptation, population diversity maintenance through orthogonal design, and focused restarts based on stagnation detection.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F_initial=0.5, CR_initial=0.7, stagnation_threshold=500):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_initial\n        self.CR = CR_initial\n        self.stagnation_threshold = stagnation_threshold\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.best_fitness = np.inf\n        self.best_individual = None\n        self.stagnation_counter = 0\n\n    def generate_orthogonal_design(self, n, k):\n        H = np.ones((n, n))\n        for i in range(1, n):\n            for j in range(1, n):\n                if (i & j) != 0:\n                    H[i, j] = -1\n        design = (H[:k] + 1) / 2\n        return design * (self.bounds_ub - self.bounds_lb) + self.bounds_lb\n    \n    def __call__(self, func):\n        # Population initialization using orthogonal design\n        if self.pop_size <= self.dim:\n           self.pop_size = self.dim + 5\n        population = self.generate_orthogonal_design(self.pop_size, self.dim).reshape(self.pop_size, self.dim)\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.best_fitness = np.min(fitness)\n        self.best_individual = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Simplified F and CR adaptation\n                self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 0.9)\n                self.CR = np.clip(np.random.normal(0.7, 0.1), 0.1, 1.0)\n\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n                x_mutated = population[i] + self.F * (x_r2 - x_r3)\n\n                # Crossover\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = x_trial\n\n                    if f_trial < self.best_fitness:\n                        self.best_fitness = f_trial\n                        self.best_individual = x_trial\n                        self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                # Focused restart: Re-initialize around the best solution\n                noise = np.random.normal(0, 0.1, size=(self.pop_size, self.dim))\n                population = self.best_individual + noise\n                population = np.clip(population, self.bounds_lb, self.bounds_ub)\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                \n                self.best_fitness = np.min(fitness)\n                self.best_individual = population[np.argmin(fitness)]\n\n                self.stagnation_counter = 0\n\n        return self.best_fitness, self.best_individual", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDE scored 0.538 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d95c4333-a030-4edc-9d7c-a7d7274d6fa1"], "operator": null, "metadata": {"aucs": [0.22954287099469228, 0.48656880718080864, 0.5728517170782037, 0.8136310096479596, 0.5153893110358815, 0.7315904547940384, 0.3176542917862263, 0.489826183434529, 0.6332158100142002, 0.2284971078546627, 0.8108049509947245, 0.9987757488563973, 0.3949938385456214, 0.4485589903905528, 0.8714111212686244, 0.6036477948088278, 0]}}
{"id": "982ca243-61f6-435b-bb54-274c04eadd77", "fitness": 0.5934159372742351, "name": "AdaptiveMutationDE", "description": "Simplified Adaptive DE with adaptive mutation strength based on recent success and reduced parameter set.", "code": "import numpy as np\n\nclass AdaptiveMutationDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, CR=0.9, F_initial=0.5):\n        \"\"\"\n        Initialize the Adaptive Mutation DE.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            CR (float): Differential evolution crossover rate.\n            F_initial (float): Initial mutation factor.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.F = F_initial  # Initial mutation factor\n        self.success_history = [] # Store successful mutation sizes\n\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using a DE strategy with adaptive mutation.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            # Differential Evolution part\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n\n                # Adaptive F:  Sample F from history if available, else use current F.\n                if self.success_history:\n                    F = np.random.choice(self.success_history)\n                else:\n                    F = self.F\n\n                x_mutated = population[i] + F * (x_r2 - x_r3)\n\n                # Crossover\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Repair bounds\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Selection:  Greedy selection\n                if f_trial < fitness[i]:\n                    # Update success history\n                    self.success_history.append(F)\n                    if len(self.success_history) > 10: # Keep history size limited\n                        self.success_history.pop(0)\n\n                    fitness[i] = f_trial\n                    population[i] = x_trial\n                    if f_trial < f_opt:\n                        f_opt = f_trial\n                        x_opt = x_trial\n\n        return f_opt, x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveMutationDE scored 0.593 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["cd2eeeab-99d0-430b-aab2-0b4769129477"], "operator": null, "metadata": {"aucs": [0.18007502635949846, 0.6254948239212389, 0.6060708426925114, 0.7995991115569291, 0.5884481015565619, 0.7594194626470143, 0.3415523156957041, 0.551067365518844, 0.6085650176359418, 0.21787008057608248, 0.8457116228982412, 0.9969003388360533, 0.6795467974701839, 0.4692101247470136, 0.903773003790686, 0.6966632588284638, 0.466057280994827, 0.8092253667093362, 0.23093627011243556, 0.49213253293713677]}}
{"id": "a1e89cb9-4a00-4c7d-bf67-15db79771dd2", "fitness": 0.4191296328046426, "name": "SimplifiedDE", "description": "Simplified DE with adaptive mutation based on fitness improvement and reduced parameter count.", "code": "import numpy as np\n\nclass SimplifiedDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, CR=0.9, F=0.5):\n        \"\"\"\n        Initialize the Simplified DE.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            CR (float): Differential evolution crossover rate.\n            F (float): Initial mutation factor.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.F = F  # Initial mutation factor\n\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using a simplified DE strategy.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            # Differential Evolution part\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n                x_mutated = population[i] + self.F * (x_r2 - x_r3)\n\n                # Crossover\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Repair bounds\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Selection: Direct replacement if trial is better\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = x_trial\n                    if f_trial < f_opt:\n                        f_opt = f_trial\n                        x_opt = x_trial\n                        self.F = min(1.0, self.F * 1.2) # Adaptive F, increase it a bit\n\n                else:\n                    self.F = max(0.1, self.F * 0.9) # Decrease F if no improvement\n\n        return f_opt, x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm SimplifiedDE scored 0.419 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2e866325-0b4f-4357-b12b-eb3fd19e01e1"], "operator": null, "metadata": {"aucs": [0.12419583752718188, 0.2726647053516513, 0.4276931016038291, 0.3766820314002268, 0.3193882545215383, 0.4713154501370954, 0.2536152109180769, 0.3565198615729218, 0.30010386213288676, 0.17152656371635988, 0.7629845665290483, 0.919274105354314, 0.5402107169465064, 0.2349364044600788, 0.7963763564898012, 0.3764978762557757, 0.34279256492308163, 0.6674689814109618, 0.1779226339846004, 0.4904235708569141]}}
{"id": "e64f1b15-bfd8-4481-80e5-06f63c22554c", "fitness": 0.552611800165605, "name": "AdaptiveMutationDE", "description": "Implements a DE variant that adjusts mutation strength adaptively based on the magnitude of improvements observed, favoring smaller steps when close to optimum.", "code": "import numpy as np\n\nclass AdaptiveMutationDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, CR=0.9, F_initial=0.5, F_decay=0.99, F_min=0.05):\n        \"\"\"\n        Initialize the Adaptive Mutation DE.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            CR (float): Differential evolution crossover rate.\n            F_initial (float): Initial mutation factor.\n            F_decay (float): Decay rate for F\n            F_min (float): Minimum value for F, to avoid premature convergence.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.F = F_initial  # Initial mutation factor\n        self.F_decay = F_decay # Decay rate for F\n        self.F_min = F_min # Minimum mutation factor\n        self.p_selection = 0.9 # Probability of selecting better individual\n        self.success_history = [] # Store successful mutation sizes\n        self.improvement_threshold = 1e-6 # Threshold for detecting significant improvement\n\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using a DE strategy with adaptive mutation.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            # Differential Evolution part\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n\n                # Adaptive F:  Sample F from history if available, else use current F.\n                if self.success_history:\n                    F = np.random.choice(self.success_history)\n                else:\n                    F = self.F\n\n                x_mutated = population[i] + F * (x_r2 - x_r3)\n\n                # Crossover\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Repair bounds\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Stochastic Ranking Selection\n                if np.random.rand() < self.p_selection:\n                    if f_trial < fitness[i]:\n                        improvement = fitness[i] - f_trial\n                        # Update success history, scale F based on improvement\n                        if improvement > self.improvement_threshold:\n                             self.success_history.append(F)\n                        else:\n                             # If improvement is small, encourage smaller mutation steps\n                             self.success_history.append(F * 0.5)\n\n\n                        if len(self.success_history) > 10: # Keep history size limited\n                            self.success_history.pop(0)\n\n                        fitness[i] = f_trial\n                        population[i] = x_trial\n                        if f_trial < f_opt:\n                            f_opt = f_trial\n                            x_opt = x_trial\n                else:\n                    if f_trial < fitness[i]:\n                        fitness[i] = f_trial\n                        population[i] = x_trial\n                        if f_trial < f_opt:\n                            f_opt = f_trial\n                            x_opt = x_trial\n\n                self.F = max(self.F * self.F_decay, self.F_min)  # Gradually reduce the base mutation factor, but maintain minimum value\n\n        return f_opt, x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveMutationDE scored 0.553 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["cd2eeeab-99d0-430b-aab2-0b4769129477"], "operator": null, "metadata": {"aucs": [0.20315682187924156, 0.5665209771509917, 0.5746019535225764, 0.8145454473014616, 0.4637534614285109, 0.6741316178809219, 0.36627365459014705, 0.5129401594167069, 0.5960240817038709, 0.21496778356721868, 0.862851342768422, 0.997166075186583, 0.5919684848163647, 0.45355158094023784, 0.649696162862996, 0.6338473621819011, 0.483994432429376, 0.665577851572484, 0.2294295835754906, 0.4972371685365964]}}
{"id": "a159e820-3908-4a5b-a691-09eff1fe9e9d", "fitness": -Infinity, "name": "AdaptiveDecayDE", "description": "Adaptive DE with a decaying exploration rate and a self-adjusting mutation factor based on the population's standard deviation and recent success.", "code": "import numpy as np\n\nclass AdaptiveDecayDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, CR=0.9, F_initial=0.7, exploration_decay=0.999):\n        \"\"\"\n        Initialize the Adaptive Decay DE.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            CR (float): Differential evolution crossover rate.\n            F_initial (float): Initial mutation factor.\n            exploration_decay (float): Decay rate for exploration range.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.F = F_initial\n        self.exploration_decay = exploration_decay\n        self.exploration_range = self.bounds_ub - self.bounds_lb\n        self.success_history = []\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using a DE strategy with adaptive mutation and decaying exploration.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            # Differential Evolution part\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n\n                # Adaptive F: Adjust F based on population diversity (std dev)\n                std_dev = np.std(population)\n                self.F = self.F_initial * (1 + std_dev) # Larger std_dev -> larger F.\n\n                # Sample from history if available\n                if self.success_history:\n                    F = np.random.choice(self.success_history)\n                else:\n                    F = self.F\n\n                x_mutated = population[i] + F * (x_r2 - x_r3)\n\n                # Crossover\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Repair bounds using decaying exploration range\n                lb = x_opt - self.exploration_range / 2\n                ub = x_opt + self.exploration_range / 2\n                x_trial = np.clip(x_trial, lb, ub) # Clip to shrinking box around best.\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Selection:  Greedy selection\n                if f_trial < fitness[i]:\n                    # Update success history\n                    self.success_history.append(F)\n                    if len(self.success_history) > 10: # Keep history size limited\n                        self.success_history.pop(0)\n\n                    fitness[i] = f_trial\n                    population[i] = x_trial\n                    if f_trial < f_opt:\n                        f_opt = f_trial\n                        x_opt = x_trial\n\n            # Decay the exploration range\n            self.exploration_range *= self.exploration_decay\n\n        return f_opt, x_opt", "configspace": "", "generation": 5, "feedback": "An exception occurred: 'AdaptiveDecayDE' object has no attribute 'F_initial'.", "error": "", "parent_ids": ["982ca243-61f6-435b-bb54-274c04eadd77"], "operator": null, "metadata": {}}
{"id": "0b9ea172-16ac-4c4a-bf67-87521297ba34", "fitness": 0.0, "name": "SocialForagingOptimization", "description": "Population-based search inspired by social foraging, with adaptive step sizes based on individual and group success.", "code": "import numpy as np\n\nclass SocialForagingOptimization:\n    def __init__(self, budget=10000, dim=10, pop_size=20, alpha=0.1, beta=0.01, gamma=0.001):\n        \"\"\"\n        Initialize the Social Foraging Optimization algorithm.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            alpha (float): Individual learning rate.\n            beta (float): Social learning rate.\n            gamma (float): Attraction to best individual.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.alpha = alpha\n        self.beta = beta\n        self.gamma = gamma\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using a social foraging strategy.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            # Foraging behavior\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Individual learning: explore around the current position\n                step_individual = self.alpha * np.random.uniform(-1, 1, size=self.dim)\n                x_individual = population[i] + step_individual\n                x_individual = np.clip(x_individual, self.bounds_lb, self.bounds_ub)\n                f_individual = func(x_individual)\n                self.budget -= 1\n\n                # Social learning: move towards a randomly chosen neighbor\n                neighbor_idx = np.random.choice([j for j in range(self.pop_size) if j != i])\n                step_social = self.beta * (population[neighbor_idx] - population[i])\n                x_social = population[i] + step_social\n                x_social = np.clip(x_social, self.bounds_lb, self.bounds_ub)\n                f_social = func(x_social)\n                self.budget -= 1\n\n                # Attraction to best individual\n                step_best = self.gamma * (x_opt - population[i])\n                x_best_attracted = population[i] + step_best\n                x_best_attracted = np.clip(x_best_attracted, self.bounds_lb, self.bounds_ub)\n                f_best_attracted = func(x_best_attracted)\n                self.budget -= 1\n\n\n                # Selection: update position based on best outcome\n                if f_individual < fitness[i] and f_individual <= f_social and f_individual <= f_best_attracted:\n                    population[i] = x_individual\n                    fitness[i] = f_individual\n                elif f_social < fitness[i] and f_social <= f_individual and f_social <= f_best_attracted:\n                    population[i] = x_social\n                    fitness[i] = f_social\n                elif f_best_attracted < fitness[i] and f_best_attracted <= f_individual and f_best_attracted <= f_social:\n                     population[i] = x_best_attracted\n                     fitness[i] = f_best_attracted\n\n                # Update the global best\n                if fitness[i] < f_opt:\n                    f_opt = fitness[i]\n                    x_opt = population[i]\n\n        return f_opt, x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm SocialForagingOptimization scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e64f1b15-bfd8-4481-80e5-06f63c22554c"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "efcbe884-8a1a-460b-b35e-70c33c3ff08e", "fitness": -Infinity, "name": "OrthogonalAdaptiveDE", "description": "Adaptive Differential Evolution with orthogonal learning, using an orthogonal array to sample the search space and refine promising regions.", "code": "import numpy as np\nfrom pyDOE import lhs\n\nclass OrthogonalAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, CR=0.9, F_initial=0.5, F_decay=0.99, F_min=0.05, orthogonal_sample_size=5):\n        \"\"\"\n        Initialize the Orthogonal Adaptive DE.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            CR (float): Differential evolution crossover rate.\n            F_initial (float): Initial mutation factor.\n            F_decay (float): Decay rate for F\n            F_min (float): Minimum value for F, to avoid premature convergence.\n            orthogonal_sample_size (int): Number of samples in the orthogonal array.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.F = F_initial  # Initial mutation factor\n        self.F_decay = F_decay # Decay rate for F\n        self.F_min = F_min # Minimum mutation factor\n        self.orthogonal_sample_size = orthogonal_sample_size\n        self.p_selection = 0.9 # Probability of selecting better individual\n        self.success_history = [] # Store successful mutation sizes\n        self.improvement_threshold = 1e-6 # Threshold for detecting significant improvement\n\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using a DE strategy with adaptive mutation and orthogonal learning.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            # Differential Evolution part\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n\n                # Adaptive F:  Sample F from history if available, else use current F.\n                if self.success_history:\n                    F = np.random.choice(self.success_history)\n                else:\n                    F = self.F\n\n                x_mutated = population[i] + F * (x_r2 - x_r3)\n\n                # Crossover\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Repair bounds\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n\n                # Orthogonal Learning\n                if np.random.rand() < 0.1: # Apply orthogonal learning with a probability\n                    orthogonal_population = self.generate_orthogonal_sample(x_trial, self.orthogonal_sample_size)\n                    orthogonal_fitness = np.array([func(x) for x in orthogonal_population])\n                    self.budget -= self.orthogonal_sample_size\n\n                    best_orthogonal_index = np.argmin(orthogonal_fitness)\n                    if orthogonal_fitness[best_orthogonal_index] < fitness[i]:\n                        x_trial = orthogonal_population[best_orthogonal_index]\n                        fitness[i] = orthogonal_fitness[best_orthogonal_index]\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Stochastic Ranking Selection\n                if np.random.rand() < self.p_selection:\n                    if f_trial < fitness[i]:\n                        improvement = fitness[i] - f_trial\n                        # Update success history, scale F based on improvement\n                        if improvement > self.improvement_threshold:\n                             self.success_history.append(F)\n                        else:\n                             # If improvement is small, encourage smaller mutation steps\n                             self.success_history.append(F * 0.5)\n\n\n                        if len(self.success_history) > 10: # Keep history size limited\n                            self.success_history.pop(0)\n\n                        fitness[i] = f_trial\n                        population[i] = x_trial\n                        if f_trial < f_opt:\n                            f_opt = f_trial\n                            x_opt = x_trial\n                else:\n                    if f_trial < fitness[i]:\n                        fitness[i] = f_trial\n                        population[i] = x_trial\n                        if f_trial < f_opt:\n                            f_opt = f_trial\n                            x_opt = x_trial\n\n                self.F = max(self.F * self.F_decay, self.F_min)  # Gradually reduce the base mutation factor, but maintain minimum value\n\n        return f_opt, x_opt\n\n    def generate_orthogonal_sample(self, center, num_samples):\n        \"\"\"\n        Generates an orthogonal sample around the given center point within the bounds.\n\n        Args:\n            center (np.ndarray): The center point for the orthogonal sample.\n            num_samples (int): The number of samples to generate.\n\n        Returns:\n            np.ndarray: An array of orthogonal samples.\n        \"\"\"\n        # Generate Latin Hypercube Sample\n        lhs_samples = lhs(self.dim, samples=num_samples)\n\n        # Scale and shift the samples to be within a reasonable range around the center\n        radius = 0.5  # Radius around the center point\n        scaled_samples = center + radius * (lhs_samples * 2 - 1)  # Scale to [-radius, radius] and shift\n\n        # Clip the samples to ensure they are within the bounds\n        clipped_samples = np.clip(scaled_samples, self.bounds_lb, self.bounds_ub)\n\n        return clipped_samples", "configspace": "", "generation": 5, "feedback": "An exception occurred: No module named 'pyDOE'.", "error": "", "parent_ids": ["e64f1b15-bfd8-4481-80e5-06f63c22554c"], "operator": null, "metadata": {}}
{"id": "dcb83ec9-711c-4e5b-847a-55e52d712c1d", "fitness": 0.19503883728685706, "name": "NeighborhoodMutation", "description": "Neighborhood-based mutation with adaptive step size control using exponential decay and random restarts to escape local optima.", "code": "import numpy as np\n\nclass NeighborhoodMutation:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_step_size=0.1, decay_rate=0.99):\n        \"\"\"\n        Initialize the Neighborhood Mutation algorithm.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The number of solutions maintained.\n            initial_step_size (float): The initial step size for the mutation.\n            decay_rate (float): The decay rate for the step size.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.initial_step_size = initial_step_size\n        self.decay_rate = decay_rate\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.population = None\n        self.fitness = None\n        self.step_size = initial_step_size\n\n    def initialize_population(self, func):\n        \"\"\"Initialize the population randomly within the bounds.\"\"\"\n        self.population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n    def mutate(self, x):\n        \"\"\"Mutate a solution by adding a random vector from a normal distribution.\"\"\"\n        mutation = np.random.normal(0, self.step_size, self.dim)\n        x_mutated = x + mutation\n        return np.clip(x_mutated, self.bounds_lb, self.bounds_ub)\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using neighborhood mutation with adaptive step size.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        self.initialize_population(func)\n\n        f_opt = np.min(self.fitness)\n        x_opt = self.population[np.argmin(self.fitness)]\n        stagnation_counter = 0\n        max_stagnation = 500 # Maximum iterations to be stuck before restart\n\n        while self.budget > 0:\n            # Select a solution to mutate (e.g., randomly or based on fitness)\n            i = np.random.randint(self.pop_size)\n            x = self.population[i]\n\n            # Mutate the solution\n            x_mutated = self.mutate(x)\n\n            # Evaluate the mutated solution\n            f_mutated = func(x_mutated)\n            self.budget -= 1\n\n            # Selection: Greedy selection\n            if f_mutated < self.fitness[i]:\n                self.fitness[i] = f_mutated\n                self.population[i] = x_mutated\n                stagnation_counter = 0 # Reset stagnation\n                if f_mutated < f_opt:\n                    f_opt = f_mutated\n                    x_opt = x_mutated\n            else:\n                stagnation_counter += 1\n\n            # Step size adaptation: Decay the step size\n            self.step_size *= self.decay_rate\n\n            # Random restart if stagnated\n            if stagnation_counter > max_stagnation:\n                self.population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.budget -= self.pop_size\n                f_opt = np.min(self.fitness)\n                x_opt = self.population[np.argmin(self.fitness)]\n                self.step_size = self.initial_step_size  # Reset step size\n                stagnation_counter = 0\n\n        return f_opt, x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm NeighborhoodMutation scored 0.195 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["982ca243-61f6-435b-bb54-274c04eadd77"], "operator": null, "metadata": {"aucs": [0.11544731650567741, 0.16048043079410412, 0.2402036802130586, 0.1383721094588074, 0.15623667274534037, 0.17266794155804088, 0.21946124184114435, 0.15840939797031484, 0.16198674283716885, 0.16615957898199563, 0.15932751763830566, 0.2144733339116286, 0.23351373261767494, 0.16336752860310821, 0.2736119188863255, 0.22163550514512453, 0.1997545267200388, 0.17392899893359337, 0.14188830365375593, 0.4298502667219335]}}
{"id": "350319a4-9e36-4a34-8c2b-f451b526cb10", "fitness": 0.5561063020918022, "name": "AdaptiveMutationDE", "description": "Simplified Adaptive DE with dynamically adjusted mutation strength based on the magnitude of fitness improvements and a streamlined update mechanism.", "code": "import numpy as np\n\nclass AdaptiveMutationDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, CR=0.9, F_initial=0.5, F_decay=0.99):\n        \"\"\"\n        Initialize the Adaptive Mutation DE.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            CR (float): Differential evolution crossover rate.\n            F_initial (float): Initial mutation factor.\n            F_decay (float): Decay rate for F.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.F = F_initial  # Initial mutation factor\n        self.F_decay = F_decay # Decay rate for F\n        self.p_selection = 0.9 # Probability of selecting better individual\n\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using a DE strategy with adaptive mutation.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            # Differential Evolution part\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n\n                x_mutated = population[i] + self.F * (x_r2 - x_r3)\n\n                # Crossover\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Repair bounds\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Stochastic Ranking Selection\n                if np.random.rand() < self.p_selection:\n                    if f_trial < fitness[i]:\n                        # Update F based on improvement\n                        if f_trial < f_opt:\n                            self.F = self.F * self.F_decay\n                            f_opt = f_trial\n                            x_opt = x_trial\n                        fitness[i] = f_trial\n                        population[i] = x_trial\n\n        return f_opt, x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveMutationDE scored 0.556 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e64f1b15-bfd8-4481-80e5-06f63c22554c"], "operator": null, "metadata": {"aucs": [0.19316328882068023, 0.4947639309230958, 0.568113100556578, 0.7962928984668743, 0.4564617077613561, 0.7215401057777993, 0.3301614250080017, 0.4869772133237297, 0.530583172639588, 0.2114978836475263, 0.8420167817575744, 0.9903326736383863, 0.6242052429656166, 0.38440895462849367, 0.9110290006836401, 0.6612457963379855, 0.45944877687022245, 0.7766444838633907, 0.19039119546441574, 0.4928484087010897]}}
{"id": "05fd63ff-5c3b-485b-b4bb-7af0f18659b1", "fitness": 0.10091890416805631, "name": "CMAES", "description": "Covariance matrix adaptation evolution strategy with adaptive population sizing based on success rate and dynamically adjusted step size.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, initial_step_size=0.1):\n        \"\"\"\n        Initialize the CMA-ES algorithm.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The population size. If None, it's set to 4 + int(3 * np.log(dim)).\n            initial_step_size (float): Initial step size (sigma).\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.initial_step_size = initial_step_size\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.mean = np.random.uniform(self.bounds_lb, self.bounds_ub, size=dim)  # Initialize mean within bounds\n        self.sigma = initial_step_size\n        self.C = np.eye(dim)  # Covariance matrix\n        self.pc = np.zeros(dim)  # Evolution path for C\n        self.ps = np.zeros(dim)  # Evolution path for sigma\n        self.damps = 1 + (dim / 2)  # Damping for sigma\n        self.mu = self.pop_size // 2  # Number of parents\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.c_sigma = (self.mueff + 2) / (dim + self.mueff + 5)\n        self.c_c = (4 + self.mueff / dim) / (dim + 4 + 2 * self.mueff / dim)\n        self.c_1 = 2 / ((dim + 1.3)**2 + self.mueff)\n        self.c_mu = min(1 - self.c_1, 2 * (self.mueff - 2 + 1 / self.mueff) / ((dim + 2.3)**2 + self.mueff))\n        self.success_rate_history = []\n\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using CMA-ES.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n\n        while eval_count < self.budget:\n            # Generate population\n            z = np.random.multivariate_normal(np.zeros(self.dim), self.C, size=self.pop_size)\n            x = self.mean + self.sigma * z\n            \n            # Repair bounds\n            x = np.clip(x, self.bounds_lb, self.bounds_ub)\n\n            # Evaluate population\n            fitness = np.array([func(xi) for xi in x])\n            eval_count += self.pop_size\n            \n            if eval_count > self.budget:\n                fitness = fitness[:self.pop_size - (eval_count - self.budget)]\n                x = x[:self.pop_size - (eval_count - self.budget)]\n                \n            \n            # Sort by fitness\n            idx = np.argsort(fitness)\n            fitness = fitness[idx]\n            x = x[idx]\n\n            # Update best solution\n            if fitness[0] < f_opt:\n                f_opt = fitness[0]\n                x_opt = x[0]\n\n            # Update mean\n            x_mu = x[:self.mu]\n            self.mean = np.sum(self.weights[:, None] * x_mu, axis=0)\n\n            # Update evolution paths\n            z_mu = (x_mu - self.mean) / self.sigma\n            self.ps = (1 - self.c_sigma) * self.ps + np.sqrt(self.c_sigma * (2 - self.c_sigma) * self.mueff) * np.linalg.solve(self.C, (self.mean - self.mean))\n            hsig = np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.c_sigma)**(2 * eval_count / self.pop_size)) / 3.3 < 1 + 0.2 / (self.dim + 1)\n\n            self.pc = (1 - self.c_c) * self.pc + hsig * np.sqrt(self.c_c * (2 - self.c_c) * self.mueff) * (self.mean - self.mean) / self.sigma\n\n            # Update covariance matrix\n            C_temp = np.zeros((self.dim, self.dim))\n            for k in range(self.mu):\n                C_temp += self.weights[k] * np.outer(z_mu[k], z_mu[k])\n            self.C = (1 - self.c_1 - self.c_mu) * self.C + self.c_1 * np.outer(self.pc, self.pc) + self.c_mu * C_temp\n\n            # Update step size\n            self.sigma *= np.exp((self.c_sigma / self.damps) * (np.linalg.norm(self.ps) / np.sqrt(self.dim) - 1))\n\n            # Ensure C remains positive definite\n            try:\n                self.C = np.triu(self.C) + np.triu(self.C, 1).T\n                np.linalg.cholesky(self.C)  # Check if positive definite\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)  # Reset if not positive definite\n\n        return f_opt, x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm CMAES scored 0.101 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["982ca243-61f6-435b-bb54-274c04eadd77"], "operator": null, "metadata": {"aucs": [0.043769426746684825, 0.056147878823718744, 0.19301861091231365, 0.06734938944836244, 0.1315825537100338, 0.11299966991163057, 0.016047536511106464, 0.11403113477984839, 0.07514403721399698, 0.12617158930620276, 0.09607159174965696, 0.18097372986520988, 0.03327522596680099, 0.07021006497653637, 0.09808153418843302, 0.15243807151478017, 0.11996061166235306, 0.1070925320475219, 0.09250614191622863, 0.13150675210970642]}}
{"id": "67685812-a66b-4f21-b7ef-b011e36b5da7", "fitness": 0.3682630761648821, "name": "EnhancedDE", "description": "Enhanced DE with adaptive mutation based on successful step sizes and a dynamic crossover rate adjustment.", "code": "import numpy as np\n\nclass EnhancedDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, CR=0.9, F=0.5, step_size_decay=0.99):\n        \"\"\"\n        Initialize the Enhanced DE.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            CR (float): Initial differential evolution crossover rate.\n            F (float): Initial mutation factor.\n            step_size_decay (float): Decay factor for step size adaptation.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.F = F  # Initial mutation factor\n        self.step_size_decay = step_size_decay\n        self.successful_steps = np.zeros(dim)  # Track successful step sizes for each dimension\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using an enhanced DE strategy.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            # Differential Evolution part\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n                x_mutated = population[i] + self.F * (x_r2 - x_r3)\n\n                # Crossover\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Repair bounds\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Selection: Direct replacement if trial is better\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    step = x_trial - population[i]  # Calculate step\n                    population[i] = x_trial\n                    if f_trial < f_opt:\n                        f_opt = f_trial\n                        x_opt = x_trial\n                        self.F = min(1.0, self.F * 1.2) # Adaptive F, increase it a bit\n                        self.successful_steps = self.step_size_decay * self.successful_steps + (1 - self.step_size_decay) * np.abs(step)\n\n                    # Adaptive CR\n                    self.CR = min(1.0, self.CR + 0.05)\n\n                else:\n                    self.F = max(0.1, self.F * 0.9)  # Decrease F if no improvement\n                    self.CR = max(0.1, self.CR - 0.05)\n\n\n        return f_opt, x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm EnhancedDE scored 0.368 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a1e89cb9-4a00-4c7d-bf67-15db79771dd2"], "operator": null, "metadata": {"aucs": [0.11741609281819287, 0.19371689210566734, 0.40761937372609924, 0.6138346830529269, 0.17130121806741172, 0.4513817549814019, 0.2392529272577517, 0.3159391094580778, 0.24502261326716412, 0.172729689678783, 0.4199014745002069, 0.8723623569061191, 0.29276779690924903, 0.18135958843795208, 0.6776923320140853, 0.44494414143992767, 0.3273344786321021, 0.5056453070840201, 0.21380092752704283, 0.5012387654334596]}}
{"id": "31acc4a0-7d04-4cef-90bc-8759e08dc082", "fitness": 0.4520674484692354, "name": "EnhancedDE", "description": "Enhanced DE with adaptive mutation strength, archive for diversity, and stochastic ranking for constraint handling.", "code": "import numpy as np\n\nclass EnhancedDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, CR=0.9, F=0.5, archive_size=10):\n        \"\"\"\n        Initialize the Enhanced DE.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            CR (float): Differential evolution crossover rate.\n            F (float): Initial mutation factor.\n            archive_size (int): Size of the archive for storing potentially useful solutions.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.F = F  # Initial mutation factor\n        self.archive_size = archive_size\n        self.archive = []\n        self.archive_fitness = []\n\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using an enhanced DE strategy.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            # Differential Evolution part\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 2, replace=False)\n                x_r1, x_r2 = population[idxs]\n\n                # Add archive to mutation pool\n                if len(self.archive) > 0 and np.random.rand() < 0.1:  # 10% chance to use archive\n                    idx_archive = np.random.randint(len(self.archive))\n                    x_r3 = self.archive[idx_archive]\n                    x_mutated = population[i] + self.F * (x_r1 - x_r2) + 0.1 * (x_r3 - population[i]) # Mutation with archive\n                else:\n                     x_mutated = population[i] + self.F * (x_r1 - x_r2)\n\n\n                # Crossover\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Repair bounds\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Selection and Archive update: Stochastic Ranking\n                if (f_trial < fitness[i]):\n                    fitness[i] = f_trial\n                    population[i] = x_trial\n\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(x_trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        # Replace worst element in archive\n                        max_archive_index = np.argmax(self.archive_fitness)\n                        if f_trial < self.archive_fitness[max_archive_index]:\n                            self.archive[max_archive_index] = x_trial\n                            self.archive_fitness[max_archive_index] = f_trial\n\n                    if f_trial < f_opt:\n                        f_opt = f_trial\n                        x_opt = x_trial\n                        self.F = min(1.0, self.F * 1.1)  # Adaptive F, increase it a bit, slower\n\n                else:\n                    self.F = max(0.1, self.F * 0.9)  # Decrease F if no improvement\n\n        return f_opt, x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm EnhancedDE scored 0.452 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a1e89cb9-4a00-4c7d-bf67-15db79771dd2"], "operator": null, "metadata": {"aucs": [0.124957032632795, 0.4295620598933536, 0.5227052511071455, 0.4639929029302876, 0.22666443358016586, 0.5329497529931384, 0.25884882652466257, 0.3287983721788087, 0.25634928375244903, 0.17752327061459305, 0.6975493814385108, 0.9994867916734776, 0.5867683669843851, 0.2156463547021077, 0.8430080795060039, 0.5726464207121125, 0.38181831245486897, 0.7213262753371742, 0.2107878350034027, 0.489959965365265]}}
{"id": "03c93406-7e7d-4a5b-b99b-a795c4e99dba", "fitness": 0.5986171006159698, "name": "AdaptiveMutationDE", "description": "Simplified Adaptive DE with a more direct F adaptation based on recent improvements and reduced parameter set.", "code": "import numpy as np\n\nclass AdaptiveMutationDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, CR=0.9, F_initial=0.5):\n        \"\"\"\n        Initialize the Adaptive Mutation DE.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            CR (float): Differential evolution crossover rate.\n            F_initial (float): Initial mutation factor.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.F = F_initial  # Initial mutation factor\n        self.F_history = [F_initial] # Store history of F values\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using a DE strategy with adaptive mutation.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            # Differential Evolution part\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n\n                # Adaptive F: Adjust F based on recent success. Use a fixed F if no recent improvement\n                if len(self.F_history) > 1 and f_opt < np.min(fitness):  # If improvement observed\n                    self.F = 0.9 * self.F + 0.1 * self.F_history[-1] # adapt F slowly\n                else:\n                    self.F = self.F_history[-1] # If no recent improvement, keep F\n                \n                self.F = np.clip(self.F, 0.1, 1.0) # Keep F within bounds\n                F = self.F\n                \n                x_mutated = population[i] + F * (x_r2 - x_r3)\n\n                # Crossover\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Repair bounds\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Selection:  Greedy selection\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = x_trial\n                    self.F_history.append(self.F) # store F values\n\n                    if f_trial < f_opt:\n                        f_opt = f_trial\n                        x_opt = x_trial\n\n        return f_opt, x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveMutationDE scored 0.599 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["982ca243-61f6-435b-bb54-274c04eadd77"], "operator": null, "metadata": {"aucs": [0.1805606992982226, 0.5658058939704858, 0.5559024893797991, 0.8357809088792705, 0.602346344331653, 0.7552123992938964, 0.3818947952941605, 0.5579283393186609, 0.6264164313756146, 0.22976349254274842, 0.8652098308451933, 0.9918975731277099, 0.6939767732943793, 0.49230427150564837, 0.8945440550364023, 0.7579403317275735, 0.4800990724608719, 0.814035038862956, 0.20033872963815713, 0.4903845421359919]}}
{"id": "01a047ce-cb87-4ab9-ba13-f7437154d275", "fitness": 0.0, "name": "AdaptiveMutationDE", "description": "Simplified Adaptive DE with dynamically adjusted mutation strength based on recent success and an optional archive for enhanced exploration.", "code": "import numpy as np\n\nclass AdaptiveMutationDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, CR=0.9, F_initial=0.5, archive_size=5):\n        \"\"\"\n        Initialize the Adaptive Mutation DE.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            CR (float): Differential evolution crossover rate.\n            F_initial (float): Initial mutation factor.\n            archive_size (int): Size of the archive to store diverse solutions.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.F = F_initial  # Initial mutation factor\n        self.success_history = []  # Store successful mutation sizes\n        self.archive = []\n        self.archive_size = archive_size\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using a DE strategy with adaptive mutation.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            # Differential Evolution part\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 2, replace=False)\n                x_r1, x_r2 = population[idxs]\n\n                # Incorporate archive if available\n                if len(self.archive) > 0 and np.random.rand() < 0.1:  # 10% chance to use archive\n                    x_r3 = self.archive[np.random.randint(len(self.archive))]\n                else:\n                    idxs_r3 = np.random.choice(self.pop_size, 1, replace=False)\n                    x_r3 = population[idxs_r3[0]]\n\n\n                # Adaptive F: Sample F from history if available, else use initial F.\n                if self.success_history:\n                    F = np.random.choice(self.success_history)\n                else:\n                    F = self.F\n\n                x_mutated = population[i] + F * (x_r1 - x_r2)\n\n                # Crossover\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Repair bounds\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    improvement = fitness[i] - f_trial\n                    # Update success history\n                    if improvement > 0:\n                        self.success_history.append(F)\n                        if len(self.success_history) > 10:  # Keep history size limited\n                            self.success_history.pop(0)\n\n                    fitness[i] = f_trial\n                    population[i] = x_trial\n\n                    if f_trial < f_opt:\n                        f_opt = f_trial\n                        x_opt = x_trial\n\n                    # Update archive (simplified, add if better than worst in archive)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(x_trial)\n                    else:\n                        fitness_archive = [func(x) for x in self.archive]\n                        worst_index = np.argmax(fitness_archive)\n                        if f_trial < fitness_archive[worst_index]:\n                            self.archive[worst_index] = x_trial\n\n        return f_opt, x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveMutationDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e64f1b15-bfd8-4481-80e5-06f63c22554c"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "a13db55d-5039-4f96-b8fe-5b4369d16317", "fitness": 0.35770467887958984, "name": "AdaptiveDE", "description": "Improved Adaptive DE with dynamic population sizing, covariance matrix adaptation for mutation, and adaptive restart strategy based on population diversity.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size_initial=None, F_initial=0.5, CR_initial=0.7, stagnation_threshold=500, diversity_threshold=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_initial = pop_size_initial if pop_size_initial is not None else 5 * dim # Dynamic pop size\n        self.pop_size = self.pop_size_initial\n        self.F = F_initial\n        self.CR = CR_initial\n        self.stagnation_threshold = stagnation_threshold\n        self.diversity_threshold = diversity_threshold\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.best_fitness = np.inf\n        self.best_individual = None\n        self.stagnation_counter = 0\n        self.population = None\n        self.fitness = None\n        self.covariance_matrix = np.eye(dim) * 0.1  # Initial covariance matrix\n\n    def generate_orthogonal_design(self, n, k):\n        H = np.ones((n, n))\n        for i in range(1, n):\n            for j in range(1, n):\n                if (i & j) != 0:\n                    H[i, j] = -1\n        design = (H[:k] + 1) / 2\n        return design * (self.bounds_ub - self.bounds_lb) + self.bounds_lb\n    \n    def __call__(self, func):\n        # Population initialization using orthogonal design\n        if self.pop_size <= self.dim:\n           self.pop_size = self.dim + 5\n        self.population = self.generate_orthogonal_design(self.pop_size, self.dim).reshape(self.pop_size, self.dim)\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        self.best_fitness = np.min(self.fitness)\n        self.best_individual = self.population[np.argmin(self.fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Simplified F and CR adaptation\n                self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 0.9)\n                self.CR = np.clip(np.random.normal(0.7, 0.1), 0.1, 1.0)\n\n                # Mutation with covariance matrix adaptation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[idxs]\n                \n                #x_mutated = population[i] + self.F * (x_r2 - x_r3) #Original Mutation\n                x_mutated = self.population[i] + self.F * (x_r2 - x_r3) + np.random.multivariate_normal(np.zeros(self.dim), self.covariance_matrix)\n\n                # Crossover\n                x_trial = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = x_trial\n\n                    if f_trial < self.best_fitness:\n                        self.best_fitness = f_trial\n                        self.best_individual = x_trial\n                        self.stagnation_counter = 0\n\n                        # Update covariance matrix (simplified Hebbian learning)\n                        diff = self.population[i] - self.population[idxs[0]]\n                        self.covariance_matrix = 0.9 * self.covariance_matrix + 0.1 * np.outer(diff, diff)\n                else:\n                    self.stagnation_counter += 1\n\n            # Diversity check\n            diversity = np.std(self.fitness)\n            if diversity < self.diversity_threshold:\n                self.stagnation_counter += 10  # Increase stagnation counter faster\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                # Adaptive restart strategy\n                if np.random.rand() < 0.5:\n                    # Option 1: Re-initialize around the best solution with increased variance\n                    noise = np.random.normal(0, 0.5, size=(self.pop_size, self.dim))\n                    self.population = self.best_individual + noise\n                    self.population = np.clip(self.population, self.bounds_lb, self.bounds_ub)\n                else:\n                    # Option 2: Re-initialize the entire population randomly\n                    self.population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n\n                self.fitness = np.array([func(x) for x in self.population])\n                self.budget -= self.pop_size\n                \n                self.best_fitness = np.min(self.fitness)\n                self.best_individual = self.population[np.argmin(self.fitness)]\n                self.covariance_matrix = np.eye(self.dim) * 0.1 #Reset Covariance Matrix\n                self.stagnation_counter = 0\n\n        return self.best_fitness, self.best_individual", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDE scored 0.358 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["40bea10b-c58d-4a93-8704-89ff50f28368"], "operator": null, "metadata": {"aucs": [0.1618043398676362, 0.23549002972518895, 0.33148816679811643, 0.42000713414897706, 0.28875133292065513, 0.3091088945939249, 0.28323347358130535, 0.2834769159948137, 0.29026236818628204, 0.23200516747943833, 0.3567169111509423, 0.998514327320349, 0.2513853270914237, 0.2858047017505041, 0.7382447927740242, 0.3269401346699419, 0.2703597769454381, 0.4275020789973555, 0.19004388162532393, 0.4729538219701561]}}
{"id": "7b51a7b1-e26c-4664-9e8b-21f7d58e4f61", "fitness": 0.010105133686405476, "name": "CMAES_AdaptivePop", "description": "Covariance matrix adaptation evolution strategy with adaptive population sizing and archive for diversity maintenance.", "code": "import numpy as np\n\nclass CMAES_AdaptivePop:\n    def __init__(self, budget=10000, dim=10, pop_factor=4, initial_sigma=0.2):\n        \"\"\"\n        Initialize the CMA-ES algorithm with adaptive population sizing and archive.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_factor (int): Factor to determine population size (pop_size = pop_factor * dim).\n            initial_sigma (float): Initial standard deviation for the search distribution.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_factor = pop_factor\n        self.pop_size = int(self.pop_factor * self.dim)\n        self.initial_sigma = initial_sigma\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.mean = np.random.uniform(self.bounds_lb, self.bounds_ub, size=self.dim)\n        self.sigma = self.initial_sigma\n        self.C = np.eye(self.dim)  # Covariance matrix\n        self.pc = np.zeros(self.dim)  # Evolution path for C\n        self.ps = np.zeros(self.dim)  # Evolution path for sigma\n        self.archive = [] # Archive to keep track of explored solutions and avoid premature convergence\n        self.archive_size = 100\n\n        self.c_sigma = (self.pop_size + 2) / (self.dim + self.pop_size + 5)\n        self.c_c = (4 + self.pop_size / self.dim) / (self.dim + 4 + 2 * self.pop_size / self.dim)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.pop_size - 1) / (self.dim + 1)) - 1) + self.c_sigma\n        self.mu = self.pop_size // 2  # Number of individuals for recombination\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.chiN = self.dim**0.5 * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.c_1 = 2 / ((self.dim + 1.3)**2 + self.mu)\n        self.c_mu = min(1 - self.c_1, 2 * (self.mu - 1 + 1/self.mu) / ((self.dim + 2)**2 + self.mu))\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using CMA-ES.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        f_opt = np.Inf\n        x_opt = None\n\n        while self.budget > 0:\n            # Sample population\n            z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n            A = np.linalg.cholesky(self.C)\n            population = self.mean + self.sigma * z @ A.T\n            population = np.clip(population, self.bounds_lb, self.bounds_ub)\n\n            # Evaluate population\n            fitness = np.array([func(x) for x in population])\n            self.budget -= self.pop_size\n\n            # Sort by fitness\n            idx = np.argsort(fitness)\n            fitness = fitness[idx]\n            population = population[idx]\n\n            if fitness[0] < f_opt:\n                f_opt = fitness[0]\n                x_opt = population[0]\n\n            # Update mean\n            mean_old = self.mean.copy()\n            self.mean = np.sum(self.weights[:, None] * population[:self.mu], axis=0)\n\n            # Update evolution paths\n            y = (self.mean - mean_old) / self.sigma\n            self.ps = (1 - self.c_sigma) * self.ps + np.sqrt(self.c_sigma * (2 - self.c_sigma)) * (A @ z[idx[0:self.mu]].T @ self.weights)\n            hsig = np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.c_sigma)**2)**0.5 / self.chiN < 1.4 + 2 / (self.dim + 1)\n            self.pc = (1 - self.c_c) * self.pc + hsig * np.sqrt(self.c_c * (2 - self.c_c)) * y\n\n            # Update covariance matrix\n            self.C = (1 - self.c_1 - self.c_mu) * self.C + self.c_1 * (self.pc[:, None] @ self.pc[None, :])\n            for i in range(self.mu):\n                self.C += self.c_mu * self.weights[i] * ((population[i] - mean_old)[:, None] @ (population[i] - mean_old)[None, :]) / (self.sigma**2)\n            \n            # Keep C positive definite\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)\n\n            # Update step size\n            self.sigma *= np.exp((self.c_sigma / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n\n            # Archive the solutions\n            for x in population:\n                if not any(np.allclose(x, archived_x) for archived_x in self.archive):\n                    self.archive.append(x)\n                    if len(self.archive) > self.archive_size:\n                        self.archive.pop(0) # Remove the oldest element\n\n\n            # Adaptive population sizing (simple version) - reduce pop size if stagnant\n            if self.budget > 0 and len(self.archive) < self.archive_size / 2 and self.pop_size > self.dim + 2:\n                self.pop_size = max(self.dim + 2, self.pop_size // 2)\n                self.mu = self.pop_size // 2\n                self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n                self.weights /= np.sum(self.weights)\n                self.c_sigma = (self.pop_size + 2) / (self.dim + self.pop_size + 5)\n                self.c_c = (4 + self.pop_size / self.dim) / (self.dim + 4 + 2 * self.pop_size / self.dim)\n                self.damps = 1 + 2 * max(0, np.sqrt((self.pop_size - 1) / (self.dim + 1)) - 1) + self.c_sigma\n                \n\n        return f_opt, x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm CMAES_AdaptivePop scored 0.010 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["982ca243-61f6-435b-bb54-274c04eadd77"], "operator": null, "metadata": {"aucs": [0.020210267372810953, 0]}}
{"id": "691795a3-4f75-485d-94ef-9bf525f650e7", "fitness": -Infinity, "name": "EnhancedDE", "description": "Enhanced DE with dimension-wise adaptive mutation strength, velocity-based exploration, and a decaying archive for diversity.", "code": "import numpy as np\n\nclass EnhancedDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, CR=0.9, F=0.5, archive_size=10, velocity_decay=0.5):\n        \"\"\"\n        Initialize the Enhanced DE.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            CR (float): Initial differential evolution crossover rate.\n            F (float): Initial mutation factor.\n            archive_size (int): Size of the archive for storing diverse solutions.\n            velocity_decay (float): Decay factor for velocity adaptation.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.F = F  # Initial mutation factor\n        self.archive_size = archive_size\n        self.archive = []\n        self.velocity = np.zeros((pop_size, dim))  # Initialize velocity for each individual\n        self.velocity_decay = velocity_decay\n\n        self.F_dim = np.full(dim, F)  # Dimension-wise mutation factors\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using an enhanced DE strategy.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            # Differential Evolution part\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Mutation\n                idxs = np.random.choice(self.pop_size + len(self.archive), 3, replace=False)\n                \n                # Select from population or archive\n                pool = np.vstack((population, np.array(self.archive)))\n                x_r1, x_r2, x_r3 = pool[idxs[:3]]\n\n                # Dimension-wise mutation\n                mutation_vector = np.zeros(self.dim)\n                for j in range(self.dim):\n                    mutation_vector[j] = self.F_dim[j] * (x_r2[j] - x_r3[j])\n                \n                x_mutated = population[i] + mutation_vector\n\n                # Velocity-based exploration\n                self.velocity[i] = self.velocity_decay * self.velocity[i] + (1 - self.velocity_decay) * (x_mutated - population[i])\n                x_mutated = population[i] + self.velocity[i]\n\n\n                # Crossover\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Repair bounds\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Selection: Direct replacement if trial is better\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    step = x_trial - population[i]  # Calculate step\n                    population[i] = x_trial\n                    if f_trial < f_opt:\n                        f_opt = f_trial\n                        x_opt = x_trial\n\n                    # Adaptive F per dimension\n                    for j in range(self.dim):\n                        if abs(step[j]) > 0.01: #Only adapt if there was a significant step in this dimension\n                            self.F_dim[j] = min(1.0, self.F_dim[j] * 1.2)  # Increase F if improvement\n                        else:\n                            self.F_dim[j] = max(0.1, self.F_dim[j] * 0.9)  # Decrease F if no improvement\n                    # Adaptive CR\n                    self.CR = min(1.0, self.CR + 0.05)\n\n                    # Archive update (decaying)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i].copy())\n                    else:\n                        if np.random.rand() < 0.5:\n                            self.archive[np.random.randint(self.archive_size)] = population[i].copy()\n\n                else:\n                     # Decrease F if no improvement\n                    for j in range(self.dim):\n                         self.F_dim[j] = max(0.1, self.F_dim[j] * 0.9)\n                    self.CR = max(0.1, self.CR - 0.05)\n\n        return f_opt, x_opt", "configspace": "", "generation": 6, "feedback": "An exception occurred: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 2 and the array at index 1 has size 0.", "error": "", "parent_ids": ["67685812-a66b-4f21-b7ef-b011e36b5da7"], "operator": null, "metadata": {}}
{"id": "b1f3cb98-2cb4-412a-8a1c-4540b1ba1d71", "fitness": 0.0, "name": "EnhancedDE", "description": "Simplified Adaptive DE with an improved F adaptation based on success rate and a reduced archive update strategy.", "code": "import numpy as np\n\nclass EnhancedDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, CR=0.9, F=0.5, archive_size=5):\n        \"\"\"\n        Initialize the Enhanced DE.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            CR (float): Differential evolution crossover rate.\n            F (float): Initial mutation factor.\n            archive_size (int): Size of the archive for storing potentially useful solutions. Reduced for simplification.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.F = F  # Initial mutation factor\n        self.archive_size = archive_size\n        self.archive = []\n\n        self.success_rate = 0.0\n        self.success_decay = 0.95\n\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using an enhanced DE strategy.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            # Differential Evolution part\n            successful_mutations = 0\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 2, replace=False)\n                x_r1, x_r2 = population[idxs]\n\n                # Add archive to mutation pool\n                if len(self.archive) > 0 and np.random.rand() < 0.1:  # 10% chance to use archive\n                    idx_archive = np.random.randint(len(self.archive))\n                    x_r3 = self.archive[idx_archive]\n                    x_mutated = population[i] + self.F * (x_r1 - x_r2) + 0.1 * (x_r3 - population[i]) # Mutation with archive\n                else:\n                    x_mutated = population[i] + self.F * (x_r1 - x_r2)\n\n                # Crossover\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Repair bounds\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Selection and Archive update\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = x_trial\n                    successful_mutations += 1\n\n                    # Update archive: add if better than worst\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(x_trial)\n                    else:\n                        fitness_archive = [func(x) for x in self.archive]\n                        worst_index = np.argmax(fitness_archive)\n                        if f_trial < fitness_archive[worst_index]:\n                            self.archive[worst_index] = x_trial\n\n                    if f_trial < f_opt:\n                        f_opt = f_trial\n                        x_opt = x_trial\n\n            # Adjust F based on success rate\n            self.success_rate = self.success_decay * self.success_rate + (1 - self.success_decay) * (successful_mutations / self.pop_size)\n            if self.success_rate > 0.2:\n                self.F = max(0.1, self.F * 0.9)  # Decrease F if doing well\n            else:\n                self.F = min(1.0, self.F * 1.1)  # Increase F if not improving\n\n        return f_opt, x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm EnhancedDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["31acc4a0-7d04-4cef-90bc-8759e08dc082"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "f204ed84-ee1a-4270-87e6-35910be4190c", "fitness": 0.06404199620313983, "name": "EnhancedDE", "description": "Enhanced DE with self-adaptive mutation strength, velocity-based archive updates, and periodic population rejuvenation.", "code": "import numpy as np\n\nclass EnhancedDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, CR=0.9, F=0.5, archive_size=10, rejuvenation_rate=0.05):\n        \"\"\"\n        Initialize the Enhanced DE with self-adaptive mutation strength, velocity-based archive updates, and periodic population rejuvenation.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            CR (float): Differential evolution crossover rate.\n            F (float): Initial mutation factor.\n            archive_size (int): Size of the archive for storing potentially useful solutions.\n            rejuvenation_rate (float): Probability of rejuvenating a population member.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.F = F  # Initial mutation factor\n        self.archive_size = archive_size\n        self.archive = []\n        self.archive_fitness = []\n        self.rejuvenation_rate = rejuvenation_rate\n        self.velocities = np.zeros((pop_size, dim))  # Initialize velocities\n\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using an enhanced DE strategy.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n        \n        success_history = []  # Track recent successes for F adaptation\n\n        while self.budget > 0:\n            # Differential Evolution part\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 2, replace=False)\n                x_r1, x_r2 = population[idxs]\n\n                # Archive interaction\n                if len(self.archive) > 0 and np.random.rand() < 0.1:  # 10% chance to use archive\n                    idx_archive = np.random.randint(len(self.archive))\n                    x_r3 = self.archive[idx_archive]\n                    x_mutated = population[i] + self.F * (x_r1 - x_r2) + 0.1 * (x_r3 - population[i]) # Mutation with archive\n                else:\n                    x_mutated = population[i] + self.F * (x_r1 - x_r2)\n\n\n                # Crossover\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Repair bounds\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Selection and Archive update\n                if (f_trial < fitness[i]):\n                    delta_fitness = fitness[i] - f_trial\n                    \n                    # Velocity update: keep track of the step\n                    self.velocities[i] = x_trial - population[i]\n                    \n                    fitness[i] = f_trial\n                    population[i] = x_trial\n                    \n                    success_history.append(delta_fitness)\n                    if len(success_history) > 20:\n                        success_history.pop(0)\n\n                    # Archive update: prioritize solutions with larger velocity magnitudes\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(x_trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        velocity_magnitudes = [np.linalg.norm(self.velocities[j]) for j in range(self.pop_size)]\n                        min_archive_index = np.argmin(self.archive_fitness)\n                        if f_trial < self.archive_fitness[min_archive_index] or np.linalg.norm(self.velocities[i]) > np.mean(velocity_magnitudes):\n                            self.archive[min_archive_index] = x_trial\n                            self.archive_fitness[min_archive_index] = f_trial\n\n                    if f_trial < f_opt:\n                        f_opt = f_trial\n                        x_opt = x_trial\n                    \n                # F self-adaptation based on success history\n                if success_history:\n                    avg_success = np.mean(success_history)\n                    self.F = np.clip(self.F * (1 + 0.2 * (avg_success - np.mean(fitness))), 0.1, 1.0) # Adjust F based on average success\n\n            # Population rejuvenation\n            for i in range(self.pop_size):\n                if np.random.rand() < self.rejuvenation_rate:\n                    population[i] = np.random.uniform(self.bounds_lb, self.bounds_ub, size=self.dim)\n                    fitness[i] = func(population[i])\n                    self.budget -= 1\n\n\n        return f_opt, x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm EnhancedDE scored 0.064 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["31acc4a0-7d04-4cef-90bc-8759e08dc082"], "operator": null, "metadata": {"aucs": [0.12808399240627966, 0]}}
{"id": "30182c4d-3f1e-42a3-994a-dc42c81c1e8b", "fitness": 0.0, "name": "EnhancedDE", "description": "Enhanced DE with dimension-wise mutation factor adaptation based on step size success, momentum-based velocity updates, and self-adaptive population size.", "code": "import numpy as np\n\nclass EnhancedDE:\n    def __init__(self, budget=10000, dim=10, pop_size=None, CR=0.9, F=0.5, step_size_decay=0.95, pop_size_adapt=True):\n        \"\"\"\n        Initialize the Enhanced DE.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population. If None, it adapts during optimization.\n            CR (float): Initial differential evolution crossover rate.\n            F (float): Initial mutation factor.\n            step_size_decay (float): Decay factor for step size adaptation.\n            pop_size_adapt (bool): Enables or disables adaptive population size.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else int(4 + np.floor(3 * np.log(dim)))  # Initial pop size\n        self.min_pop_size = 4 # Minimum Population size\n        self.max_pop_size = 50 # Maximum Population size\n        self.CR = CR\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.F = F  # Initial mutation factor\n        self.step_size_decay = step_size_decay\n        self.successful_steps = np.zeros(dim)  # Track successful step sizes for each dimension\n        self.F_dims = np.ones(dim) * F # Individual F for each dimension\n        self.velocity = np.zeros((self.pop_size, dim)) # Add momentum to the updates\n        self.momentum_decay = 0.7 # Decay factor for momentum\n        self.pop_size_adapt = pop_size_adapt\n        self.pop_size_history = []\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using an enhanced DE strategy.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n        \n        generation = 0 # Keep track of generations\n        \n        while self.budget > 0:\n            generation += 1\n            \n            # Population Size Adaptation\n            if self.pop_size_adapt and generation % 20 == 0: # Adapt every 20 generations\n                improvement = f_opt - np.min(fitness) # Overall best - current best population\n                if improvement > 1e-5: # Significant improvement, increase population size.\n                    self.pop_size = min(self.max_pop_size, int(self.pop_size * 1.1)) # Increase by 10%\n                else: # No significant improvement, decrease population size.\n                    self.pop_size = max(self.min_pop_size, int(self.pop_size * 0.9)) # Decrease by 10%\n                self.pop_size = int(self.pop_size)\n                # Reinitialize population, keeping the best solution\n                new_population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n                new_population[0] = x_opt.copy()\n                population = new_population\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size - len(fitness)\n                self.pop_size_history.append(self.pop_size)\n            \n            # Differential Evolution part\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n                \n                # Dimension-wise mutation\n                F_rand_dims = np.random.normal(self.F_dims, 0.1 * self.F_dims) # Sample from normal distribution\n                F_rand_dims = np.clip(F_rand_dims, 0.01, 1.0) # Clip values\n                x_mutated = population[i] + F_rand_dims * (x_r2 - x_r3)\n\n                # Crossover\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Repair bounds\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Selection: Direct replacement if trial is better\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    step = x_trial - population[i]  # Calculate step\n                    \n                    # Momentum update\n                    self.velocity[i] = self.momentum_decay * self.velocity[i] + (1 - self.momentum_decay) * step\n                    population[i] = x_trial + self.velocity[i]\n                    population[i] = np.clip(population[i], self.bounds_lb, self.bounds_ub)\n                    \n                    if f_trial < f_opt:\n                        f_opt = f_trial\n                        x_opt = population[i]\n                        # Adaptive F, increase it a bit\n                        self.F = min(1.0, self.F * 1.1)\n\n                    # Dimension-wise step size adaptation\n                    abs_step = np.abs(step)\n                    self.successful_steps = self.step_size_decay * self.successful_steps + (1 - self.step_size_decay) * abs_step\n                    \n                    # Adjust F_dims based on successful steps\n                    self.F_dims = np.where(abs_step > 0, self.F_dims * 1.1, self.F_dims * 0.9) # Increase F if step was successful, decrease otherwise\n                    self.F_dims = np.clip(self.F_dims, 0.1, 1.0)\n\n                    # Adaptive CR\n                    self.CR = min(1.0, self.CR + 0.05)\n\n                else:\n                    self.F = max(0.1, self.F * 0.9)  # Decrease F if no improvement\n                    self.CR = max(0.1, self.CR - 0.05)\n                    # Diminish velocity if the update was unsuccessful\n                    self.velocity[i] *= 0.5\n\n\n        return f_opt, x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm EnhancedDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["67685812-a66b-4f21-b7ef-b011e36b5da7"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "7bed280b-0e77-4a08-9012-0deb90ab29e2", "fitness": 0.14233877518964327, "name": "BudgetAwareCMAES", "description": "Covariance matrix adaptation evolution strategy (CMA-ES) with simplified update rules and budget-aware adaptation.", "code": "import numpy as np\n\nclass BudgetAwareCMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, initial_sigma=0.5):\n        \"\"\"\n        Initialize the Budget-Aware CMA-ES.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The population size (default: 4 + int(3 * np.log(dim))).\n            initial_sigma (float): Initial step size.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 4 + int(3 * np.log(dim)) if pop_size is None else pop_size\n        self.initial_sigma = initial_sigma\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n\n        # CMA-ES parameters\n        self.mu = self.pop_size // 2  # Number of parents\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))  # Recombination weights\n        self.weights /= np.sum(self.weights)  # Normalize weights\n\n        self.m = np.random.uniform(self.bounds_lb, self.bounds_ub, size=self.dim)  # Mean\n        self.sigma = self.initial_sigma  # Step size\n        self.C = np.eye(self.dim)  # Covariance matrix\n        self.p_c = np.zeros(self.dim)  # Evolution path for covariance matrix\n        self.p_sigma = np.zeros(self.dim)  # Evolution path for step size\n\n        self.c_sigma = (self.mu / (self.dim + self.mu))**0.5\n        self.d_sigma = 1 + 2 * max(0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1) + 0.1*self.c_sigma\n        self.c_c = (self.mu / (self.dim + self.mu))**0.5\n        self.c_1 = 2 / ((self.dim + 1.3)**2 + self.mu)\n        self.c_mu = 2 * (self.mu - 2 + 1/self.mu) / ((self.dim + 2)**2 + self.mu)\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using CMA-ES.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        f_opt = np.Inf\n        x_opt = None\n        \n        while self.budget > 0:\n            # Sample population\n            z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n            A = np.linalg.cholesky(self.C)\n            x = self.m + self.sigma * (A @ z.T).T\n            \n            # Handle bounds\n            x = np.clip(x, self.bounds_lb, self.bounds_ub)\n            \n            # Evaluate population\n            fitness = np.zeros(self.pop_size)\n            for i in range(self.pop_size):\n                if self.budget > 0:\n                    fitness[i] = func(x[i])\n                    self.budget -= 1\n                else:\n                    fitness[i] = np.Inf\n            \n            # Sort by fitness\n            idx = np.argsort(fitness)\n            x = x[idx]\n            fitness = fitness[idx]\n            \n            # Update optimal solution\n            if fitness[0] < f_opt:\n                f_opt = fitness[0]\n                x_opt = x[0]\n            \n            # Update CMA-ES parameters\n            x_mean = np.sum(self.weights[:, np.newaxis] * x[:self.mu], axis=0)\n            z_mean = np.sum(self.weights[:, np.newaxis] * z[idx[:self.mu]], axis=0)\n            \n            self.p_sigma = (1 - self.c_sigma) * self.p_sigma + np.sqrt(self.c_sigma * (2 - self.c_sigma)) * (A @ z_mean)\n            self.sigma *= np.exp((self.c_sigma / self.d_sigma) * (np.linalg.norm(self.p_sigma) / np.sqrt(self.dim) - 1))\n            \n            h_sigma = 1 if (np.linalg.norm(self.p_sigma) / np.sqrt(self.dim)) < (1.4 + 2/(self.dim+1)) else 0\n            self.p_c = (1-self.c_c) * self.p_c + h_sigma * np.sqrt(self.c_c * (2 - self.c_c)) * (x_mean - self.m) / self.sigma\n            \n            self.m = x_mean\n            \n            self.C = (1 - self.c_1 - self.c_mu) * self.C + self.c_1 * np.outer(self.p_c, self.p_c) + self.c_mu * np.sum(self.weights[i] * np.outer((x[i] - self.m) / self.sigma, (x[i] - self.m) / self.sigma) for i in range(self.mu))\n\n        return f_opt, x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm BudgetAwareCMAES scored 0.142 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["350319a4-9e36-4a34-8c2b-f451b526cb10"], "operator": null, "metadata": {"aucs": [0.004844767992896792, 0.05044934383699817, 0.26222667334018535, 0.11004852178159086, 0.15680519707781126, 0.15858860777466433, 0.20682992190759852, 0.13144272406194757, 0.14488031554656522, 0.0986518927397192, 0.14338128210078482, 0.1458545173099952, 0.21460498237320902, 0.06778579533768314, 0.20365547678757234, 0.16238613085686815, 0.12126148306572071, 0.13649271275064456, 0.1569295366079113, 0.1696556205424986]}}
{"id": "2756cf9e-1f19-4acc-be51-ceb333f6dde6", "fitness": 0.4655607886231626, "name": "SimplifiedAdaptiveDE", "description": "Simplified Adaptive DE with population-level mutation factor adaptation and local search.", "code": "import numpy as np\n\nclass SimplifiedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, CR=0.7, F=0.5, local_search_prob=0.1):\n        \"\"\"\n        Initialize the Simplified Adaptive DE.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            CR (float): Initial differential evolution crossover rate.\n            F (float): Initial mutation factor.\n            local_search_prob (float): Probability of performing a local search step.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.F = F  # Mutation factor\n        self.local_search_prob = local_search_prob\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using a simplified adaptive DE strategy.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n\n        mutation_factors = np.full(self.pop_size, self.F)  # Individual F values\n\n        while self.budget > 0:\n            # Differential Evolution part\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n                x_mutated = population[i] + mutation_factors[i] * (x_r2 - x_r3)\n\n                # Crossover\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Repair bounds\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Selection: Direct replacement if trial is better\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = x_trial\n                    if f_trial < f_opt:\n                        f_opt = f_trial\n                        x_opt = x_trial\n                    mutation_factors[i] = min(1.0, mutation_factors[i] * 1.1)  # Increase F\n\n                    # Local Search\n                    if np.random.rand() < self.local_search_prob:\n                        step = np.random.uniform(-0.1, 0.1, size=self.dim)\n                        x_local = np.clip(x_trial + step, self.bounds_lb, self.bounds_ub)\n                        f_local = func(x_local)\n                        self.budget -= 1\n                        if f_local < f_trial:\n                            fitness[i] = f_local\n                            population[i] = x_local\n                            if f_local < f_opt:\n                                f_opt = f_local\n                                x_opt = x_local\n\n                else:\n                    mutation_factors[i] = max(0.1, mutation_factors[i] * 0.9)  # Decrease F\n\n        return f_opt, x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm SimplifiedAdaptiveDE scored 0.466 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["67685812-a66b-4f21-b7ef-b011e36b5da7"], "operator": null, "metadata": {"aucs": [0.17326738612684878, 0.3516530938842921, 0.457546241743216, 0.6606569215089284, 0.30031471680582666, 0.5454472460642426, 0.31095926864284107, 0.3625604871928677, 0.35252660076069886, 0.1671797528125275, 0.8303721641239754, 0.9984173275171827, 0.44449191756183637, 0.31997015488263436, 0.8812224152142304, 0.47177794154702246, 0.3550734321125616, 0.6099988214840265, 0.2318094589651022, 0.48597042351238884]}}
{"id": "bc44013b-8bb7-4663-b8a6-7eca7a47ec90", "fitness": 0.5506740775756673, "name": "AdaptiveMutationDE", "description": "Adaptive DE with a mirrored sampling strategy and population diversity maintenance to balance exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveMutationDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, CR=0.9, F_initial=0.5, F_decay=0.99, diversity_threshold=0.1):\n        \"\"\"\n        Initialize the Adaptive Mutation DE.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            CR (float): Differential evolution crossover rate.\n            F_initial (float): Initial mutation factor.\n            F_decay (float): Decay rate for F.\n            diversity_threshold (float): Threshold to trigger diversity maintenance.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.F = F_initial  # Initial mutation factor\n        self.F_decay = F_decay # Decay rate for F\n        self.p_selection = 0.9 # Probability of selecting better individual\n        self.diversity_threshold = diversity_threshold\n\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using a DE strategy with adaptive mutation and mirrored sampling.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            # Differential Evolution part\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Mutation with mirrored sampling\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n\n                # Mirrored sampling: if the mutated vector goes out of bounds, reflect it back\n                x_mutated = population[i] + self.F * (x_r2 - x_r3)\n                \n                # Crossover\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Repair bounds and mirrored sampling\n                for j in range(self.dim):\n                    if x_trial[j] < self.bounds_lb:\n                        x_trial[j] = self.bounds_lb + (self.bounds_lb - x_trial[j])\n                    elif x_trial[j] > self.bounds_ub:\n                        x_trial[j] = self.bounds_ub - (x_trial[j] - self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Stochastic Ranking Selection\n                if np.random.rand() < self.p_selection:\n                    if f_trial < fitness[i]:\n                        # Update F based on improvement\n                        if f_trial < f_opt:\n                            self.F = self.F * self.F_decay\n                            f_opt = f_trial\n                            x_opt = x_trial\n                        fitness[i] = f_trial\n                        population[i] = x_trial\n\n            # Diversity maintenance\n            if self.population_diversity(population) < self.diversity_threshold:\n                # Introduce new random individuals\n                num_new = int(self.pop_size * 0.1)  # Introduce 10% new individuals\n                new_individuals = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(num_new, self.dim))\n                new_fitness = np.array([func(x) for x in new_individuals])\n                self.budget -= num_new\n\n                worst_indices = np.argsort(fitness)[-num_new:]  # Replace worst individuals\n                population[worst_indices] = new_individuals\n                fitness[worst_indices] = new_fitness\n\n                # Update best solution if necessary\n                if np.min(fitness) < f_opt:\n                    f_opt = np.min(fitness)\n                    x_opt = population[np.argmin(fitness)]\n        return f_opt, x_opt\n\n    def population_diversity(self, population):\n        \"\"\"\n        Calculate the diversity of the population based on the average distance to the centroid.\n        \"\"\"\n        centroid = np.mean(population, axis=0)\n        distances = np.linalg.norm(population - centroid, axis=1)\n        avg_distance = np.mean(distances)\n        return avg_distance / (self.bounds_ub - self.bounds_lb) # Normalize by the range", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveMutationDE scored 0.551 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["350319a4-9e36-4a34-8c2b-f451b526cb10"], "operator": null, "metadata": {"aucs": [0.19262535991172636, 0.5164018299737398, 0.5261886613027289, 0.8144177737425377, 0.4196755634117115, 0.6998258433857336, 0.3345158795447334, 0.4839129451594547, 0.4787325731246641, 0.20919036081408937, 0.8815997705335996, 0.9991624638201716, 0.5935452754071749, 0.40826499906447566, 0.9069234205956831, 0.6041794720172713, 0.4462789942877322, 0.7877087636931015, 0.21351415489229675, 0.49681744683071993]}}
{"id": "09c5ff6d-7973-43d2-909b-565f4bd8bde3", "fitness": 0.506920040955491, "name": "AdaptiveMutationDE", "description": "Improved Adaptive DE with momentum-based F adaptation, orthogonal crossover, and a simplified update mechanism.", "code": "import numpy as np\n\nclass AdaptiveMutationDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, CR=0.9, F_initial=0.5, momentum=0.5):\n        \"\"\"\n        Initialize the Adaptive Mutation DE.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            CR (float): Differential evolution crossover rate.\n            F_initial (float): Initial mutation factor.\n            momentum (float): Momentum for F adaptation.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.F = F_initial  # Initial mutation factor\n        self.F_history = [F_initial] # Store history of F values\n        self.momentum = momentum # Momentum term for F adaptation\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using a DE strategy with adaptive mutation and orthogonal crossover.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n        \n        previous_f_opt = f_opt\n\n        while self.budget > 0:\n            # Differential Evolution part\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n\n                # Adaptive F: Adjust F based on recent success using momentum.\n                if f_opt < previous_f_opt:  # If improvement observed\n                    delta_f = previous_f_opt - f_opt\n                    self.F = self.momentum * self.F + (1 - self.momentum) * self.F_history[-1] * (1 + delta_f)  # Adapt F based on improvement magnitude\n                else:\n                    self.F = self.momentum * self.F + (1 - self.momentum) * self.F_history[-1]  # Dampen F if no recent improvement\n                \n                self.F = np.clip(self.F, 0.1, 1.0) # Keep F within bounds\n                F = self.F\n                \n                x_mutated = population[i] + F * (x_r2 - x_r3)\n\n                # Crossover: Orthogonal Crossover\n                x_trial = np.copy(population[i])\n                num_changes = int(self.CR * self.dim)  # Number of dimensions to change\n                change_indices = np.random.choice(self.dim, num_changes, replace=False)\n                x_trial[change_indices] = x_mutated[change_indices]\n\n                # Repair bounds\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Selection:  Greedy selection\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = x_trial\n                    self.F_history.append(self.F) # store F values\n\n                    if f_trial < f_opt:\n                        f_opt = f_trial\n                        x_opt = x_trial\n\n            previous_f_opt = f_opt # Store current best fitness for next iteration\n\n        return f_opt, x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveMutationDE scored 0.507 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["03c93406-7e7d-4a5b-b99b-a795c4e99dba"], "operator": null, "metadata": {"aucs": [0.1931512791843496, 0.20797800788781562, 0.496134941177815, 0.7347032523131979, 0.4856707588821245, 0.6491612905880075, 0.37650534449840034, 0.46010451531513874, 0.5246609097337387, 0.2194544060858904, 0.6831134664321652, 0.9878808525169332, 0.2623545784599728, 0.3895981164318155, 0.874254657201936, 0.6610094616643827, 0.4775733822801972, 0.727789543667628, 0.23150086276530535, 0.49580119202300743]}}
{"id": "45a24211-5afc-4a9f-b130-5276945088e5", "fitness": 0.4108790544092714, "name": "AdaptiveMutationDE", "description": "Adaptive Differential Evolution with momentum-based F adaptation and population-wide best influence.", "code": "import numpy as np\n\nclass AdaptiveMutationDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, CR=0.9, F_initial=0.5, momentum=0.1):\n        \"\"\"\n        Initialize the Adaptive Mutation DE.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            CR (float): Differential evolution crossover rate.\n            F_initial (float): Initial mutation factor.\n            momentum (float): Momentum for F adaptation.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.F = F_initial  # Initial mutation factor\n        self.F_history = [F_initial] # Store history of F values\n        self.momentum = momentum\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using a DE strategy with adaptive mutation.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            # Differential Evolution part\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n\n                # Adaptive F: Adjust F based on recent success, incorporate momentum, and best influence.\n                if len(self.F_history) > 1 and f_opt < np.min(fitness):  # If improvement observed\n                    self.F = (1 - self.momentum) * self.F + self.momentum * self.F_history[-1]\n                else:\n                    self.F = self.F_history[-1] # If no recent improvement, keep F\n                \n                self.F = np.clip(self.F, 0.1, 1.0) # Keep F within bounds\n                F = self.F\n                \n                # Incorporate population best into mutation\n                x_mutated = population[i] + F * (x_opt - population[i]) + F * (x_r2 - x_r3) \n\n\n                # Crossover\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Repair bounds\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Selection:  Greedy selection\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = x_trial\n                    self.F_history.append(self.F) # store F values\n\n                    if f_trial < f_opt:\n                        f_opt = f_trial\n                        x_opt = x_trial\n\n        return f_opt, x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveMutationDE scored 0.411 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["03c93406-7e7d-4a5b-b99b-a795c4e99dba"], "operator": null, "metadata": {"aucs": [0.1568012581476299, 0.28708846173247726, 0.3246379395072252, 0.9749083587797035, 0.2406111868755637, 0.3520486242722116, 0.2805815079936348, 0.2768815332198593, 0.31793351372191925, 0.18943369378584574, 0.3369631758100371, 0.9974744011246169, 0.358213207270634, 0.30234193506283924, 0.8482062708243928, 0.618312632109957, 0.3436579062473506, 0.3140523551252954, 0.19926036383927204, 0.49817276273496225]}}
{"id": "7975199b-b167-464b-b87e-226f28e5e06e", "fitness": 0.1958332943587083, "name": "AdaptiveCovarianceDE", "description": "Adaptive DE with covariance matrix adaptation to guide mutation direction and magnitude, incorporating a success-history adaptation for step size control.", "code": "import numpy as np\n\nclass AdaptiveCovarianceDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, CR=0.9, F_initial=0.5, F_decay=0.99, archive_size=10):\n        \"\"\"\n        Initialize the Adaptive Covariance Matrix DE.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            CR (float): Differential evolution crossover rate.\n            F_initial (float): Initial mutation factor.\n            F_decay (float): Decay rate for F.\n            archive_size (int): Size of the archive for storing successful mutations.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.F = F_initial  # Initial mutation factor\n        self.F_decay = F_decay # Decay rate for F\n        self.archive_size = archive_size\n        self.archive = []\n        self.C = np.eye(dim)  # Covariance matrix\n        self.mu = np.zeros(dim) # Mean of successful steps\n        self.learning_rate_mu = 0.1 \n        self.learning_rate_C = 0.1\n        self.success_F = []\n        self.success_steps = []\n        self.hist_size = 10\n\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using a DE strategy with adaptive covariance matrix.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            # Differential Evolution part\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Mutation with covariance matrix adaptation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n                \n                # Sample mutation vector from multivariate normal distribution\n                z = np.random.multivariate_normal(np.zeros(self.dim), self.C)\n                x_mutated = population[i] + self.F * z\n\n                # Crossover\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Repair bounds\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    # Success\n                    step = x_trial - population[i]\n                    \n                    # Update mean and covariance matrix based on successful step\n                    self.mu = (1 - self.learning_rate_mu) * self.mu + self.learning_rate_mu * step\n                    self.C = (1 - self.learning_rate_C) * self.C + self.learning_rate_C * np.outer(step, step)\n\n\n                    if f_trial < f_opt:\n                        f_opt = f_trial\n                        x_opt = x_trial\n                        self.F = self.F * self.F_decay\n                    \n                    fitness[i] = f_trial\n                    population[i] = x_trial\n                    self.success_steps.append(step)\n                    self.success_F.append(self.F)\n\n                    if len(self.success_steps) > self.hist_size:\n                        self.success_steps.pop(0)\n                        self.success_F.pop(0)\n\n        return f_opt, x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveCovarianceDE scored 0.196 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["350319a4-9e36-4a34-8c2b-f451b526cb10"], "operator": null, "metadata": {"aucs": [0.09322551207001473, 0.19737566737281775, 0.22431129432038377, 0.1620285682924255, 0.13122559802220202, 0.16424848033368578, 0.1682279002158401, 0.1430361070438515, 0.16612327040795594, 0.13105845166943486, 0.15412486316131346, 0.2505138098680021, 0.22545562992494428, 0.18517394619363525, 0.6357231140684776, 0.19999595982680662, 0.18660228470259088, 0.1552747275476699, 0.16634652127186578, 0.17659418086024814]}}
{"id": "5c7c5f60-ee89-4813-8282-4ab1c1e8c56d", "fitness": 0.6234967427602097, "name": "AdaptiveMutationDE", "description": "Adaptive DE with simplified mutation, crossover, and selection based on fitness improvement and a linearly decreasing mutation factor.", "code": "import numpy as np\n\nclass AdaptiveMutationDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, CR=0.9, F_initial=0.7):\n        \"\"\"\n        Initialize the Adaptive Mutation DE.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            CR (float): Differential evolution crossover rate.\n            F_initial (float): Initial mutation factor.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.F = F_initial  # Initial mutation factor\n        self.F_decay = (0.1 - F_initial) / budget # Linear decay rate for F.\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using a DE strategy with adaptive mutation.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            # Differential Evolution part\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n                x_mutated = population[i] + self.F * (x_r2 - x_r3)\n\n                # Crossover\n                x_trial = np.where(np.random.rand(self.dim) < self.CR, x_mutated, population[i])\n\n                # Repair bounds\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    # Update F based on improvement\n                    if f_trial < f_opt:\n                        f_opt = f_trial\n                        x_opt = x_trial\n                    fitness[i] = f_trial\n                    population[i] = x_trial\n                    self.F += self.F_decay\n\n        return f_opt, x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveMutationDE scored 0.623 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["350319a4-9e36-4a34-8c2b-f451b526cb10"], "operator": null, "metadata": {"aucs": [0.19672248686315286, 0.6052582735815173, 0.5720647451869707, 0.8263084517126689, 0.6243529790329229, 0.7330016739643233, 0.36892584152464536, 0.593908729860634, 0.650746150961935, 0.4453794382044619, 0.8454358566662288, 0.9962778523387266, 0.6421904568654038, 0.6287729511725388, 0.8813876877647462, 0.745808695622285, 0.564243134158604, 0.8192174011860321, 0.2262818873544471, 0.5036501611819497]}}
{"id": "5aa8705a-b4b2-45dd-a652-ee7b9f8e974d", "fitness": 0.2256637170338914, "name": "EnhancedDE", "description": "Enhanced DE with dimension-wise adaptive mutation based on the success rate of changes in each dimension, and an archive to maintain diversity.", "code": "import numpy as np\n\nclass EnhancedDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, CR=0.9, F=0.5, archive_size=10, step_size_decay=0.99):\n        \"\"\"\n        Initialize the Enhanced DE.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            CR (float): Initial differential evolution crossover rate.\n            F (float): Initial mutation factor.\n            archive_size (int): Size of the archive for storing diverse solutions.\n            step_size_decay (float): Decay factor for step size adaptation.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.F = F  # Initial mutation factor\n        self.archive_size = archive_size\n        self.archive = []\n        self.step_size_decay = step_size_decay\n        self.successful_steps = np.zeros(dim)  # Track successful step sizes for each dimension\n        self.success_counts = np.zeros(dim)  # Track successful mutations per dimension\n        self.mutation_rates = np.ones(dim) * 0.5 # Initialize mutation rate for each dimension\n\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using an enhanced DE strategy.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            # Differential Evolution part\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n                x_mutated = population[i] + self.F * (x_r2 - x_r3)\n\n                # Crossover\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        if np.random.rand() < self.mutation_rates[j]:\n                            x_trial[j] = x_mutated[j]\n\n                # Repair bounds\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Selection: Direct replacement if trial is better\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    step = x_trial - population[i]  # Calculate step\n                    population[i] = x_trial\n                    if f_trial < f_opt:\n                        f_opt = f_trial\n                        x_opt = x_trial\n                        self.F = min(1.0, self.F * 1.2) # Adaptive F, increase it a bit\n                        self.successful_steps = self.step_size_decay * self.successful_steps + (1 - self.step_size_decay) * np.abs(step)\n                    \n                    # Update dimension-wise success counts\n                    for j in range(self.dim):\n                        if step[j] != 0:\n                            self.success_counts[j] += 1\n\n                    # Adaptive CR\n                    self.CR = min(1.0, self.CR + 0.05)\n                    self.update_archive(x_trial, f_trial)  # Update the archive\n\n                else:\n                    self.F = max(0.1, self.F * 0.9)  # Decrease F if no improvement\n                    self.CR = max(0.1, self.CR - 0.05)\n                    # Replace with a solution from the archive if it exists and is better\n                    if self.archive:\n                        idx = np.random.randint(len(self.archive))\n                        arch_x, arch_f = self.archive[idx]\n                        if arch_f < fitness[i]:\n                            fitness[i] = arch_f\n                            population[i] = arch_x\n\n\n            # Update mutation rates based on success counts\n            for j in range(self.dim):\n                if np.sum(self.success_counts) > 0:\n                    success_rate = self.success_counts[j] / np.sum(self.success_counts)\n                    self.mutation_rates[j] = min(1.0, success_rate * 2) # Adjust factor as needed\n                else:\n                     self.mutation_rates[j] = 0.5 # Reset to default if no success\n\n            self.success_counts[:] = 0 # Reset success counts after updating rates\n\n\n        return f_opt, x_opt\n\n    def update_archive(self, x, fx):\n        \"\"\"\n        Update the archive with a new solution if it's diverse enough.\n        \"\"\"\n        if not self.archive:\n            self.archive.append((x, fx))\n            return\n\n        # Check for diversity (Euclidean distance)\n        distances = [np.linalg.norm(x - arch_x) for arch_x, _ in self.archive]\n        if min(distances) > 0.1:  # Tune the diversity threshold as needed\n            if len(self.archive) < self.archive_size:\n                self.archive.append((x, fx))\n            else:\n                # Replace the worst solution in the archive\n                worst_idx = np.argmax([arch_f for _, arch_f in self.archive])\n                self.archive[worst_idx] = (x, fx)", "configspace": "", "generation": 6, "feedback": "The algorithm EnhancedDE scored 0.226 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["67685812-a66b-4f21-b7ef-b011e36b5da7"], "operator": null, "metadata": {"aucs": [0.08947645274414084, 0.17076422413455317, 0.2387458988866833, 0.16052371021180056, 0.1716481754741681, 0.22592980548721475, 0.18427437899214671, 0.1709797764777291, 0.1510692898836944, 0.13901794401234224, 0.18964837475998042, 0.9655640708951534, 0.23532187820107497, 0.128104749820673, 0.14726896176362714, 0.209145989530799, 0.18594559173656966, 0.1854368140546454, 0.15119375921697975, 0.4132144943938517]}}
{"id": "4671790b-97c3-44c3-ac42-37cc21dbc97c", "fitness": -Infinity, "name": "CMAES_with_AdaptivePopSize", "description": "Covariance matrix adaptation evolution strategy with adaptive population sizing and archive for improved exploration and exploitation.", "code": "import numpy as np\n\nclass CMAES_with_AdaptivePopSize:\n    def __init__(self, budget=10000, dim=10, pop_size=None, initial_sigma=0.5, mu_factor=0.25):\n        \"\"\"\n        Initialize the CMA-ES algorithm with adaptive population size.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): Initial population size. If None, it's set to 4 + int(3 * np.log(dim)).\n            initial_sigma (float): Initial standard deviation.\n            mu_factor (float): Factor to determine number of parents.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.mu = max(1, int(self.pop_size * mu_factor))  # Number of parents\n        self.initial_sigma = initial_sigma\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.mean = np.random.uniform(self.bounds_lb, self.bounds_ub, size=dim)\n        self.sigma = initial_sigma\n        self.C = np.eye(dim)  # Covariance matrix\n        self.p_sigma = np.zeros(dim)  # Evolution path for sigma\n        self.p_c = np.zeros(dim)  # Evolution path for covariance matrix\n        self.c_sigma = (self.mu / (self.mu + dim))**0.5\n        self.c_c = (self.mu / (self.mu + dim))**0.5\n        self.d_sigma = 1 + 2 * max(0, ((self.mu / self.pop_size) - 1))\n        self.c_1 = 2 / ((dim + 1.3)**2 + self.mu)\n        self.c_mu = min(1 - self.c_1, 2 * (self.mu - 1 + 1/self.mu) / ((dim + 2)**2 + self.mu))\n        self.archive = [] # Archive for storing solutions\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using CMA-ES.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        f_opt = np.Inf\n        x_opt = None\n        \n        weights = np.log(self.pop_size + 1) - np.log(np.arange(1, self.mu + 1))\n        weights /= np.sum(weights)\n\n        while self.budget > 0:\n            # Sample population\n            z = np.random.multivariate_normal(np.zeros(self.dim), np.eye(self.dim), size=self.pop_size)\n            A = np.real(np.linalg.cholesky(self.C))  # C = A @ A.T\n            population = self.mean + self.sigma * A @ z.T\n            population = population.T\n\n            # Repair bounds\n            population = np.clip(population, self.bounds_lb, self.bounds_ub)\n\n            # Evaluate population\n            fitness = np.array([func(x) for x in population])\n            self.budget -= self.pop_size\n            \n            # Update archive\n            for i in range(self.pop_size):\n                self.archive.append((fitness[i], population[i]))\n            \n            # Sort population and fitness\n            idx = np.argsort(fitness)\n            fitness = fitness[idx]\n            population = population[idx]\n\n            if fitness[0] < f_opt:\n                f_opt = fitness[0]\n                x_opt = population[0]\n\n            # Update mean\n            mean_diff = population[:self.mu].T @ weights - self.mean\n            self.mean += self.sigma * mean_diff\n\n            # Update evolution paths\n            self.p_sigma = (1 - self.c_sigma) * self.p_sigma + (self.c_sigma * (self.mu / np.sum(fitness[:self.mu]))**0.5) * (A @ z[idx[:self.mu]].T @ weights)\n            self.p_c = (1 - self.c_c) * self.p_c + (self.c_c * (self.mu / np.sum(fitness[:self.mu]))**0.5) * mean_diff / self.sigma\n        \n            # Update covariance matrix\n            self.C = (1 - self.c_1 - self.c_mu) * self.C + self.c_1 * np.outer(self.p_c, self.p_c)\n            self.C += self.c_mu * (population[:self.mu].T @ np.diag(weights) @ population[:self.mu])\n\n            # Update sigma\n            self.sigma *= np.exp((self.c_sigma / self.d_sigma) * (np.linalg.norm(self.p_sigma) / (self.dim**0.5) - 1))\n            \n            # Adaptive population sizing\n            if self.budget > 0 and self.archive:\n                archive_fitness, _ = zip(*self.archive)\n                archive_fitness = np.array(archive_fitness)\n                \n                best_archive_fitness = np.min(archive_fitness)\n                if best_archive_fitness < f_opt:\n                    self.pop_size = min(self.pop_size + 1, 4 + int(3 * np.log(self.dim)) * 2)  # Increase population size\n                else:\n                    self.pop_size = max(4 + int(3 * np.log(self.dim)) // 2, self.pop_size - 1)  # Decrease population size\n                self.mu = max(1, int(self.pop_size * mu_factor))\n\n        return f_opt, x_opt", "configspace": "", "generation": 7, "feedback": "An exception occurred: operands could not be broadcast together with shapes (2,) (2,6) .", "error": "", "parent_ids": ["5c7c5f60-ee89-4813-8282-4ab1c1e8c56d"], "operator": null, "metadata": {}}
{"id": "e9dbaf0e-f227-4047-9aa4-a5de3dc6dfc8", "fitness": 0.5098090854420834, "name": "SHADERestartDE", "description": "An adaptive DE with a restart mechanism based on population stagnation and adaptive parameter control using a success-history based adaptation (SHADE) strategy.", "code": "import numpy as np\n\nclass SHADERestartDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, memory_size=5, restart_threshold=10):\n        \"\"\"\n        Initialize the SHADE Restart DE algorithm.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            memory_size (int): The size of the memory for SHADE adaptation.\n            restart_threshold (int): Number of iterations without improvement before restart.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.memory_size = memory_size\n        self.restart_threshold = restart_threshold\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.memory_CR = np.full(self.memory_size, 0.5)\n        self.memory_F = np.full(self.memory_size, 0.5)\n        self.archive = []\n        self.p = 0.1  # Probability of using top p individuals for mutation\n        self.generation = 0\n        self.no_improvement_count = 0\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using the SHADE Restart DE strategy.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n        best_fitness_history = [f_opt]\n\n        while self.budget > 0:\n            self.generation += 1\n            old_population = np.copy(population)\n            old_fitness = np.copy(fitness)\n\n            # Differential Evolution part\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # SHADE parameter adaptation\n                mem_idx = np.random.randint(self.memory_size)\n                CR = self.memory_CR[mem_idx]\n                F = self.memory_F[mem_idx]\n\n                # Mutation\n                p_best_idx = np.argsort(fitness)[:int(self.p * self.pop_size)]\n                x_pbest = population[np.random.choice(p_best_idx)]\n\n                idxs = np.random.choice(self.pop_size, 2, replace=False)\n                x_r1, x_r2 = population[idxs]\n\n                x_mutated = population[i] + F * (x_pbest - population[i]) + F * (x_r1 - x_r2)\n\n                # Crossover\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Repair bounds\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    self.archive.append(population[i])\n                    fitness[i] = f_trial\n                    population[i] = x_trial\n                    if f_trial < f_opt:\n                        f_opt = f_trial\n                        x_opt = x_trial\n\n            # SHADE memory update\n            successful_CRs = []\n            successful_Fs = []\n            for i in range(self.pop_size):\n                if fitness[i] < old_fitness[i]:\n                    successful_CRs.append(CR)\n                    successful_Fs.append(F)\n\n            if successful_CRs:\n                self.memory_CR[mem_idx] = np.mean(successful_CRs)\n            if successful_Fs:\n                self.memory_F[mem_idx] = np.mean(successful_Fs)\n\n            # Restart mechanism\n            if f_opt < best_fitness_history[-1]:\n                best_fitness_history.append(f_opt)\n                self.no_improvement_count = 0\n            else:\n                self.no_improvement_count += 1\n                best_fitness_history.append(best_fitness_history[-1])\n\n            if self.no_improvement_count > self.restart_threshold:\n                population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                f_opt = np.min(fitness)\n                x_opt = population[np.argmin(fitness)]\n                self.no_improvement_count = 0\n                self.memory_CR = np.full(self.memory_size, 0.5)\n                self.memory_F = np.full(self.memory_size, 0.5)\n\n        return f_opt, x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm SHADERestartDE scored 0.510 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2756cf9e-1f19-4acc-be51-ceb333f6dde6"], "operator": null, "metadata": {"aucs": [0.1931109358728882, 0.2801005095575262, 0.7513306937152722, 0.8707847952819233, 0.8115655907415775, 0.8347868686679205, 0.3367932896995589, 0]}}
{"id": "46c9f26d-d793-467c-ab1c-dbe37f4d1f53", "fitness": 0.6175764899198345, "name": "AdaptiveMutationDE", "description": "Simplified Adaptive DE with a dynamically adjusted mutation factor based on population diversity and a greedy update strategy.", "code": "import numpy as np\n\nclass AdaptiveMutationDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, CR=0.9, F_initial=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.F = F_initial\n\n    def __call__(self, func):\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            std = np.std(population)\n            self.F = np.clip(self.F + 0.1 * (std - 0.5), 0.1, 0.9)\n\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n                x_mutated = population[i] + self.F * (x_r2 - x_r3)\n                x_trial = np.where(np.random.rand(self.dim) < self.CR, x_mutated, population[i])\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = x_trial\n                    if f_trial < f_opt:\n                        f_opt = f_trial\n                        x_opt = x_trial\n\n        return f_opt, x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveMutationDE scored 0.618 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5c7c5f60-ee89-4813-8282-4ab1c1e8c56d"], "operator": null, "metadata": {"aucs": [0.2006392491918948, 0.5349708745434716, 0.5907326491836423, 0.8394140732568804, 0.5500620141535537, 0.7178233595585297, 0.4684228114497929, 0.501713346061131, 0.641096697789229, 0.5365064405773763, 0.8335151546405792, 0.9994960015780361, 0.5234922998595433, 0.6119656142051119, 0.9018829448678357, 0.6782185430240935, 0.5386336545449335, 0.7615427314046975, 0.30012176416067304, 0.6212795743456863]}}
{"id": "377467d7-0d4e-4be9-802f-8fe2b506d891", "fitness": 0.5592164672618166, "name": "AdaptiveMutationDE", "description": "Simplified Adaptive DE with a decaying mutation factor, mirrored boundary handling, and stochastic ranking selection, while removing the explicit diversity check for efficiency.", "code": "import numpy as np\n\nclass AdaptiveMutationDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, CR=0.9, F_initial=0.5, F_decay=0.99, p_selection=0.9):\n        \"\"\"\n        Initialize the Adaptive Mutation DE.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            CR (float): Differential evolution crossover rate.\n            F_initial (float): Initial mutation factor.\n            F_decay (float): Decay rate for F.\n            p_selection (float): Probability of selecting better individual in stochastic ranking.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.F = F_initial  # Initial mutation factor\n        self.F_decay = F_decay # Decay rate for F\n        self.p_selection = p_selection\n\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using a DE strategy with adaptive mutation and mirrored sampling.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            # Differential Evolution part\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Mutation with mirrored sampling\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n\n                # Mirrored sampling: if the mutated vector goes out of bounds, reflect it back\n                x_mutated = population[i] + self.F * (x_r2 - x_r3)\n                \n                # Crossover\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Repair bounds and mirrored sampling\n                for j in range(self.dim):\n                    if x_trial[j] < self.bounds_lb:\n                        x_trial[j] = self.bounds_lb + (self.bounds_lb - x_trial[j])\n                    elif x_trial[j] > self.bounds_ub:\n                        x_trial[j] = self.bounds_ub - (x_trial[j] - self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Stochastic Ranking Selection\n                if np.random.rand() < self.p_selection:\n                    if f_trial < fitness[i]:\n                        # Update F based on improvement\n                        if f_trial < f_opt:\n                            self.F = self.F * self.F_decay\n                            f_opt = f_trial\n                            x_opt = x_trial\n                        fitness[i] = f_trial\n                        population[i] = x_trial\n        return f_opt, x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveMutationDE scored 0.559 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["bc44013b-8bb7-4663-b8a6-7eca7a47ec90"], "operator": null, "metadata": {"aucs": [0.15398900508932478, 0.5795428368051194, 0.5632793050622833, 0.7918937189518478, 0.4429581896914444, 0.7371453015412703, 0.33990850118730687, 0.503301410100132, 0.5837718808300685, 0.19494928812712464, 0.8367861461270545, 0.9938835189955085, 0.5825665782981956, 0.37654034148554205, 0.8852328280603913, 0.6630820584466055, 0.45685710189003537, 0.8009815994739848, 0.1952169866390615, 0.5024427484340286]}}
{"id": "04d31889-e8dd-4e1c-a75e-6de68ae87bdc", "fitness": 0.5411303141209038, "name": "AdaptiveMutationDE", "description": "Simplified Adaptive DE with a self-adaptive mutation factor, mirrored sampling, and stochastic ranking.", "code": "import numpy as np\n\nclass AdaptiveMutationDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, CR=0.9, F_initial=0.5):\n        \"\"\"\n        Initialize the Adaptive Mutation DE.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            CR (float): Differential evolution crossover rate.\n            F_initial (float): Initial mutation factor.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.F = F_initial  # Initial mutation factor\n        self.p_selection = 0.9 # Probability of selecting better individual\n\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using a DE strategy with adaptive mutation and mirrored sampling.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            # Differential Evolution part\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Mutation with mirrored sampling\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n\n                # Mirrored sampling: if the mutated vector goes out of bounds, reflect it back\n                x_mutated = population[i] + self.F * (x_r2 - x_r3)\n                \n                # Crossover\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Repair bounds and mirrored sampling\n                for j in range(self.dim):\n                    if x_trial[j] < self.bounds_lb:\n                        x_trial[j] = self.bounds_lb + (self.bounds_lb - x_trial[j])\n                    elif x_trial[j] > self.bounds_ub:\n                        x_trial[j] = self.bounds_ub - (x_trial[j] - self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Stochastic Ranking Selection\n                if np.random.rand() < self.p_selection:\n                    if f_trial < fitness[i]:\n                        # Adaptive F: Reduce F if the solution improves, otherwise increase\n                        if f_trial < f_opt:\n                            f_opt = f_trial\n                            x_opt = x_trial\n                            self.F = max(0.1, self.F * 0.9) # Reduce F, but not too much\n                        else:\n                            self.F = min(1.0, self.F * 1.1) # Increase F, but not too much\n                        fitness[i] = f_trial\n                        population[i] = x_trial\n\n        return f_opt, x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveMutationDE scored 0.541 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["bc44013b-8bb7-4663-b8a6-7eca7a47ec90"], "operator": null, "metadata": {"aucs": [0.22131877763463959, 0.38620612215985384, 0.4966690004375398, 0.7167224069905311, 0.5084453528528252, 0.6275107100657324, 0.32410355773670185, 0.44087254770468376, 0.5629814750639137, 0.35776783970747306, 0.785105831584223, 0.9958581216339027, 0.3563914473255646, 0.5843282642978936, 0.8743214614705801, 0.5911701858087648, 0.43590943889979594, 0.7239954311022179, 0.311622656844215, 0.5213056530970273]}}
{"id": "992eb613-8ad8-4b8a-a59f-e2e418724fb1", "fitness": 0.6019691386156232, "name": "AdaptiveMutationDE", "description": "Simplified Adaptive DE with self-adaptive mutation factor and simplified crossover to improve exploration and exploitation balance.", "code": "import numpy as np\n\nclass AdaptiveMutationDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, CR=0.9, F_initial=0.5):\n        \"\"\"\n        Initialize the Adaptive Mutation DE.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            CR (float): Differential evolution crossover rate.\n            F_initial (float): Initial mutation factor.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.F = F_initial  # Initial mutation factor\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using a DE strategy with adaptive mutation.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            # Differential Evolution part\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n                self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0) # Adapt F\n                x_mutated = population[i] + self.F * (x_r2 - x_r3)\n\n                # Crossover - Simplified: uniform crossover\n                crossover_mask = np.random.rand(self.dim) < self.CR\n                x_trial = np.where(crossover_mask, x_mutated, population[i])\n\n                # Repair bounds\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    if f_trial < f_opt:\n                        f_opt = f_trial\n                        x_opt = x_trial\n                    fitness[i] = f_trial\n                    population[i] = x_trial\n\n        return f_opt, x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveMutationDE scored 0.602 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5c7c5f60-ee89-4813-8282-4ab1c1e8c56d"], "operator": null, "metadata": {"aucs": [0.20788323767444727, 0.5856107335670417, 0.5653341798762959, 0.7986608040478007, 0.6249132999291258, 0.7076708251198813, 0.34808092637317545, 0.5293366255443674, 0.6607404149612928, 0.43138845267106707, 0.8243269568036711, 0.9983017637427073, 0.5920021809040057, 0.5592336521267023, 0.9299178009418969, 0.7013164999116639, 0.46409128266918676, 0.8060445548602571, 0.20480702958673358, 0.4997215510011406]}}
{"id": "c77935d2-d91d-4c08-a4b7-a1d4dad41a96", "fitness": 0.5897894597523229, "name": "AdaptiveMutationDE", "description": "Simplified Adaptive DE with self-adaptive mutation factor and simplified crossover for faster adaptation.", "code": "import numpy as np\n\nclass AdaptiveMutationDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, CR=0.9, F_initial=0.5):\n        \"\"\"\n        Initialize the Adaptive Mutation DE.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            CR (float): Differential evolution crossover rate.\n            F_initial (float): Initial mutation factor.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.F = F_initial  # Initial mutation factor\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using a DE strategy with adaptive mutation and simplified crossover.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            # Differential Evolution part\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n\n                # Adaptive F: Adjust F based on individual success.\n                F = self.F * (1 + np.random.normal(0, 0.1))  # Self-adaptive F\n                F = np.clip(F, 0.1, 1.0) # Keep F within bounds\n                \n                x_mutated = population[i] + F * (x_r2 - x_r3)\n\n                # Crossover: Simplified binomial crossover\n                x_trial = np.copy(population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR:\n                        x_trial[j] = x_mutated[j]\n\n                # Repair bounds\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Selection:  Greedy selection\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = x_trial\n\n                    if f_trial < f_opt:\n                        f_opt = f_trial\n                        x_opt = x_trial\n\n        return f_opt, x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveMutationDE scored 0.590 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["09c5ff6d-7973-43d2-909b-565f4bd8bde3"], "operator": null, "metadata": {"aucs": [0.1832732258329811, 0.5691160999495367, 0.6104933822846765, 0.827763579925361, 0.4911239175883044, 0.7278099073133579, 0.3565330367670807, 0.5128742531676895, 0.6354349961180443, 0.32313207706020297, 0.8604170006525651, 0.995914817340775, 0.5897795094703656, 0.48021157897584144, 0.9299174315127106, 0.6820199769701651, 0.4584303818427745, 0.8373818658545037, 0.21433595284315388, 0.5098262035763692]}}
{"id": "abc7d371-cb93-46c1-a3d3-1eb95b76f621", "fitness": 0.5379918705802298, "name": "AdaptiveMutationDE", "description": "Adaptive DE with a simplified update mechanism using a pool of F values and a decaying local search to enhance exploitation.", "code": "import numpy as np\n\nclass AdaptiveMutationDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, CR=0.9, F_pool=[0.3, 0.5, 0.7], momentum=0.3, local_search_prob=0.1):\n        \"\"\"\n        Initialize the Adaptive Mutation DE.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            CR (float): Differential evolution crossover rate.\n            F_pool (list): A pool of mutation factors to choose from.\n            momentum (float): Momentum for F adaptation.\n            local_search_prob (float): Probability of performing local search.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.F_pool = F_pool\n        self.F = np.random.choice(self.F_pool)  # Initialize F with a random value from the pool\n        self.momentum = momentum\n        self.local_search_prob = local_search_prob\n        self.ls_decay = 0.99 # Decay rate for local search probability\n        self.f_history = []\n\n    def local_search(self, x, func):\n        \"\"\"\n        Perform a local search around the given solution.\n\n        Args:\n            x (np.ndarray): The solution to perform local search around.\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The improved solution and its function value.\n        \"\"\"\n        x_new = x + np.random.normal(0, 0.1, size=self.dim)  # Add Gaussian noise\n        x_new = np.clip(x_new, self.bounds_lb, self.bounds_ub)  # Repair bounds\n        f_new = func(x_new)\n        self.budget -= 1\n        return x_new, f_new\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using a DE strategy with adaptive mutation and orthogonal crossover.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n        \n        previous_f_opt = f_opt\n\n        while self.budget > 0:\n            # Differential Evolution part\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n\n                # Choose F from pool\n                self.F = np.random.choice(self.F_pool)\n                F = self.F\n                \n                x_mutated = population[i] + F * (x_r2 - x_r3)\n\n                # Crossover: Orthogonal Crossover\n                x_trial = np.copy(population[i])\n                num_changes = int(self.CR * self.dim)  # Number of dimensions to change\n                change_indices = np.random.choice(self.dim, num_changes, replace=False)\n                x_trial[change_indices] = x_mutated[change_indices]\n\n                # Repair bounds\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Selection:  Greedy selection\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = x_trial\n\n                    if f_trial < f_opt:\n                        f_opt = f_trial\n                        x_opt = x_trial\n                        \n                # Local Search\n                if np.random.rand() < self.local_search_prob:\n                    x_ls, f_ls = self.local_search(population[i], func)\n                    if f_ls < fitness[i]:\n                        fitness[i] = f_ls\n                        population[i] = x_ls\n\n                        if f_ls < f_opt:\n                            f_opt = f_ls\n                            x_opt = x_ls\n            self.local_search_prob *= self.ls_decay\n            previous_f_opt = f_opt # Store current best fitness for next iteration\n\n        return f_opt, x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveMutationDE scored 0.538 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["09c5ff6d-7973-43d2-909b-565f4bd8bde3"], "operator": null, "metadata": {"aucs": [0.17709372609286345, 0.29459251472718273, 0.5672442431483393, 0.7757429844450678, 0.4856259189047485, 0.7363338153492938, 0.33046272173189006, 0.490094555790772, 0.5712124997007267, 0.2439003078115467, 0.8174648958428984, 0.9901965352802761, 0.3108615104014546, 0.49785601378654043, 0.8649600841387871, 0.6580250061550377, 0.4350737876305154, 0.7991496942178251, 0.2206061332013245, 0.4933404632475076]}}
{"id": "89d1bb65-56c1-4668-917d-3e97d2015167", "fitness": 0.3997372939568565, "name": "EnhancedAdaptiveDE", "description": "Enhanced Adaptive DE with aging, dynamic CR adaptation, and orthogonal learning for improved exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, CR=0.7, F=0.5, local_search_prob=0.1, age_limit=50):\n        \"\"\"\n        Initialize the Enhanced Adaptive DE.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            CR (float): Initial differential evolution crossover rate.\n            F (float): Initial mutation factor.\n            local_search_prob (float): Probability of performing a local search step.\n            age_limit (int): Maximum age before an individual is replaced.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.F = F  # Mutation factor\n        self.local_search_prob = local_search_prob\n        self.age_limit = age_limit\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using an enhanced adaptive DE strategy.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n\n        mutation_factors = np.full(self.pop_size, self.F)  # Individual F values\n        crossover_rates = np.full(self.pop_size, self.CR) # Individual CR values\n        ages = np.zeros(self.pop_size, dtype=int) # Ages of individuals\n\n        while self.budget > 0:\n            # Differential Evolution part\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n                x_mutated = population[i] + mutation_factors[i] * (x_r2 - x_r3)\n\n                # Crossover\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < crossover_rates[i] or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Repair bounds\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Selection: Direct replacement if trial is better\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = x_trial\n                    if f_trial < f_opt:\n                        f_opt = f_trial\n                        x_opt = x_trial\n                    mutation_factors[i] = min(1.0, mutation_factors[i] * 1.1)  # Increase F\n                    crossover_rates[i] = min(0.9, crossover_rates[i] * 1.1) #Increase CR\n                    ages[i] = 0 #Reset age\n\n                    # Local Search\n                    if np.random.rand() < self.local_search_prob:\n                        step = np.random.uniform(-0.1, 0.1, size=self.dim)\n                        x_local = np.clip(x_trial + step, self.bounds_lb, self.bounds_ub)\n                        f_local = func(x_local)\n                        self.budget -= 1\n                        if f_local < f_trial:\n                            fitness[i] = f_local\n                            population[i] = x_local\n                            if f_local < f_opt:\n                                f_opt = f_local\n                                x_opt = x_local\n\n                else:\n                    mutation_factors[i] = max(0.1, mutation_factors[i] * 0.9)  # Decrease F\n                    crossover_rates[i] = max(0.1, crossover_rates[i] * 0.9) #Decrease CR\n                    ages[i] += 1  # Increment age\n\n                # Orthogonal Learning (every generation)\n                if self.budget > self.pop_size: #Ensure enough budget for OL\n                    basis_vectors = np.random.normal(0, 1, size=(self.dim, self.dim))\n                    Q, R = np.linalg.qr(basis_vectors) #Orthogonal basis\n                    new_point = population[i] + 0.05 * np.dot(np.random.uniform(-1, 1, size=self.dim), Q.T)\n                    new_point = np.clip(new_point, self.bounds_lb, self.bounds_ub)\n                    f_new = func(new_point)\n                    self.budget -= 1\n                    if f_new < fitness[i]:\n                        fitness[i] = f_new\n                        population[i] = new_point\n                        if f_new < f_opt:\n                            f_opt = f_new\n                            x_opt = new_point\n\n                # Aging mechanism: replace old individuals\n                if ages[i] > self.age_limit:\n                    population[i] = np.random.uniform(self.bounds_lb, self.bounds_ub, size=self.dim)\n                    fitness[i] = func(population[i])\n                    self.budget -= 1\n                    ages[i] = 0\n                    if fitness[i] < f_opt:\n                        f_opt = fitness[i]\n                        x_opt = population[i]\n\n        return f_opt, x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm EnhancedAdaptiveDE scored 0.400 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2756cf9e-1f19-4acc-be51-ceb333f6dde6"], "operator": null, "metadata": {"aucs": [0.10630171656429388, 0.21451651817930129, 0.45974035739830565, 0.7786436439967999, 0.2563877425944967, 0.49252161775367076, 0.2763564146332228, 0.3482809188368027, 0.24322150770841777, 0.17133861865335842, 0.35640383992111624, 0.9989538564751784, 0.3103444403113025, 0.27106674149740606, 0.729208694811111, 0.4363101226146653, 0.30917997193852353, 0.6009678575501762, 0.16284143843425736, 0.47215985926472337]}}
{"id": "e70d53b9-9a34-4186-ac63-907be06875d4", "fitness": 0.4197943006526434, "name": "SelfAdaptiveDE", "description": "Self-Adaptive Differential Evolution with stochastic ranking and archive to enhance exploration and exploitation.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, archive_size=5):\n        \"\"\"\n        Initialize the Self-Adaptive Differential Evolution algorithm.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            archive_size (int): The size of the archive.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.CR = 0.5  # Initial crossover rate\n        self.F = 0.7   # Initial mutation factor\n        self.archive = []\n        self.archive_fitness = []\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using a self-adaptive DE strategy with stochastic ranking.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            # Iterate through the population\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Self-adaptive parameters\n                CR_i = np.random.normal(self.CR, 0.1)\n                CR_i = np.clip(CR_i, 0.0, 1.0)\n                F_i = np.random.normal(self.F, 0.1)\n                F_i = np.clip(F_i, 0.1, 1.0)\n\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n                \n                # Add a random vector from the archive if it's not empty\n                if self.archive:\n                    idx_archive = np.random.randint(len(self.archive))\n                    x_r4 = self.archive[idx_archive]\n                    x_mutated = population[i] + F_i * (x_r2 - x_r3) + F_i * (x_r1 - x_r4)\n                else:\n                    x_mutated = population[i] + F_i * (x_r2 - x_r3)\n\n                # Crossover\n                x_trial = np.where(np.random.rand(self.dim) < CR_i, x_mutated, population[i])\n\n                # Repair bounds\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Stochastic Ranking\n                p_rank = 0.45 # Probability of ranking based on feasibility\n\n                if np.random.rand() < p_rank or (fitness[i] <= 0 and f_trial <= 0) or (fitness[i] > 0 and f_trial > 0):\n                    if f_trial < fitness[i]:\n                        # Update the population\n                        fitness[i] = f_trial\n                        population[i] = x_trial\n\n                        # Update the archive\n                        if len(self.archive) < self.archive_size:\n                            self.archive.append(population[i].copy())\n                            self.archive_fitness.append(fitness[i])\n                        else:\n                            # Replace the worst element in the archive\n                            max_archive_index = np.argmax(self.archive_fitness)\n                            if fitness[i] < self.archive_fitness[max_archive_index]:\n                                self.archive[max_archive_index] = population[i].copy()\n                                self.archive_fitness[max_archive_index] = fitness[i]\n                else:\n                    if np.random.rand() < 0.5: # With some probability replace, to ensure progress\n                        fitness[i] = f_trial\n                        population[i] = x_trial\n                        if len(self.archive) < self.archive_size:\n                            self.archive.append(population[i].copy())\n                            self.archive_fitness.append(fitness[i])\n                        else:\n                            # Replace the worst element in the archive\n                            max_archive_index = np.argmax(self.archive_fitness)\n                            if fitness[i] < self.archive_fitness[max_archive_index]:\n                                self.archive[max_archive_index] = population[i].copy()\n                                self.archive_fitness[max_archive_index] = fitness[i]\n\n\n                # Update best solution\n                if f_trial < f_opt:\n                    f_opt = f_trial\n                    x_opt = x_trial\n\n        return f_opt, x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm SelfAdaptiveDE scored 0.420 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5c7c5f60-ee89-4813-8282-4ab1c1e8c56d"], "operator": null, "metadata": {"aucs": [0.14547305998513127, 0.21967941053711104, 0.39824216190514394, 0.5049435823582396, 0.2982747347926741, 0.5500183127733094, 0.30410937141165106, 0.39684513313105085, 0.3764150771968552, 0.22348820606617092, 0.5627655304150826, 0.9830131087475364, 0.2668035067458223, 0.3143067054970542, 0.7180822730275633, 0.4743055416072045, 0.33840092411683065, 0.6395407011780377, 0.18720987558498658, 0.49396879597541177]}}
{"id": "5c4881e1-c2c2-449b-8156-7b235bf35fd3", "fitness": 0.0, "name": "AdaptiveMutationDE", "description": "Adaptive DE with self-adaptive mutation factor and crossover rate, dynamically adjusted based on success history.", "code": "import numpy as np\n\nclass AdaptiveMutationDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20):\n        \"\"\"\n        Initialize the Adaptive Mutation DE.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.memory_size = 5  # Size of the success history memory\n        self.archive = [] # Archive for storing potentially good solutions.\n\n        # Initialize CR and F memories\n        self.CR_memory = np.full(self.memory_size, 0.5)\n        self.F_memory = np.full(self.memory_size, 0.5)\n        self.memory_idx = 0\n\n        self.CR = 0.5  # Initial crossover rate\n        self.F = 0.5  # Initial mutation factor\n\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using a DE strategy with adaptive mutation.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            # Differential Evolution part\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Adapt CR and F\n                self.CR = np.random.choice(self.CR_memory)\n                self.F = np.random.choice(self.F_memory)\n\n\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n                x_mutated = population[i] + self.F * (x_r2 - x_r3)\n\n                # Crossover\n                x_trial = np.where(np.random.rand(self.dim) < self.CR, x_mutated, population[i])\n\n                # Repair bounds\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    # Update memory\n                    self.CR_memory[self.memory_idx] = self.CR\n                    self.F_memory[self.memory_idx] = self.F\n                    self.memory_idx = (self.memory_idx + 1) % self.memory_size\n\n\n                    if f_trial < f_opt:\n                        f_opt = f_trial\n                        x_opt = x_trial\n                    fitness[i] = f_trial\n                    population[i] = x_trial\n                else:\n                     self.archive.append(population[i].copy()) # Store the parent in the archive.\n\n\n            #Stochastic ranking to maintain diversity (if archive is sufficiently large)\n            if len(self.archive) > self.pop_size:\n\n              combined_pop = np.vstack((population, np.array(self.archive[:self.pop_size]))) # Limit archive size\n\n              combined_fitness = np.array([func(x) for x in combined_pop])\n              self.budget -= self.pop_size\n\n              sorted_indices = np.argsort(combined_fitness)\n\n              population = combined_pop[sorted_indices[:self.pop_size]]\n              fitness = combined_fitness[sorted_indices[:self.pop_size]]\n              f_opt = np.min(fitness)\n              x_opt = population[np.argmin(fitness)]\n              self.archive = [] # Reset archive\n\n        return f_opt, x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveMutationDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5c7c5f60-ee89-4813-8282-4ab1c1e8c56d"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "e31bbc3c-12cb-464d-8d45-dec391d86f79", "fitness": 0.49952013250002025, "name": "AdaptiveCauchyDE", "description": "A DE variant that combines adaptive mutation with a success-history based adaptation of the scaling factor and a Cauchy mutation operator.", "code": "import numpy as np\n\nclass AdaptiveCauchyDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, CR=0.9, F_initial=0.5, F_decay=0.99, \n                 cauchy_scale=0.1, success_history_size=10):\n        \"\"\"\n        Initialize the Adaptive Cauchy DE.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            CR (float): Crossover rate.\n            F_initial (float): Initial mutation factor.\n            F_decay (float): Decay rate for F.\n            cauchy_scale (float): Scale parameter for Cauchy mutation.\n            success_history_size (int): Size of the success history for F adaptation.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.F = F_initial  # Initial mutation factor\n        self.F_decay = F_decay # Decay rate for F\n        self.cauchy_scale = cauchy_scale\n        self.success_history = []\n        self.success_history_size = success_history_size\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using a DE strategy with adaptive mutation, Cauchy mutation and success-history based F adaptation.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            # Differential Evolution part\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n                \n                # Apply Cauchy mutation with probability 0.5\n                if np.random.rand() < 0.5:\n                    cauchy_noise = self.cauchy_scale * np.random.standard_cauchy(size=self.dim)\n                    x_mutated = population[i] + self.F * (x_r2 - x_r3) + cauchy_noise\n                else:\n                     x_mutated = population[i] + self.F * (x_r2 - x_r3)\n\n                # Crossover\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Repair bounds\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    # Update success history\n                    self.success_history.append(self.F)\n                    if len(self.success_history) > self.success_history_size:\n                        self.success_history.pop(0)\n\n                    # Update F based on success history (mean of successful F values)\n                    if self.success_history:\n                        self.F = np.mean(self.success_history) * self.F_decay\n                    else:\n                        self.F = self.F_initial * self.F_decay # Reset F if no successes\n\n                    if f_trial < f_opt:\n                        f_opt = f_trial\n                        x_opt = x_trial\n                    fitness[i] = f_trial\n                    population[i] = x_trial\n        return f_opt, x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveCauchyDE scored 0.500 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["bc44013b-8bb7-4663-b8a6-7eca7a47ec90"], "operator": null, "metadata": {"aucs": [0.16219026120489255, 0.40609579612475044, 0.48746984710168373, 0.6429124216520294, 0.39234603365789256, 0.5937141608977868, 0.32534415248071236, 0.4320356720583698, 0.42722903130986967, 0.19992539848886803, 0.7957996281369875, 0.9944249790340127, 0.4952557653042492, 0.36008800769444427, 0.9100586056422777, 0.5393493064161681, 0.4059874793434092, 0.7318057078057276, 0.1902391385185297, 0.49813125712774253]}}
{"id": "7deb34f5-8284-4385-96d0-974a7f5b001d", "fitness": -Infinity, "name": "AdaptiveMutationDE", "description": "Adaptive DE with dynamic mutation factor, crossover rate, and population size based on function evaluations and landscape features.", "code": "import numpy as np\n\nclass AdaptiveMutationDE:\n    def __init__(self, budget=10000, dim=10, pop_size_initial=20, CR_initial=0.9, F_initial=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_initial = pop_size_initial\n        self.pop_size = pop_size_initial\n        self.CR_initial = CR_initial\n        self.CR = CR_initial\n        self.F_initial = F_initial\n        self.F = F_initial\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.archive_rate = 0.1 \n        self.archive = []\n\n    def __call__(self, func):\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            std = np.std(population)\n            self.F = np.clip(self.F_initial + 0.1 * (std - 0.5), 0.1, 0.9)\n            self.CR = np.clip(self.CR_initial + 0.1 * (0.5 - std), 0.1, 0.9)\n\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n\n                # Option to select from archive\n                if len(self.archive) > 0 and np.random.rand() < self.archive_rate:\n                    idx_archive = np.random.randint(len(self.archive))\n                    x_r1 = self.archive[idx_archive]\n                \n                x_mutated = population[i] + self.F * (x_r2 - x_r3)\n                x_trial = np.where(np.random.rand(self.dim) < self.CR, x_mutated, population[i])\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    # Archive the replaced individual\n                    self.archive.append(population[i].copy())\n                    if len(self.archive) > self.pop_size:\n                        self.archive.pop(0) #Keep archive size limited\n\n                    fitness[i] = f_trial\n                    population[i] = x_trial\n                    if f_trial < f_opt:\n                        f_opt = f_trial\n                        x_opt = x_trial\n\n            # Adjust population size (example: linear decrease)\n            self.pop_size = max(int(self.pop_size_initial * (1 - generation / (self.budget / self.pop_size_initial * 2) )), 10) # Ensure a minimum population size\n            if self.pop_size < population.shape[0]:\n               indices = np.argsort(fitness)[:self.pop_size]\n               population = population[indices]\n               fitness = fitness[indices]\n            elif self.pop_size > population.shape[0]:\n                num_new = self.pop_size - population.shape[0]\n                new_individuals = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(num_new, self.dim))\n                new_fitness = np.array([func(x) for x in new_individuals])\n                self.budget -= num_new\n                population = np.vstack((population, new_individuals))\n                fitness = np.concatenate((fitness, new_fitness))\n\n        return f_opt, x_opt", "configspace": "", "generation": 8, "feedback": "An exception occurred: float division by zero.", "error": "", "parent_ids": ["46c9f26d-d793-467c-ab1c-dbe37f4d1f53"], "operator": null, "metadata": {}}
{"id": "a4d7eadc-b40a-420d-be61-49a966016c3a", "fitness": 0.5750673483351367, "name": "AdaptiveMutationDE", "description": "Simplified Adaptive DE with self-adjusting mutation strength based on success, mirroring for boundary handling, and reduced complexity for faster optimization.", "code": "import numpy as np\n\nclass AdaptiveMutationDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, CR=0.9, F_initial=0.5, F_decay=0.99):\n        \"\"\"\n        Initialize the Adaptive Mutation DE.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            CR (float): Differential evolution crossover rate.\n            F_initial (float): Initial mutation factor.\n            F_decay (float): Decay rate for F.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.F = F_initial  # Initial mutation factor\n        self.F_decay = F_decay # Decay rate for F\n\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using a simplified DE strategy with adaptive mutation and mirrored sampling.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            # Differential Evolution part\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Mutation with mirrored sampling\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n\n                x_mutated = population[i] + self.F * (x_r2 - x_r3)\n                \n                # Crossover\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Repair bounds and mirrored sampling\n                for j in range(self.dim):\n                    if x_trial[j] < self.bounds_lb:\n                        x_trial[j] = self.bounds_lb + (self.bounds_lb - x_trial[j])\n                    elif x_trial[j] > self.bounds_ub:\n                        x_trial[j] = self.bounds_ub - (x_trial[j] - self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Greedy Selection\n                if f_trial < fitness[i]:\n                    # Update F based on improvement\n                    if f_trial < f_opt:\n                        self.F = self.F * self.F_decay\n                        f_opt = f_trial\n                        x_opt = x_trial\n                    fitness[i] = f_trial\n                    population[i] = x_trial\n        return f_opt, x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveMutationDE scored 0.575 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["377467d7-0d4e-4be9-802f-8fe2b506d891"], "operator": null, "metadata": {"aucs": [0.1733108660233058, 0.6693912872213904, 0.5754568709652059, 0.7843906609144559, 0.49692956482844, 0.7307196064707031, 0.3365728559496819, 0.5179967954407265, 0.5520264147056282, 0.21936571370347435, 0.8567339666361881, 0.991854707283751, 0.677235514150869, 0.4349525571015793, 0.8859690066037931, 0.654623449906, 0.41367862531212396, 0.8200028977606035, 0.21260150298667557, 0.4975341027381379]}}
{"id": "8c5cf728-0353-4af2-a3a2-f158124435b4", "fitness": 0.48999487054269497, "name": "AdaptiveMutationDE", "description": "Adaptive DE with decaying mutation factor, mirrored boundary handling, stochastic ranking, and a simplified self-adaptive CR.", "code": "import numpy as np\n\nclass AdaptiveMutationDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, CR_initial=0.9, CR_decay=0.99, F_initial=0.5, F_decay=0.99, p_selection=0.9):\n        \"\"\"\n        Initialize the Adaptive Mutation DE.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            CR_initial (float): Initial differential evolution crossover rate.\n            CR_decay (float): Decay rate for CR.\n            F_initial (float): Initial mutation factor.\n            F_decay (float): Decay rate for F.\n            p_selection (float): Probability of selecting better individual in stochastic ranking.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR_initial  # Initial CR\n        self.CR_initial = CR_initial\n        self.CR_decay = CR_decay\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.F = F_initial  # Initial mutation factor\n        self.F_decay = F_decay # Decay rate for F\n        self.p_selection = p_selection\n\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using a DE strategy with adaptive mutation and mirrored sampling.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            # Differential Evolution part\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Mutation with mirrored sampling\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n\n                # Mirrored sampling: if the mutated vector goes out of bounds, reflect it back\n                x_mutated = population[i] + self.F * (x_r2 - x_r3)\n                \n                # Crossover\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Repair bounds and mirrored sampling\n                for j in range(self.dim):\n                    if x_trial[j] < self.bounds_lb:\n                        x_trial[j] = self.bounds_lb + (self.bounds_lb - x_trial[j])\n                    elif x_trial[j] > self.bounds_ub:\n                        x_trial[j] = self.bounds_ub - (x_trial[j] - self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Stochastic Ranking Selection\n                if np.random.rand() < self.p_selection:\n                    if f_trial < fitness[i]:\n                        # Update F and CR based on improvement\n                        if f_trial < f_opt:\n                            self.F = self.F * self.F_decay\n                            self.CR = self.CR * self.CR_decay # Decay CR as well\n                            f_opt = f_trial\n                            x_opt = x_trial\n                        fitness[i] = f_trial\n                        population[i] = x_trial\n\n            #Decay even if there is no improvement in the entire generation:\n            self.F = self.F * self.F_decay\n            self.CR = self.CR * self.CR_decay\n        return f_opt, x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveMutationDE scored 0.490 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["377467d7-0d4e-4be9-802f-8fe2b506d891"], "operator": null, "metadata": {"aucs": [0.16088564499892966, 0.26229877073530006, 0.5349869394956104, 0.7810171059107166, 0.4624822056123211, 0.5514480369456187, 0.30130573306332853, 0.40265467896012186, 0.47213168897247204, 0.20382177629741038, 0.7694644130907555, 0.9989900534147557, 0.3156998145801533, 0.3532836783603186, 0.7957160489877697, 0.5526972314443321, 0.3481752477289669, 0.793744542147905, 0.2411558751717371, 0.49793792493537625]}}
{"id": "b45c6537-36cf-445f-b57a-f0a75e82d912", "fitness": 0.7308065376248905, "name": "AdaptiveMutationDE", "description": "Adaptive DE with simplified mutation and crossover, enhanced F adaptation, and efficient bound handling for improved exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveMutationDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, CR=0.9, F_initial=0.5):\n        \"\"\"\n        Initialize the Adaptive Mutation DE.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            CR (float): Differential evolution crossover rate.\n            F_initial (float): Initial mutation factor.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.F = F_initial  # Initial mutation factor\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using a DE strategy with adaptive mutation and simplified crossover.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            # Differential Evolution part\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n\n                # Adaptive F: Adjust F based on individual success. Simplified adaptation.\n                F = self.F + 0.1 * np.random.normal(0, 1)\n                F = np.clip(F, 0.1, 1.0)\n\n                x_mutated = x_r1 + F * (x_r2 - x_r3) # Simplified mutation\n\n                # Crossover: Simplified binomial crossover\n                crossover_mask = np.random.rand(self.dim) < self.CR\n                x_trial = np.where(crossover_mask, x_mutated, population[i])\n\n                # Repair bounds directly within the crossover\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Selection:  Greedy selection\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = x_trial\n\n                    if f_trial < f_opt:\n                        f_opt = f_trial\n                        x_opt = x_trial\n\n        return f_opt, x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveMutationDE scored 0.731 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c77935d2-d91d-4c08-a4b7-a1d4dad41a96"], "operator": null, "metadata": {"aucs": [0.5213946893007269, 0.26396932300421394, 0.632736056570359, 0.9455031313139687, 0.8295234496368091, 0.9133822022744176, 0.8673859688906259, 0.8650986764573803, 0.8842660467155752, 0.7712166943219517, 0.9279510469819967, 0.9975369335790418, 0.34615701300502555, 0.8694350647652637, 0.9385151994007197, 0.9106580074272235, 0.42960860360913766, 0.9294706999182665, 0.254051774670477, 0.5182701706546289]}}
{"id": "238bccac-e35a-4f48-b598-b0df32ba2aac", "fitness": 0.4383263186488735, "name": "AdaptiveMutationDE", "description": "Simplified Adaptive DE with self-adjusting mutation strength based on fitness improvement and simplified crossover for efficiency.", "code": "import numpy as np\n\nclass AdaptiveMutationDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, CR=0.9, F_initial=0.5):\n        \"\"\"\n        Initialize the Adaptive Mutation DE.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            CR (float): Differential evolution crossover rate.\n            F_initial (float): Initial mutation factor.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.F = F_initial  # Initial mutation factor\n        self.fitness_history = np.zeros(pop_size)  # Track fitness history\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using a DE strategy with adaptive mutation and simplified crossover.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        self.fitness_history[:] = fitness  # Initialize fitness history\n\n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            # Differential Evolution part\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n\n                # Adaptive F: Adjust F based on individual success.\n                # Base F adjustment on recent fitness improvement\n                fitness_improvement = self.fitness_history[i] - fitness[i] \n                if fitness_improvement > 0:\n                    self.F = self.F * 1.1  # Increase F if improvement\n                else:\n                    self.F = self.F * 0.9  # Decrease F if no improvement\n                self.F = np.clip(self.F, 0.1, 1.0) # Keep F within bounds\n\n                x_mutated = population[i] + self.F * (x_r2 - x_r3)\n\n                # Crossover: Simplified binomial crossover\n                x_trial = np.copy(population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR:\n                        x_trial[j] = x_mutated[j]\n\n                # Repair bounds\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Selection:  Greedy selection\n                if f_trial < fitness[i]:\n                    self.fitness_history[i] = fitness[i] #save fitness before updating for F adaptation\n                    fitness[i] = f_trial\n                    population[i] = x_trial\n\n                    if f_trial < f_opt:\n                        f_opt = f_trial\n                        x_opt = x_trial\n                else:\n                     self.fitness_history[i] = fitness[i] #save fitness if no update\n\n        return f_opt, x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveMutationDE scored 0.438 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c77935d2-d91d-4c08-a4b7-a1d4dad41a96"], "operator": null, "metadata": {"aucs": [0.12436937347098631, 0.3631503820853661, 0.4584597685297066, 0.33183262802080193, 0.24816605657853963, 0.44419276776492733, 0.28811563127799344, 0.3436862863519442, 0.3313829406816581, 0.1616081266412922, 0.7749225851534642, 0.9994220346568452, 0.5199908391991857, 0.20958305949343847, 0.8620134497717196, 0.520144815054577, 0.34961845439073413, 0.6482385344874342, 0.3073174360856391, 0.48031120328121624]}}
{"id": "13c87fc5-c3ce-48cf-9eea-280f72cac665", "fitness": 0.507237968909285, "name": "AdaptiveMutationDE", "description": "Adaptive DE with self-adaptive mutation and crossover rates, mirrored boundary handling, and stochastic ranking selection, aiming for better exploration.", "code": "import numpy as np\n\nclass AdaptiveMutationDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, CR_initial=0.9, F_initial=0.5, F_decay=0.99, p_selection=0.9):\n        \"\"\"\n        Initialize the Adaptive Mutation DE.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            CR_initial (float): Initial Differential evolution crossover rate.\n            F_initial (float): Initial mutation factor.\n            F_decay (float): Decay rate for F.\n            p_selection (float): Probability of selecting better individual in stochastic ranking.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR_initial  # Initial crossover rate\n        self.CR_initial = CR_initial\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.F = F_initial  # Initial mutation factor\n        self.F_decay = F_decay # Decay rate for F\n        self.p_selection = p_selection\n\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using a DE strategy with adaptive mutation and mirrored sampling.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n\n        CR_values = np.full(self.pop_size, self.CR_initial) # individual CR values\n\n        while self.budget > 0:\n            # Differential Evolution part\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Mutation with mirrored sampling\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n\n                # Mirrored sampling: if the mutated vector goes out of bounds, reflect it back\n                x_mutated = population[i] + self.F * (x_r2 - x_r3)\n                \n                # Crossover\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR_values[i] or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Repair bounds and mirrored sampling\n                for j in range(self.dim):\n                    if x_trial[j] < self.bounds_lb:\n                        x_trial[j] = self.bounds_lb + (self.bounds_lb - x_trial[j])\n                    elif x_trial[j] > self.bounds_ub:\n                        x_trial[j] = self.bounds_ub - (x_trial[j] - self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Stochastic Ranking Selection\n                if np.random.rand() < self.p_selection:\n                    if f_trial < fitness[i]:\n                        # Update F based on improvement and CR\n                        if f_trial < f_opt:\n                            self.F = self.F * self.F_decay\n                            f_opt = f_trial\n                            x_opt = x_trial\n                            CR_values[i] = np.clip(np.random.normal(self.CR_initial, 0.1), 0, 1)  # Adjust CR if improvement\n                        else:\n                            CR_values[i] = np.clip(np.random.normal(CR_values[i], 0.1), 0, 1)  # Perturb CR even without global improvement\n\n                        fitness[i] = f_trial\n                        population[i] = x_trial\n                    else:\n                        CR_values[i] = np.clip(np.random.normal(CR_values[i], 0.1), 0, 1) # Adapt CR even if no improvement in this iteration\n\n        return f_opt, x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveMutationDE scored 0.507 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["377467d7-0d4e-4be9-802f-8fe2b506d891"], "operator": null, "metadata": {"aucs": [0.16613859864649605, 0.35650591329299275, 0.5136217046145612, 0.744437815965826, 0.40921913045069414, 0.6267349201135546, 0.35163496119993176, 0.44390085743514585, 0.4571787995923373, 0.22844318026516652, 0.7788320658415862, 0.9994061654918815, 0.4360810420531259, 0.3322382980496744, 0.8617709332139303, 0.5841655071162071, 0.4336770220285414, 0.7120123893170338, 0.21913407575318833, 0.4896259977438242]}}
{"id": "6f98308a-7231-4db4-a312-1a10480ed80e", "fitness": 0.45251508076419034, "name": "AdaptiveMutationDE", "description": "Enhanced Adaptive DE with a learning rate inspired adaptation of F, mirrored boundary handling, and a worst-fit replacement strategy to improve exploration.", "code": "import numpy as np\n\nclass AdaptiveMutationDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, CR=0.9, F_initial=0.5, lr=0.1):\n        \"\"\"\n        Initialize the Adaptive Mutation DE.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            CR (float): Differential evolution crossover rate.\n            F_initial (float): Initial mutation factor.\n            lr (float): Learning rate for F adaptation.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.F = F_initial  # Initial mutation factor\n        self.lr = lr\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using a DE strategy with adaptive mutation.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            # Differential Evolution part\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n                x_mutated = population[i] + self.F * (x_r2 - x_r3)\n\n                # Crossover - Simplified: uniform crossover\n                crossover_mask = np.random.rand(self.dim) < self.CR\n                x_trial = np.where(crossover_mask, x_mutated, population[i])\n\n                # Repair bounds with mirroring\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n                x_trial = np.where(x_trial < self.bounds_lb, 2 * self.bounds_lb - x_trial, x_trial)\n                x_trial = np.where(x_trial > self.bounds_ub, 2 * self.bounds_ub - x_trial, x_trial)\n                \n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Selection: replace worst\n                if f_trial < np.max(fitness):\n                    worst_idx = np.argmax(fitness)\n                    if f_trial < f_opt:\n                        f_opt = f_trial\n                        x_opt = x_trial\n                    \n                    # Adjust F based on success (learning rate inspired)\n                    if f_trial < fitness[i]:\n                       self.F = np.clip(self.F * (1 + self.lr), 0.1, 1.0) # Increase F if successful\n                    else:\n                       self.F = np.clip(self.F * (1 - self.lr), 0.1, 1.0) # Decrease F if unsuccessful\n\n                    fitness[worst_idx] = f_trial\n                    population[worst_idx] = x_trial\n\n        return f_opt, x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveMutationDE scored 0.453 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["992eb613-8ad8-4b8a-a59f-e2e418724fb1"], "operator": null, "metadata": {"aucs": [0.3155127920336116, 0.3795197001943602, 0.2589011158526441, 0.8157696239674866, 0.8493978130487434, 0.19634657615900042, 0.21401732512625005, 0.24671956702105002, 0.24682036871486657, 0.2334490464943827, 0.9062144203944288, 0.999740868648795, 0.2548898027376427, 0.20047025767692062, 0.7371898967304475, 0.6380733620136718, 0.2869486851028813, 0.5995684081223496, 0.22021398795041913, 0.4505379972938547]}}
{"id": "fff9b752-846b-455c-8d3e-c4822e77667b", "fitness": 0.43913356705876383, "name": "AdaptiveMutationDE", "description": "Simplified Adaptive DE with a self-adaptive mutation factor based on fitness improvement and a reduced crossover strategy for efficiency.", "code": "import numpy as np\n\nclass AdaptiveMutationDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, CR=0.9, F_initial=0.5):\n        \"\"\"\n        Initialize the Adaptive Mutation DE.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            CR (float): Differential evolution crossover rate.\n            F_initial (float): Initial mutation factor.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.F = F_initial  # Initial mutation factor\n        self.F_history = [F_initial] * pop_size\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using a DE strategy with adaptive mutation.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            # Differential Evolution part\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n                \n                # Adaptive F: Adjust F based on past success (fitness improvement)\n                if i > 0 and fitness[i] < fitness[i-1]:\n                    self.F_history[i] = np.clip(self.F_history[i] * 1.1, 0.1, 1.0)  # Increase if successful\n                else:\n                    self.F_history[i] = np.clip(self.F_history[i] * 0.9, 0.1, 1.0)  # Decrease if unsuccessful\n                \n                self.F = self.F_history[i] # Use individual F\n                x_mutated = population[i] + self.F * (x_r2 - x_r3)\n\n                # Crossover - Simplified: binomial crossover\n                crossover_mask = np.random.rand(self.dim) < self.CR\n                x_trial = np.copy(population[i])\n                \n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if crossover_mask[j] or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Repair bounds\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    if f_trial < f_opt:\n                        f_opt = f_trial\n                        x_opt = x_trial\n                    fitness[i] = f_trial\n                    population[i] = x_trial\n\n        return f_opt, x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveMutationDE scored 0.439 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["992eb613-8ad8-4b8a-a59f-e2e418724fb1"], "operator": null, "metadata": {"aucs": [0.17409512338846433, 0.3402960478337248, 0.414739008531957, 0.4136791239072404, 0.3125426982809121, 0.4411651183794012, 0.3166282247559026, 0.32335115766018463, 0.317290880767632, 0.17938396346899022, 0.6781754331763101, 0.9960450028142411, 0.47227927981548956, 0.28677468863923505, 0.8346099728166392, 0.5349816867548502, 0.40150128926482076, 0.6044248484405073, 0.256278706066971, 0.4844290864118044]}}
{"id": "d4db667d-e3af-4fcc-be7b-4c76a16927e2", "fitness": 0.4953666873108725, "name": "AdaptiveMutationDE", "description": "Adaptive DE with simplified mutation and crossover, incorporating a learning rate for F and a memory of successful F values to guide adaptation.", "code": "import numpy as np\n\nclass AdaptiveMutationDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, CR=0.9, F_initial=0.5, F_learning_rate=0.1):\n        \"\"\"\n        Initialize the Adaptive Mutation DE.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            CR (float): Differential evolution crossover rate.\n            F_initial (float): Initial mutation factor.\n            F_learning_rate (float): Learning rate for updating F based on success.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.F = F_initial  # Initial mutation factor\n        self.F_learning_rate = F_learning_rate\n        self.F_memory = [F_initial] * 5  # Memory of successful F values\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using a DE strategy with adaptive mutation and simplified crossover.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            # Differential Evolution part\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n\n                # Adaptive F: Sample F from memory and add noise\n                F = np.random.choice(self.F_memory) * (1 + np.random.normal(0, 0.1))\n                F = np.clip(F, 0.1, 1.0)\n                \n                x_mutated = population[i] + F * (x_r2 - x_r3)\n\n                # Crossover: Simplified binomial crossover\n                x_trial = np.copy(population[i])\n                crossover_points = np.random.rand(self.dim) < self.CR\n                x_trial[crossover_points] = x_mutated[crossover_points]\n\n                # Repair bounds\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Selection:  Greedy selection\n                if f_trial < fitness[i]:\n                    # Update fitness and population\n                    fitness[i] = f_trial\n                    population[i] = x_trial\n\n                    # Update best solution\n                    if f_trial < f_opt:\n                        f_opt = f_trial\n                        x_opt = x_trial\n\n                    # Update F memory\n                    self.F_memory.pop(0)\n                    self.F_memory.append(F)\n\n        return f_opt, x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveMutationDE scored 0.495 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c77935d2-d91d-4c08-a4b7-a1d4dad41a96"], "operator": null, "metadata": {"aucs": [0.1789855079629319, 0.42938954755488545, 0.5264188890110166, 0.7396895426177181, 0.22737982333415807, 0.6227899944711587, 0.307362857788842, 0.39919436638794126, 0.3579078933510823, 0.19519038631937446, 0.9027799117057309, 0.9984366902255978, 0.562895337808557, 0.30748302579999776, 0.8606989749157132, 0.47485610787323984, 0.386649926784868, 0.7136781342691778, 0.22806152519765865, 0.4874853028378029]}}
{"id": "a7bf5417-b5f6-4db6-a42f-0ae8bdeb9bf7", "fitness": 0.29308034612670764, "name": "EnhancedAdaptiveDE", "description": "Enhanced Adaptive DE with population-based mutation factor adaptation, orthogonal crossover, and a Cauchy mutation operator for increased exploration.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, CR=0.9):\n        \"\"\"\n        Initialize the Enhanced Adaptive DE.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            CR (float): Differential evolution crossover rate.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n\n    def cauchy_mutation(self, x, scale=0.1):\n        \"\"\"Apply Cauchy mutation to a vector.\"\"\"\n        return x + scale * np.random.standard_cauchy(size=self.dim)\n\n    def orthogonal_crossover(self, x_target, x_donor):\n        \"\"\"Perform orthogonal crossover between two vectors.\"\"\"\n        basis = np.random.randn(self.dim, self.dim)\n        q, r = np.linalg.qr(basis)\n        \n        alpha = np.dot(x_target, q)\n        beta = np.dot(x_donor, q)\n\n        x_new = np.dot(alpha, q)\n\n        for j in range(self.dim):\n            if np.random.rand() < self.CR:\n                x_new[j] = np.dot(beta, q)[j]\n        return x_new\n    \n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using an enhanced DE strategy with population-based mutation adaptation, orthogonal crossover, and cauchy mutation.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n\n        # Mutation factor adaptation based on population fitness\n        mutation_factors = np.random.uniform(0.1, 1.0, size=self.pop_size)\n\n        while self.budget > 0:\n            # Differential Evolution part\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Mutation with adaptive F and mirrored sampling\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n\n                # Mutate the population\n                x_mutated = population[i] + mutation_factors[i] * (x_r2 - x_r3)\n                x_mutated = self.cauchy_mutation(x_mutated) # Cauchy Mutation to improve exploration\n                \n                # Crossover\n                x_trial = self.orthogonal_crossover(population[i], x_mutated)\n\n                # Repair bounds and mirrored sampling\n                for j in range(self.dim):\n                    if x_trial[j] < self.bounds_lb:\n                        x_trial[j] = self.bounds_lb + (self.bounds_lb - x_trial[j])\n                    elif x_trial[j] > self.bounds_ub:\n                        x_trial[j] = self.bounds_ub - (x_trial[j] - self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = x_trial\n                    if f_trial < f_opt:\n                        f_opt = f_trial\n                        x_opt = x_trial\n                        \n                        # Adapt mutation factor based on successful update\n                        mutation_factors[i] = np.random.uniform(0.1, 0.5)  # Reduce F for exploitation\n                else:\n                    # Increase F if update was unsuccessful\n                    mutation_factors[i] = np.random.uniform(0.5, 1.0)\n        return f_opt, x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm EnhancedAdaptiveDE scored 0.293 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["377467d7-0d4e-4be9-802f-8fe2b506d891"], "operator": null, "metadata": {"aucs": [0.12181553868947381, 0.18111010185617793, 0.3137955544621899, 0.24931886889533283, 0.20694557404534097, 0.22858698671338862, 0.2450529972655937, 0.23080295477619772, 0.201191873786705, 0.1613763023389423, 0.23081635686088608, 0.9967438479372673, 0.24813418342283433, 0.21342080309674016, 0.5937016648709482, 0.27768582179069656, 0.24223832365561193, 0.278633491651916, 0.1800466869207128, 0.46018898949719533]}}
{"id": "23852d02-70fb-40c5-b560-5da19fd6ec23", "fitness": 0.0, "name": "AdaptiveMutationDE", "description": "Adaptive DE with dynamic F and CR based on success rate and introducing a local search step for promising individuals.", "code": "import numpy as np\n\nclass AdaptiveMutationDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, CR_initial=0.9, F_initial=0.5, F_decay=0.99, p_selection=0.9, local_search_prob=0.1):\n        \"\"\"\n        Initialize the Adaptive Mutation DE.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            CR_initial (float): Initial Differential evolution crossover rate.\n            F_initial (float): Initial mutation factor.\n            F_decay (float): Decay rate for F.\n            p_selection (float): Probability of selecting better individual in stochastic ranking.\n            local_search_prob (float): Probability of performing local search on an individual.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR_initial\n        self.F = F_initial  # Initial mutation factor\n        self.F_decay = F_decay # Decay rate for F\n        self.p_selection = p_selection\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.local_search_prob = local_search_prob\n        self.success_count = 0\n        self.total_count = 0\n\n\n    def local_search(self, x, func, step_size=0.1):\n        \"\"\"\n        Perform a simple local search around a given solution.\n        \"\"\"\n        x_new = x.copy()\n        for i in range(self.dim):\n            # Explore each dimension\n            original_value = x_new[i]\n            # Try increasing the value\n            x_new[i] = min(self.bounds_ub, original_value + step_size)\n            f_plus = func(x_new)\n            self.budget -= 1\n            if f_plus < func(x):\n                x = x_new.copy()\n            else:\n                # Try decreasing the value\n                x_new[i] = max(self.bounds_lb, original_value - step_size)\n                f_minus = func(x_new)\n                self.budget -= 1\n                if f_minus < func(x):\n                    x = x_new.copy()\n                else:\n                    x_new[i] = original_value  # Revert to original value\n            if self.budget <=0:\n                break\n\n        return x\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using a DE strategy with adaptive mutation and mirrored sampling.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            # Differential Evolution part\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Mutation with mirrored sampling\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n\n                # Mirrored sampling: if the mutated vector goes out of bounds, reflect it back\n                x_mutated = population[i] + self.F * (x_r2 - x_r3)\n                \n                # Crossover\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Repair bounds and mirrored sampling\n                for j in range(self.dim):\n                    if x_trial[j] < self.bounds_lb:\n                        x_trial[j] = self.bounds_lb + (self.bounds_lb - x_trial[j])\n                    elif x_trial[j] > self.bounds_ub:\n                        x_trial[j] = self.bounds_ub - (x_trial[j] - self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Stochastic Ranking Selection\n                self.total_count += 1\n                if np.random.rand() < self.p_selection:\n                    if f_trial < fitness[i]:\n                        self.success_count += 1\n                        # Update F based on improvement\n                        if f_trial < f_opt:\n                            f_opt = f_trial\n                            x_opt = x_trial\n\n                        fitness[i] = f_trial\n                        population[i] = x_trial\n                        # Local search\n                        if np.random.rand() < self.local_search_prob:\n                            population[i] = self.local_search(population[i], func)\n                            fitness[i] = func(population[i])\n                            self.budget -=1\n\n                        # Adaptive F and CR\n                        if self.total_count > 10:\n                            success_rate = self.success_count / self.total_count\n                            self.F = self.F_decay * (1 - success_rate) + self.F * success_rate  # Adjust F dynamically\n                            self.CR = 0.1 + 0.8 * (1 - success_rate) # Adjust CR dynamically\n                            self.success_count = 0\n                            self.total_count = 0\n\n        return f_opt, x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveMutationDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["377467d7-0d4e-4be9-802f-8fe2b506d891"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "5c4aa9ec-7df0-409c-8e5e-6f53219cb6cb", "fitness": 0.4553622482334855, "name": "AdaptiveMutationDE", "description": "Improved Adaptive DE with per-dimension mutation factor adaptation and a more robust crossover strategy.", "code": "import numpy as np\n\nclass AdaptiveMutationDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, CR=0.9, F_initial=0.5, F_decay=0.99):\n        \"\"\"\n        Initialize the Adaptive Mutation DE.\n\n        Args:\n            budget (int): The total number of function evaluations allowed.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            CR (float): Differential evolution crossover rate.\n            F_initial (float): Initial mutation factor.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.F = np.full(dim, F_initial)  # Per-dimension mutation factors\n        self.F_initial = F_initial\n        self.F_decay = F_decay\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using a DE strategy with adaptive mutation and simplified crossover.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        f_opt = np.min(fitness)\n        x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            # Differential Evolution part\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n\n                # Adaptive F: Adjust F based on individual success.\n                F = self.F * (1 + np.random.normal(0, 0.1, size=self.dim))  # Self-adaptive F per dimension\n                F = np.clip(F, 0.1, 1.0)  # Keep F within bounds\n\n                x_mutated = population[i] + F * (x_r2 - x_r3)\n\n                # Crossover: Binomial crossover\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim) # Index to ensure at least one parameter is different\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Repair bounds\n                x_trial = np.clip(x_trial, self.bounds_lb, self.bounds_ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Selection:  Greedy selection\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = x_trial\n                    # If improvement, reduce F for this dimension to exploit better\n                    self.F[x_trial == x_mutated] *= self.F_decay\n\n                    if f_trial < f_opt:\n                        f_opt = f_trial\n                        x_opt = x_trial\n                else:\n                    # If no improvement, increase F for this dimension to explore more\n                    self.F[x_trial == population[i]] = np.clip(self.F[x_trial == population[i]]/ self.F_decay, 0.1, 1.0)\n\n\n\n        return f_opt, x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveMutationDE scored 0.455 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c77935d2-d91d-4c08-a4b7-a1d4dad41a96"], "operator": null, "metadata": {"aucs": [0.19255634925376774, 0.3495258112143077, 0.47764124230171723, 0.4293594219213057, 0.3617869931877423, 0.5767449437659018, 0.30888383198688685, 0.36680421820475007, 0.3354339858331724, 0.3546761812841741, 0.6850330442954025, 0.9974971592035201, 0.2818530224906448, 0.3618722366497328, 0.8322543573911924, 0.4860182360658084, 0.36045564149836085, 0.6089760768140282, 0.23560381394375762, 0.5042683973635349]}}
