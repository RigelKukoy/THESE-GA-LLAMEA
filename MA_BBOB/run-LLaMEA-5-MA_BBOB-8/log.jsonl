{"id": "2abff733-e788-4b90-a932-f363213a279c", "fitness": 0.0, "name": "AdaptiveDE", "description": "An adaptive differential evolution strategy with dynamically adjusted parameters and population members based on individual success rates.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR # Crossover rate\n        self.population = None\n        self.fitness = None\n        self.archive = []\n        self.archive_fitness = []\n        self.success_rates = np.zeros(pop_size) # Track individual success rates\n\n    def initialize_population(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size # reduce budget by initial evaluation\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n\n    def mutate(self, pop, i):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n        return np.clip(a + self.F * (b - c), -5, 5)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                target = self.population[i]\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, target)\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.success_rates[i] = min(1, self.success_rates[i] + 0.1)  # Increase success rate\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                else:\n                    self.success_rates[i] = max(0, self.success_rates[i] - 0.05)  # Decrease success rate\n\n                if self.budget <= 0:\n                    break\n\n            # Adaptive parameter adjustment\n            self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n            self.CR = np.clip(np.random.normal(0.7, 0.1), 0.1, 1.0)\n\n            # Adaptive population adjustment (simplified - keep top half and replace bottom half randomly)\n            if self.pop_size > 10:  # avoid shrinking to tiny populations\n                sorted_indices = np.argsort(self.fitness)\n                top_half_indices = sorted_indices[:self.pop_size // 2]\n                bottom_half_indices = sorted_indices[self.pop_size // 2:]\n\n                # Replace the bottom half with new random individuals.\n                lb = func.bounds.lb\n                ub = func.bounds.ub\n                self.population[bottom_half_indices] = np.random.uniform(lb, ub, size=(self.pop_size // 2, self.dim))\n                self.fitness[bottom_half_indices] = np.array([func(x) for x in self.population[bottom_half_indices]])\n                self.budget -= self.pop_size // 2  # reduce budget\n\n                # Update best fitness and solution if needed\n                if np.min(self.fitness) < self.f_opt:\n                    self.f_opt = np.min(self.fitness)\n                    self.x_opt = self.population[np.argmin(self.fitness)]\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0]}}
{"id": "4b5d5ca1-f509-40e7-a9ea-2c785bf4234b", "fitness": -Infinity, "name": "CMAES_with_Archive", "description": "Covariance matrix adaptation evolution strategy with adaptive population size and archive for exploration.", "code": "import numpy as np\n\nclass CMAES_with_Archive:\n    def __init__(self, budget=10000, dim=10, mu_factor=1/4, cs=0.3, c_cov=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.mu_factor = mu_factor\n        self.cs = cs\n        self.c_cov = c_cov\n        self.archive = []\n        self.archive_size = 100  # Adjust based on problem difficulty/dimensionality\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize parameters\n        lambda_ = int(4 + np.floor(3 * np.log(self.dim))) # Population size (adaptive)\n        mu = int(lambda_ * self.mu_factor)  # Number of parents\n        weights = np.log(mu + 1/2) - np.log(np.arange(1, mu + 1))\n        weights = weights / np.sum(weights)\n        m = np.random.uniform(lb, ub, size=self.dim)  # Mean\n        sigma = 0.1 * (ub - lb)  # Step size\n        C = np.eye(self.dim)  # Covariance matrix\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1/(4*self.dim)) + 1/(21*self.dim**2)) # Expectation of ||N(0,I)||\n\n        evals = 0\n        while evals < self.budget:\n            # Generate and evaluate offspring\n            z = np.random.randn(self.dim, lambda_)\n            y = m[:, np.newaxis] + sigma * np.dot(np.linalg.cholesky(C), z)\n            \n            # Clip to bounds\n            y = np.clip(y, lb, ub)\n            \n            f = np.array([func(y[:, i]) for i in range(lambda_)])\n            evals += lambda_\n            \n            # Sort by fitness\n            idx = np.argsort(f)\n            f = f[idx]\n            y = y[:, idx]\n            \n            # Update optimal solution\n            if f[0] < self.f_opt:\n                self.f_opt = f[0]\n                self.x_opt = y[:, 0]\n            \n            # Update mean\n            m_old = m.copy()\n            m = np.sum(weights[:, np.newaxis] * y[:, :mu], axis=1)\n\n            # Update evolution path for step size\n            ps = (1 - self.cs) * ps + np.sqrt(self.cs * (2 - self.cs) * weights @ weights) * (np.linalg.inv(np.linalg.cholesky(C)) @ ((m - m_old) / sigma))\n            \n            # Update evolution path for covariance matrix\n            pc = (1 - self.c_cov) * pc + np.sqrt(self.c_cov * (2 - self.c_cov) * weights @ weights) * ((m - m_old) / sigma)\n\n            # Heuristic: adjust step size\n            if np.linalg.norm(ps) / np.sqrt(1 - (1 - self.cs)**(evals/lambda_)) > chiN * 1.4:\n                sigma = sigma * np.exp(0.2 + self.cs / 0.2)\n            elif np.linalg.norm(ps) / np.sqrt(1 - (1 - self.cs)**(evals/lambda_)) < chiN / 1.4:\n                sigma = sigma * np.exp(-0.2 + self.cs / 0.2)\n            \n            # Update covariance matrix\n            C = (1 - self.c_cov) * C + self.c_cov * ((pc[:, np.newaxis] @ pc[np.newaxis, :]) + (weights * ((y[:, :mu] - m_old[:, np.newaxis]) @ (y[:, :mu] - m_old[:, np.newaxis]).T) / sigma**2))\n\n            # Ensure positive definiteness\n            C = np.triu(C)\n            C = C + C.T - np.diag(C.diagonal())\n            try:\n                np.linalg.cholesky(C) # Check if positive definite\n            except np.linalg.LinAlgError:\n                C = np.eye(self.dim) # Reset covariance matrix if needed\n            \n            # Adaptive lambda\n            if evals < self.budget / 2: # Reduce pop size later in optimization, makes algorithm faster\n                lambda_ = int(4 + np.floor(3 * np.log(self.dim)))\n            else:\n                 lambda_ = int(2 + np.floor(2 * np.log(self.dim)))\n                 mu = int(lambda_ * self.mu_factor)\n                 weights = np.log(mu + 1/2) - np.log(np.arange(1, mu + 1))\n                 weights = weights / np.sum(weights)\n\n            # Archive best solutions to improve exploration if needed\n            for i in range(min(mu, 5)):\n              self.archive.append((f[i], y[:,i]))\n              if len(self.archive) > self.archive_size:\n                  self.archive.pop(0) # Keep archive bounded\n              \n            # Heuristic: Random restart from archive (exploration)\n            if np.random.rand() < 0.01:  # Low probability of restart\n                if self.archive:\n                    best_archive_f, best_archive_x = min(self.archive, key=lambda item: item[0])\n                    m = best_archive_x.copy() # Restart from best archived point\n                    sigma = 0.1 * (ub - lb) # Reset step size\n                    C = np.eye(self.dim)   # Reset covariance\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "An exception occurred: operands could not be broadcast together with shapes (2,) (2,6) .", "error": "", "parent_ids": [], "operator": null, "metadata": {}}
{"id": "6157c93f-6353-435a-827a-c3194cf64c12", "fitness": 0.08758989254057578, "name": "CMAES_O", "description": "Covariance Matrix Adaptation Evolution Strategy with Orthogonal sampling, where the search distribution is adapted based on the success of previous steps, using orthogonal sampling to generate candidate solutions.", "code": "import numpy as np\n\nclass CMAES_O:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, dsigma=0.2, c_cov=2/15, c_mu=2/3, c_1=2/15, mu_factor=4):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // mu_factor\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.m = None\n        self.sigma = None\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.chiN = None\n        self.cs = cs\n        self.dsigma = dsigma\n        self.c_cov = c_cov\n        self.c_mu = c_mu\n        self.c_1 = c_1\n        self.count = 0\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.sigma = 0.5\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1/(4*self.dim) + 1/(21*self.dim**2))\n\n        while self.count < self.budget:\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            R = np.linalg.qr(z)[0]\n\n            x = np.array([self.m + self.sigma * np.dot(self.C**0.5, R[i, :]) for i in range(self.popsize)])\n            \n            # Clip x to boundaries\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            \n            f = np.array([func(xi) for xi in x])\n            self.count += self.popsize\n\n            if np.min(f) < self.f_opt:\n                    self.f_opt = np.min(f)\n                    self.x_opt = x[np.argmin(f)]\n            \n            idx = np.argsort(f)\n            x_mu = x[idx[:self.mu]]\n            z_mu = R[idx[:self.mu]]\n\n            m_old = self.m.copy()\n            self.m = np.sum(self.weights[:, None] * x_mu, axis=0)\n\n            zmean = np.sum(self.weights[:, None] * z_mu, axis=0)\n            self.ps = (1 - self.cs) * self.ps + (self.cs * (2 - self.cs))**0.5 * zmean\n            self.pc = (1 - self.c_cov) * self.pc + (self.c_cov * (2 - self.c_cov))**0.5 * (self.m - m_old) / self.sigma\n\n            hsig = np.linalg.norm(self.ps) / (1 - (1 - self.cs)**(self.count/self.popsize * self.budget)) / self.chiN < 1.4 + 2/(self.dim+1)\n            dhsig = (1-hsig) * self.c_1 * (2-self.c_1)\n\n            self.C = (1- self.c_cov - self.c_1 * dhsig) * self.C \\\n                     + self.c_1 * np.outer(self.pc, self.pc) \\\n                     + self.c_mu * np.sum(self.weights[:, None, None] * (x_mu - m_old)[..., None] * (x_mu - m_old)[:, None, :], axis=0) / self.sigma**2\n\n            self.sigma *= np.exp((self.cs/self.dsigma) * (np.linalg.norm(self.ps)/self.chiN - 1))\n            \n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm CMAES_O scored 0.088 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.017995532253159086, 0.09310161427193564, 0.20256716866730806, 0.10013896139680656, 0.05418215778468971, 0.1558475092181678, 0.044031881531681805, 0.04765940474731689, 0.03842934428265721, 0.10241410876180257, 0.1065511178553672, 0.10224519448597491, 0.0, 0.05914269090805668, 0.0847867568187941, 0.10067175202809975, 0.043753463036895535, 0.13865181573666774, 0.08098354045947165, 0.17864383656666272]}}
{"id": "63c25a13-6cbe-461f-82fe-efd158076997", "fitness": 0.7692230969970691, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with ensemble of mutation strategies and archive.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=100, F=0.5, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F  # Default scaling factor\n        self.CR = CR  # Default crossover rate\n        self.archive = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        if np.min(self.fitness) < self.f_opt:\n            self.f_opt = np.min(self.fitness)\n            self.x_opt = self.population[np.argmin(self.fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation strategies\n                if np.random.rand() < 0.3:  # \"DE/rand/1\"\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    x_mutated = self.population[i] + self.F * (x_r1 - x_r2) + self.F * (x_r3-self.population[i])\n                elif np.random.rand() < 0.6:  # \"DE/best/1\"\n                    x_best = self.population[np.argmin(self.fitness)]\n                    indices = np.random.choice(self.pop_size, 2, replace=False)\n                    x_r1, x_r2 = self.population[indices]\n                    x_mutated = x_best + self.F * (x_r1 - x_r2)\n                else: #DE/current-to-rand/1\n                    indices = np.random.choice(self.pop_size, 2, replace=False)\n                    x_r1, x_r2 = self.population[indices]\n                    x_mutated = self.population[i] + np.random.rand()*(x_r1 - self.population[i]) + self.F * (x_r2 - self.population[i])\n\n                # Crossover\n                x_trial = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(x_trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = x_trial\n                    \n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(x_trial)\n                    else:\n                        # Replace a random element in the archive\n                        idx = np.random.randint(self.archive_size)\n                        self.archive[idx] = x_trial\n                        \n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n\n                elif len(self.archive) < self.archive_size:\n                    self.archive.append(self.population[i])\n                else:\n                    idx = np.random.randint(self.archive_size)\n                    self.archive[idx] = self.population[i]\n\n                if self.budget <= 0:\n                    break\n                    \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDE scored 0.769 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.25868740517835, 0.8806979128001887, 0.8918485642435209, 0.9568083681570151, 0.8957113729748013, 0.9252000103992943, 0.3362418330544683, 0.8799151310256981, 0.9054898516395118, 0.24867985162749384, 0.9475182235050951, 0.9982375750684599, 0.8715111130498769, 0.910577563881524, 0.9582171956092693, 0.9270591883718351, 0.8547042069359616, 0.9444850830703352, 0.25265425803740704, 0.5402172313112796]}}
{"id": "d09fd68d-8f40-486d-abef-c183ddde93f5", "fitness": -Infinity, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with simplified mutation and crossover, focusing on exploration by mixing current and best solutions, and an archive to retain promising candidates.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=50, F=0.5, CR=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.CR = CR\n        self.archive = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation: current-to-best/1 with archive\n                best_index = np.argmin(self.fitness)\n                x_best = self.population[best_index]\n\n                if len(self.archive) > 0 and np.random.rand() < 0.2:\n                    x_r1 = self.archive[np.random.randint(len(self.archive))]\n                else:\n                    indices = np.random.choice(self.pop_size, 2, replace=False)\n                    x_r1 = self.population[indices[0]]\n                \n                x_mutated = self.population[i] + self.F * (x_best - self.population[i]) + self.F * (x_r1 - self.population[i])\n\n                # Crossover\n                x_trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR:\n                        x_trial[j] = x_mutated[j]\n\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = x_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(x_trial)\n                    else:\n                        idx = np.random.randint(self.archive_size)\n                        self.archive[idx] = x_trial\n\n                elif len(self.archive) < self.archive_size and np.random.rand() < 0.1:\n                    self.archive.append(self.population[i])\n                elif len(self.archive) > 0 and np.random.rand() < 0.05:\n                    idx = np.random.randint(self.archive_size)\n                    self.archive[idx] = self.population[i]\n\n\n                if self.budget <= 0:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "An exception occurred: list assignment index out of range.", "error": "", "parent_ids": ["63c25a13-6cbe-461f-82fe-efd158076997"], "operator": null, "metadata": {}}
{"id": "93c465b9-ca58-4691-8489-6d681332456a", "fitness": -Infinity, "name": "CMAES_with_Archive", "description": "Covariance matrix adaptation evolution strategy with archive and simplified parameter updates for improved robustness.", "code": "import numpy as np\n\nclass CMAES_with_Archive:\n    def __init__(self, budget=10000, dim=10, mu_factor=1/4, cs=0.3, c_cov=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.mu_factor = mu_factor\n        self.cs = cs\n        self.c_cov = c_cov\n        self.archive = []\n        self.archive_size = 50\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        lambda_ = int(4 + np.floor(3 * np.log(self.dim)))\n        mu = int(lambda_ * self.mu_factor)\n        weights = np.log(mu + 1/2) - np.log(np.arange(1, mu + 1))\n        weights /= np.sum(weights)\n        m = np.random.uniform(lb, ub, size=self.dim)\n        sigma = 0.1 * (ub - lb)\n        C = np.eye(self.dim)\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1/(4*self.dim)) + 1/(21*self.dim**2))\n\n        evals = 0\n        while evals < self.budget:\n            z = np.random.randn(self.dim, lambda_)\n            y = m[:, np.newaxis] + sigma * np.dot(np.linalg.cholesky(C), z)\n            y = np.clip(y, lb, ub)\n            \n            f = np.array([func(y[:, i]) for i in range(lambda_)]).flatten()\n            evals += lambda_\n            \n            idx = np.argsort(f)\n            f = f[idx]\n            y = y[:, idx]\n            \n            if f[0] < self.f_opt:\n                self.f_opt = f[0]\n                self.x_opt = y[:, 0]\n            \n            m_old = m.copy()\n            m = np.sum(weights[:, np.newaxis] * y[:, :mu], axis=1)\n\n            ps = (1 - self.cs) * ps + np.sqrt(self.cs * (2 - self.cs) * np.sum(weights**2)) * (np.linalg.inv(np.linalg.cholesky(C)) @ ((m - m_old) / sigma))\n            pc = (1 - self.c_cov) * pc + np.sqrt(self.c_cov * (2 - self.c_cov) * np.sum(weights**2)) * ((m - m_old) / sigma)\n\n            C = (1 - self.c_cov) * C + self.c_cov * (pc[:, np.newaxis] @ pc[np.newaxis, :]) + self.c_cov * np.sum(weights[:, np.newaxis, np.newaxis] * ((y[:, :mu] - m_old[:, np.newaxis])[:, :, np.newaxis] @ (y[:, :mu] - m_old[:, np.newaxis])[:, np.newaxis, :]), axis=0) / sigma**2\n            \n            C = np.triu(C)\n            C = C + C.T - np.diag(C.diagonal())\n            try:\n                np.linalg.cholesky(C)\n            except np.linalg.LinAlgError:\n                C = np.eye(self.dim)\n            \n            sigma *= np.exp(self.cs/5 * (np.linalg.norm(ps)/chiN -1))\n\n            for i in range(min(mu, 5)):\n                self.archive.append((f[i], y[:,i]))\n                if len(self.archive) > self.archive_size:\n                    self.archive.pop(0)\n              \n            if np.random.rand() < 0.01 and self.archive:\n                best_archive_f, best_archive_x = min(self.archive, key=lambda item: item[0])\n                m = best_archive_x.copy()\n                sigma = 0.1 * (ub - lb)\n                C = np.eye(self.dim)\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "An exception occurred: operands could not be broadcast together with shapes (2,) (2,6) .", "error": "", "parent_ids": ["4b5d5ca1-f509-40e7-a9ea-2c785bf4234b"], "operator": null, "metadata": {}}
{"id": "e2c6aa38-a1cc-4d36-9c08-57f1243966f8", "fitness": -Infinity, "name": "CMAES_Simple", "description": "Simplified CMA-ES with adaptive population size and step size adaptation, focusing on core CMA-ES updates and removing the archive for simplicity and robustness.", "code": "import numpy as np\n\nclass CMAES_Simple:\n    def __init__(self, budget=10000, dim=10, mu_factor=1/4, cs=0.3, c_cov=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.mu_factor = mu_factor\n        self.cs = cs\n        self.c_cov = c_cov\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize parameters\n        lambda_ = int(4 + np.floor(3 * np.log(self.dim))) # Population size (adaptive)\n        mu = int(lambda_ * self.mu_factor)  # Number of parents\n        weights = np.log(mu + 1/2) - np.log(np.arange(1, mu + 1))\n        weights = weights / np.sum(weights)\n        m = np.random.uniform(lb, ub, size=self.dim)  # Mean\n        sigma = 0.1 * (ub - lb)  # Step size\n        C = np.eye(self.dim)  # Covariance matrix\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1/(4*self.dim)) + 1/(21*self.dim**2)) # Expectation of ||N(0,I)||\n\n        evals = 0\n        while evals < self.budget:\n            # Generate and evaluate offspring\n            z = np.random.randn(self.dim, lambda_)\n            y = m[:, np.newaxis] + sigma * np.dot(np.linalg.cholesky(C), z)\n            \n            # Clip to bounds\n            y = np.clip(y, lb, ub)\n            \n            f = np.array([func(y[:, i]) for i in range(lambda_)])\n            evals += lambda_\n            \n            # Sort by fitness\n            idx = np.argsort(f)\n            f = f[idx]\n            y = y[:, idx]\n            \n            # Update optimal solution\n            if f[0] < self.f_opt:\n                self.f_opt = f[0]\n                self.x_opt = y[:, 0]\n            \n            # Update mean\n            m_old = m.copy()\n            m = np.sum(weights[:, np.newaxis] * y[:, :mu], axis=1)\n\n            # Update evolution path for step size\n            ps = (1 - self.cs) * ps + np.sqrt(self.cs * (2 - self.cs) * np.sum(weights**2)) * ((m - m_old) / sigma)\n            \n            # Update evolution path for covariance matrix\n            pc = (1 - self.c_cov) * pc + np.sqrt(self.c_cov * (2 - self.c_cov) * np.sum(weights**2)) * ((m - m_old) / sigma)\n\n            # Heuristic: adjust step size\n            if np.linalg.norm(ps) / np.sqrt(1 - (1 - self.cs)**(evals/lambda_)) > chiN * 1.4:\n                sigma = sigma * np.exp(0.2 + self.cs / 0.2)\n            elif np.linalg.norm(ps) / np.sqrt(1 - (1 - self.cs)**(evals/lambda_)) < chiN / 1.4:\n                sigma = sigma * np.exp(-0.2 + self.cs / 0.2)\n            \n            # Update covariance matrix\n            C = (1 - self.c_cov) * C + self.c_cov * (np.outer(pc, pc) + np.sum(weights[:, np.newaxis, np.newaxis] * ((y[:, :mu] - m_old[:, np.newaxis])[:, :, np.newaxis] @ (y[:, :mu] - m_old[:, np.newaxis])[:, np.newaxis, :]), axis=0) / sigma**2)\n\n            # Ensure positive definiteness\n            C = np.triu(C)\n            C = C + C.T - np.diag(C.diagonal())\n            try:\n                np.linalg.cholesky(C) # Check if positive definite\n            except np.linalg.LinAlgError:\n                C = np.eye(self.dim) # Reset covariance matrix if needed\n            \n            # Adaptive lambda\n            if evals < self.budget / 2: # Reduce pop size later in optimization, makes algorithm faster\n                lambda_ = int(4 + np.floor(3 * np.log(self.dim)))\n            else:\n                 lambda_ = int(2 + np.floor(2 * np.log(self.dim)))\n            mu = int(lambda_ * self.mu_factor)\n            weights = np.log(mu + 1/2) - np.log(np.arange(1, mu + 1))\n            weights = weights / np.sum(weights)\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "An exception occurred: operands could not be broadcast together with shapes (2,) (2,6) .", "error": "", "parent_ids": ["4b5d5ca1-f509-40e7-a9ea-2c785bf4234b"], "operator": null, "metadata": {}}
{"id": "9f1a335e-477f-40ef-b717-0decbfc1864d", "fitness": -Infinity, "name": "CMAES_with_Archive", "description": "Simplified CMA-ES with adaptive population size, archive for exploration, and robust handling of covariance matrix issues.", "code": "import numpy as np\n\nclass CMAES_with_Archive:\n    def __init__(self, budget=10000, dim=10, mu_factor=1/4, cs=0.3, c_cov=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.mu_factor = mu_factor\n        self.cs = cs\n        self.c_cov = c_cov\n        self.archive = []\n        self.archive_size = 50 # Reduced archive size\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize parameters\n        lambda_ = int(4 + np.floor(3 * np.log(self.dim)))\n        mu = int(lambda_ * self.mu_factor)\n        weights = np.log(mu + 1/2) - np.log(np.arange(1, mu + 1))\n        weights = weights / np.sum(weights)\n        m = np.random.uniform(lb, ub, size=self.dim)\n        sigma = 0.1 * (ub - lb)\n        C = np.eye(self.dim)\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1/(4*self.dim)) + 1/(21*self.dim**2))\n\n        evals = 0\n        while evals < self.budget:\n            # Generate and evaluate offspring\n            z = np.random.randn(self.dim, lambda_)\n            y = m[:, np.newaxis] + sigma * np.dot(np.linalg.cholesky(C), z)\n            \n            # Clip to bounds\n            y = np.clip(y, lb, ub)\n            \n            f = np.array([func(y[:, i]) for i in range(lambda_))]\n            evals += lambda_\n            \n            # Sort by fitness\n            idx = np.argsort(f)\n            f = f[idx]\n            y = y[:, idx]\n            \n            # Update optimal solution\n            if f[0] < self.f_opt:\n                self.f_opt = f[0]\n                self.x_opt = y[:, 0]\n            \n            # Update mean\n            m_old = m.copy()\n            m = np.sum(weights[:, np.newaxis] * y[:, :mu], axis=1)\n\n            # Update evolution path for step size\n            ps = (1 - self.cs) * ps + np.sqrt(self.cs * (2 - self.cs) * np.sum(weights**2)) * (np.linalg.solve(np.linalg.cholesky(C), (m - m_old) / sigma)) #Changed dot to solve\n\n            # Update evolution path for covariance matrix\n            pc = (1 - self.c_cov) * pc + np.sqrt(self.c_cov * (2 - self.c_cov) * np.sum(weights**2)) * ((m - m_old) / sigma)\n\n            # Heuristic: adjust step size\n            if np.linalg.norm(ps) / np.sqrt(1 - (1 - self.cs)**(evals/lambda_)) > chiN * 1.4:\n                sigma = sigma * np.exp(0.2 + self.cs / 0.2)\n            elif np.linalg.norm(ps) / np.sqrt(1 - (1 - self.cs)**(evals/lambda_)) < chiN / 1.4:\n                sigma = sigma * np.exp(-0.2 + self.cs / 0.2)\n            \n            # Update covariance matrix\n            C = (1 - self.c_cov) * C + self.c_cov * (pc[:, np.newaxis] @ pc[np.newaxis, :])\n            for i in range(mu):\n                C += self.c_cov * weights[i] * ((y[:, i] - m_old)[:, np.newaxis] @ (y[:, i] - m_old)[np.newaxis, :] / sigma**2)\n\n            # Ensure positive definiteness and symmetry\n            C = np.triu(C)\n            C = C + C.T - np.diag(C.diagonal())\n            try:\n                np.linalg.cholesky(C) # Check if positive definite\n            except np.linalg.LinAlgError:\n                C = np.eye(self.dim) # Reset covariance matrix if needed\n            \n            # Adaptive lambda (simplified)\n            lambda_ = int(4 + np.floor(3 * np.log(self.dim)))\n            mu = int(lambda_ * self.mu_factor)\n            weights = np.log(mu + 1/2) - np.log(np.arange(1, mu + 1))\n            weights = weights / np.sum(weights)\n\n            # Archive best solutions\n            for i in range(min(mu, 5)):\n                self.archive.append((f[i], y[:,i]))\n                if len(self.archive) > self.archive_size:\n                    self.archive.pop(0)\n              \n            # Heuristic: Random restart from archive (exploration)\n            if np.random.rand() < 0.01 and self.archive:\n                best_archive_f, best_archive_x = min(self.archive, key=lambda item: item[0])\n                m = best_archive_x.copy()\n                sigma = 0.1 * (ub - lb)\n                C = np.eye(self.dim)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "An exception occurred: closing parenthesis ')' does not match opening parenthesis '[' (<string>, line 40).", "error": "", "parent_ids": ["4b5d5ca1-f509-40e7-a9ea-2c785bf4234b"], "operator": null, "metadata": {}}
{"id": "3d0da32d-622a-41b3-b7fa-07e754641188", "fitness": 0.0, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with success-history based parameter adaptation and a local search operator.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, local_search_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR # Crossover rate\n        self.population = None\n        self.fitness = None\n        self.archive = []\n        self.archive_fitness = []\n        self.success_rates = np.zeros(pop_size) # Track individual success rates\n        self.memory_F = [0.5] * 10  # Memory for F values\n        self.memory_CR = [0.7] * 10 # Memory for CR values\n        self.memory_idx = 0\n        self.local_search_prob = local_search_prob\n\n\n    def initialize_population(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size # reduce budget by initial evaluation\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n\n    def mutate(self, pop, i, F):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n        return np.clip(a + F * (b - c), -5, 5)\n\n    def crossover(self, mutant, target, CR):\n        cross_points = np.random.rand(self.dim) < CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n    \n    def local_search(self, x, func, step_size=0.1):\n        # Simple local search around x\n        x_new = x + np.random.uniform(-step_size, step_size, size=self.dim)\n        x_new = np.clip(x_new, -5, 5)\n        f_new = func(x_new)\n        self.budget -= 1\n        if f_new < func(x):\n            return x_new, f_new\n        else:\n            return x, func(x)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                target = self.population[i]\n                \n                # Sample F and CR from memory\n                F = np.random.choice(self.memory_F)\n                CR = np.random.choice(self.memory_CR)\n\n                mutant = self.mutate(self.population, i, F)\n                trial = self.crossover(mutant, target, CR)\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.success_rates[i] = min(1, self.success_rates[i] + 0.1)  # Increase success rate\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    \n                    # Update memory with successful F and CR\n                    self.memory_F[self.memory_idx] = F\n                    self.memory_CR[self.memory_idx] = CR\n                    self.memory_idx = (self.memory_idx + 1) % len(self.memory_F)\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                else:\n                    self.success_rates[i] = max(0, self.success_rates[i] - 0.05)  # Decrease success rate\n                \n                # Perform local search with some probability\n                if np.random.rand() < self.local_search_prob:\n                    self.population[i], self.fitness[i] = self.local_search(self.population[i], func)\n                    if self.fitness[i] < self.f_opt:\n                        self.f_opt = self.fitness[i]\n                        self.x_opt = self.population[i]\n\n                if self.budget <= 0:\n                    break\n\n            # Adaptive population adjustment (simplified - keep top half and replace bottom half randomly)\n            if self.pop_size > 10:  # avoid shrinking to tiny populations\n                sorted_indices = np.argsort(self.fitness)\n                top_half_indices = sorted_indices[:self.pop_size // 2]\n                bottom_half_indices = sorted_indices[self.pop_size // 2:]\n\n                # Replace the bottom half with new random individuals.\n                lb = func.bounds.lb\n                ub = func.bounds.ub\n                self.population[bottom_half_indices] = np.random.uniform(lb, ub, size=(self.pop_size // 2, self.dim))\n                self.fitness[bottom_half_indices] = np.array([func(x) for x in self.population[bottom_half_indices]])\n                self.budget -= self.pop_size // 2  # reduce budget\n\n                # Update best fitness and solution if needed\n                if np.min(self.fitness) < self.f_opt:\n                    self.f_opt = np.min(self.fitness)\n                    self.x_opt = self.population[np.argmin(self.fitness)]\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2abff733-e788-4b90-a932-f363213a279c"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "f75155ca-c680-4500-ae05-346106042a62", "fitness": 0.0, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with self-adaptive parameters and population size based on success history, using a weighted recombination strategy.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR # Crossover rate\n        self.population = None\n        self.fitness = None\n        self.archive = []\n        self.archive_fitness = []\n        self.archive_size = archive_size\n        self.success_rates = np.zeros(pop_size) # Track individual success rates\n        self.memory_F = np.ones(archive_size) * 0.5\n        self.memory_CR = np.ones(archive_size) * 0.7\n        self.memory_idx = 0\n        self.sf = 0.5\n        self.scr = 0.7\n        self.epsilon = 1e-6\n\n    def initialize_population(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size # reduce budget by initial evaluation\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n\n    def mutate(self, pop, i):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n        return np.clip(a + self.F * (b - c), -5, 5)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        success_f = []\n        success_cr = []\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Parameter adaptation\n                self.F = self.memory_F[np.random.randint(self.archive_size)]\n                self.CR = self.memory_CR[np.random.randint(self.archive_size)]\n                self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n                self.CR = np.clip(np.random.normal(self.CR, 0.1), 0.1, 1.0)\n\n\n                target = self.population[i]\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, target)\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    # Success!\n                    success_f.append(self.F)\n                    success_cr.append(self.CR)\n\n                    delta_fitness = np.abs(self.fitness[i] - f_trial)\n\n                    self.success_rates[i] = min(1, self.success_rates[i] + 0.1)  # Increase success rate\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                else:\n                    self.success_rates[i] = max(0, self.success_rates[i] - 0.05)  # Decrease success rate\n\n                if self.budget <= 0:\n                    break\n            \n            # Update memory of F and CR\n            if success_f:\n                self.memory_F[self.memory_idx] = np.mean(success_f) if len(success_f) > 0 else 0.5\n                self.memory_CR[self.memory_idx] = np.mean(success_cr) if len(success_cr) > 0 else 0.7\n                self.memory_idx = (self.memory_idx + 1) % self.archive_size\n            success_f = []\n            success_cr = []\n\n            # Adaptive population adjustment based on success rates (more aggressive)\n            if self.pop_size > 10:\n                sorted_indices = np.argsort(self.success_rates) # sort based on success rates\n                bottom_indices = sorted_indices[:self.pop_size // 4]  # Remove the bottom 25% worst individuals\n                lb = func.bounds.lb\n                ub = func.bounds.ub\n\n                self.population[bottom_indices] = np.random.uniform(lb, ub, size=(self.pop_size // 4, self.dim))\n                self.fitness[bottom_indices] = np.array([func(x) for x in self.population[bottom_indices]])\n                self.success_rates[bottom_indices] = 0  # Reset success rate for new individuals\n                self.budget -= self.pop_size // 4\n\n                if np.min(self.fitness) < self.f_opt:\n                    self.f_opt = np.min(self.fitness)\n                    self.x_opt = self.population[np.argmin(self.fitness)]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2abff733-e788-4b90-a932-f363213a279c"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "d1fc937e-7e71-421d-a50f-38a7f94c431c", "fitness": 0.10104576459950605, "name": "CMAES_O", "description": "Improved CMA-ES with orthogonal sampling and adaptive covariance matrix updates based on eigenvalue decomposition for better exploration and exploitation.", "code": "import numpy as np\n\nclass CMAES_O:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, dsigma=0.2, c_cov=2/15, c_mu=2/3, c_1=2/15, mu_factor=4, initial_sigma=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // mu_factor\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.m = None\n        self.sigma = initial_sigma\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.chiN = None\n        self.cs = cs\n        self.dsigma = dsigma\n        self.c_cov = c_cov\n        self.c_mu = c_mu\n        self.c_1 = c_1\n        self.count = 0\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.D = None #Eigenvalues\n        self.B = None #Eigenvectors\n\n    def __call__(self, func):\n        self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1/(4*self.dim) + 1/(21*self.dim**2))\n        self.D = np.ones(self.dim)\n        self.B = np.eye(self.dim)\n\n        while self.count < self.budget:\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            R = np.linalg.qr(z)[0]\n\n            x = np.array([self.m + self.sigma * np.dot(self.B * self.D**0.5, R[i, :]) for i in range(self.popsize)])\n            \n            # Clip x to boundaries\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            \n            f = np.array([func(xi) for xi in x])\n            self.count += self.popsize\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n            \n            idx = np.argsort(f)\n            x_mu = x[idx[:self.mu]]\n            z_mu = R[idx[:self.mu]]\n\n            m_old = self.m.copy()\n            self.m = np.sum(self.weights[:, None] * x_mu, axis=0)\n\n            zmean = np.sum(self.weights[:, None] * z_mu, axis=0)\n            self.ps = (1 - self.cs) * self.ps + (self.cs * (2 - self.cs))**0.5 * np.dot(self.B, zmean)\n            self.pc = (1 - self.c_cov) * self.pc + (self.c_cov * (2 - self.c_cov))**0.5 * (self.m - m_old) / self.sigma\n\n            hsig = np.linalg.norm(self.ps) / (1 - (1 - self.cs)**(self.count/self.popsize)) / self.chiN < 1.4 + 2/(self.dim+1)\n            dhsig = (1-hsig) * self.c_1 * (2-self.c_1)\n\n            self.C = (1- self.c_cov - self.c_1 * dhsig) * self.C \\\n                     + self.c_1 * np.outer(self.pc, self.pc) \\\n                     + self.c_mu * np.sum(self.weights[:, None, None] * (x_mu - m_old)[..., None] * (x_mu - m_old)[:, None, :], axis=0) / self.sigma**2\n\n            self.sigma *= np.exp((self.cs/self.dsigma) * (np.linalg.norm(self.ps)/self.chiN - 1))\n            \n            # Eigenvalue decomposition\n            try:\n                self.D, self.B = np.linalg.eigh(self.C)\n                self.D = np.sqrt(np.maximum(self.D, 1e-16)) # Avoid negative/zero eigenvalues\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)\n                self.D = np.ones(self.dim)\n                self.B = np.eye(self.dim)\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm CMAES_O scored 0.101 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6157c93f-6353-435a-827a-c3194cf64c12"], "operator": null, "metadata": {"aucs": [0.09326040695078008, 9.999999999998899e-05, 0.15430507050180875, 0.1507222163200881, 0.10988953622251851, 0.10524570401549482, 0.026169964209185492, 0.04046148776295977, 0.01596751249382411, 0.12129191099476833, 0.05447154277402644, 0.18360545113784976, 9.999999999998899e-05, 0.07956904311128399, 0.11472098813533138, 0.20482356547559677, 0.1270894657548708, 0.16151137590338527, 0.1413224343658943, 0.13628761586045424]}}
{"id": "61d34ca1-8f81-4cd8-aaa1-990edf41ab43", "fitness": 0.4167034745674763, "name": "AdaptiveDE", "description": "Simplified Adaptive Differential Evolution with reduced mutation strategies and adaptive scaling factor, prioritizing exploration in early stages and exploitation later.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=50, F=0.5, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.CR = CR\n        self.archive = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        if np.min(self.fitness) < self.f_opt:\n            self.f_opt = np.min(self.fitness)\n            self.x_opt = self.population[np.argmin(self.fitness)]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Adaptive F\n                F = self.F * (0.5 + np.random.rand() * 0.5)\n\n                # Mutation: \"DE/rand/1\" for exploration\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                x_mutated = self.population[i] + F * (x_r1 - x_r2)\n\n                # Crossover\n                x_trial = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(x_trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = x_trial\n                    \n                    #Simple Archive update\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        idx = np.random.randint(self.archive_size)\n                        self.archive[idx] = self.population[i].copy()\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n\n                if self.budget <= 0:\n                    break\n                    \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDE scored 0.417 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["63c25a13-6cbe-461f-82fe-efd158076997"], "operator": null, "metadata": {"aucs": [0.14665204105321006, 0.30407572847976894, 0.404531941688692, 0.5171373104879393, 0.2963383559573428, 0.42685433790397775, 0.30231697135115887, 0.3531486158385745, 0.31430030741956805, 0.2031598333307364, 0.6009625982753732, 0.9844878800889455, 0.41428160499557787, 0.31577192593853687, 0.761068235783345, 0.44940074200979185, 0.33990768565898066, 0.5303645375286914, 0.17542398460256048, 0.493884852956754]}}
{"id": "757d7b9f-c0d7-451e-857f-f747bba1c803", "fitness": 0.7537412569316599, "name": "AdaptiveDE", "description": "Simplified Adaptive Differential Evolution with focus on mutation strategy diversity and reduced complexity for faster convergence.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        if np.min(self.fitness) < self.f_opt:\n            self.f_opt = np.min(self.fitness)\n            self.x_opt = self.population[np.argmin(self.fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation strategy selection\n                mutation_strategy = np.random.choice([\"rand/1\", \"best/1\", \"current-to-rand/1\"])\n\n                if mutation_strategy == \"rand/1\":\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    x_mutated = x_r1 + self.F * (x_r2 - x_r3)\n                elif mutation_strategy == \"best/1\":\n                    x_best = self.population[np.argmin(self.fitness)]\n                    indices = np.random.choice(self.pop_size, 2, replace=False)\n                    x_r1, x_r2 = self.population[indices]\n                    x_mutated = x_best + self.F * (x_r1 - x_r2)\n                else:  # \"current-to-rand/1\"\n                    indices = np.random.choice(self.pop_size, 2, replace=False)\n                    x_r1, x_r2 = self.population[indices]\n                    x_mutated = self.population[i] + np.random.rand() * (x_r1 - self.population[i]) + self.F * (x_r2 - self.population[i])\n\n                # Crossover\n                x_trial = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = x_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n\n                if self.budget <= 0:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDE scored 0.754 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["63c25a13-6cbe-461f-82fe-efd158076997"], "operator": null, "metadata": {"aucs": [0.72975666884628, 0.884737046824914, 0.8833225154992559, 0.19900320729821064, 0.8752651091478582, 0.9117584848657224, 0.8343768894659962, 0.8684843965497205, 0.9031736708457582, 0.2228935257769188, 0.9449321388309971, 0.9970624683792751, 0.40741476812565414, 0.894457971960542, 0.9710957908221278, 0.9164039748008691, 0.8718128530681775, 0.9281995712266726, 0.3181908431399243, 0.5124832431583233]}}
{"id": "0fbc38df-d13d-4133-8e2c-1f7689f790c0", "fitness": 0.7358220998143892, "name": "AdaptiveDERestart", "description": "An adaptive differential evolution with self-adaptive mutation strategies and a restart mechanism triggered by stagnation detection, aiming to enhance exploration and exploitation balance.", "code": "import numpy as np\n\nclass AdaptiveDERestart:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, stagnation_threshold=500):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR # Crossover rate\n        self.population = None\n        self.fitness = None\n        self.archive = []\n        self.archive_fitness = []\n        self.success_rates = np.zeros(pop_size) # Track individual success rates\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n\n    def initialize_population(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size # reduce budget by initial evaluation\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.best_fitness_history.append(self.f_opt)\n\n\n    def mutate(self, pop, i):\n        # Self-adaptive mutation strategy selection\n        rand = np.random.rand()\n        if rand < 0.33:\n            # DE/rand/1\n            idxs = np.random.choice(len(pop), 3, replace=False)\n            a, b, c = pop[idxs]\n            mutant = a + self.F * (b - c)\n        elif rand < 0.66:\n            # DE/current-to-best/1\n            idxs = np.random.choice(len(pop), 2, replace=False)\n            b, c = pop[idxs]\n            mutant = pop[i] + self.F * (self.x_opt - pop[i]) + self.F * (b - c)\n        else:\n            # DE/best/1\n            idxs = np.random.choice(len(pop), 2, replace=False)\n            b, c = pop[idxs]\n            mutant = self.x_opt + self.F * (b - c)\n        return np.clip(mutant, -5, 5)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                target = self.population[i]\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, target)\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.success_rates[i] = min(1, self.success_rates[i] + 0.1)  # Increase success rate\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.stagnation_counter = 0 # Reset stagnation counter\n                else:\n                    self.success_rates[i] = max(0, self.success_rates[i] - 0.05)  # Decrease success rate\n\n                if self.budget <= 0:\n                    break\n\n            # Adaptive parameter adjustment\n            self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n            self.CR = np.clip(np.random.normal(0.7, 0.1), 0.1, 1.0)\n            \n            self.best_fitness_history.append(self.f_opt)\n\n            # Stagnation detection\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if np.std(self.best_fitness_history[-self.stagnation_threshold:]) < 1e-6:  #Stagnation detected\n                    self.stagnation_counter += 1\n                    if self.stagnation_counter >= 2: # restart after consecutive stagnation\n                        # Restart population\n                        lb = func.bounds.lb\n                        ub = func.bounds.ub\n                        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n                        self.fitness = np.array([func(x) for x in self.population])\n                        self.budget -= self.pop_size  # reduce budget\n                        self.f_opt = np.min(self.fitness)\n                        self.x_opt = self.population[np.argmin(self.fitness)]\n                        self.stagnation_counter = 0\n                        self.best_fitness_history = [self.f_opt]\n                        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDERestart scored 0.736 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2abff733-e788-4b90-a932-f363213a279c"], "operator": null, "metadata": {"aucs": [0.4280695505510935, 0.7006709805839626, 0.8148814034815312, 0.9192560836945665, 0.8414760019773486, 0.8681004992126781, 0.34578819893002966, 0.7863814566744646, 0.8457935418595703, 0.8074439111516567, 0.9185053410626963, 0.9975556653132668, 0.5081731663167368, 0.8501370575073945, 0.5858069665998291, 0.8623937993172368, 0.7269032712105338, 0.9033596897273872, 0.2130751112305217, 0.7926702998852806]}}
{"id": "211af1ef-5731-48a0-8d44-3a229b8ef1db", "fitness": -Infinity, "name": "CMAES_with_Archive", "description": "Enhanced CMA-ES with archive, adaptive population size, step-size control, and covariance matrix repair, incorporating a more robust handling of covariance updates and archive restarts.", "code": "import numpy as np\n\nclass CMAES_with_Archive:\n    def __init__(self, budget=10000, dim=10, mu_factor=1/4, cs=0.3, c_cov=0.1, archive_size=100):\n        self.budget = budget\n        self.dim = dim\n        self.mu_factor = mu_factor\n        self.cs = cs\n        self.c_cov = c_cov\n        self.archive = []\n        self.archive_size = archive_size\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize parameters\n        lambda_ = int(4 + np.floor(3 * np.log(self.dim))) # Population size (adaptive)\n        mu = int(lambda_ * self.mu_factor)  # Number of parents\n        weights = np.log(mu + 1/2) - np.log(np.arange(1, mu + 1))\n        weights = weights / np.sum(weights)\n        m = np.random.uniform(lb, ub, size=self.dim)  # Mean\n        sigma = 0.1 * (ub - lb)  # Step size\n        C = np.eye(self.dim)  # Covariance matrix\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1/(4*self.dim)) + 1/(21*self.dim**2)) # Expectation of ||N(0,I)||\n        c_mu = self.c_cov # renamed to ensure consistent usage with the algorithm\n\n        evals = 0\n        while evals < self.budget:\n            # Generate and evaluate offspring\n            z = np.random.randn(self.dim, lambda_)\n            try:\n                y = m[:, np.newaxis] + sigma * np.dot(np.linalg.cholesky(C), z)\n            except np.linalg.LinAlgError:\n                C = np.eye(self.dim) # Reset covariance matrix if needed\n                y = m[:, np.newaxis] + sigma * np.dot(np.linalg.cholesky(C), z)\n            \n            # Clip to bounds\n            y = np.clip(y, lb, ub)\n            \n            f = np.array([func(y[:, i]) for i in range(lambda_)], dtype=np.float64)\n            evals += lambda_\n            \n            # Sort by fitness\n            idx = np.argsort(f)\n            f = f[idx]\n            y = y[:, idx]\n            \n            # Update optimal solution\n            if f[0] < self.f_opt:\n                self.f_opt = f[0]\n                self.x_opt = y[:, 0].copy()\n            \n            # Update mean\n            m_old = m.copy()\n            m = np.sum(weights[:, np.newaxis] * y[:, :mu], axis=1)\n\n            # Update evolution path for step size\n            ps = (1 - self.cs) * ps + np.sqrt(self.cs * (2 - self.cs) * weights @ weights) * (np.linalg.solve(np.linalg.cholesky(C).T, (m - m_old) / sigma))\n            \n            # Update evolution path for covariance matrix\n            dm = (m - m_old) / sigma\n            pc = (1 - self.c_cov) * pc + np.sqrt(self.c_cov * (2 - self.c_cov) * weights @ weights) * dm\n\n            # Heuristic: adjust step size\n            norm_ps = np.linalg.norm(ps) / np.sqrt(1 - (1 - self.cs)**(evals/lambda_))\n            if norm_ps > chiN * 1.4:\n                sigma = sigma * np.exp(0.2 + self.cs / 0.2)\n            elif norm_ps < chiN / 1.4:\n                sigma = sigma * np.exp(-0.2 + self.cs / 0.2)\n            \n            # Update covariance matrix\n            C = (1 - c_mu) * C + c_mu * (pc[:, np.newaxis] @ pc[np.newaxis, :])\n            \n            y_diff = y[:, :mu] - m_old[:, np.newaxis]\n            for i in range(mu):\n                C += c_mu * weights[i] * (y_diff[:, i][:, np.newaxis] @ y_diff[:, i][np.newaxis, :]) / sigma**2\n\n            # Ensure positive definiteness\n            C = np.triu(C)\n            C = C + C.T - np.diag(C.diagonal())\n            \n            # Regularize covariance matrix\n            mineig = np.min(np.real(np.linalg.eigvals(C)))\n            if mineig < 0:\n                C = C + (-mineig * np.eye(self.dim))\n\n            try:\n                np.linalg.cholesky(C) # Check if positive definite\n            except np.linalg.LinAlgError:\n                C = np.eye(self.dim) # Reset covariance matrix if needed\n            \n            # Adaptive lambda\n            if evals < self.budget / 2: # Reduce pop size later in optimization, makes algorithm faster\n                lambda_ = int(4 + np.floor(3 * np.log(self.dim)))\n            else:\n                 lambda_ = max(2, int(2 + np.floor(2 * np.log(self.dim))))\n                 mu = int(lambda_ * self.mu_factor)\n                 weights = np.log(mu + 1/2) - np.log(np.arange(1, mu + 1))\n                 weights = weights / np.sum(weights)\n\n            # Archive best solutions to improve exploration if needed\n            for i in range(min(mu, 5)):\n              self.archive.append((f[i], y[:,i].copy()))\n              if len(self.archive) > self.archive_size:\n                  self.archive.pop(0) # Keep archive bounded\n              \n            # Heuristic: Random restart from archive (exploration)\n            if np.random.rand() < 0.01 and len(self.archive) > 0:  # Low probability of restart\n                best_archive_f, best_archive_x = min(self.archive, key=lambda item: item[0])\n                m = best_archive_x.copy() # Restart from best archived point\n                sigma = 0.1 * (ub - lb) # Reset step size\n                C = np.eye(self.dim)   # Reset covariance\n                ps = np.zeros(self.dim) # Reset evolution paths\n                pc = np.zeros(self.dim)\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "An exception occurred: operands could not be broadcast together with shapes (2,) (2,6) .", "error": "", "parent_ids": ["4b5d5ca1-f509-40e7-a9ea-2c785bf4234b"], "operator": null, "metadata": {}}
{"id": "e9f12a4e-a06a-4c2b-9d5d-6871409080b3", "fitness": 0.0, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with self-adaptive parameters, success history adaptation of F and CR, and dynamic population size adjustment based on performance.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR # Crossover rate\n        self.population = None\n        self.fitness = None\n        self.archive = []\n        self.archive_fitness = []\n        self.success_rates = np.zeros(pop_size) # Track individual success rates\n        self.memory_F = np.ones(archive_size) * 0.5\n        self.memory_CR = np.ones(archive_size) * 0.7\n        self.archive_size = archive_size\n        self.sf = []\n        self.scr = []\n        self.p_best_rate = 0.1\n\n    def initialize_population(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size # reduce budget by initial evaluation\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n\n    def mutate(self, pop, i, p_best):\n        \n        if np.random.rand() < p_best:\n            # p_best strategy\n            idxs = np.random.choice(len(pop), 2, replace=False)\n            a, b = pop[idxs]\n            best_idx = np.argmin(self.fitness)\n            return np.clip(pop[best_idx] + self.F * (a - b), -5, 5)\n        else:\n            idxs = np.random.choice(len(pop), 3, replace=False)\n            a, b, c = pop[idxs]\n            return np.clip(a + self.F * (b - c), -5, 5)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        \n        generation = 0\n        while self.budget > 0:\n            self.sf = []\n            self.scr = []\n            for i in range(self.pop_size):\n                target = self.population[i]\n                \n                # Sample F and CR from memory\n                self.F = np.random.choice(self.memory_F)\n                self.CR = np.random.choice(self.memory_CR)\n\n                mutant = self.mutate(self.population, i, self.p_best_rate)\n                trial = self.crossover(mutant, target)\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.success_rates[i] = min(1, self.success_rates[i] + 0.1)  # Increase success rate\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    self.sf.append(self.F)\n                    self.scr.append(self.CR)\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                else:\n                    self.success_rates[i] = max(0, self.success_rates[i] - 0.05)  # Decrease success rate\n\n                if self.budget <= 0:\n                    break\n            \n            # Update memory of F and CR\n            if self.sf:\n                self.memory_F = np.concatenate(([np.mean(self.sf)], self.memory_F[:-1]))\n            if self.scr:\n                self.memory_CR = np.concatenate(([np.mean(self.scr)], self.memory_CR[:-1]))\n\n\n            # Adaptive population adjustment (simplified - keep top percentage and replace bottom randomly)\n            if self.pop_size > 10:  # avoid shrinking to tiny populations\n                sorted_indices = np.argsort(self.fitness)\n                top_indices = sorted_indices[:int(self.pop_size * 0.75)]\n                bottom_indices = sorted_indices[int(self.pop_size * 0.75):]\n\n                # Replace the bottom with new random individuals.\n                lb = func.bounds.lb\n                ub = func.bounds.ub\n                self.population[bottom_indices] = np.random.uniform(lb, ub, size=(len(bottom_indices), self.dim))\n                self.fitness[bottom_indices] = np.array([func(x) for x in self.population[bottom_indices]])\n                self.budget -= len(bottom_indices)  # reduce budget\n\n                # Update best fitness and solution if needed\n                if np.min(self.fitness) < self.f_opt:\n                    self.f_opt = np.min(self.fitness)\n                    self.x_opt = self.population[np.argmin(self.fitness)]\n\n                # Dynamically adjust population size\n                if generation % 10 == 0:\n                    if np.std(self.fitness) < 1e-3 and self.pop_size < 100:  # Reduced diversity, increase pop size\n                        num_new = min(10, self.budget // 2)\n                        new_individuals = np.random.uniform(lb, ub, size=(num_new, self.dim))\n                        new_fitness = [func(x) for x in new_individuals]\n                        self.budget -= num_new\n                        self.population = np.concatenate((self.population, new_individuals))\n                        self.fitness = np.concatenate((self.fitness, new_fitness))\n                        self.pop_size = len(self.population)\n                        self.success_rates = np.concatenate((self.success_rates, np.zeros(num_new)))\n                    elif np.std(self.fitness) > 1 and self.pop_size > 20: # High diversity, reduce population size\n                        num_remove = min(5, self.pop_size - 10)\n                        worst_indices = np.argsort(self.fitness)[-num_remove:]\n                        keep_indices = np.setdiff1d(np.arange(self.pop_size), worst_indices)\n                        self.population = self.population[keep_indices]\n                        self.fitness = self.fitness[keep_indices]\n                        self.success_rates = self.success_rates[keep_indices]\n                        self.pop_size = len(self.population)\n\n            generation += 1\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2abff733-e788-4b90-a932-f363213a279c"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "f4a9f7a7-04c0-482a-8793-124e52aed1a8", "fitness": 0.0, "name": "CMAES_Momentum", "description": "Covariance Matrix Adaptation Evolution Strategy with Momentum and Adaptive Population Size, enhancing exploration and exploitation by incorporating momentum in the mean update and dynamically adjusting population size.", "code": "import numpy as np\n\nclass CMAES_Momentum:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, dsigma=0.2, c_cov=2/15, c_mu=2/3, c_1=2/15, mu_factor=4, initial_sigma=0.5, momentum=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // mu_factor\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.m = None\n        self.sigma = initial_sigma\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.chiN = None\n        self.cs = cs\n        self.dsigma = dsigma\n        self.c_cov = c_cov\n        self.c_mu = c_mu\n        self.c_1 = c_1\n        self.count = 0\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.D = None #Eigenvalues\n        self.B = None #Eigenvectors\n        self.momentum = momentum\n        self.m_old = None\n\n    def __call__(self, func):\n        self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1/(4*self.dim) + 1/(21*self.dim**2))\n        self.D = np.ones(self.dim)\n        self.B = np.eye(self.dim)\n        self.m_old = self.m.copy()\n\n        while self.count < self.budget:\n            #Adapt population size based on sigma\n            self.popsize = max(4, int(3 * np.log(self.dim) * (1 + 0.5* np.exp(-self.sigma)))) \n            self.mu = self.popsize // 4 # keep mu consistent with popsize\n            self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n            self.weights = self.weights / np.sum(self.weights)\n            \n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            x = np.array([self.m + self.sigma * np.dot(self.B * self.D**0.5, zi) for zi in z])\n\n            # Clip x to boundaries\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n\n            f = np.array([func(xi) for xi in x])\n            self.count += self.popsize\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n\n            idx = np.argsort(f)\n            x_mu = x[idx[:self.mu]]\n            z_mu = z[idx[:self.mu]]\n\n            m_old_temp = self.m.copy()\n            self.m = (1 - self.momentum) * np.sum(self.weights[:, None] * x_mu, axis=0) + self.momentum * self.m_old\n\n            zmean = np.sum(self.weights[:, None] * z_mu, axis=0)\n            self.ps = (1 - self.cs) * self.ps + (self.cs * (2 - self.cs))**0.5 * np.dot(self.B, zmean)\n            self.pc = (1 - self.c_cov) * self.pc + (self.c_cov * (2 - self.c_cov))**0.5 * (self.m - m_old_temp) / self.sigma\n            self.m_old = m_old_temp.copy() # store current mean for next iteration momentum.\n\n            hsig = np.linalg.norm(self.ps) / (1 - (1 - self.cs)**(self.count/self.popsize)) / self.chiN < 1.4 + 2/(self.dim+1)\n            dhsig = (1-hsig) * self.c_1 * (2-self.c_1)\n\n            self.C = (1- self.c_cov - self.c_1 * dhsig) * self.C \\\n                     + self.c_1 * np.outer(self.pc, self.pc) \\\n                     + self.c_mu * np.sum(self.weights[:, None, None] * (x_mu - m_old_temp)[..., None] * (x_mu - m_old_temp)[:, None, :], axis=0) / self.sigma**2\n\n            self.sigma *= np.exp((self.cs/self.dsigma) * (np.linalg.norm(self.ps)/self.chiN - 1))\n\n            # Eigenvalue decomposition\n            try:\n                self.D, self.B = np.linalg.eigh(self.C)\n                self.D = np.sqrt(np.maximum(self.D, 1e-16)) # Avoid negative/zero eigenvalues\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)\n                self.D = np.ones(self.dim)\n                self.B = np.eye(self.dim)\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm CMAES_Momentum scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d1fc937e-7e71-421d-a50f-38a7f94c431c"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "36b4aa17-7962-4fb9-b519-d93932dd9e61", "fitness": 0.12586017942074695, "name": "AdaptiveDE", "description": "Enhanced Adaptive Differential Evolution with dynamic parameter adaptation and a more robust stagnation detection mechanism for improved exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_init=0.5, CR_init=0.9, stagnation_threshold=1e-6, stagnation_iter=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.CR = CR_init\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_iter = stagnation_iter\n        self.best_fitness_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        if np.min(self.fitness) < self.f_opt:\n            self.f_opt = np.min(self.fitness)\n            self.x_opt = self.population[np.argmin(self.fitness)]\n        \n        self.best_fitness_history.append(self.f_opt)\n\n        iteration = 0\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Dynamic F and CR adaptation\n                self.F = np.clip(np.random.normal(0.5, 0.3), 0.1, 1.0)  # Adapt F\n                self.CR = np.clip(np.random.normal(0.9, 0.2), 0.1, 1.0)  # Adapt CR\n\n                # Mutation strategy selection\n                mutation_strategy = np.random.choice([\"rand/1\", \"best/1\", \"current-to-rand/1\"])\n\n                if mutation_strategy == \"rand/1\":\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    x_mutated = x_r1 + self.F * (x_r2 - x_r3)\n                elif mutation_strategy == \"best/1\":\n                    x_best = self.population[np.argmin(self.fitness)]\n                    indices = np.random.choice(self.pop_size, 2, replace=False)\n                    x_r1, x_r2 = self.population[indices]\n                    x_mutated = x_best + self.F * (x_r1 - x_r2)\n                else:  # \"current-to-rand/1\"\n                    indices = np.random.choice(self.pop_size, 2, replace=False)\n                    x_r1, x_r2 = self.population[indices]\n                    x_mutated = self.population[i] + np.random.rand() * (x_r1 - self.population[i]) + self.F * (x_r2 - self.population[i])\n\n                # Crossover\n                x_trial = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = x_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n                        \n            self.best_fitness_history.append(self.f_opt)\n            \n            # Stagnation detection\n            if len(self.best_fitness_history) > self.stagnation_iter:\n                if np.abs(self.best_fitness_history[-1] - self.best_fitness_history[-self.stagnation_iter]) < self.stagnation_threshold:\n                    # Restart population if stagnated\n                    self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                    self.fitness = np.array([func(x) for x in self.population])\n                    self.budget -= self.pop_size\n                    \n                    if np.min(self.fitness) < self.f_opt:\n                        self.f_opt = np.min(self.fitness)\n                        self.x_opt = self.population[np.argmin(self.fitness)]\n                    self.best_fitness_history.append(self.f_opt)\n                    \n\n            if self.budget <= 0:\n                break\n            \n            iteration += 1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDE scored 0.126 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["757d7b9f-c0d7-451e-857f-f747bba1c803"], "operator": null, "metadata": {"aucs": [0.2517203588414939, 0]}}
{"id": "87bca48c-80c6-4a86-a842-70b24b377a36", "fitness": -Infinity, "name": "AdaptiveDERestartOED", "description": "Adaptive Differential Evolution with a learning-based mutation factor, dynamic population size adjustment, and orthogonal experimental design-based restart mechanism for enhanced exploration and exploitation.", "code": "import numpy as np\nfrom scipy.stats import norm\n\nclass AdaptiveDERestartOED:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, stagnation_threshold=500, min_pop_size=10, max_pop_size=100, learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.min_pop_size = min_pop_size\n        self.max_pop_size = max_pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR # Crossover rate\n        self.population = None\n        self.fitness = None\n        self.archive = []\n        self.archive_fitness = []\n        self.success_rates = np.zeros(pop_size) # Track individual success rates\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.learning_rate = learning_rate\n        self.mutation_factors = np.ones(pop_size) * self.F\n\n    def initialize_population(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size # reduce budget by initial evaluation\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.best_fitness_history.append(self.f_opt)\n\n\n    def mutate(self, pop, i):\n        # Self-adaptive mutation strategy selection\n        rand = np.random.rand()\n        if rand < 0.33:\n            # DE/rand/1\n            idxs = np.random.choice(len(pop), 3, replace=False)\n            a, b, c = pop[idxs]\n            mutant = a + self.mutation_factors[i] * (b - c)\n        elif rand < 0.66:\n            # DE/current-to-best/1\n            idxs = np.random.choice(len(pop), 2, replace=False)\n            b, c = pop[idxs]\n            mutant = pop[i] + self.mutation_factors[i] * (self.x_opt - pop[i]) + self.mutation_factors[i] * (b - c)\n        else:\n            # DE/best/1\n            idxs = np.random.choice(len(pop), 2, replace=False)\n            b, c = pop[idxs]\n            mutant = self.x_opt + self.mutation_factors[i] * (b - c)\n        return np.clip(mutant, -5, 5)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n    \n    def adjust_population_size(self):\n        # Adjust population size based on success rates\n        avg_success_rate = np.mean(self.success_rates)\n        \n        # Increase pop size if success rate is low, decrease if high\n        if avg_success_rate < 0.2:\n            self.pop_size = min(self.pop_size + 5, self.max_pop_size)\n            self.success_rates = np.concatenate((self.success_rates, np.zeros(5)))\n            self.mutation_factors = np.concatenate((self.mutation_factors, np.ones(5) * self.F))\n        elif avg_success_rate > 0.8:\n            self.pop_size = max(self.pop_size - 5, self.min_pop_size)\n            self.success_rates = self.success_rates[:self.pop_size]\n            self.mutation_factors = self.mutation_factors[:self.pop_size]\n        \n        self.pop_size = int(self.pop_size) # Ensure it's an integer\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                target = self.population[i]\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, target)\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.success_rates[i] = min(1, self.success_rates[i] + self.learning_rate)  # Increase success rate\n                    self.mutation_factors[i] *= np.random.uniform(0.9, 1.1) # Adapt mutation factor\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.stagnation_counter = 0 # Reset stagnation counter\n                else:\n                    self.success_rates[i] = max(0, self.success_rates[i] - self.learning_rate/2)  # Decrease success rate\n                    self.mutation_factors[i] *= np.random.uniform(0.9, 1.1) # Adapt mutation factor\n\n                if self.budget <= 0:\n                    break\n\n            # Adaptive parameter adjustment\n            self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n            self.CR = np.clip(np.random.normal(0.7, 0.1), 0.1, 1.0)\n            self.mutation_factors = np.clip(self.mutation_factors, 0.1, 2.0) # Clip mutation factors\n            \n            self.best_fitness_history.append(self.f_opt)\n            self.adjust_population_size()\n\n            # Stagnation detection\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if np.std(self.best_fitness_history[-self.stagnation_threshold:]) < 1e-6:  #Stagnation detected\n                    self.stagnation_counter += 1\n                    if self.stagnation_counter >= 2: # restart after consecutive stagnation\n                        # Restart population using Orthogonal Experimental Design (OED)\n                        lb = func.bounds.lb\n                        ub = func.bounds.ub\n                        \n                        # Generate a Latin Hypercube Sample (LHS) for restart\n                        from smt.sampling_methods import LHS\n                        sampling = LHS(xlimits=np.array([lb, ub] * self.dim).reshape(self.dim, 2), criterion='maximin')\n                        num_samples = min(self.pop_size, self.budget)\n                        self.population = sampling(num_samples)\n                        \n                        self.fitness = np.array([func(x) for x in self.population])\n                        self.budget -= num_samples  # reduce budget\n                        self.f_opt = np.min(self.fitness)\n                        self.x_opt = self.population[np.argmin(self.fitness)]\n                        self.stagnation_counter = 0\n                        self.best_fitness_history = [self.f_opt]\n                        self.success_rates = np.zeros(self.pop_size) # Reset success rates\n                        self.mutation_factors = np.ones(self.pop_size) * self.F  # Reset mutation factors\n                        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "An exception occurred: index 50 is out of bounds for axis 0 with size 50.", "error": "", "parent_ids": ["0fbc38df-d13d-4133-8e2c-1f7689f790c0"], "operator": null, "metadata": {}}
{"id": "7d2c9802-aedd-4867-ba93-580746df75c0", "fitness": 0.7661072302467967, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with aging, stagnation detection, and dynamic parameter adaptation to enhance exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_initial=0.5, CR_initial=0.9, age_limit=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_initial\n        self.CR = CR_initial\n        self.age_limit = age_limit\n        self.ages = np.zeros(pop_size)  # Initialize ages for each individual\n        self.stagnation_counter = 0\n        self.last_best_fitness = np.inf\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        if np.min(self.fitness) < self.f_opt:\n            self.f_opt = np.min(self.fitness)\n            self.x_opt = self.population[np.argmin(self.fitness)]\n            self.last_best_fitness = self.f_opt\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation strategy selection (simplified)\n                mutation_strategy = np.random.choice([\"rand/1\", \"best/1\"])\n\n                if mutation_strategy == \"rand/1\":\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    x_mutated = x_r1 + self.F * (x_r2 - x_r3)\n                else:  # \"best/1\"\n                    x_best = self.population[np.argmin(self.fitness)]\n                    indices = np.random.choice(self.pop_size, 2, replace=False)\n                    x_r1, x_r2 = self.population[indices]\n                    x_mutated = x_best + self.F * (x_r1 - x_r2)\n\n                # Crossover\n                x_trial = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = x_trial\n                    self.ages[i] = 0  # Reset age\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n                        self.last_best_fitness = self.f_opt\n                        self.stagnation_counter = 0  # Reset stagnation counter\n                else:\n                    self.ages[i] += 1  # Increment age\n\n                # Aging mechanism: replace old individuals\n                if self.ages[i] > self.age_limit:\n                    self.population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                    self.fitness[i] = func(self.population[i])\n                    self.ages[i] = 0\n                    self.budget -= 1\n                    if self.fitness[i] < self.f_opt:\n                        self.f_opt = self.fitness[i]\n                        self.x_opt = self.population[i]\n                        self.last_best_fitness = self.f_opt\n                        self.stagnation_counter = 0\n\n                if self.budget <= 0:\n                    break\n            \n            #Stagnation Check and Parameter Adaptation\n            if abs(self.last_best_fitness - self.f_opt) < 1e-6:\n                self.stagnation_counter +=1\n            else:\n                self.stagnation_counter = 0\n                self.last_best_fitness = self.f_opt\n                \n            if self.stagnation_counter > 50:\n                self.F = np.clip(self.F * 1.1, 0.1, 0.9)\n                self.CR = np.clip(self.CR * 0.9, 0.1, 0.9)\n                self.stagnation_counter = 0\n            else:\n                 self.F = np.clip(self.F * 0.99, 0.1, 0.9)\n                 self.CR = np.clip(self.CR * 1.01, 0.1, 0.9)\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDE scored 0.766 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["757d7b9f-c0d7-451e-857f-f747bba1c803"], "operator": null, "metadata": {"aucs": [0.25948092405826817, 0.6787514998678275, 0.9120681445795016, 0.9539543493246484, 0.9087836582561978, 0.924527780421014, 0.8812918541702371, 0.5740055317370707, 0.9173139555150688, 0.8964130586466815, 0.9442228372750484, 0.9942466577908529, 0.7382569403425467, 0.908291261716986, 0]}}
{"id": "a0050554-8003-47f0-b5eb-124f906048f3", "fitness": 0.4021143788899302, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with archive-based mutation and probabilistic parameter adaptation to balance exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=50, F=0.5, CR=0.9, p_archive=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.CR = CR\n        self.archive = []\n        self.p_archive = p_archive\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        if np.min(self.fitness) < self.f_opt:\n            self.f_opt = np.min(self.fitness)\n            self.x_opt = self.population[np.argmin(self.fitness)]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                F = self.F * (0.5 + np.random.rand() * 0.5)\n                CR = self.CR * (0.5 + np.random.rand() * 0.5)\n\n                # Mutation strategy with archive\n                if np.random.rand() < self.p_archive and len(self.archive) > 0:\n                    # Use archive\n                    idx_archive = np.random.randint(len(self.archive))\n                    x_r1 = self.archive[idx_archive]\n                    indices = np.random.choice(self.pop_size, 2, replace=False)\n                    x_r2, x_r3 = self.population[indices]\n                    x_mutated = self.population[i] + F * (x_r1 - x_r2)\n                else:\n                    # Classic DE/rand/1\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    x_mutated = self.population[i] + F * (x_r1 - x_r2)\n\n                # Crossover\n                x_trial = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(x_trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = x_trial\n                    \n                    #Simple Archive update\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        idx = np.random.randint(self.archive_size)\n                        self.archive[idx] = self.population[i].copy()\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n\n                if self.budget <= 0:\n                    break\n                    \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDE scored 0.402 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["61d34ca1-8f81-4cd8-aaa1-990edf41ab43"], "operator": null, "metadata": {"aucs": [0.15688086308386007, 0.2593268742192686, 0.3964513472464374, 0.45063294317135183, 0.28960340313413535, 0.43437760105840995, 0.2927245290827808, 0.3456365769355907, 0.3127579450540242, 0.19030222321786205, 0.581488036577308, 0.9964147302691372, 0.3220206477490082, 0.303221698214063, 0.7765834489955081, 0.4147765516763843, 0.3301137760084104, 0.5175861229276768, 0.18610383757383475, 0.48528442160355223]}}
{"id": "ee3d8d2d-0c78-410b-9468-4cc66cc84739", "fitness": 0.6309842069684245, "name": "AdaptiveDERestart", "description": "Simplified Adaptive Differential Evolution with dynamic F/CR adaptation and periodic population rejuvenation for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDERestart:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, refresh_interval=200):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR # Crossover rate\n        self.population = None\n        self.fitness = None\n        self.refresh_interval = refresh_interval\n        self.generation = 0\n\n    def initialize_population(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size # reduce budget by initial evaluation\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n\n    def mutate(self, pop, i):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n        mutant = a + self.F * (b - c)\n        return np.clip(mutant, -5, 5)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                target = self.population[i]\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, target)\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n            \n            self.generation += 1\n            # Dynamic adaptation of F and CR\n            self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n            self.CR = np.clip(np.random.normal(0.7, 0.1), 0.1, 1.0)\n\n            # Population refresh\n            if self.generation % self.refresh_interval == 0:\n                lb = func.bounds.lb\n                ub = func.bounds.ub\n                self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.budget -= self.pop_size  # reduce budget\n                self.f_opt = np.min(self.fitness)\n                self.x_opt = self.population[np.argmin(self.fitness)]\n                        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDERestart scored 0.631 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0fbc38df-d13d-4133-8e2c-1f7689f790c0"], "operator": null, "metadata": {"aucs": [0.22511927815505195, 0.5060321827395251, 0.608500847427422, 0.8617334696272974, 0.6889748562020912, 0.7643737991176035, 0.5721913841185793, 0.583818753865416, 0.6775981478910221, 0.5759663836293117, 0.7942484162344854, 0.9957737021873408, 0.38380989621783057, 0.6615975215744763, 0.890794806486096, 0.721854440731307, 0.5198421868832028, 0.8141904888885209, 0.25692701996915934, 0.5163365574227505]}}
{"id": "ff87f24e-b78e-4729-92e7-3bff4e547044", "fitness": 0.6328899971147914, "name": "AdaptiveDERestart", "description": "Simplified adaptive differential evolution with enhanced stagnation handling using L-BFGS-B local search upon stagnation.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDERestart:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, stagnation_threshold=500, local_search_iterations=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.population = None\n        self.fitness = None\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.local_search_iterations = local_search_iterations\n\n    def initialize_population(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.best_fitness_history.append(self.f_opt)\n\n    def mutate(self, pop, i):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n        mutant = a + self.F * (b - c)\n        return np.clip(mutant, -5, 5)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def local_search(self, func, x0):\n        bounds = [(-5, 5)] * self.dim\n        result = minimize(func, x0, method='L-BFGS-B', bounds=bounds, options={'maxiter': self.local_search_iterations})\n        return result.fun, result.x\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                target = self.population[i]\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, target)\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.stagnation_counter = 0\n                if self.budget <= 0:\n                    break\n\n            self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n            self.CR = np.clip(np.random.normal(0.7, 0.1), 0.1, 1.0)\n            self.best_fitness_history.append(self.f_opt)\n\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if np.std(self.best_fitness_history[-self.stagnation_threshold:]) < 1e-6:\n                    self.stagnation_counter += 1\n                    if self.stagnation_counter >= 2:\n                        # Perform Local Search on best solution\n                        f_local, x_local = self.local_search(func, self.x_opt)\n                        self.budget -= self.local_search_iterations # approximate local search cost\n                        if f_local < self.f_opt:\n                            self.f_opt = f_local\n                            self.x_opt = x_local\n\n                        # Restart Population\n                        lb = func.bounds.lb\n                        ub = func.bounds.ub\n                        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n                        self.fitness = np.array([func(x) for x in self.population])\n                        self.budget -= self.pop_size\n                        self.f_opt = np.min(self.fitness)\n                        self.x_opt = self.population[np.argmin(self.fitness)]\n                        self.stagnation_counter = 0\n                        self.best_fitness_history = [self.f_opt]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDERestart scored 0.633 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0fbc38df-d13d-4133-8e2c-1f7689f790c0"], "operator": null, "metadata": {"aucs": [0.23658401922310612, 0.46549070871584186, 0.5740570758346135, 0.8555328183821097, 0.6910386684083547, 0.7555424448188436, 0.5765216438502391, 0.6066245051448901, 0.7038583000271196, 0.6282758071919647, 0.779862918806381, 0.9926498347797399, 0.36666138857410335, 0.6176403898191933, 0.8840854816229187, 0.7433715344366092, 0.5554645758815857, 0.8035535046348204, 0.21950848327987782, 0.6014758388635167]}}
{"id": "0c5ba51b-9ca7-468e-89b0-7e951ab578d5", "fitness": 0.6334905635465948, "name": "AdaptiveDERestart", "description": "Simplified Adaptive Differential Evolution with dynamic F/CR adaptation based on population diversity and a restart mechanism upon stagnation to balance exploration and exploitation effectively.", "code": "import numpy as np\n\nclass AdaptiveDERestart:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, stagnation_threshold=500, diversity_threshold=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR # Crossover rate\n        self.population = None\n        self.fitness = None\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.diversity_threshold = diversity_threshold\n\n    def initialize_population(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size # reduce budget by initial evaluation\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.best_fitness_history.append(self.f_opt)\n\n    def mutate(self, pop, i):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n        mutant = a + self.F * (b - c)\n        return np.clip(mutant, -5, 5)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                target = self.population[i]\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, target)\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.stagnation_counter = 0 # Reset stagnation counter\n\n                if self.budget <= 0:\n                    break\n\n            # Adaptive parameter adjustment based on population diversity\n            diversity = np.std(self.population)\n            if diversity > self.diversity_threshold:\n                self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n                self.CR = np.clip(np.random.normal(0.7, 0.1), 0.1, 1.0)\n            else:\n                 self.F = np.clip(np.random.normal(0.8, 0.2), 0.1, 1.0)\n                 self.CR = np.clip(np.random.normal(0.4, 0.1), 0.1, 1.0)\n            \n            self.best_fitness_history.append(self.f_opt)\n\n            # Stagnation detection\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if np.std(self.best_fitness_history[-self.stagnation_threshold:]) < 1e-6:  #Stagnation detected\n                    self.stagnation_counter += 1\n                    if self.stagnation_counter >= 2: # restart after consecutive stagnation\n                        # Restart population\n                        lb = func.bounds.lb\n                        ub = func.bounds.ub\n                        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n                        self.fitness = np.array([func(x) for x in self.population])\n                        self.budget -= self.pop_size  # reduce budget\n                        self.f_opt = np.min(self.fitness)\n                        self.x_opt = self.population[np.argmin(self.fitness)]\n                        self.stagnation_counter = 0\n                        self.best_fitness_history = [self.f_opt]\n                        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDERestart scored 0.633 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0fbc38df-d13d-4133-8e2c-1f7689f790c0"], "operator": null, "metadata": {"aucs": [0.22435341964913036, 0.42661942745103065, 0.5923669502900698, 0.8546507408470798, 0.6808626704980454, 0.7492645602260231, 0.6222174295794358, 0.5943081150274165, 0.6945570112224966, 0.6088019761278551, 0.79599106767882, 0.9965275384382084, 0.3786169541688704, 0.6645074783785796, 0.8857031958494662, 0.7608524574529443, 0.51897016599569, 0.8143492844525947, 0.28464731000204746, 0.521643517596093]}}
{"id": "c91ae0e5-3a39-4cc8-8ed9-5005d3c5260a", "fitness": 0.10383281413571348, "name": "CMAES_O", "description": "Improved CMA-ES with adaptive covariance matrix adaptation, active covariance update, and a simplified handling of boundary constraints.", "code": "import numpy as np\n\nclass CMAES_O:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, dsigma=0.2, c_cov=2/15, c_mu=2/3, c_1=2/15, mu_factor=4, initial_sigma=0.5, active=True):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // mu_factor\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.m = None\n        self.sigma = initial_sigma\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.chiN = None\n        self.cs = cs\n        self.dsigma = dsigma\n        self.c_cov = c_cov\n        self.c_mu = c_mu\n        self.c_1 = c_1\n        self.count = 0\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.D = None #Eigenvalues\n        self.B = None #Eigenvectors\n        self.active = active\n        self.mu_eff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n\n    def __call__(self, func):\n        self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1/(4*self.dim) + 1/(21*self.dim**2))\n        self.D = np.ones(self.dim)\n        self.B = np.eye(self.dim)\n\n        while self.count < self.budget:\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            R = np.linalg.qr(z)[0]\n\n            x = np.array([self.m + self.sigma * np.dot(self.B * self.D**0.5, R[i, :]) for i in range(self.popsize)])\n            \n            # Reflect boundary handling - simplified\n            for i in range(self.popsize):\n                for j in range(self.dim):\n                    if x[i, j] < func.bounds.lb[j]:\n                        x[i, j] = 2 * func.bounds.lb[j] - x[i, j]\n                    if x[i, j] > func.bounds.ub[j]:\n                        x[i, j] = 2 * func.bounds.ub[j] - x[i, j]\n\n            f = np.array([func(xi) for xi in x])\n            self.count += self.popsize\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n            \n            idx = np.argsort(f)\n            x_mu = x[idx[:self.mu]]\n            z_mu = R[idx[:self.mu]]\n\n            m_old = self.m.copy()\n            self.m = np.sum(self.weights[:, None] * x_mu, axis=0)\n\n            zmean = np.sum(self.weights[:, None] * z_mu, axis=0)\n            self.ps = (1 - self.cs) * self.ps + (self.cs * (2 - self.cs))**0.5 * np.dot(self.B, zmean)\n            self.pc = (1 - self.c_cov) * self.pc + (self.c_cov * (2 - self.c_cov))**0.5 * (self.m - m_old) / self.sigma\n\n            hsig = np.linalg.norm(self.ps) / (1 - (1 - self.cs)**(self.count/self.popsize)) / self.chiN < 1.4 + 2/(self.dim+1)\n            dhsig = (1-hsig) * self.c_1 * (2-self.c_1)\n\n            # Active CMA\n            if self.active:\n                neg_idx = np.where(self.weights < 0)[0]\n                \n                x_neg = x[idx[neg_idx]]\n                z_neg = R[idx[neg_idx]]\n                \n                w_neg = self.weights[neg_idx]\n                \n                self.C = (1- self.c_cov - self.c_1 * dhsig) * self.C \\\n                         + self.c_1 * np.outer(self.pc, self.pc) \\\n                         + self.c_mu * np.sum(self.weights[:, None, None] * (x_mu - m_old)[..., None] * (x_mu - m_old)[:, None, :], axis=0) / self.sigma**2 \\\n                         + self.c_mu * np.sum(w_neg[:, None, None] * (x_neg - m_old)[..., None] * (x_neg - m_old)[:, None, :], axis=0) / self.sigma**2\n                \n            else:\n                self.C = (1- self.c_cov - self.c_1 * dhsig) * self.C \\\n                         + self.c_1 * np.outer(self.pc, self.pc) \\\n                         + self.c_mu * np.sum(self.weights[:, None, None] * (x_mu - m_old)[..., None] * (x_mu - m_old)[:, None, :], axis=0) / self.sigma**2\n\n\n            self.sigma *= np.exp((self.cs/self.dsigma) * (np.linalg.norm(self.ps)/self.chiN - 1))\n            \n            # Eigenvalue decomposition\n            try:\n                self.D, self.B = np.linalg.eigh(self.C)\n                self.D = np.sqrt(np.maximum(self.D, 1e-16)) # Avoid negative/zero eigenvalues\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)\n                self.D = np.ones(self.dim)\n                self.B = np.eye(self.dim)\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm CMAES_O scored 0.104 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d1fc937e-7e71-421d-a50f-38a7f94c431c"], "operator": null, "metadata": {"aucs": [9.999999999998899e-05, 9.999999999998899e-05, 0.17393101949130596, 0.1492840493390466, 0.0723743134412036, 0.11500875168238422, 0.034931396784860236, 0.04833371627724026, 0.027648723369279704, 0.0771930054313229, 0.1183397181368342, 0.2517410325356676, 0.21533846681289548, 0.0622741672061784, 0.19390665970560939, 0.10829602609160482, 0.10065982381372773, 0.12068268853804243, 0.057968844892606675, 0.148543879164459]}}
{"id": "4f9ee776-599b-4a4d-a8a2-2c140df95c9a", "fitness": 0.3060473991329485, "name": "AdaptiveDE_DistanceMutation", "description": "An adaptive Differential Evolution algorithm with a distance-based mutation operator that promotes diversity and a memory-based adaptation of F and CR.", "code": "import numpy as np\n\nclass AdaptiveDE_DistanceMutation:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, memory_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Initial mutation factor\n        self.CR = CR # Initial Crossover rate\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.F_memory = np.full(memory_size, F)\n        self.CR_memory = np.full(memory_size, CR)\n        self.memory_index = 0\n        self.archive = []  # Store previously evaluated solutions\n        self.archive_fitness = []\n\n    def initialize_population(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n\n    def distance_based_mutation(self, pop, i):\n        # Calculate distances to other individuals\n        distances = np.linalg.norm(pop - pop[i], axis=1)\n        distances[i] = np.inf  # Exclude self\n\n        # Select two individuals based on distances (farthest ones)\n        idx1 = np.argmax(distances)\n        distances[idx1] = np.inf\n        idx2 = np.argmax(distances)\n        \n        # DE/rand/1-like mutation using farthest individuals\n        idxs = np.random.choice(len(pop), 1, replace=False)\n        a = pop[idxs][0]  # Pick a random individual\n\n        mutant = a + self.F * (pop[idx1] - pop[idx2])\n        return np.clip(mutant, -5, 5)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def update_memory(self, F_successful, CR_successful):\n         if F_successful is not None and CR_successful is not None:\n                self.F_memory[self.memory_index] = F_successful\n                self.CR_memory[self.memory_index] = CR_successful\n                self.memory_index = (self.memory_index + 1) % len(self.F_memory)\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        generation = 0\n        while self.budget > 0:\n            F_successful_list = []\n            CR_successful_list = []\n            for i in range(self.pop_size):\n                target = self.population[i]\n                mutant = self.distance_based_mutation(self.population, i)\n                trial = self.crossover(mutant, target)\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    F_successful_list.append(self.F)\n                    CR_successful_list.append(self.CR)\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                if self.budget <= 0:\n                    break\n\n            # Memory-based adaptation of F and CR\n            if F_successful_list and CR_successful_list:\n                F_successful = np.mean(F_successful_list)\n                CR_successful = np.mean(CR_successful_list)\n                self.update_memory(F_successful, CR_successful)\n\n            self.F = np.clip(np.random.choice(self.F_memory), 0.1, 1.0)  # Sample from memory\n            self.CR = np.clip(np.random.choice(self.CR_memory), 0.1, 1.0) # Sample from memory\n            generation += 1\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDE_DistanceMutation scored 0.306 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0fbc38df-d13d-4133-8e2c-1f7689f790c0"], "operator": null, "metadata": {"aucs": [0.11078293297696762, 0.2453402374022503, 0.277587892787857, 0.2850722059996633, 0.22594331745797247, 0.25936396751988, 0.26254907190765187, 0.259401264421547, 0.18126620344389544, 0.15464560140350436, 0.20015257297985378, 0.9994981452851625, 0.23728096447264968, 0.22268591398471316, 0.639481334101113, 0.310778197490167, 0.2737031824167462, 0.3336602741811793, 0.18041949705087423, 0.4613352053753218]}}
{"id": "aa8a09fa-4f5b-4304-b1e2-fe97de30df90", "fitness": -Infinity, "name": "CMAES_ODS", "description": "Covariance Matrix Adaptation Evolution Strategy with orthogonal sampling, dynamic population size based on success rate, and simplified rank-one update.", "code": "import numpy as np\n\nclass CMAES_ODS:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, dsigma=0.2, c_cov=2/15, c_mu=2/3, c_1=2/15, mu_factor=4, initial_sigma=0.5, success_history_length=10):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // mu_factor\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.m = None\n        self.sigma = initial_sigma\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.chiN = None\n        self.cs = cs\n        self.dsigma = dsigma\n        self.c_cov = c_cov\n        self.c_mu = c_mu\n        self.c_1 = c_1\n        self.count = 0\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.D = None #Eigenvalues\n        self.B = None #Eigenvectors\n        self.success_history = []\n        self.success_history_length = success_history_length\n        self.target_success = 0.25 # Target success rate for population size adaptation\n\n    def __call__(self, func):\n        self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1/(4*self.dim) + 1/(21*self.dim**2))\n        self.D = np.ones(self.dim)\n        self.B = np.eye(self.dim)\n\n        while self.count < self.budget:\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            R = np.linalg.qr(z)[0]\n\n            x = np.array([self.m + self.sigma * np.dot(self.B * self.D**0.5, R[i, :]) for i in range(self.popsize)])\n            \n            # Clip x to boundaries\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            \n            f = np.array([func(xi) for xi in x])\n            self.count += self.popsize\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n            \n            idx = np.argsort(f)\n            x_mu = x[idx[:self.mu]]\n            z_mu = R[idx[:self.mu]]\n\n            m_old = self.m.copy()\n            self.m = np.sum(self.weights[:, None] * x_mu, axis=0)\n\n            zmean = np.sum(self.weights[:, None] * z_mu, axis=0)\n            self.ps = (1 - self.cs) * self.ps + (self.cs * (2 - self.cs))**0.5 * np.dot(self.B, zmean)\n            self.pc = (1 - self.c_cov) * self.pc + (self.c_cov * (2 - self.c_cov))**0.5 * (self.m - m_old) / self.sigma\n\n            hsig = np.linalg.norm(self.ps) / (1 - (1 - self.cs)**(self.count/self.popsize)) / self.chiN < 1.4 + 2/(self.dim+1)\n            dhsig = (1-hsig) * self.c_1 * (2-self.c_1)\n            \n            # Simplified Rank-One Update\n            self.C = (1 - self.c_1 * dhsig - self.c_1) * self.C + self.c_1 * np.outer(self.pc, self.pc)\n            \n            self.sigma *= np.exp((self.cs/self.dsigma) * (np.linalg.norm(self.ps)/self.chiN - 1))\n            \n            # Eigenvalue decomposition\n            try:\n                self.D, self.B = np.linalg.eigh(self.C)\n                self.D = np.sqrt(np.maximum(self.D, 1e-16)) # Avoid negative/zero eigenvalues\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)\n                self.D = np.ones(self.dim)\n                self.B = np.eye(self.dim)\n\n            # Dynamic Population Size Adjustment\n            success_rate = np.mean(f < np.median(f))\n            self.success_history.append(success_rate)\n            if len(self.success_history) > self.success_history_length:\n                self.success_history.pop(0)\n\n            avg_success = np.mean(self.success_history) if self.success_history else success_rate # Ensure there's a value even if history is empty\n\n            if avg_success > self.target_success + 0.05:\n                self.popsize = max(4, int(self.popsize * 0.9))  # Reduce population size\n                self.mu = self.popsize // mu_factor\n                self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n                self.weights = self.weights / np.sum(self.weights)\n\n            elif avg_success < self.target_success - 0.05:\n                self.popsize = min(self.budget // 10, int(self.popsize * 1.1)) # Increase population size\n                self.mu = self.popsize // mu_factor\n                self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n                self.weights = self.weights / np.sum(self.weights)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "An exception occurred: name 'mu_factor' is not defined.", "error": "", "parent_ids": ["d1fc937e-7e71-421d-a50f-38a7f94c431c"], "operator": null, "metadata": {}}
{"id": "714f5175-076b-443b-ac5f-ee2c00d0453e", "fitness": 0.7273641303474948, "name": "AdaptiveDEShrink", "description": "An adaptive differential evolution with a shrinking population size, focused sampling around the best solution, and dynamic parameter control based on success history.", "code": "import numpy as np\n\nclass AdaptiveDEShrink:\n    def __init__(self, budget=10000, dim=10, pop_size=100, F=0.5, CR=0.7, shrink_factor=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.initial_pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR # Crossover rate\n        self.population = None\n        self.fitness = None\n        self.shrink_factor = shrink_factor\n        self.success_history_F = []\n        self.success_history_CR = []\n        self.min_pop_size = 10\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n\n    def mutate(self, pop, i):\n        # Focused sampling around best\n        idxs = np.random.choice(len(pop), 2, replace=False)\n        a, b = pop[idxs]\n        mutant = self.x_opt + self.F * (a - b)\n        return np.clip(mutant, self.lb, self.ub)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def shrink_population(self):\n        if self.pop_size > self.min_pop_size:\n            self.pop_size = max(self.min_pop_size, int(self.pop_size * self.shrink_factor))\n            # Keep the best individuals\n            sorted_indices = np.argsort(self.fitness)\n            self.population = self.population[sorted_indices[:self.pop_size]]\n            self.fitness = self.fitness[sorted_indices[:self.pop_size]]\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(self.population)\n            new_fitness = np.copy(self.fitness)\n\n            successful_F = []\n            successful_CR = []\n\n            for i in range(self.pop_size):\n                target = self.population[i]\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, target)\n\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                    successful_F.append(self.F)\n                    successful_CR.append(self.CR)\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n\n            self.population = new_population\n            self.fitness = new_fitness\n\n            # Adaptive F and CR based on successful values\n            if successful_F:\n                self.F = np.clip(np.mean(successful_F), 0.1, 1.0)\n            if successful_CR:\n                self.CR = np.clip(np.mean(successful_CR), 0.1, 1.0)\n\n            # Population shrinking\n            if generation % 10 == 0:\n                self.shrink_population()\n\n            if self.budget <=0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDEShrink scored 0.727 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0fbc38df-d13d-4133-8e2c-1f7689f790c0"], "operator": null, "metadata": {"aucs": [0.24359169234431854, 0.7334625882503976, 0.7709623807325378, 0.8907403813989024, 0.8023800191610801, 0.8274200669996239, 0.3649287321979826, 0.7385989876664962, 0.7853794832940445, 0.7675197752899185, 0.8591416170338206, 0.992369765681224, 0.5417270308657893, 0.7736456845946711, 0.9152465302580405, 0.8153764868120701, 0.7381097891331965, 0.8731215358579045, 0.6005000509336609, 0.5130600084442147]}}
{"id": "28e1e55a-f685-46db-bfa1-a9bdba5701ee", "fitness": -Infinity, "name": "AdaptiveDE_NM", "description": "Hybridizes Differential Evolution with Nelder-Mead simplex for local refinement and adaptive population sizing based on performance.", "code": "import numpy as np\nfrom scipy.optimize import minimize, Bounds\n\nclass AdaptiveDE_NM:\n    def __init__(self, budget=10000, dim=10, pop_size=None, F=0.5, CR=0.7, nm_iterations=3, success_threshold=0.1, pop_reduction_factor=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 5 * dim\n        self.F = F\n        self.CR = CR\n        self.population = None\n        self.fitness = None\n        self.nm_iterations = nm_iterations\n        self.success_threshold = success_threshold\n        self.pop_reduction_factor = pop_reduction_factor\n        self.best_fitness_history = []\n        self.success_counter = 0\n\n\n    def initialize_population(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.best_fitness_history.append(self.f_opt)\n\n    def mutate(self, pop, i):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n        mutant = a + self.F * (b - c)\n        return np.clip(mutant, -5, 5)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def nelder_mead(self, func, x0):\n        bounds = Bounds(lb=-5, ub=5)\n        result = minimize(func, x0, method='Nelder-Mead', bounds=bounds, options={'maxiter': self.nm_iterations})\n        return result.fun, result.x\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            new_population = np.copy(self.population)\n            new_fitness = np.copy(self.fitness)\n\n            for i in range(self.pop_size):\n                target = self.population[i]\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, target)\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.success_counter += 1\n\n\n                if self.budget <= 0:\n                    break\n            self.population = new_population\n            self.fitness = new_fitness\n            best_index = np.argmin(self.fitness)\n            \n            # Nelder-Mead refinement on the best solution\n            f_nm, x_nm = self.nelder_mead(func, self.population[best_index])\n            if f_nm < self.f_opt:\n                self.f_opt = f_nm\n                self.x_opt = x_nm\n            self.best_fitness_history.append(self.f_opt)\n            \n            if self.success_counter / self.pop_size > self.success_threshold:\n                self.pop_size = max(int(self.pop_size * self.pop_reduction_factor), 10)\n                self.population = self.population[:self.pop_size]\n                self.fitness = self.fitness[:self.pop_size]\n                self.success_counter = 0\n                \n                # Re-initialize a portion of the population to maintain diversity\n                num_reinit = int(self.pop_size * (1 - self.pop_reduction_factor))\n                lb = func.bounds.lb\n                ub = func.bounds.ub\n                new_individuals = np.random.uniform(lb, ub, size=(num_reinit, self.dim))\n                new_fitnesses = np.array([func(x) for x in new_individuals])\n                self.budget -= num_reinit\n\n                self.population = np.concatenate((self.population, new_individuals))\n                self.fitness = np.concatenate((self.fitness, new_fitnesses))\n                self.pop_size = len(self.population)\n\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "An exception occurred: name 'Bounds' is not defined.", "error": "", "parent_ids": ["ff87f24e-b78e-4729-92e7-3bff4e547044"], "operator": null, "metadata": {}}
{"id": "5dd649fe-b364-4b90-b081-ab0648faa2ae", "fitness": -Infinity, "name": "AdaptiveDEOrtho", "description": "Adaptive Differential Evolution with orthogonal learning and dynamic population size adjustment, focusing on exploration and exploitation by integrating orthogonal design and varying population size based on performance.", "code": "import numpy as np\nimport smt.sampling_methods as smt\n\nclass AdaptiveDEOrtho:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, stagnation_threshold=500, diversity_threshold=0.01, ortho_sample_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR # Crossover rate\n        self.population = None\n        self.fitness = None\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.diversity_threshold = diversity_threshold\n        self.ortho_sample_size = ortho_sample_size\n\n    def initialize_population(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size # reduce budget by initial evaluation\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.best_fitness_history.append(self.f_opt)\n\n    def mutate(self, pop, i):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n        mutant = a + self.F * (b - c)\n        return np.clip(mutant, -5, 5)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n    \n    def orthogonal_learning(self, func, best_x):\n        # Generate orthogonal sample around the best solution\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        # Define the bounds for orthogonal sampling centered around best_x\n        ortho_lb = np.maximum(lb, best_x - 0.5) # Ensure bounds are within the function bounds.  0.5 is an arbitrary range here.\n        ortho_ub = np.minimum(ub, best_x + 0.5)\n        \n        sampling = smt.LHS(xlimits=np.stack((ortho_lb, ortho_ub), axis=-1))\n        ortho_samples = sampling(self.ortho_sample_size)\n\n        # Evaluate the orthogonal samples\n        ortho_fitness = np.array([func(x) for x in ortho_samples])\n        self.budget -= self.ortho_sample_size\n\n        # Update best solution if a better one is found\n        best_index = np.argmin(ortho_fitness)\n        if ortho_fitness[best_index] < self.f_opt:\n            self.f_opt = ortho_fitness[best_index]\n            self.x_opt = ortho_samples[best_index]\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                target = self.population[i]\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, target)\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.stagnation_counter = 0 # Reset stagnation counter\n\n                if self.budget <= 0:\n                    break\n            \n            # Orthogonal learning around the best solution\n            self.orthogonal_learning(func, self.x_opt)\n\n            # Adaptive parameter adjustment based on population diversity\n            diversity = np.std(self.population)\n            if diversity > self.diversity_threshold:\n                self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n                self.CR = np.clip(np.random.normal(0.7, 0.1), 0.1, 1.0)\n            else:\n                 self.F = np.clip(np.random.normal(0.8, 0.2), 0.1, 1.0)\n                 self.CR = np.clip(np.random.normal(0.4, 0.1), 0.1, 1.0)\n            \n            self.best_fitness_history.append(self.f_opt)\n\n            # Stagnation detection\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if np.std(self.best_fitness_history[-self.stagnation_threshold:]) < 1e-6:  #Stagnation detected\n                    self.stagnation_counter += 1\n                    if self.stagnation_counter >= 2: # restart after consecutive stagnation\n                        # Restart population (reduce population size if budget is low)\n                        lb = func.bounds.lb\n                        ub = func.bounds.ub\n\n                        #Reduce population size to conserve budget\n                        new_pop_size = min(self.pop_size, self.budget // 2) # Ensure at least two evaluations can occur.\n                        self.pop_size = new_pop_size\n\n                        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n                        self.fitness = np.array([func(x) for x in self.population])\n                        self.budget -= self.pop_size  # reduce budget\n                        self.f_opt = np.min(self.fitness)\n                        self.x_opt = self.population[np.argmin(self.fitness)]\n                        self.stagnation_counter = 0\n                        self.best_fitness_history = [self.f_opt]\n                        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "An exception occurred: No module named 'smt'.", "error": "", "parent_ids": ["0c5ba51b-9ca7-468e-89b0-7e951ab578d5"], "operator": null, "metadata": {}}
{"id": "13ad324f-ac18-42bc-9cd2-e3fcc752411b", "fitness": -Infinity, "name": "AdaptiveDEOrthoArchive", "description": "Adaptive Differential Evolution with orthogonal learning and archive-based mutation to enhance exploration and exploitation by leveraging past promising solutions.", "code": "import numpy as np\n\nclass AdaptiveDEOrthoArchive:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, F=0.5, CR=0.7, stagnation_threshold=500, diversity_threshold=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F  # Mutation factor\n        self.CR = CR # Crossover rate\n        self.population = None\n        self.fitness = None\n        self.archive = []\n        self.archive_fitness = []\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.diversity_threshold = diversity_threshold\n\n    def initialize_population(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size # reduce budget by initial evaluation\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.best_fitness_history.append(self.f_opt)\n\n    def mutate(self, pop, i):\n        if len(self.archive) > 0 and np.random.rand() < 0.2: #Archive based mutation\n            idx_archive = np.random.randint(0, len(self.archive))\n            archive_member = self.archive[idx_archive]\n\n            idxs = np.random.choice(len(pop), 2, replace=False)\n            a, b = pop[idxs]\n\n            mutant = archive_member + self.F * (a - b)\n            return np.clip(mutant, -5, 5)\n        else: #Regular DE mutation\n            idxs = np.random.choice(len(pop), 3, replace=False)\n            a, b, c = pop[idxs]\n            mutant = a + self.F * (b - c)\n            return np.clip(mutant, -5, 5)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n    \n    def orthogonal_learning(self, func, x):\n        # Create orthogonal array\n        orthogonal_array = np.array([[-1, -1], [-1, 1], [1, -1], [1, 1]])\n\n        # Sample around the current solution x\n        samples = []\n        for row in orthogonal_array:\n            new_x = x.copy()\n            for i in range(self.dim):\n                delta = 0.1 * (func.bounds.ub - func.bounds.lb)\n                new_x[i] = x[i] + row[i % 2] * delta\n                new_x[i] = np.clip(new_x[i], func.bounds.lb, func.bounds.ub)\n            samples.append(new_x)\n\n        # Evaluate the samples\n        fitness_values = [func(sample) for sample in samples]\n        self.budget -= len(samples)\n        \n        # Select the best sample\n        best_index = np.argmin(fitness_values)\n        best_sample = samples[best_index]\n        best_fitness = fitness_values[best_index]\n        \n        return best_sample, best_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                target = self.population[i]\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, target)\n                \n                trial, f_trial = self.orthogonal_learning(func, trial)\n\n                if f_trial < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.stagnation_counter = 0 # Reset stagnation counter\n\n                         # Update archive\n                        if len(self.archive) < self.archive_size:\n                            self.archive.append(trial)\n                            self.archive_fitness.append(f_trial)\n                        else:\n                            max_archive_fitness_index = np.argmax(self.archive_fitness)\n                            if f_trial < self.archive_fitness[max_archive_fitness_index]:\n                                self.archive[max_archive_fitness_index] = trial\n                                self.archive_fitness[max_archive_fitness_index] = f_trial\n\n                if self.budget <= 0:\n                    break\n\n            # Adaptive parameter adjustment based on population diversity\n            diversity = np.std(self.population)\n            if diversity > self.diversity_threshold:\n                self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n                self.CR = np.clip(np.random.normal(0.7, 0.1), 0.1, 1.0)\n            else:\n                 self.F = np.clip(np.random.normal(0.8, 0.2), 0.1, 1.0)\n                 self.CR = np.clip(np.random.normal(0.4, 0.1), 0.1, 1.0)\n            \n            self.best_fitness_history.append(self.f_opt)\n\n            # Stagnation detection\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if np.std(self.best_fitness_history[-self.stagnation_threshold:]) < 1e-6:  #Stagnation detected\n                    self.stagnation_counter += 1\n                    if self.stagnation_counter >= 2: # restart after consecutive stagnation\n                        # Restart population\n                        lb = func.bounds.lb\n                        ub = func.bounds.ub\n                        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n                        self.fitness = np.array([func(x) for x in self.population])\n                        self.budget -= self.pop_size  # reduce budget\n                        self.f_opt = np.min(self.fitness)\n                        self.x_opt = self.population[np.argmin(self.fitness)]\n                        self.stagnation_counter = 0\n                        self.best_fitness_history = [self.f_opt]\n                        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "An exception occurred: setting an array element with a sequence..", "error": "", "parent_ids": ["0c5ba51b-9ca7-468e-89b0-7e951ab578d5"], "operator": null, "metadata": {}}
{"id": "31762087-86c4-40f3-a7d9-1fc5da6c3a59", "fitness": -Infinity, "name": "AdaptiveDEEnsemble", "description": "An Adaptive Differential Evolution with a dynamic ensemble of mutation strategies, self-adaptive parameters, and a local search operator triggered by stagnation.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEEnsemble:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_initial=0.5, CR_initial=0.9, stagnation_threshold=50, local_search_iterations=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_initial\n        self.CR = CR_initial\n        self.stagnation_threshold = stagnation_threshold\n        self.local_search_iterations = local_search_iterations\n        self.stagnation_counter = 0\n        self.last_best_fitness = np.inf\n        self.mutation_strategies = [\"rand/1\", \"best/1\", \"current-to-rand/1\", \"current-to-best/1\"]\n        self.strategy_probabilities = np.ones(len(self.mutation_strategies)) / len(self.mutation_strategies)\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        if np.min(self.fitness) < self.f_opt:\n            self.f_opt = np.min(self.fitness)\n            self.x_opt = self.population[np.argmin(self.fitness)]\n            self.last_best_fitness = self.f_opt\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Strategy Selection\n                strategy = np.random.choice(self.mutation_strategies, p=self.strategy_probabilities)\n\n                # Mutation\n                if strategy == \"rand/1\":\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    x_mutated = x_r1 + self.F * (x_r2 - x_r3)\n                elif strategy == \"best/1\":\n                    x_best = self.population[np.argmin(self.fitness)]\n                    indices = np.random.choice(self.pop_size, 2, replace=False)\n                    x_r1, x_r2 = self.population[indices]\n                    x_mutated = x_best + self.F * (x_r1 - x_r2)\n                elif strategy == \"current-to-rand/1\":\n                    indices = np.random.choice(self.pop_size, 2, replace=False)\n                    x_r1, x_r2 = self.population[indices]\n                    x_mutated = self.population[i] + self.F * (x_r1 - x_r2)\n                elif strategy == \"current-to-best/1\":\n                    x_best = self.population[np.argmin(self.fitness)]\n                    x_mutated = self.population[i] + self.F * (x_best - self.population[i]) + self.F * (self.population[np.random.choice(self.pop_size)] - self.population[np.random.choice(self.pop_size)])\n                else:\n                    raise ValueError(f\"Unknown mutation strategy: {strategy}\")\n\n                # Crossover\n                x_trial = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = x_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n                        self.last_best_fitness = self.f_opt\n                        self.stagnation_counter = 0  # Reset stagnation counter\n                \n\n                if self.budget <= 0:\n                    break\n\n            #Stagnation Check and Parameter Adaptation\n            if abs(self.last_best_fitness - self.f_opt) < 1e-6:\n                self.stagnation_counter +=1\n            else:\n                self.stagnation_counter = 0\n                self.last_best_fitness = self.f_opt\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                # Local Search around the best solution\n                best_index = np.argmin(self.fitness)\n                x_local_search = self.population[best_index].copy()\n                bounds = [(func.bounds.lb, func.bounds.ub) for _ in range(self.dim)]\n                result = minimize(func, x_local_search, method='L-BFGS-B', bounds=bounds, options={'maxiter': self.local_search_iterations})\n\n                if result.fun < self.f_opt:\n                    self.f_opt = result.fun\n                    self.x_opt = result.x\n                    self.population[best_index] = result.x\n                    self.fitness[best_index] = result.fun\n                    self.last_best_fitness = self.f_opt\n                \n                self.F = np.clip(self.F * 0.8, 0.1, 0.9)\n                self.CR = np.clip(self.CR * 0.8, 0.1, 0.9)\n                self.stagnation_counter = 0\n            else:\n                self.F = np.clip(self.F * 1.02, 0.1, 0.9)\n                self.CR = np.clip(self.CR * 1.02, 0.1, 0.9)\n\n            # Strategy Adaptation (Simplified - based on recent success)\n            successful_strategies = []\n            for i in range(self.pop_size):\n                if self.fitness[i] < self.last_best_fitness:\n                    successful_strategies.append(i) #Not used here, but for more complex adaptations it might be beneficial to store the strategy used.\n\n            self.strategy_probabilities = np.ones(len(self.mutation_strategies)) / len(self.mutation_strategies) #Reset to uniform. More complex reward/penalty schemes could be used.\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "An exception occurred: name 'minimize' is not defined.", "error": "", "parent_ids": ["7d2c9802-aedd-4867-ba93-580746df75c0"], "operator": null, "metadata": {}}
{"id": "5e13bcb4-d934-4a59-9e24-f2d5e1e6c275", "fitness": -Infinity, "name": "AdaptiveDEODRestart", "description": "Adaptive Differential Evolution with orthogonal design for crossover, L-BFGS-B local search on stagnation, and dynamic F/CR adaptation with a restart mechanism.", "code": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats import norm\n\nclass AdaptiveDEODRestart:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, stagnation_threshold=500, local_search_iterations=5, od_matrix_size=7):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.population = None\n        self.fitness = None\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.local_search_iterations = local_search_iterations\n        self.od_matrix_size = min(od_matrix_size, dim)  # Ensure OD matrix size doesn't exceed dimension\n        self.levy_exponent = 1.5\n\n    def initialize_population(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.best_fitness_history.append(self.f_opt)\n\n    def mutate(self, pop, i):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n        mutant = a + self.F * (b - c)\n        return np.clip(mutant, -5, 5)\n\n    def levy_flight(self, shape, beta=1.5):\n        \"\"\"Performs Levy flight.\"\"\"\n        num = norm.rvs(0, scale=1, size=shape)\n        den = abs(norm.rvs(0, scale=1, size=shape))**(1/beta)\n        step = num / den\n        return step\n\n    def crossover(self, mutant, target):\n        # Orthogonal Design Crossover\n        if self.od_matrix_size > 1:\n            od_matrix = self.generate_orthogonal_design(self.od_matrix_size)\n            trial = target.copy()\n            num_factors = self.od_matrix_size\n            selected_indices = np.random.choice(self.dim, num_factors, replace=False) # Use only a subset of dimensions\n            for j in range(num_factors):\n                if od_matrix[np.random.randint(0, len(od_matrix))][j] == 1:\n                    trial[selected_indices[j]] = mutant[selected_indices[j]]\n            return trial\n\n        # Standard Crossover if od_matrix_size is small\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def generate_orthogonal_design(self, n):\n        # Generate a simple orthogonal design matrix (Hadamard matrix)\n        if n == 2:\n            return [[0, 0], [0, 1], [1, 0], [1, 1]]\n        if n == 3:\n            return [[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 0]]\n        if n == 4:\n            return [[0, 0, 0, 0], [0, 0, 1, 1], [0, 1, 0, 1], [0, 1, 1, 0],\n                    [1, 0, 0, 1], [1, 0, 1, 0], [1, 1, 0, 0], [1, 1, 1, 1]]\n        if n == 7:\n            return [[0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 1, 1, 1], [0, 1, 1, 0, 0, 1, 1], [0, 1, 1, 1, 1, 0, 0], [1, 0, 1, 0, 1, 0, 1], [1, 0, 1, 1, 0, 1, 0], [1, 1, 0, 0, 1, 1, 0], [1, 1, 0, 1, 0, 0, 1]]\n        # Add more if needed, otherwise revert to random crossover\n\n        return np.random.randint(0, 2, size=(n, n)).tolist()\n\n    def local_search(self, func, x0):\n        bounds = [(-5, 5)] * self.dim\n        result = minimize(func, x0, method='L-BFGS-B', bounds=bounds, options={'maxiter': self.local_search_iterations})\n        return result.fun, result.x\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                target = self.population[i]\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, target)\n\n                # Add Levy flight perturbation with small probability\n                if np.random.rand() < 0.05:\n                    levy_step = self.levy_flight(self.dim)\n                    trial += 0.01 * levy_step  # Scale down for continuous space\n                    trial = np.clip(trial, -5, 5)  # Clip to bounds\n\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.stagnation_counter = 0\n                if self.budget <= 0:\n                    break\n\n            self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n            self.CR = np.clip(np.random.normal(0.7, 0.1), 0.1, 1.0)\n            self.best_fitness_history.append(self.f_opt)\n\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if np.std(self.best_fitness_history[-self.stagnation_threshold:]) < 1e-6:\n                    self.stagnation_counter += 1\n                    if self.stagnation_counter >= 2:\n                        # Perform Local Search on best solution\n                        f_local, x_local = self.local_search(func, self.x_opt)\n                        self.budget -= self.local_search_iterations # approximate local search cost\n                        if f_local < self.f_opt:\n                            self.f_opt = f_local\n                            self.x_opt = x_local\n\n                        # Restart Population with Levy Flight around the best\n                        lb = func.bounds.lb\n                        ub = func.bounds.ub\n                        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n                        levy_steps = self.levy_flight((self.pop_size, self.dim))\n                        self.population = self.x_opt + 0.1 * levy_steps  # Levy flight around best solution\n                        self.population = np.clip(self.population, lb, ub)\n\n                        self.fitness = np.array([func(x) for x in self.population])\n                        self.budget -= self.pop_size\n                        self.f_opt = np.min(self.fitness)\n                        self.x_opt = self.population[np.argmin(self.fitness)]\n                        self.stagnation_counter = 0\n                        self.best_fitness_history = [self.f_opt]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "An exception occurred: name 'norm' is not defined.", "error": "", "parent_ids": ["ff87f24e-b78e-4729-92e7-3bff4e547044"], "operator": null, "metadata": {}}
{"id": "38d791c2-e5f0-40fd-9457-74213862fb80", "fitness": -Infinity, "name": "AdaptiveDEOrthogonal", "description": "Adaptive Differential Evolution with orthogonal design for crossover and a self-adaptive population size.", "code": "import numpy as np\n\nclass AdaptiveDEOrthogonal:\n    def __init__(self, budget=10000, dim=10, pop_size_init=50, F=0.5, CR=0.7, stagnation_threshold=500, diversity_threshold=0.01, pop_size_adapt_freq=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size_init\n        self.pop_size_init = pop_size_init\n        self.F = F  # Mutation factor\n        self.CR = CR # Crossover rate\n        self.population = None\n        self.fitness = None\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.diversity_threshold = diversity_threshold\n        self.pop_size_adapt_freq = pop_size_adapt_freq\n        self.generation = 0\n\n    def initialize_population(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size # reduce budget by initial evaluation\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.best_fitness_history.append(self.f_opt)\n\n    def mutate(self, pop, i):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n        mutant = a + self.F * (b - c)\n        return np.clip(mutant, -5, 5)\n\n    def orthogonal_crossover(self, mutant, target):\n        # Simplified orthogonal design (L4 array)\n        design = np.array([\n            [1, 1],\n            [1, 2],\n            [2, 1],\n            [2, 2]\n        ])\n\n        group_size = self.dim // 2\n        if group_size == 0:\n            return mutant # failsafe\n\n        trial = target.copy()\n        for i in range(group_size):\n            group_indices = [2*i, 2*i+1]\n            \n            best_fitness = np.inf\n            best_trial_group = None\n\n            for row in design:\n                trial_group = trial.copy()\n                for j, index in enumerate(group_indices):\n                    if row[j] == 2:\n                        trial_group[index] = mutant[index]\n\n                fitness = self.evaluate_trial_group(trial_group, target, group_indices) # only evaluates the change, saves budget\n\n                if fitness < best_fitness:\n                    best_fitness = fitness\n                    best_trial_group = trial_group[group_indices]\n            \n            trial[group_indices] = best_trial_group\n\n        remaining_indices = list(range(group_size*2, self.dim))\n        if remaining_indices:\n            cross_points = np.random.rand(len(remaining_indices)) < self.CR\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, len(remaining_indices))] = True\n            for k, idx in enumerate(remaining_indices):\n              if cross_points[k]:\n                  trial[idx] = mutant[idx]\n\n        return trial\n    \n    def evaluate_trial_group(self, trial, target, group_indices):\n      # this saves budget by only evaluating the change\n      trial_full = target.copy()\n      trial_full[group_indices] = trial[group_indices]\n      if self.budget > 0:\n        f_trial = func(trial_full)\n        self.budget -=1\n        return f_trial\n      else:\n        return np.inf\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                target = self.population[i]\n                mutant = self.mutate(self.population, i)\n                trial = self.orthogonal_crossover(mutant, target)\n                \n                # Trial is directly updated in orthogonal_crossover!\n                f_trial = func(trial) # full evaluation to compare against population\n\n                if f_trial < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.stagnation_counter = 0 # Reset stagnation counter\n\n                if self.budget <= 0:\n                    break\n\n            # Adaptive parameter adjustment based on population diversity\n            diversity = np.std(self.population)\n            if diversity > self.diversity_threshold:\n                self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n                self.CR = np.clip(np.random.normal(0.7, 0.1), 0.1, 1.0)\n            else:\n                 self.F = np.clip(np.random.normal(0.8, 0.2), 0.1, 1.0)\n                 self.CR = np.clip(np.random.normal(0.4, 0.1), 0.1, 1.0)\n            \n            self.best_fitness_history.append(self.f_opt)\n            self.generation += 1\n            # Adaptive Population Size Adjustment\n            if self.generation % self.pop_size_adapt_freq == 0:\n                if diversity > self.diversity_threshold:\n                    self.pop_size = int(min(self.pop_size * 1.1, 2 * self.pop_size_init))  # Increase pop size\n                else:\n                    self.pop_size = int(max(self.pop_size * 0.9, self.pop_size_init // 2))  # Decrease pop size\n                \n                # Resample population if pop size changes\n                old_pop = self.population\n                old_fitness = self.fitness\n                \n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.budget -= self.pop_size\n                \n                # Keep the best individuals from the previous generation\n                num_keep = min(self.pop_size, len(old_pop))\n                idx = np.argsort(old_fitness)[:num_keep]\n                self.population[:num_keep] = old_pop[idx]\n                self.fitness[:num_keep] = old_fitness[idx]\n                \n                self.f_opt = np.min(self.fitness)\n                self.x_opt = self.population[np.argmin(self.fitness)]\n            \n            # Stagnation detection\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if np.std(self.best_fitness_history[-self.stagnation_threshold:]) < 1e-6:  #Stagnation detected\n                    self.stagnation_counter += 1\n                    if self.stagnation_counter >= 2: # restart after consecutive stagnation\n                        # Restart population\n                        lb = func.bounds.lb\n                        ub = func.bounds.ub\n                        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n                        self.fitness = np.array([func(x) for x in self.population])\n                        self.budget -= self.pop_size  # reduce budget\n                        self.f_opt = np.min(self.fitness)\n                        self.x_opt = self.population[np.argmin(self.fitness)]\n                        self.stagnation_counter = 0\n                        self.best_fitness_history = [self.f_opt]\n                        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "An exception occurred: name 'func' is not defined.", "error": "", "parent_ids": ["0c5ba51b-9ca7-468e-89b0-7e951ab578d5"], "operator": null, "metadata": {}}
{"id": "4436b967-83a5-4196-bc52-55c1112af8cb", "fitness": 0.2627839398221193, "name": "AdaptiveSOMA", "description": "Self-Organizing Migrating Algorithm (SOMA) with adaptive step size control and a dynamic population topology to promote exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveSOMA:\n    def __init__(self, budget=10000, dim=10, pop_size=100, path_length=2.0, step_size=0.01, migration_interval=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.path_length = path_length  # Maximum distance a migrant can move\n        self.step_size = step_size      # Discretization of the path\n        self.migration_interval = migration_interval\n        self.population = None\n        self.fitness = None\n        self.leader_index = None\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.topology = None  # Ring or Fully Connected\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.leader_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.leader_index]\n        self.x_opt = self.population[self.leader_index]\n        self.topology = np.random.choice([\"ring\", \"fully_connected\"])\n\n\n    def migrate(self, func):\n        for i in range(self.pop_size):\n            if i == self.leader_index:\n                continue\n\n            migrant = np.copy(self.population[i])\n            best_before = self.fitness[i]\n\n            # Select a \"leader\" based on topology\n            if self.topology == \"ring\":\n                leader_index = (i + 1) % self.pop_size\n            else: # fully_connected\n                leader_index = self.leader_index # or random\n\n            leader = self.population[leader_index]\n\n\n            direction_vector = leader - migrant\n            num_steps = int(self.path_length / self.step_size)\n\n            for step in range(1, num_steps + 1):\n                new_position = migrant + direction_vector * self.step_size * step\n                new_position = np.clip(new_position, self.lb, self.ub)\n\n                f_new = func(new_position)\n                self.budget -= 1\n\n                if f_new < best_before:\n                    best_before = f_new\n                    migrant = new_position\n\n\n                if self.budget <= 0:\n                    break\n\n            self.population[i] = migrant\n            self.fitness[i] = best_before\n\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.population[i]\n                self.leader_index = i\n\n            if self.budget <= 0:\n                break\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        generation = 0\n\n        while self.budget > 0:\n            generation += 1\n            self.migrate(func)\n\n            # Adaptive Step Size: Decrease when close to optimum\n            if generation % self.migration_interval == 0:\n                if np.std(self.fitness) < 0.01:  # Convergence detected\n                    self.step_size *= 0.9\n                    self.step_size = max(self.step_size, 0.0001) # Avoid zero step size\n                    self.topology = np.random.choice([\"ring\", \"fully_connected\"]) # Change topology\n\n\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveSOMA scored 0.263 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["714f5175-076b-443b-ac5f-ee2c00d0453e"], "operator": null, "metadata": {"aucs": [0.08246519475791825, 0.1714441262235734, 0.31986739897761574, 0.18288808355136832, 0.2575537228571727, 0.2265413547859283, 0.22713280304168837, 0.16821414618122976, 0.1595634351454699, 0.15643204153833612, 0.17498431038000706, 0.9993123016381952, 0.26939726284674215, 0.18846776762646522, 0.37654201025639467, 0.24943923569364557, 0.26790212520059, 0.19537513154949115, 0.16282572774080706, 0.41933061644974756]}}
{"id": "a6a719df-6472-4e24-8ff2-001551542349", "fitness": 0.6184547134900635, "name": "AdaptiveDERestart", "description": "Adaptive Differential Evolution with simplified parameter adaptation based on success history and periodic population resets to escape local optima.", "code": "import numpy as np\n\nclass AdaptiveDERestart:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, restart_interval=500):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR # Crossover rate\n        self.population = None\n        self.fitness = None\n        self.restart_interval = restart_interval\n        self.generation = 0\n        self.success_history_F = []\n        self.success_history_CR = []\n\n    def initialize_population(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size # reduce budget by initial evaluation\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n    def mutate(self, pop, i):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n        mutant = a + self.F * (b - c)\n        return np.clip(mutant, -5, 5)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                target = self.population[i]\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, target)\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.success_history_F.append(self.F)\n                    self.success_history_CR.append(self.CR)\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n\n            # Adaptive parameter adjustment based on success history\n            if self.success_history_F:\n                self.F = np.clip(np.mean(self.success_history_F), 0.1, 0.9)\n                self.CR = np.clip(np.mean(self.success_history_CR), 0.1, 0.9)\n                self.success_history_F = []\n                self.success_history_CR = []\n                \n\n            self.generation += 1\n            if self.generation % self.restart_interval == 0:\n                # Restart population periodically\n                lb = func.bounds.lb\n                ub = func.bounds.ub\n                self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.budget -= self.pop_size  # reduce budget by initial evaluation\n                self.f_opt = np.min(self.fitness)\n                self.x_opt = self.population[np.argmin(self.fitness)]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDERestart scored 0.618 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0c5ba51b-9ca7-468e-89b0-7e951ab578d5"], "operator": null, "metadata": {"aucs": [0.22185413028936507, 0.42586314355979826, 0.6192633948606726, 0.8578500095852345, 0.668691904189962, 0.7574199248973159, 0.5724166608609906, 0.5609413018959241, 0.6714486036708431, 0.5972657420344676, 0.7623640822706129, 0.9920641829434715, 0.34068307230466377, 0.634871426253325, 0.8345838104686902, 0.7661714483685583, 0.5303550515041437, 0.8006126971408368, 0.23983274362310503, 0.51454093907929]}}
{"id": "5cebcf71-2e5e-4dfc-9537-662b5bb5e531", "fitness": 0.3911320959098138, "name": "SelfOrganizingDE", "description": "Differential Evolution with a self-organizing search strategy that dynamically adjusts mutation and crossover rates based on local search landscape characteristics and individual performance.", "code": "import numpy as np\n\nclass SelfOrganizingDE:\n    def __init__(self, budget=10000, dim=10, pop_size=100, F_base=0.5, CR_base=0.7, learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_base = F_base  # Base mutation factor\n        self.CR_base = CR_base # Base Crossover rate\n        self.learning_rate = learning_rate\n        self.population = None\n        self.fitness = None\n        self.F = None # Individual mutation factors\n        self.CR = None # Individual crossover rates\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n\n        self.F = np.full(self.pop_size, self.F_base)\n        self.CR = np.full(self.pop_size, self.CR_base)\n\n    def mutate(self, pop, i):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n        mutant = a + self.F[i] * (b - c)\n        return np.clip(mutant, self.lb, self.ub)\n\n    def crossover(self, mutant, target, i):\n        cross_points = np.random.rand(self.dim) < self.CR[i]\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            new_population = np.copy(self.population)\n            new_fitness = np.copy(self.fitness)\n\n            for i in range(self.pop_size):\n                target = self.population[i]\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, target, i)\n\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n\n                    # Adjust F and CR based on success\n                    self.F[i] = np.clip(self.F[i] * (1 + self.learning_rate), 0.1, 1.0)\n                    self.CR[i] = np.clip(self.CR[i] * (1 + self.learning_rate), 0.1, 1.0)\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                else:\n                    # Adjust F and CR based on failure\n                    self.F[i] = np.clip(self.F[i] * (1 - self.learning_rate), 0.1, 1.0)\n                    self.CR[i] = np.clip(self.CR[i] * (1 - self.learning_rate), 0.1, 1.0)\n\n\n                if self.budget <= 0:\n                    break\n\n            self.population = new_population\n            self.fitness = new_fitness\n\n            # Periodically re-initialize poorly performing individuals.\n            worst_indices = np.argsort(self.fitness)[-self.pop_size // 10:]\n            for idx in worst_indices:\n                self.population[idx] = np.random.uniform(self.lb, self.ub, size=self.dim)\n                self.fitness[idx] = func(self.population[idx])\n                self.budget -=1\n                if self.fitness[idx] < self.f_opt:\n                    self.f_opt = self.fitness[idx]\n                    self.x_opt = self.population[idx]\n                self.F[idx] = self.F_base\n                self.CR[idx] = self.CR_base\n                if self.budget <= 0:\n                    break\n\n            if self.budget <=0:\n                break\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm SelfOrganizingDE scored 0.391 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["714f5175-076b-443b-ac5f-ee2c00d0453e"], "operator": null, "metadata": {"aucs": [0.17301879017789357, 0.2366171719056459, 0.3760097588364196, 0.35740565515240397, 0.3152563023106568, 0.44166858112873075, 0.30981234249436085, 0.3409897973631769, 0.3364378763292809, 0.21779784247061873, 0.36199967341976136, 0.9973719480914224, 0.2644567873312057, 0.33307919596728497, 0.7045424630405202, 0.47802685370283093, 0.3292115218555943, 0.5376443743073321, 0.22464191140237, 0.48665307090876686]}}
{"id": "faf95cb3-ddcc-4af3-b7f9-6bee34392208", "fitness": 0.6259694499762334, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with probabilistic mutation operator selection, archive for past solutions, and dynamic parameter control using success history to balance exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=20, F_initial=0.5, CR_initial=0.9, age_limit=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F_initial\n        self.CR = CR_initial\n        self.age_limit = age_limit\n        self.ages = np.zeros(pop_size)\n        self.stagnation_counter = 0\n        self.last_best_fitness = np.inf\n        self.archive = []  # Archive for storing past solutions and their fitnesses\n        self.archive_fitness = []\n        self.mutation_probs = np.array([0.5, 0.5])  # Probabilities for \"rand/1\" and \"best/1\"\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        if np.min(self.fitness) < self.f_opt:\n            self.f_opt = np.min(self.fitness)\n            self.x_opt = self.population[np.argmin(self.fitness)]\n            self.last_best_fitness = self.f_opt\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation strategy selection (probabilistic)\n                mutation_strategy = np.random.choice([\"rand/1\", \"best/1\"], p=self.mutation_probs)\n\n                if mutation_strategy == \"rand/1\":\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    x_mutated = x_r1 + self.F * (x_r2 - x_r3)\n                else:  # \"best/1\"\n                    x_best = self.population[np.argmin(self.fitness)]\n                    indices = np.random.choice(self.pop_size, 2, replace=False)\n                    x_r1, x_r2 = self.population[indices]\n                    x_mutated = x_best + self.F * (x_r1 - x_r2)\n\n                # Crossover\n                x_trial = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    # Update mutation strategy success\n                    if mutation_strategy == \"rand/1\":\n                        self.mutation_probs[0] *= 1.05  # Increase probability\n                        self.mutation_probs[1] *= 0.95\n                    else:\n                        self.mutation_probs[1] *= 1.05\n                        self.mutation_probs[0] *= 0.95\n                    self.mutation_probs /= np.sum(self.mutation_probs)  # Normalize\n\n                    self.fitness[i] = f_trial\n                    self.population[i] = x_trial\n                    self.ages[i] = 0\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n                        self.last_best_fitness = self.f_opt\n                        self.stagnation_counter = 0\n\n                    # Add to archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(x_trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        # Replace the worst individual in the archive\n                        worst_index = np.argmax(self.archive_fitness)\n                        if f_trial < self.archive_fitness[worst_index]:\n                            self.archive[worst_index] = x_trial\n                            self.archive_fitness[worst_index] = f_trial\n                else:\n                    self.ages[i] += 1\n\n                if self.ages[i] > self.age_limit:\n                    self.population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                    self.fitness[i] = func(self.population[i])\n                    self.ages[i] = 0\n                    self.budget -= 1\n                    if self.fitness[i] < self.f_opt:\n                        self.f_opt = self.fitness[i]\n                        self.x_opt = self.population[i]\n                        self.last_best_fitness = self.f_opt\n                        self.stagnation_counter = 0\n\n                if self.budget <= 0:\n                    break\n            \n            #Stagnation Check and Parameter Adaptation\n            if abs(self.last_best_fitness - self.f_opt) < 1e-6:\n                self.stagnation_counter +=1\n            else:\n                self.stagnation_counter = 0\n                self.last_best_fitness = self.f_opt\n                \n            if self.stagnation_counter > 50:\n                self.F = np.clip(self.F * 1.1, 0.1, 0.9)\n                self.CR = np.clip(self.CR * 0.9, 0.1, 0.9)\n                self.stagnation_counter = 0\n            else:\n                 self.F = np.clip(self.F * 0.99, 0.1, 0.9)\n                 self.CR = np.clip(self.CR * 1.01, 0.1, 0.9)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDE scored 0.626 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["7d2c9802-aedd-4867-ba93-580746df75c0"], "operator": null, "metadata": {"aucs": [0.21177202897358183, 0.6139608851186918, 0.5813501237114397, 0.9646398955333396, 0.3128008944294062, 0.9387770360937645, 0.2900404400192488, 0.3302609427982357, 0.9439098948910072, 0.2648850800090303, 0.9650184834981088, 0.9989800196015014, 0.5778339258115723, 0.39266209929720786, 0.9418389881181434, 0.9416185183995482, 0.4159798270842422, 0.9632995569449515, 0.38159657465920005, 0.48816378453244547]}}
{"id": "41b12c9d-5481-49de-b79f-a3aa45b9275e", "fitness": -Infinity, "name": "AdaptiveDEShrinkOL", "description": "Adaptive Differential Evolution with shrinking population, success-history based parameter adaptation, and orthogonal learning to enhance exploration.", "code": "import numpy as np\n\nclass AdaptiveDEShrinkOL:\n    def __init__(self, budget=10000, dim=10, pop_size=100, F=0.5, CR=0.7, shrink_factor=0.9, ol_factor=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.initial_pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR # Crossover rate\n        self.population = None\n        self.fitness = None\n        self.shrink_factor = shrink_factor\n        self.ol_factor = ol_factor\n        self.success_history_F = []\n        self.success_history_CR = []\n        self.min_pop_size = 10\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n\n    def mutate(self, pop, i):\n        # Focused sampling around best\n        idxs = np.random.choice(len(pop), 2, replace=False)\n        a, b = pop[idxs]\n        mutant = self.x_opt + self.F * (a - b)\n        return np.clip(mutant, self.lb, self.ub)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def shrink_population(self):\n        if self.pop_size > self.min_pop_size:\n            self.pop_size = max(self.min_pop_size, int(self.pop_size * self.shrink_factor))\n            # Keep the best individuals\n            sorted_indices = np.argsort(self.fitness)\n            self.population = self.population[sorted_indices[:self.pop_size]]\n            self.fitness = self.fitness[sorted_indices[:self.pop_size]]\n\n    def orthogonal_learning(self, func):\n        # Perform orthogonal learning on a subset of the population\n        num_ol_samples = min(self.pop_size, int(self.ol_factor * self.initial_pop_size))  #dynamic number of samples\n        indices = np.random.choice(self.pop_size, num_ol_samples, replace=False)\n\n        for i in indices:\n            # Generate orthogonal array (simplified 2-level full factorial design)\n            oa = np.array([[-1, -1], [1, -1], [-1, 1], [1, 1]])\n\n            # Map OA to the variable range around the current individual\n            level_low = self.population[i] - 0.1 * (self.ub - self.lb)  # Smaller range\n            level_high = self.population[i] + 0.1 * (self.ub - self.lb)  # Smaller range\n            level_low = np.clip(level_low, self.lb, self.ub)\n            level_high = np.clip(level_high, self.lb, self.ub)\n\n            best_fitness_ol = self.fitness[i]\n            best_sample_ol = self.population[i]\n\n            for j in range(len(oa)):\n                sample = level_low + 0.5 * (oa[j] + 1) * (level_high - level_low)\n                sample = np.clip(sample, self.lb, self.ub)\n                fitness = func(sample)\n                self.budget -= 1\n\n                if fitness < best_fitness_ol:\n                    best_fitness_ol = fitness\n                    best_sample_ol = sample\n\n                if fitness < self.f_opt:\n                    self.f_opt = fitness\n                    self.x_opt = sample\n\n                if self.budget <= 0:\n                    break\n            if self.budget <=0:\n                break\n\n            # Replace the individual if a better solution is found\n            if best_fitness_ol < self.fitness[i]:\n                self.population[i] = best_sample_ol\n                self.fitness[i] = best_fitness_ol\n\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(self.population)\n            new_fitness = np.copy(self.fitness)\n\n            successful_F = []\n            successful_CR = []\n\n            for i in range(self.pop_size):\n                target = self.population[i]\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, target)\n\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                    successful_F.append(self.F)\n                    successful_CR.append(self.CR)\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n\n            self.population = new_population\n            self.fitness = new_fitness\n\n            # Adaptive F and CR based on successful values\n            if successful_F:\n                self.F = np.clip(np.mean(successful_F), 0.1, 1.0)\n            if successful_CR:\n                self.CR = np.clip(np.mean(successful_CR), 0.1, 1.0)\n\n            # Population shrinking\n            if generation % 10 == 0:\n                self.shrink_population()\n\n            # Orthogonal learning\n            if generation % 5 == 0:\n                self.orthogonal_learning(func)\n\n            if self.budget <=0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "An exception occurred: operands could not be broadcast together with shapes (2,) (5,) .", "error": "", "parent_ids": ["714f5175-076b-443b-ac5f-ee2c00d0453e"], "operator": null, "metadata": {}}
{"id": "9989dc86-522f-4c98-be93-212ee87b3c5c", "fitness": -Infinity, "name": "AdaptiveDEShrinkLocalSearch", "description": "Adaptive Differential Evolution with shrinking population, success-history based parameter adaptation, and L-BFGS-B local search to refine promising solutions.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEShrinkLocalSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=100, F=0.5, CR=0.7, shrink_factor=0.9, local_search_freq=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.initial_pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR # Crossover rate\n        self.population = None\n        self.fitness = None\n        self.shrink_factor = shrink_factor\n        self.success_history_F = []\n        self.success_history_CR = []\n        self.min_pop_size = 10\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.local_search_freq = local_search_freq\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n\n    def mutate(self, pop, i):\n        # Focused sampling around best\n        idxs = np.random.choice(len(pop), 2, replace=False)\n        a, b = pop[idxs]\n        mutant = self.x_opt + self.F * (a - b)\n        return np.clip(mutant, self.lb, self.ub)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def shrink_population(self):\n        if self.pop_size > self.min_pop_size:\n            self.pop_size = max(self.min_pop_size, int(self.pop_size * self.shrink_factor))\n            # Keep the best individuals\n            sorted_indices = np.argsort(self.fitness)\n            self.population = self.population[sorted_indices[:self.pop_size]]\n            self.fitness = self.fitness[sorted_indices[:self.pop_size]]\n\n    def local_search(self, func, x):\n        bounds = [(self.lb, self.ub)] * self.dim\n        result = minimize(func, x, method='L-BFGS-B', bounds=bounds, options={'maxfun': min(50, self.budget)})  # Limit local search evaluations\n        self.budget -= result.nfev\n        return result.fun, result.x\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(self.population)\n            new_fitness = np.copy(self.fitness)\n\n            successful_F = []\n            successful_CR = []\n\n            for i in range(self.pop_size):\n                target = self.population[i]\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, target)\n\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                    successful_F.append(self.F)\n                    successful_CR.append(self.CR)\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n\n            self.population = new_population\n            self.fitness = new_fitness\n\n            # Adaptive F and CR based on successful values\n            if successful_F:\n                self.F = np.clip(np.mean(successful_F), 0.1, 1.0)\n            if successful_CR:\n                self.CR = np.clip(np.mean(successful_CR), 0.1, 1.0)\n\n            # Population shrinking\n            if generation % 10 == 0:\n                self.shrink_population()\n\n            # Local search application\n            if generation % self.local_search_freq == 0:\n                f_local, x_local = self.local_search(func, self.x_opt)\n                if f_local < self.f_opt:\n                    self.f_opt = f_local\n                    self.x_opt = x_local\n\n            if self.budget <=0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "An exception occurred: name 'minimize' is not defined.", "error": "", "parent_ids": ["714f5175-076b-443b-ac5f-ee2c00d0453e"], "operator": null, "metadata": {}}
{"id": "be5f69a3-c1bf-4b6b-b030-e9c40c9729ca", "fitness": 0.255716944755235, "name": "AdaptiveSOMA", "description": "Self-Organizing Migrating Algorithm (SOMA) with velocity clamping, adaptive path length and step size control based on success rate and population diversity, and dynamic leader selection.", "code": "import numpy as np\n\nclass AdaptiveSOMA:\n    def __init__(self, budget=10000, dim=10, pop_size=100, path_length=2.0, step_size=0.01, migration_interval=10, velocity_clamp=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.path_length = path_length  # Maximum distance a migrant can move\n        self.step_size = step_size      # Discretization of the path\n        self.migration_interval = migration_interval\n        self.velocity_clamp = velocity_clamp # Limit the maximum velocity\n        self.population = None\n        self.fitness = None\n        self.leader_index = None\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.success_rate = 0.0\n        self.migration_count = 0\n\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.leader_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.leader_index]\n        self.x_opt = self.population[self.leader_index]\n\n\n    def migrate(self, func):\n        successful_migrations = 0\n        for i in range(self.pop_size):\n            if i == self.leader_index:\n                continue\n\n            migrant = np.copy(self.population[i])\n            best_before = self.fitness[i]\n\n            # Select a \"leader\" - always the best\n            leader = self.population[self.leader_index]\n            direction_vector = leader - migrant\n\n            # Velocity clamping\n            direction_vector = np.clip(direction_vector, -self.velocity_clamp, self.velocity_clamp)\n            \n            num_steps = int(self.path_length / self.step_size)\n\n            for step in range(1, num_steps + 1):\n                new_position = migrant + direction_vector * self.step_size * step\n                new_position = np.clip(new_position, self.lb, self.ub)\n\n                f_new = func(new_position)\n                self.budget -= 1\n\n                if f_new < best_before:\n                    best_before = f_new\n                    migrant = new_position\n\n\n                if self.budget <= 0:\n                    break\n\n            self.population[i] = migrant\n            self.fitness[i] = best_before\n\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.population[i]\n                self.leader_index = i\n                \n            if self.fitness[i] < self.fitness[i]:\n                successful_migrations += 1\n\n            if self.budget <= 0:\n                break\n        \n        return successful_migrations\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        generation = 0\n\n        while self.budget > 0:\n            generation += 1\n            successful_migrations = self.migrate(func)\n            self.migration_count += 1\n            self.success_rate = successful_migrations / self.pop_size\n\n            # Adaptive Path Length and Step Size\n            if self.migration_count % self.migration_interval == 0:\n                if self.success_rate > 0.3:  # High success, increase exploitation\n                    self.path_length *= 0.95\n                    self.step_size *= 0.95\n                else:  # Low success, increase exploration\n                    self.path_length *= 1.05\n                    self.step_size *= 1.05\n\n                self.path_length = np.clip(self.path_length, 0.1, 3.0)\n                self.step_size = np.clip(self.step_size, 0.001, 0.1)\n\n                # Dynamic Leader Selection: Every few generations, re-evaluate the leader\n                if generation % (self.migration_interval * 5) == 0:\n                    self.leader_index = np.argmin(self.fitness)\n\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveSOMA scored 0.256 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["4436b967-83a5-4196-bc52-55c1112af8cb"], "operator": null, "metadata": {"aucs": [0.10445459396431889, 0.1699498429911377, 0.2733489325219668, 0.20756215945538237, 0.19323709227036712, 0.20377524960067184, 0.22214317931179806, 0.19572749957833557, 0.18649730157917244, 0.13903470516977923, 0.18334687997977328, 0.9964335947196588, 0.23034855682571942, 0.22944373551359354, 0.28577159337170877, 0.270346201405104, 0.22539328282067217, 0.2313107700907292, 0.13107909442326438, 0.43513462951154547]}}
{"id": "c1e2f570-6482-4a10-96d3-e7db664a4781", "fitness": 0.27652930441386847, "name": "AdaptiveSOMA", "description": "Self-Organizing Migrating Algorithm (SOMA) with velocity clamping, adaptive parameter adjustment based on fitness landscape ruggedness, and a combined ring-and-all topology for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveSOMA:\n    def __init__(self, budget=10000, dim=10, pop_size=100, path_length=2.0, step_size=0.01, migration_interval=10, ruggedness_window=20):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.path_length = path_length  # Maximum distance a migrant can move\n        self.step_size = step_size      # Discretization of the path\n        self.migration_interval = migration_interval\n        self.population = None\n        self.fitness = None\n        self.leader_index = None\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.topology = None  # Ring or Fully Connected or Combined\n        self.velocity_clamping = 0.5 # Maximum allowed velocity\n        self.ruggedness_window = ruggedness_window\n        self.fitness_history = []\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.leader_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.leader_index]\n        self.x_opt = self.population[self.leader_index]\n        self.topology = np.random.choice([\"ring\", \"fully_connected\", \"combined\"])\n\n\n    def migrate(self, func):\n        for i in range(self.pop_size):\n            if i == self.leader_index:\n                continue\n\n            migrant = np.copy(self.population[i])\n            best_before = self.fitness[i]\n\n            # Select a \"leader\" based on topology\n            if self.topology == \"ring\":\n                leader_index = (i + 1) % self.pop_size\n            elif self.topology == \"fully_connected\":\n                leader_index = self.leader_index # or random\n            else: # combined: random choice between ring and fully connected\n                if np.random.rand() < 0.5:\n                    leader_index = (i + 1) % self.pop_size\n                else:\n                    leader_index = self.leader_index\n\n            leader = self.population[leader_index]\n\n\n            direction_vector = leader - migrant\n            \n            # Velocity clamping\n            velocity_norm = np.linalg.norm(direction_vector)\n            if velocity_norm > self.velocity_clamping:\n                direction_vector = direction_vector / velocity_norm * self.velocity_clamping\n\n            num_steps = int(self.path_length / self.step_size)\n\n            for step in range(1, num_steps + 1):\n                new_position = migrant + direction_vector * self.step_size * step\n                new_position = np.clip(new_position, self.lb, self.ub)\n\n                f_new = func(new_position)\n                self.budget -= 1\n\n                if f_new < best_before:\n                    best_before = f_new\n                    migrant = new_position\n\n\n                if self.budget <= 0:\n                    break\n\n            self.population[i] = migrant\n            self.fitness[i] = best_before\n\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.population[i]\n                self.leader_index = i\n\n            if self.budget <= 0:\n                break\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        generation = 0\n\n        while self.budget > 0:\n            generation += 1\n            self.migrate(func)\n\n            # Adaptive Step Size: Decrease when close to optimum, increase when stuck\n            self.fitness_history.append(self.f_opt)\n            if len(self.fitness_history) > self.ruggedness_window:\n                self.fitness_history.pop(0)\n\n            if generation % self.migration_interval == 0:\n                fitness_std = np.std(self.fitness_history)\n                if fitness_std < 0.001:  # Stagnation detected\n                    self.step_size *= 1.1  # Increase step size\n                    self.step_size = min(self.step_size, 0.1)\n                    self.path_length *= 1.05\n                    self.path_length = min(self.path_length, 3.0)\n                elif np.std(self.fitness) < 0.01:  # Convergence detected\n                    self.step_size *= 0.9\n                    self.step_size = max(self.step_size, 0.0001) # Avoid zero step size\n                    self.path_length *= 0.95\n                    self.path_length = max(self.path_length, 0.1)\n\n\n                self.topology = np.random.choice([\"ring\", \"fully_connected\", \"combined\"]) # Change topology\n\n\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveSOMA scored 0.277 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["4436b967-83a5-4196-bc52-55c1112af8cb"], "operator": null, "metadata": {"aucs": [0.12381780179633828, 0.16009408738517672, 0.2681098415531017, 0.23969203101412007, 0.18004993440713202, 0.18032237755380265, 0.25767941768573477, 0.24464695794510538, 0.17329744946105807, 0.15311983083097236, 0.2877324081854714, 0.9970579754565541, 0.2549559716590466, 0.16578807165245013, 0.5322141239331787, 0.27038986768640405, 0.23924643188938322, 0.22765667864099692, 0.1381865744567875, 0.4365282550845554]}}
{"id": "9f314d8c-ae4b-45dc-a5d9-7be948c15c37", "fitness": 0.6054192365198723, "name": "DEwithLocalSearch", "description": "Differential Evolution with a local search operator triggered based on a probability proportional to individual fitness improvement.", "code": "import numpy as np\n\nclass DEwithLocalSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, local_search_prob=0.1, local_search_radius=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.local_search_prob = local_search_prob\n        self.local_search_radius = local_search_radius\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n    def initialize_population(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n    def mutate(self, pop, i):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n        mutant = a + self.F * (b - c)\n        return np.clip(mutant, -5, 5)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def local_search(self, x, func):\n        # Perform a simple random local search around x\n        best_x = x\n        best_f = func(x)\n        self.budget -= 1\n        if self.budget <= 0:\n            return best_x, best_f\n\n        for _ in range(5):  # Limited local search evaluations\n            new_x = x + np.random.uniform(-self.local_search_radius, self.local_search_radius, size=self.dim)\n            new_x = np.clip(new_x, -5, 5)\n            new_f = func(new_x)\n            self.budget -= 1\n            if self.budget <= 0:\n                return best_x, best_f\n\n            if new_f < best_f:\n                best_f = new_f\n                best_x = new_x\n        return best_x, best_f\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                target = self.population[i]\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, target)\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    # Local search trigger based on fitness improvement\n                    improvement = self.fitness[i] - f_trial\n                    probability = min(1.0, self.local_search_prob * improvement)  # Probability based on improvement\n                    if np.random.rand() < probability:\n                        trial, f_trial = self.local_search(trial, func)\n\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm DEwithLocalSearch scored 0.605 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a6a719df-6472-4e24-8ff2-001551542349"], "operator": null, "metadata": {"aucs": [0.21149233841586146, 0.36888914139653883, 0.6291182706379412, 0.7709302181806601, 0.6466829202181437, 0.7557234055180289, 0.537953925030777, 0.5718326677428811, 0.6266304664933475, 0.634642729025185, 0.7647791409400113, 0.9818057340004197, 0.3380087131748837, 0.6005544455026974, 0.8816838101775355, 0.7638481377152453, 0.5167432240307408, 0.7916134110410233, 0.21146296617753202, 0.5039890649779926]}}
{"id": "e6a4724f-c912-4547-8af1-7b14a7fdd504", "fitness": 0.5676253684822669, "name": "AdaptiveDERestart", "description": "Adaptive Differential Evolution with simplified parameter adaptation using a running average of successful parameters and a probability-based restart strategy.", "code": "import numpy as np\n\nclass AdaptiveDERestart:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, restart_prob=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR # Crossover rate\n        self.population = None\n        self.fitness = None\n        self.restart_prob = restart_prob\n        self.success_F = F\n        self.success_CR = CR\n\n    def initialize_population(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n    def mutate(self, pop, i):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n        mutant = a + self.F * (b - c)\n        return np.clip(mutant, -5, 5)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                target = self.population[i]\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, target)\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.success_F = 0.9 * self.success_F + 0.1 * self.F\n                    self.success_CR = 0.9 * self.success_CR + 0.1 * self.CR\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n\n            self.F = np.clip(self.success_F, 0.1, 0.9)\n            self.CR = np.clip(self.success_CR, 0.1, 0.9)\n\n            if np.random.rand() < self.restart_prob:\n                lb = func.bounds.lb\n                ub = func.bounds.ub\n                self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.budget -= self.pop_size\n                self.f_opt = np.min(self.fitness)\n                self.x_opt = self.population[np.argmin(self.fitness)]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDERestart scored 0.568 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a6a719df-6472-4e24-8ff2-001551542349"], "operator": null, "metadata": {"aucs": [0.19322319137883082, 0.3251997094388501, 0.43877160953443695, 0.8580330504883729, 0.7012469084789605, 0.7514393547396919, 0.5462706098628589, 0.5923924776698357, 0.5725537889414714, 0.4186793776389992, 0.7578258870078513, 0.9889658039602691, 0.27604942017668865, 0.601079606585273, 0.8008585484809053, 0.7437775294156387, 0.4879486603327312, 0.5721697042824945, 0.22679375867862062, 0.49922837255255603]}}
{"id": "cb480dba-9dad-44b2-9951-eff1e0598739", "fitness": 0.4236939775040992, "name": "SelfOrganizingDE", "description": "Simplified self-adaptive Differential Evolution with a success-history based parameter adaptation and periodic population updates.", "code": "import numpy as np\n\nclass SelfOrganizingDE:\n    def __init__(self, budget=10000, dim=10, pop_size=100, F_base=0.5, CR_base=0.7, learning_rate=0.1, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.learning_rate = learning_rate\n        self.archive_size = archive_size\n        self.population = None\n        self.fitness = None\n        self.F = None\n        self.CR = None\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.success_F = []\n        self.success_CR = []\n        self.archive = []\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n\n        self.F = np.full(self.pop_size, self.F_base)\n        self.CR = np.full(self.pop_size, self.CR_base)\n\n    def mutate(self, pop, i):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n        mutant = a + self.F[i] * (b - c)\n        return np.clip(mutant, self.lb, self.ub)\n\n    def crossover(self, mutant, target, i):\n        cross_points = np.random.rand(self.dim) < self.CR[i]\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                target = self.population[i]\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, target, i)\n\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    # Success\n                    self.success_F.append(self.F[i])\n                    self.success_CR.append(self.CR[i])\n\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n\n            # Update F and CR based on success history\n            if self.success_F:\n                self.F = np.clip(np.random.choice(self.success_F, size=self.pop_size), 0.1, 1.0)\n                self.CR = np.clip(np.random.choice(self.success_CR, size=self.pop_size), 0.1, 1.0)\n\n            # Clear success history\n            self.success_F = []\n            self.success_CR = []\n\n            # Periodically re-initialize poorly performing individuals.\n            if self.budget > 0:\n              worst_indices = np.argsort(self.fitness)[-self.pop_size // 10:]\n              for idx in worst_indices:\n                  self.population[idx] = np.random.uniform(self.lb, self.ub, size=self.dim)\n                  self.fitness[idx] = func(self.population[idx])\n                  self.budget -=1\n                  if self.fitness[idx] < self.f_opt:\n                      self.f_opt = self.fitness[idx]\n                      self.x_opt = self.population[idx]\n\n                  if self.budget <= 0:\n                      break\n            if self.budget <=0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm SelfOrganizingDE scored 0.424 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5cebcf71-2e5e-4dfc-9537-662b5bb5e531"], "operator": null, "metadata": {"aucs": [0.1667906985284322, 0.24815778264915567, 0.3832757755656774, 0.5912271457593272, 0.3670766311547342, 0.4692719892445615, 0.32287782239914453, 0.3650726119571289, 0.3881010779354247, 0.2799889702149688, 0.46589979903459555, 0.9888809668103176, 0.28989271057668253, 0.3772627212307422, 0.7337302272725892, 0.4837987217751757, 0.33115983619276546, 0.5455944613009375, 0.18617582259134835, 0.48964377788827484]}}
{"id": "a4367e2e-66ae-4dbe-93ba-12f6916e16be", "fitness": 0.42199341440931804, "name": "AdaptiveDERestartCauchy", "description": "Adaptive Differential Evolution with success-history based parameter adaptation, periodic population resets, and a Cauchy mutation operator to enhance exploration.", "code": "import numpy as np\n\nclass AdaptiveDERestartCauchy:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, restart_interval=500, cauchy_scale=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR # Crossover rate\n        self.population = None\n        self.fitness = None\n        self.restart_interval = restart_interval\n        self.generation = 0\n        self.success_history_F = []\n        self.success_history_CR = []\n        self.cauchy_scale = cauchy_scale # Scale parameter for Cauchy distribution\n\n    def initialize_population(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size # reduce budget by initial evaluation\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n    def mutate(self, pop, i):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n        # Cauchy mutation\n        mutant = a + self.F * (b - c) + self.cauchy_scale * np.random.standard_cauchy(size=self.dim)\n        return np.clip(mutant, -5, 5)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                target = self.population[i]\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, target)\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.success_history_F.append(self.F)\n                    self.success_history_CR.append(self.CR)\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n\n            # Adaptive parameter adjustment based on success history\n            if self.success_history_F:\n                self.F = np.clip(np.mean(self.success_history_F), 0.1, 0.9)\n                self.CR = np.clip(np.mean(self.success_history_CR), 0.1, 0.9)\n                self.success_history_F = []\n                self.success_history_CR = []\n                \n\n            self.generation += 1\n            if self.generation % self.restart_interval == 0:\n                # Restart population periodically\n                lb = func.bounds.lb\n                ub = func.bounds.ub\n                self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.budget -= self.pop_size  # reduce budget by initial evaluation\n                self.f_opt = np.min(self.fitness)\n                self.x_opt = self.population[np.argmin(self.fitness)]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDERestartCauchy scored 0.422 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a6a719df-6472-4e24-8ff2-001551542349"], "operator": null, "metadata": {"aucs": [0.18250175666958857, 0.3124850170439677, 0.406756688271222, 0.7086569528706749, 0.3361316765332354, 0.43070873011341104, 0.32516554884735727, 0.35765892897803053, 0.328075081159817, 0.3032046272057606, 0.4491267491024631, 1.0, 0.2792247011511233, 0.3118443086664556, 0.7480076101287239, 0.4105027839648151, 0.3402220977238791, 0.507537076082042, 0.2030898371313581, 0.498968116542434]}}
{"id": "02e74099-d0e0-44fd-b8cb-78c219815886", "fitness": 0.4120686600155117, "name": "CauchyDERankOrtho", "description": "Differential Evolution with a Cauchy mutation operator and a rank-based selection scheme, combined with periodic orthogonal learning to enhance exploration around the best solutions.", "code": "import numpy as np\n\nclass CauchyDERankOrtho:\n    def __init__(self, budget=10000, dim=10, pop_size=50, CR=0.7, ortho_interval=500):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.ortho_interval = ortho_interval\n        self.population = None\n        self.fitness = None\n        self.generation = 0\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n    def initialize_population(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n    def cauchy_mutation(self, pop, i, scale=0.1):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n        cauchy_values = scale * np.random.standard_cauchy(size=self.dim)\n        mutant = a + cauchy_values * (b - c)\n        return np.clip(mutant, -5, 5)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def rank_based_selection(self):\n        ranked_indices = np.argsort(self.fitness)\n        selection_probs = np.arange(self.pop_size, 0, -1) / np.sum(np.arange(self.pop_size, 0, -1))\n        selected_indices = np.random.choice(ranked_indices, size=self.pop_size, replace=True, p=selection_probs)\n        self.population = self.population[selected_indices]\n        self.fitness = self.fitness[selected_indices]\n\n    def orthogonal_learning(self, func):\n         # Select top solutions for orthogonal learning\n        num_selected = int(self.pop_size * 0.2)  # e.g., top 20%\n        top_indices = np.argsort(self.fitness)[:num_selected]\n        \n        for i in top_indices:\n            center = self.population[i].copy()\n            direction = np.random.randn(self.dim)\n            direction /= np.linalg.norm(direction)\n\n            # Define search range around the center\n            step_size = 0.1  # Adjust step size as needed\n            \n            # Generate orthogonal points\n            x1 = center + step_size * direction\n            x2 = center - step_size * direction\n            x1 = np.clip(x1, func.bounds.lb, func.bounds.ub)\n            x2 = np.clip(x2, func.bounds.lb, func.bounds.ub)\n\n            # Evaluate orthogonal points\n            f1 = func(x1)\n            self.budget -= 1\n            if self.budget <= 0:\n                return\n                \n            f2 = func(x2)\n            self.budget -= 1\n            if self.budget <= 0:\n                return\n\n            # Update population if better solutions are found\n            if f1 < self.fitness[i]:\n                self.population[i] = x1\n                self.fitness[i] = f1\n                if f1 < self.f_opt:\n                    self.f_opt = f1\n                    self.x_opt = x1\n            elif f2 < self.fitness[i]:\n                self.population[i] = x2\n                self.fitness[i] = f2\n                if f2 < self.f_opt:\n                    self.f_opt = f2\n                    self.x_opt = x2\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            new_population = np.copy(self.population)\n            new_fitness = np.copy(self.fitness)\n            \n            for i in range(self.pop_size):\n                target = self.population[i]\n                mutant = self.cauchy_mutation(self.population, i)\n                trial = self.crossover(mutant, target)\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n            \n                if self.budget <= 0:\n                    break\n\n            self.population = new_population\n            self.fitness = new_fitness\n            \n            # Rank-based selection\n            self.rank_based_selection()\n            \n            self.generation += 1\n            if self.generation % self.ortho_interval == 0:\n                self.orthogonal_learning(func)\n                if self.budget <= 0:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm CauchyDERankOrtho scored 0.412 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a6a719df-6472-4e24-8ff2-001551542349"], "operator": null, "metadata": {"aucs": [0.09872491941114836, 0.22555230419391414, 0.3446475388544268, 0.29753046799884764, 0.2813210906711747, 0.6101721917901861, 0.28442818815847404, 0.2699619495851555, 0.2859076102557436, 0.21930938091111696, 0.3847262491138802, 0.9906719867067711, 0.30402729458135735, 0.2866678052641577, 0.7099071922161584, 0.8740436604598092, 0.2901126586287567, 0.7322076172210281, 0.2908647020812336, 0.46058839220689485]}}
{"id": "181550ac-4a5d-4175-be43-a2355d53576b", "fitness": 0.40182452254839596, "name": "SelfOrganizingDE", "description": "Simplified self-adaptive Differential Evolution with individual-based learning rates for F and CR, and reduced complexity by removing periodic re-initialization.", "code": "import numpy as np\n\nclass SelfOrganizingDE:\n    def __init__(self, budget=10000, dim=10, pop_size=100, F_base=0.5, CR_base=0.7, learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_base = F_base  # Base mutation factor\n        self.CR_base = CR_base # Base Crossover rate\n        self.learning_rate = learning_rate\n        self.population = None\n        self.fitness = None\n        self.F = None # Individual mutation factors\n        self.CR = None # Individual crossover rates\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n\n        self.F = np.full(self.pop_size, self.F_base)\n        self.CR = np.full(self.pop_size, self.CR_base)\n\n    def mutate(self, pop, i):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n        mutant = a + self.F[i] * (b - c)\n        return np.clip(mutant, self.lb, self.ub)\n\n    def crossover(self, mutant, target, i):\n        cross_points = np.random.rand(self.dim) < self.CR[i]\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            new_population = np.copy(self.population)\n            new_fitness = np.copy(self.fitness)\n\n            for i in range(self.pop_size):\n                target = self.population[i]\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, target, i)\n\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n\n                    # Adjust F and CR based on success\n                    self.F[i] = np.clip(self.F[i] * (1 + self.learning_rate), 0.1, 1.0)\n                    self.CR[i] = np.clip(self.CR[i] * (1 + self.learning_rate), 0.1, 1.0)\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                else:\n                    # Adjust F and CR based on failure\n                    self.F[i] = np.clip(self.F[i] * (1 - self.learning_rate), 0.1, 1.0)\n                    self.CR[i] = np.clip(self.CR[i] * (1 - self.learning_rate), 0.1, 1.0)\n\n\n                if self.budget <= 0:\n                    break\n\n            self.population = new_population\n            self.fitness = new_fitness\n\n\n            if self.budget <=0:\n                break\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm SelfOrganizingDE scored 0.402 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5cebcf71-2e5e-4dfc-9537-662b5bb5e531"], "operator": null, "metadata": {"aucs": [0.15552696843751823, 0.25532235811403314, 0.3593872029177252, 0.3851905040545551, 0.3382712047981896, 0.4716488042557274, 0.3171455596988263, 0.38585893464136267, 0.3481952888226023, 0.21784447023051945, 0.37910229995608635, 0.999069658160395, 0.264135324788186, 0.3518757141743366, 0.6881201666979244, 0.5465183765411535, 0.353456689077177, 0.5317536040866453, 0.20249012805851285, 0.48557719345644224]}}
{"id": "a165e6d7-b181-4e9f-8a01-1291a9a6bff5", "fitness": 0.4131458561459743, "name": "EnhancedSelfOrganizingDE", "description": "Enhanced Self-Organizing DE with dynamic population diversity control, adaptive parameter tuning using success rates, and a restart mechanism based on stagnation detection.", "code": "import numpy as np\n\nclass EnhancedSelfOrganizingDE:\n    def __init__(self, budget=10000, dim=10, pop_size=100, F_base=0.5, CR_base=0.7, learning_rate=0.1, diversity_threshold=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_base = F_base  # Base mutation factor\n        self.CR_base = CR_base # Base Crossover rate\n        self.learning_rate = learning_rate\n        self.diversity_threshold = diversity_threshold\n        self.population = None\n        self.fitness = None\n        self.F = None # Individual mutation factors\n        self.CR = None # Individual crossover rates\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.stagnation_counter = 0\n        self.max_stagnation = 50 # Number of iterations before restart\n        self.prev_best_fitness = np.Inf\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n\n        self.F = np.full(self.pop_size, self.F_base)\n        self.CR = np.full(self.pop_size, self.CR_base)\n\n    def mutate(self, pop, i):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n        mutant = a + self.F[i] * (b - c)\n        return np.clip(mutant, self.lb, self.ub)\n\n    def crossover(self, mutant, target, i):\n        cross_points = np.random.rand(self.dim) < self.CR[i]\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def calculate_diversity(self):\n        return np.std(self.population)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            new_population = np.copy(self.population)\n            new_fitness = np.copy(self.fitness)\n            successful_individuals = []\n\n            for i in range(self.pop_size):\n                target = self.population[i]\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, target, i)\n\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                    successful_individuals.append(i)\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                if self.budget <= 0:\n                    break\n            \n            # Parameter Adaptation based on success rate\n            success_rate = len(successful_individuals) / self.pop_size if self.pop_size > 0 else 0\n            for i in range(self.pop_size):\n                if i in successful_individuals:\n                    self.F[i] = np.clip(self.F[i] * (1 + self.learning_rate * success_rate), 0.1, 1.0)\n                    self.CR[i] = np.clip(self.CR[i] * (1 + self.learning_rate * success_rate), 0.1, 1.0)\n                else:\n                    self.F[i] = np.clip(self.F[i] * (1 - self.learning_rate * (1 - success_rate)), 0.1, 1.0)\n                    self.CR[i] = np.clip(self.CR[i] * (1 - self.learning_rate * (1- success_rate)), 0.1, 1.0)\n\n\n            self.population = new_population\n            self.fitness = new_fitness\n\n            # Diversity Check and Re-initialization if needed\n            diversity = self.calculate_diversity()\n            if diversity < self.diversity_threshold:\n                # Re-initialize a portion of the population\n                num_reinitialize = self.pop_size // 5\n                indices_to_reinitialize = np.random.choice(self.pop_size, num_reinitialize, replace=False)\n                for idx in indices_to_reinitialize:\n                    self.population[idx] = np.random.uniform(self.lb, self.ub, size=self.dim)\n                    self.fitness[idx] = func(self.population[idx])\n                    self.budget -= 1\n                    if self.fitness[idx] < self.f_opt:\n                        self.f_opt = self.fitness[idx]\n                        self.x_opt = self.population[idx]\n                    self.F[idx] = self.F_base\n                    self.CR[idx] = self.CR_base\n                    if self.budget <= 0:\n                        break\n            # Stagnation Check and Restart\n            if self.f_opt >= self.prev_best_fitness:\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n\n            if self.stagnation_counter >= self.max_stagnation:\n                # Re-initialize the entire population\n                self.initialize_population(func)\n                self.stagnation_counter = 0\n\n            self.prev_best_fitness = self.f_opt\n\n            if self.budget <=0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm EnhancedSelfOrganizingDE scored 0.413 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5cebcf71-2e5e-4dfc-9537-662b5bb5e531"], "operator": null, "metadata": {"aucs": [0.17210512220031926, 0.23359249705459273, 0.35733225595357376, 0.3753301705985359, 0.36413839000766546, 0.4788871812696254, 0.3259210208577986, 0.38742614672672415, 0.3632485476724485, 0.2242250312792805, 0.4123012229156555, 0.9953209209807431, 0.32838626570502416, 0.35478147773206403, 0.7482149893103491, 0.5253139530708083, 0.35193872041758234, 0.5534671703853642, 0.2141599365419956, 0.49682610223933543]}}
{"id": "6aa189dc-c842-418a-95d8-25b33c547beb", "fitness": 0.37584523991781754, "name": "SelfOrganizingDE", "description": "Self-Organizing DE with improved parameter adaptation based on fitness rank, covariance matrix adaptation for mutation, and orthogonal initialization for enhanced exploration.", "code": "import numpy as np\n\nclass SelfOrganizingDE:\n    def __init__(self, budget=10000, dim=10, pop_size=100, F_base=0.5, CR_base=0.7, learning_rate=0.1, ortho_init=True):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_base = F_base  # Base mutation factor\n        self.CR_base = CR_base # Base Crossover rate\n        self.learning_rate = learning_rate\n        self.population = None\n        self.fitness = None\n        self.F = None # Individual mutation factors\n        self.CR = None # Individual crossover rates\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.ortho_init = ortho_init\n        self.C = np.eye(dim)  # Covariance matrix for CMA-ES-like mutation\n        self.mu = None\n\n\n    def orthogonal_initialization(self):\n        # Generate an orthogonal matrix using QR decomposition\n        H = np.random.randn(self.pop_size, self.dim)\n        Q, R = np.linalg.qr(H)\n        # Scale and shift to fit within bounds\n        self.population = Q * 0.5 * (self.ub - self.lb) + 0.5 * (self.ub + self.lb)\n        self.population = np.clip(self.population, self.lb, self.ub)\n\n\n    def initialize_population(self, func):\n        if self.ortho_init:\n            self.orthogonal_initialization()\n        else:\n            self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n        self.mu = np.mean(self.population, axis=0)\n\n        self.F = np.full(self.pop_size, self.F_base)\n        self.CR = np.full(self.pop_size, self.CR_base)\n\n    def mutate(self, pop, i):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n        # CMA-ES-like mutation:\n        z = np.random.normal(0, 1, self.dim)\n        mutant = a + self.F[i] * (b - c) + 0.1 * np.sqrt(np.diag(self.C)) * z\n        return np.clip(mutant, self.lb, self.ub)\n\n    def crossover(self, mutant, target, i):\n        cross_points = np.random.rand(self.dim) < self.CR[i]\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            new_population = np.copy(self.population)\n            new_fitness = np.copy(self.fitness)\n            ranks = np.argsort(self.fitness)\n\n            for i in range(self.pop_size):\n                target = self.population[i]\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, target, i)\n\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n\n                    # Adjust F and CR based on rank: Better ranked individuals get less adjustment\n                    rank = np.where(ranks == i)[0][0]\n                    rank_factor = 1 - (rank / self.pop_size) # Scale between 0 and 1, higher is better\n\n                    self.F[i] = np.clip(self.F[i] * (1 + self.learning_rate * rank_factor), 0.1, 1.0)\n                    self.CR[i] = np.clip(self.CR[i] * (1 + self.learning_rate * rank_factor), 0.1, 1.0)\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                else:\n                    # Adjust F and CR based on failure\n                    rank = np.where(ranks == i)[0][0]\n                    rank_factor = 1 - (rank / self.pop_size)\n\n                    self.F[i] = np.clip(self.F[i] * (1 - self.learning_rate * rank_factor), 0.1, 1.0)\n                    self.CR[i] = np.clip(self.CR[i] * (1 - self.learning_rate * rank_factor), 0.1, 1.0)\n\n\n                if self.budget <= 0:\n                    break\n\n            self.population = new_population\n            self.fitness = new_fitness\n\n            # Update covariance matrix: Simple version, can be improved.\n            self.mu = 0.9 * self.mu + 0.1 * np.mean(self.population, axis=0)\n            diff = self.population - self.mu\n            self.C = 0.9 * self.C + 0.1 * np.cov(diff.T)\n\n\n            # Periodically re-initialize poorly performing individuals.\n            worst_indices = np.argsort(self.fitness)[-self.pop_size // 10:]\n            for idx in worst_indices:\n                self.population[idx] = np.random.uniform(self.lb, self.ub, size=self.dim)\n                self.fitness[idx] = func(self.population[idx])\n                self.budget -=1\n                if self.fitness[idx] < self.f_opt:\n                    self.f_opt = self.fitness[idx]\n                    self.x_opt = self.population[idx]\n                self.F[idx] = self.F_base\n                self.CR[idx] = self.CR_base\n                if self.budget <= 0:\n                    break\n\n            if self.budget <=0:\n                break\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm SelfOrganizingDE scored 0.376 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5cebcf71-2e5e-4dfc-9537-662b5bb5e531"], "operator": null, "metadata": {"aucs": [0.15532739326012812, 0.2383365454266233, 0.39214612711707164, 0.3957923334798201, 0.32617482227588557, 0.42814385810302213, 0.2910612450804628, 0.33783927519412615, 0.3207911586763418, 0.1757862985138885, 0.3796502211248278, 0.9834046705860041, 0.2627814519654511, 0.3002237347136467, 0.6898120729985876, 0.37912013681452095, 0.32802705448743386, 0.4489006246525352, 0.19680131086044705, 0.48678446302552614]}}
{"id": "bdae6e4b-fe81-402b-aa2c-00de4f448188", "fitness": 0.36247511908011454, "name": "CooperativeDE", "description": "Cooperative Differential Evolution with dynamic population management and adaptive strategy selection based on individual performance and diversity metrics to enhance exploration and exploitation.", "code": "import numpy as np\n\nclass CooperativeDE:\n    def __init__(self, budget=10000, dim=10, pop_size=100, num_subgroups=5, F=0.5, CR=0.7, diversity_threshold=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.num_subgroups = num_subgroups\n        self.F = F\n        self.CR = CR\n        self.diversity_threshold = diversity_threshold\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n\n    def calculate_diversity(self):\n        \"\"\"Calculates the average pairwise Euclidean distance in the population.\"\"\"\n        distances = np.sum((self.population[:, None, :] - self.population[None, :, :]) ** 2, axis=2)\n        diversity = np.sum(np.sqrt(distances)) / (self.pop_size * (self.pop_size - 1) + 1e-8) #Avoid zero division\n        return diversity\n\n    def mutate(self, pop, i, subgroup):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n        \n        # Dynamic strategy selection: different mutation strategies per subgroup.\n        if subgroup == 0:  # DE/rand/1\n            mutant = self.population[np.random.randint(self.pop_size)] + self.F * (b - c)\n        elif subgroup == 1: # DE/best/1\n            best_idx = np.argmin(self.fitness)\n            mutant = self.population[best_idx] + self.F * (b - c)\n        elif subgroup == 2: #DE/current-to-rand/1\n             mutant = pop[i] + np.random.rand() * (self.population[np.random.randint(self.pop_size)] - pop[i]) + self.F * (b-c)\n        elif subgroup == 3: # DE/rand/2\n            idxs2 = np.random.choice(len(pop), 2, replace=False)\n            d, e = pop[idxs2]\n            mutant = self.population[np.random.randint(self.pop_size)] + self.F * (b - c) + self.F*(d - e)\n        else: #DE/current-to-best/1\n            best_idx = np.argmin(self.fitness)\n            mutant = pop[i] + np.random.rand() * (self.population[best_idx] - pop[i]) + self.F * (b-c)\n\n        return np.clip(mutant, self.lb, self.ub)\n\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        subgroup_size = self.pop_size // self.num_subgroups\n        \n        while self.budget > 0:\n            diversity = self.calculate_diversity()\n            new_population = np.copy(self.population)\n            new_fitness = np.copy(self.fitness)\n\n            for subgroup in range(self.num_subgroups):\n                start_index = subgroup * subgroup_size\n                end_index = (subgroup + 1) * subgroup_size if subgroup < self.num_subgroups - 1 else self.pop_size\n\n                for i in range(start_index, end_index):\n                    target = self.population[i]\n                    mutant = self.mutate(self.population, i, subgroup)\n                    trial = self.crossover(mutant, target)\n\n                    f_trial = func(trial)\n                    self.budget -= 1\n\n                    if f_trial < self.fitness[i]:\n                        new_population[i] = trial\n                        new_fitness[i] = f_trial\n\n                        if f_trial < self.f_opt:\n                            self.f_opt = f_trial\n                            self.x_opt = trial\n\n                    if self.budget <= 0:\n                        break\n                if self.budget <= 0:\n                    break\n\n            self.population = new_population\n            self.fitness = new_fitness\n            \n            #Dynamic population management: re-initialize if diversity is too low\n            if diversity < self.diversity_threshold:\n                num_reinit = self.pop_size // 10\n                worst_indices = np.argsort(self.fitness)[-num_reinit:]\n                for idx in worst_indices:\n                    self.population[idx] = np.random.uniform(self.lb, self.ub, size=self.dim)\n                    self.fitness[idx] = func(self.population[idx])\n                    self.budget -=1\n                    if self.fitness[idx] < self.f_opt:\n                        self.f_opt = self.fitness[idx]\n                        self.x_opt = self.population[idx]\n                    if self.budget <= 0:\n                        break\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm CooperativeDE scored 0.362 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5cebcf71-2e5e-4dfc-9537-662b5bb5e531"], "operator": null, "metadata": {"aucs": [0.2087949433195171, 0.35328053916800795, 0.4758483997848272, 0.7744517131282205, 0]}}
{"id": "b71b2725-a1b7-4b5a-8745-5cd5f5675d04", "fitness": 0.616380017651456, "name": "AdaptiveDEPopSize", "description": "An adaptive Differential Evolution strategy that dynamically adjusts the population size during optimization based on improvement stagnation, increasing it when the search stagnates and reducing it when progress is consistent.", "code": "import numpy as np\n\nclass AdaptiveDEPopSize:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, F=0.5, CR=0.7, stagnation_threshold=500, pop_size_increase_factor=1.5, pop_size_decrease_factor=0.5, min_pop_size=20, max_pop_size=200):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.initial_pop_size = initial_pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR # Crossover rate\n        self.population = None\n        self.fitness = None\n        self.stagnation_threshold = stagnation_threshold\n        self.pop_size_increase_factor = pop_size_increase_factor\n        self.pop_size_decrease_factor = pop_size_decrease_factor\n        self.min_pop_size = min_pop_size\n        self.max_pop_size = max_pop_size\n        self.generation = 0\n        self.stagnation_counter = 0\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.success_history_F = []\n        self.success_history_CR = []\n\n    def initialize_population(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size # reduce budget by initial evaluation\n        \n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n\n    def mutate(self, pop, i):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n        mutant = a + self.F * (b - c)\n        return np.clip(mutant, -5, 5)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def adjust_population_size(self, func):\n        if self.stagnation_counter >= self.stagnation_threshold:\n            new_pop_size = int(self.pop_size * self.pop_size_increase_factor)\n            new_pop_size = min(new_pop_size, self.max_pop_size)\n            \n            if new_pop_size > self.pop_size:\n                num_new_individuals = new_pop_size - self.pop_size\n                lb = func.bounds.lb\n                ub = func.bounds.ub\n                new_individuals = np.random.uniform(lb, ub, size=(num_new_individuals, self.dim))\n                new_fitness = np.array([func(x) for x in new_individuals])\n                self.budget -= num_new_individuals\n                \n                self.population = np.vstack((self.population, new_individuals))\n                self.fitness = np.concatenate((self.fitness, new_fitness))\n                self.pop_size = new_pop_size\n                self.stagnation_counter = 0 #reset counter after expansion\n\n                best_index = np.argmin(self.fitness)\n                if self.fitness[best_index] < self.f_opt:\n                    self.f_opt = self.fitness[best_index]\n                    self.x_opt = self.population[best_index]\n\n        elif self.generation > self.stagnation_threshold and self.stagnation_counter < self.stagnation_threshold / 2 and self.initial_pop_size < self.pop_size:\n            new_pop_size = int(self.pop_size * self.pop_size_decrease_factor)\n            new_pop_size = max(new_pop_size, self.min_pop_size)\n\n            if new_pop_size < self.pop_size:\n                 # reduce population to the best individuals\n                indices = np.argsort(self.fitness)[:new_pop_size]\n                self.population = self.population[indices]\n                self.fitness = self.fitness[indices]\n                self.pop_size = new_pop_size\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            \n            \n            best_fitness_before_gen = self.f_opt  # Store best fitness before the generation\n\n            for i in range(self.pop_size):\n                target = self.population[i]\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, target)\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.success_history_F.append(self.F)\n                    self.success_history_CR.append(self.CR)\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n\n            # Adaptive parameter adjustment based on success history\n            if self.success_history_F:\n                self.F = np.clip(np.mean(self.success_history_F), 0.1, 0.9)\n                self.CR = np.clip(np.mean(self.success_history_CR), 0.1, 0.9)\n                self.success_history_F = []\n                self.success_history_CR = []\n            \n            self.generation += 1\n\n            # Check for improvement\n            if self.f_opt < best_fitness_before_gen:\n                self.stagnation_counter = 0\n            else:\n                self.stagnation_counter += 1\n\n            self.adjust_population_size(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDEPopSize scored 0.616 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a6a719df-6472-4e24-8ff2-001551542349"], "operator": null, "metadata": {"aucs": [0.2105111590314297, 0.3827241678269373, 0.5832264370446398, 0.8047608938765577, 0.7258969260493239, 0.7495786823840483, 0.521601889474124, 0.5979837547760833, 0.6893827951038447, 0.6218096166791426, 0.8069848598322825, 0.9938590954584272, 0.34972894472183, 0.6408009570906892, 0.8829841593606397, 0.7465226068506214, 0.4969177626796063, 0.7935277117234825, 0.21405625307647624, 0.5147416799889333]}}
{"id": "343ecda2-77eb-4d1a-9af9-5ed5d939c702", "fitness": -Infinity, "name": "ClusteredArchiveDE", "description": "Differential Evolution with a dynamically adjusted archive size and a diversity maintenance mechanism based on clustering.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass ClusteredArchiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=100, F_base=0.5, CR_base=0.7, archive_size_ratio=0.2, diversity_threshold=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.archive_size_ratio = archive_size_ratio\n        self.diversity_threshold = diversity_threshold\n        self.population = None\n        self.fitness = None\n        self.archive = None\n        self.archive_fitness = None\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n\n        self.archive_size = int(self.pop_size * self.archive_size_ratio)\n        self.archive = np.empty((0, self.dim))\n        self.archive_fitness = np.empty(0)\n\n    def mutate(self, i):\n        pop_indices = list(range(self.pop_size))\n        pop_indices.remove(i)\n        \n        if len(self.archive) > 0:\n            pool = np.vstack((self.population[pop_indices], self.archive))\n            indices = np.random.choice(len(pool), 2, replace=False)\n            r1, r2 = pool[indices]\n            mutant = self.population[i] + self.F_base * (r1 - r2)\n        else:\n            indices = np.random.choice(len(pop_indices), 2, replace=False)\n            r1_index, r2_index = pop_indices[indices[0]], pop_indices[indices[1]]\n            r1, r2 = self.population[r1_index], self.population[r2_index]\n            mutant = self.population[i] + self.F_base * (r1 - r2)\n        return np.clip(mutant, self.lb, self.ub)\n\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR_base\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def update_archive(self, individual, fitness):\n        if len(self.archive) < self.archive_size:\n            self.archive = np.vstack((self.archive, individual.reshape(1, -1)))\n            self.archive_fitness = np.append(self.archive_fitness, fitness)\n        else:\n            if fitness < np.max(self.archive_fitness):\n                worst_index = np.argmax(self.archive_fitness)\n                self.archive[worst_index] = individual\n                self.archive_fitness[worst_index] = fitness\n\n    def maintain_diversity(self):\n        if len(self.archive) > 1:\n            kmeans = KMeans(n_clusters=min(len(self.archive), 5), random_state=0, n_init=10).fit(self.archive)\n            cluster_labels = kmeans.labels_\n            \n            unique_labels = np.unique(cluster_labels)\n            for label in unique_labels:\n                cluster_indices = np.where(cluster_labels == label)[0]\n                if len(cluster_indices) > 1:\n                   distances = np.linalg.norm(self.archive[cluster_indices] - kmeans.cluster_centers_[label], axis=1)\n                   farthest_index = cluster_indices[np.argmax(distances)]\n                   closest_index = cluster_indices[np.argmin(distances)]\n                   if np.linalg.norm(self.archive[farthest_index] - self.archive[closest_index]) < self.diversity_threshold:\n                       # Replace farthest with random individual\n                       self.archive[farthest_index] = np.random.uniform(self.lb, self.ub, size=self.dim)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                target = self.population[i]\n                mutant = self.mutate(i)\n                trial = self.crossover(mutant, target)\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.update_archive(self.population[i], self.fitness[i])\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                else:\n                    self.update_archive(trial, f_trial)\n\n                if self.budget <= 0:\n                    break\n            self.maintain_diversity()\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "An exception occurred: name 'KMeans' is not defined.", "error": "", "parent_ids": ["cb480dba-9dad-44b2-9951-eff1e0598739"], "operator": null, "metadata": {}}
{"id": "6d21c381-29db-44fc-8ffb-734d8b1e4060", "fitness": -Infinity, "name": "AdaptiveFitnessDE", "description": "Differential Evolution with a dynamically adjusted population size and a fitness-dependent mutation factor.", "code": "import numpy as np\n\nclass AdaptiveFitnessDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_base=0.5, CR=0.9, pop_size_adjust_freq=50, pop_size_increase_factor=1.2, pop_size_decrease_factor=0.8):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_base = F_base\n        self.CR = CR\n        self.population = None\n        self.fitness = None\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.generation = 0\n        self.pop_size_adjust_freq = pop_size_adjust_freq\n        self.pop_size_increase_factor = pop_size_increase_factor\n        self.pop_size_decrease_factor = pop_size_decrease_factor\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n\n    def mutate(self, pop, i):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n\n        # Fitness-dependent mutation factor\n        fitness_rank = np.argsort(self.fitness)\n        rank = np.where(fitness_rank == i)[0][0]\n        F = self.F_base * (1 + (rank / len(pop))) # Scale F based on fitness rank\n\n        mutant = a + F * (b - c)\n        return np.clip(mutant, self.lb, self.ub)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def adjust_population_size(self):\n        if self.generation % self.pop_size_adjust_freq == 0:\n            # Calculate the standard deviation of the fitness values\n            fitness_std = np.std(self.fitness)\n\n            # If the standard deviation is low, it suggests the population has converged\n            if fitness_std < 1e-3:\n                self.pop_size = int(self.pop_size * self.pop_size_increase_factor)\n                self.pop_size = min(self.pop_size, 200)  # Avoid excessive population sizes\n                print(f\"Increasing population size to {self.pop_size}\")\n            else:\n                # Reduce population size if there is diversity in the fitness values\n                self.pop_size = int(self.pop_size * self.pop_size_decrease_factor)\n                self.pop_size = max(self.pop_size, 10)  # Ensure a minimum population size\n                print(f\"Decreasing population size to {self.pop_size}\")\n            \n            # Re-initialize population with new size\n            old_pop = self.population\n            old_fitness = self.fitness\n\n            self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n            if old_pop is not None:\n              self.population[:min(len(old_pop),len(self.population))] = old_pop[:min(len(old_pop),len(self.population))]\n            \n            # Evaluate the fitness of the new population members only, keep the rest\n            self.fitness = np.zeros(self.pop_size)\n            if old_fitness is not None:\n              self.fitness[:min(len(old_fitness),len(self.fitness))] = old_fitness[:min(len(old_fitness),len(self.fitness))]\n            \n            for i in range(min(len(old_fitness),len(self.fitness)), self.pop_size):\n              self.fitness[i] = func(self.population[i])\n              self.budget -= 1\n              if self.fitness[i] < self.f_opt:\n                  self.f_opt = self.fitness[i]\n                  self.x_opt = self.population[i]\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            self.adjust_population_size()\n\n            for i in range(self.pop_size):\n                target = self.population[i]\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, target)\n\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n\n            self.generation += 1\n\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "An exception occurred: name 'func' is not defined.", "error": "", "parent_ids": ["cb480dba-9dad-44b2-9951-eff1e0598739"], "operator": null, "metadata": {}}
{"id": "3860ef83-536d-4d14-ab92-e7c4cbb431ef", "fitness": 0.0, "name": "AdaptiveSelfOrganizingDE", "description": "Simplified self-adaptive Differential Evolution with success-history based parameter adaptation, periodic population updates, and adaptive population size adjustment.", "code": "import numpy as np\n\nclass AdaptiveSelfOrganizingDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_base=0.5, CR_base=0.7, learning_rate=0.1, archive_size=10, pop_size_adapt_freq=10, pop_size_increase_factor=1.2, pop_size_decrease_factor=0.8):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.learning_rate = learning_rate\n        self.archive_size = archive_size\n        self.population = None\n        self.fitness = None\n        self.F = None\n        self.CR = None\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.success_F = []\n        self.success_CR = []\n        self.archive = []\n        self.pop_size_adapt_freq = pop_size_adapt_freq\n        self.pop_size_increase_factor = pop_size_increase_factor\n        self.pop_size_decrease_factor = pop_size_decrease_factor\n        self.generation = 0\n        self.last_improvement = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n\n        self.F = np.full(self.pop_size, self.F_base)\n        self.CR = np.full(self.pop_size, self.CR_base)\n\n    def mutate(self, pop, i):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n        mutant = a + self.F[i] * (b - c)\n        return np.clip(mutant, self.lb, self.ub)\n\n    def crossover(self, mutant, target, i):\n        cross_points = np.random.rand(self.dim) < self.CR[i]\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target, )\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.generation = 0\n        self.last_improvement = 0\n\n        while self.budget > 0:\n            \n            for i in range(self.pop_size):\n                target = self.population[i]\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, target, i)\n\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    # Success\n                    self.success_F.append(self.F[i])\n                    self.success_CR.append(self.CR[i])\n\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.last_improvement = self.generation\n\n                if self.budget <= 0:\n                    break\n            \n            self.generation +=1\n\n            # Update F and CR based on success history\n            if self.success_F:\n                self.F = np.clip(np.random.choice(self.success_F, size=self.pop_size), 0.1, 1.0)\n                self.CR = np.clip(np.random.choice(self.success_CR, size=self.pop_size), 0.1, 1.0)\n\n            # Clear success history\n            self.success_F = []\n            self.success_CR = []\n            \n            # Periodically re-initialize poorly performing individuals. Simplified re-initialization.\n            if self.budget > 0:\n                worst_indices = np.argsort(self.fitness)[-self.pop_size // 10:]\n                self.population[worst_indices] = np.random.uniform(self.lb, self.ub, size=(len(worst_indices), self.dim))\n                for idx in worst_indices:\n                    self.fitness[idx] = func(self.population[idx])\n                    self.budget -=1\n                    if self.fitness[idx] < self.f_opt:\n                        self.f_opt = self.fitness[idx]\n                        self.x_opt = self.population[idx]\n\n                    if self.budget <= 0:\n                        break\n            if self.budget <=0:\n                break\n            \n            #Adapt population size\n            if self.generation % self.pop_size_adapt_freq == 0:\n                if (self.generation - self.last_improvement) > self.pop_size_adapt_freq * 5:  #Stagnation\n                    self.pop_size = min(int(self.pop_size * self.pop_size_increase_factor), 200) # Limit max pop_size\n                    print(\"Increasing pop size to\", self.pop_size)\n                else:\n                     self.pop_size = max(int(self.pop_size * self.pop_size_decrease_factor), 10)\n                     print(\"Decreasing pop size to\", self.pop_size)\n                \n                #Re-initialize population and parameters\n                old_pop = self.population\n                old_fitness = self.fitness\n                self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n\n                if old_pop is not None:\n                  self.population[:min(self.pop_size, len(old_pop))] = old_pop[:min(self.pop_size, len(old_pop))]\n\n                self.fitness = np.array([func(x) for x in self.population])\n                self.budget -= self.pop_size - len(old_pop) if old_pop is not None else self.pop_size\n\n                best_index = np.argmin(self.fitness)\n                if self.fitness[best_index] < self.f_opt:\n                  self.f_opt = self.fitness[best_index]\n                  self.x_opt = self.population[best_index]\n                \n                self.F = np.full(self.pop_size, self.F_base)\n                self.CR = np.full(self.pop_size, self.CR_base)\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveSelfOrganizingDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["cb480dba-9dad-44b2-9951-eff1e0598739"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "e316ffc9-e5d2-4979-828a-22ec8a77dfb8", "fitness": 0.6185590484016328, "name": "AdaptiveDE", "description": "Simplified Adaptive Differential Evolution with periodic population updates and parameter adaptation based on successful mutations.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, update_interval=1000):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR # Crossover rate\n        self.population = None\n        self.fitness = None\n        self.update_interval = update_interval\n        self.success_F_history = []\n        self.success_CR_history = []\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n    def mutate(self, pop, i):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n        mutant = a + self.F * (b - c)\n        return np.clip(mutant, self.lb, self.ub)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        evals = 0\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n                target = self.population[i]\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, target)\n                f_trial = func(trial)\n                self.budget -= 1\n                evals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.success_F_history.append(self.F)\n                    self.success_CR_history.append(self.CR)\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            if evals >= self.update_interval:\n                if self.success_F_history:\n                    self.F = np.mean(self.success_F_history)\n                    self.CR = np.mean(self.success_CR_history)\n                self.F = np.clip(self.F, 0.1, 0.9)\n                self.CR = np.clip(self.CR, 0.1, 0.9)\n                self.success_F_history = []\n                self.success_CR_history = []\n                evals = 0\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDE scored 0.619 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e6a4724f-c912-4547-8af1-7b14a7fdd504"], "operator": null, "metadata": {"aucs": [0.23191117142012707, 0.48034487757030353, 0.5765444890644631, 0.8308194953282052, 0.7142661649422211, 0.7306823251743639, 0.5500890756192685, 0.5801390721009134, 0.6889089554047395, 0.5476066313293666, 0.7934168706660886, 0.9968402534416909, 0.3508332234783005, 0.6195089862805152, 0.903067007399003, 0.7601907962196052, 0.4803503805441187, 0.8080756684038874, 0.22074938842470382, 0.5068361352207703]}}
{"id": "ad72340f-e672-4385-a9f7-57e18a63761c", "fitness": 0.4238394924805286, "name": "SelfOrganizingDE", "description": "Simplified self-adaptive Differential Evolution with success-history based parameter adaptation, periodic population updates, and a simplified update of F/CR values.", "code": "import numpy as np\n\nclass SelfOrganizingDE:\n    def __init__(self, budget=10000, dim=10, pop_size=100, F_base=0.5, CR_base=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = None\n        self.fitness = None\n        self.F = None\n        self.CR = None\n        self.success_F = []\n        self.success_CR = []\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n        self.F = np.full(self.pop_size, self.F_base)\n        self.CR = np.full(self.pop_size, self.CR_base)\n\n    def mutate(self, pop, i):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n        mutant = a + self.F[i] * (b - c)\n        return np.clip(mutant, self.lb, self.ub)\n\n    def crossover(self, mutant, target, i):\n        cross_points = np.random.rand(self.dim) < self.CR[i]\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                target = self.population[i]\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, target, i)\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.success_F.append(self.F[i])\n                    self.success_CR.append(self.CR[i])\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n\n            if self.success_F:\n                self.F = np.full(self.pop_size, np.mean(self.success_F))\n                self.CR = np.full(self.pop_size, np.mean(self.success_CR))\n\n            self.success_F = []\n            self.success_CR = []\n\n            if self.budget > 0:\n              worst_indices = np.argsort(self.fitness)[-self.pop_size // 10:]\n              for idx in worst_indices:\n                  self.population[idx] = np.random.uniform(self.lb, self.ub, size=self.dim)\n                  self.fitness[idx] = func(self.population[idx])\n                  self.budget -=1\n                  if self.fitness[idx] < self.f_opt:\n                      self.f_opt = self.fitness[idx]\n                      self.x_opt = self.population[idx]\n                  if self.budget <= 0:\n                      break\n            if self.budget <=0:\n                break\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm SelfOrganizingDE scored 0.424 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["cb480dba-9dad-44b2-9951-eff1e0598739"], "operator": null, "metadata": {"aucs": [0.17989763869066466, 0.25819545314310377, 0.4007931057644992, 0.5974970448828028, 0.37325575902522223, 0.4513973887611973, 0.3304665476442351, 0.36848493544065863, 0.36332878099233135, 0.2361009408885475, 0.4929880353063131, 0.9985811820301441, 0.29621705005430543, 0.35919884499744337, 0.7342312877251447, 0.46743620484672765, 0.33930109223785254, 0.5465817590595308, 0.1907330913546964, 0.4921037067651507]}}
{"id": "c4f020a3-c600-4cd6-946e-f4124d933ba6", "fitness": 0.6171475512766829, "name": "AdaptiveDE", "description": "Simplified adaptive Differential Evolution that adjusts the mutation factor based on success history and restarts the population when stagnating, focusing on exploration.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, stagnation_threshold=500):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR # Crossover rate\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.success_history_F = []\n\n    def initialize_population(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n\n    def mutate(self, pop, i):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n        mutant = a + self.F * (b - c)\n        return np.clip(mutant, -5, 5)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def restart_population(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            best_fitness_before_gen = self.f_opt\n\n            for i in range(self.pop_size):\n                target = self.population[i]\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, target)\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.success_history_F.append(self.F)\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n\n            if self.success_history_F:\n                self.F = np.clip(np.mean(self.success_history_F), 0.1, 0.9)\n                self.success_history_F = []\n            \n            if self.f_opt < best_fitness_before_gen:\n                self.stagnation_counter = 0\n            else:\n                self.stagnation_counter += 1\n\n            if self.stagnation_counter >= self.stagnation_threshold:\n                self.restart_population(func)\n                self.stagnation_counter = 0\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDE scored 0.617 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b71b2725-a1b7-4b5a-8745-5cd5f5675d04"], "operator": null, "metadata": {"aucs": [0.2219662863891959, 0.3762644274996264, 0.6153849615743879, 0.8493439696670304, 0.6864188193243905, 0.7476463466050143, 0.5579100393530513, 0.5885468366450011, 0.6541491542236915, 0.5295345172989424, 0.8002435330288988, 0.9995693337541723, 0.3625381475688487, 0.6774869242777823, 0.8632938154283805, 0.7603307097670886, 0.5117998808004386, 0.8081395856935815, 0.22246522921921108, 0.5099185074149232]}}
{"id": "3de18560-c11d-491e-b68c-cae96adcfc5a", "fitness": 0.5484145369713114, "name": "SelfOrganizingDE", "description": "Simplified self-adaptive Differential Evolution with a periodically updated population, success-history based parameter adaptation, and rank-based F/CR sampling.", "code": "import numpy as np\n\nclass SelfOrganizingDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_base=0.5, CR_base=0.7, learning_rate=0.1, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.learning_rate = learning_rate\n        self.archive_size = archive_size\n        self.population = None\n        self.fitness = None\n        self.F = None\n        self.CR = None\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.success_F = []\n        self.success_CR = []\n        self.archive = []\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n\n        self.F = np.full(self.pop_size, self.F_base)\n        self.CR = np.full(self.pop_size, self.CR_base)\n\n    def mutate(self, pop, i):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n        mutant = a + self.F[i] * (b - c)\n        return np.clip(mutant, self.lb, self.ub)\n\n    def crossover(self, mutant, target, i):\n        cross_points = np.random.rand(self.dim) < self.CR[i]\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                target = self.population[i]\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, target, i)\n\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    # Success\n                    self.success_F.append(self.F[i])\n                    self.success_CR.append(self.CR[i])\n\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n\n            # Update F and CR based on success history\n            if self.success_F:\n                ranked_indices = np.argsort(self.fitness)\n                num_successful = min(len(self.success_F), self.pop_size // 5)\n                selected_indices = ranked_indices[:num_successful]\n                \n                self.F = np.clip(np.random.choice(self.success_F, size=self.pop_size), 0.1, 1.0)\n                self.CR = np.clip(np.random.choice(self.success_CR, size=self.pop_size), 0.1, 1.0)\n\n            # Clear success history\n            self.success_F = []\n            self.success_CR = []\n\n            # Periodically re-initialize poorly performing individuals.\n            if self.budget > 0:\n              worst_indices = np.argsort(self.fitness)[-self.pop_size // 10:]\n              for idx in worst_indices:\n                  self.population[idx] = np.random.uniform(self.lb, self.ub, size=self.dim)\n                  self.fitness[idx] = func(self.population[idx])\n                  self.budget -=1\n                  if self.fitness[idx] < self.f_opt:\n                      self.f_opt = self.fitness[idx]\n                      self.x_opt = self.population[idx]\n\n                  if self.budget <= 0:\n                      break\n            if self.budget <=0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm SelfOrganizingDE scored 0.548 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["cb480dba-9dad-44b2-9951-eff1e0598739"], "operator": null, "metadata": {"aucs": [0.16417912007978464, 0.31560978978924636, 0.4795567797013107, 0.7764863627922027, 0.5747010868042445, 0.6768600269727807, 0.453217252813489, 0.5088096370367308, 0.5494659426471513, 0.4621428862500683, 0.7057573294324118, 0.9984885385202116, 0.29840963553590605, 0.553968524764436, 0.8489284846051834, 0.6896577033231068, 0.41190137686407147, 0.7558046433949732, 0.23027601747844373, 0.5140696006204759]}}
{"id": "6186f102-fae1-46d5-9824-bcb1f38471f8", "fitness": 0.5932644043589984, "name": "AdaptiveDEwithLocalSearch", "description": "Simplified Differential Evolution with adaptive local search and parameter adaptation based on successful parameter history.", "code": "import numpy as np\n\nclass AdaptiveDEwithLocalSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, local_search_radius=0.1, ls_frequency=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.local_search_radius = local_search_radius\n        self.ls_frequency = ls_frequency\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.memory_F = [0.5] * 10  # Memory for successful F values\n        self.memory_CR = [0.7] * 10  # Memory for successful CR values\n        self.p = 0.1  # Probability for choosing historical F and CR\n\n    def initialize_population(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n    def mutate(self, pop, i, F):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n        mutant = a + F * (b - c)\n        return np.clip(mutant, -5, 5)\n\n    def crossover(self, mutant, target, CR):\n        cross_points = np.random.rand(self.dim) < CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def local_search(self, x, func):\n        new_x = x + np.random.uniform(-self.local_search_radius, self.local_search_radius, size=self.dim)\n        new_x = np.clip(new_x, -5, 5)\n        new_f = func(new_x)\n        self.budget -= 1\n        return new_x, new_f\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        archive_F = []\n        archive_CR = []\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Parameter adaptation\n                if np.random.rand() < self.p:\n                    F = np.random.choice(self.memory_F)\n                    CR = np.random.choice(self.memory_CR)\n                else:\n                    F = self.F\n                    CR = self.CR\n                \n                target = self.population[i]\n                mutant = self.mutate(self.population, i, F)\n                trial = self.crossover(mutant, target, CR)\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if self.budget <= 0:\n                    break\n\n                if f_trial < self.fitness[i]:\n                    # Local search application\n                    if np.random.rand() < self.ls_frequency:\n                        trial, f_trial = self.local_search(trial, func)\n                        if self.budget <= 0:\n                            break\n                    \n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                    archive_F.append(F)\n                    archive_CR.append(CR)\n\n            if archive_F: # Update memory\n                self.memory_F = archive_F[-10:] if len(archive_F) >= 10 else archive_F\n                self.memory_CR = archive_CR[-10:] if len(archive_CR) >= 10 else archive_CR\n                archive_F = []\n                archive_CR = []\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDEwithLocalSearch scored 0.593 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9f314d8c-ae4b-45dc-a5d9-7be948c15c37"], "operator": null, "metadata": {"aucs": [0.21318687980661544, 0.48939452849731624, 0.537834663748967, 0.8252400650782798, 0.6117015895582495, 0.7303193034794995, 0.4634909591561345, 0.5077616059851792, 0.6238265778770278, 0.5287725627459394, 0.7891958317770491, 0.9982336233224649, 0.36039144402834367, 0.5888306900804003, 0.8683209509579841, 0.7102571671520244, 0.4681580114620343, 0.7767645713079367, 0.2656755483183174, 0.5079315128402073]}}
{"id": "cfae23f3-6eca-4578-851f-15ac2dc3e441", "fitness": 0.6341993314922392, "name": "DEArchiveBrownian", "description": "Differential Evolution with Archive and Brownian motion-based local search, integrating a historical archive to diversify the search and a Brownian motion-based local search to refine solutions.", "code": "import numpy as np\n\nclass DEArchiveBrownian:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, archive_size=10, brownian_motion_prob=0.1, brownian_step_size=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.archive_size = archive_size\n        self.archive = []\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.brownian_motion_prob = brownian_motion_prob\n        self.brownian_step_size = brownian_step_size\n\n\n    def initialize_population(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n    def mutate(self, pop, i):\n        if len(self.archive) > 0 and np.random.rand() < 0.1:  # Use archive with a probability\n            idx_p = np.random.choice(len(pop), 2, replace=False)\n            a, b = pop[idx_p]\n            arch_idx = np.random.randint(len(self.archive))\n            mutant = self.archive[arch_idx] + self.F * (a - b)\n        else:\n            idxs = np.random.choice(len(pop), 3, replace=False)\n            a, b, c = pop[idxs]\n            mutant = a + self.F * (b - c)\n        return np.clip(mutant, -5, 5)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def brownian_motion(self, x):\n        delta = np.random.normal(0, self.brownian_step_size, size=self.dim)\n        return np.clip(x + delta, -5, 5)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                target = self.population[i]\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, target)\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                        # Local search with Brownian motion\n                        if np.random.rand() < self.brownian_motion_prob:\n                            x_local = self.brownian_motion(trial)\n                            f_local = func(x_local)\n                            self.budget -= 1\n                            if f_local < self.f_opt:\n                                self.f_opt = f_local\n                                self.x_opt = x_local\n                            if f_local < f_trial:\n                                self.population[i] = x_local\n                                self.fitness[i] = f_local\n\n\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                    else:\n                        # Replace a random element in the archive\n                        idx = np.random.randint(self.archive_size)\n                        self.archive[idx] = trial\n\n                if self.budget <= 0:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm DEArchiveBrownian scored 0.634 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b71b2725-a1b7-4b5a-8745-5cd5f5675d04"], "operator": null, "metadata": {"aucs": [0.2091078441437243, 0.5057889380445562, 0.61013944711542, 0.8315998575458813, 0.6851587400331376, 0.7456555346468363, 0.6091051994416538, 0.5873004084650686, 0.715014356179002, 0.655148351441115, 0.7932111281847769, 0.9969645681871092, 0.3603951959854875, 0.6617420276527133, 0.8724725080969429, 0.7690745961669475, 0.5354782313529824, 0.8091838775360094, 0.2223060754160856, 0.5091397442093335]}}
{"id": "5c83120d-edd6-4504-b885-32f814a96b99", "fitness": 0.6278336967776335, "name": "AdaptiveDE", "description": "Differential Evolution with a dynamically adjusted population size, adaptive mutation strategies, and a local search component focusing on promising solutions.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, local_search_prob=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.local_search_prob = local_search_prob\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n\n    def mutate(self, i):\n        idxs = np.random.choice(self.pop_size, 3, replace=False)\n        a, b, c = self.population[idxs]\n\n        # Adaptive F: use different F values for different individuals\n        F_i = np.random.normal(self.F, 0.1)\n        F_i = np.clip(F_i, 0.1, 1.0)\n        \n        mutant = a + F_i * (b - c)\n        return np.clip(mutant, self.lb, self.ub)\n\n    def crossover(self, mutant, i):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, self.population[i])\n    \n    def local_search(self, x, func):\n        # Simple local search around x\n        delta = np.random.uniform(-0.1, 0.1, size=self.dim)\n        x_new = np.clip(x + delta, self.lb, self.ub)\n        f_new = func(x_new)\n        self.budget -= 1\n        return f_new, x_new\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            \n            # Dynamically adjust population size.  If the population is improving slowly, increase it.\n            if generation % 50 == 0:\n                improvement = np.mean(self.fitness) - self.f_opt\n                if improvement < 1e-3 and self.pop_size < 200:\n                    self.pop_size = min(self.pop_size + 10, 200)\n                    new_pop = np.random.uniform(self.lb, self.ub, size=(10, self.dim))\n                    new_fitness = np.array([func(x) for x in new_pop])\n                    self.budget -= 10\n                    self.population = np.vstack((self.population, new_pop))\n                    self.fitness = np.concatenate((self.fitness, new_fitness))\n                    \n            for i in range(self.pop_size):\n                mutant = self.mutate(i)\n                trial = self.crossover(mutant, i)\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        \n                        # Local search around the new best\n                        if np.random.rand() < self.local_search_prob:\n                            f_local, x_local = self.local_search(self.x_opt, func)\n                            if f_local < self.f_opt:\n                                self.f_opt = f_local\n                                self.x_opt = x_local\n                \n                # Local search around promising individuals with some probability\n                elif np.random.rand() < self.local_search_prob / 2.0:  # Reduced probability\n                    f_local, x_local = self.local_search(self.population[i], func)\n                    if f_local < self.fitness[i]:\n                        self.population[i] = x_local\n                        self.fitness[i] = f_local\n\n\n                if self.budget <= 0:\n                    break\n            if self.budget <= 0:\n                break\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDE scored 0.628 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["cb480dba-9dad-44b2-9951-eff1e0598739"], "operator": null, "metadata": {"aucs": [0.21470352025064376, 0.47094536772620554, 0.5833854071047566, 0.8228009201833597, 0.6551740229598904, 0.7474097337177038, 0.5817527195099867, 0.5780669261065372, 0.6936451091290079, 0.6116940864509903, 0.8113383505412582, 0.9887773723699439, 0.3316788032515917, 0.6606136125666862, 0.8631611587307381, 0.7678928163636727, 0.5202991068105101, 0.791563294611647, 0.34430916498761577, 0.5174624421799233]}}
{"id": "ae079774-00ee-4c5f-9c1a-14842515e6ec", "fitness": 0.5396705864545945, "name": "FitnessAwareDEShrinkingLocalSearch", "description": "Differential Evolution with a fitness-aware mutation scaling factor and a shrinking perturbation local search.", "code": "import numpy as np\n\nclass FitnessAwareDEShrinkingLocalSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, local_search_prob=0.1, initial_local_search_radius=0.1, radius_decay=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.local_search_prob = local_search_prob\n        self.initial_local_search_radius = initial_local_search_radius\n        self.radius_decay = radius_decay\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.local_search_radius = initial_local_search_radius\n\n    def initialize_population(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n    def mutate(self, pop, i):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n        # Fitness-aware scaling factor\n        fitness_diff = np.abs(self.fitness[idxs[1]] - self.fitness[idxs[2]])\n        F_adaptive = self.F * (1 + fitness_diff)  # Scale F based on fitness difference\n        mutant = a + F_adaptive * (b - c)\n        return np.clip(mutant, -5, 5)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def local_search(self, x, func):\n        # Perform a shrinking perturbation local search around x\n        best_x = x\n        best_f = func(x)\n        self.budget -= 1\n        if self.budget <= 0:\n            return best_x, best_f\n\n        for _ in range(5):  # Limited local search evaluations\n            new_x = x + np.random.uniform(-self.local_search_radius, self.local_search_radius, size=self.dim)\n            new_x = np.clip(new_x, -5, 5)\n            new_f = func(new_x)\n            self.budget -= 1\n            if self.budget <= 0:\n                return best_x, best_f\n\n            if new_f < best_f:\n                best_f = new_f\n                best_x = new_x\n\n        self.local_search_radius *= self.radius_decay #Shrink the radius\n        return best_x, best_f\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                target = self.population[i]\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, target)\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    # Local search trigger based on fitness improvement\n                    improvement = self.fitness[i] - f_trial\n                    probability = min(1.0, self.local_search_prob * improvement)  # Probability based on improvement\n                    if np.random.rand() < probability:\n                        trial, f_trial = self.local_search(trial, func)\n\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm FitnessAwareDEShrinkingLocalSearch scored 0.540 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9f314d8c-ae4b-45dc-a5d9-7be948c15c37"], "operator": null, "metadata": {"aucs": [0.1297258861363496, 0.18549641860440458, 0.5870612052498676, 0.7007834669800785, 0.5589348830237253, 0.6688363896152327, 0.5180174591582932, 0.46891415889303045, 0.5716105333599952, 0.39680011525637315, 0.6861802581828573, 0.9971842705747268, 0.24731305758903532, 0.5922085158380688, 0.8502800502077652, 0.7358136097993297, 0.45661045951305246, 0.759103913928342, 0.16993226775795123, 0.5126048094234088]}}
{"id": "643aa7d8-4f9a-4b07-87de-f9a5f9d2541d", "fitness": 0.6355457485073389, "name": "AdaptiveDEPopSize", "description": "Simplified Adaptive Differential Evolution with population size adjustment based on stagnation and a moving average for F and CR adaptation.", "code": "import numpy as np\n\nclass AdaptiveDEPopSize:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, F=0.5, CR=0.7, stagnation_threshold=500, pop_size_increase_factor=1.5, pop_size_decrease_factor=0.5, min_pop_size=20, max_pop_size=200, history_length=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.initial_pop_size = initial_pop_size\n        self.F = F\n        self.CR = CR\n        self.population = None\n        self.fitness = None\n        self.stagnation_threshold = stagnation_threshold\n        self.pop_size_increase_factor = pop_size_increase_factor\n        self.pop_size_decrease_factor = pop_size_decrease_factor\n        self.min_pop_size = min_pop_size\n        self.max_pop_size = max_pop_size\n        self.generation = 0\n        self.stagnation_counter = 0\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.history_length = history_length\n        self.F_history = [F] * history_length\n        self.CR_history = [CR] * history_length\n        self.history_idx = 0\n\n    def initialize_population(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n    def mutate(self, pop, i):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n        mutant = a + self.F * (b - c)\n        return np.clip(mutant, -5, 5)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def adjust_population_size(self, func):\n        if self.stagnation_counter >= self.stagnation_threshold:\n            new_pop_size = int(self.pop_size * self.pop_size_increase_factor)\n            new_pop_size = min(new_pop_size, self.max_pop_size)\n            \n            if new_pop_size > self.pop_size and self.budget > 0:\n                num_new_individuals = min(new_pop_size - self.pop_size, self.budget)\n                lb = func.bounds.lb\n                ub = func.bounds.ub\n                new_individuals = np.random.uniform(lb, ub, size=(num_new_individuals, self.dim))\n                new_fitness = np.array([func(x) for x in new_individuals])\n                self.budget -= num_new_individuals\n                \n                self.population = np.vstack((self.population, new_individuals))\n                self.fitness = np.concatenate((self.fitness, new_fitness))\n                self.pop_size = new_pop_size\n                self.stagnation_counter = 0\n\n                best_index = np.argmin(self.fitness)\n                if self.fitness[best_index] < self.f_opt:\n                    self.f_opt = self.fitness[best_index]\n                    self.x_opt = self.population[best_index]\n\n        elif self.generation > self.stagnation_threshold and self.stagnation_counter < self.stagnation_threshold / 2 and self.initial_pop_size < self.pop_size:\n            new_pop_size = int(self.pop_size * self.pop_size_decrease_factor)\n            new_pop_size = max(new_pop_size, self.min_pop_size)\n\n            if new_pop_size < self.pop_size:\n                indices = np.argsort(self.fitness)[:new_pop_size]\n                self.population = self.population[indices]\n                self.fitness = self.fitness[indices]\n                self.pop_size = new_pop_size\n\n    def update_parameters(self, successful_F, successful_CR):\n        if successful_F:\n            self.F_history[self.history_idx] = np.mean(successful_F)\n            self.CR_history[self.history_idx] = np.mean(successful_CR)\n            self.history_idx = (self.history_idx + 1) % self.history_length\n            self.F = np.clip(np.mean(self.F_history), 0.1, 0.9)\n            self.CR = np.clip(np.mean(self.CR_history), 0.1, 0.9)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            best_fitness_before_gen = self.f_opt\n            successful_F = []\n            successful_CR = []\n\n            for i in range(self.pop_size):\n                target = self.population[i]\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, target)\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    successful_F.append(self.F)\n                    successful_CR.append(self.CR)\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n\n            self.update_parameters(successful_F, successful_CR)\n            self.generation += 1\n\n            if self.f_opt < best_fitness_before_gen:\n                self.stagnation_counter = 0\n            else:\n                self.stagnation_counter += 1\n\n            self.adjust_population_size(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDEPopSize scored 0.636 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b71b2725-a1b7-4b5a-8745-5cd5f5675d04"], "operator": null, "metadata": {"aucs": [0.2164519819830928, 0.4455460210404274, 0.5960793379535818, 0.8163307579449538, 0.7064668706872848, 0.7437009260573004, 0.5642012122151719, 0.5858111136926256, 0.6867111793183114, 0.64882835072511, 0.8053831544831518, 0.9953524556638992, 0.33334146479654614, 0.6719365983778778, 0.8842626953051289, 0.7517267207833904, 0.5502474747500605, 0.8136644722764896, 0.3801555986089443, 0.5147165834834313]}}
{"id": "400d9ace-f6a0-45ab-a012-cacd6778ee82", "fitness": 0.0, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with mirrored sampling, covariance matrix adaptation and dynamic parameter control.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=None, F=0.5, CR=0.7, local_search_prob=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 5 * self.dim  # Adjust default pop_size\n        self.F = F\n        self.CR = CR\n        self.local_search_prob = local_search_prob\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = None\n        self.fitness = None\n        self.archive = []\n        self.archive_fitness = []\n        self.success_F = []\n        self.success_CR = []\n        self.H = 10\n        self.memory_F = np.full(self.H, F)\n        self.memory_CR = np.full(self.H, CR)\n        self.memory_idx = 0\n        self.C = 1 / np.sqrt(dim)\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n\n    def mutate(self, i):\n        idxs = np.random.choice(self.pop_size, 3, replace=False)\n        a, b, c = self.population[idxs]\n\n        # Adaptive F: use memory and a Cauchy distribution\n        mu_F = self.memory_F[self.memory_idx]\n        F_i = mu_F + 0.1 * np.tan(np.pi * (np.random.rand() - 0.5))\n        F_i = np.clip(F_i, 0.1, 1.0)\n        \n        mutant = a + F_i * (b - c)\n        \n        #Mirrored sampling\n        if np.random.rand() < 0.1:\n            mutant = a + F_i * (np.random.uniform(self.lb, self.ub, size=self.dim) - self.population[i])\n        \n        return np.clip(mutant, self.lb, self.ub)\n\n    def crossover(self, mutant, i):\n        # Adaptive CR: use memory and a normal distribution\n        mu_CR = self.memory_CR[self.memory_idx]\n        CR_i = np.random.normal(mu_CR, 0.1)\n        CR_i = np.clip(CR_i, 0.0, 1.0)\n        \n        cross_points = np.random.rand(self.dim) < CR_i\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, self.population[i])\n    \n    def local_search(self, x, func):\n        # CMA-ES like local search\n        delta = np.random.normal(0, self.C, size=self.dim)\n        x_new = np.clip(x + delta, self.lb, self.ub)\n        f_new = func(x_new)\n        self.budget -= 1\n        return f_new, x_new\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            \n            # Dynamic population size (simplified)\n            if generation % 50 == 0 and self.pop_size < 100:\n                self.pop_size = min(self.pop_size + 5, 100)\n                new_pop = np.random.uniform(self.lb, self.ub, size=(5, self.dim))\n                new_fitness = np.array([func(x) for x in new_pop])\n                self.budget -= 5\n                self.population = np.vstack((self.population, new_pop))\n                self.fitness = np.concatenate((self.fitness, new_fitness))\n                \n            trial_fitnesses = []\n            trial_vectors = []\n            for i in range(self.pop_size):\n                mutant = self.mutate(i)\n                trial = self.crossover(mutant, i)\n                f_trial = func(trial)\n                self.budget -= 1\n                trial_fitnesses.append(f_trial)\n                trial_vectors.append(trial)\n\n            trial_fitnesses = np.array(trial_fitnesses)\n            trial_vectors = np.array(trial_vectors)\n\n            for i in range(self.pop_size):\n                if trial_fitnesses[i] < self.fitness[i]:\n                    self.success_F.append(self.memory_F[self.memory_idx])\n                    self.success_CR.append(self.memory_CR[self.memory_idx])\n                    \n                    self.population[i] = trial_vectors[i]\n                    self.fitness[i] = trial_fitnesses[i]\n                    \n                    if trial_fitnesses[i] < self.f_opt:\n                        self.f_opt = trial_fitnesses[i]\n                        self.x_opt = trial_vectors[i]\n                        \n                        if np.random.rand() < self.local_search_prob:\n                            f_local, x_local = self.local_search(self.x_opt, func)\n                            if f_local < self.f_opt:\n                                self.f_opt = f_local\n                                self.x_opt = x_local\n\n                elif np.random.rand() < self.local_search_prob / 2.0:\n                    f_local, x_local = self.local_search(self.population[i], func)\n                    if f_local < self.fitness[i]:\n                        self.population[i] = x_local\n                        self.fitness[i] = f_local\n            \n            # Update Memory\n            if self.success_F:\n                self.memory_F[self.memory_idx] = np.mean(self.success_F)\n                self.memory_CR[self.memory_idx] = np.mean(self.success_CR) if self.success_CR else self.memory_CR[self.memory_idx]\n            \n            self.success_F = []\n            self.success_CR = []\n            self.memory_idx = (self.memory_idx + 1) % self.H\n\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5c83120d-edd6-4504-b885-32f814a96b99"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "99071cc5-dc3a-4498-bf04-3aae3e8e7c33", "fitness": -Infinity, "name": "OrthogonalAdaptiveDE", "description": "Adaptive Differential Evolution with orthogonal learning, dynamically adjusting F and CR, and population size control based on the performance of orthogonal design.", "code": "import numpy as np\n\nclass OrthogonalAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, F=0.5, CR=0.7, stagnation_threshold=500, pop_size_increase_factor=1.5, pop_size_decrease_factor=0.5, min_pop_size=20, max_pop_size=200, history_length=10, orthogonal_sample_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.initial_pop_size = initial_pop_size\n        self.F = F\n        self.CR = CR\n        self.population = None\n        self.fitness = None\n        self.stagnation_threshold = stagnation_threshold\n        self.pop_size_increase_factor = pop_size_increase_factor\n        self.pop_size_decrease_factor = pop_size_decrease_factor\n        self.min_pop_size = min_pop_size\n        self.max_pop_size = max_pop_size\n        self.generation = 0\n        self.stagnation_counter = 0\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.history_length = history_length\n        self.F_history = [F] * history_length\n        self.CR_history = [CR] * history_length\n        self.history_idx = 0\n        self.orthogonal_sample_size = orthogonal_sample_size\n\n    def initialize_population(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n    def mutate(self, pop, i):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n        mutant = a + self.F * (b - c)\n        return np.clip(mutant, -5, 5)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def orthogonal_design(self, func, individual):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        dim = self.dim\n        sample_size = self.orthogonal_sample_size\n        \n        # Generate orthogonal array (simplified version)\n        orthogonal_array = np.random.choice([-1, 1], size=(sample_size, dim)) # Simplified OA\n\n        samples = np.zeros((sample_size, dim))\n        for i in range(sample_size):\n            for j in range(dim):\n                samples[i, j] = individual[j] + orthogonal_array[i, j] * (ub - lb) * 0.05  # Small perturbation\n                samples[i, j] = np.clip(samples[i, j], lb, ub)\n                \n        fitness_values = np.array([func(x) for x in samples])\n        self.budget -= sample_size\n\n        best_index = np.argmin(fitness_values)\n        \n        if fitness_values[best_index] < func(individual):\n            return samples[best_index], fitness_values[best_index]\n        else:\n            return individual, func(individual)\n        \n    def adjust_population_size(self, func, improvement):\n        if self.stagnation_counter >= self.stagnation_threshold:\n            new_pop_size = int(self.pop_size * self.pop_size_increase_factor)\n            new_pop_size = min(new_pop_size, self.max_pop_size)\n            \n            if new_pop_size > self.pop_size and self.budget > 0:\n                num_new_individuals = min(new_pop_size - self.pop_size, self.budget)\n                lb = func.bounds.lb\n                ub = func.bounds.ub\n                new_individuals = np.random.uniform(lb, ub, size=(num_new_individuals, self.dim))\n                new_fitness = np.array([func(x) for x in new_individuals])\n                self.budget -= num_new_individuals\n                \n                self.population = np.vstack((self.population, new_individuals))\n                self.fitness = np.concatenate((self.fitness, new_fitness))\n                self.pop_size = new_pop_size\n                self.stagnation_counter = 0\n\n                best_index = np.argmin(self.fitness)\n                if self.fitness[best_index] < self.f_opt:\n                    self.f_opt = self.fitness[best_index]\n                    self.x_opt = self.population[best_index]\n\n        elif self.generation > self.stagnation_threshold and self.stagnation_counter < self.stagnation_threshold / 2 and self.initial_pop_size < self.pop_size:\n            new_pop_size = int(self.pop_size * self.pop_size_decrease_factor)\n            new_pop_size = max(new_pop_size, self.min_pop_size)\n\n            if new_pop_size < self.pop_size:\n                indices = np.argsort(self.fitness)[:new_pop_size]\n                self.population = self.population[indices]\n                self.fitness = self.fitness[indices]\n                self.pop_size = new_pop_size\n\n    def update_parameters(self, successful_F, successful_CR):\n        if successful_F:\n            self.F_history[self.history_idx] = np.mean(successful_F)\n            self.CR_history[self.history_idx] = np.mean(successful_CR)\n            self.history_idx = (self.history_idx + 1) % self.history_length\n            self.F = np.clip(np.mean(self.F_history), 0.1, 0.9)\n            self.CR = np.clip(np.mean(self.CR_history), 0.1, 0.9)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            best_fitness_before_gen = self.f_opt\n            successful_F = []\n            successful_CR = []\n            improvement = False\n\n            for i in range(self.pop_size):\n                target = self.population[i]\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, target)\n                \n                trial, f_trial = self.orthogonal_design(func, trial)\n                \n                if f_trial < self.fitness[i]:\n                    successful_F.append(self.F)\n                    successful_CR.append(self.CR)\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        improvement = True\n\n                if self.budget <= 0:\n                    break\n\n            self.update_parameters(successful_F, successful_CR)\n            self.generation += 1\n\n            if self.f_opt < best_fitness_before_gen:\n                self.stagnation_counter = 0\n            else:\n                self.stagnation_counter += 1\n\n            self.adjust_population_size(func, improvement)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "An exception occurred: setting an array element with a sequence..", "error": "", "parent_ids": ["643aa7d8-4f9a-4b07-87de-f9a5f9d2541d"], "operator": null, "metadata": {}}
{"id": "86bcd6c7-5d6a-41a1-93df-78b31c0ddd2c", "fitness": 0.4876163283581418, "name": "AdaptiveDEOBL", "description": "Adaptive Differential Evolution with orthogonal learning and dynamic parameter control, utilizing orthogonal experimental design to efficiently explore the search space and dynamically adjust parameters based on the population's diversity and performance.", "code": "import numpy as np\nfrom scipy.stats import norm\n\nclass AdaptiveDEOBL:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, update_interval=1000, obl_samples=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR # Crossover rate\n        self.population = None\n        self.fitness = None\n        self.update_interval = update_interval\n        self.success_F_history = []\n        self.success_CR_history = []\n        self.lb = -5.0\n        self.ub = 5.0\n        self.obl_samples = obl_samples  # Number of samples for orthogonal learning\n        self.diversity_threshold = 0.01 # Diversity threshold\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n    def mutate(self, pop, i):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n        mutant = a + self.F * (b - c)\n        return np.clip(mutant, self.lb, self.ub)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def orthogonal_learning(self, target):\n        # Generate OBL samples around the target\n        obl_points = []\n        for _ in range(self.obl_samples):\n            sample = self.lb + self.ub - target\n            obl_points.append(sample)\n            \n        return obl_points\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        evals = 0\n\n        while self.budget > 0:\n            # Calculate population diversity\n            diversity = np.std(self.fitness)\n            \n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n                target = self.population[i]\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, target)\n                f_trial = func(trial)\n                self.budget -= 1\n                evals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.success_F_history.append(self.F)\n                    self.success_CR_history.append(self.CR)\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                # Orthogonal Learning if diversity is low\n                if diversity < self.diversity_threshold and self.budget > self.obl_samples:\n                     obl_points = self.orthogonal_learning(target)\n                     obl_fitness = []\n                     for obl_point in obl_points:\n                         obl_fitness.append(func(obl_point))\n                         self.budget -= 1\n                         evals += 1\n                         \n                     best_obl_index = np.argmin(obl_fitness)\n                     if obl_fitness[best_obl_index] < self.fitness[i]:\n                         self.population[i] = obl_points[best_obl_index]\n                         self.fitness[i] = obl_fitness[best_obl_index]\n                         if obl_fitness[best_obl_index] < self.f_opt:\n                             self.f_opt = obl_fitness[best_obl_index]\n                             self.x_opt = obl_points[best_obl_index]\n                    \n\n            if evals >= self.update_interval:\n                if self.success_F_history:\n                    self.F = np.mean(self.success_F_history)\n                    self.CR = np.mean(self.success_CR_history)\n                else:\n                    # If no successful mutations, increase exploration\n                    self.F = np.random.uniform(0.5, 0.9)\n                    self.CR = np.random.uniform(0.5, 0.9)\n\n                self.F = np.clip(self.F, 0.1, 0.9)\n                self.CR = np.clip(self.CR, 0.1, 0.9)\n                self.success_F_history = []\n                self.success_CR_history = []\n                evals = 0\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveDEOBL scored 0.488 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e316ffc9-e5d2-4979-828a-22ec8a77dfb8"], "operator": null, "metadata": {"aucs": [0.2133753573319298, 0.3508323721749408, 0.3963953510109878, 0.7476101264661144, 0.47802134687721987, 0.5236130596084332, 0.3894817870771813, 0.4144332133188475, 0.4662640814444188, 0.4487620139423707, 0.6526921077269681, 0.9912938205634468, 0.31512517914083504, 0.5072651716913071, 0.7447785868189047, 0.4826602956810673, 0.40183081867637405, 0.5362863088434302, 0.21515999842060118, 0.4764455703474577]}}
{"id": "e40aa0b9-0f2a-4f93-aecc-4ed41406de41", "fitness": 0.6179976811274693, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with enhanced parameter adaptation using a moving average for F and CR, and a population size adaptation based on fitness improvement rate, further incorporating a simple restart mechanism upon stagnation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, update_interval=1000, stagnation_threshold=1e-6, stagnation_generations=500):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR # Crossover rate\n        self.population = None\n        self.fitness = None\n        self.update_interval = update_interval\n        self.success_F_history = []\n        self.success_CR_history = []\n        self.lb = -5.0\n        self.ub = 5.0\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_generations = stagnation_generations\n        self.best_fitness_history = []\n        self.generations_since_last_improvement = 0\n        self.F_mavg = F\n        self.CR_mavg = CR\n        self.mavg_window = 10\n\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.best_fitness_history.append(self.f_opt)\n\n\n    def mutate(self, pop, i):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n        mutant = a + self.F * (b - c)\n        return np.clip(mutant, self.lb, self.ub)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def adjust_population_size(self):\n        improvement_rate = 0\n        if len(self.best_fitness_history) > 1:\n            improvement_rate = (self.best_fitness_history[-2] - self.best_fitness_history[-1]) / abs(self.best_fitness_history[-2])\n\n        if improvement_rate < self.stagnation_threshold:\n            self.pop_size = int(self.pop_size * 1.1)  # Increase population size\n        else:\n            self.pop_size = int(self.pop_size * 0.9)  # Decrease population size\n\n        self.pop_size = np.clip(self.pop_size, 10, 100)  # Keep population size within bounds\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        evals = 0\n\n        while self.budget > 0:\n            \n            \n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n                target = self.population[i]\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, target)\n                f_trial = func(trial)\n                self.budget -= 1\n                evals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.success_F_history.append(self.F)\n                    self.success_CR_history.append(self.CR)\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.generations_since_last_improvement = 0\n                        \n            self.best_fitness_history.append(self.f_opt)\n            self.generations_since_last_improvement += 1\n\n            if evals >= self.update_interval:\n                if self.success_F_history:\n                    self.F_mavg = 0.9 * self.F_mavg + 0.1 * np.mean(self.success_F_history)\n                    self.CR_mavg = 0.9 * self.CR_mavg + 0.1 * np.mean(self.success_CR_history)\n                self.F = np.clip(self.F_mavg, 0.1, 0.9)\n                self.CR = np.clip(self.CR_mavg, 0.1, 0.9)\n                self.success_F_history = []\n                self.success_CR_history = []\n                evals = 0\n                #self.adjust_population_size()\n\n            if self.generations_since_last_improvement > self.stagnation_generations:\n                # Restart if stagnating\n                self.initialize_population(func)\n                self.generations_since_last_improvement = 0\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveDE scored 0.618 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e316ffc9-e5d2-4979-828a-22ec8a77dfb8"], "operator": null, "metadata": {"aucs": [0.19351338039962707, 0.4223639853209884, 0.6035987927484808, 0.812407311311659, 0.7097549575542947, 0.7539512508035413, 0.5569254694828081, 0.5860772638012042, 0.6386106200543684, 0.6288330382728708, 0.7920403087338469, 0.9935757270448138, 0.3551899217809589, 0.6663919527915241, 0.8651982184626505, 0.7548703796796802, 0.4971828076322343, 0.8038623880673204, 0.20816777522004581, 0.5174380733864686]}}
{"id": "e7c348ad-7679-4d4e-808a-6bd2868fcf3b", "fitness": 0.5104168262001061, "name": "SelfAdaptiveDECauchy", "description": "Differential Evolution with a self-adaptive strategy based on fitness improvement to adjust F and CR, combined with a local search using a Cauchy distribution.", "code": "import numpy as np\n\nclass SelfAdaptiveDECauchy:\n    def __init__(self, budget=10000, dim=10, pop_size=50, initial_F=0.5, initial_CR=0.7, local_search_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = initial_F\n        self.CR = initial_CR\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.local_search_prob = local_search_prob\n\n    def initialize_population(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n    def mutate(self, pop, i):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n        mutant = a + self.F * (b - c)\n        return np.clip(mutant, -5, 5)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def local_search(self, x, func):\n        # Cauchy distribution for local search step size\n        step_size = np.random.standard_cauchy(size=self.dim) * 0.01  # Scale the Cauchy distribution\n        new_x = x + step_size\n        new_x = np.clip(new_x, -5, 5)\n        f_new = func(new_x)\n        self.budget -= 1\n        return new_x, f_new\n    \n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                target = self.population[i]\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, target)\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n\n                    # Self-adaptive F and CR based on success\n                    self.F = np.clip(self.F * np.random.normal(1, 0.1), 0.1, 0.9)\n                    self.CR = np.clip(self.CR * np.random.normal(1, 0.1), 0.1, 0.9)\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                \n                # Local search with probability\n                if np.random.rand() < self.local_search_prob and self.budget > 0:\n                    new_x, f_new = self.local_search(self.population[i], func)\n                    if f_new < self.fitness[i]:\n                        self.population[i] = new_x\n                        self.fitness[i] = f_new\n                        if f_new < self.f_opt:\n                            self.f_opt = f_new\n                            self.x_opt = new_x\n\n\n                if self.budget <= 0:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm SelfAdaptiveDECauchy scored 0.510 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["643aa7d8-4f9a-4b07-87de-f9a5f9d2541d"], "operator": null, "metadata": {"aucs": [0.16687399456916918, 0.27613934229312853, 0.4667162840474276, 0.7211516207353073, 0.4141764791789697, 0.7109956718312093, 0.4096967378827203, 0.4899036871149327, 0.4836234725658872, 0.5082446378781751, 0.52920123654398, 0.9999146511431516, 0.2701080065749224, 0.34141039190956823, 0.8202839867105813, 0.7450624409974825, 0.3996359422473499, 0.7325978124784118, 0.2105659048221784, 0.5120342224775694]}}
{"id": "2a5ce8cb-d780-44b8-9460-c6bd0f3f5e01", "fitness": 0.32990827687180657, "name": "OrthogonalDE", "description": "Differential Evolution with Self-Adaptive parameters and orthogonal learning to diversify search.", "code": "import numpy as np\n\nclass OrthogonalDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, ol_factor=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR # Crossover rate\n        self.ol_factor = ol_factor # Orthogonal learning factor\n        self.population = None\n        self.fitness = None\n        self.success_F_history = []\n        self.success_CR_history = []\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n    def mutate(self, pop, i):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n        mutant = a + self.F * (b - c)\n        return np.clip(mutant, self.lb, self.ub)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def orthogonal_learning(self, pop, i, func):\n        # Select two parents\n        idxs = np.random.choice(len(pop), 2, replace=False)\n        parent1, parent2 = pop[idxs]\n\n        # Generate orthogonal array (simplified 2-level full factorial design)\n        oa = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n\n        # Create candidate solutions based on the orthogonal array\n        candidates = []\n        for row in oa:\n            candidate = np.copy(pop[i]) # Start with current solution\n            for j in range(self.dim):\n                if row[0] == 0:\n                    candidate[j] = parent1[j] + self.ol_factor * (parent2[j] - parent1[j])\n                else:\n                    candidate[j] = parent2[j] + self.ol_factor * (parent1[j] - parent2[j])\n                candidate[j] = np.clip(candidate[j], self.lb, self.ub) # Clip to bounds\n            candidates.append(candidate)\n        \n        # Evaluate candidates\n        fitness_values = [func(candidate) for candidate in candidates]\n        self.budget -= len(candidates)\n        \n        # Select the best candidate\n        best_index = np.argmin(fitness_values)\n        best_candidate = candidates[best_index]\n        best_fitness = fitness_values[best_index]\n        \n        return best_candidate, best_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n                target = self.population[i]\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, target)\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.success_F_history.append(self.F)\n                    self.success_CR_history.append(self.CR)\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                # Apply orthogonal learning\n                if self.budget > 0:\n                    ol_candidate, ol_fitness = self.orthogonal_learning(self.population, i, func)\n                    if ol_fitness < self.fitness[i]:\n                         self.population[i] = ol_candidate\n                         self.fitness[i] = ol_fitness\n                         if ol_fitness < self.f_opt:\n                             self.f_opt = ol_fitness\n                             self.x_opt = ol_candidate\n                \n\n            if self.success_F_history:\n                self.F = np.mean(self.success_F_history)\n                self.CR = np.mean(self.success_CR_history)\n            self.F = np.clip(self.F, 0.1, 0.9)\n            self.CR = np.clip(self.CR, 0.1, 0.9)\n            self.success_F_history = []\n            self.success_CR_history = []\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm OrthogonalDE scored 0.330 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e316ffc9-e5d2-4979-828a-22ec8a77dfb8"], "operator": null, "metadata": {"aucs": [0.15305993705319365, 0.21956074027088712, 0.3289935190034907, 0.2354364325386904, 0.21571903530949732, 0.2729383118086538, 0.2813500016859163, 0.27036766647201294, 0.3268006694143727, 0.1942998893238178, 0.2567924369854816, 0.9894372414794763, 0.2804442313867238, 0.2447980462857945, 0.6224003212931122, 0.34057836424763366, 0.2223611084546102, 0.42151453834016717, 0.23645158112816134, 0.48486146495443894]}}
{"id": "857609df-0c4e-4040-990b-4f66071ac500", "fitness": 0.6208391317875351, "name": "AdaptiveDE", "description": "Differential Evolution with adaptive parameters, simplified local search, and dynamic population size adjustment based on fitness variance.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, local_search_prob=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.local_search_prob = local_search_prob\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n\n    def mutate(self, i):\n        idxs = np.random.choice(self.pop_size, 3, replace=False)\n        a, b, c = self.population[idxs]\n\n        # Adaptive F: use different F values for different individuals\n        F_i = np.random.normal(self.F, 0.1)\n        F_i = np.clip(F_i, 0.1, 1.0)\n        \n        mutant = a + F_i * (b - c)\n        return np.clip(mutant, self.lb, self.ub)\n\n    def crossover(self, mutant, i):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, self.population[i])\n    \n    def local_search(self, x, func):\n        # Simplified local search around x\n        delta = np.random.normal(0, 0.1, size=self.dim)\n        x_new = np.clip(x + delta, self.lb, self.ub)\n        f_new = func(x_new)\n        self.budget -= 1\n        return f_new, x_new\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            \n            # Dynamically adjust population size based on fitness variance.\n            if generation % 50 == 0:\n                fitness_var = np.var(self.fitness)\n                if fitness_var < 1e-4 and self.pop_size < 100:\n                    self.pop_size = min(self.pop_size + 5, 100)\n                    new_pop = np.random.uniform(self.lb, self.ub, size=(5, self.dim))\n                    new_fitness = np.array([func(x) for x in new_pop])\n                    self.budget -= 5\n                    self.population = np.vstack((self.population, new_pop))\n                    self.fitness = np.concatenate((self.fitness, new_fitness))\n                elif fitness_var > 0.1 and self.pop_size > 20:\n                    self.pop_size = max(self.pop_size - 5, 20)\n                    self.population = self.population[:self.pop_size]\n                    self.fitness = self.fitness[:self.pop_size]\n            \n            for i in range(self.pop_size):\n                mutant = self.mutate(i)\n                trial = self.crossover(mutant, i)\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        \n                        # Local search around the new best with reduced probability\n                        if np.random.rand() < self.local_search_prob:\n                            f_local, x_local = self.local_search(self.x_opt, func)\n                            if f_local < self.f_opt:\n                                self.f_opt = f_local\n                                self.x_opt = x_local\n\n                if self.budget <= 0:\n                    break\n            if self.budget <= 0:\n                break\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveDE scored 0.621 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5c83120d-edd6-4504-b885-32f814a96b99"], "operator": null, "metadata": {"aucs": [0.21522499783214522, 0.46456689464326595, 0.5685809012161405, 0.824728876084169, 0.7283680428004891, 0.740069294564484, 0.5467200934961662, 0.5541098717021766, 0.6695220928093002, 0.5868212649122512, 0.7827452208682063, 0.9996604341447055, 0.37363065586407307, 0.6572371037096959, 0.869430136885195, 0.7594227569846707, 0.4838783667147949, 0.8115171345054979, 0.2722524696080567, 0.5082960264052196]}}
{"id": "7e8ea4f6-6b27-4263-bef8-367157246b38", "fitness": 0.5234835061190303, "name": "AdaptiveDECMA", "description": "Adaptive Differential Evolution with covariance matrix adaptation learning (CMA-ES) inspired mutation and dynamic F/CR adjustment.", "code": "import numpy as np\n\nclass AdaptiveDECMA:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, F=0.5, CR=0.7, stagnation_threshold=500, pop_size_increase_factor=1.5, pop_size_decrease_factor=0.5, min_pop_size=20, max_pop_size=200, history_length=10, CMA_decay=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.initial_pop_size = initial_pop_size\n        self.F = F\n        self.CR = CR\n        self.population = None\n        self.fitness = None\n        self.stagnation_threshold = stagnation_threshold\n        self.pop_size_increase_factor = pop_size_increase_factor\n        self.pop_size_decrease_factor = pop_size_decrease_factor\n        self.min_pop_size = min_pop_size\n        self.max_pop_size = max_pop_size\n        self.generation = 0\n        self.stagnation_counter = 0\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.history_length = history_length\n        self.F_history = [F] * history_length\n        self.CR_history = [CR] * history_length\n        self.history_idx = 0\n        self.mean = None\n        self.C = None\n        self.CMA_decay = CMA_decay # Decay factor for covariance matrix adaptation\n        self.sigma = 0.1 # Overall standard deviation\n        self.c_sigma = None\n        self.D = None\n\n    def initialize_population(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n        self.mean = self.x_opt.copy() if self.x_opt is not None else np.mean(self.population, axis=0)\n        self.C = np.eye(self.dim) # Initialize covariance matrix\n        self.c_sigma = np.zeros(self.dim) # Evolution path for sigma adaptation\n        self.D = np.ones(self.dim) # Diagonal elements of D for CMA-ES\n        \n\n    def mutate(self, pop, i):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n        z = np.random.normal(0, 1, self.dim)\n        mutant = a + self.F * (b - c) + self.sigma * np.dot(self.C,z) # CMA-ES inspired mutation\n        return np.clip(mutant, -5, 5)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def adjust_population_size(self, func):\n        if self.stagnation_counter >= self.stagnation_threshold:\n            new_pop_size = int(self.pop_size * self.pop_size_increase_factor)\n            new_pop_size = min(new_pop_size, self.max_pop_size)\n            \n            if new_pop_size > self.pop_size and self.budget > 0:\n                num_new_individuals = min(new_pop_size - self.pop_size, self.budget)\n                lb = func.bounds.lb\n                ub = func.bounds.ub\n                new_individuals = np.random.uniform(lb, ub, size=(num_new_individuals, self.dim))\n                new_fitness = np.array([func(x) for x in new_individuals])\n                self.budget -= num_new_individuals\n                \n                self.population = np.vstack((self.population, new_individuals))\n                self.fitness = np.concatenate((self.fitness, new_fitness))\n                self.pop_size = new_pop_size\n                self.stagnation_counter = 0\n\n                best_index = np.argmin(self.fitness)\n                if self.fitness[best_index] < self.f_opt:\n                    self.f_opt = self.fitness[best_index]\n                    self.x_opt = self.population[best_index]\n\n        elif self.generation > self.stagnation_threshold and self.stagnation_counter < self.stagnation_threshold / 2 and self.initial_pop_size < self.pop_size:\n            new_pop_size = int(self.pop_size * self.pop_size_decrease_factor)\n            new_pop_size = max(new_pop_size, self.min_pop_size)\n\n            if new_pop_size < self.pop_size:\n                indices = np.argsort(self.fitness)[:new_pop_size]\n                self.population = self.population[indices]\n                self.fitness = self.fitness[indices]\n                self.pop_size = new_pop_size\n\n    def update_parameters(self, successful_F, successful_CR):\n        if successful_F:\n            self.F_history[self.history_idx] = np.mean(successful_F)\n            self.CR_history[self.history_idx] = np.mean(successful_CR)\n            self.history_idx = (self.history_idx + 1) % self.history_length\n            self.F = np.clip(np.mean(self.F_history), 0.1, 0.9)\n            self.CR = np.clip(np.mean(self.CR_history), 0.1, 0.9)\n\n    def update_CMA(self):\n        # Select top individuals based on fitness\n        elite_indices = np.argsort(self.fitness)[:self.pop_size // 4] # Select top 25%\n        elite_individuals = self.population[elite_indices]\n\n        # Update the mean\n        new_mean = np.mean(elite_individuals, axis=0)\n        diff = new_mean - self.mean\n        self.mean = new_mean\n\n        #Update evolution path for sigma\n        self.c_sigma = self.CMA_decay*self.c_sigma + np.sqrt(1-self.CMA_decay)*(diff/self.sigma)\n\n        # Update covariance matrix\n        self.C = self.CMA_decay * self.C + (1 - self.CMA_decay) * np.outer(self.c_sigma, self.c_sigma)\n\n        # Update sigma using cumulation\n        self.sigma *= np.exp(0.2 * (np.linalg.norm(self.c_sigma)/np.sqrt(self.dim)-1))\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            best_fitness_before_gen = self.f_opt\n            successful_F = []\n            successful_CR = []\n\n            for i in range(self.pop_size):\n                target = self.population[i]\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, target)\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    successful_F.append(self.F)\n                    successful_CR.append(self.CR)\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n\n            self.update_parameters(successful_F, successful_CR)\n            self.update_CMA()\n            self.generation += 1\n\n            if self.f_opt < best_fitness_before_gen:\n                self.stagnation_counter = 0\n            else:\n                self.stagnation_counter += 1\n\n            self.adjust_population_size(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveDECMA scored 0.523 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["643aa7d8-4f9a-4b07-87de-f9a5f9d2541d"], "operator": null, "metadata": {"aucs": [0.16532252938943715, 0.26515019421454167, 0.5500783711373617, 0.7046933213321801, 0.6454903700386834, 0.6355024194750886, 0.41755255331304486, 0.4775536879040112, 0.4805502644743862, 0.41658565556991767, 0.6871595671478137, 0.9979883576574694, 0.2676026627802812, 0.48990424048360215, 0.8310850570587732, 0.613908671088945, 0.328989982464547, 0.7633348635464221, 0.24398721488842356, 0.4872301384156752]}}
{"id": "e8a6bdc0-cb3b-4578-8571-982d19ba2e0a", "fitness": 0.6218953371424146, "name": "DEArchiveAdaptive", "description": "Differential Evolution with simplified archive updates and adaptive F/CR parameters.", "code": "import numpy as np\n\nclass DEArchiveAdaptive:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.archive = []\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.F = 0.5\n        self.CR = 0.7\n        self.F_memory = [0.5]  # Memory for F adaptation\n        self.CR_memory = [0.7] # Memory for CR adaptation\n\n    def initialize_population(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n    def mutate(self, pop, i):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n        mutant = a + self.F * (b - c)\n\n        # Utilize archive information\n        if len(self.archive) > 0 and np.random.rand() < 0.1:\n            arch_idx = np.random.randint(len(self.archive))\n            mutant = a + self.F * (self.archive[arch_idx] - b) # Perturb based on archive\n            \n        return np.clip(mutant, -5, 5)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def update_archive(self, trial):\n        if len(self.archive) < self.archive_size:\n            self.archive.append(trial)\n        else:\n            idx = np.random.randint(self.archive_size)\n            self.archive[idx] = trial # Replace a random element\n\n    def adapt_parameters(self, success):\n        if success:\n            self.F_memory.append(self.F)\n            self.CR_memory.append(self.CR)\n\n        # Keep a maximum size for memory\n        if len(self.F_memory) > 10:\n            self.F_memory.pop(0)\n            self.CR_memory.pop(0)\n        \n        #Update F and CR\n        self.F = np.mean(self.F_memory)\n        self.CR = np.mean(self.CR_memory)\n        self.F = np.clip(self.F, 0.1, 0.9)\n        self.CR = np.clip(self.CR, 0.1, 0.9)\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                target = self.population[i]\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, target)\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    self.update_archive(trial) # Simple archive update\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                    self.adapt_parameters(True) # Adapt parameters if successful\n                else:\n                    self.adapt_parameters(False)\n\n                if self.budget <= 0:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm DEArchiveAdaptive scored 0.622 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["cfae23f3-6eca-4578-851f-15ac2dc3e441"], "operator": null, "metadata": {"aucs": [0.2170576978843568, 0.4208475519369703, 0.580485704156575, 0.8255938995447656, 0.7102221695227529, 0.7596559805954293, 0.5236225022968914, 0.5674497947264086, 0.6891358995814592, 0.6303833716448216, 0.7905830983407065, 0.98986177347024, 0.34620099525679315, 0.6438208072389174, 0.8819968373787336, 0.7686140436276734, 0.5307383373569208, 0.8055890417408055, 0.21381822093667835, 0.542229015610394]}}
{"id": "84a75486-8808-40a3-be76-cac1611dc4d8", "fitness": 0.36500937511211107, "name": "DESOM", "description": "Differential Evolution with a self-organizing map (SOM) to cluster individuals, guiding mutation and crossover based on cluster characteristics and incorporating local search.", "code": "import numpy as np\n\nclass DESOM:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, som_grid_size=5, local_search_prob=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.som_grid_size = som_grid_size\n        self.local_search_prob = local_search_prob\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = None\n        self.fitness = None\n        self.som = None\n        self.som_weights = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n        self.initialize_som()\n\n    def initialize_som(self):\n        self.som_weights = np.random.uniform(self.lb, self.ub, size=(self.som_grid_size, self.som_grid_size, self.dim))\n\n    def train_som(self, learning_rate=0.1, epochs=5):\n        for epoch in range(epochs):\n            for x in self.population:\n                bmu = self.find_bmu(x)\n                for i in range(self.som_grid_size):\n                    for j in range(self.som_grid_size):\n                        distance = np.sqrt((i - bmu[0])**2 + (j - bmu[1])**2)\n                        influence = np.exp(-distance**2 / (2 * (self.som_grid_size/2)**2))  # Gaussian neighborhood\n                        self.som_weights[i, j] += learning_rate * influence * (x - self.som_weights[i, j])\n\n    def find_bmu(self, x):\n        min_dist = np.inf\n        bmu = None\n        for i in range(self.som_grid_size):\n            for j in range(self.som_grid_size):\n                dist = np.linalg.norm(x - self.som_weights[i, j])\n                if dist < min_dist:\n                    min_dist = dist\n                    bmu = (i, j)\n        return bmu\n\n    def mutate(self, i):\n        bmu = self.find_bmu(self.population[i])\n        cluster_center = self.som_weights[bmu]\n\n        idxs = np.random.choice(self.pop_size, 2, replace=False)\n        a, b = self.population[idxs]\n\n        # Mutation towards cluster center\n        mutant = self.population[i] + self.F * (cluster_center - self.population[i]) + self.F * (a - b) \n        return np.clip(mutant, self.lb, self.ub)\n\n    def crossover(self, mutant, i):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, self.population[i])\n    \n    def local_search(self, x, func):\n        delta = np.random.uniform(-0.1, 0.1, size=self.dim)\n        x_new = np.clip(x + delta, self.lb, self.ub)\n        f_new = func(x_new)\n        self.budget -= 1\n        return f_new, x_new\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n\n            # Train SOM periodically\n            if generation % 20 == 0:\n                self.train_som()\n\n            for i in range(self.pop_size):\n                mutant = self.mutate(i)\n                trial = self.crossover(mutant, i)\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        if np.random.rand() < self.local_search_prob:\n                            f_local, x_local = self.local_search(self.x_opt, func)\n                            if f_local < self.f_opt:\n                                self.f_opt = f_local\n                                self.x_opt = x_local\n\n                elif np.random.rand() < self.local_search_prob / 2.0:\n                    f_local, x_local = self.local_search(self.population[i], func)\n                    if f_local < self.fitness[i]:\n                        self.population[i] = x_local\n                        self.fitness[i] = f_local\n\n                if self.budget <= 0:\n                    break\n            if self.budget <= 0:\n                break\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm DESOM scored 0.365 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5c83120d-edd6-4504-b885-32f814a96b99"], "operator": null, "metadata": {"aucs": [0.15692170324507515, 0.34836133143773773, 0.36693723591304583, 0.4132591314278278, 0.3021186915936088, 0.3996867366818956, 0.32702352208617014, 0.34363222153452366, 0.3299641506294333, 0.19268722209761335, 0.32063426884126056, 0.994059153137197, 0.28125210065750006, 0.3284193837462379, 0.7351931487646504, 0]}}
{"id": "5f99d9a2-afb9-425a-ae6d-bacca682c357", "fitness": 0.6183384394113951, "name": "AdaptiveDERestart", "description": "Adaptive Differential Evolution with a self-adaptive strategy for both F and CR parameters, and a restart mechanism to escape local optima.", "code": "import numpy as np\n\nclass AdaptiveDERestart:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, restart_threshold=1000):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR # Crossover rate\n        self.population = None\n        self.fitness = None\n        self.restart_threshold = restart_threshold\n        self.success_F_history = []\n        self.success_CR_history = []\n        self.lb = -5.0\n        self.ub = 5.0\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.best_fitness_history.append(self.f_opt)\n\n    def mutate(self, pop, i):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n        mutant = a + self.F * (b - c)\n        return np.clip(mutant, self.lb, self.ub)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n                target = self.population[i]\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, target)\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.success_F_history.append(self.F)\n                    self.success_CR_history.append(self.CR)\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.stagnation_counter = 0  # Reset stagnation counter\n                    else:\n                        self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter += 1\n            \n            # Adaptive F and CR\n            if self.success_F_history:\n                self.F = 0.9 * self.F + 0.1 * np.mean(self.success_F_history)\n                self.CR = 0.9 * self.CR + 0.1 * np.mean(self.success_CR_history)\n            self.F = np.clip(self.F, 0.1, 0.9)\n            self.CR = np.clip(self.CR, 0.1, 0.9)\n            self.success_F_history = []\n            self.success_CR_history = []\n\n            # Restart mechanism\n            if self.stagnation_counter > self.restart_threshold:\n                self.initialize_population(func)  # Re-initialize population\n                self.stagnation_counter = 0  # Reset stagnation counter\n                self.F = 0.5\n                self.CR = 0.7\n            \n            self.best_fitness_history.append(self.f_opt)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveDERestart scored 0.618 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e316ffc9-e5d2-4979-828a-22ec8a77dfb8"], "operator": null, "metadata": {"aucs": [0.21754176674461811, 0.4520033028984216, 0.5908263912486433, 0.8340427032562028, 0.69566775600686, 0.7439395579266481, 0.6065823650838827, 0.5817408092689846, 0.6810845991876626, 0.6227486047955482, 0.7813924156274654, 0.998905621416029, 0.283880168186741, 0.6412257892217577, 0.8917609675560917, 0.760022000994309, 0.4781671106612049, 0.810427345875886, 0.19033277828402317, 0.504476733986921]}}
{"id": "84367826-543c-49e4-9156-d00e53ca95c0", "fitness": 0.4546682951874274, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with simplified parameter adaptation, reduced memory usage, and enhanced exploration.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, lr_F=0.1, lr_CR=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR # Crossover rate\n        self.population = None\n        self.fitness = None\n        self.lb = -5.0\n        self.ub = 5.0\n        self.lr_F = lr_F  # Learning rate for F\n        self.lr_CR = lr_CR # Learning rate for CR\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n    def mutate(self, pop, i):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n        mutant = a + self.F * (b - c)\n        return np.clip(mutant, self.lb, self.ub)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n                target = self.population[i]\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, target)\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    # Update F and CR based on success\n                    self.F = (1 - self.lr_F) * self.F + self.lr_F * np.random.uniform(0.4, 0.6)\n                    self.CR = (1 - self.lr_CR) * self.CR + self.lr_CR * np.random.uniform(0.6, 0.8)\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                else:\n                    # Dampen F and CR if the trial is not successful\n                    self.F *= (1 - self.lr_F)\n                    self.CR *= (1 - self.lr_CR)\n\n\n                self.F = np.clip(self.F, 0.1, 0.9)\n                self.CR = np.clip(self.CR, 0.1, 0.9)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveDE scored 0.455 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e316ffc9-e5d2-4979-828a-22ec8a77dfb8"], "operator": null, "metadata": {"aucs": [0.13831093587289356, 0.22297065235033586, 0.461772246758239, 0.33020974041196927, 0.3858224402920629, 0.700931165750946, 0.3730935279901365, 0.4365058480254277, 0.5455432491731114, 0.19294757169143617, 0.38878092361723804, 0.9998423203193969, 0.2820831388079236, 0.3679931364852733, 0.7896926434196571, 0.707720358499545, 0.4013547256542933, 0.6271122494910926, 0.24178061176654386, 0.49889841737102536]}}
{"id": "da7b633b-eecd-4629-8b1c-800a2cdba4e8", "fitness": -Infinity, "name": "DEArchiveAdaptiveHistory", "description": "Differential Evolution with a history-based mutation strategy, adaptive F/CR parameters using a weighted average, and an archive to preserve and utilize promising solutions.", "code": "import numpy as np\n\nclass DEArchiveAdaptiveHistory:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=15, history_length=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.history_length = history_length\n        self.archive = []\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.F = 0.5\n        self.CR = 0.7\n        self.F_history = np.full(history_length, 0.5)\n        self.CR_history = np.full(history_length, 0.7)\n        self.history_index = 0\n        self.success_count = 0\n        self.weighted_avg = lambda x, w: np.sum(x * w) / np.sum(w)\n\n    def initialize_population(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n    def mutate(self, pop, i):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n        \n        # History-based mutation with a chance to use archive\n        if len(self.archive) > 0 and np.random.rand() < 0.2:\n            arch_idx = np.random.randint(len(self.archive))\n            mutant = a + self.F * (self.archive[arch_idx] - b)\n        else:\n            mutant = a + self.F * (b - c)\n\n        return np.clip(mutant, -5, 5)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def update_archive(self, trial, fitness):\n        if len(self.archive) < self.archive_size:\n            self.archive.append(trial)\n        else:\n            # Replace least fit in archive\n            archive_fitness = [func(x) for x in self.archive] # Re-evaluate archive fitness\n            worst_index = np.argmax(archive_fitness)\n            if fitness < archive_fitness[worst_index]:\n                self.archive[worst_index] = trial\n\n    def adapt_parameters(self, success):\n        if success:\n            self.success_count += 1\n            self.F_history[self.history_index] = self.F\n            self.CR_history[self.history_index] = self.CR\n        else:\n            # If no success in a while, perturb F and CR\n            if self.success_count == 0:\n                self.F = np.clip(self.F * np.random.uniform(0.8, 1.2), 0.1, 0.9)\n                self.CR = np.clip(self.CR * np.random.uniform(0.8, 1.2), 0.1, 0.9)\n\n        self.history_index = (self.history_index + 1) % self.history_length\n        self.success_count = max(0, self.success_count - 1) # Decay success\n\n        # Weighted average for F and CR based on history\n        weights = np.arange(1, self.history_length + 1)\n        self.F = np.clip(self.weighted_avg(self.F_history, weights), 0.1, 0.9)\n        self.CR = np.clip(self.weighted_avg(self.CR_history, weights), 0.1, 0.9)\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                target = self.population[i]\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, target)\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    self.update_archive(trial, f_trial)\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                    self.adapt_parameters(True)\n                else:\n                    self.adapt_parameters(False)\n\n                if self.budget <= 0:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "An exception occurred: name 'func' is not defined.", "error": "", "parent_ids": ["e8a6bdc0-cb3b-4578-8571-982d19ba2e0a"], "operator": null, "metadata": {}}
{"id": "3eb6d977-7d6b-4ef1-bf14-c65e8a86c458", "fitness": -Infinity, "name": "AdaptiveDE", "description": "Differential Evolution with adaptive F/CR, dynamic population size based on fitness variance, and a more robust local search strategy utilizing a CMA-ES inspired approach.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, local_search_prob=0.05, local_search_iterations=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.local_search_prob = local_search_prob\n        self.local_search_iterations = local_search_iterations\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n\n    def mutate(self, i):\n        idxs = np.random.choice(self.pop_size, 3, replace=False)\n        a, b, c = self.population[idxs]\n\n        # Adaptive F: use different F values for different individuals\n        F_i = np.random.normal(self.F, 0.1)\n        F_i = np.clip(F_i, 0.1, 1.0)\n        \n        mutant = a + F_i * (b - c)\n        return np.clip(mutant, self.lb, self.ub)\n\n    def crossover(self, mutant, i):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, self.population[i])\n    \n    def local_search(self, x, func):\n        # CMA-ES inspired local search\n        def obj_func(x_local):\n            f = func(x_local)\n            self.budget -= 1\n            return f\n\n        bounds = [(self.lb, self.ub)] * self.dim\n        res = minimize(obj_func, x, method='L-BFGS-B', bounds=bounds, options={'maxiter': self.local_search_iterations})\n\n        if self.budget <= 0:\n          return np.inf, x\n\n        return res.fun, res.x\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            \n            # Dynamically adjust population size based on fitness variance.\n            if generation % 50 == 0:\n                fitness_var = np.var(self.fitness)\n                if fitness_var < 1e-4 and self.pop_size < 100:\n                    self.pop_size = min(self.pop_size + 5, 100)\n                    new_pop = np.random.uniform(self.lb, self.ub, size=(5, self.dim))\n                    new_fitness = np.array([func(x) for x in new_pop])\n                    self.budget -= 5\n                    self.population = np.vstack((self.population, new_pop))\n                    self.fitness = np.concatenate((self.fitness, new_fitness))\n                elif fitness_var > 0.1 and self.pop_size > 20:\n                    self.pop_size = max(self.pop_size - 5, 20)\n                    self.population = self.population[:self.pop_size]\n                    self.fitness = self.fitness[:self.pop_size]\n            \n            # Adaptive CR: Adjust CR based on success\n            CR_list = []\n\n            for i in range(self.pop_size):\n                mutant = self.mutate(i)\n                trial = self.crossover(mutant, i)\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n\n                    CR_list.append(1)\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        \n                        # Local search around the new best with reduced probability\n                        if np.random.rand() < self.local_search_prob:\n                            f_local, x_local = self.local_search(self.x_opt, func)\n                            if f_local < self.f_opt:\n                                self.f_opt = f_local\n                                self.x_opt = x_local\n                                \n                else:\n                  CR_list.append(0)\n\n                if self.budget <= 0:\n                    break\n            if self.budget <= 0:\n                break\n                \n            # Update CR adaptively\n            if len(CR_list) > 0:\n                success_rate = np.mean(CR_list)\n                self.CR = 0.9 * self.CR + 0.1 * success_rate # Moving average of success rate\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "An exception occurred: name 'minimize' is not defined.", "error": "", "parent_ids": ["857609df-0c4e-4040-990b-4f66071ac500"], "operator": null, "metadata": {}}
{"id": "ce971571-aa92-416c-84c0-9100b1a91fad", "fitness": -Infinity, "name": "AdaptiveDE_OL", "description": "Adaptive Differential Evolution with orthogonal learning, where orthogonal array-based crossover improves exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDE_OL:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, update_interval=1000, stagnation_threshold=1e-6, stagnation_generations=500, ol_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR # Crossover rate\n        self.population = None\n        self.fitness = None\n        self.update_interval = update_interval\n        self.success_F_history = []\n        self.success_CR_history = []\n        self.lb = -5.0\n        self.ub = 5.0\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_generations = stagnation_generations\n        self.best_fitness_history = []\n        self.generations_since_last_improvement = 0\n        self.F_mavg = F\n        self.CR_mavg = CR\n        self.mavg_window = 10\n        self.ol_size = ol_size  # Size of orthogonal learning array\n\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.best_fitness_history.append(self.f_opt)\n\n\n    def mutate(self, pop, i):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n        mutant = a + self.F * (b - c)\n        return np.clip(mutant, self.lb, self.ub)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def orthogonal_crossover(self, parents, func):\n        # Generate an orthogonal array (simplified version)\n        array = np.array([[1, 1, 1, 1, 1],\n                          [1, 2, 2, 2, 2],\n                          [2, 1, 2, 3, 4],\n                          [2, 2, 1, 4, 3]])\n        \n        children = []\n        for i in range(len(array)):\n            child = np.zeros(self.dim)\n            for j in range(self.dim):\n                parent_idx = array[i % len(array), j % self.ol_size] - 1  # -1 for 0-indexing\n                child[j] = parents[parent_idx][j]\n            children.append(child)\n\n        # Evaluate children and return the best\n        fitness_values = [func(child) for child in children]\n        best_child_idx = np.argmin(fitness_values)\n        return children[best_child_idx], fitness_values[best_child_idx]\n\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        evals = 0\n\n        while self.budget > 0:\n            \n            \n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n                target = self.population[i]\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, target)\n                \n                # Orthogonal Learning\n                parents = [target, mutant, self.x_opt]\n                trial_ol, f_trial = self.orthogonal_crossover(parents, func)\n                self.budget -= 1\n                evals += 1\n\n\n                if f_trial < self.fitness[i]:\n                    self.success_F_history.append(self.F)\n                    self.success_CR_history.append(self.CR)\n                    self.population[i] = trial_ol\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial_ol\n                        self.generations_since_last_improvement = 0\n                        \n            self.best_fitness_history.append(self.f_opt)\n            self.generations_since_last_improvement += 1\n\n            if evals >= self.update_interval:\n                if self.success_F_history:\n                    self.F_mavg = 0.9 * self.F_mavg + 0.1 * np.mean(self.success_F_history)\n                    self.CR_mavg = 0.9 * self.CR_mavg + 0.1 * np.mean(self.success_CR_history)\n                self.F = np.clip(self.F_mavg, 0.1, 0.9)\n                self.CR = np.clip(self.CR_mavg, 0.1, 0.9)\n                self.success_F_history = []\n                self.success_CR_history = []\n                evals = 0\n\n            if self.generations_since_last_improvement > self.stagnation_generations:\n                # Restart if stagnating\n                self.initialize_population(func)\n                self.generations_since_last_improvement = 0\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "An exception occurred: list index out of range.", "error": "", "parent_ids": ["e40aa0b9-0f2a-4f93-aecc-4ed41406de41"], "operator": null, "metadata": {}}
{"id": "a1d65a8b-28e2-4230-a72d-73e96308a99f", "fitness": -Infinity, "name": "AdaptiveDEOrthogonal", "description": "Adaptive Differential Evolution with orthogonal learning and dynamic population size, adjusting the mutation factor based on the success rate of previous mutations and orthogonal design to improve search efficiency.", "code": "import numpy as np\n\nclass AdaptiveDEOrthogonal:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, restart_threshold=1000, ortho_samples=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR # Crossover rate\n        self.population = None\n        self.fitness = None\n        self.restart_threshold = restart_threshold\n        self.success_F_history = []\n        self.success_CR_history = []\n        self.lb = -5.0\n        self.ub = 5.0\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.ortho_samples = ortho_samples\n        self.mutation_success_rate = 0.5 # Initialize mutation success rate\n        self.pop_size_history = []\n\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.best_fitness_history.append(self.f_opt)\n        self.pop_size_history.append(self.pop_size)\n\n    def mutate(self, pop, i):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n        mutant = a + self.F * (b - c)\n        return np.clip(mutant, self.lb, self.ub)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def orthogonal_design(self, target):\n        # Generate orthogonal array (simplified for demonstration)\n        levels = 2  # Example: two levels for each dimension\n        factors = self.dim\n        orthogonal_matrix = np.random.choice([self.lb, self.ub], size=(factors, self.ortho_samples)).T\n\n        samples = []\n        for i in range(self.ortho_samples):\n             samples.append(np.clip(target + orthogonal_matrix[i], self.lb, self.ub))\n\n        return np.array(samples)\n    \n    def __call__(self, func):\n        self.initialize_population(func)\n        mutation_successes = 0\n        mutation_attempts = 0\n\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n                target = self.population[i]\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, target)\n                f_trial = func(trial)\n                self.budget -= 1\n                mutation_attempts +=1\n\n\n                if f_trial < self.fitness[i]:\n                    self.success_F_history.append(self.F)\n                    self.success_CR_history.append(self.CR)\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    mutation_successes +=1\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.stagnation_counter = 0  # Reset stagnation counter\n                    else:\n                        self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter += 1\n\n            # Orthogonal Learning\n            orthogonal_samples = self.orthogonal_design(self.x_opt)\n            orthogonal_fitness = []\n            for sample in orthogonal_samples:\n                 f_sample = func(sample)\n                 orthogonal_fitness.append(f_sample)\n                 self.budget -= 1\n                 if self.budget <= 0:\n                      break\n            \n            if self.budget <= 0:\n                 break\n\n\n            best_sample_index = np.argmin(orthogonal_fitness)\n            if orthogonal_fitness[best_sample_index] < self.f_opt:\n                 self.f_opt = orthogonal_fitness[best_sample_index]\n                 self.x_opt = orthogonal_samples[best_sample_index]\n                 self.stagnation_counter = 0\n\n            # Adaptive F and CR\n            if self.success_F_history:\n                self.F = 0.9 * self.F + 0.1 * np.mean(self.success_F_history)\n                self.CR = 0.9 * self.CR + 0.1 * np.mean(self.success_CR_history)\n            self.F = np.clip(self.F, 0.1, 0.9)\n            self.CR = np.clip(self.CR, 0.1, 0.9)\n            self.success_F_history = []\n            self.success_CR_history = []\n\n            #Adjust F based on mutation success rate\n            if mutation_attempts > 0:\n                self.mutation_success_rate = mutation_successes / mutation_attempts\n            self.F = 0.5 * (1 + self.mutation_success_rate)\n            self.F = np.clip(self.F, 0.1, 0.9) #Clip F\n\n            # Dynamic Population Size\n            if self.stagnation_counter > self.restart_threshold / 2 and self.pop_size > 10:\n                self.pop_size = int(self.pop_size * 0.9)\n                self.stagnation_counter = 0\n            elif len(self.best_fitness_history) > 1 and self.best_fitness_history[-1] < self.best_fitness_history[-2]:\n                self.pop_size = min(100, int(self.pop_size * 1.1)) # Max pop size of 100\n\n            # Restart mechanism\n            if self.stagnation_counter > self.restart_threshold:\n                self.initialize_population(func)  # Re-initialize population\n                self.stagnation_counter = 0  # Reset stagnation counter\n                self.F = 0.5\n                self.CR = 0.7\n                self.pop_size = 50\n                mutation_successes = 0\n                mutation_attempts = 0\n\n            self.best_fitness_history.append(self.f_opt)\n            self.pop_size_history.append(self.pop_size)\n            mutation_successes = 0\n            mutation_attempts = 0\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "An exception occurred: index 50 is out of bounds for axis 0 with size 50.", "error": "", "parent_ids": ["5f99d9a2-afb9-425a-ae6d-bacca682c357"], "operator": null, "metadata": {}}
{"id": "b9dbf29d-d5b1-4e6d-96c0-f24defb19c5f", "fitness": 0.6354712356526508, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with simplified parameter adaptation using a moving average, and restart mechanism.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, stagnation_generations=500):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR # Crossover rate\n        self.population = None\n        self.fitness = None\n        self.lb = -5.0\n        self.ub = 5.0\n        self.stagnation_generations = stagnation_generations\n        self.best_fitness_history = []\n        self.generations_since_last_improvement = 0\n        self.F_mavg = F\n        self.CR_mavg = CR\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.best_fitness_history.append(self.f_opt)\n\n    def mutate(self, pop, i):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n        mutant = a + self.F * (b - c)\n        return np.clip(mutant, self.lb, self.ub)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:            \n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n                target = self.population[i]\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, target)\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.generations_since_last_improvement = 0\n                        \n            self.best_fitness_history.append(self.f_opt)\n            self.generations_since_last_improvement += 1\n\n            # Adaptive F/CR: Simplified moving average\n            self.F_mavg = 0.9 * self.F_mavg + 0.1 * np.random.uniform(0.4, 0.6)\n            self.CR_mavg = 0.9 * self.CR_mavg + 0.1 * np.random.uniform(0.6, 0.8)\n            self.F = np.clip(self.F_mavg, 0.1, 0.9)\n            self.CR = np.clip(self.CR_mavg, 0.1, 0.9)\n\n\n            if self.generations_since_last_improvement > self.stagnation_generations:\n                # Restart if stagnating\n                self.initialize_population(func)\n                self.generations_since_last_improvement = 0\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveDE scored 0.635 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e40aa0b9-0f2a-4f93-aecc-4ed41406de41"], "operator": null, "metadata": {"aucs": [0.23437267987756527, 0.5501457473378748, 0.6135609545295116, 0.8339504085411441, 0.7181781124416531, 0.7578269114608911, 0.5831992137900703, 0.5766541334508336, 0.6880579619965765, 0.5689813770338925, 0.7869964157119482, 0.9947402357588054, 0.323174877097015, 0.6660671879055091, 0.898031621069318, 0.7701661497507428, 0.5393954626385604, 0.8000411187633913, 0.26899715667864144, 0.5368869872190712]}}
{"id": "73b66786-a322-420c-b0bf-531ac57c9c2c", "fitness": 0.3591079686184172, "name": "CMAES", "description": "Covariance matrix adaptation evolution strategy (CMA-ES) with population size adaptation and budget-aware restarts.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, sigma=0.5, cs=0.3, damps=None, c_cov_mean=None, c_cov_rank_one=None, c_cov_rank_mu=None, mu=None):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.sigma = sigma\n        self.mean = None\n        self.C = None\n        self.lb = -5.0\n        self.ub = 5.0\n\n        self.mu = int(self.pop_size / 2) if mu is None else mu\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mu_eff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n        self.cs = cs\n        self.damps = damps if damps is not None else 1 + 2 * max(0, np.sqrt((self.mu_eff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.c_cov_mean = c_cov_mean if c_cov_mean is not None else self.cs**2 * (self.mu_eff / (self.dim + (self.mu_eff + 3)))\n        self.c_cov_rank_one = c_cov_rank_one if c_cov_rank_one is not None else 2 / ((self.dim + 1.3)**2 + self.mu_eff)\n        self.c_cov_rank_mu = c_cov_rank_mu if c_cov_rank_mu is not None else 2 * (self.mu_eff - 2 + 1/self.mu_eff) / ((self.dim + 2)**2 + self.mu_eff)\n\n        self.p_sigma = np.zeros(self.dim)\n        self.p_c = np.zeros(self.dim)\n        self.C = np.eye(self.dim)\n        self.B = None\n        self.D = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.evals = 0\n        self.last_restart = 0\n\n\n    def sample_population(self):\n        z = np.random.randn(self.pop_size, self.dim)\n        y = np.dot(z, np.transpose(self.B * self.D))\n        x = self.mean + self.sigma * y\n        return np.clip(x, self.lb, self.ub)\n\n    def update_distribution(self, population, fitness_values):\n        idx = np.argsort(fitness_values)\n        x_mu = population[idx[:self.mu]]\n        y_mu = np.dot(x_mu - self.mean, np.dot(self.B, np.diag(1/self.D))) / self.sigma\n        \n        self.mean = np.sum(x_mu * self.weights[:, np.newaxis], axis=0)\n        \n        self.p_sigma = (1 - self.cs) * self.p_sigma + np.sqrt(self.cs * (2 - self.cs) * self.mu_eff) * np.dot(self.B, np.dot(np.diag(1/self.D), np.sum(self.weights[:, np.newaxis] * y_mu, axis=0)))\n        self.sigma *= np.exp((self.cs/self.damps) * (np.linalg.norm(self.p_sigma)/np.sqrt(self.dim) - 1))\n        \n        h_sigma = int((np.linalg.norm(self.p_sigma)/np.sqrt(1 - (1 - self.cs)**(2*(self.budget/self.pop_size)))/1.4) < (1 + 2/ (self.dim + 1)))\n        \n        self.p_c = (1 - self.c_cov_mean) * self.p_c + h_sigma * np.sqrt(self.c_cov_mean * (2 - self.c_cov_mean) * self.mu_eff) * np.sum(self.weights[:, np.newaxis] * y_mu, axis=0)\n        \n        \n        C_rank_one = self.c_cov_rank_one * np.outer(self.p_c, self.p_c)\n        C_rank_mu = self.c_cov_rank_mu * np.dot(np.transpose(y_mu), np.diag(self.weights)).dot(y_mu)\n        self.C = (1 - self.c_cov_rank_one - self.c_cov_rank_mu) * self.C + C_rank_one + C_rank_mu\n        \n        self.C = np.triu(self.C) + np.transpose(np.triu(self.C, 1))\n        self.D, self.B = np.linalg.eigh(self.C)\n        self.D = np.sqrt(np.maximum(self.D, 1e-16))\n        \n    def adjust_population_size(self, improvement_rate):\n        if improvement_rate < 0.01:\n            self.pop_size = int(self.pop_size * 1.1)\n        else:\n            self.pop_size = int(self.pop_size * 0.9)\n            \n        self.pop_size = np.clip(self.pop_size, 4 + int(3 * np.log(self.dim)), 2 * self.dim)\n        self.mu = int(self.pop_size / 2)\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mu_eff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mu_eff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.c_cov_mean = self.cs**2 * (self.mu_eff / (self.dim + (self.mu_eff + 3)))\n        self.c_cov_rank_mu = 2 * (self.mu_eff - 2 + 1/self.mu_eff) / ((self.dim + 2)**2 + self.mu_eff)\n\n    def __call__(self, func):\n        self.mean = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.B, self.D = np.eye(self.dim), np.ones(self.dim)\n        best_fitness_history = []\n        generations_since_last_improvement = 0\n        \n        while self.budget > 0:\n            population = self.sample_population()\n            fitness_values = np.array([func(x) for x in population])\n            self.budget -= self.pop_size\n            self.evals += self.pop_size\n\n            best_idx = np.argmin(fitness_values)\n            if fitness_values[best_idx] < self.f_opt:\n                self.f_opt = fitness_values[best_idx]\n                self.x_opt = population[best_idx]\n                generations_since_last_improvement = 0\n            else:\n                 generations_since_last_improvement += 1\n\n            best_fitness_history.append(self.f_opt)\n            \n            self.update_distribution(population, fitness_values)\n            \n            if len(best_fitness_history) > 1:\n                improvement_rate = (best_fitness_history[-2] - best_fitness_history[-1]) / abs(best_fitness_history[-2])\n            else:\n                improvement_rate = 0\n\n            if generations_since_last_improvement > 50 and self.evals - self.last_restart > self.dim * 10:\n                 self.mean = np.random.uniform(self.lb, self.ub, size=self.dim)\n                 self.C = np.eye(self.dim)\n                 self.B, self.D = np.eye(self.dim), np.ones(self.dim)\n                 self.sigma = 0.5\n                 generations_since_last_improvement = 0\n                 self.last_restart = self.evals\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm CMAES scored 0.359 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e40aa0b9-0f2a-4f93-aecc-4ed41406de41"], "operator": null, "metadata": {"aucs": [0.15736054411061218, 0.22822384051590383, 0.3512265555655918, 0.48459055749417224, 0.2582326916451959, 0.3875665659742228, 0.2762318916344455, 0.32656773524353844, 0.27703122030801086, 0.1889799987181614, 0.31542626878135194, 0.9904956139694336, 0.24517061829701348, 0.28976132513923025, 0.6621381363042653, 0.35701542237132555, 0.28248027570061385, 0.44550952343453065, 0.18052883659969599, 0.47762175056102885]}}
{"id": "2f028cbc-2623-4fa7-8f27-8e7c62c82cf9", "fitness": 0.5016420735610081, "name": "DualPopulationDE", "description": "Differential Evolution with a dual population, one for exploration and one for exploitation, and adaptive switching between them based on performance.", "code": "import numpy as np\n\nclass DualPopulationDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, switch_prob=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.switch_prob = switch_prob\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.exploration_population = None\n        self.exploitation_population = None\n        self.exploration_fitness = None\n        self.exploitation_fitness = None\n\n    def initialize_population(self, func):\n        self.exploration_population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.exploitation_population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.exploration_fitness = np.array([func(x) for x in self.exploration_population])\n        self.exploitation_fitness = np.array([func(x) for x in self.exploitation_population])\n        self.budget -= 2 * self.pop_size\n        best_index = np.argmin(self.exploitation_fitness)\n        self.f_opt = self.exploitation_fitness[best_index]\n        self.x_opt = self.exploitation_population[best_index]\n\n    def mutate(self, population, i, F):\n        idxs = np.random.choice(self.pop_size, 3, replace=False)\n        a, b, c = population[idxs]\n        mutant = a + F * (b - c)\n        return np.clip(mutant, self.lb, self.ub)\n\n    def crossover(self, mutant, individual, CR):\n        cross_points = np.random.rand(self.dim) < CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, individual)\n    \n    def evolve_population(self, func, population, fitness, F, CR):\n        new_population = np.copy(population)\n        new_fitness = np.copy(fitness)\n        for i in range(self.pop_size):\n            mutant = self.mutate(population, i, F)\n            trial = self.crossover(mutant, population[i], CR)\n            f_trial = func(trial)\n            self.budget -= 1\n\n            if f_trial < fitness[i]:\n                new_population[i] = trial\n                new_fitness[i] = f_trial\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n            \n            if self.budget <= 0:\n                break\n        return new_population, new_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            # Evolve exploration population with higher F and lower CR for exploration\n            self.exploration_population, self.exploration_fitness = self.evolve_population(\n                func, self.exploration_population, self.exploration_fitness, F=0.7, CR=0.3\n            )\n\n            # Evolve exploitation population with lower F and higher CR for exploitation\n            self.exploitation_population, self.exploitation_fitness = self.evolve_population(\n                func, self.exploitation_population, self.exploitation_fitness, F=0.3, CR=0.9\n            )\n\n            # Adaptive switching between populations: replace individuals in exploitation\n            # population with promising individuals from the exploration population\n            if np.random.rand() < self.switch_prob:\n                # Select a random subset of exploration population\n                num_to_switch = int(self.pop_size * 0.1)  # Switch 10% of the population\n                indices_to_switch = np.random.choice(self.pop_size, num_to_switch, replace=False)\n\n                # Find the best individuals in the exploration population\n                best_exploration_indices = np.argsort(self.exploration_fitness)[:num_to_switch]\n\n                # Replace the randomly selected individuals in exploitation population\n                self.exploitation_population[indices_to_switch] = self.exploration_population[best_exploration_indices]\n                self.exploitation_fitness[indices_to_switch] = self.exploration_fitness[best_exploration_indices]\n\n            best_index = np.argmin(self.exploitation_fitness)\n            if self.exploitation_fitness[best_index] < self.f_opt:\n                self.f_opt = self.exploitation_fitness[best_index]\n                self.x_opt = self.exploitation_population[best_index]\n\n            if self.budget <= 0:\n                break\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm DualPopulationDE scored 0.502 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["857609df-0c4e-4040-990b-4f66071ac500"], "operator": null, "metadata": {"aucs": [0.2550975845317621, 0.3757851629776692, 0.42508092072056747, 0.6916986143578732, 0.4300622616841392, 0.5816872359991703, 0.3877359895442076, 0.4662856421846374, 0.5678218925269156, 0.410150906779002, 0.6974290283861615, 0.9994087437392011, 0.2969323657901216, 0.49089170309359853, 0.6892674810846671, 0.47752956504074895, 0.43315615933851137, 0.6494155233149742, 0.2116184779682756, 0.4957862121579585]}}
{"id": "9ba400b0-5262-4b92-b492-304d1692336b", "fitness": 0.6238791501527795, "name": "SimplifiedDE", "description": "Simplified Adaptive Differential Evolution with memory-based parameter adaptation and periodic population rejuvenation.", "code": "import numpy as np\n\nclass SimplifiedDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, memory_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.memory_size = memory_size\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.F_memory = [0.5] * memory_size\n        self.CR_memory = [0.7] * memory_size\n        self.generation = 0\n        self.rejuvenation_period = 50 # Rejuvenate population every N generations\n\n    def initialize_population(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n    def mutate(self, pop, i, F):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n        mutant = a + F * (b - c)\n        return np.clip(mutant, -5, 5)\n\n    def crossover(self, mutant, target, CR):\n        cross_points = np.random.rand(self.dim) < CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def adapt_parameters(self, success, F, CR):\n        if success:\n            self.F_memory.append(F)\n            self.CR_memory.append(CR)\n            self.F_memory = self.F_memory[1:]\n            self.CR_memory = self.CR_memory[1:]\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            self.generation += 1\n            \n            # Population rejuvenation\n            if self.generation % self.rejuvenation_period == 0:\n                worst_index = np.argmax(self.fitness)\n                lb = func.bounds.lb\n                ub = func.bounds.ub\n                self.population[worst_index] = np.random.uniform(lb, ub, size=(self.dim))\n                self.fitness[worst_index] = func(self.population[worst_index])\n                self.budget -= 1\n                if self.fitness[worst_index] < self.f_opt:\n                    self.f_opt = self.fitness[worst_index]\n                    self.x_opt = self.population[worst_index]\n            \n            for i in range(self.pop_size):\n                F = np.mean(self.F_memory)\n                CR = np.mean(self.CR_memory)\n                \n                target = self.population[i]\n                mutant = self.mutate(self.population, i, F)\n                trial = self.crossover(mutant, target, CR)\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                    self.adapt_parameters(True, F, CR)\n                else:\n                    self.adapt_parameters(False, F, CR)\n\n                if self.budget <= 0:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm SimplifiedDE scored 0.624 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e8a6bdc0-cb3b-4578-8571-982d19ba2e0a"], "operator": null, "metadata": {"aucs": [0.20924080061599282, 0.3983047624171868, 0.6222548880048367, 0.8318368202170323, 0.673822291791762, 0.7465795430593969, 0.5994474154660899, 0.5508337117205888, 0.7036916830292044, 0.6206617650970132, 0.8046087484628139, 0.9961712063083219, 0.33750409432591777, 0.6099586509815875, 0.873330722321551, 0.7557644858080399, 0.5105726335427231, 0.8176745811025589, 0.2962696605247037, 0.5190545382582687]}}
{"id": "cea070f1-d81c-4228-918c-382ec746a15b", "fitness": 0.630247532437936, "name": "DESimpleAdaptive", "description": "Simplified Differential Evolution with adaptive F/CR parameters based on recent success and archive interaction.", "code": "import numpy as np\n\nclass DESimpleAdaptive:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.archive = []\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.F = 0.5\n        self.CR = 0.7\n        self.success_F = []\n        self.success_CR = []\n\n    def initialize_population(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n    def mutate(self, i):\n        idxs = np.random.choice(self.pop_size, 3, replace=False)\n        a, b, c = self.population[idxs]\n        mutant = a + self.F * (b - c)\n        \n        if len(self.archive) > 0 and np.random.rand() < 0.1:\n            arch_idx = np.random.randint(len(self.archive))\n            mutant = a + self.F * (self.archive[arch_idx] - b)\n            \n        return np.clip(mutant, -5, 5)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def update_archive(self, trial):\n        if len(self.archive) < self.archive_size:\n            self.archive.append(trial)\n        else:\n            idx = np.random.randint(self.archive_size)\n            self.archive[idx] = trial\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                target = self.population[i]\n                mutant = self.mutate(i)\n                trial = self.crossover(mutant, target)\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.success_F.append(self.F)\n                    self.success_CR.append(self.CR)\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    self.update_archive(trial)\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                \n                if len(self.success_F) > 10:\n                    self.success_F.pop(0)\n                    self.success_CR.pop(0)\n                \n                if self.success_F:\n                    self.F = np.mean(self.success_F)\n                    self.CR = np.mean(self.success_CR)\n                    self.F = np.clip(self.F, 0.1, 0.9)\n                    self.CR = np.clip(self.CR, 0.1, 0.9)\n\n\n                if self.budget <= 0:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm DESimpleAdaptive scored 0.630 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e8a6bdc0-cb3b-4578-8571-982d19ba2e0a"], "operator": null, "metadata": {"aucs": [0.2196567054679215, 0.456986982253869, 0.6040745600856225, 0.8513985179354688, 0.6979254518094444, 0.7638866708372019, 0.5859013142715834, 0.5714783663905021, 0.7020383285632479, 0.6037731013281724, 0.8196116540649411, 0.9932997877538353, 0.3248088303035148, 0.6808952037410019, 0.8769116673254608, 0.7382717980932256, 0.4660780507226805, 0.8163483909345484, 0.31840616339877403, 0.5131991034777014]}}
{"id": "61b40155-ebbb-45d1-ae30-a9cbedb24fe5", "fitness": 0.6296688922010525, "name": "DESimpleAdaptive", "description": "Simplified Differential Evolution with adaptive F/CR parameters using weighted averages based on success, and a periodically refreshed archive.", "code": "import numpy as np\n\nclass DESimpleAdaptive:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, refresh_interval=500):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.archive = []\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.F = 0.5\n        self.CR = 0.7\n        self.success_F = []\n        self.success_CR = []\n        self.refresh_interval = refresh_interval\n        self.eval_count = 0\n\n\n    def initialize_population(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.eval_count += self.pop_size\n        \n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n    def mutate(self, pop, i):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n        mutant = a + self.F * (b - c)\n\n        #Utilize archive if available\n        if len(self.archive) > 0 and np.random.rand() < 0.1:\n            arch_idx = np.random.randint(len(self.archive))\n            mutant = a + self.F * (self.archive[arch_idx] - b)\n\n        return np.clip(mutant, -5, 5)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def update_archive(self, trial):\n        if len(self.archive) < self.archive_size:\n            self.archive.append(trial)\n        else:\n            idx = np.random.randint(self.archive_size)\n            self.archive[idx] = trial\n\n    def adapt_parameters(self, success, F, CR):\n        if success:\n            self.success_F.append(F)\n            self.success_CR.append(CR)\n\n        if len(self.success_F) > 0:\n            self.F = np.mean(self.success_F)\n            self.CR = np.mean(self.success_CR)\n\n            # Keep the lists short\n            if len(self.success_F) > 10:\n                self.success_F.pop(0)\n                self.success_CR.pop(0)\n\n\n        self.F = np.clip(self.F, 0.1, 0.9)\n        self.CR = np.clip(self.CR, 0.1, 0.9)\n        \n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                target = self.population[i]\n                F = self.F\n                CR = self.CR\n\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, target)\n                f_trial = func(trial)\n                self.budget -= 1\n                self.eval_count += 1\n\n                if f_trial < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    self.update_archive(trial)\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                    self.adapt_parameters(True, F, CR)\n                else:\n                    self.adapt_parameters(False, F, CR)\n                \n                if self.eval_count % self.refresh_interval == 0:\n                     self.archive = []\n\n                if self.budget <= 0:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm DESimpleAdaptive scored 0.630 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e8a6bdc0-cb3b-4578-8571-982d19ba2e0a"], "operator": null, "metadata": {"aucs": [0.20799964159792617, 0.46742148704705455, 0.6029455244773417, 0.851014506247966, 0.6624351506464283, 0.7554749677125004, 0.5647201393495844, 0.576565190184078, 0.666560000528852, 0.6259687275677059, 0.7995909168787939, 0.9918551727248426, 0.34905043145005454, 0.6496171880350707, 0.8988364691354214, 0.7545352061206224, 0.49421835621076693, 0.8021618949001857, 0.3435987083848818, 0.5288081648209717]}}
{"id": "863de81e-a631-45d6-be15-9acf948ff59d", "fitness": 0.08341960712662339, "name": "AdaptiveDEArchiveNichingRestart", "description": "Adaptive Differential Evolution with Archive, self-adaptation of F and CR using Lehmer mean, and a niching mechanism to maintain diversity, combined with a restart strategy.", "code": "import numpy as np\n\nclass AdaptiveDEArchiveNichingRestart:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=20, F=0.5, CR=0.7, restart_threshold=1000, niche_radius=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F  # Mutation factor\n        self.CR = CR # Crossover rate\n        self.population = None\n        self.fitness = None\n        self.archive = []\n        self.archive_fitness = []\n        self.restart_threshold = restart_threshold\n        self.success_F_history = []\n        self.success_CR_history = []\n        self.lb = -5.0\n        self.ub = 5.0\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.niche_radius = niche_radius # Radius for niching\n        self.epsilon = 1e-8\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.best_fitness_history.append(self.f_opt)\n\n    def mutate(self, pop, i):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n        mutant = a + self.F * (b - c)\n        return np.clip(mutant, self.lb, self.ub)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def update_archive(self, x, fx):\n        if len(self.archive) < self.archive_size:\n            self.archive.append(x)\n            self.archive_fitness.append(fx)\n        else:\n            if fx < np.max(self.archive_fitness):\n                worst_index = np.argmax(self.archive_fitness)\n                self.archive[worst_index] = x\n                self.archive_fitness[worst_index] = fx\n                \n    def niching(self, trial):\n        for archive_member in self.archive:\n            if np.linalg.norm(trial - archive_member) < self.niche_radius:\n                return True  # Trial is too close to an existing archive member\n        return False\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n                target = self.population[i]\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, target)\n                \n                if self.niching(trial):\n                    continue # Skip evaluation if trial is too close to archive members\n\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.success_F_history.append(self.F)\n                    self.success_CR_history.append(self.CR)\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    \n                    self.update_archive(trial, f_trial)\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.stagnation_counter = 0  # Reset stagnation counter\n                    else:\n                        self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter += 1\n            \n            # Adaptive F and CR using Lehmer mean\n            if self.success_F_history:\n                self.F = np.sum(np.array(self.success_F_history)**2) / (np.sum(np.array(self.success_F_history)) + self.epsilon)\n                self.CR = np.sum(np.array(self.success_CR_history)**2) / (np.sum(np.array(self.success_CR_history)) + self.epsilon)\n            \n            self.F = np.clip(self.F, 0.1, 0.9)\n            self.CR = np.clip(self.CR, 0.1, 0.9)\n            self.success_F_history = []\n            self.success_CR_history = []\n\n            # Restart mechanism\n            if self.stagnation_counter > self.restart_threshold:\n                self.initialize_population(func)  # Re-initialize population\n                self.stagnation_counter = 0  # Reset stagnation counter\n                self.F = 0.5\n                self.CR = 0.7\n                self.archive = []\n                self.archive_fitness = []\n            \n            self.best_fitness_history.append(self.f_opt)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveDEArchiveNichingRestart scored 0.083 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5f99d9a2-afb9-425a-ae6d-bacca682c357"], "operator": null, "metadata": {"aucs": [0.16683921425324677, 0]}}
{"id": "c4b61599-9880-4583-ae11-007c1af539e3", "fitness": 0.28426317263253387, "name": "AdaptiveDE", "description": "Simplified Adaptive Differential Evolution with stagnation detection based on fitness improvement and periodic population shuffling.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, stagnation_threshold=0.001, shuffle_interval=500):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population = None\n        self.fitness = None\n        self.best_fitness = np.inf\n        self.best_solution = None\n        self.stagnation_threshold = stagnation_threshold\n        self.shuffle_interval = shuffle_interval\n        self.last_improvement = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.best_fitness = self.fitness[best_index]\n        self.best_solution = self.population[best_index].copy()\n\n\n    def mutate(self, pop, i):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n        mutant = a + self.F * (b - c)\n        return np.clip(mutant, self.lb, self.ub)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        evaluations = 0\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                target = self.population[i]\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, target)\n                f_trial = func(trial)\n                self.budget -= 1\n                evaluations +=1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    if f_trial < self.best_fitness:\n                        self.best_fitness = f_trial\n                        self.best_solution = trial.copy()\n                        self.last_improvement = evaluations\n            \n            #Stagnation Check and Population Shuffle\n            if evaluations - self.last_improvement > self.shuffle_interval:\n                np.random.shuffle(self.population)\n                self.last_improvement = evaluations\n\n            # Adaptive F and CR - simplified\n            self.F = 0.5 + 0.3 * np.random.randn()\n            self.CR = np.random.rand()\n            self.F = np.clip(self.F, 0.1, 0.9)\n            self.CR = np.clip(self.CR, 0.1, 0.9)\n\n\n        return self.best_fitness, self.best_solution", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveDE scored 0.284 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5f99d9a2-afb9-425a-ae6d-bacca682c357"], "operator": null, "metadata": {"aucs": [0.10837422601986779, 0.18692967918900538, 0.2746940727571747, 0.21643892791689212, 0.20132683466799695, 0.23009027988917563, 0.2325550263773113, 0.2082097325587875, 0.19988125937779655, 0.15896122096248289, 0.23634310280565762, 0.9955394267507071, 0.27054150867697213, 0.2140393635275437, 0.5464615354960443, 0.29076302669023546, 0.25586793133934704, 0.23821721352886915, 0.15845017738755118, 0.4615789067312589]}}
{"id": "3e448154-2a89-4972-b6f5-a050879a5669", "fitness": -Infinity, "name": "AdaptivePopulationDE", "description": "Differential Evolution with self-adaptive population size based on improvement stagnation, and orthogonal learning.", "code": "import numpy as np\n\nclass AdaptivePopulationDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, archive_size=10, stagnation_limit=500):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.min_pop_size = 10  # Minimum population size\n        self.max_pop_size = 100  # Maximum population size\n        self.archive_size = archive_size\n        self.archive = []\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.F = 0.5\n        self.CR = 0.7\n        self.success_F = []\n        self.success_CR = []\n        self.stagnation_limit = stagnation_limit\n        self.stagnation_counter = 0\n        self.eval_count = 0\n        self.last_improvement = 0  # Evaluation count of last improvement\n\n    def initialize_population(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.eval_count += self.pop_size\n        \n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n            self.last_improvement = self.eval_count\n\n    def mutate(self, pop, i):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n        mutant = a + self.F * (b - c)\n\n        #Utilize archive if available\n        if len(self.archive) > 0 and np.random.rand() < 0.1:\n            arch_idx = np.random.randint(len(self.archive))\n            mutant = a + self.F * (self.archive[arch_idx] - b)\n\n        return np.clip(mutant, -5, 5)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def orthogonal_learning(self, func, individual):\n        # Generate orthogonal array (simplified, just two levels)\n        oa = np.array([[-1, -1], [-1, 1], [1, -1], [1, 1]])\n        \n        # Select a random subset of dimensions\n        num_factors = min(4, self.dim)  # Use at most 4 dimensions\n        selected_dims = np.random.choice(self.dim, num_factors, replace=False)\n\n        best_fitness = np.inf\n        best_solution = individual.copy()\n\n        for row in oa:\n            trial = individual.copy()\n            for i in range(num_factors):\n                dim_index = selected_dims[i]\n                # Map -1 to lb and 1 to ub for the selected dimension\n                lb = func.bounds.lb[dim_index] if isinstance(func.bounds.lb, np.ndarray) else func.bounds.lb\n                ub = func.bounds.ub[dim_index] if isinstance(func.bounds.ub, np.ndarray) else func.bounds.ub\n                trial[dim_index] = (row[i] + 1) / 2 * (ub - lb) + lb\n            \n            f_trial = func(trial)\n            self.budget -= 1\n            self.eval_count += 1\n\n            if f_trial < best_fitness:\n                best_fitness = f_trial\n                best_solution = trial\n        return best_solution, best_fitness\n\n\n    def update_archive(self, trial):\n        if len(self.archive) < self.archive_size:\n            self.archive.append(trial)\n        else:\n            idx = np.random.randint(self.archive_size)\n            self.archive[idx] = trial\n\n    def adapt_parameters(self, success, F, CR):\n        if success:\n            self.success_F.append(F)\n            self.success_CR.append(CR)\n\n        if len(self.success_F) > 0:\n            self.F = np.mean(self.success_F)\n            self.CR = np.mean(self.success_CR)\n\n            # Keep the lists short\n            if len(self.success_F) > 10:\n                self.success_F.pop(0)\n                self.success_CR.pop(0)\n\n\n        self.F = np.clip(self.F, 0.1, 0.9)\n        self.CR = np.clip(self.CR, 0.1, 0.9)\n    \n    def adjust_population_size(self):\n        if self.eval_count - self.last_improvement > self.stagnation_limit:\n            # Reduce population size if stagnating\n            self.pop_size = max(self.min_pop_size, int(self.pop_size * 0.9))\n            self.stagnation_counter = 0\n            print(f\"Reducing population size to {self.pop_size}\") # Debugging\n        else:\n            # Increase population size if improving\n            self.pop_size = min(self.max_pop_size, int(self.pop_size * 1.1))\n            print(f\"Increasing population size to {self.pop_size}\") # Debugging\n        \n        # Ensure population size does not exceed budget\n        self.pop_size = min(self.pop_size, self.budget)\n        \n        # Reinitialize population to the new size\n        \n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            new_population = np.copy(self.population)\n            new_fitness = np.copy(self.fitness)\n\n            for i in range(self.pop_size):\n                target = self.population[i]\n                F = self.F\n                CR = self.CR\n\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, target)\n\n                # Apply orthogonal learning\n                trial, f_trial = self.orthogonal_learning(func, trial)\n                \n\n                if f_trial < self.fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                    self.update_archive(trial)\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.last_improvement = self.eval_count\n                    self.adapt_parameters(True, F, CR)\n                else:\n                    self.adapt_parameters(False, F, CR)\n\n                if self.budget <= 0:\n                    break\n            \n            self.population = new_population\n            self.fitness = new_fitness\n            \n            self.adjust_population_size()\n            # Re-evaluate and re-initialize population if size changed significantly\n            if len(self.population) != self.pop_size and self.budget > self.pop_size:\n                \n                lb = func.bounds.lb\n                ub = func.bounds.ub\n                new_individuals = np.random.uniform(lb, ub, size=(self.pop_size - len(self.population), self.dim))\n                new_fitnesses = np.array([func(x) for x in new_individuals])\n                self.budget -= len(new_individuals)\n                self.eval_count += len(new_individuals)\n                \n                self.population = np.concatenate((self.population, new_individuals))\n                self.fitness = np.concatenate((self.fitness, new_fitnesses))\n                \n                best_index = np.argmin(self.fitness)\n                if self.fitness[best_index] < self.f_opt:\n                    self.f_opt = self.fitness[best_index]\n                    self.x_opt = self.population[best_index]\n                    self.last_improvement = self.eval_count\n            \n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "An exception occurred: index 2 is out of bounds for axis 0 with size 2.", "error": "", "parent_ids": ["61b40155-ebbb-45d1-ae30-a9cbedb24fe5"], "operator": null, "metadata": {}}
{"id": "055a1f5c-5f37-44da-99c4-62c5345b9176", "fitness": 0.0, "name": "OrthogonalDE", "description": "Adaptive Differential Evolution with orthogonal learning and a self-adaptive population size.", "code": "import numpy as np\n\nclass OrthogonalDE:\n    def __init__(self, budget=10000, dim=10, pop_size_min=10, pop_size_max=100, archive_size=50, ortho_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_min = pop_size_min\n        self.pop_size_max = pop_size_max\n        self.pop_size = pop_size_max #Start with a larger population\n        self.archive_size = archive_size\n        self.archive = []\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.F = 0.5\n        self.CR = 0.7\n        self.success_F = []\n        self.success_CR = []\n        self.eval_count = 0\n        self.ortho_size = ortho_size\n\n    def initialize_population(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.eval_count += self.pop_size\n        \n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n    def mutate(self, pop, i):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n        mutant = a + self.F * (b - c)\n\n        #Utilize archive if available\n        if len(self.archive) > 0 and np.random.rand() < 0.1:\n            arch_idx = np.random.randint(len(self.archive))\n            mutant = a + self.F * (self.archive[arch_idx] - b)\n\n        return np.clip(mutant, -5, 5)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def update_archive(self, trial):\n        if len(self.archive) < self.archive_size:\n            self.archive.append(trial)\n        else:\n            idx = np.random.randint(self.archive_size)\n            self.archive[idx] = trial\n            \n    def orthogonal_learning(self, func):\n        # Select ortho_size random individuals\n        idxs = np.random.choice(len(self.population), self.ortho_size, replace=False)\n        sample = self.population[idxs]\n\n        # Generate orthogonal array (simplified: Latin hypercube sampling)\n        oa = np.random.rand(self.ortho_size, self.dim)\n\n        # Map OA to bounds of selected individuals\n        lb = np.min(sample, axis=0)\n        ub = np.max(sample, axis=0)\n        candidates = lb + oa * (ub - lb)\n\n        # Evaluate candidates\n        fitness = np.array([func(x) for x in candidates])\n        self.budget -= self.ortho_size\n        self.eval_count += self.ortho_size\n\n        # Update population if a candidate is better\n        best_idx = np.argmin(fitness)\n        worst_idx = np.argmax(self.fitness[idxs])\n\n        if fitness[best_idx] < self.fitness[idxs][worst_idx]:\n            self.population[idxs][worst_idx] = candidates[best_idx]\n            self.fitness[idxs][worst_idx] = fitness[best_idx]\n\n            if fitness[best_idx] < self.f_opt:\n                self.f_opt = fitness[best_idx]\n                self.x_opt = candidates[best_idx]\n\n    def adapt_parameters(self, success, F, CR):\n        if success:\n            self.success_F.append(F)\n            self.success_CR.append(CR)\n\n        if len(self.success_F) > 0:\n            self.F = np.mean(self.success_F)\n            self.CR = np.mean(self.success_CR)\n\n            # Keep the lists short\n            if len(self.success_F) > 10:\n                self.success_F.pop(0)\n                self.success_CR.pop(0)\n\n\n        self.F = np.clip(self.F, 0.1, 0.9)\n        self.CR = np.clip(self.CR, 0.1, 0.9)\n\n    def adapt_population_size(self):\n        # Simple adaptation: if improvement is stagnant, reduce pop size, else increase\n        if len(self.success_F) < 3: #Not enough success to determine change\n            return\n\n        if self.success_F[-1] == self.success_F[-2] == self.success_F[-3]:\n            self.pop_size = max(self.pop_size_min, int(self.pop_size * 0.9))\n        else:\n            self.pop_size = min(self.pop_size_max, int(self.pop_size * 1.1))\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            self.adapt_population_size()\n            for i in range(self.pop_size):\n                target = self.population[i]\n                F = self.F\n                CR = self.CR\n\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, target)\n                f_trial = func(trial)\n                self.budget -= 1\n                self.eval_count += 1\n\n                if f_trial < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    self.update_archive(trial)\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                    self.adapt_parameters(True, F, CR)\n                else:\n                    self.adapt_parameters(False, F, CR)\n\n                if self.eval_count % 500 == 0:\n                    self.orthogonal_learning(func)\n                \n                if self.budget <= 0:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm OrthogonalDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["61b40155-ebbb-45d1-ae30-a9cbedb24fe5"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "1bc46465-7736-4f1e-8acb-87f22603e82e", "fitness": 0.0, "name": "CMAES_DE", "description": "Differential Evolution with self-adaptive population size and covariance matrix adaptation for mutation.", "code": "import numpy as np\n\nclass CMAES_DE:\n    def __init__(self, budget=10000, dim=10, pop_size=None, archive_size=10, initial_sigma=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_base = 4 + int(3 * np.log(self.dim)) if pop_size is None else pop_size\n        self.pop_size = self.pop_size_base\n        self.archive_size = archive_size\n        self.archive = []\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.F = 0.5\n        self.CR = 0.7\n        self.success_F = []\n        self.success_CR = []\n        self.initial_sigma = initial_sigma\n        self.C = np.eye(self.dim)  # Covariance matrix\n        self.mean = None\n        self.p_c = np.zeros(self.dim)\n        self.p_sigma = np.zeros(self.dim)\n        self.c_c = (4 + self.dim / 3) / (self.dim + 4)\n        self.c_sigma = (4 + self.dim / 2) / (self.dim + 4)\n        self.d_sigma = 1 + 2 * max(0, np.sqrt((self.dim - 1) / (self.dim + 4)) - 1)\n        self.sigma = self.initial_sigma\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.mu = self.pop_size // 2\n\n    def initialize_population(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.mean = np.random.uniform(lb, ub, size=self.dim)\n        self.population = np.random.normal(self.mean, self.sigma, size=(self.pop_size, self.dim))\n        self.population = np.clip(self.population, lb, ub)\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n    def mutate(self, i):\n        idxs = np.random.choice(self.pop_size, 3, replace=False)\n        a, b, c = self.population[idxs]\n        \n        z = np.random.multivariate_normal(np.zeros(self.dim), self.C)\n        mutant = a + self.F * (b - c) + self.sigma * z\n\n        if len(self.archive) > 0 and np.random.rand() < 0.1:\n            arch_idx = np.random.randint(len(self.archive))\n            mutant = a + self.F * (self.archive[arch_idx] - b)\n        \n        return np.clip(mutant, -5, 5)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def update_archive(self, trial):\n        if len(self.archive) < self.archive_size:\n            self.archive.append(trial)\n        else:\n            idx = np.random.randint(self.archive_size)\n            self.archive[idx] = trial\n\n    def update_CMA(self):\n         # Sort population based on fitness\n        indices = np.argsort(self.fitness)\n        self.population = self.population[indices]\n        self.fitness = self.fitness[indices]\n\n        # Calculate new mean\n        old_mean = self.mean.copy()\n        self.mean = np.mean(self.population[:self.mu], axis=0)\n\n        # Update evolution paths\n        y = (self.mean - old_mean) / self.sigma\n        self.p_c = (1 - self.c_c) * self.p_c + np.sqrt(self.c_c * (2 - self.c_c)) * y\n        self.p_sigma = (1 - self.c_sigma) * self.p_sigma + np.sqrt(self.c_sigma * (2 - self.c_sigma)) * (np.linalg.norm(y) / np.sqrt(self.dim) - self.chiN)\n\n        # Update covariance matrix\n        self.C = (1 - self.c_c) * self.C + self.c_c * np.outer(self.p_c, self.p_c)\n\n        # Update step size\n        self.sigma *= np.exp(self.c_sigma / self.d_sigma * (np.linalg.norm(self.p_sigma) / self.chiN - 1))\n        self.sigma = np.clip(self.sigma, self.initial_sigma / 10, 1)\n\n    def adjust_population_size(self):\n        if len(self.success_F) > 20:\n            if np.mean(self.fitness[:self.pop_size//2]) < np.mean(self.fitness[self.pop_size//2:]):\n                 self.pop_size = min(self.pop_size + 1, 2 * self.pop_size_base)\n            else:\n                self.pop_size = max(self.pop_size - 1, self.pop_size_base // 2)\n        self.pop_size = max(self.pop_size, 4)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                target = self.population[i]\n                mutant = self.mutate(i)\n                trial = self.crossover(mutant, target)\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.success_F.append(self.F)\n                    self.success_CR.append(self.CR)\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    self.update_archive(trial)\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                \n                if len(self.success_F) > 10:\n                    self.success_F.pop(0)\n                    self.success_CR.pop(0)\n                \n                if self.success_F:\n                    self.F = np.mean(self.success_F)\n                    self.CR = np.mean(self.success_CR)\n                    self.F = np.clip(self.F, 0.1, 0.9)\n                    self.CR = np.clip(self.CR, 0.1, 0.9)\n\n                if self.budget <= 0:\n                    break\n            \n            self.update_CMA()\n            self.population = np.random.multivariate_normal(self.mean, self.sigma**2 * self.C, size=self.pop_size)\n            self.population = np.clip(self.population, func.bounds.lb, func.bounds.ub)\n            self.fitness = np.array([func(x) for x in self.population])\n            self.budget -= self.pop_size\n            \n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm CMAES_DE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["cea070f1-d81c-4228-918c-382ec746a15b"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "589c2f90-ac3c-40db-afb7-8735602608b0", "fitness": 0.6205544809886022, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with simplified parameter adaptation using a moving average and scaling factor adaptation based on fitness improvement, and a restart mechanism.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, stagnation_generations=500):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR # Crossover rate\n        self.population = None\n        self.fitness = None\n        self.lb = -5.0\n        self.ub = 5.0\n        self.stagnation_generations = stagnation_generations\n        self.best_fitness_history = []\n        self.generations_since_last_improvement = 0\n        self.F_mavg = F\n        self.CR_mavg = CR\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.best_fitness_history.append(self.f_opt)\n\n    def mutate(self, pop, i, F):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n        mutant = a + F * (b - c)\n        return np.clip(mutant, self.lb, self.ub)\n\n    def crossover(self, mutant, target, CR):\n        cross_points = np.random.rand(self.dim) < CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:            \n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n                target = self.population[i]\n                \n                # Adaptive F: Scale with improvement\n                F = self.F_mavg * (1 + 0.1 * np.random.randn()) \n                F = np.clip(F, 0.1, 0.9)\n                mutant = self.mutate(self.population, i, F)\n\n                trial = self.crossover(mutant, target, self.CR_mavg)\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.generations_since_last_improvement = 0\n                        # Adjust F_mavg based on improvement\n                        self.F_mavg = 0.8 * self.F_mavg + 0.2 * F\n                        self.F_mavg = np.clip(self.F_mavg, 0.1, 0.9)\n                        \n            self.best_fitness_history.append(self.f_opt)\n            self.generations_since_last_improvement += 1\n\n            # Adaptive CR: Simplified moving average\n            self.CR_mavg = 0.9 * self.CR_mavg + 0.1 * np.random.uniform(0.6, 0.8)\n            self.CR_mavg = np.clip(self.CR_mavg, 0.1, 0.9)\n\n\n            if self.generations_since_last_improvement > self.stagnation_generations:\n                # Restart if stagnating\n                self.initialize_population(func)\n                self.generations_since_last_improvement = 0\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveDE scored 0.621 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b9dbf29d-d5b1-4e6d-96c0-f24defb19c5f"], "operator": null, "metadata": {"aucs": [0.21364571608780203, 0.3759896565132974, 0.6057390084720093, 0.8295470215476314, 0.6696058265373492, 0.7353476312171503, 0.534830951106588, 0.635124035768154, 0.6874224337059711, 0.637276804838226, 0.7639256368925686, 0.9961084128381035, 0.33117491915860064, 0.6460955481905157, 0.8543755230782741, 0.7775517876063064, 0.5268338149464472, 0.8322836691370761, 0.25220458530354195, 0.5060066368264309]}}
{"id": "135bd5db-910d-4b68-9e48-61af1e79ffac", "fitness": 0.5295489655206982, "name": "DiversityDE", "description": "Differential Evolution with self-adaptive parameters based on population diversity and fitness improvement rate.", "code": "import numpy as np\n\nclass DiversityDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.archive = []\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.F = 0.5\n        self.CR = 0.7\n        self.eval_count = 0\n        self.diversity_history = []\n\n\n    def initialize_population(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.eval_count += self.pop_size\n        \n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n    def calculate_diversity(self):\n        \"\"\"Calculates the diversity of the population based on Euclidean distance.\"\"\"\n        centroid = np.mean(self.population, axis=0)\n        distances = np.linalg.norm(self.population - centroid, axis=1)\n        return np.mean(distances)\n\n    def mutate(self, pop, i):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n        mutant = a + self.F * (b - c)\n\n        #Utilize archive if available\n        if len(self.archive) > 0 and np.random.rand() < 0.1:\n            arch_idx = np.random.randint(len(self.archive))\n            mutant = a + self.F * (self.archive[arch_idx] - b)\n\n        return np.clip(mutant, -5, 5)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def update_archive(self, trial):\n        if len(self.archive) < self.archive_size:\n            self.archive.append(trial)\n        else:\n            idx = np.random.randint(self.archive_size)\n            self.archive[idx] = trial\n\n    def adapt_parameters(self, diversity_before, diversity_after, fitness_before, fitness_after):\n        \"\"\"Adapts F and CR based on the change in population diversity and fitness.\"\"\"\n        \n        # Diversity change\n        diversity_change = diversity_after - diversity_before\n\n        # Fitness improvement rate (avoid division by zero)\n        fitness_improvement = (fitness_before - fitness_after) / (abs(fitness_before) + 1e-8)\n\n        # Adjust F and CR based on diversity and fitness improvement\n        self.F = np.clip(self.F * (1 + 0.5 * diversity_change + 0.5 * fitness_improvement), 0.1, 0.9)\n        self.CR = np.clip(self.CR * (1 + 0.5 * diversity_change + 0.5 * fitness_improvement), 0.1, 0.9)\n        \n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            diversity_before = self.calculate_diversity()\n            fitness_before = np.mean(self.fitness)\n\n            for i in range(self.pop_size):\n                target = self.population[i]\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, target)\n                f_trial = func(trial)\n                self.budget -= 1\n                self.eval_count += 1\n\n                if f_trial < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    self.update_archive(trial)\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n\n            diversity_after = self.calculate_diversity()\n            fitness_after = np.mean(self.fitness)\n            self.adapt_parameters(diversity_before, diversity_after, fitness_before, fitness_after)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm DiversityDE scored 0.530 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["61b40155-ebbb-45d1-ae30-a9cbedb24fe5"], "operator": null, "metadata": {"aucs": [0.19741564319284366, 0.35022884685239963, 0.46085133595770167, 0.7581830488587171, 0.5203971386098566, 0.6139881113418776, 0.42274210279483193, 0.4752086174974668, 0.5603296007060747, 0.4242016077119224, 0.6812907420364315, 0.9944795278220845, 0.28671037992680704, 0.4756113153972523, 0.8655216956293517, 0.641270217838587, 0.45574507587390223, 0.7170981239573564, 0.1849523163298924, 0.5047538620786048]}}
{"id": "00b68d45-51ae-40a9-a865-b13f74192d45", "fitness": 0.5233149520840266, "name": "ImprovedDE", "description": "Improved Differential Evolution with adaptive parameter control using exponential moving averages and a diversity-enhancing archive.", "code": "import numpy as np\n\nclass ImprovedDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, memory_size=5, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.memory_size = memory_size\n        self.archive_size = archive_size\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.F_ema = 0.5  # Exponential Moving Average for F\n        self.CR_ema = 0.7  # Exponential Moving Average for CR\n        self.ema_alpha = 0.2  # EMA smoothing factor\n        self.generation = 0\n        self.rejuvenation_period = 50  # Rejuvenate population every N generations\n        self.archive = []  # Archive for storing diverse solutions\n        self.archive_fitness = []\n\n    def initialize_population(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n        # Initialize Archive\n        indices = np.argsort(self.fitness)[-self.archive_size:]\n        self.archive = self.population[indices].copy()\n        self.archive_fitness = self.fitness[indices].copy()\n\n    def mutate(self, pop, i, F):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n        mutant = a + F * (b - c)\n        return np.clip(mutant, -5, 5)\n\n    def mutate_with_archive(self, pop, i, F):\n        if len(self.archive) > 0 and np.random.rand() < 0.1: # Use archive with probability 0.1\n          idx_archive = np.random.randint(0, len(self.archive))\n          archive_member = self.archive[idx_archive]\n          idxs = np.random.choice(len(pop), 2, replace=False)\n          a, b = pop[idxs]\n          mutant = archive_member + F * (a - b)\n        else:\n          idxs = np.random.choice(len(pop), 3, replace=False)\n          a, b, c = pop[idxs]\n          mutant = a + F * (b - c)\n\n        return np.clip(mutant, -5, 5)\n\n    def crossover(self, mutant, target, CR):\n        cross_points = np.random.rand(self.dim) < CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def update_archive(self, x, fx):\n        if fx < np.max(self.archive_fitness):\n            worst_index = np.argmax(self.archive_fitness)\n            self.archive[worst_index] = x\n            self.archive_fitness[worst_index] = fx\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            self.generation += 1\n\n            # Population rejuvenation\n            if self.generation % self.rejuvenation_period == 0:\n                worst_index = np.argmax(self.fitness)\n                lb = func.bounds.lb\n                ub = func.bounds.ub\n                self.population[worst_index] = np.random.uniform(lb, ub, size=(self.dim))\n                self.fitness[worst_index] = func(self.population[worst_index])\n                self.budget -= 1\n                if self.fitness[worst_index] < self.f_opt:\n                    self.f_opt = self.fitness[worst_index]\n                    self.x_opt = self.population[worst_index]\n                self.update_archive(self.population[worst_index], self.fitness[worst_index])\n\n            for i in range(self.pop_size):\n                F = self.F_ema\n                CR = self.CR_ema\n\n                target = self.population[i]\n                mutant = self.mutate_with_archive(self.population, i, F)\n                trial = self.crossover(mutant, target, CR)\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    delta_f = self.fitness[i] - f_trial\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                    # Update EMA for F and CR based on success\n                    self.F_ema = self.ema_alpha * F + (1 - self.ema_alpha) * 0.9 # Move F towards higher value if successful\n                    self.CR_ema = self.ema_alpha * CR + (1 - self.ema_alpha) * 0.9 # Move CR towards higher value if successful\n\n                    self.update_archive(trial, f_trial)\n\n\n                else:\n                    # Adapt F and CR downwards when the trial is unsuccesful\n                    self.F_ema = self.ema_alpha * F + (1 - self.ema_alpha) * 0.1\n                    self.CR_ema = self.ema_alpha * CR + (1 - self.ema_alpha) * 0.1\n\n                if self.budget <= 0:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm ImprovedDE scored 0.523 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9ba400b0-5262-4b92-b492-304d1692336b"], "operator": null, "metadata": {"aucs": [0.18654943141163072, 0.29985011780020354, 0.4907098092340971, 0.8039832459371183, 0.6232369246588506, 0.6629651930135461, 0.4379151318151444, 0.49739026255000596, 0.5753679034765034, 0.2940608018955789, 0.38331832080367234, 0.9961351105860776, 0.2758860898364832, 0.51194957262599, 0.7765823687578045, 0.7126410620769574, 0.45982278782093033, 0.7362529493403409, 0.23270062704948846, 0.5089813309901059]}}
{"id": "9558a0d5-283f-4fa1-b9a0-2e90bc221fc4", "fitness": 0.6150443159949368, "name": "ImprovedDE", "description": "Improved Adaptive Differential Evolution with separate F/CR memory for successful and unsuccessful updates, and a more aggressive rejuvenation strategy.", "code": "import numpy as np\n\nclass ImprovedDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, memory_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.memory_size = memory_size\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.F_success_memory = [0.5] * memory_size\n        self.CR_success_memory = [0.7] * memory_size\n        self.F_failure_memory = [0.5] * memory_size\n        self.CR_failure_memory = [0.7] * memory_size\n        self.generation = 0\n        self.rejuvenation_period = 20 # Rejuvenate population more aggressively\n        self.rejuvenation_percentage = 0.2 # Percentage of population to rejuvenate\n\n    def initialize_population(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n    def mutate(self, pop, i, F):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n        mutant = a + F * (b - c)\n        return np.clip(mutant, -5, 5)\n\n    def crossover(self, mutant, target, CR):\n        cross_points = np.random.rand(self.dim) < CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def adapt_parameters(self, success, F, CR):\n        if success:\n            self.F_success_memory.append(F)\n            self.CR_success_memory.append(CR)\n            self.F_success_memory = self.F_success_memory[1:]\n            self.CR_success_memory = self.CR_success_memory[1:]\n        else:\n            self.F_failure_memory.append(F)\n            self.CR_failure_memory.append(CR)\n            self.F_failure_memory = self.F_failure_memory[1:]\n            self.CR_failure_memory = self.CR_failure_memory[1:]\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            self.generation += 1\n            \n            # Population rejuvenation\n            if self.generation % self.rejuvenation_period == 0:\n                num_rejuvenate = int(self.pop_size * self.rejuvenation_percentage)\n                worst_indices = np.argsort(self.fitness)[-num_rejuvenate:]\n                lb = func.bounds.lb\n                ub = func.bounds.ub\n                for i in worst_indices:\n                    self.population[i] = np.random.uniform(lb, ub, size=(self.dim))\n                    self.fitness[i] = func(self.population[i])\n                    self.budget -= 1\n                    if self.fitness[i] < self.f_opt:\n                        self.f_opt = self.fitness[i]\n                        self.x_opt = self.population[i]\n                    if self.budget <= 0:\n                        break\n                if self.budget <= 0:\n                    break\n            \n            for i in range(self.pop_size):\n                F_success = np.mean(self.F_success_memory)\n                CR_success = np.mean(self.CR_success_memory)\n                F_failure = np.mean(self.F_failure_memory)\n                CR_failure = np.mean(self.CR_failure_memory)\n                \n                #Probabilistically choose parameters from success or failure memory\n                if np.random.rand() < 0.7:\n                    F = F_success\n                    CR = CR_success\n                else:\n                    F = F_failure\n                    CR = CR_failure\n                \n                target = self.population[i]\n                mutant = self.mutate(self.population, i, F)\n                trial = self.crossover(mutant, target, CR)\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                    self.adapt_parameters(True, F, CR)\n                else:\n                    self.adapt_parameters(False, F, CR)\n\n                if self.budget <= 0:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm ImprovedDE scored 0.615 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9ba400b0-5262-4b92-b492-304d1692336b"], "operator": null, "metadata": {"aucs": [0.20524663698292256, 0.427786191757366, 0.6167270214708712, 0.8249701573553636, 0.7116698455145669, 0.7517554265522891, 0.582386676370253, 0.5526321785518056, 0.6622260395008273, 0.5958080506599915, 0.7790862247623418, 0.9988606354778027, 0.34195827344278373, 0.61306632789843, 0.8620801442989716, 0.7364848679765077, 0.5045627419903312, 0.7958876389674532, 0.2164238591433837, 0.5212673812244759]}}
{"id": "007e2dba-6cf8-4cb3-9a03-09a8dca01763", "fitness": 0.6208030307940432, "name": "DESimpleAdaptive", "description": "Simplified Differential Evolution with adaptive parameters and periodic archive refresh, focusing on parameter exploration and exploitation balance.", "code": "import numpy as np\n\nclass DESimpleAdaptive:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, refresh_interval=500):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.archive = []\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.F = 0.5\n        self.CR = 0.7\n        self.success_F = []\n        self.success_CR = []\n        self.refresh_interval = refresh_interval\n        self.eval_count = 0\n        self.lb = -5.0\n        self.ub = 5.0\n\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.eval_count += self.pop_size\n        \n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n\n    def mutate(self, pop, i):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n        mutant = a + self.F * (b - c)\n        return np.clip(mutant, self.lb, self.ub)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def update_archive(self, trial):\n        if len(self.archive) < self.archive_size:\n            self.archive.append(trial)\n        else:\n            idx = np.random.randint(self.archive_size)\n            self.archive[idx] = trial\n\n    def adapt_parameters(self, success, F, CR):\n        if success:\n            self.success_F.append(F)\n            self.success_CR.append(CR)\n\n        if len(self.success_F) > 0:\n            self.F = np.mean(self.success_F)\n            self.CR = np.mean(self.success_CR)\n\n            if len(self.success_F) > 10:\n                self.success_F.pop(0)\n                self.success_CR.pop(0)\n\n        self.F = np.clip(self.F, 0.1, 0.9)\n        self.CR = np.clip(self.CR, 0.1, 0.9)\n        \n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                target = self.population[i]\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, target)\n                f_trial = func(trial)\n                self.budget -= 1\n                self.eval_count += 1\n\n                if f_trial < self.fitness[i]:\n                    success = True\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    self.update_archive(trial)\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                else:\n                    success = False\n                    \n                self.adapt_parameters(success, self.F, self.CR)\n                \n                if self.eval_count % self.refresh_interval == 0:\n                     self.archive = []\n\n                if self.budget <= 0:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm DESimpleAdaptive scored 0.621 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["61b40155-ebbb-45d1-ae30-a9cbedb24fe5"], "operator": null, "metadata": {"aucs": [0.20369035626173515, 0.4725805628902944, 0.5700900802273934, 0.8104786541658575, 0.7012924015480595, 0.7547028656664238, 0.5850408267980873, 0.5748704016616956, 0.6604382750643716, 0.5466246157643737, 0.7759433662414918, 0.9954815615837567, 0.36328189443794845, 0.6073272711949784, 0.8806410482649178, 0.7584633768341202, 0.5289681642164901, 0.8151905402114568, 0.29488482116543624, 0.5160695316819762]}}
{"id": "73c20feb-2992-4684-a75f-7551ae5196d8", "fitness": 0.7612445935730847, "name": "DESimpleAdaptive", "description": "Simplified Differential Evolution with self-adaptive parameters, a smaller population, and a dynamically sized archive for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass DESimpleAdaptive:\n    def __init__(self, budget=10000, dim=10, pop_size=25, archive_size_factor=0.2, refresh_interval=500):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = int(pop_size * archive_size_factor)\n        self.archive = []\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.F = 0.5\n        self.CR = 0.7\n        self.refresh_interval = refresh_interval\n        self.eval_count = 0\n        self.sf_mem = []\n        self.scr_mem = []\n        self.mem_length = 5\n\n\n    def initialize_population(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.eval_count += self.pop_size\n        \n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n    def mutate(self, pop, i):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n        mutant = a + self.F * (b - c)\n\n        if len(self.archive) > 0 and np.random.rand() < 0.1:\n            arch_idx = np.random.randint(len(self.archive))\n            mutant = a + self.F * (self.archive[arch_idx] - b)\n\n        return np.clip(mutant, -5, 5)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def update_archive(self, trial):\n        if len(self.archive) < self.archive_size:\n            self.archive.append(trial)\n        else:\n            idx = np.random.randint(self.archive_size)\n            self.archive[idx] = trial\n\n    def adapt_parameters(self, success, F, CR):\n        if success:\n            self.sf_mem.append(F)\n            self.scr_mem.append(CR)\n            if len(self.sf_mem) > self.mem_length:\n                self.sf_mem.pop(0)\n                self.scr_mem.pop(0)\n        \n        if len(self.sf_mem) > 0:\n            self.F = np.mean(self.sf_mem)\n            self.CR = np.mean(self.scr_mem)\n            self.F = np.clip(self.F, 0.1, 0.9)\n            self.CR = np.clip(self.CR, 0.1, 0.9)\n        else:\n            self.F = 0.5\n            self.CR = 0.7\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                target = self.population[i]\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, target)\n                f_trial = func(trial)\n                self.budget -= 1\n                self.eval_count += 1\n\n                if f_trial < self.fitness[i]:\n                    self.update_archive(self.population[i].copy())\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                    self.adapt_parameters(True, self.F, self.CR)\n                else:\n                    self.adapt_parameters(False, self.F, self.CR)\n\n                if self.eval_count % self.refresh_interval == 0:\n                    self.archive = []\n\n                if self.budget <= 0:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm DESimpleAdaptive scored 0.761 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["61b40155-ebbb-45d1-ae30-a9cbedb24fe5"], "operator": null, "metadata": {"aucs": [0.32023912820157263, 0.6649272339378505, 0.762889578077314, 0.902320536272098, 0.8504333845218901, 0.878011141657407, 0.7905781663744245, 0.786471721280265, 0.8326884951884108, 0.6792820472832775, 0.8870636417059407, 0.9984282037465958, 0.3521116491475649, 0.8372368957695605, 0.9341228395105787, 0.8765149340985282, 0.7521232283225057, 0.9006996498429152, 0.5455654414831826, 0.6731839550398127]}}
{"id": "b313c3c1-91bc-446d-acc0-ca8650ce15fe", "fitness": 0.6097100254078912, "name": "DESimpleAdaptive", "description": "Simplified Differential Evolution with adaptive F/CR, archive interaction, and a less frequent archive refresh.", "code": "import numpy as np\n\nclass DESimpleAdaptive:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, refresh_interval=1000):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.archive = []\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.F = 0.5\n        self.CR = 0.7\n        self.success_F = []\n        self.success_CR = []\n        self.refresh_interval = refresh_interval\n        self.eval_count = 0\n\n\n    def initialize_population(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.eval_count += self.pop_size\n        \n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n    def mutate(self, pop, i):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n        mutant = a + self.F * (b - c)\n\n        #Utilize archive if available\n        if len(self.archive) > 0 and np.random.rand() < 0.1:\n            arch_idx = np.random.randint(len(self.archive))\n            mutant = a + self.F * (self.archive[arch_idx] - b)\n\n        return np.clip(mutant, -5, 5)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def update_archive(self, trial):\n        if len(self.archive) < self.archive_size:\n            self.archive.append(trial)\n        else:\n            idx = np.random.randint(self.archive_size)\n            self.archive[idx] = trial\n\n    def adapt_parameters(self, success, F, CR):\n        if success:\n            self.success_F.append(F)\n            self.success_CR.append(CR)\n\n        if len(self.success_F) > 0:\n            self.F = np.mean(self.success_F)\n            self.CR = np.mean(self.success_CR)\n\n            # Keep the lists short\n            if len(self.success_F) > 10:\n                self.success_F.pop(0)\n                self.success_CR.pop(0)\n\n\n        self.F = np.clip(self.F, 0.1, 0.9)\n        self.CR = np.clip(self.CR, 0.1, 0.9)\n        \n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                target = self.population[i]\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, target)\n                f_trial = func(trial)\n                self.budget -= 1\n                self.eval_count += 1\n\n                if f_trial < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    self.update_archive(trial)\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                    self.adapt_parameters(True, self.F, self.CR)\n                else:\n                    self.adapt_parameters(False, self.F, self.CR)\n                \n                if self.eval_count % self.refresh_interval == 0:\n                     self.archive = []\n\n                if self.budget <= 0:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm DESimpleAdaptive scored 0.610 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["61b40155-ebbb-45d1-ae30-a9cbedb24fe5"], "operator": null, "metadata": {"aucs": [0.21672412450986622, 0.3682094084326283, 0.5980149120405858, 0.8291564386587734, 0.6734271127958302, 0.7513986852929821, 0.5746092103909275, 0.5866997111784935, 0.6797343156440334, 0.5112981575938851, 0.7662096705837934, 0.9995705690648079, 0.33294112510611995, 0.6534353000650435, 0.8668311574304467, 0.7322013721108136, 0.47506028588608173, 0.7978501063121739, 0.2604443943313681, 0.5203844507291685]}}
{"id": "c5d8af2b-bff6-483b-8190-0933e100daaa", "fitness": -Infinity, "name": "DESimpleAdaptiveNiching", "description": "Differential Evolution with self-adaptive population size and F/CR parameters based on a weighted historical memory, combined with a niching strategy to promote diversity.", "code": "import numpy as np\n\nclass DESimpleAdaptiveNiching:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, niche_radius=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.archive = []\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.F = 0.5\n        self.CR = 0.7\n        self.memory_F = [0.5] * 5  # Memory for F\n        self.memory_CR = [0.7] * 5  # Memory for CR\n        self.memory_weights = [0.2] * 5  # Weights for memory entries\n        self.niche_radius = niche_radius\n        self.min_pop_size = 10  # Minimum population size\n        self.max_pop_size = 100  # Maximum population size\n\n    def initialize_population(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n    def mutate(self, i):\n        idxs = np.random.choice(self.pop_size, 3, replace=False)\n        a, b, c = self.population[idxs]\n        mutant = a + self.F * (b - c)\n        \n        if len(self.archive) > 0 and np.random.rand() < 0.1:\n            arch_idx = np.random.randint(len(self.archive))\n            mutant = a + self.F * (self.archive[arch_idx] - b)\n            \n        return np.clip(mutant, -5, 5)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def update_archive(self, trial):\n        if len(self.archive) < self.archive_size:\n            self.archive.append(trial)\n        else:\n            idx = np.random.randint(self.archive_size)\n            self.archive[idx] = trial\n\n    def adjust_population_size(self):\n        if self.f_opt == np.inf:\n            return\n        \n        # Simple heuristic to adjust population size\n        if np.std(self.fitness) < 1e-3:\n            self.pop_size = max(self.min_pop_size, self.pop_size // 2)  # Reduce population\n        else:\n            self.pop_size = min(self.max_pop_size, self.pop_size + 5)  # Increase population\n\n        self.population = self.population[:self.pop_size]\n        self.fitness = self.fitness[:self.pop_size]\n    \n    def niching(self):\n        for i in range(self.pop_size):\n            for j in range(i + 1, self.pop_size):\n                if np.linalg.norm(self.population[i] - self.population[j]) < self.niche_radius:\n                    if self.fitness[i] > self.fitness[j]:\n                        self.population[i] = np.random.uniform(-5, 5, self.dim)\n                        self.fitness[i] = np.inf  # Mark as needing evaluation\n                    else:\n                        self.population[j] = np.random.uniform(-5, 5, self.dim)\n                        self.fitness[j] = np.inf  # Mark as needing evaluation\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            self.niching()\n            \n            for i in range(self.pop_size):\n                if self.fitness[i] == np.inf:  # Re-evaluate individuals marked by niching\n                    self.fitness[i] = func(self.population[i])\n                    self.budget -=1\n                    if self.fitness[i] < self.f_opt:\n                        self.f_opt = self.fitness[i]\n                        self.x_opt = self.population[i]\n                    if self.budget <= 0:\n                        break\n                \n                target = self.population[i]\n                mutant = self.mutate(i)\n                trial = self.crossover(mutant, target)\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    weight = 0.8\n                    self.F = weight * self.F + (1 - weight) * self.memory_F[np.random.randint(0, len(self.memory_F))]\n                    self.CR = weight * self.CR + (1 - weight) * self.memory_CR[np.random.randint(0, len(self.memory_CR))]\n\n                    self.memory_F.pop(0)\n                    self.memory_CR.pop(0)\n                    self.memory_F.append(self.F)\n                    self.memory_CR.append(self.CR)\n\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    self.update_archive(trial)\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                \n                self.F = np.clip(self.F, 0.1, 0.9)\n                self.CR = np.clip(self.CR, 0.1, 0.9)\n\n\n                if self.budget <= 0:\n                    break\n\n            self.adjust_population_size()\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "An exception occurred: index 50 is out of bounds for axis 0 with size 50.", "error": "", "parent_ids": ["cea070f1-d81c-4228-918c-382ec746a15b"], "operator": null, "metadata": {}}
{"id": "c506e984-ddba-4866-8e18-bed5b6f5c204", "fitness": 0.6282363435046803, "name": "DESimpleAdaptive", "description": "Simplified Differential Evolution with adaptive F/CR, archive interaction, and periodic archive refresh with best individuals.", "code": "import numpy as np\n\nclass DESimpleAdaptive:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, refresh_interval=500, p_archive=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.archive = []\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.F = 0.5\n        self.CR = 0.7\n        self.success_F = []\n        self.success_CR = []\n        self.refresh_interval = refresh_interval\n        self.eval_count = 0\n        self.p_archive = p_archive\n\n\n    def initialize_population(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.eval_count += self.pop_size\n        \n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n    def mutate(self, pop, i):\n        idxs = np.random.choice(len(pop), 3, replace=False)\n        a, b, c = pop[idxs]\n        mutant = a + self.F * (b - c)\n\n        #Utilize archive if available\n        if len(self.archive) > 0 and np.random.rand() < self.p_archive:\n            arch_idx = np.random.randint(len(self.archive))\n            mutant = a + self.F * (self.archive[arch_idx] - b)\n\n        return np.clip(mutant, -5, 5)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def update_archive(self, trial):\n        if len(self.archive) < self.archive_size:\n            self.archive.append(trial)\n        else:\n            idx = np.random.randint(self.archive_size)\n            self.archive[idx] = trial\n\n    def adapt_parameters(self, success, F, CR):\n        if success:\n            self.success_F.append(F)\n            self.success_CR.append(CR)\n\n            if len(self.success_F) > 10:\n                self.success_F.pop(0)\n                self.success_CR.pop(0)\n\n        if len(self.success_F) > 0:\n            self.F = np.mean(self.success_F)\n            self.CR = np.mean(self.success_CR)\n\n        self.F = np.clip(self.F, 0.1, 0.9)\n        self.CR = np.clip(self.CR, 0.1, 0.9)\n        \n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                target = self.population[i]\n                F = self.F\n                CR = self.CR\n\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, target)\n                f_trial = func(trial)\n                self.budget -= 1\n                self.eval_count += 1\n\n                if f_trial < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                    self.adapt_parameters(True, F, CR)\n                else:\n                    self.adapt_parameters(False, F, CR)\n                \n                if self.eval_count % self.refresh_interval == 0:\n                    best_indices = np.argsort(self.fitness)[:self.archive_size]\n                    self.archive = [self.population[i] for i in best_indices]\n\n                if self.budget <= 0:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm DESimpleAdaptive scored 0.628 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["61b40155-ebbb-45d1-ae30-a9cbedb24fe5"], "operator": null, "metadata": {"aucs": [0.21740491062603817, 0.4005176923079472, 0.5907554948911722, 0.822936824195938, 0.7151701511639441, 0.7500696039624466, 0.5622762962591861, 0.5965444230889958, 0.6908040618308147, 0.6506225498107787, 0.8008582938883407, 0.9978657412953166, 0.33618340074201825, 0.6836331402679076, 0.8783792036625919, 0.765465295029977, 0.5498441102169199, 0.8062134917673981, 0.2345814420562352, 0.5146007430296361]}}
