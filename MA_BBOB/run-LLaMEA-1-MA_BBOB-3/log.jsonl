{"id": "b0c26a66-6109-42a8-ad7a-09e63175d9ee", "fitness": -Infinity, "name": "AdaptiveVelocitySearch", "description": "Population-based algorithm employing a velocity-based update rule with adaptive exploration and exploitation balance using a diversity metric.", "code": "import numpy as np\n\nclass AdaptiveVelocitySearch:\n    def __init__(self, budget=10000, dim=10, pop_size=20, inertia=0.7, cognitive_rate=1.5, social_rate=1.5, diversification_threshold=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.inertia = inertia\n        self.cognitive_rate = cognitive_rate\n        self.social_rate = social_rate\n        self.diversification_threshold = diversification_threshold\n        self.positions = None\n        self.velocities = None\n        self.fitness = None\n        self.best_position = None\n        self.best_fitness = np.Inf\n        self.eval_count = 0\n\n    def initialize_population(self, func):\n        self.positions = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.positions])\n        self.eval_count += self.pop_size\n        self.best_position = self.positions[np.argmin(self.fitness)]\n        self.best_fitness = np.min(self.fitness)\n\n    def update_velocity(self, func):\n        r1 = np.random.rand(self.pop_size, self.dim)\n        r2 = np.random.rand(self.pop_size, self.dim)\n        cognitive_component = self.cognitive_rate * r1 * (self.best_position - self.positions)\n        social_component = self.social_rate * r2 * (self.positions[np.argmin(self.fitness)] - self.positions)\n        self.velocities = self.inertia * self.velocities + cognitive_component + social_component\n\n        # Adaptive inertia based on population diversity\n        diversity = self.calculate_diversity()\n        if diversity < self.diversification_threshold:\n          self.inertia = min(self.inertia + 0.05, 0.9)  # Increase inertia to explore more\n        else:\n          self.inertia = max(self.inertia - 0.05, 0.4)  # Decrease inertia to exploit more\n\n    def update_position(self, func):\n        self.positions = self.positions + self.velocities\n        # Boundary handling\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.positions = np.clip(self.positions, lb, ub)\n\n        new_fitness = np.array([func(x) for x in self.positions])\n        self.eval_count += self.pop_size\n\n        for i in range(self.pop_size):\n            if new_fitness[i] < self.fitness[i]:\n                self.fitness[i] = new_fitness[i]\n                if new_fitness[i] < self.best_fitness:\n                    self.best_fitness = new_fitness[i]\n                    self.best_position = self.positions[i]\n\n    def calculate_diversity(self):\n        # Calculate the average distance of each particle from the population mean\n        mean_position = np.mean(self.positions, axis=0)\n        distances = np.linalg.norm(self.positions - mean_position, axis=1)\n        diversity = np.mean(distances) / (func.bounds.ub[0] - func.bounds.lb[0])  # Normalize by search space range\n        return diversity\n    \n    def __call__(self, func):\n        self.initialize_population(func)\n        while self.eval_count < self.budget:\n            self.update_velocity(func)\n            self.update_position(func)\n            \n            # Check budget again in case func calls in update_position overran the budget slightly.\n            if self.eval_count >= self.budget:\n                break\n        return self.best_fitness, self.best_position", "configspace": "", "generation": 0, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 68, in __call__\n  File \"<string>\", line 35, in update_velocity\n  File \"<string>\", line 62, in calculate_diversity\nNameError: name 'func' is not defined\n.", "error": "", "parent_ids": [], "operator": null, "metadata": {}}
{"id": "cdc47377-bd82-4930-b4a2-9a5790b2dfe5", "fitness": 0.0, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with Archive and stochastic ranking for constraint handling.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=100, f=0.5, cr=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = f  # Mutation factor\n        self.CR = cr # Crossover rate\n        self.archive = []\n        self.archive_fitness = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n\n        for gen in range(self.budget // self.pop_size):\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = self.pop[idxs]\n                v = x1 + self.F * (x2 - x3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = np.copy(self.pop[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        u[j] = v[j]\n                \n                # Selection\n                f = func(u)\n                self.budget -= 1\n                \n                if f < self.fitness[i]:\n                    \n                    if self.fitness[i] < self.f_opt:\n                        self.f_opt = self.fitness[i]\n                        self.x_opt = self.pop[i]\n                    \n                    \n                    \n                    # Update Archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.pop[i])\n                    else:\n                        idx_to_replace = np.random.randint(self.archive_size)\n                        self.archive[idx_to_replace] = self.pop[i]\n                    \n                    self.pop[i] = u\n                    self.fitness[i] = f\n                    \n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n                else:\n                    #If the candidate is not better, use archive with prob 0.1\n                    if np.random.rand() < 0.1 and len(self.archive) > 0:\n                        xa = self.archive[np.random.randint(len(self.archive))]\n                        v = x1 + self.F * (x2 - xa)\n                        v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                        u = np.copy(self.pop[i])\n                        j_rand = np.random.randint(self.dim)\n                        for j in range(self.dim):\n                            if np.random.rand() < self.CR or j == j_rand:\n                                u[j] = v[j]\n                        f = func(u)\n                        self.budget -= 1\n                        \n                        if f < self.fitness[i]:\n                            if self.fitness[i] < self.f_opt:\n                                self.f_opt = self.fitness[i]\n                                self.x_opt = self.pop[i]\n\n                            if len(self.archive) < self.archive_size:\n                                self.archive.append(self.pop[i])\n                            else:\n                                idx_to_replace = np.random.randint(len(self.archive))\n                                self.archive[idx_to_replace] = self.pop[i]\n\n                            self.pop[i] = u\n                            self.fitness[i] = f\n                            if f < self.f_opt:\n                                self.f_opt = f\n                                self.x_opt = u\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0]}}
{"id": "9612db1f-9e79-4b41-a3f0-303c59bb155a", "fitness": -Infinity, "name": "CMAES", "description": "Covariance matrix adaptation evolution strategy with a population size of 4 + floor(3*log(D)), where D is the dimensionality, using a mirrored sampling strategy.", "code": "import numpy as np\nimport math\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, sigma0 = 0.5):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n\n        self.popsize = 4 + math.floor(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n\n    def __call__(self, func):\n        # Initialize variables\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)  # Covariance matrix\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n        # Parameters (using common defaults)\n        c_sigma = (self.mu + 2) / (self.dim + self.mu + 5)\n        c_c = (4 + self.mu / self.dim) / (self.dim + 4 + 2 * self.mu / self.dim)\n        c_1 = 2 / ((self.dim + 1.3)**2 + self.mu)\n        c_mu = min(1 - c_1, 2 * (self.mu - 1 + 1 / self.mu) / ((self.dim + 2)**2 + self.mu))\n        d_sigma = 1 + 2 * max(0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1) + c_sigma\n\n        # Eigen decomposition of C (expensive, do it rarely)\n        C_evals, C_evecs = np.linalg.eigh(C)\n        C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n        \n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n\n        while eval_count < self.budget:\n            # Sample population\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            x = mean + sigma * (C_sqrt @ z.T).T\n            \n            # Mirror sampling to improve exploration\n            x_mirrored = mean - sigma * (C_sqrt @ z.T).T\n            x = np.concatenate([x, x_mirrored], axis=0)\n            z = np.concatenate([z, -z], axis=0)\n            \n            # Evaluate the new points, but respect the budget\n            f = np.array([func(xi) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n            eval_count += len(x) \n\n            # Sort by fitness\n            idx = np.argsort(f)\n            x = x[idx]\n            z = z[idx]\n            f = f[idx]\n\n            # Update optimal solution\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[0]\n                \n            # Selection and recombination\n            x_mu = x[:self.mu]\n            z_mu = z[:self.mu]\n\n            mean_new = np.mean(x_mu, axis=0)\n            z_w = np.mean(z_mu, axis=0)\n            \n            # Covariance matrix adaptation\n            ps = (1 - c_sigma) * ps + np.sqrt(c_sigma * (2 - c_sigma) * self.mu) * (C_sqrt @ z_w)\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - c_sigma)**(2 * eval_count / self.popsize)) < chiN * (1.4 + 2 / (self.dim + 1))\n            \n            pc = (1 - c_c) * pc + hsig * np.sqrt(c_c * (2 - c_c) * self.mu) * z_w\n\n            C = (1 - c_1 - c_mu) * C + c_1 * (pc[:, None] @ pc[None, :])\n            for i in range(self.mu):\n                C += c_mu * (z_mu[i, :, None] @ z_mu[i, None, :])\n                \n            # Update step size\n            sigma *= np.exp((c_sigma / d_sigma) * (np.linalg.norm(ps) / chiN - 1))\n                \n            # Update mean\n            mean = mean_new\n\n            # Eigen decomposition of C\n            if eval_count % (self.popsize * 10) == 0:  # Re-compute after every 10 generations\n                C = np.triu(C) + np.triu(C, 1).T  # Enforce symmetry\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_evals = np.maximum(C_evals, 1e-10) # Avoid zero or negative eigenvalues\n                C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                \n            if np.any(np.isnan(mean)) or np.any(np.isnan(C)):\n                print(\"NaN detected, resetting...\")\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                sigma = self.sigma0\n        \n        return f_opt, x_opt", "configspace": "", "generation": 0, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 120, in evaluate\n    algorithm = local_env[algorithm_name](budget=100, dim=2)\n  File \"<string>\", line 10, in __init__\nNameError: name 'math' is not defined\n.", "error": "", "parent_ids": [], "operator": null, "metadata": {}}
{"id": "7c561926-53cc-4c30-84dd-a6b1f890809f", "fitness": 0.15165510683534594, "name": "AdaptiveDE", "description": "Population-based algorithm employing a combination of global exploration and local exploitation using differential evolution operators and a restart mechanism.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.7, CR=0.9, restart_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.restart_prob = restart_prob #Probability of restarting population\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        if self.fitness[best_idx] < self.f_opt:\n            self.f_opt = self.fitness[best_idx]\n            self.x_opt = self.pop[best_idx]\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Mutation\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.pop[idxs]\n            x_mutated = x_r1 + self.F * (x_r2 - x_r3)\n            x_mutated = np.clip(x_mutated, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            x_trial = self.pop[i].copy()\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.CR or j == j_rand:\n                    x_trial[j] = x_mutated[j]\n\n            # Selection\n            f_trial = func(x_trial)\n            self.eval_count += 1\n\n            if f_trial < self.fitness[i]:\n                self.pop[i] = x_trial\n                self.fitness[i] = f_trial\n                \n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = x_trial\n                    \n    def restart_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        if self.fitness[best_idx] < self.f_opt:\n            self.f_opt = self.fitness[best_idx]\n            self.x_opt = self.pop[best_idx]\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n            if np.random.rand() < self.restart_prob:\n                self.restart_population(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDE scored 0.152 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.13648303368204417, 0.18585560247707267, 0.28428179118226693, 0]}}
{"id": "6a45a800-ce52-4bce-8238-0594324a4d9a", "fitness": 0.0, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with self-adaptive mutation factor and crossover rate, combined with a dynamically updated archive for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=100, f_min=0.1, f_max=0.9, cr_min=0.1, cr_max=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.f_min = f_min\n        self.f_max = f_max\n        self.cr_min = cr_min\n        self.cr_max = cr_max\n        self.archive = []\n        self.archive_fitness = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n\n        for gen in range(self.budget // self.pop_size):\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                F = np.random.uniform(self.f_min, self.f_max)\n                CR = np.random.uniform(self.cr_min, self.cr_max)\n\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = self.pop[idxs]\n                v = x1 + F * (x2 - x3)\n                \n                #Use archive with probability 0.1\n                if np.random.rand() < 0.1 and len(self.archive) > 0:\n                        xa = self.archive[np.random.randint(len(self.archive))]\n                        v = x1 + F * (x2 - xa)\n\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = np.copy(self.pop[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        u[j] = v[j]\n                \n                # Selection\n                f = func(u)\n                self.budget -= 1\n                \n                if f < self.fitness[i]:\n                    \n                    if self.fitness[i] < self.f_opt:\n                        self.f_opt = self.fitness[i]\n                        self.x_opt = self.pop[i]\n                    \n                    # Update Archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.pop[i])\n                    else:\n                        idx_to_replace = np.random.randint(len(self.archive))\n                        self.archive[idx_to_replace] = self.pop[i]\n                    \n                    self.pop[i] = u\n                    self.fitness[i] = f\n                    \n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n\n                # Dynamically update archive based on fitness\n                if len(self.archive) > 0:\n                    if f < np.max([func(x) for x in self.archive]): #Replace worst in archive\n                       worst_index = np.argmax([func(x) for x in self.archive])\n                       self.archive[worst_index] = u\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["cdc47377-bd82-4930-b4a2-9a5790b2dfe5"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "32d337e2-d993-4eb0-a2fb-4fcb9afd6c36", "fitness": 0.0, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with self-adaptive parameters, archive, and stochastic ranking for constraint handling, with a focus on improved parameter adaptation and archive management.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.7, CR=0.9, archive_size=10, restart_prob=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.restart_prob = restart_prob\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.archive = []  # Archive for storing promising solutions\n\n        # Self-adaptive parameters\n        self.F_memory = np.ones(self.pop_size) * 0.5\n        self.CR_memory = np.ones(self.pop_size) * 0.5\n        self.sf = np.ones(self.pop_size) * 0.5\n        self.scr = np.ones(self.pop_size) * 0.5\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        if self.fitness[best_idx] < self.f_opt:\n            self.f_opt = self.fitness[best_idx]\n            self.x_opt = self.pop[best_idx]\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Parameter adaptation\n            F = np.random.choice(self.sf)\n            CR = np.random.choice(self.scr)\n\n            # Mutation\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.pop[idxs]\n            x_mutated = x_r1 + F * (x_r2 - x_r3)\n            if len(self.archive) > 0 and np.random.rand() < 0.1:\n                x_r4 = self.archive[np.random.randint(len(self.archive))]\n                x_mutated = x_r1 + F * (x_r2 - x_r3) + F * (x_r4 - self.pop[i])\n            x_mutated = np.clip(x_mutated, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            x_trial = self.pop[i].copy()\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < CR or j == j_rand:\n                    x_trial[j] = x_mutated[j]\n\n            # Selection\n            f_trial = func(x_trial)\n            self.eval_count += 1\n\n            if f_trial < self.fitness[i]:\n                # Update parameters\n                self.sf[i] = F\n                self.scr[i] = CR\n\n                # Archive management\n                if len(self.archive) < self.archive_size:\n                    self.archive.append(self.pop[i].copy())\n                else:\n                    idx_to_replace = np.random.randint(self.archive_size)\n                    self.archive[idx_to_replace] = self.pop[i].copy()\n\n                self.pop[i] = x_trial\n                self.fitness[i] = f_trial\n                \n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = x_trial\n            else:\n                #If trial vector is not better, but better than the worst archive member replace worst archive member with it\n                if len(self.archive) > 0:\n                    worst_archive_index = np.argmax([func(x) for x in self.archive])\n                    if f_trial < func(self.archive[worst_archive_index]):\n                        self.archive[worst_archive_index] = x_trial\n\n    def restart_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.archive = []  # Clear the archive on restart\n        \n        best_idx = np.argmin(self.fitness)\n        if self.fitness[best_idx] < self.f_opt:\n            self.f_opt = self.fitness[best_idx]\n            self.x_opt = self.pop[best_idx]\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n            if np.random.rand() < self.restart_prob:\n                self.restart_population(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["7c561926-53cc-4c30-84dd-a6b1f890809f"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "d5909eeb-8779-465a-860d-860b63e6d0db", "fitness": -Infinity, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with self-adaptive parameters (F and CR) and a dynamically adjusted population size based on performance.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size_init=50, F_init=0.7, CR_init=0.9, restart_prob=0.1, pop_size_min=10, pop_size_max=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_init = pop_size_init\n        self.pop_size = pop_size_init\n        self.pop_size_min = pop_size_min\n        self.pop_size_max = pop_size_max\n        self.F_init = F_init  # Initial mutation factor\n        self.CR_init = CR_init  # Initial crossover rate\n        self.F = F_init\n        self.CR = CR_init\n        self.restart_prob = restart_prob #Probability of restarting population\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.success_F = []\n        self.success_CR = []\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        if self.fitness[best_idx] < self.f_opt:\n            self.f_opt = self.fitness[best_idx]\n            self.x_opt = self.pop[best_idx]\n\n    def evolve(self, func):\n        successful_individuals = []\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Mutation\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.pop[idxs]\n            x_mutated = x_r1 + self.F * (x_r2 - x_r3)\n            x_mutated = np.clip(x_mutated, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            x_trial = self.pop[i].copy()\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.CR or j == j_rand:\n                    x_trial[j] = x_mutated[j]\n\n            # Selection\n            f_trial = func(x_trial)\n            self.eval_count += 1\n\n            if f_trial < self.fitness[i]:\n                successful_individuals.append((self.F, self.CR))\n                self.pop[i] = x_trial\n                self.fitness[i] = f_trial\n                \n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = x_trial\n        self.update_parameters(successful_individuals)\n                    \n    def update_parameters(self, successful_individuals):\n        if successful_individuals:\n            self.success_F = [f for f, cr in successful_individuals]\n            self.success_CR = [cr for f, cr in successful_individuals]\n\n            if self.success_F:\n                self.F = np.mean(self.success_F)\n                self.F = 0.1 + 0.9 * self.F # Ensure F is not zero.\n            else:\n                self.F = self.F_init # Revert to initial value if no success\n\n            if self.success_CR:\n                self.CR = np.mean(self.success_CR)\n            else:\n                self.CR = self.CR_init # Revert to initial value if no success\n        else:\n            self.F = self.F_init\n            self.CR = self.CR_init\n\n    def restart_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        if self.fitness[best_idx] < self.f_opt:\n            self.f_opt = self.fitness[best_idx]\n            self.x_opt = self.pop[best_idx]\n            \n    def adjust_population_size(self):\n        # Simple adjustment based on improvement rate\n        improvement_rate = 0\n        if self.eval_count > self.pop_size_init:\n            improvement_rate = (self.f_opt_prev - self.f_opt) / self.f_opt_prev if self.f_opt_prev != 0 else 0  #Avoid division by zero\n\n        if improvement_rate > 0.01: #tune\n            self.pop_size = min(self.pop_size + 5, self.pop_size_max) #increase by 5\n        elif improvement_rate < 0.001: #tune\n            self.pop_size = max(self.pop_size - 5, self.pop_size_min) #decrease by 5\n            \n        self.pop_size = int(self.pop_size)\n        #print(f\"Adjusting population size to {self.pop_size}\")\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.pop_size = self.pop_size_init\n        self.initialize_population(func)\n        self.f_opt_prev = np.Inf #Initialize previous best fitness\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n            if np.random.rand() < self.restart_prob:\n                self.restart_population(func)\n            \n            self.adjust_population_size()\n            self.f_opt_prev = self.f_opt #Store current best fitness before next iteration\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 144, in evaluate\n    algorithm(f_new)\n  File \"<string>\", line 119, in __call__\n  File \"<string>\", line 42, in evolve\nIndexError: index 51 is out of bounds for axis 0 with size 50\n.", "error": "", "parent_ids": ["7c561926-53cc-4c30-84dd-a6b1f890809f"], "operator": null, "metadata": {}}
{"id": "62ad0f34-238a-4daa-a867-666ddc1508cf", "fitness": 0.0, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with self-adaptive parameters, archive, and periodic local search to refine solutions.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=100, f_init=0.5, cr_init=0.9, local_search_interval=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = f_init  # Initial mutation factor\n        self.CR = cr_init # Initial Crossover rate\n        self.archive = []\n        self.archive_fitness = []\n        self.local_search_interval = local_search_interval\n        self.generation = 0\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Adaptive Parameter Control\n                self.F = np.clip(np.random.normal(0.5, 0.3), 0.1, 1.0) # Self-adaptive F\n                self.CR = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0) # Self-adaptive CR\n\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = self.pop[idxs]\n                v = x1 + self.F * (x2 - x3)\n                \n                #Using archive with probability of 0.2\n                if np.random.rand() < 0.2 and len(self.archive) > 0:\n                    xa = self.archive[np.random.randint(len(self.archive))]\n                    v = x1 + self.F * (x2 - xa)\n                    \n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = np.copy(self.pop[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        u[j] = v[j]\n                \n                # Selection\n                f = func(u)\n                self.budget -= 1\n                \n                if f < self.fitness[i]:\n                    \n                    if self.fitness[i] < self.f_opt:\n                        self.f_opt = self.fitness[i]\n                        self.x_opt = self.pop[i]\n                    \n                    # Update Archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.pop[i])\n                    else:\n                        idx_to_replace = np.random.randint(len(self.archive))\n                        self.archive[idx_to_replace] = self.pop[i]\n                    \n                    self.pop[i] = u\n                    self.fitness[i] = f\n                    \n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n\n            # Periodic Local Search\n            if self.generation % self.local_search_interval == 0:\n                best_idx = np.argmin(self.fitness)\n                x_local = self.pop[best_idx].copy()\n                \n                # Perturb each dimension with small random step\n                for j in range(self.dim):\n                    x_local_perturbed = x_local.copy()\n                    x_local_perturbed[j] += np.random.normal(0, 0.1)  # Small perturbation\n                    x_local_perturbed = np.clip(x_local_perturbed, func.bounds.lb, func.bounds.ub)\n                    f_local = func(x_local_perturbed)\n                    self.budget -=1\n\n                    if f_local < self.fitness[best_idx] and self.budget > 0:\n                        self.pop[best_idx] = x_local_perturbed\n                        self.fitness[best_idx] = f_local\n                        if f_local < self.f_opt:\n                            self.f_opt = f_local\n                            self.x_opt = x_local_perturbed\n\n            self.generation += 1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["cdc47377-bd82-4930-b4a2-9a5790b2dfe5"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "d01bf23d-21ac-4381-a31a-36429909013e", "fitness": 0.44454857032639994, "name": "AdaptiveVelocitySearch", "description": "Population-based optimization with adaptive inertia and velocity clamping for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveVelocitySearch:\n    def __init__(self, budget=10000, dim=10, pop_size=20, inertia_start=0.9, inertia_end=0.4, cognitive_rate=1.5, social_rate=1.5, diversification_threshold=0.1, velocity_clamp=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.inertia_start = inertia_start\n        self.inertia_end = inertia_end\n        self.cognitive_rate = cognitive_rate\n        self.social_rate = social_rate\n        self.diversification_threshold = diversification_threshold\n        self.velocity_clamp = velocity_clamp\n        self.positions = None\n        self.velocities = None\n        self.fitness = None\n        self.best_position = None\n        self.best_fitness = np.Inf\n        self.eval_count = 0\n        self.func = None\n\n    def initialize_population(self):\n        self.positions = np.random.uniform(self.func.bounds.lb, self.func.bounds.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.random.uniform(-self.velocity_clamp, self.velocity_clamp, size=(self.pop_size, self.dim))\n        self.fitness = np.array([self.func(x) for x in self.positions])\n        self.eval_count += self.pop_size\n        self.best_position = self.positions[np.argmin(self.fitness)]\n        self.best_fitness = np.min(self.fitness)\n\n    def update_velocity(self):\n        r1 = np.random.rand(self.pop_size, self.dim)\n        r2 = np.random.rand(self.pop_size, self.dim)\n        cognitive_component = self.cognitive_rate * r1 * (self.best_position - self.positions)\n        social_component = self.social_rate * r2 * (self.positions[np.argmin(self.fitness)] - self.positions)\n        inertia = self.inertia_start - (self.inertia_start - self.inertia_end) * (self.eval_count / self.budget)\n        self.velocities = inertia * self.velocities + cognitive_component + social_component\n\n        # Velocity clamping\n        self.velocities = np.clip(self.velocities, -self.velocity_clamp, self.velocity_clamp)\n\n    def update_position(self):\n        self.positions = self.positions + self.velocities\n        # Boundary handling\n        lb = self.func.bounds.lb\n        ub = self.func.bounds.ub\n        self.positions = np.clip(self.positions, lb, ub)\n\n        new_fitness = np.array([self.func(x) for x in self.positions])\n        self.eval_count += self.pop_size\n\n        for i in range(self.pop_size):\n            if new_fitness[i] < self.fitness[i]:\n                self.fitness[i] = new_fitness[i]\n                if new_fitness[i] < self.best_fitness:\n                    self.best_fitness = new_fitness[i]\n                    self.best_position = self.positions[i]\n\n    def calculate_diversity(self):\n        # Calculate the average distance of each particle from the population mean\n        mean_position = np.mean(self.positions, axis=0)\n        distances = np.linalg.norm(self.positions - mean_position, axis=1)\n        diversity = np.mean(distances) / (self.func.bounds.ub[0] - self.func.bounds.lb[0])  # Normalize by search space range\n        return diversity\n    \n    def __call__(self, func):\n        self.func = func\n        self.initialize_population()\n        while self.eval_count < self.budget:\n            self.update_velocity()\n            self.update_position()\n            \n            # Check budget again in case func calls in update_position overran the budget slightly.\n            if self.eval_count >= self.budget:\n                break\n        return self.best_fitness, self.best_position", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveVelocitySearch scored 0.445 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b0c26a66-6109-42a8-ad7a-09e63175d9ee"], "operator": null, "metadata": {"aucs": [0.167234715799486, 0.23007600105273818, 0.5795067657286217, 0.17940639732522123, 0.2641410451652749, 0.7098206903937732, 0.30714488527726036, 0.4557471971201231, 0.6647834030213179, 0.2294527177627178, 0.7931453144308621, 0.9989159257561663, 0.2639533625749727, 0.2867703206322846, 0.7303157667815442, 0.6637157118694241, 0.25856999050968454, 0.3790638573274404, 0.21010226974477897, 0.5191050682543084]}}
{"id": "fb15a393-a626-4abb-b121-f282b961e3e0", "fitness": 0.1570701855204661, "name": "AdaptiveVelocitySearch", "description": "Adaptive Velocity Search with dynamic inertia and velocity clamping based on remaining budget to balance exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveVelocitySearch:\n    def __init__(self, budget=10000, dim=10, pop_size=20, inertia=0.7, cognitive_rate=1.5, social_rate=1.5, diversification_threshold=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.inertia = inertia\n        self.cognitive_rate = cognitive_rate\n        self.social_rate = social_rate\n        self.diversification_threshold = diversification_threshold\n        self.positions = None\n        self.velocities = None\n        self.fitness = None\n        self.best_position = None\n        self.best_fitness = np.Inf\n        self.eval_count = 0\n        self.lb = None\n        self.ub = None\n\n    def initialize_population(self, func):\n        self.lb = func.bounds.lb\n        self.ub = func.bounds.ub\n        self.positions = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.positions])\n        self.eval_count += self.pop_size\n        self.best_position = self.positions[np.argmin(self.fitness)]\n        self.best_fitness = np.min(self.fitness)\n\n    def update_velocity(self, func):\n        r1 = np.random.rand(self.pop_size, self.dim)\n        r2 = np.random.rand(self.pop_size, self.dim)\n        cognitive_component = self.cognitive_rate * r1 * (self.best_position - self.positions)\n        social_component = self.social_rate * r2 * (self.positions[np.argmin(self.fitness)] - self.positions)\n        self.velocities = self.inertia * self.velocities + cognitive_component + social_component\n        \n        # Velocity clamping based on remaining budget\n        remaining_evals = self.budget - self.eval_count\n        max_velocity = (self.ub[0] - self.lb[0]) / remaining_evals if remaining_evals > 0 else (self.ub[0] - self.lb[0])\n        self.velocities = np.clip(self.velocities, -max_velocity, max_velocity)\n\n        # Adaptive inertia based on population diversity\n        diversity = self.calculate_diversity(func)\n        if diversity < self.diversification_threshold:\n          self.inertia = min(self.inertia + 0.05, 0.9)  # Increase inertia to explore more\n        else:\n          self.inertia = max(self.inertia - 0.05, 0.4)  # Decrease inertia to exploit more\n\n\n    def update_position(self, func):\n        self.positions = self.positions + self.velocities\n        # Boundary handling\n        self.positions = np.clip(self.positions, self.lb, self.ub)\n\n        new_fitness = np.array([func(x) for x in self.positions])\n        self.eval_count += self.pop_size\n\n        for i in range(self.pop_size):\n            if new_fitness[i] < self.fitness[i]:\n                self.fitness[i] = new_fitness[i]\n                if new_fitness[i] < self.best_fitness:\n                    self.best_fitness = new_fitness[i]\n                    self.best_position = self.positions[i]\n\n    def calculate_diversity(self, func):\n        # Calculate the average distance of each particle from the population mean\n        mean_position = np.mean(self.positions, axis=0)\n        distances = np.linalg.norm(self.positions - mean_position, axis=1)\n        diversity = np.mean(distances) / (self.ub[0] - self.lb[0])  # Normalize by search space range\n        return diversity\n    \n    def __call__(self, func):\n        self.initialize_population(func)\n        while self.eval_count < self.budget:\n            self.update_velocity(func)\n            self.update_position(func)\n            \n            # Check budget again in case func calls in update_position overran the budget slightly.\n            if self.eval_count >= self.budget:\n                break\n        return self.best_fitness, self.best_position", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveVelocitySearch scored 0.157 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b0c26a66-6109-42a8-ad7a-09e63175d9ee"], "operator": null, "metadata": {"aucs": [0.08404575701133676, 0.10606808112812649, 0.22434220431608431, 0.1576102405879063, 0.14934552490334285, 0.17275342604640176, 0.22910476112341926, 0.1412227077781839, 0.15922363127469485, 0.12490571997079203, 0.1453259556173112, 0.19307167256363422, 0.10724318801850963, 0.15455115130951325, 0.12398156725940335, 0.21985376218084762, 0.15163714453742283, 0.18351672457316393, 0.14701669780994908, 0.16658379239927767]}}
{"id": "52306c16-dbc7-43b7-ac05-811fd7674d8a", "fitness": 0.7032504969869062, "name": "CMAES", "description": "CMA-ES with corrected initialization and NaN handling, and simplified eigen decomposition frequency.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, sigma0 = 0.5):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n\n        self.popsize = 4 + int(np.floor(3 * np.log(self.dim)))\n        self.mu = self.popsize // 2\n\n    def __call__(self, func):\n        # Initialize variables\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)  # Covariance matrix\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n        # Parameters (using common defaults)\n        c_sigma = (self.mu + 2) / (self.dim + self.mu + 5)\n        c_c = (4 + self.mu / self.dim) / (self.dim + 4 + 2 * self.mu / self.dim)\n        c_1 = 2 / ((self.dim + 1.3)**2 + self.mu)\n        c_mu = min(1 - c_1, 2 * (self.mu - 1 + 1 / self.mu) / ((self.dim + 2)**2 + self.mu))\n        d_sigma = 1 + 2 * max(0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1) + c_sigma\n\n        # Eigen decomposition of C (expensive, do it rarely)\n        C_evals, C_evecs = np.linalg.eigh(C)\n        C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n        \n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n\n        while eval_count < self.budget:\n            # Sample population\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            x = mean + sigma * (C_sqrt @ z.T).T\n            \n            # Mirror sampling to improve exploration\n            x_mirrored = mean - sigma * (C_sqrt @ z.T).T\n            x = np.concatenate([x, x_mirrored], axis=0)\n            z = np.concatenate([z, -z], axis=0)\n            \n            # Evaluate the new points, but respect the budget\n            f = np.array([func(xi) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n            eval_count += len(x) \n\n            # Sort by fitness\n            idx = np.argsort(f)\n            x = x[idx]\n            z = z[idx]\n            f = f[idx]\n\n            # Update optimal solution\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[0]\n                \n            # Selection and recombination\n            x_mu = x[:self.mu]\n            z_mu = z[:self.mu]\n\n            mean_new = np.mean(x_mu, axis=0)\n            z_w = np.mean(z_mu, axis=0)\n            \n            # Covariance matrix adaptation\n            ps = (1 - c_sigma) * ps + np.sqrt(c_sigma * (2 - c_sigma) * self.mu) * (C_sqrt @ z_w)\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - c_sigma)**(2 * eval_count / self.popsize)) < chiN * (1.4 + 2 / (self.dim + 1))\n            \n            pc = (1 - c_c) * pc + hsig * np.sqrt(c_c * (2 - c_c) * self.mu) * z_w\n\n            C = (1 - c_1 - c_mu) * C + c_1 * (pc[:, None] @ pc[None, :])\n            for i in range(self.mu):\n                C += c_mu * (z_mu[i, :, None] @ z_mu[i, None, :])\n                \n            # Update step size\n            sigma *= np.exp((c_sigma / d_sigma) * (np.linalg.norm(ps) / chiN - 1))\n                \n            # Update mean\n            mean = mean_new\n\n            # Eigen decomposition of C\n            if eval_count % (self.popsize * 5) == 0:  # Re-compute after every 5 generations\n                C = np.triu(C) + np.triu(C, 1).T  # Enforce symmetry\n                try:\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_evals = np.maximum(C_evals, 1e-10) # Avoid zero or negative eigenvalues\n                    C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                except np.linalg.LinAlgError:\n                    print(\"LinAlgError encountered, resetting C\")\n                    C = np.eye(self.dim)\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                \n            if np.any(np.isnan(mean)) or np.any(np.isnan(C)):\n                print(\"NaN detected, resetting...\")\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                sigma = self.sigma0\n        \n        return f_opt, x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm CMAES scored 0.703 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9612db1f-9e79-4b41-a3f0-303c59bb155a"], "operator": null, "metadata": {"aucs": [0.2495969035246135, 0.829489406972368, 0.8348641517081864, 0.9330600381155151, 0.8870928847989532, 0.9132043198131029, 0.5928443501967072, 0.8655217177342486, 0.8982195024189039, 0.7485842892235302, 0.924572781390354, 0.9880409087887473, 0.2193324471607575, 0.871662472524585, 0.6127647468211204, 0.3331498604530029, 0.7996955720919543, 0.9072028883030463, 0.23671029783733832, 0.41940039986109034]}}
{"id": "2cc39ae0-4204-4f8b-aa4f-1549d2eef521", "fitness": 0.6509352811941682, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with dynamically adjusted mutation factor and crossover rate based on population diversity and success rate, employing an archive and stochastic ranking.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=100, f_initial=0.5, cr_initial=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = f_initial  # Initial mutation factor\n        self.CR = cr_initial # Initial Crossover rate\n        self.archive = []\n        self.archive_fitness = []\n        self.success_F = []\n        self.success_CR = []\n        self.success_count = 0\n        self.memory_size = 10\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n\n        for gen in range(self.budget // self.pop_size):\n            new_pop = np.copy(self.pop)\n            new_fitness = np.copy(self.fitness)\n\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = self.pop[idxs]\n\n                # Use archive with a probability\n                if np.random.rand() < 0.1 and len(self.archive) > 0:\n                    xa = self.archive[np.random.randint(len(self.archive))]\n                    v = x1 + self.F * (x2 - xa)\n                else:\n                    v = x1 + self.F * (x2 - x3)\n\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = np.copy(self.pop[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        u[j] = v[j]\n                \n                # Selection\n                f = func(u)\n                self.budget -= 1\n                \n                if f < self.fitness[i]:\n                    self.success_count += 1\n                    self.success_F.append(self.F)\n                    self.success_CR.append(self.CR)\n\n                    if self.fitness[i] < self.f_opt:\n                        self.f_opt = self.fitness[i]\n                        self.x_opt = self.pop[i]\n                    \n                    # Update Archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.pop[i])\n                    else:\n                        idx_to_replace = np.random.randint(self.archive_size)\n                        self.archive[idx_to_replace] = self.pop[i]\n                    \n                    new_pop[i] = u\n                    new_fitness[i] = f\n                    \n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n                else:\n                    new_pop[i] = self.pop[i]\n                    new_fitness[i] = self.fitness[i]\n            \n            #Adaptive F and CR\n            if self.success_count > self.memory_size:\n                self.F = np.mean(self.success_F[-self.memory_size:])\n                self.CR = np.mean(self.success_CR[-self.memory_size:])\n                self.success_F = []\n                self.success_CR = []\n                self.success_count = 0\n            \n            self.pop = new_pop\n            self.fitness = new_fitness\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDE scored 0.651 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["cdc47377-bd82-4930-b4a2-9a5790b2dfe5"], "operator": null, "metadata": {"aucs": [0.26551831560076, 0.6267821330240245, 0.580871016220261, 0.8118772332126173, 0.6686565952053445, 0.7505411445086824, 0.5944376937553981, 0.5838110894410843, 0.676539210824316, 0.6187434282168974, 0.8256483525701749, 0.9991636115464811, 0.5967393063244899, 0.6679008762527556, 0.9042152107948895, 0.7216393663589733, 0.5388431725648999, 0.7854519208468055, 0.23733806637514077, 0.5639878802393705]}}
{"id": "1a015ef9-c330-4bbe-8d14-f960084bfe5c", "fitness": 0.3327126672143279, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with self-adaptive parameters and a repair mechanism to enhance exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_init=0.5, CR_init=0.9, restart_prob=0.1, F_adapt_rate=0.1, CR_adapt_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = np.full(pop_size, F_init)  # Mutation factor\n        self.CR = np.full(pop_size, CR_init)  # Crossover rate\n        self.restart_prob = restart_prob #Probability of restarting population\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.F_adapt_rate = F_adapt_rate\n        self.CR_adapt_rate = CR_adapt_rate\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        if self.fitness[best_idx] < self.f_opt:\n            self.f_opt = self.fitness[best_idx]\n            self.x_opt = self.pop[best_idx]\n\n    def repair(self, x, lb, ub):\n        \"\"\"Repair solution to lie within bounds.\"\"\"\n        return np.clip(x, lb, ub)\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Mutation\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.pop[idxs]\n            x_mutated = x_r1 + self.F[i] * (x_r2 - x_r3)\n            x_mutated = self.repair(x_mutated, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            x_trial = self.pop[i].copy()\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.CR[i] or j == j_rand:\n                    x_trial[j] = x_mutated[j]\n            \n            x_trial = self.repair(x_trial, func.bounds.lb, func.bounds.ub)\n\n            # Selection\n            f_trial = func(x_trial)\n            self.eval_count += 1\n\n            if f_trial < self.fitness[i]:\n                # Update parameters if the trial vector is better\n                self.F[i] = np.clip(self.F[i] + self.F_adapt_rate * np.random.normal(0, 1), 0.1, 1.0)\n                self.CR[i] = np.clip(self.CR[i] + self.CR_adapt_rate * np.random.normal(0, 1), 0.1, 1.0)\n\n                self.pop[i] = x_trial\n                self.fitness[i] = f_trial\n                \n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = x_trial\n            else:\n                 # If the trial vector is worse, slightly adjust parameters\n                self.F[i] = np.clip(self.F[i] - self.F_adapt_rate * np.random.normal(0, 1), 0.1, 1.0)\n                self.CR[i] = np.clip(self.CR[i] - self.CR_adapt_rate * np.random.normal(0, 1), 0.1, 1.0)\n                    \n    def restart_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.F = np.full(self.pop_size, 0.5)\n        self.CR = np.full(self.pop_size, 0.9)\n        \n        best_idx = np.argmin(self.fitness)\n        if self.fitness[best_idx] < self.f_opt:\n            self.f_opt = self.fitness[best_idx]\n            self.x_opt = self.pop[best_idx]\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n            if np.random.rand() < self.restart_prob:\n                self.restart_population(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDE scored 0.333 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["7c561926-53cc-4c30-84dd-a6b1f890809f"], "operator": null, "metadata": {"aucs": [0.16423015745016656, 0.2549380400241277, 0.32567680415513667, 0.344232793888229, 0.2530263450766743, 0.36105448112505234, 0.29360535117253006, 0.30172097358996175, 0.3041594366107042, 0.19795079432182128, 0.31394496199500077, 0.9946468823301572, 0.2875090675219072, 0.27745756037973734, 0.6690793529649801, 0.35948111451334397, 0.2861138927383716, 0]}}
{"id": "8efcbe0d-7d04-4a71-9eea-a102c991adeb", "fitness": 0.46579610527045395, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with self-adaptive parameters and a focused restart mechanism based on population convergence.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_mu=0.5, F_sigma=0.3, CR_mu=0.5, CR_sigma=0.1, restart_prob=0.05, convergence_threshold=1e-6):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_mu = F_mu  # Mean mutation factor\n        self.F_sigma = F_sigma # Std dev of mutation factor\n        self.CR_mu = CR_mu  # Mean crossover rate\n        self.CR_sigma = CR_sigma # Std dev of crossover rate\n        self.restart_prob = restart_prob # Probability of restarting population\n        self.convergence_threshold = convergence_threshold\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.F_values = np.ones(pop_size) * F_mu\n        self.CR_values = np.ones(pop_size) * CR_mu\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        if self.fitness[best_idx] < self.f_opt:\n            self.f_opt = self.fitness[best_idx]\n            self.x_opt = self.pop[best_idx]\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Adaptive F and CR\n            self.F_values[i] = np.clip(np.random.normal(self.F_mu, self.F_sigma), 0.1, 1.0)\n            self.CR_values[i] = np.clip(np.random.normal(self.CR_mu, self.CR_sigma), 0.1, 1.0)\n            F = self.F_values[i]\n            CR = self.CR_values[i]\n\n\n            # Mutation\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.pop[idxs]\n            x_mutated = x_r1 + F * (x_r2 - x_r3)\n            x_mutated = np.clip(x_mutated, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            x_trial = self.pop[i].copy()\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < CR or j == j_rand:\n                    x_trial[j] = x_mutated[j]\n\n            # Selection\n            f_trial = func(x_trial)\n            self.eval_count += 1\n\n            if f_trial < self.fitness[i]:\n                \n                # Update F and CR based on success (optional)\n                # If trial vector is better, adjust F and CR towards the values used\n                # self.F_mu = 0.9 * self.F_mu + 0.1 * F\n                # self.CR_mu = 0.9 * self.CR_mu + 0.1 * CR\n                \n                self.pop[i] = x_trial\n                self.fitness[i] = f_trial\n                \n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = x_trial\n                    \n    def restart_population(self, func):\n        # Focused restart: generate new population around the current best solution\n        radius = 0.1 * (func.bounds.ub - func.bounds.lb)  # Adjust radius as needed\n        self.pop = np.random.uniform(np.maximum(func.bounds.lb, self.x_opt - radius), np.minimum(func.bounds.ub, self.x_opt + radius), size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        if self.fitness[best_idx] < self.f_opt:\n            self.f_opt = self.fitness[best_idx]\n            self.x_opt = self.pop[best_idx]\n\n    def check_convergence(self):\n        # Check if the population has converged (small variance in fitness)\n        return np.std(self.fitness) < self.convergence_threshold\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n            \n            if self.check_convergence() or np.random.rand() < self.restart_prob:\n                self.restart_population(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDE scored 0.466 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["7c561926-53cc-4c30-84dd-a6b1f890809f"], "operator": null, "metadata": {"aucs": [0.17077772330425178, 0.22023120519467942, 0.45749396952466026, 0.4675230757398827, 0.566695613266974, 0.6126562651842207, 0.3031758495229596, 0.4868478238865168, 0.35531125081485593, 0.2126906840462348, 0.633838043811324, 0.9962417341855712, 0.2754384919621693, 0.4952731387733924, 0.7623598033222646, 0.6241562635675895, 0.3706635626513225, 0.5957780825089678, 0.2047408735692986, 0.5040286505719438]}}
{"id": "553a3557-45a5-43ed-bc85-19665369b564", "fitness": 0.5578084821122775, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with self-adaptive mutation factor and crossover rate, and a simplified restart mechanism.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_init=0.5, CR_init=0.7, restart_prob=0.05, F_adapt_rate=0.1, CR_adapt_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = np.full(pop_size, F_init)  # Mutation factor for each individual\n        self.CR = np.full(pop_size, CR_init)  # Crossover rate for each individual\n        self.restart_prob = restart_prob\n        self.F_adapt_rate = F_adapt_rate\n        self.CR_adapt_rate = CR_adapt_rate\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        if self.fitness[best_idx] < self.f_opt:\n            self.f_opt = self.fitness[best_idx]\n            self.x_opt = self.pop[best_idx]\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Mutation\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.pop[idxs]\n            x_mutated = x_r1 + self.F[i] * (x_r2 - x_r3)\n            x_mutated = np.clip(x_mutated, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            x_trial = self.pop[i].copy()\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.CR[i] or j == j_rand:\n                    x_trial[j] = x_mutated[j]\n\n            # Selection\n            f_trial = func(x_trial)\n            self.eval_count += 1\n\n            if f_trial < self.fitness[i]:\n                # Adaptation of F and CR\n                self.F[i] = np.clip(self.F[i] * (1 + self.F_adapt_rate * np.random.normal(0, 1)), 0.1, 1.0)\n                self.CR[i] = np.clip(self.CR[i] * (1 + self.CR_adapt_rate * np.random.normal(0, 1)), 0.1, 1.0)\n\n                self.pop[i] = x_trial\n                self.fitness[i] = f_trial\n                \n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = x_trial\n            else:\n                # Adaptation of F and CR (opposite direction if no improvement)\n                self.F[i] = np.clip(self.F[i] * (1 - self.F_adapt_rate * np.random.normal(0, 1)), 0.1, 1.0)\n                self.CR[i] = np.clip(self.CR[i] * (1 - self.CR_adapt_rate * np.random.normal(0, 1)), 0.1, 1.0)\n\n                    \n    def restart_population(self, func):\n        # Restart only the worst half of the population\n        worst_half_indices = np.argsort(self.fitness)[self.pop_size // 2:]\n        \n        new_pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(len(worst_half_indices), self.dim))\n        new_fitness = np.array([func(x) for x in new_pop])\n        self.eval_count += len(worst_half_indices)\n\n        self.pop[worst_half_indices] = new_pop\n        self.fitness[worst_half_indices] = new_fitness\n        \n        best_idx = np.argmin(self.fitness)\n        if self.fitness[best_idx] < self.f_opt:\n            self.f_opt = self.fitness[best_idx]\n            self.x_opt = self.pop[best_idx]\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n            if np.random.rand() < self.restart_prob:\n                self.restart_population(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDE scored 0.558 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["7c561926-53cc-4c30-84dd-a6b1f890809f"], "operator": null, "metadata": {"aucs": [0.21037130811655447, 0.38179806177566866, 0.5269938433606884, 0.7722127148508096, 0.6790590552395221, 0.6508815487062447, 0.4581088571745455, 0.43702072502630973, 0.5577863696056804, 0.5535455926646777, 0.724277395066192, 0.9888516114415256, 0.3442988405818448, 0.5289313118177895, 0.858194845341888, 0.580430747231643, 0.4657721299348765, 0.704584427628732, 0.22184681553710883, 0.5112034411432476]}}
{"id": "db401c1d-e29c-43db-8c90-56c0952c6ffb", "fitness": 0.4013968637445098, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with self-adaptive parameters and a success history archive to enhance exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, archive_size=10, restart_prob=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.archive_size = archive_size\n        self.archive_F = []\n        self.archive_CR = []\n        self.restart_prob = restart_prob\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.success_F = []\n        self.success_CR = []\n        self.success_fitness_diff = []\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        if self.fitness[best_idx] < self.f_opt:\n            self.f_opt = self.fitness[best_idx]\n            self.x_opt = self.pop[best_idx]\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Parameter adaptation\n            if self.success_F:\n                self.F = np.clip(np.random.choice(self.success_F), 0.1, 1.0)\n            else:\n                self.F = np.random.uniform(0.1, 0.9) #Exploration if no prior success\n            \n            if self.success_CR:\n                self.CR = np.clip(np.random.choice(self.success_CR), 0.1, 1.0)\n            else:\n                self.CR = np.random.uniform(0.1, 0.9)\n\n            # Mutation\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.pop[idxs]\n            x_mutated = x_r1 + self.F * (x_r2 - x_r3)\n            x_mutated = np.clip(x_mutated, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            x_trial = self.pop[i].copy()\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.CR or j == j_rand:\n                    x_trial[j] = x_mutated[j]\n\n            # Selection\n            f_trial = func(x_trial)\n            self.eval_count += 1\n\n            if f_trial < self.fitness[i]:\n                # Success history update\n                self.success_F.append(self.F)\n                self.success_CR.append(self.CR)\n                self.success_fitness_diff.append(self.fitness[i] - f_trial)\n\n                # Keep success history bounded\n                if len(self.success_F) > self.archive_size:\n                    self.success_F.pop(0)\n                    self.success_CR.pop(0)\n                    self.success_fitness_diff.pop(0)\n\n                self.pop[i] = x_trial\n                self.fitness[i] = f_trial\n                \n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = x_trial\n\n    def restart_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        if self.fitness[best_idx] < self.f_opt:\n            self.f_opt = self.fitness[best_idx]\n            self.x_opt = self.pop[best_idx]\n        # Clear success history after restart\n        self.success_F = []\n        self.success_CR = []\n        self.success_fitness_diff = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.initialize_population(func)\n        self.success_F = []\n        self.success_CR = []\n        self.success_fitness_diff = []\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n            if np.random.rand() < self.restart_prob:\n                self.restart_population(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDE scored 0.401 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["7c561926-53cc-4c30-84dd-a6b1f890809f"], "operator": null, "metadata": {"aucs": [0.14579458507493126, 0.23163590204324425, 0.3603937015859705, 0.5765509253399591, 0.4294420523417912, 0.49114849834594576, 0.28425911311415497, 0.33450229561048517, 0.42374478218573164, 0.3009131040834443, 0.28595033631849676, 0.991611651161828, 0.2756941276737914, 0.28908044260552856, 0.7026171641129625, 0.3968554014949308, 0.3486115666015497, 0.47828773076559605, 0.18368194528932724, 0.4971619491405269]}}
{"id": "13fd1a2d-99ca-4f5b-b917-9f07dfbbe310", "fitness": -Infinity, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with orthogonal learning to enhance exploration and exploitation, adjusting F and CR based on success history and incorporating a dynamic archive.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=100, f_initial=0.5, cr_initial=0.9, orthogonal_components = 5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = f_initial  # Initial mutation factor\n        self.CR = cr_initial # Initial Crossover rate\n        self.archive = []\n        self.archive_fitness = []\n        self.success_F = []\n        self.success_CR = []\n        self.success_count = 0\n        self.memory_size = 10\n        self.orthogonal_components = orthogonal_components\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n\n        for gen in range(self.budget // self.pop_size):\n            new_pop = np.copy(self.pop)\n            new_fitness = np.copy(self.fitness)\n\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = self.pop[idxs]\n\n                # Use archive with a probability\n                if np.random.rand() < 0.1 and len(self.archive) > 0:\n                    xa = self.archive[np.random.randint(len(self.archive))]\n                    v = x1 + self.F * (x2 - xa)\n                else:\n                    v = x1 + self.F * (x2 - x3)\n\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = np.copy(self.pop[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        u[j] = v[j]\n\n                # Orthogonal learning\n                if np.random.rand() < 0.1:  # Apply orthogonal learning occasionally\n                    orthogonal_basis = np.random.randn(self.dim, self.orthogonal_components)\n                    orthogonal_basis, _ = np.linalg.qr(orthogonal_basis)  # Orthogonalize\n                    coeffs = np.random.uniform(-0.1, 0.1, size=self.orthogonal_components) # Adjust coefficients\n                    orthogonal_vector = np.dot(orthogonal_basis, coeffs)\n                    u = np.clip(u + orthogonal_vector, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f = func(u)\n                self.budget -= 1\n                \n                if f < self.fitness[i]:\n                    self.success_count += 1\n                    self.success_F.append(self.F)\n                    self.success_CR.append(self.CR)\n\n                    if self.fitness[i] < self.f_opt:\n                        self.f_opt = self.fitness[i]\n                        self.x_opt = self.pop[i]\n                    \n                    # Update Archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.pop[i])\n                    else:\n                        idx_to_replace = np.random.randint(self.archive_size)\n                        self.archive[idx_to_replace] = self.pop[i]\n                    \n                    new_pop[i] = u\n                    new_fitness[i] = f\n                    \n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n                else:\n                    new_pop[i] = self.pop[i]\n                    new_fitness[i] = self.fitness[i]\n            \n            #Adaptive F and CR\n            if self.success_count > self.memory_size:\n                self.F = np.mean(self.success_F[-self.memory_size:])\n                self.CR = np.mean(self.success_CR[-self.memory_size:])\n                self.success_F = []\n                self.success_CR = []\n                self.success_count = 0\n            \n            self.pop = new_pop\n            self.fitness = new_fitness\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 56, in __call__\n  File \"<__array_function__ internals>\", line 200, in dot\nValueError: shapes (2,2) and (5,) not aligned: 2 (dim 1) != 5 (dim 0)\n.", "error": "", "parent_ids": ["2cc39ae0-4204-4f8b-aa4f-1549d2eef521"], "operator": null, "metadata": {}}
{"id": "edee76fb-bf78-4d9b-aa2c-aa96fa086f6c", "fitness": 0.0, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with orthogonal learning to generate trial vectors and enhanced parameter adaptation based on success history.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_init=0.5, CR_init=0.7, restart_prob=0.05, F_adapt_rate=0.1, CR_adapt_rate=0.1, ortho_group_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = np.full(pop_size, F_init)  # Mutation factor for each individual\n        self.CR = np.full(pop_size, CR_init)  # Crossover rate for each individual\n        self.restart_prob = restart_prob\n        self.F_adapt_rate = F_adapt_rate\n        self.CR_adapt_rate = CR_adapt_rate\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.ortho_group_size = ortho_group_size\n\n        # Success history for F and CR\n        self.SF = []\n        self.SCR = []\n        self.success_count = np.zeros(pop_size)\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        if self.fitness[best_idx] < self.f_opt:\n            self.f_opt = self.fitness[best_idx]\n            self.x_opt = self.pop[best_idx]\n\n    def orthogonal_learning(self, x, func):\n        # Generate orthogonal vectors around x\n        orthogonal_vectors = []\n        for _ in range(self.ortho_group_size -1):\n            v = np.random.randn(self.dim)\n            v = v - np.dot(v, x) / np.dot(x, x) * x  # Ensure orthogonality\n            v = v / np.linalg.norm(v) # Normalize\n            orthogonal_vectors.append(x + 0.1 * v) # Scale by small value and add\n        orthogonal_vectors = np.clip(np.array(orthogonal_vectors), func.bounds.lb, func.bounds.ub)\n        \n        fitness_values = [func(xi) for xi in orthogonal_vectors]\n        self.eval_count += len(fitness_values)\n\n        best_idx = np.argmin(fitness_values)\n        return orthogonal_vectors[best_idx], fitness_values[best_idx]\n\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Mutation\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.pop[idxs]\n            x_mutated = x_r1 + self.F[i] * (x_r2 - x_r3)\n            x_mutated = np.clip(x_mutated, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            x_trial = self.pop[i].copy()\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.CR[i] or j == j_rand:\n                    x_trial[j] = x_mutated[j]\n\n            # Orthogonal Learning\n            x_trial, f_trial = self.orthogonal_learning(x_trial, func)\n\n\n            # Selection\n            #f_trial = func(x_trial) # Orthogonal learning has this evaluation\n            #self.eval_count += 1\n\n            if f_trial < self.fitness[i]:\n                # Adaptation of F and CR based on success history\n                self.SF.append(self.F[i])\n                self.SCR.append(self.CR[i])\n                self.success_count[i] += 1\n\n                self.pop[i] = x_trial\n                self.fitness[i] = f_trial\n                \n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = x_trial\n            else:\n                self.success_count[i] = 0 # Reset success count\n\n\n            # Update F and CR\n            if self.SF:\n                self.F[i] = np.clip(np.median(self.SF), 0.1, 1.0) # Use median of successful F values\n            if self.SCR:\n                self.CR[i] = np.clip(np.median(self.SCR), 0.1, 1.0) # Use median of successful CR values\n            \n            # Clear success history periodically\n            if np.random.rand() < 0.1:\n                self.SF = []\n                self.SCR = []\n\n\n                    \n    def restart_population(self, func):\n        # Restart only the worst half of the population\n        worst_half_indices = np.argsort(self.fitness)[self.pop_size // 2:]\n        \n        new_pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(len(worst_half_indices), self.dim))\n        new_fitness = np.array([func(x) for x in new_pop])\n        self.eval_count += len(worst_half_indices)\n\n        self.pop[worst_half_indices] = new_pop\n        self.fitness[worst_half_indices] = new_fitness\n        \n        best_idx = np.argmin(self.fitness)\n        if self.fitness[best_idx] < self.f_opt:\n            self.f_opt = self.fitness[best_idx]\n            self.x_opt = self.pop[best_idx]\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n            if np.random.rand() < self.restart_prob:\n                self.restart_population(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["553a3557-45a5-43ed-bc85-19665369b564"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "4b66226b-c26c-461a-8473-3ebe9fae0409", "fitness": -Infinity, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with orthogonal learning, self-adaptive parameters, and a combined convergence and stagnation-based restart strategy.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_mu=0.5, F_sigma=0.3, CR_mu=0.5, CR_sigma=0.1, restart_prob=0.05, convergence_threshold=1e-6, stagnation_threshold=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_mu = F_mu  # Mean mutation factor\n        self.F_sigma = F_sigma # Std dev of mutation factor\n        self.CR_mu = CR_mu  # Mean crossover rate\n        self.CR_sigma = CR_sigma # Std dev of crossover rate\n        self.restart_prob = restart_prob # Probability of restarting population\n        self.convergence_threshold = convergence_threshold\n        self.stagnation_threshold = stagnation_threshold # Number of iterations without improvement before restart\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.F_values = np.ones(pop_size) * F_mu\n        self.CR_values = np.ones(pop_size) * CR_mu\n        self.stagnation_counter = 0 # Tracks iterations without improvement\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        if self.fitness[best_idx] < self.f_opt:\n            self.f_opt = self.fitness[best_idx]\n            self.x_opt = self.pop[best_idx]\n            self.stagnation_counter = 0 # Reset counter when a new best is found\n\n    def orthogonal_learning(self, func, x_current):\n        # Generate orthogonal array (simplified version)\n        levels = 3  # Number of levels for each factor\n        factors = self.dim  # Number of factors (dimensions)\n        \n        # Generate a random orthogonal array (replace with a proper OA generator for better performance)\n        oa = np.random.randint(0, levels, size=(factors, factors))  # Example: random OA\n\n        # Create candidate solutions based on the orthogonal array\n        candidates = np.zeros((factors, self.dim))\n        for i in range(factors):\n            candidates[i] = x_current.copy()\n            for j in range(self.dim):\n                step = (func.bounds.ub - func.bounds.lb) / (levels - 1)\n                candidates[i][j] = np.clip(x_current[j] + (oa[i, j] - 1) * step/2, func.bounds.lb, func.bounds.ub)\n\n        # Evaluate candidate solutions\n        fitness_values = np.array([func(x) for x in candidates])\n        self.eval_count += factors\n\n        # Select the best solution\n        best_idx = np.argmin(fitness_values)\n        return candidates[best_idx], fitness_values[best_idx]\n\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Adaptive F and CR\n            self.F_values[i] = np.clip(np.random.normal(self.F_mu, self.F_sigma), 0.1, 1.0)\n            self.CR_values[i] = np.clip(np.random.normal(self.CR_mu, self.CR_sigma), 0.1, 1.0)\n            F = self.F_values[i]\n            CR = self.CR_values[i]\n\n\n            # Mutation\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.pop[idxs]\n            x_mutated = x_r1 + F * (x_r2 - x_r3)\n            x_mutated = np.clip(x_mutated, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            x_trial = self.pop[i].copy()\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < CR or j == j_rand:\n                    x_trial[j] = x_mutated[j]\n\n            # Selection\n            f_trial = func(x_trial)\n            self.eval_count += 1\n\n            if f_trial < self.fitness[i]:\n                \n                # Update F and CR based on success (optional)\n                # If trial vector is better, adjust F and CR towards the values used\n                self.F_mu = 0.9 * self.F_mu + 0.1 * F\n                self.CR_mu = 0.9 * self.CR_mu + 0.1 * CR\n                \n                self.pop[i] = x_trial\n                self.fitness[i] = f_trial\n                \n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = x_trial\n                    self.stagnation_counter = 0 # Reset counter when a new best is found\n            else:\n                self.stagnation_counter += 1 # Increment counter if no improvement\n\n    def restart_population(self, func):\n        # Focused restart: generate new population around the current best solution\n        radius = 0.1 * (func.bounds.ub - func.bounds.lb)  # Adjust radius as needed\n        self.pop = np.random.uniform(np.maximum(func.bounds.lb, self.x_opt - radius), np.minimum(func.bounds.ub, self.x_opt + radius), size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        if self.fitness[best_idx] < self.f_opt:\n            self.f_opt = self.fitness[best_idx]\n            self.x_opt = self.pop[best_idx]\n            self.stagnation_counter = 0  # Reset counter upon finding a new best\n\n    def check_convergence(self):\n        # Check if the population has converged (small variance in fitness)\n        return np.std(self.fitness) < self.convergence_threshold\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.stagnation_counter = 0\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n\n            # Apply orthogonal learning to the best solution\n            x_ol, f_ol = self.orthogonal_learning(func, self.x_opt)\n            if f_ol < self.f_opt:\n                self.f_opt = f_ol\n                self.x_opt = x_ol\n                self.stagnation_counter = 0 # Reset stagnation counter\n            else:\n                 self.stagnation_counter += self.dim # each orthogonal learning step evaluates dim solutions\n\n\n            # Restart if converged or stagnated, or random chance\n            if self.check_convergence() or self.stagnation_counter > self.stagnation_threshold or np.random.rand() < self.restart_prob:\n                self.restart_population(func)\n                self.stagnation_counter = 0  # Reset counter after restart\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "An exception occured: TypeError: only size-1 arrays can be converted to Python scalars\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 134, in __call__\n  File \"<string>\", line 49, in orthogonal_learning\nValueError: setting an array element with a sequence.\n.", "error": "", "parent_ids": ["8efcbe0d-7d04-4a71-9eea-a102c991adeb"], "operator": null, "metadata": {}}
{"id": "8cabf80b-c0dd-4ca9-abcc-0940f4d5da5f", "fitness": 0.0, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with orthogonal learning and population diversity-based parameter adaptation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_init=0.5, CR_init=0.7, restart_prob=0.05, F_adapt_rate=0.1, CR_adapt_rate=0.1, diversity_threshold=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = np.full(pop_size, F_init)  # Mutation factor for each individual\n        self.CR = np.full(pop_size, CR_init)  # Crossover rate for each individual\n        self.restart_prob = restart_prob\n        self.F_adapt_rate = F_adapt_rate\n        self.CR_adapt_rate = CR_adapt_rate\n        self.diversity_threshold = diversity_threshold\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        if self.fitness[best_idx] < self.f_opt:\n            self.f_opt = self.fitness[best_idx]\n            self.x_opt = self.pop[best_idx]\n\n    def calculate_diversity(self):\n        # Calculate the average distance of each individual from the population center\n        center = np.mean(self.pop, axis=0)\n        distances = np.linalg.norm(self.pop - center, axis=1)\n        diversity = np.mean(distances)\n        return diversity\n\n    def orthogonal_learning(self, func, x_mutated, i):\n        # Generate an orthogonal array\n        orthogonal_array = np.random.choice([0, 1], size=(self.dim, self.dim))\n        \n        # Create trial vectors based on the orthogonal array\n        trial_vectors = np.zeros((self.dim, self.dim))\n        for j in range(self.dim):\n            trial_vector = self.pop[i].copy()\n            for k in range(self.dim):\n                if orthogonal_array[j, k] == 1:\n                    trial_vector[k] = x_mutated[k]\n            trial_vectors[j] = trial_vector\n\n        # Evaluate the trial vectors\n        fitness_values = [func(x) for x in trial_vectors]\n        self.eval_count += self.dim\n\n        # Select the best trial vector\n        best_idx = np.argmin(fitness_values)\n        return trial_vectors[best_idx], fitness_values[best_idx]\n\n    def evolve(self, func):\n        diversity = self.calculate_diversity()\n        \n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Mutation\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.pop[idxs]\n            x_mutated = x_r1 + self.F[i] * (x_r2 - x_r3)\n            x_mutated = np.clip(x_mutated, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            x_trial = self.pop[i].copy()\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.CR[i] or j == j_rand:\n                    x_trial[j] = x_mutated[j]\n\n            # Orthogonal Learning\n            x_ol, f_ol = self.orthogonal_learning(func, x_mutated, i)\n\n            # Selection\n            f_trial = func(x_trial)\n            self.eval_count += 1\n\n            if f_ol < self.fitness[i] and f_ol <= f_trial:\n                # Adaptation of F and CR\n                self.F[i] = np.clip(self.F[i] * (1 + self.F_adapt_rate * np.random.normal(0, 1)), 0.1, 1.0)\n                self.CR[i] = np.clip(self.CR[i] * (1 + self.CR_adapt_rate * np.random.normal(0, 1)), 0.1, 1.0)\n\n                self.pop[i] = x_ol\n                self.fitness[i] = f_ol\n                \n                if f_ol < self.f_opt:\n                    self.f_opt = f_ol\n                    self.x_opt = x_ol\n            elif f_trial < self.fitness[i]:\n                # Adaptation of F and CR\n                self.F[i] = np.clip(self.F[i] * (1 + self.F_adapt_rate * np.random.normal(0, 1)), 0.1, 1.0)\n                self.CR[i] = np.clip(self.CR[i] * (1 + self.CR_adapt_rate * np.random.normal(0, 1)), 0.1, 1.0)\n\n                self.pop[i] = x_trial\n                self.fitness[i] = f_trial\n                \n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = x_trial\n            else:\n                # Adaptation of F and CR (opposite direction if no improvement)\n                self.F[i] = np.clip(self.F[i] * (1 - self.F_adapt_rate * np.random.normal(0, 1)), 0.1, 1.0)\n                self.CR[i] = np.clip(self.CR[i] * (1 - self.CR_adapt_rate * np.random.normal(0, 1)), 0.1, 1.0)\n\n            # Adjust F and CR based on population diversity\n            if diversity < self.diversity_threshold:\n                self.F[i] = np.clip(self.F[i] * (1 + 0.5 * np.random.normal(0, 1)), 0.1, 1.0)  # Increase F to explore more\n                self.CR[i] = np.clip(self.CR[i] * (1 - 0.5 * np.random.normal(0, 1)), 0.1, 1.0)  # Decrease CR to avoid premature convergence\n\n\n    def restart_population(self, func):\n        # Restart only the worst half of the population\n        worst_half_indices = np.argsort(self.fitness)[self.pop_size // 2:]\n        \n        new_pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(len(worst_half_indices), self.dim))\n        new_fitness = np.array([func(x) for x in new_pop])\n        self.eval_count += len(worst_half_indices)\n\n        self.pop[worst_half_indices] = new_pop\n        self.fitness[worst_half_indices] = new_fitness\n        \n        best_idx = np.argmin(self.fitness)\n        if self.fitness[best_idx] < self.f_opt:\n            self.f_opt = self.fitness[best_idx]\n            self.x_opt = self.pop[best_idx]\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n            if np.random.rand() < self.restart_prob:\n                self.restart_population(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["553a3557-45a5-43ed-bc85-19665369b564"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "15b97af2-8155-4cb8-87fa-7123f9d5d1bb", "fitness": 0.0, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with orthogonal learning and covariance matrix adaptation for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_mu=0.5, F_sigma=0.3, CR_mu=0.5, CR_sigma=0.1, restart_prob=0.05, convergence_threshold=1e-6, orthogonal_learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_mu = F_mu  # Mean mutation factor\n        self.F_sigma = F_sigma # Std dev of mutation factor\n        self.CR_mu = CR_mu  # Mean crossover rate\n        self.CR_sigma = CR_sigma # Std dev of crossover rate\n        self.restart_prob = restart_prob # Probability of restarting population\n        self.convergence_threshold = convergence_threshold\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.F_values = np.ones(pop_size) * F_mu\n        self.CR_values = np.ones(pop_size) * CR_mu\n        self.archive = []  # Archive for storing discarded solutions\n        self.archive_size = pop_size  # Size of the archive\n        self.C = np.eye(dim)  # Covariance matrix for CMA-like adaptation\n        self.learning_rate_C = 0.1 # Learning rate for covariance matrix\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        if self.fitness[best_idx] < self.f_opt:\n            self.f_opt = self.fitness[best_idx]\n            self.x_opt = self.pop[best_idx]\n\n    def orthogonal_learning(self, x, func):\n        # Generate an orthogonal direction\n        d = np.random.randn(self.dim)\n        d /= np.linalg.norm(d)\n\n        # Define two points along the orthogonal direction\n        delta = self.orthogonal_learning_rate * (func.bounds.ub - func.bounds.lb)\n        x1 = np.clip(x + delta * d, func.bounds.lb, func.bounds.ub)\n        x2 = np.clip(x - delta * d, func.bounds.lb, func.bounds.ub)\n\n        # Evaluate the points\n        f1 = func(x1)\n        f2 = func(x2)\n        self.eval_count += 2\n\n        # Choose the better point\n        if f1 < f2:\n            return x1, f1\n        else:\n            return x2, f2\n\n    def update_covariance_matrix(self):\n        # CMA-like update of the covariance matrix\n        diffs = self.pop - self.x_opt  # Differences from the best solution\n        self.C = (1 - self.learning_rate_C) * self.C + self.learning_rate_C * np.mean([np.outer(diff, diff) for diff in diffs], axis=0)\n        # Ensure C is positive definite (add small diagonal if needed)\n        min_eig = np.min(np.linalg.eigvalsh(self.C))\n        if min_eig < 0:\n            self.C += (abs(min_eig) + 1e-10) * np.eye(self.dim)\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Adaptive F and CR\n            self.F_values[i] = np.clip(np.random.normal(self.F_mu, self.F_sigma), 0.1, 1.0)\n            self.CR_values[i] = np.clip(np.random.normal(self.CR_mu, self.CR_sigma), 0.1, 1.0)\n            F = self.F_values[i]\n            CR = self.CR_values[i]\n\n\n            # Mutation\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.pop[idxs]\n            x_mutated = x_r1 + F * (x_r2 - x_r3)\n            \n            # Incorporate covariance matrix information (CMA-like sampling)\n            z = np.random.multivariate_normal(np.zeros(self.dim), self.C)\n            x_mutated = x_mutated + 0.1 * z # Scale the CMA-like exploration\n\n            x_mutated = np.clip(x_mutated, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            x_trial = self.pop[i].copy()\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < CR or j == j_rand:\n                    x_trial[j] = x_mutated[j]\n\n            # Selection\n            f_trial = func(x_trial)\n            self.eval_count += 1\n\n            if f_trial < self.fitness[i]:\n                self.pop[i] = x_trial\n                self.fitness[i] = f_trial\n                \n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = x_trial\n            else:\n                 # Archive the discarded solution\n                if len(self.archive) < self.archive_size:\n                    self.archive.append(self.pop[i])\n                else:\n                    # Replace a random element in the archive\n                    idx_to_replace = np.random.randint(self.archive_size)\n                    self.archive[idx_to_replace] = self.pop[i]                    \n                    \n    def restart_population(self, func):\n        # Focused restart: generate new population around the current best solution\n        radius = 0.1 * (func.bounds.ub - func.bounds.lb)  # Adjust radius as needed\n        self.pop = np.random.uniform(np.maximum(func.bounds.lb, self.x_opt - radius), np.minimum(func.bounds.ub, self.x_opt + radius), size=(self.pop_size, self.dim))\n        \n        # Incorporate archived solutions into the new population\n        if len(self.archive) > 0:\n            num_archive_to_include = min(len(self.archive), self.pop_size // 2)  # Adjust as needed\n            archive_indices = np.random.choice(len(self.archive), num_archive_to_include, replace=False)\n            self.pop[:num_archive_to_include] = [self.archive[i] for i in archive_indices] # Replace the first elements in the pop with archive solutions\n        \n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        if self.fitness[best_idx] < self.f_opt:\n            self.f_opt = self.fitness[best_idx]\n            self.x_opt = self.pop[best_idx]\n\n    def check_convergence(self):\n        # Check if the population has converged (small variance in fitness)\n        return np.std(self.fitness) < self.convergence_threshold\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n            \n            # Orthogonal learning on the best solution\n            x_ol, f_ol = self.orthogonal_learning(self.x_opt, func)\n            if f_ol < self.f_opt:\n                self.f_opt = f_ol\n                self.x_opt = x_ol\n                \n            self.update_covariance_matrix()\n            \n            if self.check_convergence() or np.random.rand() < self.restart_prob:\n                self.restart_population(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["8efcbe0d-7d04-4a71-9eea-a102c991adeb"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "a84c3eb6-cfc0-480b-be1c-cb25ff192c4a", "fitness": 0.0, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with a diversity-guided mutation strategy and a learning rate for F and CR based on individual success.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_mu=0.5, F_sigma=0.3, CR_mu=0.5, CR_sigma=0.1, restart_prob=0.05, convergence_threshold=1e-6, F_lr=0.1, CR_lr=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_mu = F_mu  # Mean mutation factor\n        self.F_sigma = F_sigma # Std dev of mutation factor\n        self.CR_mu = CR_mu  # Mean crossover rate\n        self.CR_sigma = CR_sigma # Std dev of crossover rate\n        self.restart_prob = restart_prob # Probability of restarting population\n        self.convergence_threshold = convergence_threshold\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.F_values = np.ones(pop_size) * F_mu\n        self.CR_values = np.ones(pop_size) * CR_mu\n        self.F_lr = F_lr # Learning rate for F adaptation\n        self.CR_lr = CR_lr # Learning rate for CR adaptation\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        if self.fitness[best_idx] < self.f_opt:\n            self.f_opt = self.fitness[best_idx]\n            self.x_opt = self.pop[best_idx]\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Adaptive F and CR\n            F = np.clip(np.random.normal(self.F_mu, self.F_sigma), 0.1, 1.0)\n            CR = np.clip(np.random.normal(self.CR_mu, self.CR_sigma), 0.1, 1.0)\n\n            # Diversity-guided Mutation\n            if np.random.rand() < 0.5:  # 50% chance to use current-to-best\n                idxs = np.random.choice(self.pop_size, 2, replace=False)\n                x_r1, x_r2 = self.pop[idxs]\n                x_mutated = self.pop[i] + F * (self.x_opt - self.pop[i]) + F * (x_r1 - x_r2)\n            else:  # Otherwise, use standard DE mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.pop[idxs]\n                x_mutated = x_r1 + F * (x_r2 - x_r3)\n\n            x_mutated = np.clip(x_mutated, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            x_trial = self.pop[i].copy()\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < CR or j == j_rand:\n                    x_trial[j] = x_mutated[j]\n\n            # Selection\n            f_trial = func(x_trial)\n            self.eval_count += 1\n\n            if f_trial < self.fitness[i]:\n                # Update F and CR based on success using learning rates\n                self.F_mu = (1 - self.F_lr) * self.F_mu + self.F_lr * F\n                self.CR_mu = (1 - self.CR_lr) * self.CR_mu + self.CR_lr * CR\n                \n                self.pop[i] = x_trial\n                self.fitness[i] = f_trial\n                \n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = x_trial\n                    \n    def restart_population(self, func):\n        # Focused restart: generate new population around the current best solution\n        radius = 0.1 * (func.bounds.ub - func.bounds.lb)  # Adjust radius as needed\n        self.pop = np.random.uniform(np.maximum(func.bounds.lb, self.x_opt - radius), np.minimum(func.bounds.ub, self.x_opt + radius), size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        if self.fitness[best_idx] < self.f_opt:\n            self.f_opt = self.fitness[best_idx]\n            self.x_opt = self.pop[best_idx]\n\n    def check_convergence(self):\n        # Check if the population has converged (small variance in fitness)\n        return np.std(self.fitness) < self.convergence_threshold\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n            \n            if self.check_convergence() or np.random.rand() < self.restart_prob:\n                self.restart_population(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["8efcbe0d-7d04-4a71-9eea-a102c991adeb"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "de67a27a-40b3-478f-90e4-5d1162f9c0f3", "fitness": 0.3460313660478442, "name": "AdaptiveCMAES", "description": "CMA-ES with adaptive population size, covariance matrix clipping, and a more aggressive step size adaptation.", "code": "import numpy as np\n\nclass AdaptiveCMAES:\n    def __init__(self, budget=10000, dim=10, sigma0 = 0.5):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n\n        self.popsize = 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.min_popsize = 4\n        self.max_popsize = 100\n        self.adapt_popsize_freq = 10\n\n    def __call__(self, func):\n        # Initialize variables\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)  # Covariance matrix\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n        # Parameters (using common defaults)\n        c_sigma = (self.mu + 2) / (self.dim + self.mu + 5)\n        c_c = (4 + self.mu / self.dim) / (self.dim + 4 + 2 * self.mu / self.dim)\n        c_1 = 2 / ((self.dim + 1.3)**2 + self.mu)\n        c_mu = min(1 - c_1, 2 * (self.mu - 1 + 1 / self.mu) / ((self.dim + 2)**2 + self.mu))\n        d_sigma = 1 + 2 * max(0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1) + c_sigma\n\n        # Eigen decomposition of C (expensive, do it rarely)\n        C_evals, C_evecs = np.linalg.eigh(C)\n        C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n        \n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n\n        success_history = []\n        \n        while eval_count < self.budget:\n            # Sample population\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            x = mean + sigma * (C_sqrt @ z.T).T\n            \n            # Mirror sampling to improve exploration\n            x_mirrored = mean - sigma * (C_sqrt @ z.T).T\n            x = np.concatenate([x, x_mirrored], axis=0)\n            z = np.concatenate([z, -z], axis=0)\n            \n            # Evaluate the new points, but respect the budget\n            f = np.array([func(xi) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n            eval_count += len(x) \n\n            # Sort by fitness\n            idx = np.argsort(f)\n            x = x[idx]\n            z = z[idx]\n            f = f[idx]\n\n            # Update optimal solution\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[0]\n                \n            # Selection and recombination\n            x_mu = x[:self.mu]\n            z_mu = z[:self.mu]\n\n            mean_new = np.mean(x_mu, axis=0)\n            z_w = np.mean(z_mu, axis=0)\n            \n            # Covariance matrix adaptation\n            ps = (1 - c_sigma) * ps + np.sqrt(c_sigma * (2 - c_sigma) * self.mu) * (C_sqrt @ z_w)\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - c_sigma)**(2 * eval_count / self.popsize)) < chiN * (1.4 + 2 / (self.dim + 1))\n            \n            pc = (1 - c_c) * pc + hsig * np.sqrt(c_c * (2 - c_c) * self.mu) * z_w\n\n            C = (1 - c_1 - c_mu) * C + c_1 * (pc[:, None] @ pc[None, :])\n            for i in range(self.mu):\n                C += c_mu * (z_mu[i, :, None] @ z_mu[i, None, :])\n                \n            # Update step size (more aggressive)\n            sigma *= np.exp((c_sigma / d_sigma) * (np.linalg.norm(ps) / chiN - 1) + 0.1 * (f.mean() - f[0]))  # added fitness difference term\n            sigma = max(sigma, 1e-10) # prevent sigma from becoming too small\n\n            # Update mean\n            mean = mean_new\n\n            # Eigen decomposition of C\n            if eval_count % (self.popsize * 5) == 0:  # Re-compute after every 5 generations\n                C = np.triu(C) + np.triu(C, 1).T  # Enforce symmetry\n                \n                # Clip covariance matrix eigenvalues to avoid ill-conditioning\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_evals = np.maximum(C_evals, 1e-10)\n                C_evals = np.minimum(C_evals, 1e10) # cap eigenvalues\n                C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n\n            if np.any(np.isnan(mean)) or np.any(np.isnan(C)):\n                print(\"NaN detected, resetting...\")\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                sigma = self.sigma0\n\n            # Adapt population size\n            if eval_count % (self.popsize * self.adapt_popsize_freq) == 0:\n                success_rate = (f_opt < np.mean(f)) if len(f) > 0 else 0.5\n                success_history.append(success_rate)\n\n                if len(success_history) > 5:\n                    avg_success = np.mean(success_history[-5:])\n                    if avg_success > 0.6:\n                        self.popsize = min(self.popsize + 1, self.max_popsize)\n                        self.mu = self.popsize // 2\n                        print(f\"Increasing popsize to {self.popsize}\")\n                    elif avg_success < 0.4:\n                        self.popsize = max(self.popsize - 1, self.min_popsize)\n                        self.mu = self.popsize // 2\n                        print(f\"Decreasing popsize to {self.popsize}\")\n        \n        return f_opt, x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveCMAES scored 0.346 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["52306c16-dbc7-43b7-ac05-811fd7674d8a"], "operator": null, "metadata": {"aucs": [0.11023718868263499, 0.18175881901854618, 0.8448865985218929, 0.1828587039323858, 0.21382262682000697, 0.2298521634337386, 0.3524016552275343, 0.23526683944041937, 0.21649636436438313, 0.15698928631495324, 0.20491414480330483, 0.9962007850304753, 0.24798524506899544, 0.19960118472701271, 0.591247457111736, 0.29444288514448425, 0.25940109693241364, 0.7651828587000162, 0.17114819767071143, 0.4659332200112384]}}
{"id": "12a79555-b49e-4199-b9f8-dce5cdded369", "fitness": 0.6192158730876713, "name": "CMAES", "description": "CMA-ES with active covariance matrix adaptation and step-size control based on the population fitness ranking to enhance exploration and convergence.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, sigma0 = 0.5):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n\n        self.popsize = 4 + int(np.floor(3 * np.log(self.dim)))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n\n    def __call__(self, func):\n        # Initialize variables\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)  # Covariance matrix\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n        # Parameters (using common defaults)\n        c_sigma = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        c_c = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        c_1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        c_mu = min(1 - c_1, 2 * (self.mueff - 1 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        d_sigma = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + c_sigma\n        c_1a = c_1\n        c_mua = c_mu\n\n        # Eigen decomposition of C (expensive, do it rarely)\n        C_evals, C_evecs = np.linalg.eigh(C)\n        C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n        C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n        \n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n\n        while eval_count < self.budget:\n            # Sample population\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            x = mean + sigma * (C_sqrt @ z.T).T\n            \n            # Evaluate the new points\n            f = np.array([func(xi) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n            eval_count += len(x) \n\n            # Sort by fitness\n            idx = np.argsort(f)\n            x = x[idx]\n            z = z[idx]\n            f = f[idx]\n\n            # Update optimal solution\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[0]\n                \n            # Selection and recombination\n            x_mu = x[:self.mu]\n            z_mu = z[:self.mu]\n\n            mean_new = np.sum(x_mu * self.weights[:,None], axis=0)\n            z_w = np.sum(z_mu * self.weights[:,None], axis=0)\n            \n            # Covariance matrix adaptation\n            ps = (1 - c_sigma) * ps + np.sqrt(c_sigma * (2 - c_sigma) * self.mueff) * (C_invsqrt @ (mean_new - mean) / sigma)\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - c_sigma)**(2 * eval_count / self.popsize)) < chiN * (1.4 + 2 / (self.dim + 1))\n            \n            pc = (1 - c_c) * pc + hsig * np.sqrt(c_c * (2 - c_c) * self.mueff) * (mean_new - mean) / sigma\n\n            C = (1 - c_1 - c_mu) * C + c_1 * (pc[:, None] @ pc[None, :])\n            for i in range(self.mu):\n                C += c_mu * self.weights[i] * (z_mu[i, :, None] @ z_mu[i, None, :])\n                \n            # Active CMA\n            if c_1a > 0 and c_mua > 0:\n                negidx = np.where(self.weights < 0)[0]\n                znorm = np.zeros((len(negidx), self.dim))\n                for i, idx in enumerate(negidx):\n                    znorm[i] = z_mu[idx] * np.sqrt(-self.weights[idx])\n                C += c_1a * (1/np.linalg.norm(ps)**2 + c_mua) * np.sum(znorm.T @ znorm, axis=1)\n\n            # Update step size\n            sigma *= np.exp((c_sigma / d_sigma) * (np.linalg.norm(ps) / chiN - 1))\n                \n            # Update mean\n            mean = mean_new\n\n            # Eigen decomposition of C\n            if eval_count % (self.popsize * 5) == 0:  # Re-compute after every 5 generations\n                C = np.triu(C) + np.triu(C, 1).T  # Enforce symmetry\n                try:\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_evals = np.maximum(C_evals, 1e-10) # Avoid zero or negative eigenvalues\n                    C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                except np.linalg.LinAlgError:\n                    print(\"LinAlgError encountered, resetting C\")\n                    C = np.eye(self.dim)\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                \n            if np.any(np.isnan(mean)) or np.any(np.isnan(C)):\n                print(\"NaN detected, resetting...\")\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                sigma = self.sigma0\n        \n        return f_opt, x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm CMAES scored 0.619 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["52306c16-dbc7-43b7-ac05-811fd7674d8a"], "operator": null, "metadata": {"aucs": [0.25639453749918484, 0.1519037227307274, 0.9332773121760718, 0.22238583384354327, 0.9381539944854541, 0.9468784400160795, 0.31350797212542536, 0.9326969774432623, 0.9495077875103899, 0.37385242819475584, 0.9623731371440272, 0.9778453197052424, 0.2581139251051746, 0.4170608727885279, 0.6361842944969736, 0.9313837044507215, 0.4548970605156446, 0.9531831593259452, 0.279154995277607, 0.4955619869186697]}}
{"id": "2f2eeea5-a6ae-485e-a46d-2f15f54f2eee", "fitness": 0.20903734898505322, "name": "AdaptiveCMAES", "description": "Improved CMA-ES with adaptive population size, learning rate adaptation based on function evaluations, and orthogonal sampling.", "code": "import numpy as np\n\nclass AdaptiveCMAES:\n    def __init__(self, budget=10000, dim=10, sigma0 = 0.5):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.best_eval_count = 0\n        self.last_improvement = 0\n\n        # Adaptive population size\n        self.popsize = 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.last_restart = 0\n        self.restarts = 0\n\n    def __call__(self, func):\n        # Initialize variables\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)  # Covariance matrix\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n        # Parameters (using common defaults)\n        c_sigma = (self.mu + 2) / (self.dim + self.mu + 5)\n        c_c = (4 + self.mu / self.dim) / (self.dim + 4 + 2 * self.mu / self.dim)\n        c_1 = 2 / ((self.dim + 1.3)**2 + self.mu)\n        c_mu = min(1 - c_1, 2 * (self.mu - 1 + 1 / self.mu) / ((self.dim + 2)**2 + self.mu))\n        d_sigma = 1 + 2 * max(0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1) + c_sigma\n\n        # Learning rate adaptation\n        c_1_adaptive = c_1\n        c_mu_adaptive = c_mu\n\n        # Eigen decomposition of C (expensive, do it rarely)\n        C_evals, C_evecs = np.linalg.eigh(C)\n        C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n        \n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n\n        while self.eval_count < self.budget:\n            # Sample population\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n\n            # Orthogonal sampling to improve diversity\n            Q, _ = np.linalg.qr(z.T)\n            z = Q.T\n\n            x = mean + sigma * (C_sqrt @ z.T).T\n            \n            # Mirror sampling to improve exploration\n            x_mirrored = mean - sigma * (C_sqrt @ z.T).T\n            x = np.concatenate([x, x_mirrored], axis=0)\n            z = np.concatenate([z, -z], axis=0)\n            \n            # Evaluate the new points, but respect the budget\n            f = np.array([func(xi) if self.eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n            self.eval_count += len(x) \n\n            # Sort by fitness\n            idx = np.argsort(f)\n            x = x[idx]\n            z = z[idx]\n            f = f[idx]\n\n            # Update optimal solution\n            if f[0] < self.f_opt:\n                self.f_opt = f[0]\n                self.x_opt = x[0]\n                self.best_eval_count = self.eval_count\n                self.last_improvement = self.eval_count\n                \n            # Selection and recombination\n            x_mu = x[:self.mu]\n            z_mu = z[:self.mu]\n\n            mean_new = np.mean(x_mu, axis=0)\n            z_w = np.mean(z_mu, axis=0)\n            \n            # Covariance matrix adaptation\n            ps = (1 - c_sigma) * ps + np.sqrt(c_sigma * (2 - c_sigma) * self.mu) * (C_sqrt @ z_w)\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - c_sigma)**(2 * self.eval_count / self.popsize)) < chiN * (1.4 + 2 / (self.dim + 1))\n            \n            pc = (1 - c_c) * pc + hsig * np.sqrt(c_c * (2 - c_c) * self.mu) * z_w\n\n            C = (1 - c_1_adaptive - c_mu_adaptive) * C + c_1_adaptive * (pc[:, None] @ pc[None, :])\n            for i in range(self.mu):\n                C += c_mu_adaptive * (z_mu[i, :, None] @ z_mu[i, None, :])\n                \n            # Update step size\n            sigma *= np.exp((c_sigma / d_sigma) * (np.linalg.norm(ps) / chiN - 1))\n\n            # Adaptive learning rate update\n            c_1_adaptive = c_1 * np.exp(0.1 * (self.best_eval_count - self.eval_count) / self.budget)\n            c_mu_adaptive = c_mu * np.exp(0.1 * (self.best_eval_count - self.eval_count) / self.budget)\n\n            # Update mean\n            mean = mean_new\n\n            # Eigen decomposition of C\n            if self.eval_count % (self.popsize * 5) == 0:  # Re-compute after every 5 generations\n                C = np.triu(C) + np.triu(C, 1).T  # Enforce symmetry\n                try:\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_evals = np.maximum(C_evals, 1e-10) # Avoid zero or negative eigenvalues\n                    C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                except np.linalg.LinAlgError:\n                    print(\"LinAlgError encountered, resetting C\")\n                    C = np.eye(self.dim)\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                \n            if np.any(np.isnan(mean)) or np.any(np.isnan(C)):\n                print(\"NaN detected, resetting...\")\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                sigma = self.sigma0\n\n            # Restart mechanism\n            if self.eval_count - self.last_improvement > self.budget / 5 and self.eval_count - self.last_restart > self.budget / 4:\n                self.restarts += 1\n                self.last_restart = self.eval_count\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                sigma = self.sigma0\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                c_1_adaptive = c_1\n                c_mu_adaptive = c_mu\n                print(f\"Restarting at iteration {self.eval_count}, restart number: {self.restarts}\")\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveCMAES scored 0.209 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["52306c16-dbc7-43b7-ac05-811fd7674d8a"], "operator": null, "metadata": {"aucs": [0.04901860289606541, 0.19695659262566856, 0.17560430104899083, 0.10897357590350898, 0.0789645477310763, 0.09682291109893182, 0.1704582991627398, 0.12756817224944894, 0.10376357594446495, 0.08534974604388224, 0.13654725005027124, 0.4770825382793362, 0.23136132555865707, 0.1717187742453472, 0.1549791639660294, 0.304902621727957, 0.19275071473142225, 0.9431264420040221, 0.21897980858474675, 0.15581801584849786]}}
{"id": "2c64e2e0-cc6b-4d1e-b44d-265eda06efcb", "fitness": 0.21875493307308264, "name": "AdaptiveDE_CMA", "description": "Adaptive Differential Evolution with covariance matrix adaptation for mutation and orthogonal crossover.", "code": "import numpy as np\n\nclass AdaptiveDE_CMA:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_init=0.5, CR_init=0.7, restart_prob=0.05, F_adapt_rate=0.1, CR_adapt_rate=0.1, cma_learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = np.full(pop_size, F_init)  # Mutation factor for each individual\n        self.CR = np.full(pop_size, CR_init)  # Crossover rate for each individual\n        self.restart_prob = restart_prob\n        self.F_adapt_rate = F_adapt_rate\n        self.CR_adapt_rate = CR_adapt_rate\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.cma_learning_rate = cma_learning_rate\n        self.C = np.eye(dim)  # Covariance matrix for CMA\n        self.mean = None # Mean of the population\n\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        if self.fitness[best_idx] < self.f_opt:\n            self.f_opt = self.fitness[best_idx]\n            self.x_opt = self.pop[best_idx]\n\n        self.mean = np.mean(self.pop, axis=0)\n\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Mutation with CMA\n            z = np.random.normal(0, 1, self.dim)\n            x_mutated = self.pop[i] + self.F[i] * np.dot(self.C, z)  #np.dot(np.linalg.cholesky(self.C), z)\n            x_mutated = np.clip(x_mutated, func.bounds.lb, func.bounds.ub)\n\n            # Orthogonal Crossover\n            x_trial = self.pop[i].copy()\n            num_changed_vars = max(1, int(self.CR[i] * self.dim))  # Ensure at least one variable changes\n            indices = np.random.choice(self.dim, num_changed_vars, replace=False)\n            x_trial[indices] = x_mutated[indices]\n\n\n            # Selection\n            f_trial = func(x_trial)\n            self.eval_count += 1\n\n            if f_trial < self.fitness[i]:\n                # Adaptation of F and CR\n                self.F[i] = np.clip(self.F[i] * (1 + self.F_adapt_rate * np.random.normal(0, 1)), 0.1, 1.0)\n                self.CR[i] = np.clip(self.CR[i] * (1 + self.CR_adapt_rate * np.random.normal(0, 1)), 0.1, 1.0)\n\n                # CMA Update\n                d = x_trial - self.pop[i] #self.mean\n                self.C = (1 - self.cma_learning_rate) * self.C + self.cma_learning_rate * np.outer(d, d)\n\n                self.pop[i] = x_trial\n                self.fitness[i] = f_trial\n                \n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = x_trial\n            else:\n                # Adaptation of F and CR (opposite direction if no improvement)\n                self.F[i] = np.clip(self.F[i] * (1 - self.F_adapt_rate * np.random.normal(0, 1)), 0.1, 1.0)\n                self.CR[i] = np.clip(self.CR[i] * (1 - self.CR_adapt_rate * np.random.normal(0, 1)), 0.1, 1.0)\n\n        self.mean = np.mean(self.pop, axis=0)\n                    \n    def restart_population(self, func):\n        # Restart only the worst half of the population\n        worst_half_indices = np.argsort(self.fitness)[self.pop_size // 2:]\n        \n        new_pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(len(worst_half_indices), self.dim))\n        new_fitness = np.array([func(x) for x in new_pop])\n        self.eval_count += len(worst_half_indices)\n\n        self.pop[worst_half_indices] = new_pop\n        self.fitness[worst_half_indices] = new_fitness\n        self.mean = np.mean(self.pop, axis=0)\n        \n        best_idx = np.argmin(self.fitness)\n        if self.fitness[best_idx] < self.f_opt:\n            self.f_opt = self.fitness[best_idx]\n            self.x_opt = self.pop[best_idx]\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n            if np.random.rand() < self.restart_prob:\n                self.restart_population(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDE_CMA scored 0.219 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["553a3557-45a5-43ed-bc85-19665369b564"], "operator": null, "metadata": {"aucs": [0.081929818765347, 0.15468951773306128, 0.23717469097954624, 0.1803134012390859, 0.15483722871530914, 0.2185019735097833, 0.23954728987699847, 0.17940593668326021, 0.1913602476448113, 0.148444610741979, 0.22409693169515055, 0.9960855407532614, 0.10789121498304066, 0.16704559277560527, 0]}}
{"id": "f4f652b6-0048-43dd-8789-372480f1c072", "fitness": 0.35889371382544666, "name": "CMAES_Active", "description": "CMA-ES with active covariance matrix adaptation, coordinate-wise selection, and dynamic population size adjustment.", "code": "import numpy as np\n\nclass CMAES_Active:\n    def __init__(self, budget=10000, dim=10, sigma0 = 0.5):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n\n        self.popsize = 4 + int(np.floor(3 * np.log(self.dim)))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n\n    def __call__(self, func):\n        # Initialize variables\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)  # Covariance matrix\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n        # Parameters (using common defaults)\n        c_sigma = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        c_c = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        c_1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        c_mu = min(1 - c_1, 2 * (self.mueff - 1 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        c_mueff = c_mu\n        d_sigma = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + c_sigma\n        c_1a = c_1\n        c_mua = c_mu\n\n        # Eigen decomposition of C (expensive, do it rarely)\n        C_evals, C_evecs = np.linalg.eigh(C)\n        C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n        C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n        \n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n\n        archive = []\n        \n        while eval_count < self.budget:\n            # Dynamic population size adjustment\n            self.popsize = 4 + int(np.floor(3 * np.log(self.dim)))\n            self.mu = self.popsize // 2\n            self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n            self.weights = self.weights / np.sum(self.weights)\n            self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n            c_sigma = (self.mueff + 2) / (self.dim + self.mueff + 5)\n            c_c = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n            c_1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n            c_mu = min(1 - c_1, 2 * (self.mueff - 1 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n            c_mueff = c_mu\n            d_sigma = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + c_sigma\n\n\n            # Sample population\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            x = mean + sigma * (C_sqrt @ z.T).T\n            \n            # Mirror sampling to improve exploration\n            x_mirrored = mean - sigma * (C_sqrt @ z.T).T\n            x = np.concatenate([x, x_mirrored], axis=0)\n            z = np.concatenate([z, -z], axis=0)\n            \n            # Evaluate the new points, but respect the budget\n            f = np.array([func(xi) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n            eval_count += len(x) \n\n            # Sort by fitness\n            idx = np.argsort(f)\n            x = x[idx]\n            z = z[idx]\n            f = f[idx]\n\n            # Update optimal solution\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[0]\n\n            # Selection and recombination\n            x_mu = x[:self.mu]\n            z_mu = z[:self.mu]\n            f_mu = f[:self.mu]\n            mean_new = np.average(x_mu, axis=0, weights=self.weights)\n\n            z_w = np.average(z_mu,axis=0, weights=self.weights)\n            \n            # Covariance matrix adaptation\n            ps = (1 - c_sigma) * ps + np.sqrt(c_sigma * (2 - c_sigma) * self.mueff) * (C_invsqrt @ z_w)\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - c_sigma)**(2 * eval_count / self.popsize)) < chiN * (1.4 + 2 / (self.dim + 1))\n            \n            pc = (1 - c_c) * pc + hsig * np.sqrt(c_c * (2 - c_c) * self.mueff) * z_w\n\n            C = (1 - c_1 - c_mu + c_1a * (1-hsig) * c_c * (2-c_c)) * C + c_1 * (pc[:, None] @ pc[None, :])\n\n            for i in range(self.mu):\n                C += c_mu * self.weights[i] * (z_mu[i, :, None] @ z_mu[i, None, :])\n\n            # Active CMA\n            idx_bad = np.argsort(f[self.mu:])\n            z_bad = z[self.mu:][idx_bad[:min(self.mu, len(z)-self.mu)]] # worst z values from worse half\n            for i in range(min(self.mu, len(z)-self.mu)):\n                 C += - c_mueff * self.weights[0] / self.dim * (z_bad[i, :, None] @ z_bad[i, None, :]) # Subtract\n\n            # Update step size\n            sigma *= np.exp((c_sigma / d_sigma) * (np.linalg.norm(ps) / chiN - 1))\n                \n            # Update mean\n            mean = mean_new\n\n            # Eigen decomposition of C\n            if eval_count % (self.popsize * 5) == 0:  # Re-compute after every 5 generations\n                C = np.triu(C) + np.triu(C, 1).T  # Enforce symmetry\n                try:\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_evals = np.maximum(C_evals, 1e-10) # Avoid zero or negative eigenvalues\n                    C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                except np.linalg.LinAlgError:\n                    print(\"LinAlgError encountered, resetting C\")\n                    C = np.eye(self.dim)\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                \n            if np.any(np.isnan(mean)) or np.any(np.isnan(C)):\n                print(\"NaN detected, resetting...\")\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                sigma = self.sigma0\n        \n        return f_opt, x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm CMAES_Active scored 0.359 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["52306c16-dbc7-43b7-ac05-811fd7674d8a"], "operator": null, "metadata": {"aucs": [0.18718294969581784, 0.23524169541934548, 0.3575668787018216, 0.3618691004171476, 0.28793920705610776, 0.3272571783719943, 0.2751719605337437, 0.3029694168125495, 0.3091028950577689, 0.189581947170969, 0.39811648905127117, 0.9995301349440658, 0.2915795842473844, 0.25905044763473617, 0.702731632113792, 0.32809251458316957, 0.31469461990914127, 0.40374952445573054, 0.18419671634313917, 0.46224938398923765]}}
{"id": "2e6d1785-a297-40a4-aaa5-56041c800eed", "fitness": 0.6139842547259055, "name": "AdaptiveDE", "description": "Improved Adaptive Differential Evolution with separate F/CR adaptation for successful and unsuccessful individuals, along with a more aggressive restart strategy based on stagnation detection.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_init=0.5, CR_init=0.7, restart_prob=0.05, F_adapt_rate=0.1, CR_adapt_rate=0.1, stagnation_threshold=1000):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = np.full(pop_size, F_init)  # Mutation factor for each individual\n        self.CR = np.full(pop_size, CR_init)  # Crossover rate for each individual\n        self.restart_prob = restart_prob\n        self.F_adapt_rate = F_adapt_rate\n        self.CR_adapt_rate = CR_adapt_rate\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.stagnation_threshold = stagnation_threshold\n\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        if self.fitness[best_idx] < self.f_opt:\n            self.f_opt = self.fitness[best_idx]\n            self.x_opt = self.pop[best_idx]\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Mutation\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.pop[idxs]\n            x_mutated = x_r1 + self.F[i] * (x_r2 - x_r3)\n            x_mutated = np.clip(x_mutated, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            x_trial = self.pop[i].copy()\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.CR[i] or j == j_rand:\n                    x_trial[j] = x_mutated[j]\n\n            # Selection\n            f_trial = func(x_trial)\n            self.eval_count += 1\n\n            if f_trial < self.fitness[i]:\n                # Successful adaptation of F and CR\n                self.F[i] = np.clip(self.F[i] + self.F_adapt_rate * np.random.normal(0, 0.3), 0.1, 1.0) # Small adaptation\n                self.CR[i] = np.clip(self.CR[i] + self.CR_adapt_rate * np.random.normal(0, 0.3), 0.1, 1.0) # Small adaptation\n\n                self.pop[i] = x_trial\n                self.fitness[i] = f_trial\n                \n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = x_trial\n            else:\n                # Unsuccessful adaptation of F and CR\n                self.F[i] = np.clip(self.F[i] - self.F_adapt_rate * np.random.normal(0, 0.3), 0.1, 1.0)  # Reduce F\n                self.CR[i] = np.clip(self.CR[i] - self.CR_adapt_rate * np.random.normal(0, 0.3), 0.1, 1.0)  # Reduce CR\n\n                    \n    def restart_population(self, func):\n        # Restart all the population except the best individual\n        best_idx = np.argmin(self.fitness)\n        \n        new_pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size -1 , self.dim))\n        new_fitness = np.array([func(x) for x in new_pop])\n        self.eval_count += self.pop_size -1\n\n        # Insert the best individual from the previous population\n        temp_pop = np.vstack((self.pop[best_idx], new_pop))\n        temp_fitness = np.hstack((self.fitness[best_idx], new_fitness))\n\n        self.pop = temp_pop\n        self.fitness = temp_fitness\n        \n        best_idx = np.argmin(self.fitness)\n        if self.fitness[best_idx] < self.f_opt:\n            self.f_opt = self.fitness[best_idx]\n            self.x_opt = self.pop[best_idx]\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n\n            # Stagnation check\n            if self.f_opt < self.previous_best_fitness:\n                self.stagnation_counter = 0\n                self.previous_best_fitness = self.f_opt\n            else:\n                self.stagnation_counter += self.pop_size # Increment by population size each generation\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.restart_population(func)\n                self.stagnation_counter = 0 # Reset stagnation counter\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDE scored 0.614 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["553a3557-45a5-43ed-bc85-19665369b564"], "operator": null, "metadata": {"aucs": [0.18792073081755867, 0.4333535358233396, 0.612414204947054, 0.8403050685641514, 0.705439533485198, 0.7427815568456306, 0.5257672529994377, 0.5639551047176259, 0.7044563802671805, 0.6082914515181279, 0.7986377800488983, 0.990702722725376, 0.3082496566040924, 0.6405480345892284, 0.8908846422078552, 0.7414763225180387, 0.4664217738672507, 0.809689800194112, 0.20976768933651613, 0.4986218524414384]}}
{"id": "84431366-cc9d-431a-baa9-11fe30a6b19c", "fitness": 0.0, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with archive for past solutions and tournament selection for diversity.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_init=0.5, CR_init=0.7, restart_prob=0.05, F_adapt_rate=0.1, CR_adapt_rate=0.1, stagnation_threshold=1000, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = np.full(pop_size, F_init)\n        self.CR = np.full(pop_size, CR_init)\n        self.restart_prob = restart_prob\n        self.F_adapt_rate = F_adapt_rate\n        self.CR_adapt_rate = CR_adapt_rate\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.stagnation_threshold = stagnation_threshold\n        self.archive_size = archive_size\n        self.archive = []  # Archive to store past solutions\n\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        if self.fitness[best_idx] < self.f_opt:\n            self.f_opt = self.fitness[best_idx]\n            self.x_opt = self.pop[best_idx]\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Mutation: Tournament selection to increase diversity\n            idxs = np.random.choice(self.pop_size, 2, replace=False) # Select 2 individuals\n            if self.fitness[idxs[0]] < self.fitness[idxs[1]]:\n                x_r1 = self.pop[idxs[0]]\n            else:\n                x_r1 = self.pop[idxs[1]]\n            \n            # Choose the other two indices from population and archive\n            indices = list(range(self.pop_size))\n            indices.remove(i)\n            \n            if len(self.archive) > 0:\n              combined_pop = np.vstack((self.pop[indices], self.archive))\n              rand_idx = np.random.choice(len(combined_pop), 2, replace=False)\n\n              x_r2 = combined_pop[rand_idx[0]]\n              x_r3 = combined_pop[rand_idx[1]]\n            else:\n              idxs = np.random.choice(indices, 2, replace=False)\n              x_r2 = self.pop[idxs[0]]\n              x_r3 = self.pop[idxs[1]]\n\n            x_mutated = x_r1 + self.F[i] * (x_r2 - x_r3)\n            x_mutated = np.clip(x_mutated, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            x_trial = self.pop[i].copy()\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.CR[i] or j == j_rand:\n                    x_trial[j] = x_mutated[j]\n\n            # Selection\n            f_trial = func(x_trial)\n            self.eval_count += 1\n\n            if f_trial < self.fitness[i]:\n                # Successful adaptation of F and CR\n                self.F[i] = np.clip(self.F[i] + self.F_adapt_rate * np.random.normal(0, 0.3), 0.1, 1.0)\n                self.CR[i] = np.clip(self.CR[i] + self.CR_adapt_rate * np.random.normal(0, 0.3), 0.1, 1.0)\n\n                self.pop[i] = x_trial\n                self.fitness[i] = f_trial\n                \n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = x_trial\n\n                # Update archive\n                if len(self.archive) < self.archive_size:\n                    self.archive.append(x_trial)\n                else:\n                    # Replace a random element in the archive\n                    idx_to_replace = np.random.randint(self.archive_size)\n                    self.archive[idx_to_replace] = x_trial\n\n            else:\n                # Unsuccessful adaptation of F and CR\n                self.F[i] = np.clip(self.F[i] - self.F_adapt_rate * np.random.normal(0, 0.3), 0.1, 1.0)\n                self.CR[i] = np.clip(self.CR[i] - self.CR_adapt_rate * np.random.normal(0, 0.3), 0.1, 1.0)\n\n    def restart_population(self, func):\n        # Restart all the population except the best individual\n        best_idx = np.argmin(self.fitness)\n        \n        new_pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size -1 , self.dim))\n        new_fitness = np.array([func(x) for x in new_pop])\n        self.eval_count += self.pop_size -1\n\n        # Insert the best individual from the previous population\n        temp_pop = np.vstack((self.pop[best_idx], new_pop))\n        temp_fitness = np.hstack((self.fitness[best_idx], new_fitness))\n\n        self.pop = temp_pop\n        self.fitness = temp_fitness\n        \n        best_idx = np.argmin(self.fitness)\n        if self.fitness[best_idx] < self.f_opt:\n            self.f_opt = self.fitness[best_idx]\n            self.x_opt = self.pop[best_idx]\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.archive = []  # Reset archive\n\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n\n            # Stagnation check\n            if self.f_opt < self.previous_best_fitness:\n                self.stagnation_counter = 0\n                self.previous_best_fitness = self.f_opt\n            else:\n                self.stagnation_counter += self.pop_size\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.restart_population(func)\n                self.stagnation_counter = 0\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2e6d1785-a297-40a4-aaa5-56041c800eed"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "f91a5cfa-d1ee-45c5-a2f8-f713c6a3b03f", "fitness": 0.19651182173959558, "name": "AdaptiveRestartCMAES", "description": "CMA-ES with multiple independent restarts based on stagnation detection and adaptive population sizing to escape local optima and improve exploration.", "code": "import numpy as np\n\nclass AdaptiveRestartCMAES:\n    def __init__(self, budget=10000, dim=10, sigma0 = 0.5, restarts=5):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.restarts = restarts\n\n    def __call__(self, func):\n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n        restart_count = 0\n        stagnation_counter = 0\n        \n        while eval_count < self.budget and restart_count < self.restarts:\n            # Adaptive population size\n            popsize = 4 + int(np.floor(3 * np.log(self.dim)))\n            if stagnation_counter > 5:\n                popsize = 4 + int(np.floor(5 * np.log(self.dim))) # Increase popsize if stagnating\n            mu = popsize // 2\n            weights = np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))\n            weights = weights / np.sum(weights)\n            mueff = np.sum(weights)**2 / np.sum(weights**2)\n\n            # Initialize variables\n            mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n            sigma = self.sigma0\n            C = np.eye(self.dim)  # Covariance matrix\n            pc = np.zeros(self.dim)\n            ps = np.zeros(self.dim)\n            chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n            # Parameters (using common defaults)\n            c_sigma = (mueff + 2) / (self.dim + mueff + 5)\n            c_c = (4 + mueff / self.dim) / (self.dim + 4 + 2 * mueff / self.dim)\n            c_1 = 2 / ((self.dim + 1.3)**2 + mueff)\n            c_mu = min(1 - c_1, 2 * (mueff - 1 + 1 / mueff) / ((self.dim + 2)**2 + mueff))\n            d_sigma = 1 + 2 * max(0, np.sqrt((mueff - 1) / (self.dim + 1)) - 1) + c_sigma\n            c_1a = c_1\n            c_mua = c_mu\n\n            # Eigen decomposition of C (expensive, do it rarely)\n            try:\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n            except np.linalg.LinAlgError:\n                print(\"LinAlgError encountered during initialization, resetting C\")\n                C = np.eye(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n            \n            last_f_opt = np.Inf\n            local_eval_count = 0\n            \n            while eval_count < self.budget:\n                # Sample population\n                z = np.random.normal(0, 1, size=(popsize, self.dim))\n                x = mean + sigma * (C_sqrt @ z.T).T\n                \n                # Evaluate the new points\n                f = np.array([func(xi) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n                eval_count += len(x)\n                local_eval_count += len(x)\n\n                # Sort by fitness\n                idx = np.argsort(f)\n                x = x[idx]\n                z = z[idx]\n                f = f[idx]\n\n                # Update optimal solution\n                if f[0] < f_opt:\n                    f_opt = f[0]\n                    x_opt = x[0]\n                    stagnation_counter = 0  # Reset stagnation counter\n                else:\n                    stagnation_counter += 1\n\n                # Check for stagnation\n                if abs(f_opt - last_f_opt) < 1e-9 or stagnation_counter > 20 * popsize: # increased stagnation\n                    print(f\"Stagnation detected after {local_eval_count} evaluations, restarting CMA-ES.\")\n                    restart_count += 1\n                    break # Restart CMA-ES\n                    \n                last_f_opt = f_opt\n                    \n                # Selection and recombination\n                x_mu = x[:mu]\n                z_mu = z[:mu]\n\n                mean_new = np.sum(x_mu * weights[:,None], axis=0)\n                z_w = np.sum(z_mu * weights[:,None], axis=0)\n                \n                # Covariance matrix adaptation\n                ps = (1 - c_sigma) * ps + np.sqrt(c_sigma * (2 - c_sigma) * mueff) * (C_invsqrt @ (mean_new - mean) / sigma)\n                hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - c_sigma)**(2 * eval_count / popsize)) < chiN * (1.4 + 2 / (self.dim + 1))\n                \n                pc = (1 - c_c) * pc + hsig * np.sqrt(c_c * (2 - c_c) * mueff) * (mean_new - mean) / sigma\n\n                C = (1 - c_1 - c_mu) * C + c_1 * (pc[:, None] @ pc[None, :])\n                for i in range(mu):\n                    C += c_mu * weights[i] * (z_mu[i, :, None] @ z_mu[i, None, :])\n                    \n                # Active CMA\n                if c_1a > 0 and c_mua > 0:\n                    negidx = np.where(weights < 0)[0]\n                    znorm = np.zeros((len(negidx), self.dim))\n                    for i, idx in enumerate(negidx):\n                        znorm[i] = z_mu[idx] * np.sqrt(-weights[idx])\n                    C += c_1a * (1/np.linalg.norm(ps)**2 + c_mua) * np.sum(znorm.T @ znorm, axis=1)\n\n                # Update step size\n                sigma *= np.exp((c_sigma / d_sigma) * (np.linalg.norm(ps) / chiN - 1))\n                    \n                # Update mean\n                mean = mean_new\n\n                # Eigen decomposition of C\n                if local_eval_count % (popsize * 5) == 0:  # Re-compute after every 5 generations\n                    C = np.triu(C) + np.triu(C, 1).T  # Enforce symmetry\n                    try:\n                        C_evals, C_evecs = np.linalg.eigh(C)\n                        C_evals = np.maximum(C_evals, 1e-10) # Avoid zero or negative eigenvalues\n                        C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n                        C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                        C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                    except np.linalg.LinAlgError:\n                        print(\"LinAlgError encountered, resetting C\")\n                        C = np.eye(self.dim)\n                        C_evals, C_evecs = np.linalg.eigh(C)\n                        C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                        C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                    \n                if np.any(np.isnan(mean)) or np.any(np.isnan(C)):\n                    print(\"NaN detected, resetting...\")\n                    mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                    C = np.eye(self.dim)\n                    pc = np.zeros(self.dim)\n                    ps = np.zeros(self.dim)\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                    sigma = self.sigma0\n                    stagnation_counter = 0\n                    \n        return f_opt, x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveRestartCMAES scored 0.197 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["12a79555-b49e-4199-b9f8-dce5cdded369"], "operator": null, "metadata": {"aucs": [0.057517153665480825, 0.08904729572045511, 0.23366366225147372, 0.11095182362330902, 0.15678543544667756, 0.1471044535118604, 0.21777234072176455, 0.221444715231491, 0.18314329298974885, 0.14025621244145026, 0.1816805578016295, 0.16523582397216474, 0.19630623630015387, 0.1950576621241331, 0.23220537384320838, 0.30372343664261325, 0.22026403694154617, 0.30641390216386755, 0.1331546739504338, 0.43850834544844974]}}
{"id": "82c68998-cc30-43c3-b9ac-5e88949a01de", "fitness": -Infinity, "name": "AdaptiveDE_OL", "description": "Adaptive Differential Evolution with orthogonal learning and aging mechanism for population diversity and convergence speed.", "code": "import numpy as np\n\nclass AdaptiveDE_OL:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_init=0.5, CR_init=0.7, restart_prob=0.05, F_adapt_rate=0.1, CR_adapt_rate=0.1, stagnation_threshold=1000, aging_rate=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = np.full(pop_size, F_init)  # Mutation factor for each individual\n        self.CR = np.full(pop_size, CR_init)  # Crossover rate for each individual\n        self.restart_prob = restart_prob\n        self.F_adapt_rate = F_adapt_rate\n        self.CR_adapt_rate = CR_adapt_rate\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.stagnation_threshold = stagnation_threshold\n        self.aging_rate = aging_rate\n        self.age = np.zeros(pop_size)\n\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        if self.fitness[best_idx] < self.f_opt:\n            self.f_opt = self.fitness[best_idx]\n            self.x_opt = self.pop[best_idx]\n\n    def orthogonal_learning(self, func, x_current):\n        # Generate an orthogonal array design\n        oa = self.create_orthogonal_array(self.dim)\n        \n        # Generate test points based on orthogonal array and current solution\n        test_points = self.generate_test_points(x_current, oa, func.bounds.lb, func.bounds.ub)\n        \n        # Evaluate the test points\n        fitness_values = np.array([func(x) for x in test_points])\n        self.eval_count += len(test_points)\n        \n        # Find the best test point\n        best_idx = np.argmin(fitness_values)\n        best_point = test_points[best_idx]\n        best_fitness = fitness_values[best_idx]\n\n        return best_point, best_fitness\n\n    def create_orthogonal_array(self, dim):\n        # A simplified orthogonal array creation (example with L9 array for up to 4 factors at 3 levels)\n        # For higher dimensions/levels, use a proper orthogonal array library like pyDOE\n        if dim <= 4:  # Example for dimensions up to 4\n            oa = np.array([\n                [0, 0, 0],\n                [0, 1, 1],\n                [0, 2, 2],\n                [1, 0, 1],\n                [1, 1, 2],\n                [1, 2, 0],\n                [2, 0, 2],\n                [2, 1, 0],\n                [2, 2, 1]\n            ])\n            if dim < 3:\n              oa = oa[:,:dim]\n            return oa\n        else:\n            # Return a random array if dim > 4 (replace with more proper OA)\n            return np.random.randint(0, 3, size=(9, dim))\n\n    def generate_test_points(self, x_current, oa, lb, ub):\n        num_points, dim = oa.shape\n        test_points = np.zeros((num_points, dim))\n        \n        for i in range(num_points):\n            for j in range(dim):\n                level = oa[i, j]\n                test_points[i, j] = x_current[j] + (level - 1) * (ub - lb) / 4 # Create three levels near x_current\n\n                test_points[i,j] = np.clip(test_points[i,j], lb, ub)\n        return test_points\n\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Mutation\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.pop[idxs]\n            x_mutated = x_r1 + self.F[i] * (x_r2 - x_r3)\n            x_mutated = np.clip(x_mutated, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            x_trial = self.pop[i].copy()\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.CR[i] or j == j_rand:\n                    x_trial[j] = x_mutated[j]\n\n            # Selection\n            f_trial = func(x_trial)\n            self.eval_count += 1\n\n            # Orthogonal learning\n            x_ol, f_ol = self.orthogonal_learning(func, x_trial)\n\n            if f_ol < f_trial:\n                f_trial = f_ol\n                x_trial = x_ol\n\n            if f_trial < self.fitness[i]:\n                # Successful adaptation of F and CR\n                self.F[i] = np.clip(self.F[i] + self.F_adapt_rate * np.random.normal(0, 0.3), 0.1, 1.0) # Small adaptation\n                self.CR[i] = np.clip(self.CR[i] + self.CR_adapt_rate * np.random.normal(0, 0.3), 0.1, 1.0) # Small adaptation\n\n                self.pop[i] = x_trial\n                self.fitness[i] = f_trial\n                self.age[i] = 0 # Reset age\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = x_trial\n            else:\n                # Unsuccessful adaptation of F and CR\n                self.F[i] = np.clip(self.F[i] - self.F_adapt_rate * np.random.normal(0, 0.3), 0.1, 1.0)  # Reduce F\n                self.CR[i] = np.clip(self.CR[i] - self.CR_adapt_rate * np.random.normal(0, 0.3), 0.1, 1.0)  # Reduce CR\n                self.age[i] += 1 # Increase age\n            \n\n    def restart_population(self, func):\n        # Restart all the population except the best individual and some young individuals\n        best_idx = np.argmin(self.fitness)\n\n        # Select individuals to keep based on age (favor younger ones)\n        num_to_keep = int(self.pop_size * 0.2) # Keep top 20% youngest\n        age_sorted_indices = np.argsort(self.age)\n        indices_to_keep = age_sorted_indices[:num_to_keep]\n\n        new_pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size - len(indices_to_keep) - 1, self.dim))\n        new_fitness = np.array([func(x) for x in new_pop])\n        self.eval_count += self.pop_size - len(indices_to_keep) - 1\n\n        # Insert the best individual from the previous population and the younger individuals\n        temp_pop = np.vstack((self.pop[best_idx], self.pop[indices_to_keep], new_pop))\n        temp_fitness = np.hstack((self.fitness[best_idx], self.fitness[indices_to_keep], new_fitness))\n        temp_age = np.hstack((self.age[best_idx], self.age[indices_to_keep], np.zeros(len(new_fitness))))\n\n        self.pop = temp_pop\n        self.fitness = temp_fitness\n        self.age = temp_age\n        \n        best_idx = np.argmin(self.fitness)\n        if self.fitness[best_idx] < self.f_opt:\n            self.f_opt = self.fitness[best_idx]\n            self.x_opt = self.pop[best_idx]\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.age = np.zeros(self.pop_size)\n\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n\n            # Stagnation check\n            if self.f_opt < self.previous_best_fitness:\n                self.stagnation_counter = 0\n                self.previous_best_fitness = self.f_opt\n            else:\n                self.stagnation_counter += self.pop_size # Increment by population size each generation\n\n            # Aging mechanism: increment age of all individuals\n            self.age += self.aging_rate\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.restart_population(func)\n                self.stagnation_counter = 0 # Reset stagnation counter\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "An exception occured: TypeError: only size-1 arrays can be converted to Python scalars\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 174, in __call__\n  File \"<string>\", line 111, in evolve\n  File \"<string>\", line 40, in orthogonal_learning\n  File \"<string>\", line 82, in generate_test_points\nValueError: setting an array element with a sequence.\n.", "error": "", "parent_ids": ["2e6d1785-a297-40a4-aaa5-56041c800eed"], "operator": null, "metadata": {}}
{"id": "a40cbb30-3b38-48f5-bf24-b069c4774eda", "fitness": 0.17224270122010613, "name": "AdaptiveDE_CMA", "description": "Adaptive Differential Evolution with a mirrored sampling strategy, population diversity maintenance, and covariance matrix adaptation for improved exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDE_CMA:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_init=0.5, CR_init=0.7, restart_prob=0.05, F_adapt_rate=0.1, CR_adapt_rate=0.1, stagnation_threshold=1000, cma_decay=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = np.full(pop_size, F_init)\n        self.CR = np.full(pop_size, CR_init)\n        self.restart_prob = restart_prob\n        self.F_adapt_rate = F_adapt_rate\n        self.CR_adapt_rate = CR_adapt_rate\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.stagnation_threshold = stagnation_threshold\n        self.cma_decay = cma_decay\n        self.C = np.eye(dim)  # Covariance matrix for CMA\n        self.mean = None\n\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        if self.fitness[best_idx] < self.f_opt:\n            self.f_opt = self.fitness[best_idx]\n            self.x_opt = self.pop[best_idx]\n        self.mean = self.x_opt.copy()\n\n\n    def mirrored_sampling(self, x, func):\n        x_mirrored = self.mean + (self.mean - x)\n        x_mirrored = np.clip(x_mirrored, func.bounds.lb, func.bounds.ub)\n        return x_mirrored\n\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Mutation with mirrored sampling\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.pop[idxs]\n\n            # Mirrored sampling\n            x_mirrored = self.mirrored_sampling(self.pop[i], func)\n\n            if np.random.rand() < 0.5: #probabilistic choice for mirrored sampling to diversify the population\n                x_mutated = x_r1 + self.F[i] * (x_r2 - x_r3)\n            else:\n                 x_mutated = x_mirrored + self.F[i] * (x_r2 - x_r3) # Use mirrored sampling mutation\n            \n            x_mutated = np.clip(x_mutated, func.bounds.lb, func.bounds.ub)\n\n\n            # Crossover\n            x_trial = self.pop[i].copy()\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.CR[i] or j == j_rand:\n                    x_trial[j] = x_mutated[j]\n\n            # Selection\n            f_trial = func(x_trial)\n            self.eval_count += 1\n\n            if f_trial < self.fitness[i]:\n                # Successful adaptation of F and CR\n                self.F[i] = np.clip(self.F[i] + self.F_adapt_rate * np.random.normal(0, 0.3), 0.1, 1.0)\n                self.CR[i] = np.clip(self.CR[i] + self.CR_adapt_rate * np.random.normal(0, 0.3), 0.1, 1.0)\n\n                self.pop[i] = x_trial\n                self.fitness[i] = f_trial\n                \n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = x_trial\n                    self.mean = self.x_opt.copy()\n\n                # Update covariance matrix (CMA-like update)\n                diff = x_trial - self.mean\n                self.C = self.cma_decay * self.C + (1 - self.cma_decay) * np.outer(diff, diff)\n\n\n            else:\n                # Unsuccessful adaptation of F and CR\n                self.F[i] = np.clip(self.F[i] - self.F_adapt_rate * np.random.normal(0, 0.3), 0.1, 1.0)\n                self.CR[i] = np.clip(self.CR[i] - self.CR_adapt_rate * np.random.normal(0, 0.3), 0.1, 1.0)\n\n\n    def restart_population(self, func):\n        # Restart all the population except the best individual\n        best_idx = np.argmin(self.fitness)\n        \n        new_pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size -1 , self.dim))\n        new_fitness = np.array([func(x) for x in new_pop])\n        self.eval_count += self.pop_size -1\n\n        # Insert the best individual from the previous population\n        temp_pop = np.vstack((self.pop[best_idx], new_pop))\n        temp_fitness = np.hstack((self.fitness[best_idx], new_fitness))\n\n        self.pop = temp_pop\n        self.fitness = temp_fitness\n        \n        best_idx = np.argmin(self.fitness)\n        if self.fitness[best_idx] < self.f_opt:\n            self.f_opt = self.fitness[best_idx]\n            self.x_opt = self.pop[best_idx]\n            self.mean = self.x_opt.copy()\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.C = np.eye(self.dim)  # Reset covariance matrix\n        \n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n\n            # Stagnation check\n            if self.f_opt < self.previous_best_fitness:\n                self.stagnation_counter = 0\n                self.previous_best_fitness = self.f_opt\n            else:\n                self.stagnation_counter += self.pop_size # Increment by population size each generation\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.restart_population(func)\n                self.stagnation_counter = 0 # Reset stagnation counter\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDE_CMA scored 0.172 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2e6d1785-a297-40a4-aaa5-56041c800eed"], "operator": null, "metadata": {"aucs": [0.20445518235243765, 0.31227292130788076, 0]}}
{"id": "f92908a0-17d9-4686-b30f-8ea4b64f0e34", "fitness": 0.38417285521342653, "name": "AdaptiveCMAES", "description": "Adaptive CMA-ES with mirrored sampling, dynamic population size based on fitness improvement, covariance matrix adaptation with eigenvalue clipping, and a time-dependent learning rate to balance exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveCMAES:\n    def __init__(self, budget=10000, dim=10, sigma0 = 0.5):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n\n        self.popsize = 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.min_popsize = 4\n        self.max_popsize = 100\n        self.adapt_popsize_freq = 10\n\n    def __call__(self, func):\n        # Initialize variables\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)  # Covariance matrix\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n        # Parameters (using common defaults)\n        c_sigma = (self.mu + 2) / (self.dim + self.mu + 5)\n        c_c = (4 + self.mu / self.dim) / (self.dim + 4 + 2 * self.mu / self.dim)\n        c_1 = 2 / ((self.dim + 1.3)**2 + self.mu)\n        c_mu = min(1 - c_1, 2 * (self.mu - 1 + 1 / self.mu) / ((self.dim + 2)**2 + self.mu))\n        d_sigma = 1 + 2 * max(0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1) + c_sigma\n\n        # Eigen decomposition of C (expensive, do it rarely)\n        C_evals, C_evecs = np.linalg.eigh(C)\n        C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n        \n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n\n        success_history = []\n        learning_rate = 1.0 # added learning rate\n\n        while eval_count < self.budget:\n            # Sample population\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            x = mean + sigma * (C_sqrt @ z.T).T\n            \n            # Mirror sampling to improve exploration\n            x_mirrored = mean - sigma * (C_sqrt @ z.T).T\n            x = np.concatenate([x, x_mirrored], axis=0)\n            z = np.concatenate([z, -z], axis=0)\n            \n            # Evaluate the new points, but respect the budget\n            f = np.array([func(xi) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n            eval_count += len(x) \n\n            # Sort by fitness\n            idx = np.argsort(f)\n            x = x[idx]\n            z = z[idx]\n            f = f[idx]\n\n            # Update optimal solution\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[0]\n                \n            # Selection and recombination\n            x_mu = x[:self.mu]\n            z_mu = z[:self.mu]\n\n            mean_new = np.mean(x_mu, axis=0)\n            z_w = np.mean(z_mu, axis=0)\n            \n            # Covariance matrix adaptation\n            ps = (1 - c_sigma) * ps + np.sqrt(c_sigma * (2 - c_sigma) * self.mu) * (C_sqrt @ z_w)\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - c_sigma)**(2 * eval_count / self.popsize)) < chiN * (1.4 + 2 / (self.dim + 1))\n            \n            pc = (1 - c_c) * pc + hsig * np.sqrt(c_c * (2 - c_c) * self.mu) * z_w\n\n            C = (1 - c_1 - c_mu) * C + c_1 * (pc[:, None] @ pc[None, :])\n            for i in range(self.mu):\n                C += c_mu * (z_mu[i, :, None] @ z_mu[i, None, :])\n                \n            # Update step size (more aggressive)\n            sigma *= np.exp((c_sigma / d_sigma) * (np.linalg.norm(ps) / chiN - 1) + learning_rate * 0.1 * (f.mean() - f[0]))  # added fitness difference term and learning rate\n            sigma = max(sigma, 1e-10) # prevent sigma from becoming too small\n\n            # Update mean\n            mean = mean_new\n\n            # Eigen decomposition of C\n            if eval_count % (self.popsize * 5) == 0:  # Re-compute after every 5 generations\n                C = np.triu(C) + np.triu(C, 1).T  # Enforce symmetry\n                \n                # Clip covariance matrix eigenvalues to avoid ill-conditioning\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_evals = np.maximum(C_evals, 1e-10)\n                C_evals = np.minimum(C_evals, 1e10) # cap eigenvalues\n                C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n\n            if np.any(np.isnan(mean)) or np.any(np.isnan(C)):\n                print(\"NaN detected, resetting...\")\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                sigma = self.sigma0\n\n            # Adapt population size\n            if eval_count % (self.popsize * self.adapt_popsize_freq) == 0:\n                success_rate = (f_opt < np.mean(f)) if len(f) > 0 else 0.5\n                success_history.append(success_rate)\n\n                if len(success_history) > 5:\n                    avg_success = np.mean(success_history[-5:])\n                    if avg_success > 0.6:\n                        self.popsize = min(self.popsize + 1, self.max_popsize)\n                        self.mu = self.popsize // 2\n                        print(f\"Increasing popsize to {self.popsize}\")\n                    elif avg_success < 0.4:\n                        self.popsize = max(self.popsize - 1, self.min_popsize)\n                        self.mu = self.popsize // 2\n                        print(f\"Decreasing popsize to {self.popsize}\")\n            \n            # Decay learning rate\n            learning_rate *= 0.999 # decay learning rate over time\n        \n        return f_opt, x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveCMAES scored 0.384 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["de67a27a-40b3-478f-90e4-5d1162f9c0f3"], "operator": null, "metadata": {"aucs": [0.11847033846359267, 0.18012510021581252, 0.8206978859463324, 0.22914807149431216, 0.22430131653547558, 0.22782937207107312, 0.23566979709277458, 0.268491229961046, 0.4760811706567051, 0.1590032955157712, 0.22719931157152595, 0.9912546833610694, 0.23944224116294077, 0.5963678915979186, 0.5637952389424816, 0.32720896436614033, 0.4115921975927158, 0.7496223833341318, 0.16791996125366493, 0.46923665313304574]}}
{"id": "99748312-b329-4b52-a9fa-499daf5b1417", "fitness": 0.6249863700772721, "name": "CMAES_History", "description": "Enhanced CMA-ES with a history of successful steps to adapt the covariance matrix more effectively and prevent premature convergence.", "code": "import numpy as np\n\nclass CMAES_History:\n    def __init__(self, budget=10000, dim=10, sigma0=0.5, history_length=10):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.history_length = history_length  # Length of step history for adaptation\n\n        self.popsize = 4 + int(np.floor(3 * np.log(self.dim)))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.step_history = []\n\n\n    def __call__(self, func):\n        # Initialize variables\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)  # Covariance matrix\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n        # Parameters (using common defaults)\n        c_sigma = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        c_c = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        c_1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        c_mu = min(1 - c_1, 2 * (self.mueff - 1 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        d_sigma = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + c_sigma\n        c_1a = c_1\n        c_mua = c_mu\n\n        # Eigen decomposition of C (expensive, do it rarely)\n        C_evals, C_evecs = np.linalg.eigh(C)\n        C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n        C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n        \n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n\n        while eval_count < self.budget:\n            # Sample population\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            x = mean + sigma * (C_sqrt @ z.T).T\n            \n            # Evaluate the new points\n            f = np.array([func(xi) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n            eval_count += len(x) \n\n            # Sort by fitness\n            idx = np.argsort(f)\n            x = x[idx]\n            z = z[idx]\n            f = f[idx]\n\n            # Update optimal solution\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[0]\n                \n            # Selection and recombination\n            x_mu = x[:self.mu]\n            z_mu = z[:self.mu]\n\n            mean_new = np.sum(x_mu * self.weights[:,None], axis=0)\n            z_w = np.sum(z_mu * self.weights[:,None], axis=0)\n            \n            # Covariance matrix adaptation\n            ps = (1 - c_sigma) * ps + np.sqrt(c_sigma * (2 - c_sigma) * self.mueff) * (C_invsqrt @ (mean_new - mean) / sigma)\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - c_sigma)**(2 * eval_count / self.popsize)) < chiN * (1.4 + 2 / (self.dim + 1))\n            \n            pc = (1 - c_c) * pc + hsig * np.sqrt(c_c * (2 - c_c) * self.mueff) * (mean_new - mean) / sigma\n\n            C = (1 - c_1 - c_mu) * C + c_1 * (pc[:, None] @ pc[None, :])\n            for i in range(self.mu):\n                C += c_mu * self.weights[i] * (z_mu[i, :, None] @ z_mu[i, None, :])\n                \n            # Active CMA\n            if c_1a > 0 and c_mua > 0:\n                negidx = np.where(self.weights < 0)[0]\n                znorm = np.zeros((len(negidx), self.dim))\n                for i, idx in enumerate(negidx):\n                    znorm[i] = z_mu[idx] * np.sqrt(-self.weights[idx])\n                C += c_1a * (1/np.linalg.norm(ps)**2 + c_mua) * np.sum(znorm.T @ znorm, axis=1)\n\n            # Update step size\n            sigma *= np.exp((c_sigma / d_sigma) * (np.linalg.norm(ps) / chiN - 1))\n\n            # Step history adaptation\n            step = (mean_new - mean) / sigma\n            self.step_history.append(step)\n            if len(self.step_history) > self.history_length:\n                self.step_history.pop(0)\n\n            # Adapt C based on step history\n            for h_step in self.step_history:\n                C += 0.1 * c_mu * (h_step[:, None] @ h_step[None, :])  # Dampened update\n                \n            # Update mean\n            mean = mean_new\n\n            # Eigen decomposition of C\n            if eval_count % (self.popsize * 5) == 0:  # Re-compute after every 5 generations\n                C = np.triu(C) + np.triu(C, 1).T  # Enforce symmetry\n                try:\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_evals = np.maximum(C_evals, 1e-10) # Avoid zero or negative eigenvalues\n                    C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                except np.linalg.LinAlgError:\n                    print(\"LinAlgError encountered, resetting C\")\n                    C = np.eye(self.dim)\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                \n            if np.any(np.isnan(mean)) or np.any(np.isnan(C)):\n                print(\"NaN detected, resetting...\")\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                sigma = self.sigma0\n        \n        return f_opt, x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm CMAES_History scored 0.625 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["12a79555-b49e-4199-b9f8-dce5cdded369"], "operator": null, "metadata": {"aucs": [0.25004981921624303, 0.22502091856362794, 0.930132069455842, 0.9541713443957704, 0.9449655273530125, 0.9562585711891031, 0.3511206780211783, 0.9345170254929008, 0.9351968567100971, 0.15680297277770028, 0.9533288359518173, 0.9947243297177526, 0.21901949748346283, 0.9455726966231939, 0.6329809315919565, 0.9262090708278777, 0.38788534094908067, 0.15439809963079676, 0.14074406894459968, 0.5066287466494288]}}
{"id": "8d396969-ba5a-47d4-8075-374586c711f9", "fitness": 0.5858312950204644, "name": "EnhancedCMAES", "description": "Enhanced CMA-ES with restart mechanism based on stagnation detection and adaptive step-size scaling.", "code": "import numpy as np\n\nclass EnhancedCMAES:\n    def __init__(self, budget=10000, dim=10, sigma0=0.5, stagnation_threshold=10):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.stagnation_threshold = stagnation_threshold  # Number of generations without improvement\n        self.stagnation_counter = 0\n        self.best_fitness = np.inf\n\n        self.popsize = 4 + int(np.floor(3 * np.log(self.dim)))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n    def __call__(self, func):\n        # Initialize variables\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)  # Covariance matrix\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n        # Parameters (using common defaults)\n        c_sigma = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        c_c = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        c_1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        c_mu = min(1 - c_1, 2 * (self.mueff - 1 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        d_sigma = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + c_sigma\n        c_1a = c_1\n        c_mua = c_mu\n\n        # Eigen decomposition of C (expensive, do it rarely)\n        C_evals, C_evecs = np.linalg.eigh(C)\n        C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n        C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n        \n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n\n        while eval_count < self.budget:\n            # Sample population\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            x = mean + sigma * (C_sqrt @ z.T).T\n            \n            # Evaluate the new points\n            f = np.array([func(xi) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n            eval_count += len(x) \n\n            # Sort by fitness\n            idx = np.argsort(f)\n            x = x[idx]\n            z = z[idx]\n            f = f[idx]\n\n            # Update optimal solution\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[0]\n                self.stagnation_counter = 0\n            else:\n                self.stagnation_counter += 1\n                \n            # Stagnation check and restart\n            if self.stagnation_counter > self.stagnation_threshold:\n                # Restart strategy: reset mean and increase sigma\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                sigma = self.sigma0 * 2  # Increase sigma to explore more\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                self.stagnation_counter = 0  # Reset counter\n                print(\"Restarting CMA-ES due to stagnation\")\n\n            # Selection and recombination\n            x_mu = x[:self.mu]\n            z_mu = z[:self.mu]\n\n            mean_new = np.sum(x_mu * self.weights[:,None], axis=0)\n            z_w = np.sum(z_mu * self.weights[:,None], axis=0)\n            \n            # Covariance matrix adaptation\n            ps = (1 - c_sigma) * ps + np.sqrt(c_sigma * (2 - c_sigma) * self.mueff) * (C_invsqrt @ (mean_new - mean) / sigma)\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - c_sigma)**(2 * eval_count / self.popsize)) < chiN * (1.4 + 2 / (self.dim + 1))\n            \n            pc = (1 - c_c) * pc + hsig * np.sqrt(c_c * (2 - c_c) * self.mueff) * (mean_new - mean) / sigma\n\n            C = (1 - c_1 - c_mu) * C + c_1 * (pc[:, None] @ pc[None, :])\n            for i in range(self.mu):\n                C += c_mu * self.weights[i] * (z_mu[i, :, None] @ z_mu[i, None, :])\n                \n            # Active CMA\n            if c_1a > 0 and c_mua > 0:\n                negidx = np.where(self.weights < 0)[0]\n                znorm = np.zeros((len(negidx), self.dim))\n                for i, idx in enumerate(negidx):\n                    znorm[i] = z_mu[idx] * np.sqrt(-self.weights[idx])\n                C += c_1a * (1/np.linalg.norm(ps)**2 + c_mua) * np.sum(znorm.T @ znorm, axis=1)\n\n            # Update step size\n            sigma *= np.exp((c_sigma / d_sigma) * (np.linalg.norm(ps) / chiN - 1))\n\n            # Adaptive sigma scaling (if not improving, reduce sigma)\n            if f[0] >= self.best_fitness:\n                sigma *= 0.95  # Reduce step size\n            else:\n                self.best_fitness = f[0]\n            \n            # Update mean\n            mean = mean_new\n\n            # Eigen decomposition of C\n            if eval_count % (self.popsize * 5) == 0:  # Re-compute after every 5 generations\n                C = np.triu(C) + np.triu(C, 1).T  # Enforce symmetry\n                try:\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_evals = np.maximum(C_evals, 1e-10) # Avoid zero or negative eigenvalues\n                    C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                except np.linalg.LinAlgError:\n                    print(\"LinAlgError encountered, resetting C\")\n                    C = np.eye(self.dim)\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                \n            if np.any(np.isnan(mean)) or np.any(np.isnan(C)):\n                print(\"NaN detected, resetting...\")\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                sigma = self.sigma0\n        \n        return f_opt, x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm EnhancedCMAES scored 0.586 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["12a79555-b49e-4199-b9f8-dce5cdded369"], "operator": null, "metadata": {"aucs": [0.2897471450205279, 0.40538256077463153, 0.9255298939344224, 0.7714124351937842, 0.2620763480978099, 0.9593282732292046, 0.2966281946436675, 0.9200400458077758, 0.878890745233433, 0.18555776170764948, 0.9355818520296791, 0.9921593749022538, 0.31541601375290573, 0.5758547211716099, 0.6137540473760863, 0.3545489176806632, 0.3906057174691765, 0.9677670112239206, 0.1754136956673854, 0.5009311454927003]}}
{"id": "b4fe9559-b6a1-4d0c-8bc9-956996b0768c", "fitness": -Infinity, "name": "EnhancedAdaptiveDE", "description": "Enhanced Adaptive Differential Evolution with a mirrored boundary handling strategy, orthogonal crossover, and adaptive population size reduction during stagnation to improve exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_init=0.5, CR_init=0.7, restart_prob=0.05, F_adapt_rate=0.1, CR_adapt_rate=0.1, stagnation_threshold=1000, pop_reduce_factor=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = np.full(pop_size, F_init)  # Mutation factor for each individual\n        self.CR = np.full(pop_size, CR_init)  # Crossover rate for each individual\n        self.restart_prob = restart_prob\n        self.F_adapt_rate = F_adapt_rate\n        self.CR_adapt_rate = CR_adapt_rate\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.stagnation_threshold = stagnation_threshold\n        self.pop_reduce_factor = pop_reduce_factor  # Factor to reduce population size during stagnation\n        self.min_pop_size = 10  # Minimum population size\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        if self.fitness[best_idx] < self.f_opt:\n            self.f_opt = self.fitness[best_idx]\n            self.x_opt = self.pop[best_idx]\n\n    def mirrored_boundary_handling(self, x, lb, ub):\n        \"\"\"Handles boundaries using a mirrored strategy.\"\"\"\n        x_corrected = x.copy()\n        for i in range(len(x)):\n            if x[i] < lb:\n                x_corrected[i] = lb + (lb - x[i])\n            elif x[i] > ub:\n                x_corrected[i] = ub - (x[i] - ub)\n        return x_corrected\n    \n    def orthogonal_crossover(self, x_target, x_mutated):\n        \"\"\"Performs orthogonal crossover.\"\"\"\n        num_groups = 3 # increased groups for potentially better exploration.\n        group_size = self.dim // num_groups\n        x_trial = x_target.copy()\n\n        for g in range(num_groups):\n            start_idx = g * group_size\n            end_idx = (g + 1) * group_size if g < num_groups - 1 else self.dim\n\n            if np.random.rand() < self.CR[i]: #Apply with certain prob, might also adapt this.\n               for j in range(start_idx, end_idx):\n                   x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Mutation\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.pop[idxs]\n            x_mutated = x_r1 + self.F[i] * (x_r2 - x_r3)\n            x_mutated = self.mirrored_boundary_handling(x_mutated, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            x_trial = self.orthogonal_crossover(self.pop[i], x_mutated) #Use orthogonal crossover\n\n            # Selection\n            f_trial = func(x_trial)\n            self.eval_count += 1\n\n            if f_trial < self.fitness[i]:\n                # Successful adaptation of F and CR\n                self.F[i] = np.clip(self.F[i] + self.F_adapt_rate * np.random.normal(0, 0.3), 0.1, 1.0) # Small adaptation\n                self.CR[i] = np.clip(self.CR[i] + self.CR_adapt_rate * np.random.normal(0, 0.3), 0.1, 1.0) # Small adaptation\n\n                self.pop[i] = x_trial\n                self.fitness[i] = f_trial\n                \n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = x_trial\n            else:\n                # Unsuccessful adaptation of F and CR\n                self.F[i] = np.clip(self.F[i] - self.F_adapt_rate * np.random.normal(0, 0.3), 0.1, 1.0)  # Reduce F\n                self.CR[i] = np.clip(self.CR[i] - self.CR_adapt_rate * np.random.normal(0, 0.3), 0.1, 1.0)  # Reduce CR\n\n    def reduce_population_size(self):\n        \"\"\"Reduces the population size.\"\"\"\n        if self.pop_size > self.min_pop_size:\n            new_pop_size = int(self.pop_size * self.pop_reduce_factor)\n            new_pop_size = max(new_pop_size, self.min_pop_size)  # Ensure it doesn't go below minimum\n\n            # Select the best individuals to keep\n            best_indices = np.argsort(self.fitness)[:new_pop_size]\n            self.pop = self.pop[best_indices]\n            self.fitness = self.fitness[best_indices]\n            self.pop_size = new_pop_size\n            self.F = self.F[best_indices]\n            self.CR = self.CR[best_indices]\n\n    def restart_population(self, func):\n        # Restart all the population except the best individual\n        best_idx = np.argmin(self.fitness)\n        \n        new_pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size -1 , self.dim))\n        new_fitness = np.array([func(x) for x in new_pop])\n        self.eval_count += self.pop_size -1\n\n        # Insert the best individual from the previous population\n        temp_pop = np.vstack((self.pop[best_idx], new_pop))\n        temp_fitness = np.hstack((self.fitness[best_idx], new_fitness))\n\n        self.pop = temp_pop\n        self.fitness = temp_fitness\n        \n        best_idx = np.argmin(self.fitness)\n        if self.fitness[best_idx] < self.f_opt:\n            self.f_opt = self.fitness[best_idx]\n            self.x_opt = self.pop[best_idx]\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n\n            # Stagnation check\n            if self.f_opt < self.previous_best_fitness:\n                self.stagnation_counter = 0\n                self.previous_best_fitness = self.f_opt\n            else:\n                self.stagnation_counter += self.pop_size # Increment by population size each generation\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.reduce_population_size()\n                self.restart_population(func) #restart after reduction\n                self.stagnation_counter = 0 # Reset stagnation counter\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 137, in __call__\n  File \"<string>\", line 68, in evolve\n  File \"<string>\", line 38, in mirrored_boundary_handling\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n.", "error": "", "parent_ids": ["2e6d1785-a297-40a4-aaa5-56041c800eed"], "operator": null, "metadata": {}}
{"id": "457debd7-c06a-4882-a676-c504d7397490", "fitness": 0.642655059458882, "name": "CMAES", "description": "CMA-ES with spectral correction of the covariance matrix, adaptive step size based on success rate, and a more robust handling of covariance matrix decomposition failures.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, sigma0 = 0.5):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n\n        self.popsize = 4 + int(np.floor(3 * np.log(self.dim)))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n        self.success_rate = 0.2  # Initial success rate\n        self.learning_rate_sigma = 0.2  # Learning rate for step size\n\n    def __call__(self, func):\n        # Initialize variables\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)  # Covariance matrix\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n        # Parameters (using common defaults)\n        c_sigma = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        c_c = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        c_1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        c_mu = min(1 - c_1, 2 * (self.mueff - 1 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        d_sigma = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + c_sigma\n        c_1a = c_1\n        c_mua = c_mu\n\n        # Eigen decomposition of C (expensive, do it rarely)\n        try:\n            C_evals, C_evecs = np.linalg.eigh(C)\n            C_evals = np.maximum(C_evals, 1e-10)\n            C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n            C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n        except np.linalg.LinAlgError:\n            print(\"Initial LinAlgError encountered, resetting C\")\n            C = np.eye(self.dim)\n            C_evals, C_evecs = np.linalg.eigh(C)\n            C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n            C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n\n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n        successes = 0\n\n        while eval_count < self.budget:\n            # Sample population\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            x = mean + sigma * (C_sqrt @ z.T).T\n            \n            # Evaluate the new points\n            f = np.array([func(xi) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n            eval_count += len(x) \n\n            # Sort by fitness\n            idx = np.argsort(f)\n            x = x[idx]\n            z = z[idx]\n            f = f[idx]\n\n            # Update optimal solution\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[0]\n                successes += 1\n                \n            # Selection and recombination\n            x_mu = x[:self.mu]\n            z_mu = z[:self.mu]\n\n            mean_new = np.sum(x_mu * self.weights[:,None], axis=0)\n            z_w = np.sum(z_mu * self.weights[:,None], axis=0)\n            \n            # Covariance matrix adaptation\n            ps = (1 - c_sigma) * ps + np.sqrt(c_sigma * (2 - c_sigma) * self.mueff) * (C_invsqrt @ (mean_new - mean) / sigma)\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - c_sigma)**(2 * eval_count / self.popsize)) < chiN * (1.4 + 2 / (self.dim + 1))\n            \n            pc = (1 - c_c) * pc + hsig * np.sqrt(c_c * (2 - c_c) * self.mueff) * (mean_new - mean) / sigma\n\n            C = (1 - c_1 - c_mu) * C + c_1 * (pc[:, None] @ pc[None, :])\n            for i in range(self.mu):\n                C += c_mu * self.weights[i] * (z_mu[i, :, None] @ z_mu[i, None, :])\n                \n            # Active CMA\n            if c_1a > 0 and c_mua > 0:\n                negidx = np.where(self.weights < 0)[0]\n                znorm = np.zeros((len(negidx), self.dim))\n                for i, idx in enumerate(negidx):\n                    znorm[i] = z_mu[idx] * np.sqrt(-self.weights[idx])\n                C += c_1a * (1/np.linalg.norm(ps)**2 + c_mua) * np.sum(znorm.T @ znorm, axis=1)\n            \n            # Spectral Clipping (Correct negative eigenvalues)\n            C = np.triu(C) + np.triu(C, 1).T  # Enforce symmetry\n            try:\n                evals, evecs = np.linalg.eigh(C)\n                evals = np.maximum(evals, 1e-10)  # Clip small/negative eigenvalues\n                C = evecs @ np.diag(evals) @ evecs.T\n            except np.linalg.LinAlgError:\n                print(\"LinAlgError during spectral clipping, resetting C\")\n                C = np.eye(self.dim)\n\n            # Update success rate\n            self.success_rate = 0.9 * self.success_rate + 0.1 * (successes > 0) # Binary success or failure\n            successes = 0\n\n            # Update step size based on success rate\n            sigma *= np.exp(self.learning_rate_sigma * (self.success_rate - 0.2))  # Adjust towards target rate of 0.2\n            \n            # Update mean\n            mean = mean_new\n\n            # Eigen decomposition of C\n            if eval_count % (self.popsize * 5) == 0:  # Re-compute after every 5 generations\n                C = np.triu(C) + np.triu(C, 1).T  # Enforce symmetry\n                try:\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_evals = np.maximum(C_evals, 1e-10) # Avoid zero or negative eigenvalues\n                    C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                except np.linalg.LinAlgError:\n                    print(\"LinAlgError encountered, resetting C\")\n                    C = np.eye(self.dim)\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                \n            if np.any(np.isnan(mean)) or np.any(np.isnan(C)):\n                print(\"NaN detected, resetting...\")\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                sigma = self.sigma0\n        \n        return f_opt, x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm CMAES scored 0.643 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["12a79555-b49e-4199-b9f8-dce5cdded369"], "operator": null, "metadata": {"aucs": [0.39838569992459283, 0.2198418388883452, 0.7905355992693692, 0.8984582072071543, 0.8005254029139585, 0.8286970228632535, 0.38479437749342515, 0.7596466569347916, 0.7919146453836167, 0.2325689968414013, 0.9304123067432875, 0.9906788381001617, 0.2913774921412565, 0.7731194554573997, 0.6320930603216453, 0.8127920250873972, 0.6945115546897611, 0.8752461661139092, 0.2507342648956252, 0.4967675779072892]}}
{"id": "04946dbd-1246-4d39-93a4-1de8b4641ee2", "fitness": 0.2352934263651379, "name": "AdaptiveCMAES", "description": "Adaptive CMA-ES with mirrored sampling, active covariance update, dynamic population size, and a combination of fitness difference and normalized path length to adjust step size, alongside constraint handling.", "code": "import numpy as np\n\nclass AdaptiveCMAES:\n    def __init__(self, budget=10000, dim=10, sigma0 = 0.5):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n\n        self.popsize = 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.min_popsize = 4\n        self.max_popsize = 100\n        self.adapt_popsize_freq = 10\n        self.lb = -5.0\n        self.ub = 5.0\n\n\n    def __call__(self, func):\n        # Initialize variables\n        mean = np.random.uniform(self.lb, self.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)  # Covariance matrix\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n        # Parameters (using common defaults)\n        c_sigma = (self.mu + 2) / (self.dim + self.mu + 5)\n        c_c = (4 + self.mu / self.dim) / (self.dim + 4 + 2 * self.mu / self.dim)\n        c_1 = 2 / ((self.dim + 1.3)**2 + self.mu)\n        c_mu = min(1 - c_1, 2 * (self.mu - 1 + 1 / self.mu) / ((self.dim + 2)**2 + self.mu))\n        d_sigma = 1 + 2 * max(0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1) + c_sigma\n        c_1a = c_1 / c_mu # Scaling factor for active update\n\n        # Eigen decomposition of C (expensive, do it rarely)\n        C_evals, C_evecs = np.linalg.eigh(C)\n        C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n        C_sqrt_inv = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n        \n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n\n        success_history = []\n        \n        while eval_count < self.budget:\n            # Sample population\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            x = mean + sigma * (C_sqrt @ z.T).T\n            \n            # Mirror sampling to improve exploration\n            x_mirrored = mean - sigma * (C_sqrt @ z.T).T\n            x = np.concatenate([x, x_mirrored], axis=0)\n            z = np.concatenate([z, -z], axis=0)\n            \n            # Constraint handling: clip to bounds\n            x = np.clip(x, self.lb, self.ub)\n\n            # Evaluate the new points, but respect the budget\n            f = np.array([func(xi) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n            eval_count += len(x) \n\n            # Sort by fitness\n            idx = np.argsort(f)\n            x = x[idx]\n            z = z[idx]\n            f = f[idx]\n\n            # Update optimal solution\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[0]\n                \n            # Selection and recombination\n            x_mu = x[:self.mu]\n            z_mu = z[:self.mu]\n\n            mean_new = np.mean(x_mu, axis=0)\n            z_w = np.mean(z_mu, axis=0)\n            \n            # Covariance matrix adaptation\n            ps = (1 - c_sigma) * ps + np.sqrt(c_sigma * (2 - c_sigma) * self.mu) * (C_sqrt @ z_w)\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - c_sigma)**(2 * eval_count / self.popsize)) < chiN * (1.4 + 2 / (self.dim + 1))\n            \n            pc = (1 - c_c) * pc + hsig * np.sqrt(c_c * (2 - c_c) * self.mu) * z_w\n\n            # Active CMA update\n            C_temp = (1 - c_1 - c_mu) * C + c_1 * (pc[:, None] @ pc[None, :])\n            for i in range(self.mu):\n                C_temp += c_mu * (z_mu[i, :, None] @ z_mu[i, None, :])\n\n            # Rank-one update for negative eigenvalue correction (active CMA)\n            C = C_temp + c_1a * (1 - hsig) * c_c * (2 - c_c) * C @ (pc[:, None] @ pc[None, :]) @ C\n                \n            # Update step size (more aggressive)\n            sigma *= np.exp((c_sigma / d_sigma) * (np.linalg.norm(ps) / chiN - 1) + 0.1 * (f.mean() - f[0]))  # added fitness difference term\n            sigma = max(sigma, 1e-10) # prevent sigma from becoming too small\n\n            # Update mean\n            mean = mean_new\n\n            # Eigen decomposition of C\n            if eval_count % (self.popsize * 5) == 0:  # Re-compute after every 5 generations\n                C = np.triu(C) + np.triu(C, 1).T  # Enforce symmetry\n                \n                # Clip covariance matrix eigenvalues to avoid ill-conditioning\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_evals = np.maximum(C_evals, 1e-10)\n                C_evals = np.minimum(C_evals, 1e10) # cap eigenvalues\n                C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_sqrt_inv = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n\n            if np.any(np.isnan(mean)) or np.any(np.isnan(C)):\n                print(\"NaN detected, resetting...\")\n                mean = np.random.uniform(self.lb, self.ub, size=self.dim)\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_sqrt_inv = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                sigma = self.sigma0\n\n            # Adapt population size\n            if eval_count % (self.popsize * self.adapt_popsize_freq) == 0:\n                success_rate = (f_opt < np.mean(f)) if len(f) > 0 else 0.5\n                success_history.append(success_rate)\n\n                if len(success_history) > 5:\n                    avg_success = np.mean(success_history[-5:])\n                    if avg_success > 0.6:\n                        self.popsize = min(self.popsize + 1, self.max_popsize)\n                        self.mu = self.popsize // 2\n                    elif avg_success < 0.4:\n                        self.popsize = max(self.popsize - 1, self.min_popsize)\n                        self.mu = self.popsize // 2\n        \n        return f_opt, x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveCMAES scored 0.235 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["de67a27a-40b3-478f-90e4-5d1162f9c0f3"], "operator": null, "metadata": {"aucs": [0.015692998755769794, 0.07780258645461924, 0.2725448220737794, 0.12322859314055423, 0.11721373339466168, 0.16001608712444082, 0.16726010223045484, 0.12361712184924145, 0.14558724295624592, 0.12861195464777975, 0.17124579102654192, 0.995285325532274, 0.20435300735178208, 0.13772195284526656, 0.6214050799722673, 0.2352966869058628, 0.2195211409507004, 0.18082220695892381, 0.12479950147811747, 0.48384259165347465]}}
{"id": "32d76cfb-7a40-4aea-bd64-12c4f59733c7", "fitness": 0.3613813208827669, "name": "AdaptiveDE_Archive", "description": "Adaptive Differential Evolution with archive, distance-based mutation factor adaptation, and orthogonal crossover to improve exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDE_Archive:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=50, F_init=0.5, CR_init=0.7, restart_prob=0.05, F_adapt_rate=0.1, CR_adapt_rate=0.1, stagnation_threshold=1000):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = np.full(pop_size, F_init)  # Mutation factor for each individual\n        self.CR = np.full(pop_size, CR_init)  # Crossover rate for each individual\n        self.restart_prob = restart_prob\n        self.F_adapt_rate = F_adapt_rate\n        self.CR_adapt_rate = CR_adapt_rate\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.stagnation_threshold = stagnation_threshold\n        self.archive = []\n        self.archive_fitness = []\n\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        if self.fitness[best_idx] < self.f_opt:\n            self.f_opt = self.fitness[best_idx]\n            self.x_opt = self.pop[best_idx]\n\n    def update_archive(self, x, f):\n        if len(self.archive) < self.archive_size:\n            self.archive.append(x)\n            self.archive_fitness.append(f)\n        else:\n            # Replace a random element in the archive\n            idx = np.random.randint(self.archive_size)\n            self.archive[idx] = x\n            self.archive_fitness[idx] = f\n            \n    def distance_based_F(self, x):\n        # Calculate distances to other population members and archive members\n        distances = np.linalg.norm(self.pop - x, axis=1)\n        if len(self.archive) > 0:\n            archive_distances = np.linalg.norm(np.array(self.archive) - x, axis=1)\n            distances = np.concatenate((distances, archive_distances))\n\n        # Use inverse distance to weight F values (closer individuals have more influence)\n        weights = 1.0 / (distances + 1e-6)  # Add a small constant to avoid division by zero\n        \n        # Sample F value based on weights (example: weighted average)\n        if len(self.archive) > 0:\n            all_F = np.concatenate((self.F, np.full(len(self.archive), np.mean(self.F))))\n        else:\n            all_F = self.F\n            \n        F = np.average(all_F, weights=weights)\n        return np.clip(F, 0.1, 1.0)\n    \n    def orthogonal_crossover(self, x_mutated, x_target):\n        # Perform orthogonal crossover\n        x_trial = x_target.copy()\n        \n        # Select two random indices\n        idx1, idx2 = np.random.choice(self.dim, 2, replace=False)\n        \n        # Perform orthogonal array based crossover\n        x_trial[idx1] = x_mutated[idx1]\n        x_trial[idx2] = x_mutated[idx2]\n\n        return x_trial\n\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Mutation\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.pop[idxs]\n            \n            # Adaptive F based on distance\n            F = self.distance_based_F(self.pop[i])\n            x_mutated = x_r1 + F * (x_r2 - x_r3)\n            x_mutated = np.clip(x_mutated, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            #x_trial = self.pop[i].copy()\n            #j_rand = np.random.randint(self.dim)\n            #for j in range(self.dim):\n            #    if np.random.rand() < self.CR[i] or j == j_rand:\n            #        x_trial[j] = x_mutated[j]\n            \n            x_trial = self.orthogonal_crossover(x_mutated, self.pop[i])\n\n\n            # Selection\n            f_trial = func(x_trial)\n            self.eval_count += 1\n\n            if f_trial < self.fitness[i]:\n                # Successful adaptation of CR\n                self.CR[i] = np.clip(self.CR[i] + self.CR_adapt_rate * np.random.normal(0, 0.3), 0.1, 1.0) # Small adaptation\n\n                self.pop[i] = x_trial\n                self.fitness[i] = f_trial\n                \n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = x_trial\n                \n                self.update_archive(x_trial, f_trial)\n            else:\n                # Unsuccessful adaptation of CR\n                self.CR[i] = np.clip(self.CR[i] - self.CR_adapt_rate * np.random.normal(0, 0.3), 0.1, 1.0)  # Reduce CR\n                self.update_archive(self.pop[i], self.fitness[i])\n\n                    \n    def restart_population(self, func):\n        # Restart all the population except the best individual\n        best_idx = np.argmin(self.fitness)\n        \n        new_pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size -1 , self.dim))\n        new_fitness = np.array([func(x) for x in new_pop])\n        self.eval_count += self.pop_size -1\n\n        # Insert the best individual from the previous population\n        temp_pop = np.vstack((self.pop[best_idx], new_pop))\n        temp_fitness = np.hstack((self.fitness[best_idx], new_fitness))\n\n        self.pop = temp_pop\n        self.fitness = temp_fitness\n        \n        best_idx = np.argmin(self.fitness)\n        if self.fitness[best_idx] < self.f_opt:\n            self.f_opt = self.fitness[best_idx]\n            self.x_opt = self.pop[best_idx]\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.archive = []\n        self.archive_fitness = []\n\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n\n            # Stagnation check\n            if self.f_opt < self.previous_best_fitness:\n                self.stagnation_counter = 0\n                self.previous_best_fitness = self.f_opt\n            else:\n                self.stagnation_counter += self.pop_size # Increment by population size each generation\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.restart_population(func)\n                self.stagnation_counter = 0 # Reset stagnation counter\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDE_Archive scored 0.361 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2e6d1785-a297-40a4-aaa5-56041c800eed"], "operator": null, "metadata": {"aucs": [0.1449041395492886, 0.2151261582867744, 0.4511586756879693, 0.37804698954949145, 0.5191345435591802, 0.6614163561368996, 0.3405664615519547, 0.4105002643598975, 0.5110295070658355, 0.3433114339631449, 0]}}
{"id": "b12bc78c-f5aa-4acb-b4bd-919d8412592c", "fitness": 0.7288068786786217, "name": "AdaptiveDE_CMA", "description": "Adaptive Differential Evolution with archive, rank-based selection, and covariance matrix adaptation for improved exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDE_CMA:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_init=0.5, CR_init=0.7, restart_prob=0.05, F_adapt_rate=0.1, CR_adapt_rate=0.1, stagnation_threshold=1000, archive_size=10, cma_learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = np.full(pop_size, F_init)  # Mutation factor for each individual\n        self.CR = np.full(pop_size, CR_init)  # Crossover rate for each individual\n        self.restart_prob = restart_prob\n        self.F_adapt_rate = F_adapt_rate\n        self.CR_adapt_rate = CR_adapt_rate\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.stagnation_threshold = stagnation_threshold\n        self.archive_size = archive_size\n        self.archive = []\n        self.archive_fitness = []\n        self.cma_learning_rate = cma_learning_rate\n        self.C = np.eye(dim) # Covariance matrix for CMA-ES-like adaptation\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        if self.fitness[best_idx] < self.f_opt:\n            self.f_opt = self.fitness[best_idx]\n            self.x_opt = self.pop[best_idx]\n\n    def update_archive(self, x, f):\n        if len(self.archive) < self.archive_size:\n            self.archive.append(x)\n            self.archive_fitness.append(f)\n        else:\n            # Replace the worst member in the archive\n            worst_idx = np.argmax(self.archive_fitness)\n            if f < self.archive_fitness[worst_idx]:\n                self.archive[worst_idx] = x\n                self.archive_fitness[worst_idx] = f\n\n    def evolve(self, func):\n        ranked_indices = np.argsort(self.fitness)\n        \n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Mutation\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.pop[idxs]\n            \n            # Rank-based selection of x_r1: Prefer better individuals\n            rank_index = ranked_indices[np.random.randint(0, self.pop_size // 2)] # Choose from the top half\n            x_r1 = self.pop[rank_index]\n\n            # Use CMA-ES-like sampling\n            z = np.random.multivariate_normal(np.zeros(self.dim), self.C)\n            x_mutated = x_r1 + self.F[i] * (x_r2 - x_r3) + np.sqrt(self.F[i]) * z # Add CMA-ES-like exploration\n\n            x_mutated = np.clip(x_mutated, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            x_trial = self.pop[i].copy()\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.CR[i] or j == j_rand:\n                    x_trial[j] = x_mutated[j]\n\n            # Selection\n            f_trial = func(x_trial)\n            self.eval_count += 1\n\n            if f_trial < self.fitness[i]:\n                # Successful adaptation of F and CR\n                self.F[i] = np.clip(self.F[i] + self.F_adapt_rate * np.random.normal(0, 0.3), 0.1, 1.0) # Small adaptation\n                self.CR[i] = np.clip(self.CR[i] + self.CR_adapt_rate * np.random.normal(0, 0.3), 0.1, 1.0) # Small adaptation\n\n                self.pop[i] = x_trial\n                self.fitness[i] = f_trial\n                \n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = x_trial\n\n                # Update CMA-ES covariance matrix\n                diff = x_trial - self.pop[i]\n                self.C = (1 - self.cma_learning_rate) * self.C + self.cma_learning_rate * np.outer(diff, diff)\n\n            else:\n                # Unsuccessful adaptation of F and CR\n                self.F[i] = np.clip(self.F[i] - self.F_adapt_rate * np.random.normal(0, 0.3), 0.1, 1.0)  # Reduce F\n                self.CR[i] = np.clip(self.CR[i] - self.CR_adapt_rate * np.random.normal(0, 0.3), 0.1, 1.0)  # Reduce CR\n                \n            self.update_archive(self.pop[i], self.fitness[i])\n\n\n    def restart_population(self, func):\n        # Restart all the population except the best individual\n        best_idx = np.argmin(self.fitness)\n        \n        new_pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size -1 , self.dim))\n        new_fitness = np.array([func(x) for x in new_pop])\n        self.eval_count += self.pop_size -1\n\n        # Insert the best individual from the previous population\n        temp_pop = np.vstack((self.pop[best_idx], new_pop))\n        temp_fitness = np.hstack((self.fitness[best_idx], new_fitness))\n\n        self.pop = temp_pop\n        self.fitness = temp_fitness\n        \n        best_idx = np.argmin(self.fitness)\n        if self.fitness[best_idx] < self.f_opt:\n            self.f_opt = self.fitness[best_idx]\n            self.x_opt = self.pop[best_idx]\n            \n        # Reset CMA-ES covariance matrix upon restart\n        self.C = np.eye(self.dim)\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.archive = []\n        self.archive_fitness = []\n        self.C = np.eye(self.dim) # Reset CMA-ES covariance matrix\n\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n\n            # Stagnation check\n            if self.f_opt < self.previous_best_fitness:\n                self.stagnation_counter = 0\n                self.previous_best_fitness = self.f_opt\n            else:\n                self.stagnation_counter += self.pop_size # Increment by population size each generation\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.restart_population(func)\n                self.stagnation_counter = 0 # Reset stagnation counter\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDE_CMA scored 0.729 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2e6d1785-a297-40a4-aaa5-56041c800eed"], "operator": null, "metadata": {"aucs": [0.34016880887431333, 0.6376079715455354, 0.7702286000775569, 0.90049459115656, 0.8121274198105435, 0.8356265512504767, 0.7491843008928134, 0.7345730453145831, 0.7995967437810502, 0.7620649595396698, 0.8656032825888036, 0.9981411766950274, 0.469923819296752, 0.8091636156149788, 0.9286297177393623, 0.8501676708542454, 0.6850591637053284, 0.8625779190251335, 0.23479482242970684, 0.530403393379991]}}
{"id": "27e8fea7-7b12-45f6-b239-2db790b1531b", "fitness": -Infinity, "name": "AdaptiveDE_CMA_Niching", "description": "Adaptive Differential Evolution with covariance matrix adaptation, niching to maintain diversity, and adaptive population sizing.", "code": "import numpy as np\n\nclass AdaptiveDE_CMA_Niching:\n    def __init__(self, budget=10000, dim=10, pop_size_init=50, pop_size_min=20, pop_size_max=100, F_init=0.5, CR_init=0.7, restart_prob=0.05, F_adapt_rate=0.1, CR_adapt_rate=0.1, stagnation_threshold=1000, archive_size=10, cma_learning_rate=0.1, niche_radius=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_init = pop_size_init\n        self.pop_size_min = pop_size_min\n        self.pop_size_max = pop_size_max\n        self.pop_size = pop_size_init\n        self.F = np.full(self.pop_size, F_init)  # Mutation factor for each individual\n        self.CR = np.full(self.pop_size, CR_init)  # Crossover rate for each individual\n        self.restart_prob = restart_prob\n        self.F_adapt_rate = F_adapt_rate\n        self.CR_adapt_rate = CR_adapt_rate\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.stagnation_threshold = stagnation_threshold\n        self.archive_size = archive_size\n        self.archive = []\n        self.archive_fitness = []\n        self.cma_learning_rate = cma_learning_rate\n        self.C = np.eye(dim) # Covariance matrix for CMA-ES-like adaptation\n        self.niche_radius = niche_radius\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        if self.fitness[best_idx] < self.f_opt:\n            self.f_opt = self.fitness[best_idx]\n            self.x_opt = self.pop[best_idx]\n\n    def update_archive(self, x, f):\n        if len(self.archive) < self.archive_size:\n            self.archive.append(x)\n            self.archive_fitness.append(f)\n        else:\n            # Replace the worst member in the archive\n            worst_idx = np.argmax(self.archive_fitness)\n            if f < self.archive_fitness[worst_idx]:\n                self.archive[worst_idx] = x\n                self.archive_fitness[worst_idx] = f\n\n    def calculate_crowding_distance(self):\n        distances = np.zeros(self.pop_size)\n        for i in range(self.pop_size):\n            for j in range(self.pop_size):\n                if i != j:\n                    distance = np.linalg.norm(self.pop[i] - self.pop[j])\n                    if distance < self.niche_radius:\n                        distances[i] += 1  # Count neighbors within the niche radius\n        return distances\n\n    def evolve(self, func):\n        crowding_distances = self.calculate_crowding_distance()\n        ranked_indices = np.argsort(self.fitness + crowding_distances * 0.001) # Promote diversity\n\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Mutation\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.pop[idxs]\n            \n            # Rank-based selection of x_r1: Prefer better individuals\n            rank_index = ranked_indices[np.random.randint(0, self.pop_size // 2)] # Choose from the top half\n            x_r1 = self.pop[rank_index]\n\n            # Use CMA-ES-like sampling\n            z = np.random.multivariate_normal(np.zeros(self.dim), self.C)\n            x_mutated = x_r1 + self.F[i] * (x_r2 - x_r3) + np.sqrt(self.F[i]) * z # Add CMA-ES-like exploration\n\n            x_mutated = np.clip(x_mutated, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            x_trial = self.pop[i].copy()\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.CR[i] or j == j_rand:\n                    x_trial[j] = x_mutated[j]\n\n            # Selection\n            f_trial = func(x_trial)\n            self.eval_count += 1\n\n            if f_trial < self.fitness[i]:\n                # Successful adaptation of F and CR\n                self.F[i] = np.clip(self.F[i] + self.F_adapt_rate * np.random.normal(0, 0.3), 0.1, 1.0) # Small adaptation\n                self.CR[i] = np.clip(self.CR[i] + self.CR_adapt_rate * np.random.normal(0, 0.3), 0.1, 1.0) # Small adaptation\n\n                self.pop[i] = x_trial\n                self.fitness[i] = f_trial\n                \n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = x_trial\n\n                # Update CMA-ES covariance matrix\n                diff = x_trial - self.pop[i]\n                self.C = (1 - self.cma_learning_rate) * self.C + self.cma_learning_rate * np.outer(diff, diff)\n\n            else:\n                # Unsuccessful adaptation of F and CR\n                self.F[i] = np.clip(self.F[i] - self.F_adapt_rate * np.random.normal(0, 0.3), 0.1, 1.0)  # Reduce F\n                self.CR[i] = np.clip(self.CR[i] - self.CR_adapt_rate * np.random.normal(0, 0.3), 0.1, 1.0)  # Reduce CR\n                \n            self.update_archive(self.pop[i], self.fitness[i])\n\n\n    def restart_population(self, func):\n        # Restart all the population except the best individual\n        best_idx = np.argmin(self.fitness)\n        \n        new_pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size -1 , self.dim))\n        new_fitness = np.array([func(x) for x in new_pop])\n        self.eval_count += self.pop_size -1\n\n        # Insert the best individual from the previous population\n        temp_pop = np.vstack((self.pop[best_idx], new_pop))\n        temp_fitness = np.hstack((self.fitness[best_idx], new_fitness))\n\n        self.pop = temp_pop\n        self.fitness = temp_fitness\n        \n        best_idx = np.argmin(self.fitness)\n        if self.fitness[best_idx] < self.f_opt:\n            self.f_opt = self.fitness[best_idx]\n            self.x_opt = self.pop[best_idx]\n            \n        # Reset CMA-ES covariance matrix upon restart\n        self.C = np.eye(self.dim)\n\n    def adapt_population_size(self):\n        crowding_distances = self.calculate_crowding_distance()\n        avg_crowding = np.mean(crowding_distances)\n\n        if avg_crowding > 2:  # High crowding, reduce population\n            self.pop_size = max(self.pop_size - 5, self.pop_size_min)\n        elif avg_crowding < 1:  # Low crowding, increase population\n            self.pop_size = min(self.pop_size + 5, self.pop_size_max)\n        \n        #Ensure population size doesn't drop to 0\n        self.pop_size = max(1, self.pop_size)\n        \n        # Resize F and CR arrays\n        self.F = np.resize(self.F, self.pop_size)\n        self.CR = np.resize(self.CR, self.pop_size)\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.archive = []\n        self.archive_fitness = []\n        self.C = np.eye(self.dim) # Reset CMA-ES covariance matrix\n        self.pop_size = self.pop_size_init # Reset population size\n\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n            self.adapt_population_size()\n\n            # Stagnation check\n            if self.f_opt < self.previous_best_fitness:\n                self.stagnation_counter = 0\n                self.previous_best_fitness = self.f_opt\n            else:\n                self.stagnation_counter += self.pop_size # Increment by population size each generation\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.restart_population(func)\n                self.stagnation_counter = 0 # Reset stagnation counter\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 144, in evaluate\n    algorithm(f_new)\n  File \"<string>\", line 172, in __call__\n  File \"<string>\", line 63, in evolve\n  File \"<string>\", line 57, in calculate_crowding_distance\nIndexError: index 50 is out of bounds for axis 0 with size 50\n.", "error": "", "parent_ids": ["b12bc78c-f5aa-4acb-b4bd-919d8412592c"], "operator": null, "metadata": {}}
{"id": "a9641b97-c578-4c35-9f15-95dbcb9c23fd", "fitness": -Infinity, "name": "OrthogonalCMAES", "description": "Introducing orthogonal sampling within CMA-ES to improve diversity and exploration, especially in higher dimensions.", "code": "import numpy as np\n\nclass OrthogonalCMAES:\n    def __init__(self, budget=10000, dim=10, sigma0=0.5, stagnation_threshold=10, orthogonal_sampling_rate=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.stagnation_threshold = stagnation_threshold  # Number of generations without improvement\n        self.stagnation_counter = 0\n        self.best_fitness = np.inf\n        self.orthogonal_sampling_rate = orthogonal_sampling_rate # Probability of using orthogonal sampling\n\n        self.popsize = 4 + int(np.floor(3 * np.log(self.dim)))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n    def __orthogonal_sample(self, n, dim):\n        \"\"\"\n        Generate an orthogonal sample set.\n        \"\"\"\n        H = np.eye(dim)\n        for i in range(n):\n            v = np.random.randn(dim - i)\n            alpha = 2 / (v @ v)\n            H_sub = np.eye(dim - i) - alpha * np.outer(v, v)\n            H[:dim - i, :dim - i] = H[:dim - i, :dim - i] @ H_sub\n        return H[:n, :]\n\n\n    def __call__(self, func):\n        # Initialize variables\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)  # Covariance matrix\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n        # Parameters (using common defaults)\n        c_sigma = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        c_c = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        c_1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        c_mu = min(1 - c_1, 2 * (self.mueff - 1 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        d_sigma = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + c_sigma\n        c_1a = c_1\n        c_mua = c_mu\n\n        # Eigen decomposition of C (expensive, do it rarely)\n        C_evals, C_evecs = np.linalg.eigh(C)\n        C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n        C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n        \n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n\n        while eval_count < self.budget:\n            # Sample population\n            if np.random.rand() < self.orthogonal_sampling_rate:\n                # Orthogonal sampling\n                z = self.__orthogonal_sample(self.popsize, self.dim)\n                z = np.random.normal(0, 1, size=(self.popsize, self.dim)) # revert back to original normal sampling\n            else:\n                # Standard CMA-ES sampling\n                z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            \n            x = mean + sigma * (C_sqrt @ z.T).T\n            \n            # Evaluate the new points\n            f = np.array([func(xi) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n            eval_count += len(x) \n\n            # Sort by fitness\n            idx = np.argsort(f)\n            x = x[idx]\n            z = z[idx]\n            f = f[idx]\n\n            # Update optimal solution\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[0]\n                self.stagnation_counter = 0\n            else:\n                self.stagnation_counter += 1\n                \n            # Stagnation check and restart\n            if self.stagnation_counter > self.stagnation_threshold:\n                # Restart strategy: reset mean and increase sigma\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                sigma = self.sigma0 * 2  # Increase sigma to explore more\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                self.stagnation_counter = 0  # Reset counter\n                print(\"Restarting CMA-ES due to stagnation\")\n\n            # Selection and recombination\n            x_mu = x[:self.mu]\n            z_mu = z[:self.mu]\n\n            mean_new = np.sum(x_mu * self.weights[:,None], axis=0)\n            z_w = np.sum(z_mu * self.weights[:,None], axis=0)\n            \n            # Covariance matrix adaptation\n            ps = (1 - c_sigma) * ps + np.sqrt(c_sigma * (2 - c_sigma) * self.mueff) * (C_invsqrt @ (mean_new - mean) / sigma)\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - c_sigma)**(2 * eval_count / self.popsize)) < chiN * (1.4 + 2 / (self.dim + 1))\n            \n            pc = (1 - c_c) * pc + hsig * np.sqrt(c_c * (2 - c_c) * self.mueff) * (mean_new - mean) / sigma\n\n            C = (1 - c_1 - c_mu) * C + c_1 * (pc[:, None] @ pc[None, :])\n            for i in range(self.mu):\n                C += c_mu * self.weights[i] * (z_mu[i, :, None] @ z_mu[i, None, :])\n                \n            # Active CMA\n            if c_1a > 0 and c_mua > 0:\n                negidx = np.where(self.weights < 0)[0]\n                znorm = np.zeros((len(negidx), self.dim))\n                for i, idx in enumerate(negidx):\n                    znorm[i] = z_mu[idx] * np.sqrt(-self.weights[idx])\n                C += c_1a * (1/np.linalg.norm(ps)**2 + c_mua) * np.sum(znorm.T @ znorm, axis=1)\n\n            # Update step size\n            sigma *= np.exp((c_sigma / d_sigma) * (np.linalg.norm(ps) / chiN - 1))\n\n            # Adaptive sigma scaling (if not improving, reduce sigma)\n            if f[0] >= self.best_fitness:\n                sigma *= 0.95  # Reduce step size\n            else:\n                self.best_fitness = f[0]\n            \n            # Update mean\n            mean = mean_new\n\n            # Eigen decomposition of C\n            if eval_count % (self.popsize * 5) == 0:  # Re-compute after every 5 generations\n                C = np.triu(C) + np.triu(C, 1).T  # Enforce symmetry\n                try:\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_evals = np.maximum(C_evals, 1e-10) # Avoid zero or negative eigenvalues\n                    C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                except np.linalg.LinAlgError:\n                    print(\"LinAlgError encountered, resetting C\")\n                    C = np.eye(self.dim)\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                \n            if np.any(np.isnan(mean)) or np.any(np.isnan(C)):\n                print(\"NaN detected, resetting...\")\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                sigma = self.sigma0\n        \n        return f_opt, x_opt", "configspace": "", "generation": 4, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 63, in __call__\n  File \"<string>\", line 25, in __orthogonal_sample\n  File \"mtrand.pyx\", line 1270, in numpy.random.mtrand.RandomState.randn\n  File \"mtrand.pyx\", line 1431, in numpy.random.mtrand.RandomState.standard_normal\n  File \"_common.pyx\", line 636, in numpy.random._common.cont\nValueError: negative dimensions are not allowed\n.", "error": "", "parent_ids": ["8d396969-ba5a-47d4-8075-374586c711f9"], "operator": null, "metadata": {}}
{"id": "3848f4f9-f671-42a5-8099-8a33e9b35a3d", "fitness": -Infinity, "name": "EnhancedCMAES", "description": "Enhanced CMA-ES with adaptive population size based on problem dimensionality and budget, and improved restart strategy with memory of past good solutions.", "code": "import numpy as np\n\nclass EnhancedCMAES:\n    def __init__(self, budget=10000, dim=10, sigma0=0.5, stagnation_threshold=10, restart_multiple=2):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.stagnation_threshold = stagnation_threshold  # Number of generations without improvement\n        self.stagnation_counter = 0\n        self.best_fitness = np.inf\n        self.restart_multiple = restart_multiple\n        self.population_scaling = min(1.0, budget / (dim * 1e4))  # scale population size based on budget\n        self.popsize = max(4, int(np.floor((4 + 3 * np.log(self.dim)) * self.population_scaling)))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.archive_size = int(self.budget/1000)\n        self.archive_x = []\n        self.archive_f = []\n\n    def __call__(self, func):\n        # Initialize variables\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)  # Covariance matrix\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n        # Parameters (using common defaults)\n        c_sigma = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        c_c = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        c_1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        c_mu = min(1 - c_1, 2 * (self.mueff - 1 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        d_sigma = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + c_sigma\n        c_1a = c_1\n        c_mua = c_mu\n\n        # Eigen decomposition of C (expensive, do it rarely)\n        C_evals, C_evecs = np.linalg.eigh(C)\n        C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n        C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n        \n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n\n        while eval_count < self.budget:\n            # Sample population\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            x = mean + sigma * (C_sqrt @ z.T).T\n            \n            # Evaluate the new points\n            f = np.array([func(xi) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n            eval_count += len(x) \n\n            # Sort by fitness\n            idx = np.argsort(f)\n            x = x[idx]\n            z = z[idx]\n            f = f[idx]\n\n            # Update optimal solution\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[0]\n                self.stagnation_counter = 0\n            else:\n                self.stagnation_counter += 1\n\n            # Archive best solutions\n            for xi, fi in zip(x, f):\n                if len(self.archive_x) < self.archive_size:\n                    self.archive_x.append(xi)\n                    self.archive_f.append(fi)\n                else:\n                    max_archive_f = np.max(self.archive_f)\n                    if fi < max_archive_f:\n                        max_idx = np.argmax(self.archive_f)\n                        self.archive_x[max_idx] = xi\n                        self.archive_f[max_idx] = fi\n                \n            # Stagnation check and restart\n            if self.stagnation_counter > self.stagnation_threshold:\n                # Restart strategy: reset mean and increase sigma, and leverage archive\n                if len(self.archive_x) > 0:\n                    # Select a random solution from the archive as the new mean\n                    idx = np.random.randint(len(self.archive_x))\n                    mean = self.archive_x[idx]  # Use archive for informed restart\n                    print(\"Restarting CMA-ES from archive\")\n                else:\n                    mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                    print(\"Restarting CMA-ES randomly\")\n                sigma = self.sigma0 * self.restart_multiple  # Increase sigma to explore more\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                self.stagnation_counter = 0  # Reset counter\n                self.restart_multiple *= 1.2\n                if self.restart_multiple > 5:\n                  self.restart_multiple = 2\n\n            # Selection and recombination\n            x_mu = x[:self.mu]\n            z_mu = z[:self.mu]\n\n            mean_new = np.sum(x_mu * self.weights[:,None], axis=0)\n            z_w = np.sum(z_mu * self.weights[:,None], axis=0)\n            \n            # Covariance matrix adaptation\n            ps = (1 - c_sigma) * ps + np.sqrt(c_sigma * (2 - c_sigma) * self.mueff) * (C_invsqrt @ (mean_new - mean) / sigma)\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - c_sigma)**(2 * eval_count / self.popsize)) < chiN * (1.4 + 2 / (self.dim + 1))\n            \n            pc = (1 - c_c) * pc + hsig * np.sqrt(c_c * (2 - c_c) * self.mueff) * (mean_new - mean) / sigma\n\n            C = (1 - c_1 - c_mu) * C + c_1 * (pc[:, None] @ pc[None, :])\n            for i in range(self.mu):\n                C += c_mu * self.weights[i] * (z_mu[i, :, None] @ z_mu[i, None, :])\n                \n            # Active CMA\n            if c_1a > 0 and c_mua > 0:\n                negidx = np.where(self.weights < 0)[0]\n                znorm = np.zeros((len(negidx), self.dim))\n                for i, idx in enumerate(negidx):\n                    znorm[i] = z_mu[idx] * np.sqrt(-self.weights[idx])\n                C += c_1a * (1/np.linalg.norm(ps)**2 + c_mua) * np.sum(znorm.T @ znorm, axis=1)\n\n            # Update step size\n            sigma *= np.exp((c_sigma / d_sigma) * (np.linalg.norm(ps) / chiN - 1))\n\n            # Adaptive sigma scaling (if not improving, reduce sigma)\n            if f[0] >= self.best_fitness:\n                sigma *= 0.95  # Reduce step size\n            else:\n                self.best_fitness = f[0]\n            \n            # Update mean\n            mean = mean_new\n\n            # Eigen decomposition of C\n            if eval_count % (self.popsize * 5) == 0:  # Re-compute after every 5 generations\n                C = np.triu(C) + np.triu(C, 1).T  # Enforce symmetry\n                try:\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_evals = np.maximum(C_evals, 1e-10) # Avoid zero or negative eigenvalues\n                    C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                except np.linalg.LinAlgError:\n                    print(\"LinAlgError encountered, resetting C\")\n                    C = np.eye(self.dim)\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                \n            if np.any(np.isnan(mean)) or np.any(np.isnan(C)):\n                print(\"NaN detected, resetting...\")\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                sigma = self.sigma0\n        \n        return f_opt, x_opt", "configspace": "", "generation": 4, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 78, in __call__\n  File \"<__array_function__ internals>\", line 200, in amax\n  File \"/home/neocortex/.cache/pypoetry/virtualenvs/blade-xXF9vABH-py3.10/lib/python3.10/site-packages/numpy/core/fromnumeric.py\", line 2820, in amax\n    return _wrapreduction(a, np.maximum, 'max', axis, None, out,\n  File \"/home/neocortex/.cache/pypoetry/virtualenvs/blade-xXF9vABH-py3.10/lib/python3.10/site-packages/numpy/core/fromnumeric.py\", line 86, in _wrapreduction\n    return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\nValueError: zero-size array to reduction operation maximum which has no identity\n.", "error": "", "parent_ids": ["8d396969-ba5a-47d4-8075-374586c711f9"], "operator": null, "metadata": {}}
{"id": "14c9529d-c79a-4a90-bf93-b1c05656aa4a", "fitness": -Infinity, "name": "AdaptiveDE_CMA_OL", "description": "Adaptive Differential Evolution with orthogonal learning, covariance matrix adaptation and adaptive population sizing to enhance exploration, exploitation, and convergence speed.", "code": "import numpy as np\n\nclass AdaptiveDE_CMA_OL:\n    def __init__(self, budget=10000, dim=10, pop_size_init=50, pop_size_min=10, pop_size_max=100, F_init=0.5, CR_init=0.7, restart_prob=0.05, F_adapt_rate=0.1, CR_adapt_rate=0.1, stagnation_threshold=1000, archive_size=10, cma_learning_rate=0.1, orthogonal_learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_init = pop_size_init\n        self.pop_size_min = pop_size_min\n        self.pop_size_max = pop_size_max\n        self.pop_size = pop_size_init\n        self.F = np.full(pop_size_init, F_init)  # Mutation factor for each individual\n        self.CR = np.full(pop_size_init, CR_init)  # Crossover rate for each individual\n        self.restart_prob = restart_prob\n        self.F_adapt_rate = F_adapt_rate\n        self.CR_adapt_rate = CR_adapt_rate\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.stagnation_threshold = stagnation_threshold\n        self.archive_size = archive_size\n        self.archive = []\n        self.archive_fitness = []\n        self.cma_learning_rate = cma_learning_rate\n        self.C = np.eye(dim) # Covariance matrix for CMA-ES-like adaptation\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        if self.fitness[best_idx] < self.f_opt:\n            self.f_opt = self.fitness[best_idx]\n            self.x_opt = self.pop[best_idx]\n\n    def update_archive(self, x, f):\n        if len(self.archive) < self.archive_size:\n            self.archive.append(x)\n            self.archive_fitness.append(f)\n        else:\n            # Replace the worst member in the archive\n            worst_idx = np.argmax(self.archive_fitness)\n            if f < self.archive_fitness[worst_idx]:\n                self.archive[worst_idx] = x\n                self.archive_fitness[worst_idx] = f\n\n    def orthogonal_learning(self, func, x_current):\n        # Generate orthogonal array\n        orthogonal_matrix = self.generate_orthogonal_array(self.dim)\n        \n        trial_points = []\n        for row in orthogonal_matrix:\n            x_trial = x_current.copy()\n            for j in range(self.dim):\n                # Perturb each dimension based on the orthogonal array\n                perturbation = (row[j] - 0.5) * self.orthogonal_learning_rate  # Scale perturbation\n                x_trial[j] = x_current[j] + perturbation\n                x_trial[j] = np.clip(x_trial[j], func.bounds.lb, func.bounds.ub)  # Clip to bounds\n            trial_points.append(x_trial)\n            \n        # Evaluate trial points\n        fitness_values = [func(x) for x in trial_points]\n        self.eval_count += len(trial_points)\n        \n        # Select the best point\n        best_idx = np.argmin(fitness_values)\n        x_best_orthogonal = trial_points[best_idx]\n        f_best_orthogonal = fitness_values[best_idx]\n        \n        return x_best_orthogonal, f_best_orthogonal\n\n    def generate_orthogonal_array(self, dim):\n        # A simple method to generate an orthogonal array.  Can be improved.\n        return np.random.randint(0, 2, size=(dim + 1, dim))\n\n    def evolve(self, func):\n        ranked_indices = np.argsort(self.fitness)\n        \n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Mutation\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.pop[idxs]\n            \n            # Rank-based selection of x_r1: Prefer better individuals\n            rank_index = ranked_indices[np.random.randint(0, self.pop_size // 2)] # Choose from the top half\n            x_r1 = self.pop[rank_index]\n\n            # Use CMA-ES-like sampling\n            z = np.random.multivariate_normal(np.zeros(self.dim), self.C)\n            x_mutated = x_r1 + self.F[i] * (x_r2 - x_r3) + np.sqrt(self.F[i]) * z # Add CMA-ES-like exploration\n\n            x_mutated = np.clip(x_mutated, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            x_trial = self.pop[i].copy()\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.CR[i] or j == j_rand:\n                    x_trial[j] = x_mutated[j]\n\n            # Selection\n            f_trial = func(x_trial)\n            self.eval_count += 1\n\n            # Orthogonal Learning\n            x_orthogonal, f_orthogonal = self.orthogonal_learning(func, x_trial)\n\n            if f_orthogonal < f_trial:\n                x_trial = x_orthogonal\n                f_trial = f_orthogonal\n\n            if f_trial < self.fitness[i]:\n                # Successful adaptation of F and CR\n                self.F[i] = np.clip(self.F[i] + self.F_adapt_rate * np.random.normal(0, 0.3), 0.1, 1.0) # Small adaptation\n                self.CR[i] = np.clip(self.CR[i] + self.CR_adapt_rate * np.random.normal(0, 0.3), 0.1, 1.0) # Small adaptation\n\n                self.pop[i] = x_trial\n                self.fitness[i] = f_trial\n                \n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = x_trial\n\n                # Update CMA-ES covariance matrix\n                diff = x_trial - self.pop[i]\n                self.C = (1 - self.cma_learning_rate) * self.C + self.cma_learning_rate * np.outer(diff, diff)\n\n            else:\n                # Unsuccessful adaptation of F and CR\n                self.F[i] = np.clip(self.F[i] - self.F_adapt_rate * np.random.normal(0, 0.3), 0.1, 1.0)  # Reduce F\n                self.CR[i] = np.clip(self.CR[i] - self.CR_adapt_rate * np.random.normal(0, 0.3), 0.1, 1.0)  # Reduce CR\n                \n            self.update_archive(self.pop[i], self.fitness[i])\n\n\n    def restart_population(self, func):\n        # Restart all the population except the best individual\n        best_idx = np.argmin(self.fitness)\n        \n        new_pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size -1 , self.dim))\n        new_fitness = np.array([func(x) for x in new_pop])\n        self.eval_count += self.pop_size -1\n\n        # Insert the best individual from the previous population\n        temp_pop = np.vstack((self.pop[best_idx], new_pop))\n        temp_fitness = np.hstack((self.fitness[best_idx], new_fitness))\n\n        self.pop = temp_pop\n        self.fitness = temp_fitness\n        \n        best_idx = np.argmin(self.fitness)\n        if self.fitness[best_idx] < self.f_opt:\n            self.f_opt = self.fitness[best_idx]\n            self.x_opt = self.pop[best_idx]\n            \n        # Reset CMA-ES covariance matrix upon restart\n        self.C = np.eye(self.dim)\n\n    def adapt_population_size(self):\n        # Adapt population size based on stagnation\n        if self.stagnation_counter > self.stagnation_threshold / 2:\n            self.pop_size = max(self.pop_size - 5, self.pop_size_min)  # Reduce pop size\n        else:\n            self.pop_size = min(self.pop_size + 2, self.pop_size_max)  # Increase pop size\n\n        # Ensure F and CR are properly sized\n        if len(self.F) != self.pop_size:\n            self.F = np.full(self.pop_size, np.mean(self.F))\n        if len(self.CR) != self.pop_size:\n            self.CR = np.full(self.pop_size, np.mean(self.CR))\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.Inf\n        self.archive = []\n        self.archive_fitness = []\n        self.C = np.eye(self.dim) # Reset CMA-ES covariance matrix\n        self.pop_size = self.pop_size_init # Reset population size\n\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n\n            # Stagnation check\n            if self.f_opt < self.previous_best_fitness:\n                self.stagnation_counter = 0\n                self.previous_best_fitness = self.f_opt\n            else:\n                self.stagnation_counter += self.pop_size # Increment by population size each generation\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.restart_population(func)\n                self.stagnation_counter = 0 # Reset stagnation counter\n\n            self.adapt_population_size()\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "An exception occured: TypeError: only size-1 arrays can be converted to Python scalars\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 194, in __call__\n  File \"<string>\", line 114, in evolve\n  File \"<string>\", line 63, in orthogonal_learning\nValueError: setting an array element with a sequence.\n.", "error": "", "parent_ids": ["b12bc78c-f5aa-4acb-b4bd-919d8412592c"], "operator": null, "metadata": {}}
{"id": "01f39612-4640-4647-aada-b9956458ca86", "fitness": 0.7448839797382276, "name": "CMAES_History_Mirrored", "description": "Introducing a mirrored sampling strategy around the mean in CMA-ES to enhance exploration and accelerate convergence by leveraging symmetry.", "code": "import numpy as np\n\nclass CMAES_History_Mirrored:\n    def __init__(self, budget=10000, dim=10, sigma0=0.5, history_length=10, mirrored_sampling=True):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.history_length = history_length  # Length of step history for adaptation\n        self.mirrored_sampling = mirrored_sampling\n\n        self.popsize = 4 + int(np.floor(3 * np.log(self.dim)))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.step_history = []\n\n\n    def __call__(self, func):\n        # Initialize variables\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)  # Covariance matrix\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n        # Parameters (using common defaults)\n        c_sigma = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        c_c = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        c_1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        c_mu = min(1 - c_1, 2 * (self.mueff - 1 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        d_sigma = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + c_sigma\n        c_1a = c_1\n        c_mua = c_mu\n\n        # Eigen decomposition of C (expensive, do it rarely)\n        C_evals, C_evecs = np.linalg.eigh(C)\n        C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n        C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n        \n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n\n        while eval_count < self.budget:\n            # Sample population\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            x = mean + sigma * (C_sqrt @ z.T).T\n\n            # Mirrored sampling\n            if self.mirrored_sampling:\n                x_mirrored = mean - sigma * (C_sqrt @ z.T).T\n                x = np.vstack((x, x_mirrored))\n                z = np.vstack((z, -z)) # also mirror z for covariance update\n            \n            # Clipping to bounds\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            \n            # Evaluate the new points\n            f = np.array([func(xi) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n            eval_count += len(x) \n\n            # Sort by fitness\n            idx = np.argsort(f)\n            x = x[idx]\n            z = z[idx]\n            f = f[idx]\n\n            # Update optimal solution\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[0]\n                \n            # Selection and recombination\n            x_mu = x[:self.mu]\n            z_mu = z[:self.mu]\n\n            mean_new = np.sum(x_mu * self.weights[:,None], axis=0)\n            z_w = np.sum(z_mu * self.weights[:,None], axis=0)\n            \n            # Covariance matrix adaptation\n            ps = (1 - c_sigma) * ps + np.sqrt(c_sigma * (2 - c_sigma) * self.mueff) * (C_invsqrt @ (mean_new - mean) / sigma)\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - c_sigma)**(2 * eval_count / self.popsize)) < chiN * (1.4 + 2 / (self.dim + 1))\n            \n            pc = (1 - c_c) * pc + hsig * np.sqrt(c_c * (2 - c_c) * self.mueff) * (mean_new - mean) / sigma\n\n            C = (1 - c_1 - c_mu) * C + c_1 * (pc[:, None] @ pc[None, :])\n            for i in range(self.mu):\n                C += c_mu * self.weights[i] * (z_mu[i, :, None] @ z_mu[i, None, :])\n                \n            # Active CMA\n            if c_1a > 0 and c_mua > 0:\n                negidx = np.where(self.weights < 0)[0]\n                znorm = np.zeros((len(negidx), self.dim))\n                for i, idx in enumerate(negidx):\n                    znorm[i] = z_mu[idx] * np.sqrt(-self.weights[idx])\n                C += c_1a * (1/np.linalg.norm(ps)**2 + c_mua) * np.sum(znorm.T @ znorm, axis=1)\n\n            # Update step size\n            sigma *= np.exp((c_sigma / d_sigma) * (np.linalg.norm(ps) / chiN - 1))\n\n            # Step history adaptation\n            step = (mean_new - mean) / sigma\n            self.step_history.append(step)\n            if len(self.step_history) > self.history_length:\n                self.step_history.pop(0)\n\n            # Adapt C based on step history\n            for h_step in self.step_history:\n                C += 0.1 * c_mu * (h_step[:, None] @ h_step[None, :])  # Dampened update\n                \n            # Update mean\n            mean = mean_new\n\n            # Eigen decomposition of C\n            if eval_count % (self.popsize * 5) == 0:  # Re-compute after every 5 generations\n                C = np.triu(C) + np.triu(C, 1).T  # Enforce symmetry\n                try:\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_evals = np.maximum(C_evals, 1e-10) # Avoid zero or negative eigenvalues\n                    C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                except np.linalg.LinAlgError:\n                    print(\"LinAlgError encountered, resetting C\")\n                    C = np.eye(self.dim)\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                \n            if np.any(np.isnan(mean)) or np.any(np.isnan(C)):\n                print(\"NaN detected, resetting...\")\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                sigma = self.sigma0\n        \n        return f_opt, x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm CMAES_History_Mirrored scored 0.745 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["99748312-b329-4b52-a9fa-499daf5b1417"], "operator": null, "metadata": {"aucs": [0.2606974807014659, 0.1978436892990295, 0.9059418392038866, 0.9610101704952134, 0.9279275768107208, 0.9287145081184911, 0.8892780147776402, 0.9042725364623214, 0.9154893461540066, 0.913223163766736, 0.9529118737769325, 0.9855481511896692, 0.30360955262001854, 0.9196083180141061, 0.96310148459816, 0.932977790032683, 0.4529244596402975, 0.9505716175491952, 0.1379646313443742, 0.49406339020960355]}}
{"id": "a1430f95-eb82-4ea4-9944-98b7e36ea3f1", "fitness": 0.6479721177475473, "name": "DynamicCMAES", "description": "Enhanced CMA-ES with dynamic population size based on budget and dimension, adaptive restart based on fitness improvement, and a simplified step history for covariance adaptation.", "code": "import numpy as np\n\nclass DynamicCMAES:\n    def __init__(self, budget=10000, dim=10, sigma0=0.5, history_length=5):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.history_length = history_length\n        self.min_popsize = 4\n        self.max_popsize = 50\n        self.target_popsize = min(self.max_popsize, self.min_popsize + int(np.floor(3 * np.log(self.dim))))\n        self.popsize = self.target_popsize\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.step_history = []\n        self.last_f_opt = np.inf\n        self.stagnation_counter = 0\n        self.restart_iterations = 50\n\n    def __call__(self, func):\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n        c_sigma = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        c_c = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        c_1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        c_mu = min(1 - c_1, 2 * (self.mueff - 1 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        d_sigma = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + c_sigma\n\n        C_evals, C_evecs = np.linalg.eigh(C)\n        C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n        C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n        \n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n\n        while eval_count < self.budget:\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            x = mean + sigma * (C_sqrt @ z.T).T\n            \n            f = np.array([func(xi) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n            eval_count += len(x)\n\n            idx = np.argsort(f)\n            x = x[idx]\n            z = z[idx]\n            f = f[idx]\n\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[0]\n                self.stagnation_counter = 0\n            else:\n                self.stagnation_counter += 1\n                \n            if self.stagnation_counter > self.restart_iterations:\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                sigma = self.sigma0\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                self.stagnation_counter = 0\n\n            x_mu = x[:self.mu]\n            z_mu = z[:self.mu]\n\n            mean_new = np.sum(x_mu * self.weights[:, None], axis=0)\n            z_w = np.sum(z_mu * self.weights[:, None], axis=0)\n            \n            ps = (1 - c_sigma) * ps + np.sqrt(c_sigma * (2 - c_sigma) * self.mueff) * (C_invsqrt @ (mean_new - mean) / sigma)\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - c_sigma)**(2 * eval_count / self.popsize)) < chiN * (1.4 + 2 / (self.dim + 1))\n            pc = (1 - c_c) * pc + hsig * np.sqrt(c_c * (2 - c_c) * self.mueff) * (mean_new - mean) / sigma\n\n            C = (1 - c_1 - c_mu) * C + c_1 * (pc[:, None] @ pc[None, :])\n            for i in range(self.mu):\n                C += c_mu * self.weights[i] * (z_mu[i, :, None] @ z_mu[i, None, :])\n            \n            sigma *= np.exp((c_sigma / d_sigma) * (np.linalg.norm(ps) / chiN - 1))\n\n            # Simplified Step history\n            step = (mean_new - mean) / sigma\n            self.step_history.append(step)\n            if len(self.step_history) > self.history_length:\n                self.step_history.pop(0)\n\n            # Adapt C based on step history, simplified\n            for h_step in self.step_history:\n                C += 0.1 * c_mu * (h_step[:, None] @ h_step[None, :])\n\n            mean = mean_new\n            \n            if eval_count % (self.popsize * 5) == 0:\n                C = np.triu(C) + np.triu(C, 1).T\n                try:\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_evals = np.maximum(C_evals, 1e-10)\n                    C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                except np.linalg.LinAlgError:\n                    C = np.eye(self.dim)\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                \n            if np.any(np.isnan(mean)) or np.any(np.isnan(C)):\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                sigma = self.sigma0\n\n        return f_opt, x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm DynamicCMAES scored 0.648 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["99748312-b329-4b52-a9fa-499daf5b1417"], "operator": null, "metadata": {"aucs": [0.44072476342185274, 0.5552394149295273, 0.9307682371743502, 0.3153573819579937, 0.9360658409714409, 0.9545359941306047, 0.3204495599925826, 0.9334629680640188, 0.9461193435227057, 0.21245385551539508, 0.9657398281542132, 0.9892796477935741, 0.2554561225551346, 0.9336334479434751, 0.6380068677903696, 0.48147864325103396, 0.4142448378930892, 0.9618807905319232, 0.2670509570854762, 0.5074938522721826]}}
{"id": "0c04d146-ce73-4054-9913-f7b1b0e17e84", "fitness": 0.617301432740222, "name": "EnhancedCMAES", "description": "Introducing a dynamic population size adjustment and a learning rate annealing schedule for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedCMAES:\n    def __init__(self, budget=10000, dim=10, sigma0=0.5, stagnation_threshold=10, popsize_multiplier=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.stagnation_threshold = stagnation_threshold  # Number of generations without improvement\n        self.stagnation_counter = 0\n        self.best_fitness = np.inf\n        self.popsize_multiplier = popsize_multiplier # multiplier for population size.\n\n        self.popsize = int(self.popsize_multiplier * (4 + int(np.floor(3 * np.log(self.dim)))))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n        self.learning_rate = 1.0 # Initial learning rate\n        self.learning_rate_decay = 0.9995 # Decay factor for learning rate\n\n\n    def __call__(self, func):\n        # Initialize variables\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)  # Covariance matrix\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n        # Parameters (using common defaults)\n        c_sigma = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        c_c = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        c_1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        c_mu = min(1 - c_1, 2 * (self.mueff - 1 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        d_sigma = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + c_sigma\n        c_1a = c_1\n        c_mua = c_mu\n\n        # Eigen decomposition of C (expensive, do it rarely)\n        C_evals, C_evecs = np.linalg.eigh(C)\n        C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n        C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n        \n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n\n        while eval_count < self.budget:\n            # Sample population\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            x = mean + sigma * (C_sqrt @ z.T).T\n            \n            # Evaluate the new points\n            f = np.array([func(xi) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n            eval_count += len(x) \n\n            # Sort by fitness\n            idx = np.argsort(f)\n            x = x[idx]\n            z = z[idx]\n            f = f[idx]\n\n            # Update optimal solution\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[0]\n                self.stagnation_counter = 0\n            else:\n                self.stagnation_counter += 1\n                \n            # Stagnation check and restart\n            if self.stagnation_counter > self.stagnation_threshold:\n                # Restart strategy: reset mean and increase sigma\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                sigma = self.sigma0 * 2  # Increase sigma to explore more\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                self.stagnation_counter = 0  # Reset counter\n                # Adjust popsize on restart\n                self.popsize = int(self.popsize_multiplier * (4 + int(np.floor(3 * np.log(self.dim)))))\n                self.mu = self.popsize // 2\n                self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n                self.weights = self.weights / np.sum(self.weights)\n                self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n                print(\"Restarting CMA-ES due to stagnation\")\n\n            # Selection and recombination\n            x_mu = x[:self.mu]\n            z_mu = z[:self.mu]\n\n            mean_new = np.sum(x_mu * self.weights[:,None], axis=0)\n            z_w = np.sum(z_mu * self.weights[:,None], axis=0)\n            \n            # Covariance matrix adaptation\n            ps = (1 - c_sigma) * ps + np.sqrt(c_sigma * (2 - c_sigma) * self.mueff) * (C_invsqrt @ (mean_new - mean) / sigma)\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - c_sigma)**(2 * eval_count / self.popsize)) < chiN * (1.4 + 2 / (self.dim + 1))\n            \n            pc = (1 - c_c) * pc + hsig * np.sqrt(c_c * (2 - c_c) * self.mueff) * (mean_new - mean) / sigma\n\n            C = (1 - c_1 - c_mu) * C + c_1 * (pc[:, None] @ pc[None, :])\n            for i in range(self.mu):\n                C += c_mu * self.weights[i] * (z_mu[i, :, None] @ z_mu[i, None, :])\n                \n            # Active CMA\n            if c_1a > 0 and c_mua > 0:\n                negidx = np.where(self.weights < 0)[0]\n                znorm = np.zeros((len(negidx), self.dim))\n                for i, idx in enumerate(negidx):\n                    znorm[i] = z_mu[idx] * np.sqrt(-self.weights[idx])\n                C += c_1a * (1/np.linalg.norm(ps)**2 + c_mua) * np.sum(znorm.T @ znorm, axis=1)\n\n            # Update step size\n            sigma *= np.exp((c_sigma / d_sigma) * (np.linalg.norm(ps) / chiN - 1))\n            \n            # Adaptive learning rate for sigma scaling\n            if f[0] >= self.best_fitness:\n                sigma *= (0.95 * self.learning_rate)  # Reduce step size\n            else:\n                self.best_fitness = f[0]\n\n            # Decay learning rate\n            self.learning_rate *= self.learning_rate_decay\n            \n            # Update mean\n            mean = mean_new\n\n            # Eigen decomposition of C\n            if eval_count % (self.popsize * 5) == 0:  # Re-compute after every 5 generations\n                C = np.triu(C) + np.triu(C, 1).T  # Enforce symmetry\n                try:\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_evals = np.maximum(C_evals, 1e-10) # Avoid zero or negative eigenvalues\n                    C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                except np.linalg.LinAlgError:\n                    print(\"LinAlgError encountered, resetting C\")\n                    C = np.eye(self.dim)\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                \n            if np.any(np.isnan(mean)) or np.any(np.isnan(C)):\n                print(\"NaN detected, resetting...\")\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                sigma = self.sigma0\n        \n        return f_opt, x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm EnhancedCMAES scored 0.617 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["8d396969-ba5a-47d4-8075-374586c711f9"], "operator": null, "metadata": {"aucs": [0.19695164531269227, 0.291489795956963, 0.922094655958184, 0.963319333980712, 0.920819420394743, 0.9653720085872729, 0.29586808114407936, 0.7778495266366067, 0.9504528143084061, 0.23370163621132123, 0.8937730294996682, 0.9916964796124972, 0.29412244812798927, 0.3857421595680124, 0.6361467814620672, 0.34838759424004884, 0.31667306970202236, 0.9606155371289494, 0.5201099697842257, 0.4808426671879773]}}
{"id": "1ca7cb59-a8df-4b3e-915f-72474900a939", "fitness": 0.3218821977056391, "name": "EnhancedCMAES", "description": "Enhanced CMA-ES with adaptive population sizing, dynamic sigma scaling based on fitness variance, and a more aggressive restart strategy triggered by low diversity in the population.", "code": "import numpy as np\n\nclass EnhancedCMAES:\n    def __init__(self, budget=10000, dim=10, sigma0=0.5, stagnation_threshold=10, diversity_threshold=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness = np.inf\n        self.diversity_threshold = diversity_threshold # Threshold for population diversity\n\n        self.popsize = 4 + int(np.floor(3 * np.log(self.dim)))  # Initial popsize\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.min_popsize = 4 + int(np.floor(3 * np.log(self.dim)))  # Minimum population size\n        self.max_popsize = 2 * self.min_popsize # Maximum population size\n        self.adaptive_popsize = True\n\n    def __call__(self, func):\n        # Initialize variables\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)  # Covariance matrix\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n        # Parameters (using common defaults)\n        c_sigma = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        c_c = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        c_1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        c_mu = min(1 - c_1, 2 * (self.mueff - 1 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        d_sigma = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + c_sigma\n        c_1a = c_1\n        c_mua = c_mu\n\n        # Eigen decomposition of C (expensive, do it rarely)\n        C_evals, C_evecs = np.linalg.eigh(C)\n        C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n        C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n        \n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n\n        while eval_count < self.budget:\n            # Sample population\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            x = mean + sigma * (C_sqrt @ z.T).T\n            \n            # Evaluate the new points\n            f = np.array([func(xi) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n            eval_count += len(x) \n\n            # Sort by fitness\n            idx = np.argsort(f)\n            x = x[idx]\n            z = z[idx]\n            f = f[idx]\n\n            # Update optimal solution\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[0]\n                self.stagnation_counter = 0\n            else:\n                self.stagnation_counter += 1\n            \n            # Stagnation check and restart\n            if self.stagnation_counter > self.stagnation_threshold:\n                # Restart strategy: reset mean and increase sigma\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                sigma = self.sigma0 * 2  # Increase sigma to explore more\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                self.stagnation_counter = 0  # Reset counter\n                print(\"Restarting CMA-ES due to stagnation\")\n            \n            # Diversity check and restart\n            if np.std(f) < self.diversity_threshold:\n                # Restart strategy: reset mean and increase sigma\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                sigma = self.sigma0 * 3  # Increase sigma to explore more\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                self.stagnation_counter = 0  # Reset counter\n                print(\"Restarting CMA-ES due to low diversity\")\n                \n            # Selection and recombination\n            x_mu = x[:self.mu]\n            z_mu = z[:self.mu]\n\n            mean_new = np.sum(x_mu * self.weights[:,None], axis=0)\n            z_w = np.sum(z_mu * self.weights[:,None], axis=0)\n            \n            # Covariance matrix adaptation\n            ps = (1 - c_sigma) * ps + np.sqrt(c_sigma * (2 - c_sigma) * self.mueff) * (C_invsqrt @ (mean_new - mean) / sigma)\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - c_sigma)**(2 * eval_count / self.popsize)) < chiN * (1.4 + 2 / (self.dim + 1))\n            \n            pc = (1 - c_c) * pc + hsig * np.sqrt(c_c * (2 - c_c) * self.mueff) * (mean_new - mean) / sigma\n\n            C = (1 - c_1 - c_mu) * C + c_1 * (pc[:, None] @ pc[None, :])\n            for i in range(self.mu):\n                C += c_mu * self.weights[i] * (z_mu[i, :, None] @ z_mu[i, None, :])\n                \n            # Active CMA\n            if c_1a > 0 and c_mua > 0:\n                negidx = np.where(self.weights < 0)[0]\n                znorm = np.zeros((len(negidx), self.dim))\n                for i, idx in enumerate(negidx):\n                    znorm[i] = z_mu[idx] * np.sqrt(-self.weights[idx])\n                C += c_1a * (1/np.linalg.norm(ps)**2 + c_mua) * np.sum(znorm.T @ znorm, axis=1)\n\n            # Update step size\n            sigma *= np.exp((c_sigma / d_sigma) * (np.linalg.norm(ps) / chiN - 1))\n\n            # Adaptive sigma scaling based on variance of fitness values\n            sigma *= (1 + 0.1 * np.std(f) / (np.abs(np.mean(f)) + 1e-8))\n\n            # Adaptive population sizing\n            if self.adaptive_popsize:\n                if np.std(f) > 0.01:  # High variance, increase population size\n                    self.popsize = min(self.popsize + 1, self.max_popsize)\n                else:  # Low variance, decrease population size\n                    self.popsize = max(self.popsize - 1, self.min_popsize)\n                self.mu = self.popsize // 2\n                self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n                self.weights = self.weights / np.sum(self.weights)\n                self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n            # Update mean\n            mean = mean_new\n\n            # Eigen decomposition of C\n            if eval_count % (self.popsize * 5) == 0:  # Re-compute after every 5 generations\n                C = np.triu(C) + np.triu(C, 1).T  # Enforce symmetry\n                try:\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_evals = np.maximum(C_evals, 1e-10) # Avoid zero or negative eigenvalues\n                    C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                except np.linalg.LinAlgError:\n                    print(\"LinAlgError encountered, resetting C\")\n                    C = np.eye(self.dim)\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                \n            if np.any(np.isnan(mean)) or np.any(np.isnan(C)):\n                print(\"NaN detected, resetting...\")\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                sigma = self.sigma0\n        \n        return f_opt, x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm EnhancedCMAES scored 0.322 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["8d396969-ba5a-47d4-8075-374586c711f9"], "operator": null, "metadata": {"aucs": [0.19690552923021465, 0.1851530023943173, 0.325242946664407, 0.2556809477827158, 0.33272257714072073, 0.3106866394525225, 0.2725003790643381, 0.23383038377352605, 0.2879697719759171, 0.16441006284799076, 0.3835790520848603, 0.9844890658642946, 0.25436846200092655, 0.19823210002250546, 0.5696917516482833, 0.2961813106991632, 0.2381583423638992, 0.34052953798673413, 0.1647027893676919, 0.44260930174775215]}}
{"id": "103a230d-1d68-42b7-a20d-e696049fb067", "fitness": 0.5614659542390036, "name": "CoordinateCMAES", "description": "CMA-ES with coordinate-wise adaptation of the step size and a more aggressive step size increase when successful to accelerate convergence.", "code": "import numpy as np\n\nclass CoordinateCMAES:\n    def __init__(self, budget=10000, dim=10, sigma0 = 0.5):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n\n        self.popsize = 4 + int(np.floor(3 * np.log(self.dim)))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n        self.success_rate = 0.2  # Initial success rate\n        self.learning_rate_sigma = 0.2  # Learning rate for step size\n        self.coordinate_sigma = np.full(self.dim, sigma0)\n\n    def __call__(self, func):\n        # Initialize variables\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        #sigma = self.sigma0\n        C = np.eye(self.dim)  # Covariance matrix\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n        # Parameters (using common defaults)\n        c_sigma = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        c_c = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        c_1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        c_mu = min(1 - c_1, 2 * (self.mueff - 1 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        d_sigma = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + c_sigma\n        c_1a = c_1\n        c_mua = c_mu\n\n        # Eigen decomposition of C (expensive, do it rarely)\n        try:\n            C_evals, C_evecs = np.linalg.eigh(C)\n            C_evals = np.maximum(C_evals, 1e-10)\n            C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n            C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n        except np.linalg.LinAlgError:\n            print(\"Initial LinAlgError encountered, resetting C\")\n            C = np.eye(self.dim)\n            C_evals, C_evecs = np.linalg.eigh(C)\n            C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n            C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n\n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n        successes = 0\n\n        while eval_count < self.budget:\n            # Sample population\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            x = mean + (self.coordinate_sigma * (C_sqrt @ z.T).T)\n            \n            # Evaluate the new points\n            f = np.array([func(xi) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n            eval_count += len(x) \n\n            # Sort by fitness\n            idx = np.argsort(f)\n            x = x[idx]\n            z = z[idx]\n            f = f[idx]\n\n            # Update optimal solution\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[0]\n                successes += 1\n                \n            # Selection and recombination\n            x_mu = x[:self.mu]\n            z_mu = z[:self.mu]\n\n            mean_new = np.sum(x_mu * self.weights[:,None], axis=0)\n            z_w = np.sum(z_mu * self.weights[:,None], axis=0)\n            \n            # Covariance matrix adaptation\n            ps = (1 - c_sigma) * ps + np.sqrt(c_sigma * (2 - c_sigma) * self.mueff) * (C_invsqrt @ (mean_new - mean) / self.coordinate_sigma)\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - c_sigma)**(2 * eval_count / self.popsize)) < chiN * (1.4 + 2 / (self.dim + 1))\n            \n            pc = (1 - c_c) * pc + hsig * np.sqrt(c_c * (2 - c_c) * self.mueff) * (mean_new - mean) / self.coordinate_sigma\n\n            C = (1 - c_1 - c_mu) * C + c_1 * (pc[:, None] @ pc[None, :])\n            for i in range(self.mu):\n                C += c_mu * self.weights[i] * (z_mu[i, :, None] @ z_mu[i, None, :])\n                \n            # Active CMA\n            if c_1a > 0 and c_mua > 0:\n                negidx = np.where(self.weights < 0)[0]\n                znorm = np.zeros((len(negidx), self.dim))\n                for i, idx in enumerate(negidx):\n                    znorm[i] = z_mu[idx] * np.sqrt(-self.weights[idx])\n                C += c_1a * (1/np.linalg.norm(ps)**2 + c_mua) * np.sum(znorm.T @ znorm, axis=1)\n            \n            # Spectral Clipping (Correct negative eigenvalues)\n            C = np.triu(C) + np.triu(C, 1).T  # Enforce symmetry\n            try:\n                evals, evecs = np.linalg.eigh(C)\n                evals = np.maximum(evals, 1e-10)  # Clip small/negative eigenvalues\n                C = evecs @ np.diag(evals) @ evecs.T\n            except np.linalg.LinAlgError:\n                print(\"LinAlgError during spectral clipping, resetting C\")\n                C = np.eye(self.dim)\n\n            # Update success rate\n            self.success_rate = 0.9 * self.success_rate + 0.1 * (successes > 0) # Binary success or failure\n            successes = 0\n            \n            # Coordinate-wise step size adaptation\n            for i in range(self.dim):\n                if self.success_rate > 0.2:\n                    self.coordinate_sigma[i] *= np.exp(0.1 + 0.4 * (self.success_rate - 0.2)) # Increase more aggressively upon success\n                else:\n                    self.coordinate_sigma[i] *= np.exp(0.1 * (self.success_rate - 0.2))  # Adjust towards target rate of 0.2\n\n            # Update mean\n            mean = mean_new\n\n            # Eigen decomposition of C\n            if eval_count % (self.popsize * 5) == 0:  # Re-compute after every 5 generations\n                C = np.triu(C) + np.triu(C, 1).T  # Enforce symmetry\n                try:\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_evals = np.maximum(C_evals, 1e-10) # Avoid zero or negative eigenvalues\n                    C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                except np.linalg.LinAlgError:\n                    print(\"LinAlgError encountered, resetting C\")\n                    C = np.eye(self.dim)\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                \n            if np.any(np.isnan(mean)) or np.any(np.isnan(C)):\n                print(\"NaN detected, resetting...\")\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                self.coordinate_sigma = np.full(self.dim, self.sigma0)\n        \n        return f_opt, x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm CoordinateCMAES scored 0.561 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["457debd7-c06a-4882-a676-c504d7397490"], "operator": null, "metadata": {"aucs": [0.2488690600819511, 0.723727745961752, 0.5867269445332415, 0.7599990275370547, 0.6495622013745477, 0.658440794948822, 0.32048121154096265, 0.5450269354220203, 0.6256244137742518, 0.20612476729460538, 0.8224302800159242, 0.9963105127790379, 0.2134879014022043, 0.6211077161718396, 0.6136469008643951, 0.6621888642056242, 0.553402813600024, 0.7343454310423803, 0.18958516455158259, 0.49823039767785227]}}
{"id": "61ef2351-8de6-492f-a81c-a83dff2dea15", "fitness": 0.6514596573174548, "name": "CMAES_AdaptiveFrequency", "description": "Adaptively adjust the covariance matrix update frequency based on the observed change in function value to balance exploration and exploitation effectively.", "code": "import numpy as np\n\nclass CMAES_AdaptiveFrequency:\n    def __init__(self, budget=10000, dim=10, sigma0 = 0.5):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n\n        self.popsize = 4 + int(np.floor(3 * np.log(self.dim)))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n        self.success_rate = 0.2  # Initial success rate\n        self.learning_rate_sigma = 0.2  # Learning rate for step size\n\n        self.C_update_frequency = self.popsize * 5  # Initial frequency\n        self.C_update_factor = 2 # Factor to increase/decrease frequency\n\n        self.f_opt_history = []\n        self.history_length = 10\n\n    def __call__(self, func):\n        # Initialize variables\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)  # Covariance matrix\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n        # Parameters (using common defaults)\n        c_sigma = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        c_c = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        c_1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        c_mu = min(1 - c_1, 2 * (self.mueff - 1 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        d_sigma = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + c_sigma\n        c_1a = c_1\n        c_mua = c_mu\n\n        # Eigen decomposition of C (expensive, do it rarely)\n        try:\n            C_evals, C_evecs = np.linalg.eigh(C)\n            C_evals = np.maximum(C_evals, 1e-10)\n            C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n            C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n        except np.linalg.LinAlgError:\n            print(\"Initial LinAlgError encountered, resetting C\")\n            C = np.eye(self.dim)\n            C_evals, C_evecs = np.linalg.eigh(C)\n            C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n            C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n\n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n        successes = 0\n\n        while eval_count < self.budget:\n            # Sample population\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            x = mean + sigma * (C_sqrt @ z.T).T\n            \n            # Evaluate the new points\n            f = np.array([func(xi) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n            eval_count += len(x) \n\n            # Sort by fitness\n            idx = np.argsort(f)\n            x = x[idx]\n            z = z[idx]\n            f = f[idx]\n\n            # Update optimal solution\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[0]\n                successes += 1\n                \n            # Selection and recombination\n            x_mu = x[:self.mu]\n            z_mu = z[:self.mu]\n\n            mean_new = np.sum(x_mu * self.weights[:,None], axis=0)\n            z_w = np.sum(z_mu * self.weights[:,None], axis=0)\n            \n            # Covariance matrix adaptation\n            ps = (1 - c_sigma) * ps + np.sqrt(c_sigma * (2 - c_sigma) * self.mueff) * (C_invsqrt @ (mean_new - mean) / sigma)\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - c_sigma)**(2 * eval_count / self.popsize)) < chiN * (1.4 + 2 / (self.dim + 1))\n            \n            pc = (1 - c_c) * pc + hsig * np.sqrt(c_c * (2 - c_c) * self.mueff) * (mean_new - mean) / sigma\n\n            C = (1 - c_1 - c_mu) * C + c_1 * (pc[:, None] @ pc[None, :])\n            for i in range(self.mu):\n                C += c_mu * self.weights[i] * (z_mu[i, :, None] @ z_mu[i, None, :])\n                \n            # Active CMA\n            if c_1a > 0 and c_mua > 0:\n                negidx = np.where(self.weights < 0)[0]\n                znorm = np.zeros((len(negidx), self.dim))\n                for i, idx in enumerate(negidx):\n                    znorm[i] = z_mu[idx] * np.sqrt(-self.weights[idx])\n                C += c_1a * (1/np.linalg.norm(ps)**2 + c_mua) * np.sum(znorm.T @ znorm, axis=1)\n            \n            # Spectral Clipping (Correct negative eigenvalues)\n            C = np.triu(C) + np.triu(C, 1).T  # Enforce symmetry\n            try:\n                evals, evecs = np.linalg.eigh(C)\n                evals = np.maximum(evals, 1e-10)  # Clip small/negative eigenvalues\n                C = evecs @ np.diag(evals) @ evecs.T\n            except np.linalg.LinAlgError:\n                print(\"LinAlgError during spectral clipping, resetting C\")\n                C = np.eye(self.dim)\n\n            # Update success rate\n            self.success_rate = 0.9 * self.success_rate + 0.1 * (successes > 0) # Binary success or failure\n            successes = 0\n\n            # Update step size based on success rate\n            sigma *= np.exp(self.learning_rate_sigma * (self.success_rate - 0.2))  # Adjust towards target rate of 0.2\n            \n            # Update mean\n            mean = mean_new\n\n            # Adaptive C update frequency\n            self.f_opt_history.append(f_opt)\n            if len(self.f_opt_history) > self.history_length:\n                self.f_opt_history.pop(0)\n\n                # Calculate the change in f_opt\n                change = abs(self.f_opt_history[-1] - self.f_opt_history[0])\n\n                # Adjust the update frequency based on the change.\n                if change < 1e-3: # Stagnation: Reduce frequency.\n                    self.C_update_frequency = int(min(self.C_update_frequency * self.C_update_factor, self.budget))\n                else: # Improvement: Increase frequency\n                    self.C_update_frequency = int(max(self.C_update_frequency / self.C_update_factor, self.popsize))\n\n            # Eigen decomposition of C\n            if eval_count % self.C_update_frequency == 0:  # Re-compute after a dynamic number of generations\n                C = np.triu(C) + np.triu(C, 1).T  # Enforce symmetry\n                try:\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_evals = np.maximum(C_evals, 1e-10) # Avoid zero or negative eigenvalues\n                    C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                except np.linalg.LinAlgError:\n                    print(\"LinAlgError encountered, resetting C\")\n                    C = np.eye(self.dim)\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                \n            if np.any(np.isnan(mean)) or np.any(np.isnan(C)):\n                print(\"NaN detected, resetting...\")\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                sigma = self.sigma0\n        \n        return f_opt, x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm CMAES_AdaptiveFrequency scored 0.651 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["457debd7-c06a-4882-a676-c504d7397490"], "operator": null, "metadata": {"aucs": [0.3906752635470826, 0.8105244306368438, 0.7911832594375672, 0.9001365101918162, 0.813239831406477, 0.7997656358799576, 0.32730219619481504, 0.7681363927014154, 0.7893477037916659, 0.2316000554475005, 0.9049721579568367, 0.9883051096060423, 0.25266415157994093, 0.797075568905899, 0.6339661937679673, 0.8288032232072862, 0.38674630495017503, 0.8888368442987996, 0.2190457887260392, 0.5068665241149684]}}
{"id": "896b823e-fc89-4083-ab1f-3a79e952a269", "fitness": 0.6871664274647346, "name": "EnhancedCMAESMirroredSampling", "description": "Introduce a mirrored sampling strategy to enhance exploration, particularly in problems with symmetries or near-symmetries.", "code": "import numpy as np\n\nclass EnhancedCMAESMirroredSampling:\n    def __init__(self, budget=10000, dim=10, sigma0=0.5, stagnation_threshold=10):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.stagnation_threshold = stagnation_threshold  # Number of generations without improvement\n        self.stagnation_counter = 0\n        self.best_fitness = np.inf\n\n        self.popsize = 4 + int(np.floor(3 * np.log(self.dim)))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n    def __call__(self, func):\n        # Initialize variables\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)  # Covariance matrix\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n        # Parameters (using common defaults)\n        c_sigma = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        c_c = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        c_1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        c_mu = min(1 - c_1, 2 * (self.mueff - 1 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        d_sigma = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + c_sigma\n        c_1a = c_1\n        c_mua = c_mu\n\n        # Eigen decomposition of C (expensive, do it rarely)\n        C_evals, C_evecs = np.linalg.eigh(C)\n        C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n        C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n        \n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n\n        while eval_count < self.budget:\n            # Sample population and mirror samples\n            z = np.random.normal(0, 1, size=(self.popsize // 2, self.dim))\n            x = mean + sigma * (C_sqrt @ z.T).T\n            x_mirrored = mean - sigma * (C_sqrt @ z.T).T  # Mirrored samples\n\n            x = np.vstack((x, x_mirrored))  # Combine original and mirrored samples\n            z = np.vstack((z, -z)) #also mirror z\n\n            # Evaluate the new points\n            f = np.array([func(xi) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n            eval_count += len(x) \n\n            # Sort by fitness\n            idx = np.argsort(f)\n            x = x[idx]\n            z = z[idx]\n            f = f[idx]\n\n            # Update optimal solution\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[0]\n                self.stagnation_counter = 0\n            else:\n                self.stagnation_counter += 1\n                \n            # Stagnation check and restart\n            if self.stagnation_counter > self.stagnation_threshold:\n                # Restart strategy: reset mean and increase sigma\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                sigma = self.sigma0 * 2  # Increase sigma to explore more\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                self.stagnation_counter = 0  # Reset counter\n                print(\"Restarting CMA-ES due to stagnation\")\n\n            # Selection and recombination\n            x_mu = x[:self.mu]\n            z_mu = z[:self.mu]\n\n            mean_new = np.sum(x_mu * self.weights[:,None], axis=0)\n            z_w = np.sum(z_mu * self.weights[:,None], axis=0)\n            \n            # Covariance matrix adaptation\n            ps = (1 - c_sigma) * ps + np.sqrt(c_sigma * (2 - c_sigma) * self.mueff) * (C_invsqrt @ (mean_new - mean) / sigma)\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - c_sigma)**(2 * eval_count / self.popsize)) < chiN * (1.4 + 2 / (self.dim + 1))\n            \n            pc = (1 - c_c) * pc + hsig * np.sqrt(c_c * (2 - c_c) * self.mueff) * (mean_new - mean) / sigma\n\n            C = (1 - c_1 - c_mu) * C + c_1 * (pc[:, None] @ pc[None, :])\n            for i in range(self.mu):\n                C += c_mu * self.weights[i] * (z_mu[i, :, None] @ z_mu[i, None, :])\n                \n            # Active CMA\n            if c_1a > 0 and c_mua > 0:\n                negidx = np.where(self.weights < 0)[0]\n                znorm = np.zeros((len(negidx), self.dim))\n                for i, idx in enumerate(negidx):\n                    znorm[i] = z_mu[idx] * np.sqrt(-self.weights[idx])\n                C += c_1a * (1/np.linalg.norm(ps)**2 + c_mua) * np.sum(znorm.T @ znorm, axis=1)\n\n            # Update step size\n            sigma *= np.exp((c_sigma / d_sigma) * (np.linalg.norm(ps) / chiN - 1))\n\n            # Adaptive sigma scaling (if not improving, reduce sigma)\n            if f[0] >= self.best_fitness:\n                sigma *= 0.95  # Reduce step size\n            else:\n                self.best_fitness = f[0]\n            \n            # Update mean\n            mean = mean_new\n\n            # Eigen decomposition of C\n            if eval_count % (self.popsize * 5) == 0:  # Re-compute after every 5 generations\n                C = np.triu(C) + np.triu(C, 1).T  # Enforce symmetry\n                try:\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_evals = np.maximum(C_evals, 1e-10) # Avoid zero or negative eigenvalues\n                    C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                except np.linalg.LinAlgError:\n                    print(\"LinAlgError encountered, resetting C\")\n                    C = np.eye(self.dim)\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                \n            if np.any(np.isnan(mean)) or np.any(np.isnan(C)):\n                print(\"NaN detected, resetting...\")\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                sigma = self.sigma0\n        \n        return f_opt, x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm EnhancedCMAESMirroredSampling scored 0.687 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["8d396969-ba5a-47d4-8075-374586c711f9"], "operator": null, "metadata": {"aucs": [0.2405821895864909, 0.35460012334744884, 0.9491871848518233, 0.9741697399046895, 0.9587719691705214, 0.967456934615049, 0.2878894255959118, 0.9589556576240392, 0.9563077009513725, 0.8532074037661438, 0.9747035864512549, 0.99669916967048, 0.27733826555122254, 0.9212802364768504, 0.6376547843258399, 0.34154546310269374, 0.4525194850923058, 0.9716667640933125, 0.1692777534747607, 0.4995147116424796]}}
{"id": "41a13550-f3dd-4046-a341-5ffa7c046869", "fitness": 0.38432536737858725, "name": "ImprovedCMAES", "description": "Improved CMA-ES with adaptive population size, step-size control using a learning rate, and covariance matrix adaptation considering both rank-one and rank-mu updates, while also incorporating a bound constraint handling strategy.", "code": "import numpy as np\n\nclass ImprovedCMAES:\n    def __init__(self, budget=10000, dim=10, sigma0=0.5, stagnation_threshold=15, adapt_popsize=True, bound_handling='clip'):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness = np.inf\n        self.adapt_popsize = adapt_popsize\n        self.bound_handling = bound_handling\n        \n        self.popsize = 4 + int(3 * np.log(self.dim)) if not adapt_popsize else int(4 + 3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.stepsize_learning_rate = 0.2\n\n    def __call__(self, func):\n        # Initialize variables\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n        # Parameters\n        c_sigma = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        c_c = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        c_1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        c_mu = min(1 - c_1, 2 * (self.mueff - 1 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        d_sigma = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + c_sigma\n        c_1a = c_1\n        c_mua = c_mu\n\n        # Eigen decomposition of C\n        C_evals, C_evecs = np.linalg.eigh(C)\n        C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n        C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n        \n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n        \n        stepsize_factor = 1.0\n\n        while eval_count < self.budget:\n            # Adjust popsize\n            if self.adapt_popsize:\n                self.popsize = max(4, int(np.floor(4 + 3 * np.log(self.dim) * (0.8 + 0.2 * (self.budget - eval_count) / self.budget))))\n                self.mu = self.popsize // 2\n                self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n                self.weights = self.weights / np.sum(self.weights)\n                self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n                c_sigma = (self.mueff + 2) / (self.dim + self.mueff + 5)\n                c_c = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n                c_1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n                c_mu = min(1 - c_1, 2 * (self.mueff - 1 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n                d_sigma = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + c_sigma\n\n            # Sample population\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            x = mean + sigma * (C_sqrt @ z.T).T\n\n            # Boundary Handling\n            if self.bound_handling == 'clip':\n                x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            elif self.bound_handling == 'reflect':\n                x = np.where(x < func.bounds.lb, 2 * func.bounds.lb - x, x)\n                x = np.where(x > func.bounds.ub, 2 * func.bounds.ub - x, x)\n            else: # 'none'\n                pass # Do nothing\n\n            # Evaluate the new points\n            f = np.array([func(xi) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n            eval_count += len(x)\n\n            # Sort by fitness\n            idx = np.argsort(f)\n            x = x[idx]\n            z = z[idx]\n            f = f[idx]\n\n            # Update optimal solution\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[0]\n                self.stagnation_counter = 0\n            else:\n                self.stagnation_counter += 1\n                \n            # Stagnation check and restart\n            if self.stagnation_counter > self.stagnation_threshold:\n                # Restart strategy: reset mean and increase sigma\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                sigma = self.sigma0 * 1.5\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                self.stagnation_counter = 0\n\n            # Selection and recombination\n            x_mu = x[:self.mu]\n            z_mu = z[:self.mu]\n\n            mean_new = np.sum(x_mu * self.weights[:,None], axis=0)\n            z_w = np.sum(z_mu * self.weights[:,None], axis=0)\n            \n            # Covariance matrix adaptation\n            ps = (1 - c_sigma) * ps + np.sqrt(c_sigma * (2 - c_sigma) * self.mueff) * (C_invsqrt @ (mean_new - mean) / sigma)\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - c_sigma)**(2 * eval_count / self.popsize)) < chiN * (1.4 + 2 / (self.dim + 1))\n            \n            pc = (1 - c_c) * pc + hsig * np.sqrt(c_c * (2 - c_c) * self.mueff) * (mean_new - mean) / sigma\n\n            C = (1 - c_1 - c_mu) * C + c_1 * (pc[:, None] @ pc[None, :])\n            for i in range(self.mu):\n                C += c_mu * self.weights[i] * (z_mu[i, :, None] @ z_mu[i, None, :])\n                \n            # Active CMA\n            if c_1a > 0 and c_mua > 0:\n                negidx = np.where(self.weights < 0)[0]\n                znorm = np.zeros((len(negidx), self.dim))\n                for i, idx in enumerate(negidx):\n                    znorm[i] = z_mu[idx] * np.sqrt(-self.weights[idx])\n                C += c_1a * (1/np.linalg.norm(ps)**2 + c_mua) * np.sum(znorm.T @ znorm, axis=1)\n\n            # Update step size\n            sigma *= np.exp((c_sigma / d_sigma) * (np.linalg.norm(ps) / chiN - 1))\n            \n            if f[0] < self.best_fitness:\n                self.best_fitness = f[0]\n                stepsize_factor *= np.exp(self.stepsize_learning_rate)  # Increase stepsize\n            else:\n                stepsize_factor *= np.exp(-self.stepsize_learning_rate/5)  # Decrease stepsize slower\n            \n            sigma *= stepsize_factor\n\n            # Update mean\n            mean = mean_new\n\n            # Eigen decomposition of C\n            if eval_count % (self.popsize * 5) == 0:\n                C = np.triu(C) + np.triu(C, 1).T\n                try:\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_evals = np.maximum(C_evals, 1e-10)\n                    C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                except np.linalg.LinAlgError:\n                    C = np.eye(self.dim)\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                \n            if np.any(np.isnan(mean)) or np.any(np.isnan(C)):\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                sigma = self.sigma0\n        \n        return f_opt, x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm ImprovedCMAES scored 0.384 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["8d396969-ba5a-47d4-8075-374586c711f9"], "operator": null, "metadata": {"aucs": [0.16868044649785552, 0.2731717381121722, 0.2875281519228392, 0.9238521020190419, 0.28134032023327926, 0.44531890163140164, 0.28127065182669675, 0.31734152029950957, 0.262775590202206, 0.19750919387584343, 0.40351302352620044, 0.9925259881281403, 0.25197718511637823, 0.33367133200276833, 0.6089626674429889, 0.2723941813314692, 0.32106078109935077, 0.418028337770599, 0.18626704091176438, 0.45931819362123916]}}
{"id": "b209bf25-3403-4f8e-9afc-e0f7ff5dc8e3", "fitness": 0.49659652444806, "name": "AdaptiveCMAESMirroredSampling", "description": "Adaptive CMA-ES with mirrored sampling, dynamic sigma adaptation based on success rate, and a simplified covariance matrix update.", "code": "import numpy as np\n\nclass AdaptiveCMAESMirroredSampling:\n    def __init__(self, budget=10000, dim=10, sigma0=0.5, stagnation_threshold=10, success_history_length=10):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness = np.inf\n        self.success_history_length = success_history_length\n        self.success_history = []\n\n        self.popsize = 4 + int(np.floor(3 * np.log(self.dim)))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n        self.c_sigma = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.d_sigma = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.c_sigma\n\n    def __call__(self, func):\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)\n        eval_count = 0\n        f_opt = np.inf\n        x_opt = None\n        \n        while eval_count < self.budget:\n            z = np.random.normal(0, 1, size=(self.popsize // 2, self.dim))\n            x = mean + sigma * (np.linalg.cholesky(C) @ z.T).T\n            x_mirrored = mean - sigma * (np.linalg.cholesky(C) @ z.T).T\n            x = np.vstack((x, x_mirrored))\n            z = np.vstack((z, -z))\n            \n            f = np.array([func(xi) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n            eval_count += len(x)\n            \n            idx = np.argsort(f)\n            x = x[idx]\n            z = z[idx]\n            f = f[idx]\n\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[0]\n                self.stagnation_counter = 0\n            else:\n                self.stagnation_counter += 1\n                \n            if self.stagnation_counter > self.stagnation_threshold:\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                sigma = self.sigma0 * 2\n                C = np.eye(self.dim)\n                self.stagnation_counter = 0\n\n            x_mu = x[:self.mu]\n            z_mu = z[:self.mu]\n            mean_new = np.sum(x_mu * self.weights[:,None], axis=0)\n\n            # Simplified Covariance Matrix Update\n            C = (1 - self.c_sigma) * C + self.c_sigma * np.cov(z_mu.T)\n\n            #Dynamic Sigma adaptation\n            success_rate = (self.best_fitness > f[0])\n            self.success_history.append(success_rate)\n            if len(self.success_history) > self.success_history_length:\n                self.success_history.pop(0)\n            \n            recent_success_rate = np.mean(self.success_history) if self.success_history else 0.5\n            \n            if recent_success_rate > 0.7:\n                sigma *= 1.1\n            elif recent_success_rate < 0.3:\n                sigma *= 0.9\n\n            self.best_fitness = min(self.best_fitness, f[0])\n            mean = mean_new\n\n            if eval_count % self.popsize == 0:\n                try:\n                    C = np.triu(C) + np.triu(C, 1).T\n                    C = np.linalg.cholesky(C) @ np.linalg.cholesky(C).T\n                    eigenvalues = np.linalg.eigvalsh(C)\n                    if np.any(eigenvalues < 1e-6):\n                        C += np.eye(self.dim) * 1e-5\n                except np.linalg.LinAlgError:\n                    C = np.eye(self.dim)\n                    \n        return f_opt, x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveCMAESMirroredSampling scored 0.497 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["896b823e-fc89-4083-ab1f-3a79e952a269"], "operator": null, "metadata": {"aucs": [0.23152856825394696, 0.4478252108331513, 0.38222502093853705, 0.827876747984285, 0.33635532969174287, 0.6193733238278771, 0.30907615280223943, 0.42915948102637125, 0.6912721664941235, 0.24871893365975473, 0.9452014116531385, 0.9899128781207509, 0.2999829940035653, 0.3214145302844843, 0.6346792420944817, 0.3540180047563094, 0.3813343471523948, 0.7637287540723863, 0.21201159963215055, 0.5062357916795097]}}
{"id": "fd9aa359-87b2-4518-81a4-814479aa4447", "fitness": 0.7301817810811568, "name": "RobustCMAES", "description": "Implements a more robust and efficient CMA-ES by incorporating a mirrored sampling strategy, adaptive covariance matrix regularization, and dynamic population sizing with restarts to better handle complex optimization landscapes.", "code": "import numpy as np\n\nclass RobustCMAES:\n    def __init__(self, budget=10000, dim=10, sigma0=0.5, history_length=5, mirror_ratio=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.history_length = history_length\n        self.min_popsize = 4\n        self.max_popsize = 50\n        self.target_popsize = min(self.max_popsize, self.min_popsize + int(np.floor(3 * np.log(self.dim))))\n        self.popsize = self.target_popsize\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.step_history = []\n        self.last_f_opt = np.inf\n        self.stagnation_counter = 0\n        self.restart_iterations = 50\n        self.mirror_ratio = mirror_ratio\n\n    def __call__(self, func):\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n        c_sigma = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        c_c = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        c_1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        c_mu = min(1 - c_1, 2 * (self.mueff - 1 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        d_sigma = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + c_sigma\n\n        C_evals, C_evecs = np.linalg.eigh(C)\n        C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n        C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n        \n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n\n        while eval_count < self.budget:\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            x = mean + sigma * (C_sqrt @ z.T).T\n\n            # Mirrored Sampling\n            num_mirrored = int(self.popsize * self.mirror_ratio)\n            z_mirrored = -z[:num_mirrored]\n            x_mirrored = mean + sigma * (C_sqrt @ z_mirrored.T).T\n            x = np.concatenate((x, x_mirrored), axis=0)\n            z = np.concatenate((z, z_mirrored), axis=0)\n\n            f = np.array([func(xi) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n            eval_count += len(x)\n\n            idx = np.argsort(f)\n            x = x[idx]\n            z = z[idx]\n            f = f[idx]\n\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[0]\n                self.stagnation_counter = 0\n            else:\n                self.stagnation_counter += 1\n                \n            if self.stagnation_counter > self.restart_iterations:\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                sigma = self.sigma0\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                self.stagnation_counter = 0\n\n            x_mu = x[:self.mu]\n            z_mu = z[:self.mu]\n\n            mean_new = np.sum(x_mu * self.weights[:, None], axis=0)\n            z_w = np.sum(z_mu * self.weights[:, None], axis=0)\n            \n            ps = (1 - c_sigma) * ps + np.sqrt(c_sigma * (2 - c_sigma) * self.mueff) * (C_invsqrt @ (mean_new - mean) / sigma)\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - c_sigma)**(2 * eval_count / self.popsize)) < chiN * (1.4 + 2 / (self.dim + 1))\n            pc = (1 - c_c) * pc + hsig * np.sqrt(c_c * (2 - c_c) * self.mueff) * (mean_new - mean) / sigma\n\n            C = (1 - c_1 - c_mu) * C + c_1 * (pc[:, None] @ pc[None, :])\n            for i in range(self.mu):\n                C += c_mu * self.weights[i] * (z_mu[i, :, None] @ z_mu[i, None, :])\n            \n            sigma *= np.exp((c_sigma / d_sigma) * (np.linalg.norm(ps) / chiN - 1))\n\n            # Simplified Step history\n            step = (mean_new - mean) / sigma\n            self.step_history.append(step)\n            if len(self.step_history) > self.history_length:\n                self.step_history.pop(0)\n\n            # Adapt C based on step history, simplified\n            for h_step in self.step_history:\n                C += 0.1 * c_mu * (h_step[:, None] @ h_step[None, :])\n\n            mean = mean_new\n            \n            if eval_count % (self.popsize * 5) == 0:\n                C = np.triu(C) + np.triu(C, 1).T\n                try:\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_evals = np.maximum(C_evals, 1e-10)\n                    C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                except np.linalg.LinAlgError:\n                    C = np.eye(self.dim)\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n\n                # Regularize Covariance Matrix\n                C = C + 1e-8 * np.eye(self.dim)\n                \n            if np.any(np.isnan(mean)) or np.any(np.isnan(C)):\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                sigma = self.sigma0\n\n        return f_opt, x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm RobustCMAES scored 0.730 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a1430f95-eb82-4ea4-9944-98b7e36ea3f1"], "operator": null, "metadata": {"aucs": [0.2442883200059387, 0.429290376183861, 0.9250640300426848, 0.9674745111027532, 0.9278338626340992, 0.9411779911244412, 0.3830692302530081, 0.9262002761649104, 0.938559440873177, 0.22551275568904638, 0.9641409922826734, 0.990431172266373, 0.2539300873438578, 0.9283077257363572, 0.6340169301021252, 0.9261734787347528, 0.8994651519938366, 0.9576358289495756, 0.21977271925550768, 0.9212907408841595]}}
{"id": "a813641a-f575-4987-a2e8-75be0d8f88b2", "fitness": 0.6559976534271508, "name": "CMAES_AdaptiveMirrorRank", "description": "Adaptively adjusts the mirrored sampling rate and uses a rank-based update for the covariance matrix to improve exploration and exploitation.", "code": "import numpy as np\n\nclass CMAES_AdaptiveMirrorRank:\n    def __init__(self, budget=10000, dim=10, sigma0=0.5, history_length=10, initial_mirrored_fraction=0.5, mirrored_decay=0.99):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.history_length = history_length\n        self.mirrored_fraction = initial_mirrored_fraction  # Initial fraction of population to mirror\n        self.mirrored_decay = mirrored_decay  # Decay rate for mirrored fraction\n\n        self.popsize = 4 + int(np.floor(3 * np.log(self.dim)))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.step_history = []\n\n    def __call__(self, func):\n        # Initialize variables\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n        # Parameters (using common defaults)\n        c_sigma = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        c_c = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        c_1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        c_mu = min(1 - c_1, 2 * (self.mueff - 1 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        d_sigma = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1)\n        c_1a = c_1\n        c_mua = c_mu\n\n        # Eigen decomposition of C\n        C_evals, C_evecs = np.linalg.eigh(C)\n        C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n        C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n        \n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n\n        while eval_count < self.budget:\n            # Sample population\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            x = mean + sigma * (C_sqrt @ z.T).T\n\n            # Mirrored sampling\n            num_mirrored = int(self.popsize * self.mirrored_fraction)\n            x_mirrored = mean - sigma * (C_sqrt @ z[:num_mirrored].T).T\n            x = np.vstack((x, x_mirrored))\n            z_mirrored = -z[:num_mirrored]\n            z = np.vstack((z, z_mirrored))\n            \n            # Clipping to bounds\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            \n            # Evaluate the new points\n            f = np.array([func(xi) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n            eval_count += len(x) \n\n            # Sort by fitness\n            idx = np.argsort(f)\n            x = x[idx]\n            z = z[idx]\n            f = f[idx]\n\n            # Update optimal solution\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[0]\n                \n            # Selection and recombination\n            x_mu = x[:self.mu]\n            z_mu = z[:self.mu]\n\n            mean_new = np.sum(x_mu * self.weights[:,None], axis=0)\n            z_w = np.sum(z_mu * self.weights[:,None], axis=0)\n            \n            # Covariance matrix adaptation using rank-mu update\n            ps = (1 - c_sigma) * ps + np.sqrt(c_sigma * (2 - c_sigma) * self.mueff) * (C_invsqrt @ (mean_new - mean) / sigma)\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - c_sigma)**(2 * eval_count / self.popsize)) < chiN * (1.4 + 2 / (self.dim + 1))\n            \n            pc = (1 - c_c) * pc + hsig * np.sqrt(c_c * (2 - c_c) * self.mueff) * (mean_new - mean) / sigma\n\n            C = (1 - c_1 - c_mu) * C + c_1 * (pc[:, None] @ pc[None, :])\n\n            # Rank-mu update\n            for i in range(self.mu):\n                C += c_mu * self.weights[i] * (z_mu[i, :, None] @ z_mu[i, None, :])\n                \n            # Update step size\n            sigma *= np.exp((c_sigma / d_sigma) * (np.linalg.norm(ps) / chiN - 1))\n\n            # Step history adaptation\n            step = (mean_new - mean) / sigma\n            self.step_history.append(step)\n            if len(self.step_history) > self.history_length:\n                self.step_history.pop(0)\n\n            # Adapt C based on step history (dampened)\n            for h_step in self.step_history:\n                C += 0.1 * c_mu * (h_step[:, None] @ h_step[None, :])  # Dampened update\n                \n            # Update mean\n            mean = mean_new\n\n            # Adaptive Mirrored fraction\n            self.mirrored_fraction *= self.mirrored_decay\n\n            # Eigen decomposition of C\n            if eval_count % (self.popsize * 5) == 0:\n                C = np.triu(C) + np.triu(C, 1).T\n                try:\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_evals = np.maximum(C_evals, 1e-10)\n                    C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                except np.linalg.LinAlgError:\n                    print(\"LinAlgError encountered, resetting C\")\n                    C = np.eye(self.dim)\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                \n            if np.any(np.isnan(mean)) or np.any(np.isnan(C)):\n                print(\"NaN detected, resetting...\")\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                sigma = self.sigma0\n        \n        return f_opt, x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm CMAES_AdaptiveMirrorRank scored 0.656 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["01f39612-4640-4647-aada-b9956458ca86"], "operator": null, "metadata": {"aucs": [0.24296373718871322, 0.9243614395770046, 0.9285861412967221, 0.15657031271408217, 0.9356151404684302, 0.9524371791268329, 0.36338304999700854, 0.9374687305024231, 0.9442046808904145, 0.23653195681730343, 0.9629990490625746, 0.9739104969149495, 0.29169033994557325, 0.9465371215174618, 0.9736750992882529, 0.3335876285080098, 0.3417337203996409, 0.9646478800398602, 0.19903848065295104, 0.5100108836348063]}}
{"id": "f44e146f-3561-47d5-8d9e-2a6988c6dce9", "fitness": 0.6467903760696104, "name": "DynamicCMAES", "description": "Adapt the step size dynamically using a success rate based mechanism, combined with a moving average filter to smooth the adaptation.", "code": "import numpy as np\n\nclass DynamicCMAES:\n    def __init__(self, budget=10000, dim=10, sigma0=0.5, history_length=5, success_rate_history=10):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.history_length = history_length\n        self.success_rate_history_length = success_rate_history\n        self.min_popsize = 4\n        self.max_popsize = 50\n        self.target_popsize = min(self.max_popsize, self.min_popsize + int(np.floor(3 * np.log(self.dim))))\n        self.popsize = self.target_popsize\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.step_history = []\n        self.last_f_opt = np.inf\n        self.stagnation_counter = 0\n        self.restart_iterations = 50\n        self.success_history = []\n        self.success_rate = 0.5 # Initial success rate\n        self.success_rate_window = np.array([0.5] * self.success_rate_history_length) # initialize success rate\n\n    def __call__(self, func):\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n        c_sigma = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        c_c = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        c_1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        c_mu = min(1 - c_1, 2 * (self.mueff - 1 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        d_sigma = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + c_sigma\n\n        C_evals, C_evecs = np.linalg.eigh(C)\n        C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n        C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n        \n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n\n        while eval_count < self.budget:\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            x = mean + sigma * (C_sqrt @ z.T).T\n            \n            f = np.array([func(xi) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n            eval_count += len(x)\n\n            idx = np.argsort(f)\n            x = x[idx]\n            z = z[idx]\n            f = f[idx]\n\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[0]\n                self.stagnation_counter = 0\n                self.success_history.append(1)\n            else:\n                self.stagnation_counter += 1\n                self.success_history.append(0)\n                \n            if self.stagnation_counter > self.restart_iterations:\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                sigma = self.sigma0\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                self.stagnation_counter = 0\n\n            x_mu = x[:self.mu]\n            z_mu = z[:self.mu]\n\n            mean_new = np.sum(x_mu * self.weights[:, None], axis=0)\n            z_w = np.sum(z_mu * self.weights[:, None], axis=0)\n            \n            ps = (1 - c_sigma) * ps + np.sqrt(c_sigma * (2 - c_sigma) * self.mueff) * (C_invsqrt @ (mean_new - mean) / sigma)\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - c_sigma)**(2 * eval_count / self.popsize)) < chiN * (1.4 + 2 / (self.dim + 1))\n            pc = (1 - c_c) * pc + hsig * np.sqrt(c_c * (2 - c_c) * self.mueff) * (mean_new - mean) / sigma\n\n            C = (1 - c_1 - c_mu) * C + c_1 * (pc[:, None] @ pc[None, :])\n            for i in range(self.mu):\n                C += c_mu * self.weights[i] * (z_mu[i, :, None] @ z_mu[i, None, :])\n            \n            # Adapt step size based on success rate\n            if len(self.success_history) > 0:\n                self.success_rate_window = np.concatenate((self.success_rate_window[1:], [self.success_history[-1]]))\n                self.success_rate = np.mean(self.success_rate_window)\n                \n                if self.success_rate > 0.6:\n                    sigma *= np.exp(0.1)\n                elif self.success_rate < 0.4:\n                    sigma *= np.exp(-0.1)\n\n            # Simplified Step history\n            step = (mean_new - mean) / sigma\n            self.step_history.append(step)\n            if len(self.step_history) > self.history_length:\n                self.step_history.pop(0)\n\n            # Adapt C based on step history, simplified\n            for h_step in self.step_history:\n                C += 0.1 * c_mu * (h_step[:, None] @ h_step[None, :])\n\n            mean = mean_new\n            \n            if eval_count % (self.popsize * 5) == 0:\n                C = np.triu(C) + np.triu(C, 1).T\n                try:\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_evals = np.maximum(C_evals, 1e-10)\n                    C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                except np.linalg.LinAlgError:\n                    C = np.eye(self.dim)\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                \n            if np.any(np.isnan(mean)) or np.any(np.isnan(C)):\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                sigma = self.sigma0\n\n        return f_opt, x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm DynamicCMAES scored 0.647 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a1430f95-eb82-4ea4-9944-98b7e36ea3f1"], "operator": null, "metadata": {"aucs": [0.17468564290058386, 0.18155466170883094, 0.932583125017151, 0.959613659561098, 0.929505970971368, 0.9438108416756738, 0.32700698289841734, 0.9159707615604081, 0.9241310113280642, 0.15873024601086705, 0.9584746508378695, 0.6861697466765834, 0.2594534844878462, 0.9314937950481211, 0.5866932875741413, 0.9336230982888493, 0.4599214879506204, 0.9554304626416409, 0.1980749584899567, 0.5188796457641192]}}
{"id": "750a5306-82d8-43df-bda0-02e595718b35", "fitness": 0.5524772374747523, "name": "CMAES_AdaptiveMean_StepSize", "description": "CMA-ES with adaptive step size and covariance matrix adaptation, incorporating a population-based step size adaptation and dynamic mean adaptation based on fitness improvement.", "code": "import numpy as np\n\nclass CMAES_AdaptiveMean_StepSize:\n    def __init__(self, budget=10000, dim=10, sigma0 = 0.5):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n\n        self.popsize = 4 + int(np.floor(3 * np.log(self.dim)))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n        self.success_rate = 0.2  # Initial success rate\n        self.learning_rate_sigma = 0.2  # Learning rate for step size\n\n        self.C_update_frequency = self.popsize * 5  # Initial frequency\n        self.C_update_factor = 2 # Factor to increase/decrease frequency\n\n        self.f_opt_history = []\n        self.history_length = 10\n        self.mean_history = []\n\n    def __call__(self, func):\n        # Initialize variables\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)  # Covariance matrix\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n        # Parameters (using common defaults)\n        c_sigma = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        c_c = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        c_1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        c_mu = min(1 - c_1, 2 * (self.mueff - 1 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        d_sigma = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1)\n        c_1a = c_1\n        c_mua = c_mu\n\n        # Eigen decomposition of C (expensive, do it rarely)\n        try:\n            C_evals, C_evecs = np.linalg.eigh(C)\n            C_evals = np.maximum(C_evals, 1e-10)\n            C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n            C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n        except np.linalg.LinAlgError:\n            print(\"Initial LinAlgError encountered, resetting C\")\n            C = np.eye(self.dim)\n            C_evals, C_evecs = np.linalg.eigh(C)\n            C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n            C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n\n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n        successes = 0\n        step_size_adaptation_history = [] # Store past step sizes.\n\n        while eval_count < self.budget:\n            # Sample population\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            x = mean + sigma * (C_sqrt @ z.T).T\n            \n            # Evaluate the new points\n            f = np.array([func(xi) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n            eval_count += len(x) \n\n            # Sort by fitness\n            idx = np.argsort(f)\n            x = x[idx]\n            z = z[idx]\n            f = f[idx]\n\n            # Update optimal solution\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[0]\n                successes += 1\n                \n            # Selection and recombination\n            x_mu = x[:self.mu]\n            z_mu = z[:self.mu]\n\n            mean_new = np.sum(x_mu * self.weights[:,None], axis=0)\n            z_w = np.sum(z_mu * self.weights[:,None], axis=0)\n            \n            # Covariance matrix adaptation\n            ps = (1 - c_sigma) * ps + np.sqrt(c_sigma * (2 - c_sigma) * self.mueff) * (C_invsqrt @ (mean_new - mean) / sigma)\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - c_sigma)**(2 * eval_count / self.popsize)) < chiN * (1.4 + 2 / (self.dim + 1))\n            \n            pc = (1 - c_c) * pc + hsig * np.sqrt(c_c * (2 - c_c) * self.mueff) * (mean_new - mean) / sigma\n\n            C = (1 - c_1 - c_mu) * C + c_1 * (pc[:, None] @ pc[None, :])\n            for i in range(self.mu):\n                C += c_mu * self.weights[i] * (z_mu[i, :, None] @ z_mu[i, None, :])\n                \n            # Active CMA\n            if c_1a > 0 and c_mua > 0:\n                negidx = np.where(self.weights < 0)[0]\n                znorm = np.zeros((len(negidx), self.dim))\n                for i, idx in enumerate(negidx):\n                    znorm[i] = z_mu[idx] * np.sqrt(-self.weights[idx])\n                C += c_1a * (1/np.linalg.norm(ps)**2 + c_mua) * np.sum(znorm.T @ znorm, axis=1)\n            \n            # Spectral Clipping (Correct negative eigenvalues)\n            C = np.triu(C) + np.triu(C, 1).T  # Enforce symmetry\n            try:\n                evals, evecs = np.linalg.eigh(C)\n                evals = np.maximum(evals, 1e-10)  # Clip small/negative eigenvalues\n                C = evecs @ np.diag(evals) @ evecs.T\n            except np.linalg.LinAlgError:\n                print(\"LinAlgError during spectral clipping, resetting C\")\n                C = np.eye(self.dim)\n\n            # Update success rate\n            self.success_rate = 0.9 * self.success_rate + 0.1 * (successes > 0) # Binary success or failure\n            successes = 0\n\n            # Population based step size adaptation\n            delta_f = np.mean(f) - f_opt # Average fitness of the population minus best fitness\n            if len(step_size_adaptation_history) > 5:\n                # Use the average of the step sizes\n                avg_sigma = np.mean(step_size_adaptation_history[-5:])\n                if delta_f > 0:\n                    sigma = avg_sigma * np.exp(self.learning_rate_sigma * (self.success_rate - 0.2)) # Reduce\n                else:\n                    sigma = avg_sigma * np.exp(self.learning_rate_sigma * (self.success_rate + 0.2)) # Increase\n            else:\n                sigma *= np.exp(self.learning_rate_sigma * (self.success_rate - 0.2))  # Adjust towards target rate of 0.2\n            step_size_adaptation_history.append(sigma)\n            \n            # Dynamic Mean Adaptation: If improvement stagnates, perturb the mean.\n            self.f_opt_history.append(f_opt)\n            self.mean_history.append(mean)\n            if len(self.f_opt_history) > self.history_length:\n                self.f_opt_history.pop(0)\n                self.mean_history.pop(0)\n                \n                change = abs(self.f_opt_history[-1] - self.f_opt_history[0])\n\n                if change < 1e-5: # Very small improvement, consider perturbing.\n                    # Perturb mean, with decaying perturbation size\n                    perturbation = np.random.normal(0, sigma * 0.1, size=self.dim) # Scale with current sigma\n                    mean = self.mean_history[-1] + perturbation # Perturb the mean\n                    \n                    # Clip to bounds\n                    mean = np.clip(mean, func.bounds.lb, func.bounds.ub)\n\n            # Update mean\n            mean = mean_new\n\n            # Adaptive C update frequency\n            if len(self.f_opt_history) > self.history_length:\n                # Calculate the change in f_opt\n                change = abs(self.f_opt_history[-1] - self.f_opt_history[0])\n\n                # Adjust the update frequency based on the change.\n                if change < 1e-3: # Stagnation: Reduce frequency.\n                    self.C_update_frequency = int(min(self.C_update_frequency * self.C_update_factor, self.budget))\n                else: # Improvement: Increase frequency\n                    self.C_update_frequency = int(max(self.C_update_frequency / self.C_update_factor, self.popsize))\n\n            # Eigen decomposition of C\n            if eval_count % self.C_update_frequency == 0:  # Re-compute after a dynamic number of generations\n                C = np.triu(C) + np.triu(C, 1).T  # Enforce symmetry\n                try:\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_evals = np.maximum(C_evals, 1e-10) # Avoid zero or negative eigenvalues\n                    C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                except np.linalg.LinAlgError:\n                    print(\"LinAlgError encountered, resetting C\")\n                    C = np.eye(self.dim)\n                \n            if np.any(np.isnan(mean)) or np.any(np.isnan(C)):\n                print(\"NaN detected, resetting...\")\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                sigma = self.sigma0\n        \n        return f_opt, x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm CMAES_AdaptiveMean_StepSize scored 0.552 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["61ef2351-8de6-492f-a81c-a83dff2dea15"], "operator": null, "metadata": {"aucs": [0.18814198634192725, 0.18163124401899622, 0.6140668931811809, 0.8603927244172793, 0.6246811495751079, 0.685308051529762, 0.3415097722441547, 0.551894071632574, 0.6884366153664131, 0.2296316639724626, 0.8449789474084946, 0.9983138955411869, 0.2541879682938193, 0.6610028418317573, 0.5902499976723283, 0.6656182369328694, 0.5426601731792307, 0.7942635670520717, 0.19908498032981792, 0.5334899689736112]}}
{"id": "4b1dabd7-2d08-4dc1-b5a3-a63bdb39dbe2", "fitness": -Infinity, "name": "CMAES_Orthogonal_AdaptiveMean_StepSize", "description": "CMA-ES with adaptive step size, covariance matrix adaptation, dynamic mean adaptation using orthogonal subspace learning to improve exploration and exploitation.", "code": "import numpy as np\n\nclass CMAES_Orthogonal_AdaptiveMean_StepSize:\n    def __init__(self, budget=10000, dim=10, sigma0 = 0.5):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n\n        self.popsize = 4 + int(np.floor(3 * np.log(self.dim)))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n        self.success_rate = 0.2  # Initial success rate\n        self.learning_rate_sigma = 0.2  # Learning rate for step size\n\n        self.C_update_frequency = self.popsize * 5  # Initial frequency\n        self.C_update_factor = 2 # Factor to increase/decrease frequency\n\n        self.f_opt_history = []\n        self.history_length = 10\n        self.mean_history = []\n\n    def __call__(self, func):\n        # Initialize variables\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)  # Covariance matrix\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n        # Parameters (using common defaults)\n        c_sigma = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        c_c = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        c_1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        c_mu = min(1 - c_1, 2 * (self.mueff - 1 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        d_sigma = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1)\n        c_1a = c_1\n        c_mua = c_mu\n\n        # Eigen decomposition of C (expensive, do it rarely)\n        try:\n            C_evals, C_evecs = np.linalg.eigh(C)\n            C_evals = np.maximum(C_evals, 1e-10)\n            C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n            C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n        except np.linalg.LinAlgError:\n            print(\"Initial LinAlgError encountered, resetting C\")\n            C = np.eye(self.dim)\n            C_evals, C_evecs = np.linalg.eigh(C)\n            C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n            C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n\n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n        successes = 0\n        step_size_adaptation_history = [] # Store past step sizes.\n\n        # Orthogonal Subspace Learning (OSL) parameters\n        self.osl_alpha = 0.5\n        self.osl_lambda = 5\n        self.osl_vectors = []\n\n        while eval_count < self.budget:\n            # Sample population\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            x = mean + sigma * (C_sqrt @ z.T).T\n            \n            # Evaluate the new points\n            f = np.array([func(xi) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n            eval_count += len(x) \n\n            # Sort by fitness\n            idx = np.argsort(f)\n            x = x[idx]\n            z = z[idx]\n            f = f[idx]\n\n            # Update optimal solution\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[0]\n                successes += 1\n                \n            # Selection and recombination\n            x_mu = x[:self.mu]\n            z_mu = z[:self.mu]\n\n            mean_new = np.sum(x_mu * self.weights[:,None], axis=0)\n            z_w = np.sum(z_mu * self.weights[:,None], axis=0)\n            \n            # Covariance matrix adaptation\n            ps = (1 - c_sigma) * ps + np.sqrt(c_sigma * (2 - c_sigma) * self.mueff) * (C_invsqrt @ (mean_new - mean) / sigma)\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - c_sigma)**(2 * eval_count / self.popsize)) < chiN * (1.4 + 2 / (self.dim + 1))\n            \n            pc = (1 - c_c) * pc + hsig * np.sqrt(c_c * (2 - c_c) * self.mueff) * (mean_new - mean) / sigma\n\n            C = (1 - c_1 - c_mu) * C + c_1 * (pc[:, None] @ pc[None, :])\n            for i in range(self.mu):\n                C += c_mu * self.weights[i] * (z_mu[i, :, None] @ z_mu[i, None, :])\n                \n            # Active CMA\n            if c_1a > 0 and c_mua > 0:\n                negidx = np.where(self.weights < 0)[0]\n                znorm = np.zeros((len(negidx), self.dim))\n                for i, idx in enumerate(negidx):\n                    znorm[i] = z_mu[idx] * np.sqrt(-self.weights[idx])\n                C += c_1a * (1/np.linalg.norm(ps)**2 + c_mua) * np.sum(znorm.T @ znorm, axis=1)\n            \n            # Spectral Clipping (Correct negative eigenvalues)\n            C = np.triu(C) + np.triu(C, 1).T  # Enforce symmetry\n            try:\n                evals, evecs = np.linalg.eigh(C)\n                evals = np.maximum(evals, 1e-10)  # Clip small/negative eigenvalues\n                C = evecs @ np.diag(evals) @ evecs.T\n            except np.linalg.LinAlgError:\n                print(\"LinAlgError during spectral clipping, resetting C\")\n                C = np.eye(self.dim)\n\n            # Update success rate\n            self.success_rate = 0.9 * self.success_rate + 0.1 * (successes > 0) # Binary success or failure\n            successes = 0\n\n            # Population based step size adaptation\n            delta_f = np.mean(f) - f_opt # Average fitness of the population minus best fitness\n            if len(step_size_adaptation_history) > 5:\n                # Use the average of the step sizes\n                avg_sigma = np.mean(step_size_adaptation_history[-5:])\n                if delta_f > 0:\n                    sigma = avg_sigma * np.exp(self.learning_rate_sigma * (self.success_rate - 0.2)) # Reduce\n                else:\n                    sigma = avg_sigma * np.exp(self.learning_rate_sigma * (self.success_rate + 0.2)) # Increase\n            else:\n                sigma *= np.exp(self.learning_rate_sigma * (self.success_rate - 0.2))  # Adjust towards target rate of 0.2\n            step_size_adaptation_history.append(sigma)\n            \n            # Dynamic Mean Adaptation: If improvement stagnates, perturb the mean.\n            self.f_opt_history.append(f_opt)\n            self.mean_history.append(mean)\n            if len(self.f_opt_history) > self.history_length:\n                self.f_opt_history.pop(0)\n                self.mean_history.pop(0)\n                \n                change = abs(self.f_opt_history[-1] - self.f_opt_history[0])\n\n                if change < 1e-5: # Very small improvement, consider perturbing.\n                    # Perturb mean, with decaying perturbation size\n                    perturbation = np.random.normal(0, sigma * 0.1, size=self.dim) # Scale with current sigma\n                    mean = self.mean_history[-1] + perturbation # Perturb the mean\n                    \n                    # Clip to bounds\n                    mean = np.clip(mean, func.bounds.lb, func.bounds.ub)\n\n            # Update mean\n            mean = mean_new\n\n            # Orthogonal Subspace Learning (OSL)\n            if len(self.osl_vectors) < self.osl_lambda and eval_count < self.budget:\n                # Add the difference vector to the OSL vector list\n                diff = mean_new - mean\n                self.osl_vectors.append(diff / np.linalg.norm(diff))\n            \n            if len(self.osl_vectors) >= self.osl_lambda and eval_count < self.budget:\n                # Create orthogonal basis\n                Q, R = np.linalg.qr(np.array(self.osl_vectors).T)\n                \n                # Sample in orthogonal subspace\n                z_osl = np.random.normal(0, 1, size=(self.popsize, self.osl_lambda))\n                x_osl = mean + self.osl_alpha * sigma * (Q @ z_osl.T).T\n                \n                # Evaluate OSL samples\n                f_osl = np.array([func(xi) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x_osl)])\n                eval_count += len(x_osl)\n                \n                # Update best solution\n                idx_osl_best = np.argmin(f_osl)\n                if f_osl[idx_osl_best] < f_opt:\n                    f_opt = f_osl[idx_osl_best]\n                    x_opt = x_osl[idx_osl_best]\n                    \n                # Update mean with OSL information.\n                mean = mean_new * (1-self.osl_alpha) + x_osl[idx_osl_best] * self.osl_alpha\n                \n                # Reset OSL vectors\n                self.osl_vectors = []\n\n            # Adaptive C update frequency\n            if len(self.f_opt_history) > self.history_length:\n                # Calculate the change in f_opt\n                change = abs(self.f_opt_history[-1] - self.f_opt_history[0])\n\n                # Adjust the update frequency based on the change.\n                if change < 1e-3: # Stagnation: Reduce frequency.\n                    self.C_update_frequency = int(min(self.C_update_frequency * self.C_update_factor, self.budget))\n                else: # Improvement: Increase frequency\n                    self.C_update_frequency = int(max(self.C_update_frequency / self.C_update_factor, self.popsize))\n\n            # Eigen decomposition of C\n            if eval_count % self.C_update_frequency == 0:  # Re-compute after a dynamic number of generations\n                C = np.triu(C) + np.triu(C, 1).T  # Enforce symmetry\n                try:\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_evals = np.maximum(C_evals, 1e-10) # Avoid zero or negative eigenvalues\n                    C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                except np.linalg.LinAlgError:\n                    print(\"LinAlgError encountered, resetting C\")\n                    C = np.eye(self.dim)\n                \n            if np.any(np.isnan(mean)) or np.any(np.isnan(C)):\n                print(\"NaN detected, resetting...\")\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                sigma = self.sigma0\n        \n        return f_opt, x_opt", "configspace": "", "generation": 6, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 172, in __call__\nValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 5 is different from 2)\n.", "error": "", "parent_ids": ["750a5306-82d8-43df-bda0-02e595718b35"], "operator": null, "metadata": {}}
{"id": "dce1a78a-4739-4500-b240-29b6d11521f9", "fitness": 0.3437835585025845, "name": "AdaptiveCMAES", "description": "Adaptive CMA-ES with step-size control using a success rate, covariance adaptation, dynamic mean perturbation, and restarts based on stagnation detection.", "code": "import numpy as np\n\nclass AdaptiveCMAES:\n    def __init__(self, budget=10000, dim=10, sigma0=0.5, restarts=3):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.restarts = restarts  # Number of restarts\n\n        self.popsize = 4 + int(np.floor(3 * np.log(self.dim)))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n        self.success_rate = 0.2  # Initial success rate\n        self.learning_rate_sigma = 0.2  # Learning rate for step size\n\n        self.C_update_frequency = self.popsize * 5  # Initial frequency\n        self.C_update_factor = 2 # Factor to increase/decrease frequency\n\n        self.f_opt_history = []\n        self.history_length = 10\n        self.mean_history = []\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 20 # Number of iterations with little improvement before restart.\n\n    def __call__(self, func):\n        f_opt_best_overall = np.Inf\n        x_opt_best_overall = None\n\n        for restart in range(self.restarts):\n            # Initialize variables\n            mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n            sigma = self.sigma0\n            C = np.eye(self.dim)  # Covariance matrix\n            pc = np.zeros(self.dim)\n            ps = np.zeros(self.dim)\n            chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n            # Parameters (using common defaults)\n            c_sigma = (self.mueff + 2) / (self.dim + self.mueff + 5)\n            c_c = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n            c_1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n            c_mu = min(1 - c_1, 2 * (self.mueff - 1 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n            d_sigma = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1)\n            c_1a = c_1\n            c_mua = c_mu\n\n            # Eigen decomposition of C (expensive, do it rarely)\n            try:\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_evals = np.maximum(C_evals, 1e-10)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n            except np.linalg.LinAlgError:\n                print(\"Initial LinAlgError encountered, resetting C\")\n                C = np.eye(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n\n            f_opt = np.Inf\n            x_opt = None\n            eval_count = 0\n            successes = 0\n            step_size_adaptation_history = [] # Store past step sizes.\n            self.f_opt_history = [] # Reset history for restarts\n            self.mean_history = []\n            self.stagnation_counter = 0\n\n            while eval_count < self.budget / self.restarts:  # Divide budget across restarts\n\n                # Sample population\n                z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n                x = mean + sigma * (C_sqrt @ z.T).T\n                \n                # Evaluate the new points\n                f = np.array([func(xi) if eval_count + i < self.budget / self.restarts else np.inf for i, xi in enumerate(x)])\n                eval_count += len(x) \n\n                # Sort by fitness\n                idx = np.argsort(f)\n                x = x[idx]\n                z = z[idx]\n                f = f[idx]\n\n                # Update optimal solution\n                if f[0] < f_opt:\n                    f_opt = f[0]\n                    x_opt = x[0]\n                    successes += 1\n                    \n                    if f_opt < f_opt_best_overall:\n                        f_opt_best_overall = f_opt\n                        x_opt_best_overall = x_opt\n                        self.stagnation_counter = 0  # Reset counter if global best improves\n                else:\n                    self.stagnation_counter += 1\n\n                # Selection and recombination\n                x_mu = x[:self.mu]\n                z_mu = z[:self.mu]\n\n                mean_new = np.sum(x_mu * self.weights[:,None], axis=0)\n                z_w = np.sum(z_mu * self.weights[:,None], axis=0)\n                \n                # Covariance matrix adaptation\n                ps = (1 - c_sigma) * ps + np.sqrt(c_sigma * (2 - c_sigma) * self.mueff) * (C_invsqrt @ (mean_new - mean) / sigma)\n                hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - c_sigma)**(2 * eval_count / self.popsize)) < chiN * (1.4 + 2 / (self.dim + 1))\n                \n                pc = (1 - c_c) * pc + hsig * np.sqrt(c_c * (2 - c_c) * self.mueff) * (mean_new - mean) / sigma\n\n                C = (1 - c_1 - c_mu) * C + c_1 * (pc[:, None] @ pc[None, :])\n                for i in range(self.mu):\n                    C += c_mu * self.weights[i] * (z_mu[i, :, None] @ z_mu[i, None, :])\n                    \n                # Active CMA\n                if c_1a > 0 and c_mua > 0:\n                    negidx = np.where(self.weights < 0)[0]\n                    znorm = np.zeros((len(negidx), self.dim))\n                    for i, idx in enumerate(negidx):\n                        znorm[i] = z_mu[idx] * np.sqrt(-self.weights[idx])\n                    C += c_1a * (1/np.linalg.norm(ps)**2 + c_mua) * np.sum(znorm.T @ znorm, axis=1)\n                \n                # Spectral Clipping (Correct negative eigenvalues)\n                C = np.triu(C) + np.triu(C, 1).T  # Enforce symmetry\n                try:\n                    evals, evecs = np.linalg.eigh(C)\n                    evals = np.maximum(evals, 1e-10)  # Clip small/negative eigenvalues\n                    C = evecs @ np.diag(evals) @ evecs.T\n                except np.linalg.LinAlgError:\n                    print(\"LinAlgError during spectral clipping, resetting C\")\n                    C = np.eye(self.dim)\n\n                # Update success rate\n                self.success_rate = 0.9 * self.success_rate + 0.1 * (successes > 0) # Binary success or failure\n                successes = 0\n\n                # Population based step size adaptation\n                delta_f = np.mean(f) - f_opt # Average fitness of the population minus best fitness\n                if len(step_size_adaptation_history) > 5:\n                    # Use the average of the step sizes\n                    avg_sigma = np.mean(step_size_adaptation_history[-5:])\n                    if delta_f > 0:\n                        sigma = avg_sigma * np.exp(self.learning_rate_sigma * (self.success_rate - 0.2)) # Reduce\n                    else:\n                        sigma = avg_sigma * np.exp(self.learning_rate_sigma * (self.success_rate + 0.2)) # Increase\n                else:\n                    sigma *= np.exp(self.learning_rate_sigma * (self.success_rate - 0.2))  # Adjust towards target rate of 0.2\n                step_size_adaptation_history.append(sigma)\n                \n                # Dynamic Mean Adaptation: If improvement stagnates, perturb the mean.\n                self.f_opt_history.append(f_opt)\n                self.mean_history.append(mean)\n                if len(self.f_opt_history) > self.history_length:\n                    self.f_opt_history.pop(0)\n                    self.mean_history.pop(0)\n                    \n                    change = abs(self.f_opt_history[-1] - self.f_opt_history[0])\n\n                    if change < 1e-5: # Very small improvement, consider perturbing.\n                        # Perturb mean, with decaying perturbation size\n                        perturbation = np.random.normal(0, sigma * 0.1, size=self.dim) # Scale with current sigma\n                        mean = self.mean_history[-1] + perturbation # Perturb the mean\n                        \n                        # Clip to bounds\n                        mean = np.clip(mean, func.bounds.lb, func.bounds.ub)\n\n                # Update mean\n                mean = mean_new\n\n                # Adaptive C update frequency\n                if len(self.f_opt_history) > self.history_length:\n                    # Calculate the change in f_opt\n                    change = abs(self.f_opt_history[-1] - self.f_opt_history[0])\n\n                    # Adjust the update frequency based on the change.\n                    if change < 1e-3: # Stagnation: Reduce frequency.\n                        self.C_update_frequency = int(min(self.C_update_frequency * self.C_update_factor, self.budget))\n                    else: # Improvement: Increase frequency\n                        self.C_update_frequency = int(max(self.C_update_frequency / self.C_update_factor, self.popsize))\n\n                # Eigen decomposition of C\n                if eval_count % self.C_update_frequency == 0:  # Re-compute after a dynamic number of generations\n                    C = np.triu(C) + np.triu(C, 1).T  # Enforce symmetry\n                    try:\n                        C_evals, C_evecs = np.linalg.eigh(C)\n                        C_evals = np.maximum(C_evals, 1e-10) # Avoid zero or negative eigenvalues\n                        C = evecs @ np.diag(C_evals) @ evecs.T\n                        C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                        C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                    except np.linalg.LinAlgError:\n                        print(\"LinAlgError encountered, resetting C\")\n                        C = np.eye(self.dim)\n                    \n                if np.any(np.isnan(mean)) or np.any(np.isnan(C)):\n                    print(\"NaN detected, resetting...\")\n                    mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                    C = np.eye(self.dim)\n                    pc = np.zeros(self.dim)\n                    ps = np.zeros(self.dim)\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                    sigma = self.sigma0\n\n                # Stagnation Check: Restart if stagnation is detected.\n                if self.stagnation_counter > self.stagnation_threshold:\n                    print(\"Stagnation detected, restarting...\")\n                    break  # Break out of the inner loop (restart)\n                    \n            # End of inner loop (restart)\n        # End of outer loop (restarts finished)\n        return f_opt_best_overall, x_opt_best_overall", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveCMAES scored 0.344 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["750a5306-82d8-43df-bda0-02e595718b35"], "operator": null, "metadata": {"aucs": [0.1558588086355912, 0.27969376822562686, 0.3573644193665194, 0.18294936384196314, 0.27173931858056766, 0.38709796791964823, 0.28386565392291707, 0.3132618002415738, 0.2616809930279471, 0.17231363579317605, 0.33294168080100495, 0.9888035582390128, 0.2288163332023898, 0.26849816237949675, 0.6663879922415522, 0.34432761621490926, 0.3021037887340474, 0.40011867996027894, 0.19078462063691481, 0.48706300808655323]}}
{"id": "40dc8c81-b37f-4293-82dd-3e74b756acb5", "fitness": 0.5502793056969583, "name": "CoordinateCMAES", "description": "Implements CMA-ES with coordinate-wise step size adaptation, adaptive mirrored sampling, and covariance matrix adaptation with rank-one updates for faster convergence.", "code": "import numpy as np\n\nclass CoordinateCMAES:\n    def __init__(self, budget=10000, dim=10, sigma0=0.5, history_length=5, mirror_ratio=0.5, cs_damp=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.history_length = history_length\n        self.min_popsize = 4\n        self.max_popsize = 50\n        self.target_popsize = min(self.max_popsize, self.min_popsize + int(np.floor(3 * np.log(self.dim))))\n        self.popsize = self.target_popsize\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.step_history = []\n        self.last_f_opt = np.inf\n        self.stagnation_counter = 0\n        self.restart_iterations = 50\n        self.mirror_ratio = mirror_ratio\n        self.cs_damp = cs_damp  # Damping for coordinate-wise step size\n\n    def __call__(self, func):\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = np.ones(self.dim) * self.sigma0  # Coordinate-wise step sizes\n        C = np.eye(self.dim)\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n        c_sigma = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        c_c = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        c_1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        c_mu = min(1 - c_1, 2 * (self.mueff - 1 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        d_sigma = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + c_sigma\n\n        C_evals, C_evecs = np.linalg.eigh(C)\n        C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n        C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n        \n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n\n        while eval_count < self.budget:\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            x = mean + (sigma * (C_sqrt @ z.T).T)\n\n            # Mirrored Sampling\n            num_mirrored = int(self.popsize * self.mirror_ratio)\n            z_mirrored = -z[:num_mirrored]\n            x_mirrored = mean + (sigma * (C_sqrt @ z_mirrored.T).T)\n            x = np.concatenate((x, x_mirrored), axis=0)\n            z = np.concatenate((z, z_mirrored), axis=0)\n\n            f = np.array([func(xi) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n            eval_count += len(x)\n\n            idx = np.argsort(f)\n            x = x[idx]\n            z = z[idx]\n            f = f[idx]\n\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[0]\n                self.stagnation_counter = 0\n            else:\n                self.stagnation_counter += 1\n                \n            if self.stagnation_counter > self.restart_iterations:\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                sigma = np.ones(self.dim) * self.sigma0\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                self.stagnation_counter = 0\n\n            x_mu = x[:self.mu]\n            z_mu = z[:self.mu]\n\n            mean_new = np.sum(x_mu * self.weights[:, None], axis=0)\n            z_w = np.sum(z_mu * self.weights[:, None], axis=0)\n            \n            ps = (1 - c_sigma) * ps + np.sqrt(c_sigma * (2 - c_sigma) * self.mueff) * (C_invsqrt @ ((mean_new - mean) / sigma))\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - c_sigma)**(2 * eval_count / self.popsize)) < chiN * (1.4 + 2 / (self.dim + 1))\n            pc = (1 - c_c) * pc + hsig * np.sqrt(c_c * (2 - c_c) * self.mueff) * ((mean_new - mean) / sigma)\n\n            C = (1 - c_1 - c_mu) * C + c_1 * (pc[:, None] @ pc[None, :])\n            for i in range(self.mu):\n                C += c_mu * self.weights[i] * (z_mu[i, :, None] @ z_mu[i, None, :])\n            \n            sigma *= np.exp((c_sigma / d_sigma) * (np.linalg.norm(ps) / chiN - 1))  # Global step size adaptation\n            sigma *= np.exp(self.cs_damp * pc)  # Adapt coordinate-wise\n\n            # Simplified Step history\n            step = (mean_new - mean) / sigma\n            self.step_history.append(step)\n            if len(self.step_history) > self.history_length:\n                self.step_history.pop(0)\n\n            # Adapt C based on step history, simplified\n            for h_step in self.step_history:\n                C += 0.1 * c_mu * (h_step[:, None] @ h_step[None, :])\n\n            mean = mean_new\n            \n            if eval_count % (self.popsize * 5) == 0:\n                C = np.triu(C) + np.triu(C, 1).T\n                try:\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_evals = np.maximum(C_evals, 1e-10)\n                    C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                except np.linalg.LinAlgError:\n                    C = np.eye(self.dim)\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n\n                # Regularize Covariance Matrix\n                C = C + 1e-8 * np.eye(self.dim)\n                \n            if np.any(np.isnan(mean)) or np.any(np.isnan(C)):\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                sigma = np.ones(self.dim) * self.sigma0\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                sigma = np.ones(self.dim) * self.sigma0\n\n        return f_opt, x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm CoordinateCMAES scored 0.550 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["fd9aa359-87b2-4518-81a4-814479aa4447"], "operator": null, "metadata": {"aucs": [0.28915905715060053, 0.3941591912933795, 0.3478045727262331, 0.9580810249782039, 0.4074677991167378, 0.37968982161044496, 0.5641951634614253, 0.9169422003226606, 0.9260092704176395, 0.1955362455098285, 0.8162563944859319, 0.9194624186633628, 0.2313214496314827, 0.7739534054583234, 0.6337149687864125, 0.4011852879367154, 0.41055217598643723, 0.6949039469022518, 0.19023302553701815, 0.5549586939640729]}}
{"id": "b46e7608-7521-4948-af52-c33d6a764b7b", "fitness": 0.7003765939657454, "name": "AdvancedCMAES", "description": "CMA-ES with adaptive step size and covariance matrix, incorporating mirrored sampling, orthogonal subspace learning, and dynamic population sizing.", "code": "import numpy as np\n\nclass AdvancedCMAES:\n    def __init__(self, budget=10000, dim=10, sigma0=0.5, history_length=5, success_rate_history=10, mirror_rate=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.history_length = history_length\n        self.success_rate_history_length = success_rate_history\n        self.min_popsize = 4\n        self.max_popsize = 50\n        self.target_popsize = min(self.max_popsize, self.min_popsize + int(np.floor(3 * np.log(self.dim))))\n        self.popsize = self.target_popsize\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.step_history = []\n        self.last_f_opt = np.inf\n        self.stagnation_counter = 0\n        self.restart_iterations = 50\n        self.success_history = []\n        self.success_rate = 0.5 # Initial success rate\n        self.success_rate_window = np.array([0.5] * self.success_rate_history_length) # initialize success rate\n        self.mirror_rate = mirror_rate  # Rate of mirrored samples\n        self.orthogonal_basis = np.linalg.qr(np.random.randn(dim, dim))[0] # Initial basis for orthogonal sampling\n\n    def __call__(self, func):\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n        c_sigma = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        c_c = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        c_1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        c_mu = min(1 - c_1, 2 * (self.mueff - 1 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        d_sigma = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + c_sigma\n\n        C_evals, C_evecs = np.linalg.eigh(C)\n        C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n        C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n        \n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n\n        while eval_count < self.budget:\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            \n            # Mirrored sampling\n            z_mirrored = -z[:int(self.popsize * self.mirror_rate)]\n            z = np.concatenate((z, z_mirrored))\n            \n            x = mean + sigma * (C_sqrt @ z.T).T\n            \n            f = np.array([func(xi) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n            eval_count += len(x)\n\n            idx = np.argsort(f)\n            x = x[idx]\n            z = z[idx]\n            f = f[idx]\n\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[0]\n                self.stagnation_counter = 0\n                self.success_history.append(1)\n            else:\n                self.stagnation_counter += 1\n                self.success_history.append(0)\n                \n            if self.stagnation_counter > self.restart_iterations:\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                sigma = self.sigma0\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                self.stagnation_counter = 0\n\n            x_mu = x[:self.mu]\n            z_mu = z[:self.mu]\n\n            mean_new = np.sum(x_mu * self.weights[:, None], axis=0)\n            z_w = np.sum(z_mu * self.weights[:, None], axis=0)\n            \n            ps = (1 - c_sigma) * ps + np.sqrt(c_sigma * (2 - c_sigma) * self.mueff) * (C_invsqrt @ (mean_new - mean) / sigma)\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - c_sigma)**(2 * eval_count / self.popsize)) < chiN * (1.4 + 2 / (self.dim + 1))\n            pc = (1 - c_c) * pc + hsig * np.sqrt(c_c * (2 - c_c) * self.mueff) * (mean_new - mean) / sigma\n\n            C = (1 - c_1 - c_mu) * C + c_1 * (pc[:, None] @ pc[None, :])\n            for i in range(self.mu):\n                C += c_mu * self.weights[i] * (z_mu[i, :, None] @ z_mu[i, None, :])\n            \n            # Adapt step size based on success rate\n            if len(self.success_history) > 0:\n                self.success_rate_window = np.concatenate((self.success_rate_window[1:], [self.success_history[-1]]))\n                self.success_rate = np.mean(self.success_rate_window)\n                \n                if self.success_rate > 0.6:\n                    sigma *= np.exp(0.1)\n                elif self.success_rate < 0.4:\n                    sigma *= np.exp(-0.1)\n\n            # Simplified Step history\n            step = (mean_new - mean) / sigma\n            self.step_history.append(step)\n            if len(self.step_history) > self.history_length:\n                self.step_history.pop(0)\n\n            # Adapt C based on step history, simplified\n            for h_step in self.step_history:\n                C += 0.1 * c_mu * (h_step[:, None] @ h_step[None, :])\n            \n            # Orthogonal Subspace learning\n            if eval_count % (self.popsize * 10) == 0:\n                # Project step history onto the orthogonal basis\n                projected_steps = [self.orthogonal_basis.T @ h_step for h_step in self.step_history]\n                \n                # Update covariance matrix using projected steps\n                for p_step in projected_steps:\n                    C += 0.05 * c_mu * (self.orthogonal_basis @ p_step[:, None] @ p_step[None, :] @ self.orthogonal_basis.T)\n\n            mean = mean_new\n            \n            if eval_count % (self.popsize * 5) == 0:\n                C = np.triu(C) + np.triu(C, 1).T\n                try:\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_evals = np.maximum(C_evals, 1e-10)\n                    C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                except np.linalg.LinAlgError:\n                    C = np.eye(self.dim)\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                \n            if np.any(np.isnan(mean)) or np.any(np.isnan(C)):\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                sigma = self.sigma0\n\n        return f_opt, x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdvancedCMAES scored 0.700 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f44e146f-3561-47d5-8d9e-2a6988c6dce9"], "operator": null, "metadata": {"aucs": [0.2639332738517117, 0.9321771722736829, 0.8901093249959097, 0.8788223971009982, 0.8949756659106489, 0.9327616624106112, 0.3284903676254174, 0.8921702448158102, 0.9070513151141262, 0.16307690475332515, 0.9564774514219655, 0.984847002471813, 0.27330195162620663, 0.8875214097597532, 0.9577392479101515, 0.42502693914060885, 0.8879673009717663, 0.9248631043834505, 0.11888150615584892, 0.5073376366211002]}}
{"id": "2d4890c9-8e91-435b-a727-67b91c01d30c", "fitness": 0.7179346201713568, "name": "RobustCMAES_Pro", "description": "Improved Robust CMA-ES with adaptive step size damping, covariance matrix regularization based on condition number, and a more aggressive stagnation detection mechanism.", "code": "import numpy as np\n\nclass RobustCMAES_Pro(object):\n    def __init__(self, budget=10000, dim=10, sigma0=0.5, history_length=5, mirror_ratio=0.5, restart_strategy=\"random\"):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.history_length = history_length\n        self.min_popsize = 4\n        self.max_popsize = 50\n        self.target_popsize = min(self.max_popsize, self.min_popsize + int(np.floor(3 * np.log(self.dim))))\n        self.popsize = self.target_popsize\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.step_history = []\n        self.last_f_opt = np.inf\n        self.stagnation_counter = 0\n        self.restart_iterations = 10 * self.dim  # More aggressive stagnation detection\n        self.mirror_ratio = mirror_ratio\n        self.restart_strategy = restart_strategy # \"random\" or \"jitter\"\n        self.condition_number_target = 1e14\n        self.damps = 1.0 # Damping factor for step size\n\n    def __call__(self, func):\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n        c_sigma = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        c_c = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        c_1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        c_mu = min(1 - c_1, 2 * (self.mueff - 1 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        d_sigma = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + c_sigma\n\n        C_evals, C_evecs = np.linalg.eigh(C)\n        C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n        C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n        \n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n\n        while eval_count < self.budget:\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            x = mean + sigma * (C_sqrt @ z.T).T\n\n            # Mirrored Sampling\n            num_mirrored = int(self.popsize * self.mirror_ratio)\n            z_mirrored = -z[:num_mirrored]\n            x_mirrored = mean + sigma * (C_sqrt @ z_mirrored.T).T\n            x = np.concatenate((x, x_mirrored), axis=0)\n            z = np.concatenate((z, z_mirrored), axis=0)\n\n            f = np.array([func(xi) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n            eval_count += len(x)\n\n            idx = np.argsort(f)\n            x = x[idx]\n            z = z[idx]\n            f = f[idx]\n\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[0]\n                self.stagnation_counter = 0\n                self.last_f_opt = f[0] #Store for stagnation criterion\n            else:\n                self.stagnation_counter += 1\n            \n            # Restart strategy\n            if self.stagnation_counter > self.restart_iterations:\n                if self.restart_strategy == \"random\":\n                    mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                elif self.restart_strategy == \"jitter\":\n                     mean = mean + 0.1 * np.random.normal(0, 1, size=self.dim) #Jitter the mean\n                     mean = np.clip(mean, func.bounds.lb, func.bounds.ub) #Ensure bounds\n                else:\n                    raise ValueError(\"Invalid restart strategy.\")\n\n                sigma = self.sigma0\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                self.stagnation_counter = 0\n\n            x_mu = x[:self.mu]\n            z_mu = z[:self.mu]\n\n            mean_new = np.sum(x_mu * self.weights[:, None], axis=0)\n            z_w = np.sum(z_mu * self.weights[:, None], axis=0)\n            \n            ps = (1 - c_sigma) * ps + np.sqrt(c_sigma * (2 - c_sigma) * self.mueff) * (C_invsqrt @ (mean_new - mean) / sigma)\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - c_sigma)**(2 * eval_count / self.popsize)) < chiN * (1.4 + 2 / (self.dim + 1))\n            pc = (1 - c_c) * pc + hsig * np.sqrt(c_c * (2 - c_c) * self.mueff) * (mean_new - mean) / sigma\n\n            C = (1 - c_1 - c_mu) * C + c_1 * (pc[:, None] @ pc[None, :])\n            for i in range(self.mu):\n                C += c_mu * self.weights[i] * (z_mu[i, :, None] @ z_mu[i, None, :])\n            \n            sigma *= np.exp((c_sigma / d_sigma) * (np.linalg.norm(ps) / chiN - 1))\n            sigma *= self.damps # Apply damping\n\n            # Simplified Step history\n            step = (mean_new - mean) / sigma\n            self.step_history.append(step)\n            if len(self.step_history) > self.history_length:\n                self.step_history.pop(0)\n\n            # Adapt C based on step history, simplified\n            for h_step in self.step_history:\n                C += 0.1 * c_mu * (h_step[:, None] @ h_step[None, :])\n\n            mean = mean_new\n            \n            if eval_count % (self.popsize * 5) == 0:\n                C = np.triu(C) + np.triu(C, 1).T\n                try:\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_evals = np.maximum(C_evals, 1e-10)\n                    C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n\n                     # Condition number based regularization\n                    condition_number = np.max(C_evals) / np.min(C_evals)\n                    if condition_number > self.condition_number_target:\n                        regularization_factor = 1e-8 * (condition_number / self.condition_number_target)\n                        C = C + regularization_factor * np.eye(self.dim)\n\n                except np.linalg.LinAlgError:\n                    C = np.eye(self.dim)\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n\n                # Regularize Covariance Matrix\n                C = C + 1e-8 * np.eye(self.dim)\n                \n            if np.any(np.isnan(mean)) or np.any(np.isnan(C)):\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                sigma = self.sigma0\n\n        return f_opt, x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm RobustCMAES_Pro scored 0.718 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["fd9aa359-87b2-4518-81a4-814479aa4447"], "operator": null, "metadata": {"aucs": [0.2634646795447787, 0.18639658537671955, 0.9132288306262608, 0.9644432984910501, 0.936342085470184, 0.9525275696674753, 0.9015366480793721, 0.9166859226898353, 0.9276629867635172, 0.4827579023842319, 0.9549075755058046, 0.9930930474147407, 0.26656170074343144, 0.9298861777942561, 0.6369148611535679, 0.9265509543878258, 0.5186490637847935, 0.9473032555570067, 0.19817829463907455, 0.5416009633532106]}}
{"id": "103b0eaa-78d9-49d4-94b2-00a582e60b2b", "fitness": 0.6671802136947731, "name": "AdaptiveMirrorCMAES", "description": "Improves RobustCMAES by dynamically adjusting the mirror ratio based on the success rate of mirrored samples and adaptively adjusting the covariance matrix regularization strength.", "code": "import numpy as np\n\nclass AdaptiveMirrorCMAES:\n    def __init__(self, budget=10000, dim=10, sigma0=0.5, history_length=5, initial_mirror_ratio=0.5, min_mirror_ratio=0.1, max_mirror_ratio=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.history_length = history_length\n        self.min_popsize = 4\n        self.max_popsize = 50\n        self.target_popsize = min(self.max_popsize, self.min_popsize + int(np.floor(3 * np.log(self.dim))))\n        self.popsize = self.target_popsize\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.step_history = []\n        self.last_f_opt = np.inf\n        self.stagnation_counter = 0\n        self.restart_iterations = 50\n        self.mirror_ratio = initial_mirror_ratio\n        self.min_mirror_ratio = min_mirror_ratio\n        self.max_mirror_ratio = max_mirror_ratio\n        self.mirror_success_rate = 0.5\n        self.success_rate_alpha = 0.1  # Learning rate for success rate\n        self.min_eigval = 1e-10\n        self.regularization_strength = 1e-8\n        self.regularization_alpha = 0.1 # Learning rate for regularization strength\n        self.target_condition_number = 1e10\n\n    def __call__(self, func):\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n        c_sigma = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        c_c = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        c_1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        c_mu = min(1 - c_1, 2 * (self.mueff - 1 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        d_sigma = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + c_sigma\n\n        C_evals, C_evecs = np.linalg.eigh(C)\n        C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n        C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n        \n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n\n        while eval_count < self.budget:\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            x = mean + sigma * (C_sqrt @ z.T).T\n\n            # Mirrored Sampling\n            num_mirrored = int(self.popsize * self.mirror_ratio)\n            z_mirrored = -z[:num_mirrored]\n            x_mirrored = mean + sigma * (C_sqrt @ z_mirrored.T).T\n            x = np.concatenate((x, x_mirrored), axis=0)\n            z = np.concatenate((z, z_mirrored), axis=0)\n\n            f = np.array([func(xi) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n            eval_count += len(x)\n\n            idx = np.argsort(f)\n            x = x[idx]\n            z = z[idx]\n            f = f[idx]\n\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[0]\n                self.stagnation_counter = 0\n            else:\n                self.stagnation_counter += 1\n                \n            if self.stagnation_counter > self.restart_iterations:\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                sigma = self.sigma0\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                self.stagnation_counter = 0\n\n            x_mu = x[:self.mu]\n            z_mu = z[:self.mu]\n\n            mean_new = np.sum(x_mu * self.weights[:, None], axis=0)\n            z_w = np.sum(z_mu * self.weights[:, None], axis=0)\n            \n            ps = (1 - c_sigma) * ps + np.sqrt(c_sigma * (2 - c_sigma) * self.mueff) * (C_invsqrt @ (mean_new - mean) / sigma)\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - c_sigma)**(2 * eval_count / self.popsize)) < chiN * (1.4 + 2 / (self.dim + 1))\n            pc = (1 - c_c) * pc + hsig * np.sqrt(c_c * (2 - c_c) * self.mueff) * (mean_new - mean) / sigma\n\n            C = (1 - c_1 - c_mu) * C + c_1 * (pc[:, None] @ pc[None, :])\n            for i in range(self.mu):\n                C += c_mu * self.weights[i] * (z_mu[i, :, None] @ z_mu[i, None, :])\n            \n            sigma *= np.exp((c_sigma / d_sigma) * (np.linalg.norm(ps) / chiN - 1))\n\n            # Simplified Step history\n            step = (mean_new - mean) / sigma\n            self.step_history.append(step)\n            if len(self.step_history) > self.history_length:\n                self.step_history.pop(0)\n\n            # Adapt C based on step history, simplified\n            for h_step in self.step_history:\n                C += 0.1 * c_mu * (h_step[:, None] @ h_step[None, :])\n\n            mean = mean_new\n            \n            if eval_count % (self.popsize * 5) == 0:\n                C = np.triu(C) + np.triu(C, 1).T\n                try:\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_evals = np.maximum(C_evals, self.min_eigval)\n                    C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                except np.linalg.LinAlgError:\n                    C = np.eye(self.dim)\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n\n                # Regularize Covariance Matrix adaptively\n                condition_number = np.max(C_evals) / np.min(C_evals)\n                if condition_number > self.target_condition_number:\n                   self.regularization_strength *= 1.1\n                else:\n                   self.regularization_strength *= 0.9\n                self.regularization_strength = np.clip(self.regularization_strength, 1e-10, 0.1)\n                C = C + self.regularization_strength * np.eye(self.dim)\n                \n            if np.any(np.isnan(mean)) or np.any(np.isnan(C)):\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                sigma = self.sigma0\n\n            #Adapt mirror ratio\n            num_mirrored_actual = len(z_mirrored)\n            num_better_mirrored = np.sum(f[:num_mirrored_actual] < f[num_mirrored_actual:2 * num_mirrored_actual])\n            success_rate = num_better_mirrored / num_mirrored_actual if num_mirrored_actual > 0 else 0.5\n            self.mirror_success_rate = (1 - self.success_rate_alpha) * self.mirror_success_rate + self.success_rate_alpha * success_rate\n            if self.mirror_success_rate > 0.6:\n                self.mirror_ratio = min(self.mirror_ratio + 0.05, self.max_mirror_ratio)\n            elif self.mirror_success_rate < 0.4:\n                self.mirror_ratio = max(self.mirror_ratio - 0.05, self.min_mirror_ratio)\n\n        return f_opt, x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveMirrorCMAES scored 0.667 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["fd9aa359-87b2-4518-81a4-814479aa4447"], "operator": null, "metadata": {"aucs": [0.24698793467482438, 0.22139714531835242, 0.9056930908254187, 0.9577782319748912, 0.9319814242650526, 0.9256298684516544, 0.3229885943855021, 0.9049681328893238, 0.9262427547712805, 0.42903717992838775, 0.9415622576661079, 0.9792490938137232, 0.2533781580562906, 0.9173789649110253, 0.6385247633000476, 0.33363991261574033, 0.8632719362435319, 0.9472225647729094, 0.15802274494995894, 0.5386495200814385]}}
{"id": "93fbc659-a0c3-4a48-8ec9-305de2c6478f", "fitness": 0.5687846227100071, "name": "EnhancedRobustCMAES", "description": "Robust CMA-ES with adaptive step size, covariance regularization using a moving average, and orthogonal subspace search for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedRobustCMAES:\n    def __init__(self, budget=10000, dim=10, sigma0=0.5, history_length=5, mirror_ratio=0.5, subspace_dim=3):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.history_length = history_length\n        self.min_popsize = 4\n        self.max_popsize = 50\n        self.target_popsize = min(self.max_popsize, self.min_popsize + int(np.floor(3 * np.log(self.dim))))\n        self.popsize = self.target_popsize\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.step_history = []\n        self.last_f_opt = np.inf\n        self.stagnation_counter = 0\n        self.restart_iterations = 50\n        self.mirror_ratio = mirror_ratio\n        self.subspace_dim = min(subspace_dim, dim // 2)  # Ensure subspace dimension is valid\n        self.step_size_ema_alpha = 0.2  # EMA alpha for step size adaptation\n        self.sigma_ema = sigma0  # Initialize EMA for sigma\n        self.C_ema_alpha = 0.1  # EMA alpha for covariance matrix regularization\n        self.C_ema = None # Initialize EMA covariance matrix\n        self.success_rate_history = []\n        self.success_rate_window = 10\n        self.target_success_rate = 0.25\n\n    def __call__(self, func):\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n        c_sigma = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        c_c = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        c_1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        c_mu = min(1 - c_1, 2 * (self.mueff - 1 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        d_sigma = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + c_sigma\n\n        C_evals, C_evecs = np.linalg.eigh(C)\n        C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n        C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n\n        if self.C_ema is None:\n            self.C_ema = C\n\n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n\n        while eval_count < self.budget:\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            x = mean + sigma * (C_sqrt @ z.T).T\n\n            # Mirrored Sampling\n            num_mirrored = int(self.popsize * self.mirror_ratio)\n            z_mirrored = -z[:num_mirrored]\n            x_mirrored = mean + sigma * (C_sqrt @ z_mirrored.T).T\n            x = np.concatenate((x, x_mirrored), axis=0)\n            z = np.concatenate((z, z_mirrored), axis=0)\n\n            f = np.array([func(xi) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n            eval_count += len(x)\n\n            idx = np.argsort(f)\n            x = x[idx]\n            z = z[idx]\n            f = f[idx]\n\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[0]\n                self.stagnation_counter = 0\n            else:\n                self.stagnation_counter += 1\n\n            if self.stagnation_counter > self.restart_iterations:\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                sigma = self.sigma0\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                self.stagnation_counter = 0\n\n            x_mu = x[:self.mu]\n            z_mu = z[:self.mu]\n\n            mean_new = np.sum(x_mu * self.weights[:, None], axis=0)\n            z_w = np.sum(z_mu * self.weights[:, None], axis=0)\n\n            ps = (1 - c_sigma) * ps + np.sqrt(c_sigma * (2 - c_sigma) * self.mueff) * (C_invsqrt @ (mean_new - mean) / sigma)\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - c_sigma)**(2 * eval_count / self.popsize)) < chiN * (1.4 + 2 / (self.dim + 1))\n            pc = (1 - c_c) * pc + hsig * np.sqrt(c_c * (2 - c_c) * self.mueff) * (mean_new - mean) / sigma\n\n            C = (1 - c_1 - c_mu) * C + c_1 * (pc[:, None] @ pc[None, :])\n            for i in range(self.mu):\n                C += c_mu * self.weights[i] * (z_mu[i, :, None] @ z_mu[i, None, :])\n\n            C = np.triu(C) + np.triu(C, 1).T\n            try:\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_evals = np.maximum(C_evals, 1e-10)\n                C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n            except np.linalg.LinAlgError:\n                C = np.eye(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n            \n            # EMA Regularization\n            self.C_ema = self.C_ema_alpha * C + (1 - self.C_ema_alpha) * self.C_ema\n            C = self.C_ema\n            \n            sigma_old = sigma\n            sigma *= np.exp((c_sigma / d_sigma) * (np.linalg.norm(ps) / chiN - 1))\n            self.sigma_ema = self.step_size_ema_alpha * sigma + (1 - self.step_size_ema_alpha) * self.sigma_ema\n            sigma = self.sigma_ema\n            \n            # Simplified Step history\n            step = (mean_new - mean) / sigma\n            self.step_history.append(step)\n            if len(self.step_history) > self.history_length:\n                self.step_history.pop(0)\n\n            # Adapt C based on step history, simplified\n            for h_step in self.step_history:\n                C += 0.1 * c_mu * (h_step[:, None] @ h_step[None, :])\n            \n            # Orthogonal Subspace Search\n            Q = np.random.randn(self.dim, self.subspace_dim)\n            Q, _ = np.linalg.qr(Q)  # Orthonormal basis for the subspace\n            \n            # Sample in the subspace\n            z_subspace = np.random.normal(0, 1, size=(self.popsize, self.subspace_dim))\n            x_subspace = mean_new + sigma * (C_sqrt @ (Q @ z_subspace.T)).T\n            f_subspace = np.array([func(xi) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x_subspace)])\n            eval_count += len(x_subspace)\n\n            if np.min(f_subspace) < f_opt:\n                best_idx = np.argmin(f_subspace)\n                f_opt = f_subspace[best_idx]\n                x_opt = x_subspace[best_idx]\n                mean_new = x_subspace[best_idx] # Move mean to the best point in subspace\n                self.stagnation_counter = 0\n\n            mean = mean_new\n\n            # Regularize Covariance Matrix\n            C = C + 1e-8 * np.eye(self.dim)\n\n            if np.any(np.isnan(mean)) or np.any(np.isnan(C)):\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                sigma = self.sigma0\n\n        return f_opt, x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm EnhancedRobustCMAES scored 0.569 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["fd9aa359-87b2-4518-81a4-814479aa4447"], "operator": null, "metadata": {"aucs": [0.2254793479343895, 0.6777495457479268, 0.6509895465365951, 0.8914929676882006, 0.6725702406010552, 0.73248778654344, 0.3389204478391824, 0.568370503336316, 0.6696448382928808, 0.19735412336784608, 0.8620090610163585, 0.9780695103513823, 0.23963093160337234, 0.618337887462586, 0.8816428103024664, 0.690754776524052, 0.5581783306460247, 0.2588693584159152, 0.170244606696345, 0.4928958332938088]}}
{"id": "3b639466-7637-400b-8b30-c5cb8c1dee5f", "fitness": 0.5078625143912643, "name": "DynamicCMAES_PopSize", "description": "Enhance DynamicCMAES with population size adaptation based on success rate and dynamic covariance matrix adaptation using rank-one updates.", "code": "import numpy as np\n\nclass DynamicCMAES_PopSize:\n    def __init__(self, budget=10000, dim=10, sigma0=0.5, history_length=5, success_rate_history=10, min_popsize=4, max_popsize=50):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.history_length = history_length\n        self.success_rate_history_length = success_rate_history\n        self.min_popsize = min_popsize\n        self.max_popsize = max_popsize\n        self.target_popsize = min(self.max_popsize, self.min_popsize + int(np.floor(3 * np.log(self.dim))))\n        self.popsize = self.target_popsize\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.step_history = []\n        self.last_f_opt = np.inf\n        self.stagnation_counter = 0\n        self.restart_iterations = 50\n        self.success_history = []\n        self.success_rate = 0.5 # Initial success rate\n        self.success_rate_window = np.array([0.5] * self.success_rate_history_length) # initialize success rate\n\n    def __call__(self, func):\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n        c_sigma = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        c_c = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        c_1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        c_mu = min(1 - c_1, 2 * (self.mueff - 1 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        d_sigma = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + c_sigma\n\n        C_evals, C_evecs = np.linalg.eigh(C)\n        C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n        C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n        \n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n\n        while eval_count < self.budget:\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            x = mean + sigma * (C_sqrt @ z.T).T\n            \n            f = np.array([func(xi) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n            eval_count += len(x)\n\n            idx = np.argsort(f)\n            x = x[idx]\n            z = z[idx]\n            f = f[idx]\n\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[0]\n                self.stagnation_counter = 0\n                self.success_history.append(1)\n            else:\n                self.stagnation_counter += 1\n                self.success_history.append(0)\n                \n            if self.stagnation_counter > self.restart_iterations:\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                sigma = self.sigma0\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                self.stagnation_counter = 0\n\n            x_mu = x[:self.mu]\n            z_mu = z[:self.mu]\n\n            mean_new = np.sum(x_mu * self.weights[:, None], axis=0)\n            z_w = np.sum(z_mu * self.weights[:, None], axis=0)\n            \n            ps = (1 - c_sigma) * ps + np.sqrt(c_sigma * (2 - c_sigma) * self.mueff) * (C_invsqrt @ (mean_new - mean) / sigma)\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - c_sigma)**(2 * eval_count / self.popsize)) < chiN * (1.4 + 2 / (self.dim + 1))\n            pc = (1 - c_c) * pc + hsig * np.sqrt(c_c * (2 - c_c) * self.mueff) * (mean_new - mean) / sigma\n\n            C = (1 - c_1 - c_mu) * C + c_1 * (pc[:, None] @ pc[None, :])\n            for i in range(self.mu):\n                C += c_mu * self.weights[i] * (z_mu[i, :, None] @ z_mu[i, None, :])\n            \n            # Adapt step size based on success rate\n            if len(self.success_history) > 0:\n                self.success_rate_window = np.concatenate((self.success_rate_window[1:], [self.success_history[-1]]))\n                self.success_rate = np.mean(self.success_rate_window)\n                \n                if self.success_rate > 0.6:\n                    sigma *= np.exp(0.1)\n                    if self.popsize < self.max_popsize:\n                        self.popsize += 1\n                        self.mu = self.popsize // 2\n                        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n                        self.weights = self.weights / np.sum(self.weights)\n                        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n                elif self.success_rate < 0.4:\n                    sigma *= np.exp(-0.1)\n                    if self.popsize > self.min_popsize:\n                        self.popsize -= 1\n                        self.mu = self.popsize // 2\n                        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n                        self.weights = self.weights / np.sum(self.weights)\n                        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n            # Simplified Step history\n            step = (mean_new - mean) / sigma\n            self.step_history.append(step)\n            if len(self.step_history) > self.history_length:\n                self.step_history.pop(0)\n\n            # Adapt C based on step history, simplified. Rank-one updates\n            for h_step in self.step_history:\n                C += 0.1 * c_mu * (h_step[:, None] @ h_step[None, :])\n\n            mean = mean_new\n            \n            if eval_count % (self.popsize * 5) == 0:\n                C = np.triu(C) + np.triu(C, 1).T\n                try:\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_evals = np.maximum(C_evals, 1e-10)\n                    C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                except np.linalg.LinAlgError:\n                    C = np.eye(self.dim)\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                \n            if np.any(np.isnan(mean)) or np.any(np.isnan(C)):\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                sigma = self.sigma0\n\n        return f_opt, x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm DynamicCMAES_PopSize scored 0.508 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f44e146f-3561-47d5-8d9e-2a6988c6dce9"], "operator": null, "metadata": {"aucs": [0.11008437868727483, 0.3510583415640278, 0.6185082648768193, 0.9478088637700497, 0.2619930087380682, 0.8933319836214257, 0.3320768183082603, 0.5484603261345079, 0.4286503244120685, 0.20314837248021078, 0.8399867052707735, 0.9884722264088165, 0.24045564933833896, 0.28936892098767364, 0.6276551837589097, 0.5130570465124146, 0.36355997920004435, 0.9490737739394287, 0.1448954098675178, 0.5056047099486543]}}
{"id": "6d109d13-e16f-4de9-8bdf-7b1a2c8e2519", "fitness": 0.6528381989363544, "name": "DynamicCMAESv2", "description": "Implements a CMA-ES variant with dynamic population size adjustment based on success rate and incorporates a simplified step history for covariance matrix adaptation.", "code": "import numpy as np\n\nclass DynamicCMAESv2:\n    def __init__(self, budget=10000, dim=10, sigma0=0.5, history_length=5, success_rate_history=10):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.history_length = history_length\n        self.success_rate_history_length = success_rate_history\n        self.min_popsize = 4\n        self.max_popsize = 50\n        self.target_popsize = min(self.max_popsize, self.min_popsize + int(np.floor(3 * np.log(self.dim))))\n        self.popsize = self.target_popsize\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.step_history = []\n        self.last_f_opt = np.inf\n        self.stagnation_counter = 0\n        self.restart_iterations = 50\n        self.success_history = []\n        self.success_rate = 0.5 # Initial success rate\n        self.success_rate_window = np.array([0.5] * self.success_rate_history_length) # initialize success rate\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.cc = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 1 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n\n    def __call__(self, func):\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)\n        ps = np.zeros(self.dim)\n        pc = np.zeros(self.dim)\n        \n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n\n        while eval_count < self.budget:\n            # Adjust population size based on success rate\n            if len(self.success_history) > self.success_rate_history_length:\n                self.success_rate_window = np.array(self.success_history[-self.success_rate_history_length:])\n                self.success_rate = np.mean(self.success_rate_window)\n\n                if self.success_rate > 0.7 and self.popsize < self.max_popsize:\n                    self.popsize = min(self.popsize + 1, self.max_popsize)\n                    self.mu = self.popsize // 2\n                    self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n                    self.weights = self.weights / np.sum(self.weights)\n                    self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n                    self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n                    self.cmu = min(1 - self.c1, 2 * (self.mueff - 1 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n\n                elif self.success_rate < 0.3 and self.popsize > self.min_popsize:\n                    self.popsize = max(self.popsize - 1, self.min_popsize)\n                    self.mu = self.popsize // 2\n                    self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n                    self.weights = self.weights / np.sum(self.weights)\n                    self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n                    self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n                    self.cmu = min(1 - self.c1, 2 * (self.mueff - 1 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n            \n            try:\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_evals = np.maximum(C_evals, 1e-10)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n            except np.linalg.LinAlgError:\n                C = np.eye(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            x = mean + sigma * (C_sqrt @ z.T).T\n            \n            f = np.array([func(xi) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n            eval_count += len(x)\n\n            idx = np.argsort(f)\n            x = x[idx]\n            z = z[idx]\n            f = f[idx]\n\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[0]\n                self.stagnation_counter = 0\n                self.success_history.append(1)\n            else:\n                self.stagnation_counter += 1\n                self.success_history.append(0)\n                \n            if self.stagnation_counter > self.restart_iterations:\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                sigma = self.sigma0\n                C = np.eye(self.dim)\n                ps = np.zeros(self.dim)\n                pc = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                self.stagnation_counter = 0\n\n            x_mu = x[:self.mu]\n            z_mu = z[:self.mu]\n\n            mean_new = np.sum(x_mu * self.weights[:, None], axis=0)\n            z_w = np.sum(z_mu * self.weights[:, None], axis=0)\n\n            ps = (1 - self.cs) * ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * (C_invsqrt @ (mean_new - mean) / sigma)\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - self.cs)**(2 * eval_count / self.popsize)) < self.chiN * (1.4 + 2 / (self.dim + 1))\n            pc = (1 - self.cc) * pc + hsig * np.sqrt(self.cc * (2 - self.cc) * self.mueff) * (mean_new - mean) / sigma\n            \n            C = (1 - self.c1 - self.cmu) * C + self.c1 * (pc[:, None] @ pc[None, :])\n            C += self.cmu * np.sum(self.weights[:, None, None] * (z_mu[:, :, None] @ z_mu[:, None, :]), axis=0)\n\n            # Adapt step size based on success rate\n            if len(self.success_history) > 0:\n                self.success_rate_window = np.concatenate((self.success_rate_window[1:], [self.success_history[-1]]))\n                self.success_rate = np.mean(self.success_rate_window)\n                sigma *= np.exp(0.1 * (self.success_rate - 0.5))\n\n            mean = mean_new\n            \n            if eval_count % (self.popsize * 5) == 0:\n                C = np.triu(C) + np.triu(C, 1).T\n                try:\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_evals = np.maximum(C_evals, 1e-10)\n                    C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n                except np.linalg.LinAlgError:\n                    C = np.eye(self.dim)\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n                    \n\n        return f_opt, x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm DynamicCMAESv2 scored 0.653 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f44e146f-3561-47d5-8d9e-2a6988c6dce9"], "operator": null, "metadata": {"aucs": [0.23085525589408695, 0.34316236916088505, 0.9122598230487214, 0.9607595901361449, 0.8977716552568064, 0.9201155946234408, 0.32862509159093556, 0.7683207699317087, 0.9210415246542901, 0.26766563378817987, 0.9536709653362098, 0.9922664828603206, 0.24277067459606738, 0.48065527138327857, 0.8530797365919037, 0.6745665893502443, 0.6708222976819117, 0.9233622280689596, 0.23647309595777466, 0.47851932881521786]}}
{"id": "2743ba55-81b3-452e-b95c-dba6941bcb16", "fitness": 0.534015732603357, "name": "CMAES_AdaptiveMean_StepSize", "description": "Implements CMA-ES with adaptive step size and covariance matrix adaptation, incorporates a history-based step size control, dynamic mean adaptation based on fitness improvement, and an adaptive population size strategy to balance exploration and exploitation.", "code": "import numpy as np\n\nclass CMAES_AdaptiveMean_StepSize:\n    def __init__(self, budget=10000, dim=10, sigma0 = 0.5):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n\n        self.popsize = 4 + int(np.floor(3 * np.log(self.dim)))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n        self.success_rate = 0.2  # Initial success rate\n        self.learning_rate_sigma = 0.2  # Learning rate for step size\n\n        self.C_update_frequency = self.popsize * 5  # Initial frequency\n        self.C_update_factor = 2 # Factor to increase/decrease frequency\n\n        self.f_opt_history = []\n        self.history_length = 10\n        self.mean_history = []\n        \n        self.population_size_history = []\n        self.adapt_popsize = True\n        self.min_popsize = 4 + int(np.floor(3 * np.log(self.dim)))\n        self.max_popsize = 4 + int(np.floor(10 * np.log(self.dim)))\n\n    def __call__(self, func):\n        # Initialize variables\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)  # Covariance matrix\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n        # Parameters (using common defaults)\n        c_sigma = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        c_c = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        c_1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        c_mu = min(1 - c_1, 2 * (self.mueff - 1 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        d_sigma = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1)\n        c_1a = c_1\n        c_mua = c_mu\n\n        # Eigen decomposition of C (expensive, do it rarely)\n        try:\n            C_evals, C_evecs = np.linalg.eigh(C)\n            C_evals = np.maximum(C_evals, 1e-10)\n            C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n            C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n        except np.linalg.LinAlgError:\n            print(\"Initial LinAlgError encountered, resetting C\")\n            C = np.eye(self.dim)\n            C_evals, C_evecs = np.linalg.eigh(C)\n            C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n            C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n\n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n        successes = 0\n        step_size_adaptation_history = [] # Store past step sizes.\n\n        while eval_count < self.budget:\n            # Sample population\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            x = mean + sigma * (C_sqrt @ z.T).T\n            \n            # Evaluate the new points\n            f = np.array([func(xi) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n            eval_count += len(x) \n\n            # Sort by fitness\n            idx = np.argsort(f)\n            x = x[idx]\n            z = z[idx]\n            f = f[idx]\n\n            # Update optimal solution\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[0]\n                successes += 1\n                \n            # Selection and recombination\n            x_mu = x[:self.mu]\n            z_mu = z[:self.mu]\n\n            mean_new = np.sum(x_mu * self.weights[:,None], axis=0)\n            z_w = np.sum(z_mu * self.weights[:,None], axis=0)\n            \n            # Covariance matrix adaptation\n            ps = (1 - c_sigma) * ps + np.sqrt(c_sigma * (2 - c_sigma) * self.mueff) * (C_invsqrt @ (mean_new - mean) / sigma)\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - c_sigma)**(2 * eval_count / self.popsize)) < chiN * (1.4 + 2 / (self.dim + 1))\n            \n            pc = (1 - c_c) * pc + hsig * np.sqrt(c_c * (2 - c_c) * self.mueff) * (mean_new - mean) / sigma\n\n            C = (1 - c_1 - c_mu) * C + c_1 * (pc[:, None] @ pc[None, :])\n            for i in range(self.mu):\n                C += c_mu * self.weights[i] * (z_mu[i, :, None] @ z_mu[i, None, :])\n                \n            # Active CMA\n            if c_1a > 0 and c_mua > 0:\n                negidx = np.where(self.weights < 0)[0]\n                znorm = np.zeros((len(negidx), self.dim))\n                for i, idx in enumerate(negidx):\n                    znorm[i] = z_mu[idx] * np.sqrt(-self.weights[idx])\n                C += c_1a * (1/np.linalg.norm(ps)**2 + c_mua) * np.sum(znorm.T @ znorm, axis=1)\n            \n            # Spectral Clipping (Correct negative eigenvalues)\n            C = np.triu(C) + np.triu(C, 1).T  # Enforce symmetry\n            try:\n                evals, evecs = np.linalg.eigh(C)\n                evals = np.maximum(evals, 1e-10)  # Clip small/negative eigenvalues\n                C = evecs @ np.diag(evals) @ evecs.T\n            except np.linalg.LinAlgError:\n                print(\"LinAlgError during spectral clipping, resetting C\")\n                C = np.eye(self.dim)\n\n            # Update success rate\n            self.success_rate = 0.9 * self.success_rate + 0.1 * (successes > 0) # Binary success or failure\n            successes = 0\n\n            # Population based step size adaptation\n            delta_f = np.mean(f) - f_opt # Average fitness of the population minus best fitness\n            if len(step_size_adaptation_history) > 5:\n                # Use the average of the step sizes\n                avg_sigma = np.mean(step_size_adaptation_history[-5:])\n                if delta_f > 0:\n                    sigma = avg_sigma * np.exp(self.learning_rate_sigma * (self.success_rate - 0.2)) # Reduce\n                else:\n                    sigma = avg_sigma * np.exp(self.learning_rate_sigma * (self.success_rate + 0.2)) # Increase\n            else:\n                sigma *= np.exp(self.learning_rate_sigma * (self.success_rate - 0.2))  # Adjust towards target rate of 0.2\n            step_size_adaptation_history.append(sigma)\n            \n            # Dynamic Mean Adaptation: If improvement stagnates, perturb the mean.\n            self.f_opt_history.append(f_opt)\n            self.mean_history.append(mean)\n            if len(self.f_opt_history) > self.history_length:\n                self.f_opt_history.pop(0)\n                self.mean_history.pop(0)\n                \n                change = abs(self.f_opt_history[-1] - self.f_opt_history[0])\n\n                if change < 1e-5: # Very small improvement, consider perturbing.\n                    # Perturb mean, with decaying perturbation size\n                    perturbation = np.random.normal(0, sigma * 0.1, size=self.dim) # Scale with current sigma\n                    mean = self.mean_history[-1] + perturbation # Perturb the mean\n                    \n                    # Clip to bounds\n                    mean = np.clip(mean, func.bounds.lb, func.bounds.ub)\n\n            # Update mean\n            mean = mean_new\n\n            # Adaptive C update frequency\n            if len(self.f_opt_history) > self.history_length:\n                # Calculate the change in f_opt\n                change = abs(self.f_opt_history[-1] - self.f_opt_history[0])\n\n                # Adjust the update frequency based on the change.\n                if change < 1e-3: # Stagnation: Reduce frequency.\n                    self.C_update_frequency = int(min(self.C_update_frequency * self.C_update_factor, self.budget))\n                else: # Improvement: Increase frequency\n                    self.C_update_frequency = int(max(self.C_update_frequency / self.C_update_factor, self.popsize))\n                    \n            # Population size adaptation\n            if self.adapt_popsize:\n                self.population_size_history.append(f_opt)\n                if len(self.population_size_history) > self.history_length:\n                    self.population_size_history.pop(0)\n                    \n                    improvement = abs(self.population_size_history[-1] - self.population_size_history[0])\n\n                    if improvement < 1e-5: # Stagnation\n                        self.popsize = max(self.min_popsize, self.popsize // 2) # Reduce population size\n                        self.mu = self.popsize // 2\n                        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n                        self.weights = self.weights / np.sum(self.weights)\n                        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n                    else:\n                        self.popsize = min(self.max_popsize, self.popsize * 2) # Increase population size\n                        self.mu = self.popsize // 2\n                        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n                        self.weights = self.weights / np.sum(self.weights)\n                        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n                        \n            # Eigen decomposition of C\n            if eval_count % self.C_update_frequency == 0:  # Re-compute after a dynamic number of generations\n                C = np.triu(C) + np.triu(C, 1).T  # Enforce symmetry\n                try:\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_evals = np.maximum(C_evals, 1e-10) # Avoid zero or negative eigenvalues\n                    C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                except np.linalg.LinAlgError:\n                    print(\"LinAlgError encountered, resetting C\")\n                    C = np.eye(self.dim)\n                \n            if np.any(np.isnan(mean)) or np.any(np.isnan(C)):\n                print(\"NaN detected, resetting...\")\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                sigma = self.sigma0\n        \n        return f_opt, x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm CMAES_AdaptiveMean_StepSize scored 0.534 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["750a5306-82d8-43df-bda0-02e595718b35"], "operator": null, "metadata": {"aucs": [0.23493057859032251, 0.5983398136175443, 0.5141448520420704, 0.7795355706589207, 0.5236580872858057, 0.5738199011582603, 0.3443058629566097, 0.4429924265822329, 0.47044353616835877, 0.4968531093348857, 0.8104907888766228, 0.9897785819025513, 0.23550745822240382, 0.5230013128109061, 0.6232677415822108, 0.6237748549590058, 0.3981508598846981, 0.6979938976787472, 0.2686290811629599, 0.5306963365920203]}}
{"id": "ee0a84f2-5ebb-4fad-928d-809f48880481", "fitness": 0.712885540632602, "name": "CMAES_AdaptiveMirrorRankHyperbolic", "description": "CMA-ES with adaptive mirrored sampling using a hyperbolic decay, rank-based covariance update with a dynamic damping factor, and adaptive step size control based on conjugate evolution paths.", "code": "import numpy as np\n\nclass CMAES_AdaptiveMirrorRankHyperbolic:\n    def __init__(self, budget=10000, dim=10, sigma0=0.5, history_length=10, initial_mirrored_fraction=0.5, mirrored_decay=0.99, hyperbolic_decay_factor=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.history_length = history_length\n        self.mirrored_fraction = initial_mirrored_fraction  # Initial fraction of population to mirror\n        self.mirrored_decay = mirrored_decay  # Decay rate for mirrored fraction\n        self.hyperbolic_decay_factor = hyperbolic_decay_factor\n\n        self.popsize = 4 + int(np.floor(3 * np.log(self.dim)))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.step_history = []\n\n    def __call__(self, func):\n        # Initialize variables\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n        # Parameters (using common defaults)\n        c_sigma = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        c_c = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        c_1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        c_mu = min(1 - c_1, 2 * (self.mueff - 1 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        d_sigma = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1)\n        c_1a = c_1\n        c_mua = c_mu\n        damp_C = 0.2 + c_1 + c_mu  # Damping factor for covariance matrix update\n\n        # Eigen decomposition of C\n        C_evals, C_evecs = np.linalg.eigh(C)\n        C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n        C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n        \n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n\n        while eval_count < self.budget:\n            # Sample population\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            x = mean + sigma * (C_sqrt @ z.T).T\n\n            # Mirrored sampling with hyperbolic decay\n            mirrored_fraction = self.mirrored_fraction / (1 + self.hyperbolic_decay_factor * (eval_count / self.budget))\n\n            num_mirrored = int(self.popsize * mirrored_fraction)\n            x_mirrored = mean - sigma * (C_sqrt @ z[:num_mirrored].T).T\n            x = np.vstack((x, x_mirrored))\n            z_mirrored = -z[:num_mirrored]\n            z = np.vstack((z, z_mirrored))\n            \n            # Clipping to bounds\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            \n            # Evaluate the new points\n            f = np.array([func(xi) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n            eval_count += len(x) \n\n            # Sort by fitness\n            idx = np.argsort(f)\n            x = x[idx]\n            z = z[idx]\n            f = f[idx]\n\n            # Update optimal solution\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[0]\n                \n            # Selection and recombination\n            x_mu = x[:self.mu]\n            z_mu = z[:self.mu]\n\n            mean_new = np.sum(x_mu * self.weights[:,None], axis=0)\n            z_w = np.sum(z_mu * self.weights[:,None], axis=0)\n            \n            # Covariance matrix adaptation using rank-mu update\n            ps = (1 - c_sigma) * ps + np.sqrt(c_sigma * (2 - c_sigma) * self.mueff) * (C_invsqrt @ (mean_new - mean) / sigma)\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - c_sigma)**(2 * eval_count / self.popsize)) < chiN * (1.4 + 2 / (self.dim + 1))\n            \n            pc = (1 - c_c) * pc + hsig * np.sqrt(c_c * (2 - c_c) * self.mueff) * (mean_new - mean) / sigma\n\n            C = (1 - c_1 - c_mu + damp_C * c_1) * C + c_1 * (pc[:, None] @ pc[None, :])\n\n            # Rank-mu update\n            for i in range(self.mu):\n                C += c_mu * self.weights[i] * (z_mu[i, :, None] @ z_mu[i, None, :])\n                \n            # Update step size using conjugate evolution path\n            step = (mean_new - mean) / sigma\n            self.step_history.append(step)\n            if len(self.step_history) > self.history_length:\n                self.step_history.pop(0)\n\n            # Adapt step size based on step history\n            step_correlation = 0\n            for h_step in self.step_history:\n                step_correlation += np.dot(step, h_step)\n            \n            sigma_factor = np.exp(0.1 * step_correlation / self.dim)\n            sigma *= np.exp((c_sigma / d_sigma) * (np.linalg.norm(ps) / chiN - 1)) * sigma_factor\n\n            # Update mean\n            mean = mean_new\n\n            # Adaptive Mirrored fraction (decay moved to start)\n            self.mirrored_fraction *= self.mirrored_decay\n\n            # Eigen decomposition of C\n            if eval_count % (self.popsize * 5) == 0:\n                C = np.triu(C) + np.triu(C, 1).T\n                try:\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_evals = np.maximum(C_evals, 1e-10)\n                    C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                except np.linalg.LinAlgError:\n                    print(\"LinAlgError encountered, resetting C\")\n                    C = np.eye(self.dim)\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                \n            if np.any(np.isnan(mean)) or np.any(np.isnan(C)):\n                print(\"NaN detected, resetting...\")\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                sigma = self.sigma0\n        \n        return f_opt, x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm CMAES_AdaptiveMirrorRankHyperbolic scored 0.713 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a813641a-f575-4987-a2e8-75be0d8f88b2"], "operator": null, "metadata": {"aucs": [0.2675594009526767, 0.9427816011289487, 0.928816401277717, 0.9633438251634693, 0.9522231348479799, 0.9502245704670967, 0.33737362520960357, 0.7388108772562747, 0.9493014395953653, 0.9364823167273901, 0.9663986308002485, 0.9929991685382524, 0.24155409059287747, 0.9475784203449928, 0.5859301043888154, 0.38493344780518157, 0.4551487740716613, 0.963546979300132, 0.23190592481220607, 0.5207980793711524]}}
{"id": "652a0ee5-9606-46be-afc8-4bdc23d3a249", "fitness": 0.6996047285647239, "name": "CMAES_AdaptiveMirrorRank_ActiveAdaptation", "description": "Adaptively adjusts mirrored sampling rate, uses a rank-based covariance update with active covariance matrix adaptation to better handle rugged landscapes, and dynamically adjusts the step size based on a success rate.", "code": "import numpy as np\n\nclass CMAES_AdaptiveMirrorRank_ActiveAdaptation:\n    def __init__(self, budget=10000, dim=10, sigma0=0.5, history_length=10, initial_mirrored_fraction=0.5, mirrored_decay=0.99, active_adaptation_multiplier=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.history_length = history_length\n        self.mirrored_fraction = initial_mirrored_fraction\n        self.mirrored_decay = mirrored_decay\n        self.active_adaptation_multiplier = active_adaptation_multiplier\n\n        self.popsize = 4 + int(np.floor(3 * np.log(self.dim)))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.step_history = []\n        self.success_rate_history = []\n        self.success_rate_window = 10\n\n    def __call__(self, func):\n        # Initialize variables\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n        # Parameters (using common defaults)\n        c_sigma = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        c_c = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        c_1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        c_mu = min(1 - c_1, 2 * (self.mueff - 1 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        d_sigma = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1)\n        c_1a = c_1\n        c_mua = c_mu\n\n        # Eigen decomposition of C\n        C_evals, C_evecs = np.linalg.eigh(C)\n        C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n        C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n        \n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n        successes = 0\n\n        while eval_count < self.budget:\n            # Sample population\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            x = mean + sigma * (C_sqrt @ z.T).T\n\n            # Mirrored sampling\n            num_mirrored = int(self.popsize * self.mirrored_fraction)\n            x_mirrored = mean - sigma * (C_sqrt @ z[:num_mirrored].T).T\n            x = np.vstack((x, x_mirrored))\n            z_mirrored = -z[:num_mirrored]\n            z = np.vstack((z, z_mirrored))\n            \n            # Clipping to bounds\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            \n            # Evaluate the new points\n            f = np.array([func(xi) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n            eval_count += len(x) \n\n            # Sort by fitness\n            idx = np.argsort(f)\n            x = x[idx]\n            z = z[idx]\n            f = f[idx]\n\n            # Update optimal solution\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[0]\n                successes += 1 # increment successful steps\n                \n            # Selection and recombination\n            x_mu = x[:self.mu]\n            z_mu = z[:self.mu]\n\n            mean_new = np.sum(x_mu * self.weights[:,None], axis=0)\n            z_w = np.sum(z_mu * self.weights[:,None], axis=0)\n            \n            # Covariance matrix adaptation using rank-mu update\n            ps = (1 - c_sigma) * ps + np.sqrt(c_sigma * (2 - c_sigma) * self.mueff) * (C_invsqrt @ (mean_new - mean) / sigma)\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - c_sigma)**(2 * eval_count / self.popsize)) < chiN * (1.4 + 2 / (self.dim + 1))\n            \n            pc = (1 - c_c) * pc + hsig * np.sqrt(c_c * (2 - c_c) * self.mueff) * (mean_new - mean) / sigma\n\n            C = (1 - c_1 - c_mu + self.active_adaptation_multiplier * c_1 * (1 - hsig**2)) * C + c_1 * (pc[:, None] @ pc[None, :])\n\n            # Rank-mu update\n            for i in range(self.mu):\n                C += c_mu * self.weights[i] * (z_mu[i, :, None] @ z_mu[i, None, :])\n                \n            # Active covariance matrix adaptation: encourage/discourage steps\n            if f[0] > f_opt:\n                 C -= self.active_adaptation_multiplier * c_1 * (pc[:, None] @ pc[None, :])  # Discourage step if fitness worsened\n            \n            # Update step size: using adaptive success rate\n            success_rate = successes / (eval_count / self.popsize)\n            self.success_rate_history.append(success_rate)\n    \n            if len(self.success_rate_history) > self.success_rate_window:\n                self.success_rate_history.pop(0)\n    \n            avg_success_rate = np.mean(self.success_rate_history)\n            \n            # Adjust sigma based on success rate\n            if avg_success_rate > 0.25:\n                sigma *= 1.1  # Increase step-size if doing well\n            elif avg_success_rate < 0.15:\n                sigma *= 0.9  # Decrease step-size if not improving\n\n            # Step history adaptation\n            step = (mean_new - mean) / sigma\n            self.step_history.append(step)\n            if len(self.step_history) > self.history_length:\n                self.step_history.pop(0)\n\n            # Adapt C based on step history (dampened)\n            for h_step in self.step_history:\n                C += 0.1 * c_mu * (h_step[:, None] @ h_step[None, :])  # Dampened update\n                \n            # Update mean\n            mean = mean_new\n\n            # Adaptive Mirrored fraction\n            self.mirrored_fraction *= self.mirrored_decay\n\n            # Eigen decomposition of C\n            if eval_count % (self.popsize * 5) == 0:\n                C = np.triu(C) + np.triu(C, 1).T\n                try:\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_evals = np.maximum(C_evals, 1e-10)\n                    C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                except np.linalg.LinAlgError:\n                    print(\"LinAlgError encountered, resetting C\")\n                    C = np.eye(self.dim)\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                \n            if np.any(np.isnan(mean)) or np.any(np.isnan(C)):\n                print(\"NaN detected, resetting...\")\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                sigma = self.sigma0\n                successes = 0\n                self.success_rate_history = []\n        \n        return f_opt, x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm CMAES_AdaptiveMirrorRank_ActiveAdaptation scored 0.700 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a813641a-f575-4987-a2e8-75be0d8f88b2"], "operator": null, "metadata": {"aucs": [0.22327397011101802, 0.8579228978069111, 0.8514208947508926, 0.9154799606116829, 0.8500191899663236, 0.870342159287937, 0.3162760950580593, 0.6257153727875338, 0.8571638461624821, 0.8864580211979881, 0.9308280772153683, 0.9954855546730786, 0.27398793988527503, 0.8104708827679602, 0.7201477206707927, 0.6565140191651775, 0.6711791127550057, 0.8931660775655981, 0.2952759133090853, 0.4909668655463084]}}
{"id": "2674758b-6674-4aa9-b6a6-4ce62296eab4", "fitness": 0.44311076839771857, "name": "CMAES_SimplifiedCovariance", "description": "CMA-ES with dynamic mirrored sampling, rank-based covariance update with active adaptation, step size adaptation based on conjugate evolution paths, and a simplified covariance matrix adaptation.", "code": "import numpy as np\n\nclass CMAES_SimplifiedCovariance:\n    def __init__(self, budget=10000, dim=10, sigma0=0.5, initial_mirrored_fraction=0.5, mirrored_decay=0.99, active_adaptation_multiplier=0.1, cs=0.3, cc=0.4):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.mirrored_fraction = initial_mirrored_fraction\n        self.mirrored_decay = mirrored_decay\n        self.active_adaptation_multiplier = active_adaptation_multiplier\n        self.cs = cs\n        self.cc = cc\n\n        self.popsize = 4 + int(np.floor(3 * np.log(self.dim)))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.success_history = []\n        self.success_window = 10\n\n    def __call__(self, func):\n        # Initialize variables\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        p_sigma = np.zeros(self.dim)\n        C = np.eye(self.dim)\n\n        # Parameters (simplified)\n        damps = 1 + self.cs + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1)\n        chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n        successes = 0\n\n        while eval_count < self.budget:\n            # Sample population\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            x = mean + sigma * z\n\n            # Mirrored sampling\n            num_mirrored = int(self.popsize * self.mirrored_fraction)\n            x_mirrored = mean - sigma * z[:num_mirrored]\n            x = np.vstack((x, x_mirrored))\n            z_mirrored = -z[:num_mirrored]\n            z = np.vstack((z, z_mirrored))\n\n            # Clipping to bounds\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n\n            # Evaluate the new points\n            f = np.array([func(xi) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n            eval_count += len(x)\n\n            # Sort by fitness\n            idx = np.argsort(f)\n            x = x[idx]\n            z = z[idx]\n            f = f[idx]\n\n            # Update optimal solution\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[0]\n                successes += 1\n\n            # Selection and recombination\n            x_mu = x[:self.mu]\n            z_mu = z[:self.mu]\n            mean_new = np.sum(x_mu * self.weights[:, None], axis=0)\n            z_w = np.sum(z_mu * self.weights[:, None], axis=0)\n\n            # Simplified covariance matrix adaptation\n            p_sigma = (1 - self.cs) * p_sigma + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * z_w\n            C = (1 - self.cc) * C + self.cc * (p_sigma[:, None] @ p_sigma[None, :])\n\n            # Active Covariance Adaptation\n            if f[0] > f_opt:\n                C -= self.active_adaptation_multiplier * self.cc * (p_sigma[:, None] @ p_sigma[None, :])\n\n            # Update step size\n            sigma *= np.exp((self.cs / damps) * (np.linalg.norm(p_sigma) / chiN - 1))\n            sigma = max(sigma, 1e-10)\n\n            # Update mean\n            mean = mean_new\n\n            # Adaptive Mirrored fraction\n            self.mirrored_fraction *= self.mirrored_decay\n\n            # Repair covariance matrix if needed\n            if eval_count % (self.popsize * 5) == 0:\n                 C = np.triu(C) + np.triu(C, 1).T\n                 try:\n                     C_evals, C_evecs = np.linalg.eigh(C)\n                     C_evals = np.maximum(C_evals, 1e-10)\n                     C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n                 except np.linalg.LinAlgError:\n                     C = np.eye(self.dim)\n\n            if np.any(np.isnan(mean)) or np.any(np.isnan(C)):\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                C = np.eye(self.dim)\n                p_sigma = np.zeros(self.dim)\n                sigma = self.sigma0\n                successes = 0\n\n        return f_opt, x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm CMAES_SimplifiedCovariance scored 0.443 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["652a0ee5-9606-46be-afc8-4bdc23d3a249"], "operator": null, "metadata": {"aucs": [0.26134850874056015, 0.9238046083810323, 0.9090136086063495, 0.19430658185101246, 0.1717106211423275, 0.19944419340225483, 0.30735846281623624, 0.17630474526715556, 0.935444102766302, 0.14835433935777997, 0.942238270948476, 0.9883872812974479, 0.22650191084910398, 0.20691772203393033, 0.5499960107026693, 0.3922155595352267, 0.4494489611127317, 0.2135046550665245, 0.21150450119248732, 0.4544107228847625]}}
{"id": "90ac9ca4-7d93-410b-8850-dede06d98999", "fitness": 0.5512472672103054, "name": "SimplifiedCMAES", "description": "CMA-ES with a simplified covariance matrix adaptation based on rank-one updates and a modified step-size adaptation using a cumulative success rate.", "code": "import numpy as np\n\nclass SimplifiedCMAES:\n    def __init__(self, budget=10000, dim=10, sigma0=0.5, success_rate_history=10):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.popsize = 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.c_sigma = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.d_sigma = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.c_sigma\n        self.success_history_length = success_rate_history\n        self.success_rate_window = np.array([0.5] * self.success_history_length)\n        self.success_rate = 0.5\n        self.stagnation_threshold = 50\n        self.stagnation_counter = 0\n\n    def __call__(self, func):\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)\n        f_opt = np.inf\n        x_opt = None\n        eval_count = 0\n        ps = np.zeros(self.dim)\n\n        while eval_count < self.budget:\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            x = mean + sigma * (np.linalg.cholesky(C) @ z.T).T\n            \n            f = np.array([func(xi) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n            eval_count += len(x)\n            \n            idx = np.argsort(f)\n            x = x[idx]\n            z = z[idx]\n            f = f[idx]\n\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[0]\n                self.stagnation_counter = 0\n                self.success_history_update(1)\n            else:\n                self.stagnation_counter += 1\n                self.success_history_update(0)\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                sigma = self.sigma0\n                C = np.eye(self.dim)\n                ps = np.zeros(self.dim)\n                self.stagnation_counter = 0\n                self.success_rate = 0.5\n                self.success_rate_window = np.array([0.5] * self.success_history_length)\n\n            x_mu = x[:self.mu]\n            z_mu = z[:self.mu]\n            mean_new = np.sum(x_mu * self.weights[:, None], axis=0)\n            z_w = np.sum(z_mu * self.weights[:, None], axis=0)\n\n            ps = (1 - self.c_sigma) * ps + np.sqrt(self.c_sigma * (2 - self.c_sigma)) * z_w\n            C = (1 - self.c_sigma) * C + self.c_sigma * np.outer(ps, ps)\n\n            sigma *= np.exp(0.1 * (self.success_rate - 0.5) / self.d_sigma)\n\n            mean = mean_new\n\n            try:\n                C = np.triu(C) + np.triu(C, 1).T\n                C_evals = np.linalg.eigvalsh(C)\n                C_evals = np.maximum(C_evals, 1e-10)\n                C = np.linalg.eigh(C)[1] @ np.diag(C_evals) @ np.linalg.eigh(C)[1].T\n            except np.linalg.LinAlgError:\n                C = np.eye(self.dim)\n\n        return f_opt, x_opt\n    \n    def success_history_update(self, success):\n        self.success_rate_window = np.concatenate((self.success_rate_window[1:], [success]))\n        self.success_rate = np.mean(self.success_rate_window)", "configspace": "", "generation": 7, "feedback": "The algorithm SimplifiedCMAES scored 0.551 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b46e7608-7521-4948-af52-c33d6a764b7b"], "operator": null, "metadata": {"aucs": [0.24722360049437564, 0.11398128097592175, 0.8469530001276834, 0.5329519645716612, 0.8536850219115384, 0.8912962284873058, 0.3069407328650179, 0.820842246648047, 0.8664584120389469, 0.21503186315254308, 0.8933467834354873, 0.9706466049461369, 0.21946916437519493, 0.8682611386930328, 0.8212677381188299, 0.6038931443407567, 0.44482913117475653, 0.17297038549286714, 0.16643213075091257, 0.168464771605091]}}
{"id": "e00ef585-ae18-4cad-9648-bf64037b7b97", "fitness": 0.6980373863051405, "name": "AdvancedCMAES_OrthoPopSize", "description": "CMA-ES with adaptive population sizing, spectral decay covariance matrix adaptation, and orthogonal learning to improve exploration.", "code": "import numpy as np\n\nclass AdvancedCMAES_OrthoPopSize(object):\n    def __init__(self, budget=10000, dim=10, sigma0=0.5, history_length=5, mirror_ratio=0.5, restart_strategy=\"random\"):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.history_length = history_length\n        self.min_popsize = 4\n        self.max_popsize = 50\n        self.target_popsize = min(self.max_popsize, self.min_popsize + int(np.floor(3 * np.log(self.dim))))\n        self.popsize = self.target_popsize\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.step_history = []\n        self.last_f_opt = np.inf\n        self.stagnation_counter = 0\n        self.restart_iterations = 10 * self.dim  # More aggressive stagnation detection\n        self.mirror_ratio = mirror_ratio\n        self.restart_strategy = restart_strategy # \"random\" or \"jitter\"\n        self.condition_number_target = 1e14\n        self.damps = 1.0 # Damping factor for step size\n        self.cs = 0.3  # Step-size learning rate\n        self.cc = 0.4  # Cumulation for covariance matrix rank-one update\n        self.c1 = 1.5 / (self.dim + 2)**2  # Learning rate for rank-one update\n        self.cmu = 1.5 * self.mueff / (self.dim + 2)**2  # Learning rate for rank-mu update\n        self.init_lambda = 2 + np.floor(3 * np.log(self.dim))\n        self.lambda_factor = 2\n        self.orthogonal_basis = np.random.randn(self.dim, self.dim)\n        self.orthogonal_basis, _ = np.linalg.qr(self.orthogonal_basis)\n\n    def __call__(self, func):\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n        C_evals, C_evecs = np.linalg.eigh(C)\n        C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n        C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n        \n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n\n        while eval_count < self.budget:\n            # Adapt population size\n            self.target_popsize = min(self.max_popsize, self.min_popsize + int(np.floor(3 * np.log(self.dim))))\n            if self.target_popsize != self.popsize:\n                self.popsize = self.target_popsize\n                self.mu = self.popsize // 2\n                self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n                self.weights = self.weights / np.sum(self.weights)\n                self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n                self.cmu = 1.5 * self.mueff / (self.dim + 2)**2\n\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            x = mean + sigma * (C_sqrt @ z.T).T\n\n            # Mirrored Sampling\n            num_mirrored = int(self.popsize * self.mirror_ratio)\n            z_mirrored = -z[:num_mirrored]\n            x_mirrored = mean + sigma * (C_sqrt @ z_mirrored.T).T\n            x = np.concatenate((x, x_mirrored), axis=0)\n            z = np.concatenate((z, z_mirrored), axis=0)\n\n            f = np.array([func(xi) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n            eval_count += len(x)\n\n            idx = np.argsort(f)\n            x = x[idx]\n            z = z[idx]\n            f = f[idx]\n\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[0]\n                self.stagnation_counter = 0\n                self.last_f_opt = f[0] #Store for stagnation criterion\n            else:\n                self.stagnation_counter += 1\n            \n            # Restart strategy\n            if self.stagnation_counter > self.restart_iterations:\n                if self.restart_strategy == \"random\":\n                    mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                elif self.restart_strategy == \"jitter\":\n                     mean = mean + 0.1 * np.random.normal(0, 1, size=self.dim) #Jitter the mean\n                     mean = np.clip(mean, func.bounds.lb, func.bounds.ub) #Ensure bounds\n                else:\n                    raise ValueError(\"Invalid restart strategy.\")\n\n                sigma = self.sigma0\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                self.stagnation_counter = 0\n\n            x_mu = x[:self.mu]\n            z_mu = z[:self.mu]\n\n            mean_new = np.sum(x_mu * self.weights[:, None], axis=0)\n            z_w = np.sum(z_mu * self.weights[:, None], axis=0)\n            \n            ps = (1 - self.cs) * ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * (C_invsqrt @ (mean_new - mean) / sigma)\n            \n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - self.cs)**(2 * eval_count / self.popsize)) < chiN * (1.4 + 2 / (self.dim + 1))\n            \n            pc = (1 - self.cc) * pc + hsig * np.sqrt(self.cc * (2 - self.cc) * self.mueff) * (mean_new - mean) / sigma\n\n            # Covariance matrix update with spectral decay\n            C = (1 - self.c1 - self.cmu) * C + self.c1 * (pc[:, None] @ pc[None, :])\n            for i in range(self.mu):\n                C += self.cmu * self.weights[i] * (z_mu[i, :, None] @ z_mu[i, None, :])\n\n            # Ensure symmetry\n            C = np.triu(C) + np.triu(C, 1).T\n            \n            sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(ps) / chiN - 1))\n            sigma *= self.damps # Apply damping\n            \n            # Orthogonal subspace learning every 10 generations\n            if eval_count % (10 * self.popsize) == 0:\n                subspace_size = min(self.dim // 2, 10)  # Adjust subspace size as needed\n                Q = self.orthogonal_basis[:, :subspace_size]\n                y = Q.T @ (mean_new - mean) #Project into subspace\n                C_subspace = Q.T @ C @ Q #Project covariance matrix\n                try:\n                    C_evals_sub, C_evecs_sub = np.linalg.eigh(C_subspace) #Eigen-decomposition\n                    Q_update = Q @ C_evecs_sub #Update vectors\n                    self.orthogonal_basis[:, :subspace_size] = Q_update #Update orthogonal basis\n                except np.linalg.LinAlgError:\n                    pass #Do not update\n\n            # Regularize and update covariance matrix\n            if eval_count % (self.popsize * 5) == 0:\n                C = np.triu(C) + np.triu(C, 1).T\n                try:\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_evals = np.maximum(C_evals, 1e-10)\n\n                    # Spectral Decay: Shrink smaller eigenvalues\n                    C_evals = C_evals * (1 - 0.01 * np.exp(-np.arange(self.dim)))\n\n                    C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n\n                     # Condition number based regularization\n                    condition_number = np.max(C_evals) / np.min(C_evals)\n                    if condition_number > self.condition_number_target:\n                        regularization_factor = 1e-8 * (condition_number / self.condition_number_target)\n                        C = C + regularization_factor * np.eye(self.dim)\n\n                except np.linalg.LinAlgError:\n                    C = np.eye(self.dim)\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n\n                # Regularize Covariance Matrix\n                C = C + 1e-8 * np.eye(self.dim)\n                \n            if np.any(np.isnan(mean)) or np.any(np.isnan(C)):\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                sigma = self.sigma0\n\n            mean = mean_new\n\n        return f_opt, x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdvancedCMAES_OrthoPopSize scored 0.698 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2d4890c9-8e91-435b-a727-67b91c01d30c"], "operator": null, "metadata": {"aucs": [0.2587963484222675, 0.22685240918683802, 0.941753694390515, 0.9535662158912972, 0.9362653201286183, 0.9485740835896415, 0.35140554406716384, 0.9260738436286459, 0.9400783570101325, 0.47985400226073616, 0.9480520905850359, 0.9920849942623043, 0.2284049933270481, 0.9354980696577643, 0.9585400810666246, 0.3822990091775962, 0.9038860334352065, 0.949633798690374, 0.1999762835421447, 0.49915255378285406]}}
{"id": "91171688-0458-4793-b5fa-87c522d891c5", "fitness": 0.6919381763224111, "name": "AdaptiveCMAES", "description": "CMA-ES with adaptive mirrored sampling using a sigmoid function, dynamic rank-based covariance update with a Tikhonov regularization, adaptive step size control, and an adaptive population size strategy based on the condition number of the covariance matrix.", "code": "import numpy as np\n\nclass AdaptiveCMAES:\n    def __init__(self, budget=10000, dim=10, sigma0=0.5, history_length=5, success_rate_history=10, mirror_rate=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.history_length = history_length\n        self.success_rate_history_length = success_rate_history\n        self.min_popsize = 4\n        self.max_popsize = 50\n        self.target_popsize = min(self.max_popsize, self.min_popsize + int(np.floor(3 * np.log(self.dim))))\n        self.popsize = self.target_popsize\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.step_history = []\n        self.last_f_opt = np.inf\n        self.stagnation_counter = 0\n        self.restart_iterations = 50\n        self.success_history = []\n        self.success_rate = 0.5 # Initial success rate\n        self.success_rate_window = np.array([0.5] * self.success_rate_history_length) # initialize success rate\n        self.mirror_rate = mirror_rate  # Rate of mirrored samples\n        self.orthogonal_basis = np.linalg.qr(np.random.randn(dim, dim))[0] # Initial basis for orthogonal sampling\n        self.condition_number_history = []\n\n    def __call__(self, func):\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n        c_sigma = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        c_c = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        c_1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        c_mu = min(1 - c_1, 2 * (self.mueff - 1 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        d_sigma = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + c_sigma\n\n        C_evals, C_evecs = np.linalg.eigh(C)\n        C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n        C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n        \n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n\n        while eval_count < self.budget:\n            # Population size adaptation based on condition number\n            if len(self.condition_number_history) > 5:\n                recent_condition_numbers = self.condition_number_history[-5:]\n                if np.mean(recent_condition_numbers) > 1e6:\n                    self.popsize = max(self.min_popsize, self.popsize // 2) # Reduce population size\n                elif np.mean(recent_condition_numbers) < 1e3 and self.popsize < self.max_popsize:\n                    self.popsize = min(self.max_popsize, self.popsize + 2)  # Increase population size\n                self.mu = self.popsize // 2\n                self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n                self.weights = self.weights / np.sum(self.weights)\n                self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n                c_mu = min(1 - c_1, 2 * (self.mueff - 1 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n\n\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            \n            # Mirrored sampling with adaptive rate using sigmoid function\n            mirror_probabilities = 1 / (1 + np.exp(-((np.linalg.norm(z, axis=1) - np.sqrt(self.dim))))) # Adaptive mirror rate\n            mirrored_indices = np.random.rand(self.popsize) < mirror_probabilities\n            z_mirrored = -z[mirrored_indices]\n            z = np.concatenate((z, z_mirrored))\n            \n            x = mean + sigma * (C_sqrt @ z.T).T\n            \n            f = np.array([func(xi) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n            eval_count += len(x)\n\n            idx = np.argsort(f)\n            x = x[idx]\n            z = z[idx]\n            f = f[idx]\n\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[0]\n                self.stagnation_counter = 0\n                self.success_history.append(1)\n            else:\n                self.stagnation_counter += 1\n                self.success_history.append(0)\n                \n            if self.stagnation_counter > self.restart_iterations:\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                sigma = self.sigma0\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                self.stagnation_counter = 0\n\n            x_mu = x[:self.mu]\n            z_mu = z[:self.mu]\n\n            mean_new = np.sum(x_mu * self.weights[:, None], axis=0)\n            z_w = np.sum(z_mu * self.weights[:, None], axis=0)\n            \n            ps = (1 - c_sigma) * ps + np.sqrt(c_sigma * (2 - c_sigma) * self.mueff) * (C_invsqrt @ (mean_new - mean) / sigma)\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - c_sigma)**(2 * eval_count / self.popsize)) < chiN * (1.4 + 2 / (self.dim + 1))\n            pc = (1 - c_c) * pc + hsig * np.sqrt(c_c * (2 - c_c) * self.mueff) * (mean_new - mean) / sigma\n\n            C = (1 - c_1 - c_mu) * C + c_1 * (pc[:, None] @ pc[None, :])\n            for i in range(self.mu):\n                C += c_mu * self.weights[i] * (z_mu[i, :, None] @ z_mu[i, None, :])\n            \n            # Adapt step size based on success rate\n            if len(self.success_history) > 0:\n                self.success_rate_window = np.concatenate((self.success_rate_window[1:], [self.success_history[-1]]))\n                self.success_rate = np.mean(self.success_rate_window)\n                \n                if self.success_rate > 0.6:\n                    sigma *= np.exp(0.1)\n                elif self.success_rate < 0.4:\n                    sigma *= np.exp(-0.1)\n\n            # Simplified Step history\n            step = (mean_new - mean) / sigma\n            self.step_history.append(step)\n            if len(self.step_history) > self.history_length:\n                self.step_history.pop(0)\n\n            # Adapt C based on step history, simplified\n            for h_step in self.step_history:\n                C += 0.1 * c_mu * (h_step[:, None] @ h_step[None, :])\n            \n            # Orthogonal Subspace learning\n            if eval_count % (self.popsize * 10) == 0:\n                # Project step history onto the orthogonal basis\n                projected_steps = [self.orthogonal_basis.T @ h_step for h_step in self.step_history]\n                \n                # Update covariance matrix using projected steps\n                for p_step in projected_steps:\n                    C += 0.05 * c_mu * (self.orthogonal_basis @ p_step[:, None] @ p_step[None, :] @ self.orthogonal_basis.T)\n\n            mean = mean_new\n            \n            if eval_count % (self.popsize * 5) == 0:\n                C = np.triu(C) + np.triu(C, 1).T\n                try:\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_evals = np.maximum(C_evals, 1e-10)\n                    C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                    \n                    # Tikhonov regularization to prevent ill-conditioning\n                    trace = np.trace(C)\n                    C = C + np.eye(self.dim) * 1e-12 * trace\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    condition_number = np.max(C_evals) / np.min(C_evals)\n                    self.condition_number_history.append(condition_number)\n\n                except np.linalg.LinAlgError:\n                    C = np.eye(self.dim)\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                    self.condition_number_history.append(1.0)\n                \n            if np.any(np.isnan(mean)) or np.any(np.isnan(C)):\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                sigma = self.sigma0\n                self.condition_number_history.append(1.0)\n\n        return f_opt, x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveCMAES scored 0.692 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b46e7608-7521-4948-af52-c33d6a764b7b"], "operator": null, "metadata": {"aucs": [0.47229857566617983, 0.23390029571141546, 0.8851819428499049, 0.9578236771816728, 0.905325402514659, 0.9131204181736985, 0.36323431749997603, 0.8902270507795067, 0.8794362885489345, 0.8750109908482104, 0.9403252529174085, 0.9922070897330129, 0.30079738729066297, 0.8929780626456921, 0.6323972893624048, 0.38212262602606384, 0.7758744123284067, 0.9412208878431808, 0.1032951739623248, 0.5019863845649093]}}
{"id": "496ac514-845a-411c-896f-ee0f392b4057", "fitness": 0.7780690378343315, "name": "CMAES_SimpleRankOneMirrorHyperbolic", "description": "CMA-ES with a simplified rank-one update, adaptive step size using a moving average, and a mirrored sampling fraction adapted by a hyperbolic decay function and enhanced bound handling.", "code": "import numpy as np\n\nclass CMAES_SimpleRankOneMirrorHyperbolic:\n    def __init__(self, budget=10000, dim=10, sigma0=0.5, history_length=5, initial_mirrored_fraction=0.5, mirrored_decay=0.99, hyperbolic_decay_factor=0.1, step_size_adaptation_rate=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.history_length = history_length\n        self.mirrored_fraction = initial_mirrored_fraction\n        self.mirrored_decay = mirrored_decay\n        self.hyperbolic_decay_factor = hyperbolic_decay_factor\n        self.step_size_adaptation_rate = step_size_adaptation_rate\n\n        self.popsize = 4 + int(np.floor(3 * np.log(self.dim)))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n        self.step_history = []\n\n    def __call__(self, func):\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)\n        pc = np.zeros(self.dim)\n        \n        c_sigma = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        c_c = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        c_1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        \n        chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n\n        while eval_count < self.budget:\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            x = mean + sigma * (np.linalg.cholesky(C) @ z.T).T\n\n            # Mirrored sampling\n            mirrored_fraction = self.mirrored_fraction / (1 + self.hyperbolic_decay_factor * (eval_count / self.budget))\n            num_mirrored = int(self.popsize * mirrored_fraction)\n            x_mirrored = mean - sigma * (np.linalg.cholesky(C) @ z[:num_mirrored].T).T\n            x = np.vstack((x, x_mirrored))\n            z_mirrored = -z[:num_mirrored]\n            z = np.vstack((z, z_mirrored))\n            \n            # Enhanced bound handling - clipping and penalty\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            \n            f = np.array([func(xi) + 1e10 * (np.sum((xi < func.bounds.lb) | (xi > func.bounds.ub))) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n            eval_count += len(x)\n\n            idx = np.argsort(f)\n            x = x[idx]\n            z = z[idx]\n            f = f[idx]\n\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[0]\n\n            x_mu = x[:self.mu]\n            mean_new = np.sum(x_mu * self.weights[:,None], axis=0)\n\n            # Simplified Rank-One Update\n            pc = (1 - c_c) * pc + np.sqrt(c_c * (2 - c_c) * self.mueff) * (mean_new - mean) / sigma\n            C = (1 - c_1) * C + c_1 * np.outer(pc, pc)\n            \n            # Step size adaptation using moving average\n            step = (mean_new - mean) / sigma\n            self.step_history.append(np.linalg.norm(step))\n            if len(self.step_history) > self.history_length:\n                self.step_history.pop(0)\n\n            avg_step_size = np.mean(self.step_history) if self.step_history else 1\n            sigma *= np.exp(self.step_size_adaptation_rate * (avg_step_size - 1))\n\n            mean = mean_new\n            self.mirrored_fraction *= self.mirrored_decay\n\n            # Ensure C is positive definite\n            C = np.triu(C) + np.triu(C, 1).T\n            try:\n                C_evals = np.linalg.eigvalsh(C)\n                C_min_eig = np.min(C_evals)\n                if C_min_eig < 1e-10:\n                    C += (1e-10 - C_min_eig) * np.eye(self.dim)\n            except np.linalg.LinAlgError:\n                C = np.eye(self.dim)\n\n            if np.any(np.isnan(mean)) or np.any(np.isnan(C)):\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                sigma = self.sigma0\n\n        return f_opt, x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm CMAES_SimpleRankOneMirrorHyperbolic scored 0.778 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ee0a84f2-5ebb-4fad-928d-809f48880481"], "operator": null, "metadata": {"aucs": [0.26168053669148383, 0.9367211290054905, 0.9003377898479122, 0.9741100184358661, 0.9327165366784371, 0.9321450396214885, 0.3424326882438825, 0.5895116634185629, 0.9359502896767209, 0.9095363264255969, 0.9526530878317695, 0.9954780344896524, 0.7514756647380343, 0.8817486709284064, 0.7539975482076088, 0.9367762748167834, 0.8881738930976871, 0.9504594298542121, 0.2396001437646299, 0.49587599091240564]}}
{"id": "ffc5462c-2730-4391-bb00-5c519a1e6dad", "fitness": 0.6841404605192593, "name": "AdvancedCMAES_KLDivergence", "description": "CMA-ES with covariance matrix adaptation using a combination of rank-one and rank- updates, adaptive step size control based on the Kullback-Leibler divergence between successive distributions, and a simplified variant of orthogonal subspace learning based on historical search steps.", "code": "import numpy as np\n\nclass AdvancedCMAES_KLDivergence:\n    def __init__(self, budget=10000, dim=10, sigma0=0.5, history_length=5, success_rate_history=10, mirror_rate=0.5, kld_threshold=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.history_length = history_length\n        self.success_rate_history_length = success_rate_history\n        self.min_popsize = 4\n        self.max_popsize = 50\n        self.target_popsize = min(self.max_popsize, self.min_popsize + int(np.floor(3 * np.log(self.dim))))\n        self.popsize = self.target_popsize\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.step_history = []\n        self.last_f_opt = np.inf\n        self.stagnation_counter = 0\n        self.restart_iterations = 50\n        self.success_history = []\n        self.success_rate = 0.5 # Initial success rate\n        self.success_rate_window = np.array([0.5] * self.success_rate_history_length) # initialize success rate\n        self.mirror_rate = mirror_rate  # Rate of mirrored samples\n        self.orthogonal_basis = np.linalg.qr(np.random.randn(dim, dim))[0] # Initial basis for orthogonal sampling\n        self.kld_threshold = kld_threshold  # Threshold for KL divergence\n        self.last_mean = None # Store the last mean for KL divergence calculation\n\n    def __call__(self, func):\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n        c_sigma = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        c_c = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        c_1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        c_mu = min(1 - c_1, 2 * (self.mueff - 1 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        d_sigma = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + c_sigma\n\n        C_evals, C_evecs = np.linalg.eigh(C)\n        C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n        C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n        \n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n\n        while eval_count < self.budget:\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            \n            # Mirrored sampling\n            z_mirrored = -z[:int(self.popsize * self.mirror_rate)]\n            z = np.concatenate((z, z_mirrored))\n            \n            x = mean + sigma * (C_sqrt @ z.T).T\n            \n            f = np.array([func(xi) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n            eval_count += len(x)\n\n            idx = np.argsort(f)\n            x = x[idx]\n            z = z[idx]\n            f = f[idx]\n\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[0]\n                self.stagnation_counter = 0\n                self.success_history.append(1)\n            else:\n                self.stagnation_counter += 1\n                self.success_history.append(0)\n                \n            if self.stagnation_counter > self.restart_iterations:\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                sigma = self.sigma0\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                self.stagnation_counter = 0\n\n            x_mu = x[:self.mu]\n            z_mu = z[:self.mu]\n\n            mean_new = np.sum(x_mu * self.weights[:, None], axis=0)\n            z_w = np.sum(z_mu * self.weights[:, None], axis=0)\n            \n            ps = (1 - c_sigma) * ps + np.sqrt(c_sigma * (2 - c_sigma) * self.mueff) * (C_invsqrt @ (mean_new - mean) / sigma)\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - c_sigma)**(2 * eval_count / self.popsize)) < chiN * (1.4 + 2 / (self.dim + 1))\n            pc = (1 - c_c) * pc + hsig * np.sqrt(c_c * (2 - c_c) * self.mueff) * (mean_new - mean) / sigma\n\n            C = (1 - c_1 - c_mu) * C + c_1 * (pc[:, None] @ pc[None, :])\n            for i in range(self.mu):\n                C += c_mu * self.weights[i] * (z_mu[i, :, None] @ z_mu[i, None, :])\n            \n            # Adapt step size based on success rate\n            if len(self.success_history) > 0:\n                self.success_rate_window = np.concatenate((self.success_rate_window[1:], [self.success_history[-1]]))\n                self.success_rate = np.mean(self.success_rate_window)\n                \n                if self.success_rate > 0.6:\n                    sigma *= np.exp(0.1)\n                elif self.success_rate < 0.4:\n                    sigma *= np.exp(-0.1)\n            \n            # Adapt step size based on KL divergence between distributions\n            if self.last_mean is not None:\n                kld = self.kl_divergence(mean, C, mean_new, C) # Simplified KL divergence, assuming C remains similar\n\n                if kld > self.kld_threshold:\n                    sigma *= np.exp(-0.05)  # Reduce step size if KL divergence is high\n                else:\n                     sigma *= np.exp(0.02)\n\n            # Simplified Step history\n            step = (mean_new - mean) / sigma\n            self.step_history.append(step)\n            if len(self.step_history) > self.history_length:\n                self.step_history.pop(0)\n\n            # Adapt C based on step history, simplified\n            for h_step in self.step_history:\n                C += 0.1 * c_mu * (h_step[:, None] @ h_step[None, :])\n            \n            # Orthogonal Subspace learning\n            if eval_count % (self.popsize * 10) == 0:\n                # Project step history onto the orthogonal basis\n                projected_steps = [self.orthogonal_basis.T @ h_step for h_step in self.step_history]\n                \n                # Update covariance matrix using projected steps\n                for p_step in projected_steps:\n                    C += 0.05 * c_mu * (self.orthogonal_basis @ p_step[:, None] @ p_step[None, :] @ self.orthogonal_basis.T)\n\n            self.last_mean = mean\n            mean = mean_new\n            \n            if eval_count % (self.popsize * 5) == 0:\n                C = np.triu(C) + np.triu(C, 1).T\n                try:\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_evals = np.maximum(C_evals, 1e-10)\n                    C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                except np.linalg.LinAlgError:\n                    C = np.eye(self.dim)\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                \n            if np.any(np.isnan(mean)) or np.any(np.isnan(C)):\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                sigma = self.sigma0\n\n        return f_opt, x_opt\n\n    def kl_divergence(self, mean1, cov1, mean2, cov2):\n        \"\"\"\n        Calculates the Kullback-Leibler divergence between two multivariate Gaussian distributions.\n        This is a simplified calculation assuming cov1 and cov2 are similar for computational efficiency.\n        \"\"\"\n        # In a full implementation, use:\n        # cov1_inv = np.linalg.inv(cov1)\n        # kld = 0.5 * (np.trace(cov1_inv @ cov2) + (mean2 - mean1).T @ cov1_inv @ (mean2 - mean1) - self.dim + np.log(np.linalg.det(cov1) / np.linalg.det(cov2)))\n\n        # Simplified version assuming cov1 and cov2 are relatively similar.\n        kld = 0.5 * np.sum((mean2 - mean1)**2)\n\n        return kld", "configspace": "", "generation": 7, "feedback": "The algorithm AdvancedCMAES_KLDivergence scored 0.684 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b46e7608-7521-4948-af52-c33d6a764b7b"], "operator": null, "metadata": {"aucs": [0.17162728344001044, 0.9193992492998504, 0.8671697311784607, 0.9645181405972866, 0.9146161985947021, 0.8900633226869645, 0.31955843913785276, 0.870869269986853, 0.9082305769678607, 0.19908580776977924, 0.9454025317673446, 0.9892507099435301, 0.23970483177086765, 0.8876881766089714, 0.9544589299883581, 0.917081995984958, 0.8803073140020707, 0.1442532063359948, 0.1914552400139703, 0.508068254309499]}}
{"id": "62d275e3-a2dc-4fdc-8133-8c1fa0f9702b", "fitness": 0.5899668433224793, "name": "CMAES_AdaptiveMirrorRankHyperbolic", "description": "CMA-ES with adaptive mirrored sampling controlled by both hyperbolic and exponential decay, dynamic rank-based covariance update with active adaptation and a weighted recombination strategy based on fitness rank, and adaptive step size control using a history of the conjugate evolution path.", "code": "import numpy as np\n\nclass CMAES_AdaptiveMirrorRankHyperbolic:\n    def __init__(self, budget=10000, dim=10, sigma0=0.5, history_length=10, initial_mirrored_fraction=0.5, mirrored_decay=0.99, hyperbolic_decay_factor=0.1, exponential_decay_rate=0.001):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.history_length = history_length\n        self.mirrored_fraction = initial_mirrored_fraction  # Initial fraction of population to mirror\n        self.mirrored_decay = mirrored_decay  # Decay rate for mirrored fraction\n        self.hyperbolic_decay_factor = hyperbolic_decay_factor\n        self.exponential_decay_rate = exponential_decay_rate\n\n        self.popsize = 4 + int(np.floor(3 * np.log(self.dim)))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.step_history = []\n        self.old_f = np.inf\n\n    def __call__(self, func):\n        # Initialize variables\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n        # Parameters (using common defaults)\n        c_sigma = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        c_c = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        c_1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        c_mu = min(1 - c_1, 2 * (self.mueff - 1 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        d_sigma = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1)\n        c_1a = c_1\n        c_mua = c_mu\n        damp_C = 0.2 + c_1 + c_mu  # Damping factor for covariance matrix update\n\n        # Eigen decomposition of C\n        C_evals, C_evecs = np.linalg.eigh(C)\n        C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n        C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n        \n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n\n        while eval_count < self.budget:\n            # Sample population\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            x = mean + sigma * (C_sqrt @ z.T).T\n\n            # Mirrored sampling with hyperbolic decay AND exponential decay\n            mirrored_fraction = self.mirrored_fraction / (1 + self.hyperbolic_decay_factor * (eval_count / self.budget)) * np.exp(-self.exponential_decay_rate * eval_count)\n\n            num_mirrored = int(self.popsize * mirrored_fraction)\n            x_mirrored = mean - sigma * (C_sqrt @ z[:num_mirrored].T).T\n            x = np.vstack((x, x_mirrored))\n            z_mirrored = -z[:num_mirrored]\n            z = np.vstack((z, z_mirrored))\n            \n            # Clipping to bounds\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            \n            # Evaluate the new points\n            f = np.array([func(xi) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n            eval_count += len(x) \n\n            # Sort by fitness\n            idx = np.argsort(f)\n            x = x[idx]\n            z = z[idx]\n            f = f[idx]\n\n            # Update optimal solution\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[0]\n\n            # Adaptive weights based on fitness rank\n            weights = np.exp(-np.arange(len(f)) / (len(f) / 4))  # Exponential decay\n            weights = weights / np.sum(weights)  # Normalize\n            \n            # Selection and recombination with adaptive weights\n            x_mu = x[:self.mu]\n            z_mu = z[:self.mu]\n\n            # Recombination using adaptive weights\n            mean_new = np.sum(x[:len(weights)] * weights[:, None], axis=0)\n            z_w = np.sum(z[:len(weights)] * weights[:, None], axis=0)\n            \n            # Covariance matrix adaptation using rank-mu update\n            ps = (1 - c_sigma) * ps + np.sqrt(c_sigma * (2 - c_sigma) * self.mueff) * (C_invsqrt @ (mean_new - mean) / sigma)\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - c_sigma)**(2 * eval_count / self.popsize)) < chiN * (1.4 + 2 / (self.dim + 1))\n            \n            pc = (1 - c_c) * pc + hsig * np.sqrt(c_c * (2 - c_c) * self.mueff) * (mean_new - mean) / sigma\n\n            C = (1 - c_1 - c_mu + damp_C * c_1) * C + c_1 * (pc[:, None] @ pc[None, :])\n\n            # Rank-mu update\n            for i in range(self.mu):\n                C += c_mu * self.weights[i] * (z_mu[i, :, None] @ z_mu[i, None, :])\n                \n            # Update step size using conjugate evolution path\n            step = (mean_new - mean) / sigma\n            self.step_history.append(step)\n            if len(self.step_history) > self.history_length:\n                self.step_history.pop(0)\n\n            # Adapt step size based on step history\n            step_correlation = 0\n            for h_step in self.step_history:\n                step_correlation += np.dot(step, h_step)\n            \n            sigma_factor = np.exp(0.1 * step_correlation / self.dim)\n            sigma *= np.exp((c_sigma / d_sigma) * (np.linalg.norm(ps) / chiN - 1)) * sigma_factor\n\n            # Update mean\n            mean = mean_new\n\n            # Adaptive Mirrored fraction (decay moved to start)\n            self.mirrored_fraction *= self.mirrored_decay\n\n            # Eigen decomposition of C\n            if eval_count % (self.popsize * 5) == 0:\n                C = np.triu(C) + np.triu(C, 1).T\n                try:\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_evals = np.maximum(C_evals, 1e-10)\n                    C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                except np.linalg.LinAlgError:\n                    print(\"LinAlgError encountered, resetting C\")\n                    C = np.eye(self.dim)\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                \n            if np.any(np.isnan(mean)) or np.any(np.isnan(C)):\n                print(\"NaN detected, resetting...\")\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                sigma = self.sigma0\n        \n        return f_opt, x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm CMAES_AdaptiveMirrorRankHyperbolic scored 0.590 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ee0a84f2-5ebb-4fad-928d-809f48880481"], "operator": null, "metadata": {"aucs": [0.2463612947021684, 0.07895736423745436, 0.9116412225071683, 0.9639234752700859, 0.6652470694614249, 0.9522138836544054, 0.3109645661706705, 0.9481158410431084, 0.8608194416665084, 0.1779576445560921, 0.9611147351157712, 0.974955287928182, 0.2410810862169891, 0.38532269720502776, 0.5863246367950563, 0.33151997729904614, 0.6562620113729539, 0.9574161903282262, 0.10135879443078544, 0.48777964648846006]}}
{"id": "705df9f8-7314-492a-b801-ce49a7daa45b", "fitness": 0.6379504300117426, "name": "RobustCMAES_ProAdaptivePop", "description": "Robust CMA-ES with adaptive step size, covariance matrix regularization, aggressive stagnation detection, and dynamic population size adjustment based on landscape features.", "code": "import numpy as np\n\nclass RobustCMAES_ProAdaptivePop(object):\n    def __init__(self, budget=10000, dim=10, sigma0=0.5, history_length=5, mirror_ratio=0.5, restart_strategy=\"random\", adaptive_popsize=True):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.history_length = history_length\n        self.min_popsize = 4\n        self.max_popsize = 50\n        self.target_popsize = min(self.max_popsize, self.min_popsize + int(np.floor(3 * np.log(self.dim))))\n        self.popsize = self.target_popsize\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.step_history = []\n        self.last_f_opt = np.inf\n        self.stagnation_counter = 0\n        self.restart_iterations = 10 * self.dim  # More aggressive stagnation detection\n        self.mirror_ratio = mirror_ratio\n        self.restart_strategy = restart_strategy # \"random\" or \"jitter\"\n        self.condition_number_target = 1e14\n        self.damps = 1.0 # Damping factor for step size\n        self.adaptive_popsize = adaptive_popsize\n        self.local_improvement_history = []\n        self.local_improvement_window = 10\n\n    def __call__(self, func):\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n        c_sigma = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        c_c = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        c_1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        c_mu = min(1 - c_1, 2 * (self.mueff - 1 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        d_sigma = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + c_sigma\n\n        C_evals, C_evecs = np.linalg.eigh(C)\n        C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n        C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n        \n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n\n        while eval_count < self.budget:\n            # Dynamically adjust population size based on recent progress\n            if self.adaptive_popsize:\n                improvement_rate = np.mean(self.local_improvement_history) if self.local_improvement_history else 0.5\n                self.target_popsize = int(np.clip(self.min_popsize + (self.max_popsize - self.min_popsize) * improvement_rate, self.min_popsize, self.max_popsize))\n\n                # Reduce population size if stagnating (less exploration)\n                if self.stagnation_counter > self.restart_iterations / 2:\n                    self.target_popsize = max(self.min_popsize, self.target_popsize // 2)\n                self.popsize = self.target_popsize\n                self.mu = self.popsize // 2\n                self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n                self.weights = self.weights / np.sum(self.weights)\n                self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n                c_sigma = (self.mueff + 2) / (self.dim + self.mueff + 5)\n                c_c = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n                c_1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n                c_mu = min(1 - c_1, 2 * (self.mueff - 1 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n                d_sigma = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + c_sigma\n\n\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            x = mean + sigma * (C_sqrt @ z.T).T\n\n            # Mirrored Sampling\n            num_mirrored = int(self.popsize * self.mirror_ratio)\n            z_mirrored = -z[:num_mirrored]\n            x_mirrored = mean + sigma * (C_sqrt @ z_mirrored.T).T\n            x = np.concatenate((x, x_mirrored), axis=0)\n            z = np.concatenate((z, z_mirrored), axis=0)\n\n            f = np.array([func(xi) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n            eval_count += len(x)\n\n            idx = np.argsort(f)\n            x = x[idx]\n            z = z[idx]\n            f = f[idx]\n\n            if f[0] < f_opt:\n                # Record improvement\n                if f_opt != np.inf:\n                    self.local_improvement_history.append(1) #Significant Improvement\n                else:\n                    self.local_improvement_history.append(0.5)\n                    \n                if len(self.local_improvement_history) > self.local_improvement_window:\n                    self.local_improvement_history.pop(0)\n                \n                f_opt = f[0]\n                x_opt = x[0]\n                self.stagnation_counter = 0\n                self.last_f_opt = f[0] #Store for stagnation criterion\n            else:\n                 self.local_improvement_history.append(0)\n                 if len(self.local_improvement_history) > self.local_improvement_window:\n                    self.local_improvement_history.pop(0)\n                 self.stagnation_counter += 1\n            \n            # Restart strategy\n            if self.stagnation_counter > self.restart_iterations:\n                if self.restart_strategy == \"random\":\n                    mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                elif self.restart_strategy == \"jitter\":\n                     mean = mean + 0.1 * np.random.normal(0, 1, size=self.dim) #Jitter the mean\n                     mean = np.clip(mean, func.bounds.lb, func.bounds.ub) #Ensure bounds\n                else:\n                    raise ValueError(\"Invalid restart strategy.\")\n\n                sigma = self.sigma0\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                self.stagnation_counter = 0\n                self.local_improvement_history = [] # Reset improvement history\n\n            x_mu = x[:self.mu]\n            z_mu = z[:self.mu]\n\n            mean_new = np.sum(x_mu * self.weights[:, None], axis=0)\n            z_w = np.sum(z_mu * self.weights[:, None], axis=0)\n            \n            ps = (1 - c_sigma) * ps + np.sqrt(c_sigma * (2 - c_sigma) * self.mueff) * (C_invsqrt @ (mean_new - mean) / sigma)\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - c_sigma)**(2 * eval_count / self.popsize)) < chiN * (1.4 + 2 / (self.dim + 1))\n            pc = (1 - c_c) * pc + hsig * np.sqrt(c_c * (2 - c_c) * self.mueff) * (mean_new - mean) / sigma\n\n            C = (1 - c_1 - c_mu) * C + c_1 * (pc[:, None] @ pc[None, :])\n            for i in range(self.mu):\n                C += c_mu * self.weights[i] * (z_mu[i, :, None] @ z_mu[i, None, :])\n            \n            sigma *= np.exp((c_sigma / d_sigma) * (np.linalg.norm(ps) / chiN - 1))\n            sigma *= self.damps # Apply damping\n\n            # Simplified Step history\n            step = (mean_new - mean) / sigma\n            self.step_history.append(step)\n            if len(self.step_history) > self.history_length:\n                self.step_history.pop(0)\n\n            # Adapt C based on step history, simplified\n            for h_step in self.step_history:\n                C += 0.1 * c_mu * (h_step[:, None] @ h_step[None, :])\n\n            mean = mean_new\n            \n            if eval_count % (self.popsize * 5) == 0:\n                C = np.triu(C) + np.triu(C, 1).T\n                try:\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_evals = np.maximum(C_evals, 1e-10)\n                    C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n\n                     # Condition number based regularization\n                    condition_number = np.max(C_evals) / np.min(C_evals)\n                    if condition_number > self.condition_number_target:\n                        regularization_factor = 1e-8 * (condition_number / self.condition_number_target)\n                        C = C + regularization_factor * np.eye(self.dim)\n\n                except np.linalg.LinAlgError:\n                    C = np.eye(self.dim)\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n\n                # Regularize Covariance Matrix\n                C = C + 1e-8 * np.eye(self.dim)\n                \n            if np.any(np.isnan(mean)) or np.any(np.isnan(C)):\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                sigma = self.sigma0\n\n        return f_opt, x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm RobustCMAES_ProAdaptivePop scored 0.638 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2d4890c9-8e91-435b-a727-67b91c01d30c"], "operator": null, "metadata": {"aucs": [0.24638533419685338, 0.8196867506380494, 0.7749606757187182, 0.9069808470889059, 0.8669115607917781, 0.8503990939659819, 0.3255593931912516, 0.804411969014599, 0.8347732014806284, 0.30517372179242375, 0.8830210271405068, 0.9774304295202693, 0.21711855007551872, 0.8407297748568925, 0.721818538905961, 0.3543540283220623, 0.41453281484643967, 0.9062043508941193, 0.22144477166887178, 0.4871117661250197]}}
{"id": "631e4fe3-7b88-4600-8e74-f00879a79055", "fitness": 0.5457432468000363, "name": "RobustCMAES_ProPlus", "description": "Robust CMA-ES with adaptive step size damping based on fitness variance, covariance matrix regularization based on condition number and determinant, and active population size adaptation based on the function's ruggedness.", "code": "import numpy as np\n\nclass RobustCMAES_ProPlus(object):\n    def __init__(self, budget=10000, dim=10, sigma0=0.5, history_length=5, mirror_ratio=0.5, restart_strategy=\"random\"):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.history_length = history_length\n        self.min_popsize = 4\n        self.max_popsize = 50\n        self.target_popsize = min(self.max_popsize, self.min_popsize + int(np.floor(3 * np.log(self.dim))))\n        self.popsize = self.target_popsize\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.step_history = []\n        self.last_f_opt = np.inf\n        self.stagnation_counter = 0\n        self.restart_iterations = 10 * self.dim  # More aggressive stagnation detection\n        self.mirror_ratio = mirror_ratio\n        self.restart_strategy = restart_strategy # \"random\" or \"jitter\"\n        self.condition_number_target = 1e14\n        self.damps = 1.0 # Damping factor for step size\n        self.fitness_history = []\n        self.popsize_adaptation_interval = 10 * self.dim\n        self.variance_history_length = 10\n\n    def __call__(self, func):\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n        c_sigma = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        c_c = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        c_1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        c_mu = min(1 - c_1, 2 * (self.mueff - 1 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        d_sigma = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + c_sigma\n\n        C_evals, C_evecs = np.linalg.eigh(C)\n        C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n        C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n        \n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n\n        while eval_count < self.budget:\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            x = mean + sigma * (C_sqrt @ z.T).T\n\n            # Mirrored Sampling\n            num_mirrored = int(self.popsize * self.mirror_ratio)\n            z_mirrored = -z[:num_mirrored]\n            x_mirrored = mean + sigma * (C_sqrt @ z_mirrored.T).T\n            x = np.concatenate((x, x_mirrored), axis=0)\n            z = np.concatenate((z, z_mirrored), axis=0)\n\n            f = np.array([func(xi) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n            eval_count += len(x)\n\n            idx = np.argsort(f)\n            x = x[idx]\n            z = z[idx]\n            f = f[idx]\n            \n            # Store fitness values for variance calculation\n            self.fitness_history.extend(f[:self.mu].tolist())\n            if len(self.fitness_history) > self.variance_history_length * self.popsize:\n                self.fitness_history = self.fitness_history[-self.variance_history_length * self.popsize:]\n            \n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[0]\n                self.stagnation_counter = 0\n                self.last_f_opt = f[0] #Store for stagnation criterion\n            else:\n                self.stagnation_counter += 1\n            \n            # Restart strategy\n            if self.stagnation_counter > self.restart_iterations:\n                if self.restart_strategy == \"random\":\n                    mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                elif self.restart_strategy == \"jitter\":\n                     mean = mean + 0.1 * np.random.normal(0, 1, size=self.dim) #Jitter the mean\n                     mean = np.clip(mean, func.bounds.lb, func.bounds.ub) #Ensure bounds\n                else:\n                    raise ValueError(\"Invalid restart strategy.\")\n\n                sigma = self.sigma0\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                self.stagnation_counter = 0\n\n            x_mu = x[:self.mu]\n            z_mu = z[:self.mu]\n\n            mean_new = np.sum(x_mu * self.weights[:, None], axis=0)\n            z_w = np.sum(z_mu * self.weights[:, None], axis=0)\n            \n            ps = (1 - c_sigma) * ps + np.sqrt(c_sigma * (2 - c_sigma) * self.mueff) * (C_invsqrt @ (mean_new - mean) / sigma)\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - c_sigma)**(2 * eval_count / self.popsize)) < chiN * (1.4 + 2 / (self.dim + 1))\n            pc = (1 - c_c) * pc + hsig * np.sqrt(c_c * (2 - c_c) * self.mueff) * (mean_new - mean) / sigma\n\n            C = (1 - c_1 - c_mu) * C + c_1 * (pc[:, None] @ pc[None, :])\n            for i in range(self.mu):\n                C += c_mu * self.weights[i] * (z_mu[i, :, None] @ z_mu[i, None, :])\n            \n            sigma *= np.exp((c_sigma / d_sigma) * (np.linalg.norm(ps) / chiN - 1))\n\n            # Adaptive step size damping based on fitness variance\n            if len(self.fitness_history) >= self.variance_history_length * self.popsize:\n                fitness_variance = np.var(self.fitness_history)\n                if fitness_variance > 0:\n                    self.damps = 1 + 0.1 * np.log(fitness_variance)  # Increase damping if variance is high\n                else:\n                    self.damps = 1.0  # Reset damping if variance is low\n\n            sigma *= self.damps # Apply damping\n\n            # Simplified Step history\n            step = (mean_new - mean) / sigma\n            self.step_history.append(step)\n            if len(self.step_history) > self.history_length:\n                self.step_history.pop(0)\n\n            # Adapt C based on step history, simplified\n            for h_step in self.step_history:\n                C += 0.1 * c_mu * (h_step[:, None] @ h_step[None, :])\n\n            mean = mean_new\n            \n            if eval_count % (self.popsize * 5) == 0:\n                C = np.triu(C) + np.triu(C, 1).T\n                try:\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_evals = np.maximum(C_evals, 1e-10)\n                    C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n\n                     # Condition number based regularization\n                    condition_number = np.max(C_evals) / np.min(C_evals)\n                    if condition_number > self.condition_number_target:\n                        regularization_factor = 1e-8 * (condition_number / self.condition_number_target)\n                        C = C + regularization_factor * np.eye(self.dim)\n\n                    # Determinant based regularization\n                    determinant = np.linalg.det(C)\n                    if determinant <= 0:\n                         C = C + 1e-8 * np.eye(self.dim)\n\n\n                except np.linalg.LinAlgError:\n                    C = np.eye(self.dim)\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n\n                # Regularize Covariance Matrix\n                C = C + 1e-8 * np.eye(self.dim)\n                \n            if np.any(np.isnan(mean)) or np.any(np.isnan(C)):\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                sigma = self.sigma0\n\n            # Population size adaptation\n            if eval_count % self.popsize_adaptation_interval == 0:\n                if len(self.fitness_history) >= self.variance_history_length * self.popsize:\n                   fitness_variance = np.var(self.fitness_history)\n                   if fitness_variance > 0.01:\n                       self.popsize = min(self.popsize + 2, self.max_popsize)\n                       self.mu = self.popsize // 2\n                       self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n                       self.weights = self.weights / np.sum(self.weights)\n                       self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n                   elif self.popsize > self.min_popsize:\n                       self.popsize = max(self.popsize - 2, self.min_popsize)\n                       self.mu = self.popsize // 2\n                       self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n                       self.weights = self.weights / np.sum(self.weights)\n                       self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n\n        return f_opt, x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm RobustCMAES_ProPlus scored 0.546 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2d4890c9-8e91-435b-a727-67b91c01d30c"], "operator": null, "metadata": {"aucs": [0.1153511395437341, 0.15538121658144333, 0.6208233426526555, 0.7596830471049525, 0.4021135981118822, 0.7458409318462953, 0.2731804205635442, 0.4941507445266021, 0.7899690580280401, 0.927187225782003, 0.9530744293464498, 0.9896436018404489, 0.2528393749566876, 0.35227867133418245, 0.6301472765151459, 0.3441287281626838, 0.3581499763631084, 0.8939507709767033, 0.3532712113966009, 0.5037001703675621]}}
{"id": "2703f30e-573d-43a3-baf8-ec69703cc562", "fitness": 0.6088902415466482, "name": "CMAES_AdaptiveMirrorRank_ActiveAdaptation_OrthogonalLearning", "description": "CMA-ES with adaptive mirrored sampling and covariance update, including orthogonal subspace learning for faster convergence, dynamic population size adjustment and spectral correction to maintain covariance matrix properties.", "code": "import numpy as np\n\nclass CMAES_AdaptiveMirrorRank_ActiveAdaptation_OrthogonalLearning:\n    def __init__(self, budget=10000, dim=10, sigma0=0.5, history_length=10, initial_mirrored_fraction=0.5, mirrored_decay=0.99, active_adaptation_multiplier=0.1, orthogonal_learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.history_length = history_length\n        self.mirrored_fraction = initial_mirrored_fraction\n        self.mirrored_decay = mirrored_decay\n        self.active_adaptation_multiplier = active_adaptation_multiplier\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n\n        self.popsize = 4 + int(np.floor(3 * np.log(self.dim)))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.step_history = []\n        self.success_rate_history = []\n        self.success_rate_window = 10\n        self.population_size_adaptation_frequency = 10  # Adjust popsize every this many generations\n        self.population_size_scale = 1.0\n\n    def __call__(self, func):\n        # Initialize variables\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n        # Parameters (using common defaults)\n        c_sigma = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        c_c = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        c_1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        c_mu = min(1 - c_1, 2 * (self.mueff - 1 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        d_sigma = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1)\n        c_1a = c_1\n        c_mua = c_mu\n\n        # Eigen decomposition of C\n        C_evals, C_evecs = np.linalg.eigh(C)\n        C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n        C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n        \n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n        successes = 0\n        generation = 0\n\n        while eval_count < self.budget:\n            # Adjust population size\n            self.popsize = int(self.population_size_scale * (4 + int(np.floor(3 * np.log(self.dim)))))\n            self.mu = self.popsize // 2\n            self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n            self.weights = self.weights / np.sum(self.weights)\n            self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n            \n            # Sample population\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            x = mean + sigma * (C_sqrt @ z.T).T\n\n            # Mirrored sampling\n            num_mirrored = int(self.popsize * self.mirrored_fraction)\n            x_mirrored = mean - sigma * (C_sqrt @ z[:num_mirrored].T).T\n            x = np.vstack((x, x_mirrored))\n            z_mirrored = -z[:num_mirrored]\n            z = np.vstack((z, z_mirrored))\n            \n            # Clipping to bounds\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            \n            # Evaluate the new points\n            f = np.array([func(xi) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n            eval_count += len(x) \n\n            # Sort by fitness\n            idx = np.argsort(f)\n            x = x[idx]\n            z = z[idx]\n            f = f[idx]\n\n            # Update optimal solution\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[0]\n                successes += 1 # increment successful steps\n                \n            # Selection and recombination\n            x_mu = x[:self.mu]\n            z_mu = z[:self.mu]\n\n            mean_new = np.sum(x_mu * self.weights[:,None], axis=0)\n            z_w = np.sum(z_mu * self.weights[:,None], axis=0)\n            \n            # Covariance matrix adaptation using rank-mu update\n            ps = (1 - c_sigma) * ps + np.sqrt(c_sigma * (2 - c_sigma) * self.mueff) * (C_invsqrt @ (mean_new - mean) / sigma)\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - c_sigma)**(2 * eval_count / self.popsize)) < chiN * (1.4 + 2 / (self.dim + 1))\n            \n            pc = (1 - c_c) * pc + hsig * np.sqrt(c_c * (2 - c_c) * self.mueff) * (mean_new - mean) / sigma\n\n            C = (1 - c_1 - c_mu + self.active_adaptation_multiplier * c_1 * (1 - hsig**2)) * C + c_1 * (pc[:, None] @ pc[None, :])\n\n            # Rank-mu update\n            for i in range(self.mu):\n                C += c_mu * self.weights[i] * (z_mu[i, :, None] @ z_mu[i, None, :])\n\n            # Orthogonal subspace learning\n            if self.orthogonal_learning_rate > 0:\n                Q, R = np.linalg.qr(z_mu.T)  # Orthogonal basis\n                delta_C = np.zeros_like(C)\n                for i in range(Q.shape[1]):\n                    delta_C += self.orthogonal_learning_rate * (Q[:, i:i+1] @ Q[:, i:i+1].T)\n                C += delta_C\n                \n            # Active covariance matrix adaptation: encourage/discourage steps\n            if f[0] > f_opt:\n                 C -= self.active_adaptation_multiplier * c_1 * (pc[:, None] @ pc[None, :])  # Discourage step if fitness worsened\n            \n            # Update step size: using adaptive success rate\n            success_rate = successes / (eval_count / self.popsize)\n            self.success_rate_history.append(success_rate)\n    \n            if len(self.success_rate_history) > self.success_rate_window:\n                self.success_rate_history.pop(0)\n    \n            avg_success_rate = np.mean(self.success_rate_history)\n            \n            # Adjust sigma based on success rate\n            if avg_success_rate > 0.25:\n                sigma *= 1.1  # Increase step-size if doing well\n            elif avg_success_rate < 0.15:\n                sigma *= 0.9  # Decrease step-size if not improving\n\n            # Step history adaptation\n            step = (mean_new - mean) / sigma\n            self.step_history.append(step)\n            if len(self.step_history) > self.history_length:\n                self.step_history.pop(0)\n\n            # Adapt C based on step history (dampened)\n            for h_step in self.step_history:\n                C += 0.1 * c_mu * (h_step[:, None] @ h_step[None, :])  # Dampened update\n                \n            # Update mean\n            mean = mean_new\n\n            # Adaptive Mirrored fraction\n            self.mirrored_fraction *= self.mirrored_decay\n            \n            # Spectral correction to ensure C remains positive definite\n            C = np.triu(C) + np.triu(C, 1).T\n            C_evals, C_evecs = np.linalg.eigh(C)\n            C_evals = np.maximum(C_evals, 1e-10)  # Ensure positive definite\n            C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n            \n            # Eigen decomposition of C\n            if eval_count % (self.popsize * 5) == 0:\n                C = np.triu(C) + np.triu(C, 1).T\n                try:\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_evals = np.maximum(C_evals, 1e-10)\n                    C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                except np.linalg.LinAlgError:\n                    print(\"LinAlgError encountered, resetting C\")\n                    C = np.eye(self.dim)\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n            \n            if np.any(np.isnan(mean)) or np.any(np.isnan(C)):\n                print(\"NaN detected, resetting...\")\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                sigma = self.sigma0\n                successes = 0\n                self.success_rate_history = []\n                \n            generation += 1\n            if generation % self.population_size_adaptation_frequency == 0:\n                if avg_success_rate > 0.3:\n                    self.population_size_scale *= 1.1\n                elif avg_success_rate < 0.1:\n                    self.population_size_scale *= 0.9\n                self.population_size_scale = np.clip(self.population_size_scale, 0.5, 2.0)  # Limit population size scaling\n\n        \n        return f_opt, x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm CMAES_AdaptiveMirrorRank_ActiveAdaptation_OrthogonalLearning scored 0.609 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["652a0ee5-9606-46be-afc8-4bdc23d3a249"], "operator": null, "metadata": {"aucs": [0.20804172853282998, 0.4315139746044854, 0.8723544839761849, 0.8513374935688651, 0.7736025249835834, 0.8406655671452188, 0.3151387248556944, 0.7617899940287064, 0.8406979286659209, 0.25705870567675826, 0.8907905661330942, 0.9906624842207025, 0.31420309353539055, 0.8860345238692419, 0.700659586717622, 0.3464672852082604, 0.3321195564191062, 0.887979820766981, 0.18516522804665536, 0.4915215599776618]}}
{"id": "24bca9ca-66d4-46c6-940e-5e4b89072caa", "fitness": 0.719828142880933, "name": "RobustCMAES_ProPlus", "description": "Robust CMA-ES with adaptive step size damping based on conjugate evolution paths, covariance matrix regularization using spectral clipping, and enhanced stagnation detection using both objective value and population diversity.", "code": "import numpy as np\n\nclass RobustCMAES_ProPlus(object):\n    def __init__(self, budget=10000, dim=10, sigma0=0.5, history_length=5, mirror_ratio=0.5, restart_strategy=\"random\"):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.history_length = history_length\n        self.min_popsize = 4\n        self.max_popsize = 50\n        self.target_popsize = min(self.max_popsize, self.min_popsize + int(np.floor(3 * np.log(self.dim))))\n        self.popsize = self.target_popsize\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.step_history = []\n        self.last_f_opt = np.inf\n        self.stagnation_counter = 0\n        self.restart_iterations = 10 * self.dim  # More aggressive stagnation detection\n        self.mirror_ratio = mirror_ratio\n        self.restart_strategy = restart_strategy # \"random\" or \"jitter\"\n        self.condition_number_target = 1e14\n        self.damps = 1.0 # Damping factor for step size\n        self.cs = 0.2 # cumulation factor for step-size\n        self.ccov = 2 / (self.dim**2 + 6)\n\n    def __call__(self, func):\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n        c_sigma = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        c_c = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        c_1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        c_mu = min(1 - c_1, 2 * (self.mueff - 1 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        d_sigma = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + c_sigma\n\n        C_evals, C_evecs = np.linalg.eigh(C)\n        C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n        C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n        \n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n        diversity_threshold = 0.01  # Diversity threshold for stagnation\n        no_improvement_streak = 0 # Streak of no improvement\n\n        while eval_count < self.budget:\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            x = mean + sigma * (C_sqrt @ z.T).T\n\n            # Mirrored Sampling\n            num_mirrored = int(self.popsize * self.mirror_ratio)\n            z_mirrored = -z[:num_mirrored]\n            x_mirrored = mean + sigma * (C_sqrt @ z_mirrored.T).T\n            x = np.concatenate((x, x_mirrored), axis=0)\n            z = np.concatenate((z, z_mirrored), axis=0)\n\n            f = np.array([func(xi) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n            eval_count += len(x)\n\n            idx = np.argsort(f)\n            x = x[idx]\n            z = z[idx]\n            f = f[idx]\n\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[0]\n                self.stagnation_counter = 0\n                self.last_f_opt = f[0] #Store for stagnation criterion\n                no_improvement_streak = 0  # Reset no improvement streak\n            else:\n                self.stagnation_counter += 1\n                no_improvement_streak += 1 # Increment no improvement streak\n            \n            # Restart strategy\n            if self.stagnation_counter > self.restart_iterations or no_improvement_streak > 2 * self.restart_iterations:\n                if self.restart_strategy == \"random\":\n                    mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                elif self.restart_strategy == \"jitter\":\n                     mean = mean + 0.1 * np.random.normal(0, 1, size=self.dim) #Jitter the mean\n                     mean = np.clip(mean, func.bounds.lb, func.bounds.ub) #Ensure bounds\n                else:\n                    raise ValueError(\"Invalid restart strategy.\")\n\n                sigma = self.sigma0\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                self.stagnation_counter = 0\n                no_improvement_streak = 0 #reset streak\n\n            x_mu = x[:self.mu]\n            z_mu = z[:self.mu]\n\n            mean_new = np.sum(x_mu * self.weights[:, None], axis=0)\n            z_w = np.sum(z_mu * self.weights[:, None], axis=0)\n            \n            ps = (1 - self.cs) * ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * (C_invsqrt @ (mean_new - mean) / sigma)\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - self.cs)**(2 * eval_count / self.popsize)) < chiN * (1.4 + 2 / (self.dim + 1))\n            pc = (1 - c_c) * pc + hsig * np.sqrt(c_c * (2 - c_c) * self.mueff) * (mean_new - mean) / sigma\n\n            C = (1 - c_1 - c_mu) * C + c_1 * (pc[:, None] @ pc[None, :])\n            for i in range(self.mu):\n                C += c_mu * self.weights[i] * (z_mu[i, :, None] @ z_mu[i, None, :])\n            \n            sigma *= np.exp((c_sigma / d_sigma) * (np.linalg.norm(ps) / chiN - 1))\n            sigma *= self.damps # Apply damping\n\n            # Simplified Step history\n            step = (mean_new - mean) / sigma\n            self.step_history.append(step)\n            if len(self.step_history) > self.history_length:\n                self.step_history.pop(0)\n\n            # Adapt C based on step history, simplified\n            for h_step in self.step_history:\n                C += 0.1 * c_mu * (h_step[:, None] @ h_step[None, :])\n\n            mean = mean_new\n            \n            if eval_count % (self.popsize * 5) == 0:\n                C = np.triu(C) + np.triu(C, 1).T\n                try:\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_evals = np.maximum(C_evals, 1e-10)\n                    C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n\n                     # Condition number based regularization\n                    condition_number = np.max(C_evals) / np.min(C_evals)\n                    if condition_number > self.condition_number_target:\n                        regularization_factor = 1e-8 * (condition_number / self.condition_number_target)\n                        C = C + regularization_factor * np.eye(self.dim)\n\n                     # Spectral clipping regularization\n                    C_evals = np.clip(C_evals, np.max(C_evals) / self.condition_number_target, np.max(C_evals))\n                    C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n\n                except np.linalg.LinAlgError:\n                    C = np.eye(self.dim)\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n\n                # Regularize Covariance Matrix\n                C = C + 1e-8 * np.eye(self.dim)\n                \n            if np.any(np.isnan(mean)) or np.any(np.isnan(C)):\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                sigma = self.sigma0\n\n        return f_opt, x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm RobustCMAES_ProPlus scored 0.720 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2d4890c9-8e91-435b-a727-67b91c01d30c"], "operator": null, "metadata": {"aucs": [0.23868122130696212, 0.8174806411723998, 0.9208343722031656, 0.9615881824566889, 0.9363712002888226, 0.9469050886766424, 0.32164265909248846, 0.9194386096442283, 0.9249401211529252, 0.5889117956759723, 0.9553323500672173, 0.9982937086575725, 0.2639309369895675, 0.8973407316579214, 0.6307457330955752, 0.9163975064077244, 0.45426514221205727, 0.9479806082444278, 0.2198965616037516, 0.535585687012547]}}
{"id": "41e310bc-fcf5-4b15-9cbf-b4a3ad61eace", "fitness": 0.7271543658152392, "name": "CMAES_AdaptiveMirrorRankHyperbolic_SelfAdaptC1Cmu", "description": "CMA-ES with adaptive mirrored sampling and hyperbolic decay, rank-based covariance update, conjugate evolution path step size adaptation, and a self-adaptation mechanism for the covariance matrix learning rate based on the trace of the covariance matrix.", "code": "import numpy as np\n\nclass CMAES_AdaptiveMirrorRankHyperbolic_SelfAdaptC1Cmu:\n    def __init__(self, budget=10000, dim=10, sigma0=0.5, history_length=10, initial_mirrored_fraction=0.5, mirrored_decay=0.99, hyperbolic_decay_factor=0.1, initial_c1=None, initial_cmu=None):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.history_length = history_length\n        self.mirrored_fraction = initial_mirrored_fraction  # Initial fraction of population to mirror\n        self.mirrored_decay = mirrored_decay  # Decay rate for mirrored fraction\n        self.hyperbolic_decay_factor = hyperbolic_decay_factor\n\n        self.popsize = 4 + int(np.floor(3 * np.log(self.dim)))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.step_history = []\n\n        # Initialize c_1 and c_mu\n        if initial_c1 is None:\n            self.c_1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        else:\n            self.c_1 = initial_c1\n        if initial_cmu is None:\n            self.c_mu = min(1 - self.c_1, 2 * (self.mueff - 1 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        else:\n            self.c_mu = initial_cmu\n\n        self.damp_C = 0.2 + self.c_1 + self.c_mu  # Damping factor for covariance matrix update\n        self.c_sigma = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.c_c = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        self.d_sigma = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1)\n\n    def __call__(self, func):\n        # Initialize variables\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n\n        # Eigen decomposition of C\n        C_evals, C_evecs = np.linalg.eigh(C)\n        C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n        C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n        \n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n\n        while eval_count < self.budget:\n            # Sample population\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            x = mean + sigma * (C_sqrt @ z.T).T\n\n            # Mirrored sampling with hyperbolic decay\n            mirrored_fraction = self.mirrored_fraction / (1 + self.hyperbolic_decay_factor * (eval_count / self.budget))\n\n            num_mirrored = int(self.popsize * mirrored_fraction)\n            x_mirrored = mean - sigma * (C_sqrt @ z[:num_mirrored].T).T\n            x = np.vstack((x, x_mirrored))\n            z_mirrored = -z[:num_mirrored]\n            z = np.vstack((z, z_mirrored))\n            \n            # Clipping to bounds\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            \n            # Evaluate the new points\n            f = np.array([func(xi) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n            eval_count += len(x) \n\n            # Sort by fitness\n            idx = np.argsort(f)\n            x = x[idx]\n            z = z[idx]\n            f = f[idx]\n\n            # Update optimal solution\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[0]\n                \n            # Selection and recombination\n            x_mu = x[:self.mu]\n            z_mu = z[:self.mu]\n\n            mean_new = np.sum(x_mu * self.weights[:,None], axis=0)\n            z_w = np.sum(z_mu * self.weights[:,None], axis=0)\n            \n            # Covariance matrix adaptation using rank-mu update\n            ps = (1 - self.c_sigma) * ps + np.sqrt(self.c_sigma * (2 - self.c_sigma) * self.mueff) * (C_invsqrt @ (mean_new - mean) / sigma)\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - self.c_sigma)**(2 * eval_count / self.popsize)) < chiN * (1.4 + 2 / (self.dim + 1))\n            \n            pc = (1 - self.c_c) * pc + hsig * np.sqrt(self.c_c * (2 - self.c_c) * self.mueff) * (mean_new - mean) / sigma\n\n            C = (1 - self.c_1 - self.c_mu + self.damp_C * self.c_1) * C + self.c_1 * (pc[:, None] @ pc[None, :])\n\n            # Rank-mu update\n            for i in range(self.mu):\n                C += self.c_mu * self.weights[i] * (z_mu[i, :, None] @ z_mu[i, None, :])\n            \n            # Update step size using conjugate evolution path\n            step = (mean_new - mean) / sigma\n            self.step_history.append(step)\n            if len(self.step_history) > self.history_length:\n                self.step_history.pop(0)\n\n            # Adapt step size based on step history\n            step_correlation = 0\n            for h_step in self.step_history:\n                step_correlation += np.dot(step, h_step)\n            \n            sigma_factor = np.exp(0.1 * step_correlation / self.dim)\n            sigma *= np.exp((self.c_sigma / self.d_sigma) * (np.linalg.norm(ps) / chiN - 1)) * sigma_factor\n\n            # Update mean\n            mean = mean_new\n\n            # Adaptive Mirrored fraction (decay moved to start)\n            self.mirrored_fraction *= self.mirrored_decay\n\n            # Eigen decomposition of C\n            if eval_count % (self.popsize * 5) == 0:\n                C = np.triu(C) + np.triu(C, 1).T\n                try:\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_evals = np.maximum(C_evals, 1e-10)\n                    C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                except np.linalg.LinAlgError:\n                    print(\"LinAlgError encountered, resetting C\")\n                    C = np.eye(self.dim)\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n\n                # Self-adaptation of c_1 and c_mu based on trace of C\n                trace_C = np.trace(C)\n                learning_rate = 0.1  # Adjust as needed\n                self.c_1 *= np.exp(learning_rate * (trace_C - self.dim))\n                self.c_mu *= np.exp(-learning_rate * (trace_C - self.dim))\n\n                # Ensure c_1 and c_mu remain within reasonable bounds\n                self.c_1 = np.clip(self.c_1, 1e-8, 1.0)\n                self.c_mu = np.clip(self.c_mu, 1e-8, 1.0)\n\n                self.damp_C = 0.2 + self.c_1 + self.c_mu  # Update damping factor\n\n            if np.any(np.isnan(mean)) or np.any(np.isnan(C)):\n                print(\"NaN detected, resetting...\")\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                sigma = self.sigma0\n        \n        return f_opt, x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm CMAES_AdaptiveMirrorRankHyperbolic_SelfAdaptC1Cmu scored 0.727 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ee0a84f2-5ebb-4fad-928d-809f48880481"], "operator": null, "metadata": {"aucs": [0.2600023204753684, 0.875484547733294, 0.8987894932044412, 0.9624615829447051, 0.9465486946783884, 0.9083056717871382, 0.3819407000390561, 0.9198983542596942, 0.9442921248277031, 0.9307014008065252, 0.9633295802684396, 0.9944581013990715, 0.22955247261672507, 0.7076307610113604, 0.951494238464583, 0.3358216820116373, 0.41334095217327527, 0.9595276818790371, 0.4550238218809072, 0.5044831338434332]}}
{"id": "820b1b92-fe10-48c0-bbd0-7423f464fe37", "fitness": -Infinity, "name": "AdvancedCMAES_OrthoPopSizeWavelet", "description": "CMA-ES with spectral decay, orthogonal basis adaptation, adaptive population sizing, and a wavelet mutation operator for enhanced exploration.", "code": "import numpy as np\nimport pywt\n\nclass AdvancedCMAES_OrthoPopSizeWavelet(object):\n    def __init__(self, budget=10000, dim=10, sigma0=0.5, history_length=5, mirror_ratio=0.5, restart_strategy=\"random\", wavelet='db1'):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.history_length = history_length\n        self.min_popsize = 4\n        self.max_popsize = 50\n        self.target_popsize = min(self.max_popsize, self.min_popsize + int(np.floor(3 * np.log(self.dim))))\n        self.popsize = self.target_popsize\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.step_history = []\n        self.last_f_opt = np.inf\n        self.stagnation_counter = 0\n        self.restart_iterations = 10 * self.dim  # More aggressive stagnation detection\n        self.mirror_ratio = mirror_ratio\n        self.restart_strategy = restart_strategy # \"random\" or \"jitter\"\n        self.condition_number_target = 1e14\n        self.damps = 1.0 # Damping factor for step size\n        self.cs = 0.3  # Step-size learning rate\n        self.cc = 0.4  # Cumulation for covariance matrix rank-one update\n        self.c1 = 1.5 / (self.dim + 2)**2  # Learning rate for rank-one update\n        self.cmu = 1.5 * self.mueff / (self.dim + 2)**2  # Learning rate for rank-mu update\n        self.init_lambda = 2 + np.floor(3 * np.log(self.dim))\n        self.lambda_factor = 2\n        self.orthogonal_basis = np.random.randn(self.dim, self.dim)\n        self.orthogonal_basis, _ = np.linalg.qr(self.orthogonal_basis)\n        self.wavelet = wavelet # Wavelet type\n        self.wavelet_level = int(np.floor(np.log2(self.dim))) # Wavelet decomposition levels\n\n    def __call__(self, func):\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n        C_evals, C_evecs = np.linalg.eigh(C)\n        C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n        C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n        \n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n\n        while eval_count < self.budget:\n            # Adapt population size\n            self.target_popsize = min(self.max_popsize, self.min_popsize + int(np.floor(3 * np.log(self.dim))))\n            if self.target_popsize != self.popsize:\n                self.popsize = self.target_popsize\n                self.mu = self.popsize // 2\n                self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n                self.weights = self.weights / np.sum(self.weights)\n                self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n                self.cmu = 1.5 * self.mueff / (self.dim + 2)**2\n\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            x = mean + sigma * (C_sqrt @ z.T).T\n\n            # Wavelet Mutation\n            for i in range(self.popsize):\n                if np.random.rand() < 0.1: # 10% chance of wavelet mutation\n                    coeffs = pywt.wavedec(x[i], self.wavelet, level=self.wavelet_level)\n                    detail_coeffs = [np.random.normal(0, 0.1 * sigma, size=len(coeff)) for coeff in coeffs[1:]] #Adjust the sigma scaling\n                    coeffs_modified = [coeffs[0]] + detail_coeffs\n                    try:\n                        x[i] = pywt.waverec(coeffs_modified, self.wavelet)\n                    except ValueError:\n                        pass # Handle potential size mismatch after wavelet transform\n                    x[i] = np.clip(x[i], func.bounds.lb, func.bounds.ub) #Clip to bounds\n                    \n\n            # Mirrored Sampling\n            num_mirrored = int(self.popsize * self.mirror_ratio)\n            z_mirrored = -z[:num_mirrored]\n            x_mirrored = mean + sigma * (C_sqrt @ z_mirrored.T).T\n            x = np.concatenate((x, x_mirrored), axis=0)\n            z = np.concatenate((z, z_mirrored), axis=0)\n\n            f = np.array([func(xi) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n            eval_count += len(x)\n\n            idx = np.argsort(f)\n            x = x[idx]\n            z = z[idx]\n            f = f[idx]\n\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[0]\n                self.stagnation_counter = 0\n                self.last_f_opt = f[0] #Store for stagnation criterion\n            else:\n                self.stagnation_counter += 1\n            \n            # Restart strategy\n            if self.stagnation_counter > self.restart_iterations:\n                if self.restart_strategy == \"random\":\n                    mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                elif self.restart_strategy == \"jitter\":\n                     mean = mean + 0.1 * np.random.normal(0, 1, size=self.dim) #Jitter the mean\n                     mean = np.clip(mean, func.bounds.lb, func.bounds.ub) #Ensure bounds\n                else:\n                    raise ValueError(\"Invalid restart strategy.\")\n\n                sigma = self.sigma0\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                self.stagnation_counter = 0\n\n            x_mu = x[:self.mu]\n            z_mu = z[:self.mu]\n\n            mean_new = np.sum(x_mu * self.weights[:, None], axis=0)\n            z_w = np.sum(z_mu * self.weights[:, None], axis=0)\n            \n            ps = (1 - self.cs) * ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * (C_invsqrt @ (mean_new - mean) / sigma)\n            \n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - self.cs)**(2 * eval_count / self.popsize)) < chiN * (1.4 + 2 / (self.dim + 1))\n            \n            pc = (1 - self.cc) * pc + hsig * np.sqrt(self.cc * (2 - self.cc) * self.mueff) * (mean_new - mean) / sigma\n\n            # Covariance matrix update with spectral decay\n            C = (1 - self.c1 - self.cmu) * C + self.c1 * (pc[:, None] @ pc[None, :])\n            for i in range(self.mu):\n                C += self.cmu * self.weights[i] * (z_mu[i, :, None] @ z_mu[i, None, :])\n\n            # Ensure symmetry\n            C = np.triu(C) + np.triu(C, 1).T\n            \n            sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(ps) / chiN - 1))\n            sigma *= self.damps # Apply damping\n            \n            # Orthogonal subspace learning every 10 generations\n            if eval_count % (10 * self.popsize) == 0:\n                subspace_size = min(self.dim // 2, 10)  # Adjust subspace size as needed\n                Q = self.orthogonal_basis[:, :subspace_size]\n                y = Q.T @ (mean_new - mean) #Project into subspace\n                C_subspace = Q.T @ C @ Q #Project covariance matrix\n                try:\n                    C_evals_sub, C_evecs_sub = np.linalg.eigh(C_subspace) #Eigen-decomposition\n                    Q_update = Q @ C_evecs_sub #Update vectors\n                    self.orthogonal_basis[:, :subspace_size] = Q_update #Update orthogonal basis\n                except np.linalg.LinAlgError:\n                    pass #Do not update\n\n            # Regularize and update covariance matrix\n            if eval_count % (self.popsize * 5) == 0:\n                C = np.triu(C) + np.triu(C, 1).T\n                try:\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_evals = np.maximum(C_evals, 1e-10)\n\n                    # Spectral Decay: Shrink smaller eigenvalues\n                    C_evals = C_evals * (1 - 0.01 * np.exp(-np.arange(self.dim)))\n\n                    C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n\n                     # Condition number based regularization\n                    condition_number = np.max(C_evals) / np.min(C_evals)\n                    if condition_number > self.condition_number_target:\n                        regularization_factor = 1e-8 * (condition_number / self.condition_number_target)\n                        C = C + regularization_factor * np.eye(self.dim)\n\n                except np.linalg.LinAlgError:\n                    C = np.eye(self.dim)\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n\n                # Regularize Covariance Matrix\n                C = C + 1e-8 * np.eye(self.dim)\n                \n            if np.any(np.isnan(mean)) or np.any(np.isnan(C)):\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                sigma = self.sigma0\n\n            mean = mean_new\n\n        return f_opt, x_opt", "configspace": "", "generation": 8, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 111, in evaluate\n    exec(code, safe_globals, local_env)\n  File \"<string>\", line 2, in <module>\nModuleNotFoundError: No module named 'pywt'\n.", "error": "", "parent_ids": ["e00ef585-ae18-4cad-9648-bf64037b7b97"], "operator": null, "metadata": {}}
{"id": "608aae4b-e602-4370-8bd7-68bed122c22c", "fitness": -Infinity, "name": "ActiveCMAES_SplineBound", "description": "CMA-ES with active covariance matrix adaptation based on the worst performing solutions, improving exploration in deceptive landscapes and better bound handling with a cubic spline interpolation.", "code": "import numpy as np\nfrom scipy.interpolate import CubicSpline\n\nclass ActiveCMAES_SplineBound(object):\n    def __init__(self, budget=10000, dim=10, sigma0=0.5, history_length=5, mirror_ratio=0.5, restart_strategy=\"random\", active_adaptation=True, spline_bound_handling=True):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.history_length = history_length\n        self.min_popsize = 4\n        self.max_popsize = 50\n        self.target_popsize = min(self.max_popsize, self.min_popsize + int(np.floor(3 * np.log(self.dim))))\n        self.popsize = self.target_popsize\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.step_history = []\n        self.last_f_opt = np.inf\n        self.stagnation_counter = 0\n        self.restart_iterations = 10 * self.dim  # More aggressive stagnation detection\n        self.mirror_ratio = mirror_ratio\n        self.restart_strategy = restart_strategy # \"random\" or \"jitter\"\n        self.condition_number_target = 1e14\n        self.damps = 1.0 # Damping factor for step size\n        self.cs = 0.2 # cumulation factor for step-size\n        self.ccov = 2 / (self.dim**2 + 6)\n        self.active_adaptation = active_adaptation\n        self.spline_bound_handling = spline_bound_handling\n        self.c_mu_neg = 0.1 * c_mu  # Learning rate for negative update\n\n    def __call__(self, func):\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n        c_sigma = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        c_c = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        c_1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        c_mu = min(1 - c_1, 2 * (self.mueff - 1 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        d_sigma = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + c_sigma\n\n        C_evals, C_evecs = np.linalg.eigh(C)\n        C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n        C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n        \n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n        diversity_threshold = 0.01  # Diversity threshold for stagnation\n        no_improvement_streak = 0 # Streak of no improvement\n\n        while eval_count < self.budget:\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            x = mean + sigma * (C_sqrt @ z.T).T\n\n            # Spline Bound Handling\n            if self.spline_bound_handling:\n                lb = func.bounds.lb\n                ub = func.bounds.ub\n                \n                # Define spline interpolation points and values\n                num_interp_points = 5\n                interp_points = np.linspace(lb, ub, num_interp_points)\n                \n                # Evaluate function at interpolation points\n                interp_values = np.array([func(np.full(self.dim, point)) for point in interp_points])\n                \n                # Create cubic spline\n                spline = CubicSpline(interp_points, interp_values)\n                \n                # Clip x values and adjust based on spline\n                for i in range(x.shape[0]):\n                    for j in range(x.shape[1]):\n                        if x[i, j] < lb or x[i, j] > ub:\n                            if x[i, j] < lb:\n                                x[i, j] = lb + 0.01 * (ub - lb) * np.random.rand()\n                            elif x[i, j] > ub:\n                                x[i, j] = ub - 0.01 * (ub - lb) * np.random.rand()\n\n            # Mirrored Sampling\n            num_mirrored = int(self.popsize * self.mirror_ratio)\n            z_mirrored = -z[:num_mirrored]\n            x_mirrored = mean + sigma * (C_sqrt @ z_mirrored.T).T\n            x = np.concatenate((x, x_mirrored), axis=0)\n            z = np.concatenate((z, z_mirrored), axis=0)\n\n            f = np.array([func(xi) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n            eval_count += len(x)\n\n            idx = np.argsort(f)\n            x = x[idx]\n            z = z[idx]\n            f = f[idx]\n\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[0]\n                self.stagnation_counter = 0\n                self.last_f_opt = f[0] #Store for stagnation criterion\n                no_improvement_streak = 0  # Reset no improvement streak\n            else:\n                self.stagnation_counter += 1\n                no_improvement_streak += 1 # Increment no improvement streak\n            \n            # Restart strategy\n            if self.stagnation_counter > self.restart_iterations or no_improvement_streak > 2 * self.restart_iterations:\n                if self.restart_strategy == \"random\":\n                    mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                elif self.restart_strategy == \"jitter\":\n                     mean = mean + 0.1 * np.random.normal(0, 1, size=self.dim) #Jitter the mean\n                     mean = np.clip(mean, func.bounds.lb, func.bounds.ub) #Ensure bounds\n                else:\n                    raise ValueError(\"Invalid restart strategy.\")\n\n                sigma = self.sigma0\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                self.stagnation_counter = 0\n                no_improvement_streak = 0 #reset streak\n\n            x_mu = x[:self.mu]\n            z_mu = z[:self.mu]\n\n            mean_new = np.sum(x_mu * self.weights[:, None], axis=0)\n            z_w = np.sum(z_mu * self.weights[:, None], axis=0)\n            \n            ps = (1 - self.cs) * ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * (C_invsqrt @ (mean_new - mean) / sigma)\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - self.cs)**(2 * eval_count / self.popsize)) < chiN * (1.4 + 2 / (self.dim + 1))\n            pc = (1 - c_c) * pc + hsig * np.sqrt(c_c * (2 - c_c) * self.mueff) * (mean_new - mean) / sigma\n\n            C = (1 - c_1 - c_mu) * C + c_1 * (pc[:, None] @ pc[None, :])\n            for i in range(self.mu):\n                C += c_mu * self.weights[i] * (z_mu[i, :, None] @ z_mu[i, None, :])\n\n            # Active CMA Update\n            if self.active_adaptation:\n                mu_w = self.popsize - self.mu\n                weights_neg = np.ones(mu_w) * -1 / (mu_w)\n                z_worst = z[self.mu:]\n                for i in range(mu_w):\n                    C += self.c_mu_neg * weights_neg[i] * (z_worst[i, :, None] @ z_worst[i, None, :])\n\n            \n            sigma *= np.exp((c_sigma / d_sigma) * (np.linalg.norm(ps) / chiN - 1))\n            sigma *= self.damps # Apply damping\n\n            # Simplified Step history\n            step = (mean_new - mean) / sigma\n            self.step_history.append(step)\n            if len(self.step_history) > self.history_length:\n                self.step_history.pop(0)\n\n            # Adapt C based on step history, simplified\n            for h_step in self.step_history:\n                C += 0.1 * c_mu * (h_step[:, None] @ h_step[None, :])\n\n            mean = mean_new\n            \n            if eval_count % (self.popsize * 5) == 0:\n                C = np.triu(C) + np.triu(C, 1).T\n                try:\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_evals = np.maximum(C_evals, 1e-10)\n                    C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n\n                     # Condition number based regularization\n                    condition_number = np.max(C_evals) / np.min(C_evals)\n                    if condition_number > self.condition_number_target:\n                        regularization_factor = 1e-8 * (condition_number / self.condition_number_target)\n                        C = C + regularization_factor * np.eye(self.dim)\n\n                     # Spectral clipping regularization\n                    C_evals = np.clip(C_evals, np.max(C_evals) / self.condition_number_target, np.max(C_evals))\n                    C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n\n                except np.linalg.LinAlgError:\n                    C = np.eye(self.dim)\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n\n                # Regularize Covariance Matrix\n                C = C + 1e-8 * np.eye(self.dim)\n                \n            if np.any(np.isnan(mean)) or np.any(np.isnan(C)):\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                sigma = self.sigma0\n\n        return f_opt, x_opt", "configspace": "", "generation": 8, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 120, in evaluate\n    algorithm = local_env[algorithm_name](budget=100, dim=2)\n  File \"<string>\", line 30, in __init__\nNameError: name 'c_mu' is not defined\n.", "error": "", "parent_ids": ["24bca9ca-66d4-46c6-940e-5e4b89072caa"], "operator": null, "metadata": {}}
{"id": "7b3f568f-7f12-49e0-9fff-d5b9a3ce8e76", "fitness": 0.669147017913061, "name": "AdvancedCMAES_OrthoPopSizeV2", "description": "CMA-ES with orthogonal basis guided sampling, adaptive mirrored sampling fraction, and covariance matrix adaptation with spectral decay and dynamic regularization based on stagnation and condition number.", "code": "import numpy as np\n\nclass AdvancedCMAES_OrthoPopSizeV2(object):\n    def __init__(self, budget=10000, dim=10, sigma0=0.5, history_length=5, mirror_ratio=0.5, restart_strategy=\"random\"):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.history_length = history_length\n        self.min_popsize = 4\n        self.max_popsize = 50\n        self.target_popsize = min(self.max_popsize, self.min_popsize + int(np.floor(3 * np.log(self.dim))))\n        self.popsize = self.target_popsize\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.step_history = []\n        self.last_f_opt = np.inf\n        self.stagnation_counter = 0\n        self.restart_iterations = 10 * self.dim  # More aggressive stagnation detection\n        self.mirror_ratio = mirror_ratio\n        self.restart_strategy = restart_strategy # \"random\" or \"jitter\"\n        self.condition_number_target = 1e14\n        self.damps = 1.0 # Damping factor for step size\n        self.cs = 0.3  # Step-size learning rate\n        self.cc = 0.4  # Cumulation for covariance matrix rank-one update\n        self.c1 = 1.5 / (self.dim + 2)**2  # Learning rate for rank-one update\n        self.cmu = 1.5 * self.mueff / (self.dim + 2)**2  # Learning rate for rank-mu update\n        self.init_lambda = 2 + np.floor(3 * np.log(self.dim))\n        self.lambda_factor = 2\n        self.orthogonal_basis = np.random.randn(self.dim, self.dim)\n        self.orthogonal_basis, _ = np.linalg.qr(self.orthogonal_basis)\n        self.mirrored_sampling_adaptation_rate = 0.1\n        self.min_mirror_ratio = 0.1\n        self.max_mirror_ratio = 0.9\n\n    def __call__(self, func):\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n        C_evals, C_evecs = np.linalg.eigh(C)\n        C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n        C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n        \n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n\n        while eval_count < self.budget:\n            # Adapt population size\n            self.target_popsize = min(self.max_popsize, self.min_popsize + int(np.floor(3 * np.log(self.dim))))\n            if self.target_popsize != self.popsize:\n                self.popsize = self.target_popsize\n                self.mu = self.popsize // 2\n                self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n                self.weights = self.weights / np.sum(self.weights)\n                self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n                self.cmu = 1.5 * self.mueff / (self.dim + 2)**2\n\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            x = mean + sigma * (C_sqrt @ z.T).T\n\n            # Mirrored Sampling\n            num_mirrored = int(self.popsize * self.mirror_ratio)\n            z_mirrored = -z[:num_mirrored]\n            x_mirrored = mean + sigma * (C_sqrt @ z_mirrored.T).T\n            x = np.concatenate((x, x_mirrored), axis=0)\n            z = np.concatenate((z, z_mirrored), axis=0)\n\n            f = np.array([func(xi) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n            eval_count += len(x)\n\n            idx = np.argsort(f)\n            x = x[idx]\n            z = z[idx]\n            f = f[idx]\n\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[0]\n                self.stagnation_counter = 0\n                self.last_f_opt = f[0] #Store for stagnation criterion\n                # Adapt mirror ratio based on success\n                self.mirror_ratio += self.mirrored_sampling_adaptation_rate * (1 - self.mirror_ratio)\n                self.mirror_ratio = min(self.mirror_ratio, self.max_mirror_ratio)\n            else:\n                self.stagnation_counter += 1\n                #Reduce mirror ratio if no improvement\n                self.mirror_ratio -= self.mirrored_sampling_adaptation_rate * self.mirror_ratio\n                self.mirror_ratio = max(self.mirror_ratio, self.min_mirror_ratio)\n            \n            # Restart strategy\n            if self.stagnation_counter > self.restart_iterations:\n                if self.restart_strategy == \"random\":\n                    mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                elif self.restart_strategy == \"jitter\":\n                     mean = mean + 0.1 * np.random.normal(0, 1, size=self.dim) #Jitter the mean\n                     mean = np.clip(mean, func.bounds.lb, func.bounds.ub) #Ensure bounds\n                else:\n                    raise ValueError(\"Invalid restart strategy.\")\n\n                sigma = self.sigma0\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                self.stagnation_counter = 0\n\n            x_mu = x[:self.mu]\n            z_mu = z[:self.mu]\n\n            mean_new = np.sum(x_mu * self.weights[:, None], axis=0)\n            z_w = np.sum(z_mu * self.weights[:, None], axis=0)\n            \n            ps = (1 - self.cs) * ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * (C_invsqrt @ (mean_new - mean) / sigma)\n            \n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - self.cs)**(2 * eval_count / self.popsize)) < chiN * (1.4 + 2 / (self.dim + 1))\n            \n            pc = (1 - self.cc) * pc + hsig * np.sqrt(self.cc * (2 - self.cc) * self.mueff) * (mean_new - mean) / sigma\n\n            # Covariance matrix update with spectral decay\n            C = (1 - self.c1 - self.cmu) * C + self.c1 * (pc[:, None] @ pc[None, :])\n            for i in range(self.mu):\n                C += self.cmu * self.weights[i] * (z_mu[i, :, None] @ z_mu[i, None, :])\n\n            # Ensure symmetry\n            C = np.triu(C) + np.triu(C, 1).T\n            \n            sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(ps) / chiN - 1))\n            sigma *= self.damps # Apply damping\n            \n            # Orthogonal subspace learning every 10 generations\n            if eval_count % (10 * self.popsize) == 0:\n                subspace_size = min(self.dim // 2, 10)  # Adjust subspace size as needed\n                Q = self.orthogonal_basis[:, :subspace_size]\n                y = Q.T @ (mean_new - mean) #Project into subspace\n                C_subspace = Q.T @ C @ Q #Project covariance matrix\n                try:\n                    C_evals_sub, C_evecs_sub = np.linalg.eigh(C_subspace) #Eigen-decomposition\n                    Q_update = Q @ C_evecs_sub #Update vectors\n                    self.orthogonal_basis[:, :subspace_size] = Q_update #Update orthogonal basis\n                except np.linalg.LinAlgError:\n                    pass #Do not update\n\n            # Regularize and update covariance matrix\n            if eval_count % (self.popsize * 5) == 0:\n                C = np.triu(C) + np.triu(C, 1).T\n                try:\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_evals = np.maximum(C_evals, 1e-10)\n\n                    # Spectral Decay: Shrink smaller eigenvalues\n                    C_evals = C_evals * (1 - 0.01 * np.exp(-np.arange(self.dim)))\n\n                    C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n\n                     # Condition number based regularization\n                    condition_number = np.max(C_evals) / np.min(C_evals)\n                    if condition_number > self.condition_number_target:\n                        regularization_factor = 1e-8 * (condition_number / self.condition_number_target)\n                        C = C + regularization_factor * np.eye(self.dim)\n\n                except np.linalg.LinAlgError:\n                    C = np.eye(self.dim)\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n\n                # Regularize Covariance Matrix\n                C = C + 1e-8 * np.eye(self.dim)\n                \n            if np.any(np.isnan(mean)) or np.any(np.isnan(C)):\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                sigma = self.sigma0\n\n            mean = mean_new\n\n        return f_opt, x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdvancedCMAES_OrthoPopSizeV2 scored 0.669 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e00ef585-ae18-4cad-9648-bf64037b7b97"], "operator": null, "metadata": {"aucs": [0.263863211219181, 0.1808785918702429, 0.9281007717980536, 0.9666152330507333, 0.9470845673289371, 0.9468884596371647, 0.31056199896059056, 0.9246625268430164, 0.9313396254129875, 0.21285346531186278, 0.9741332235858984, 0.9862494845935471, 0.2504670025243858, 0.9314297554913574, 0.6339938466454925, 0.9346384135350345, 0.3858986600802925, 0.9541405217125009, 0.24032255309184014, 0.47881844556810116]}}
{"id": "016d45cd-ed98-460a-9a44-3a43bd779e35", "fitness": 0.7082101423320475, "name": "AdaptiveCMAES_ConditionNumStagnation", "description": "CMA-ES with an adaptive learning rate for the covariance matrix based on the condition number and stagnation detection, along with spectral decay and orthogonal basis learning for better exploration.", "code": "import numpy as np\n\nclass AdaptiveCMAES_ConditionNumStagnation(object):\n    def __init__(self, budget=10000, dim=10, sigma0=0.5, history_length=5, mirror_ratio=0.5, restart_strategy=\"random\"):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.history_length = history_length\n        self.min_popsize = 4\n        self.max_popsize = 50\n        self.target_popsize = min(self.max_popsize, self.min_popsize + int(np.floor(3 * np.log(self.dim))))\n        self.popsize = self.target_popsize\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.step_history = []\n        self.last_f_opt = np.inf\n        self.stagnation_counter = 0\n        self.restart_iterations = 10 * self.dim  # More aggressive stagnation detection\n        self.mirror_ratio = mirror_ratio\n        self.restart_strategy = restart_strategy # \"random\" or \"jitter\"\n        self.condition_number_target = 1e14\n        self.damps = 1.0 # Damping factor for step size\n        self.cs = 0.3  # Step-size learning rate\n        self.cc = 0.4  # Cumulation for covariance matrix rank-one update\n        self.c1 = 1.5 / (self.dim + 2)**2  # Learning rate for rank-one update\n        self.cmu = 1.5 * self.mueff / (self.dim + 2)**2  # Learning rate for rank-mu update\n        self.init_lambda = 2 + np.floor(3 * np.log(self.dim))\n        self.lambda_factor = 2\n        self.orthogonal_basis = np.random.randn(self.dim, self.dim)\n        self.orthogonal_basis, _ = np.linalg.qr(self.orthogonal_basis)\n        self.condition_number_history = []\n        self.condition_number_learning_rate = 0.1 # Learning rate for condition number adaptation\n\n    def __call__(self, func):\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n        C_evals, C_evecs = np.linalg.eigh(C)\n        C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n        C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n        \n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n\n        while eval_count < self.budget:\n            # Adapt population size\n            self.target_popsize = min(self.max_popsize, self.min_popsize + int(np.floor(3 * np.log(self.dim))))\n            if self.target_popsize != self.popsize:\n                self.popsize = self.target_popsize\n                self.mu = self.popsize // 2\n                self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n                self.weights = self.weights / np.sum(self.weights)\n                self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n                self.cmu = 1.5 * self.mueff / (self.dim + 2)**2\n\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            x = mean + sigma * (C_sqrt @ z.T).T\n\n            # Mirrored Sampling\n            num_mirrored = int(self.popsize * self.mirror_ratio)\n            z_mirrored = -z[:num_mirrored]\n            x_mirrored = mean + sigma * (C_sqrt @ z_mirrored.T).T\n            x = np.concatenate((x, x_mirrored), axis=0)\n            z = np.concatenate((z, z_mirrored), axis=0)\n\n            f = np.array([func(xi) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n            eval_count += len(x)\n\n            idx = np.argsort(f)\n            x = x[idx]\n            z = z[idx]\n            f = f[idx]\n\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[0]\n                self.stagnation_counter = 0\n                self.last_f_opt = f[0] #Store for stagnation criterion\n            else:\n                self.stagnation_counter += 1\n            \n            # Restart strategy\n            if self.stagnation_counter > self.restart_iterations:\n                if self.restart_strategy == \"random\":\n                    mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                elif self.restart_strategy == \"jitter\":\n                     mean = mean + 0.1 * np.random.normal(0, 1, size=self.dim) #Jitter the mean\n                     mean = np.clip(mean, func.bounds.lb, func.bounds.ub) #Ensure bounds\n                else:\n                    raise ValueError(\"Invalid restart strategy.\")\n\n                sigma = self.sigma0\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                self.stagnation_counter = 0\n\n            x_mu = x[:self.mu]\n            z_mu = z[:self.mu]\n\n            mean_new = np.sum(x_mu * self.weights[:, None], axis=0)\n            z_w = np.sum(z_mu * self.weights[:, None], axis=0)\n            \n            ps = (1 - self.cs) * ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * (C_invsqrt @ (mean_new - mean) / sigma)\n            \n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - self.cs)**(2 * eval_count / self.popsize)) < chiN * (1.4 + 2 / (self.dim + 1))\n            \n            pc = (1 - self.cc) * pc + hsig * np.sqrt(self.cc * (2 - self.cc) * self.mueff) * (mean_new - mean) / sigma\n\n            # Covariance matrix update with spectral decay\n            C = (1 - self.c1 - self.cmu) * C + self.c1 * (pc[:, None] @ pc[None, :])\n            for i in range(self.mu):\n                C += self.cmu * self.weights[i] * (z_mu[i, :, None] @ z_mu[i, None, :])\n\n            # Ensure symmetry\n            C = np.triu(C) + np.triu(C, 1).T\n            \n            sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(ps) / chiN - 1))\n            sigma *= self.damps # Apply damping\n            \n            # Orthogonal subspace learning every 10 generations\n            if eval_count % (10 * self.popsize) == 0:\n                subspace_size = min(self.dim // 2, 10)  # Adjust subspace size as needed\n                Q = self.orthogonal_basis[:, :subspace_size]\n                y = Q.T @ (mean_new - mean) #Project into subspace\n                C_subspace = Q.T @ C @ Q #Project covariance matrix\n                try:\n                    C_evals_sub, C_evecs_sub = np.linalg.eigh(C_subspace) #Eigen-decomposition\n                    Q_update = Q @ C_evecs_sub #Update vectors\n                    self.orthogonal_basis[:, :subspace_size] = Q_update #Update orthogonal basis\n                except np.linalg.LinAlgError:\n                    pass #Do not update\n\n            # Regularize and update covariance matrix\n            if eval_count % (self.popsize * 5) == 0:\n                C = np.triu(C) + np.triu(C, 1).T\n                try:\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_evals = np.maximum(C_evals, 1e-10)\n\n                    # Spectral Decay: Shrink smaller eigenvalues\n                    C_evals = C_evals * (1 - 0.01 * np.exp(-np.arange(self.dim)))\n\n                    C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n\n                     # Condition number based regularization and learning\n                    condition_number = np.max(C_evals) / np.min(C_evals)\n                    self.condition_number_history.append(condition_number)\n                    \n                    # Adaptive cmu: Increase learning rate if condition number is high, decrease if low\n                    deviation = np.log10(condition_number) - np.log10(self.condition_number_target)\n                    adaptive_cmu_factor = np.clip(1 + self.condition_number_learning_rate * deviation, 0.1, 10) #Clip the factor\n                    self.cmu = 1.5 * self.mueff / (self.dim + 2)**2 * adaptive_cmu_factor\n\n                    if condition_number > self.condition_number_target:\n                        regularization_factor = 1e-8 * (condition_number / self.condition_number_target)\n                        C = C + regularization_factor * np.eye(self.dim)\n\n                except np.linalg.LinAlgError:\n                    C = np.eye(self.dim)\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n\n                # Regularize Covariance Matrix\n                C = C + 1e-8 * np.eye(self.dim)\n                \n            if np.any(np.isnan(mean)) or np.any(np.isnan(C)):\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                sigma = self.sigma0\n\n            mean = mean_new\n\n        return f_opt, x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveCMAES_ConditionNumStagnation scored 0.708 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e00ef585-ae18-4cad-9648-bf64037b7b97"], "operator": null, "metadata": {"aucs": [0.25931974430120863, 0.8257539283802957, 0.9345353010492217, 0.19780594811490715, 0.9447868251110567, 0.9432522121971105, 0.322635575542203, 0.929425425202395, 0.9392943537286109, 0.18812443125826672, 0.9521377040861962, 0.9860644341841196, 0.27517710636032566, 0.8961126057276073, 0.6329549707056457, 0.8745538410485308, 0.8934909077449362, 0.770716134612446, 0.8881058457811375, 0.5099555515047303]}}
{"id": "40dcbeaf-531b-4cb9-87d8-bb3db3a32d79", "fitness": 0.5374998219286671, "name": "CMAES_AdaptiveMirrorRankHyperbolic_SelfAdaptC1Cmu_ConditionNumber", "description": "CMA-ES with dynamically adjusted mirrored sampling, rank-based covariance update, adaptive step size using a history buffer and self-adaptation for c1/cmu based on the condition number of C and enhanced bound handling.", "code": "import numpy as np\n\nclass CMAES_AdaptiveMirrorRankHyperbolic_SelfAdaptC1Cmu_ConditionNumber:\n    def __init__(self, budget=10000, dim=10, sigma0=0.5, history_length=10, initial_mirrored_fraction=0.5, mirrored_decay=0.99, hyperbolic_decay_factor=0.1, initial_c1=None, initial_cmu=None, condition_number_adaptation_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.history_length = history_length\n        self.mirrored_fraction = initial_mirrored_fraction  # Initial fraction of population to mirror\n        self.mirrored_decay = mirrored_decay  # Decay rate for mirrored fraction\n        self.hyperbolic_decay_factor = hyperbolic_decay_factor\n        self.condition_number_adaptation_rate = condition_number_adaptation_rate\n\n        self.popsize = 4 + int(np.floor(3 * np.log(self.dim)))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.step_history = []\n\n        # Initialize c_1 and c_mu\n        if initial_c1 is None:\n            self.c_1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        else:\n            self.c_1 = initial_c1\n        if initial_cmu is None:\n            self.c_mu = min(1 - self.c_1, 2 * (self.mueff - 1 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        else:\n            self.c_mu = initial_cmu\n\n        self.damp_C = 0.2 + self.c_1 + self.c_mu  # Damping factor for covariance matrix update\n        self.c_sigma = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.c_c = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        self.d_sigma = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1)\n\n    def __call__(self, func):\n        # Initialize variables\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n\n        # Eigen decomposition of C\n        C_evals, C_evecs = np.linalg.eigh(C)\n        C_evals = np.maximum(C_evals, 1e-10)\n        C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n        C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n        \n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n\n        while eval_count < self.budget:\n            # Sample population\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            x = mean + sigma * (C_sqrt @ z.T).T\n\n            # Mirrored sampling with hyperbolic decay\n            mirrored_fraction = self.mirrored_fraction / (1 + self.hyperbolic_decay_factor * (eval_count / self.budget))\n\n            num_mirrored = int(self.popsize * mirrored_fraction)\n            x_mirrored = mean - sigma * (C_sqrt @ z[:num_mirrored].T).T\n            x = np.vstack((x, x_mirrored))\n            z_mirrored = -z[:num_mirrored]\n            z = np.vstack((z, z_mirrored))\n            \n            # Clipping to bounds\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            \n            # Evaluate the new points\n            f = np.array([func(xi) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n            eval_count += len(x) \n\n            # Sort by fitness\n            idx = np.argsort(f)\n            x = x[idx]\n            z = z[idx]\n            f = f[idx]\n\n            # Update optimal solution\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[0]\n                \n            # Selection and recombination\n            x_mu = x[:self.mu]\n            z_mu = z[:self.mu]\n\n            mean_new = np.sum(x_mu * self.weights[:,None], axis=0)\n            z_w = np.sum(z_mu * self.weights[:,None], axis=0)\n            \n            # Covariance matrix adaptation using rank-mu update\n            ps = (1 - self.c_sigma) * ps + np.sqrt(self.c_sigma * (2 - self.c_sigma) * self.mueff) * (C_invsqrt @ (mean_new - mean) / sigma)\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - self.c_sigma)**(2 * eval_count / self.popsize)) < chiN * (1.4 + 2 / (self.dim + 1))\n            \n            pc = (1 - self.c_c) * pc + hsig * np.sqrt(self.c_c * (2 - self.c_c) * self.mueff) * (mean_new - mean) / sigma\n\n            C = (1 - self.c_1 - self.c_mu + self.damp_C * self.c_1) * C + self.c_1 * (pc[:, None] @ pc[None, :])\n\n            # Rank-mu update\n            for i in range(self.mu):\n                C += self.c_mu * self.weights[i] * (z_mu[i, :, None] @ z_mu[i, None, :])\n            \n            # Update step size using conjugate evolution path\n            step = (mean_new - mean) / sigma\n            self.step_history.append(step)\n            if len(self.step_history) > self.history_length:\n                self.step_history.pop(0)\n\n            # Adapt step size based on step history\n            step_correlation = 0\n            for h_step in self.step_history:\n                step_correlation += np.dot(step, h_step)\n            \n            sigma_factor = np.exp(0.1 * step_correlation / self.dim)\n            sigma *= np.exp((self.c_sigma / self.d_sigma) * (np.linalg.norm(ps) / chiN - 1)) * sigma_factor\n\n            # Update mean\n            mean = mean_new\n\n            # Adaptive Mirrored fraction (decay moved to start)\n            self.mirrored_fraction *= self.mirrored_decay\n\n            # Eigen decomposition of C\n            if eval_count % (self.popsize * 5) == 0:\n                C = np.triu(C) + np.triu(C, 1).T\n                try:\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_evals = np.maximum(C_evals, 1e-10)\n                    C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                except np.linalg.LinAlgError:\n                    print(\"LinAlgError encountered, resetting C\")\n                    C = np.eye(self.dim)\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n\n                # Self-adaptation of c_1 and c_mu based on condition number of C\n                condition_number = np.max(C_evals) / np.min(C_evals)\n                self.c_1 *= np.exp(self.condition_number_adaptation_rate * (condition_number - 1))\n                self.c_mu *= np.exp(-self.condition_number_adaptation_rate * (condition_number - 1))\n\n                # Ensure c_1 and c_mu remain within reasonable bounds\n                self.c_1 = np.clip(self.c_1, 1e-8, 1.0)\n                self.c_mu = np.clip(self.c_mu, 1e-8, 1.0)\n\n                self.damp_C = 0.2 + self.c_1 + self.c_mu  # Update damping factor\n\n            if np.any(np.isnan(mean)) or np.any(np.isnan(C)):\n                print(\"NaN detected, resetting...\")\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                sigma = self.sigma0\n        \n        return f_opt, x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm CMAES_AdaptiveMirrorRankHyperbolic_SelfAdaptC1Cmu_ConditionNumber scored 0.537 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["41e310bc-fcf5-4b15-9cbf-b4a3ad61eace"], "operator": null, "metadata": {"aucs": [0.2180236473146746, 0.2406677978385242, 0.5114641676226974, 0.9656050296761882, 0.7095961836933604, 0.963851696274324, 0.3269739342191461, 0.49508453262268837, 0.5086273724127228, 0.21671868585184206, 0.9640703297982561, 0.998489398502308, 0.30614521817414864, 0.5623157896403279, 0.7248687832761169, 0.35305232345260573, 0.2641465402367411, 0.7941257196038749, 0.14362496654285473, 0.48254432181993834]}}
{"id": "69fc4a83-0ef4-4785-b724-d0dbeacfdfdf", "fitness": 0.4949959784334904, "name": "RobustCMAES_ProPlusAdaptivePop", "description": "Robust CMA-ES with an adaptive population size adjustment based on the success rate of improvement, spectral regularization and dynamic damping of the step size using a history buffer.", "code": "import numpy as np\n\nclass RobustCMAES_ProPlusAdaptivePop(object):\n    def __init__(self, budget=10000, dim=10, sigma0=0.5, history_length=5, mirror_ratio=0.5, restart_strategy=\"random\", initial_popsize=None):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.history_length = history_length\n        self.min_popsize = 4\n        self.max_popsize = 50\n        if initial_popsize is None:\n             self.target_popsize = min(self.max_popsize, self.min_popsize + int(np.floor(3 * np.log(self.dim))))\n        else:\n            self.target_popsize = initial_popsize\n        self.popsize = self.target_popsize\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.step_history = []\n        self.last_f_opt = np.inf\n        self.stagnation_counter = 0\n        self.restart_iterations = 10 * self.dim  # More aggressive stagnation detection\n        self.mirror_ratio = mirror_ratio\n        self.restart_strategy = restart_strategy # \"random\" or \"jitter\"\n        self.condition_number_target = 1e14\n        self.damps = 1.0 # Damping factor for step size\n        self.cs = 0.2 # cumulation factor for step-size\n        self.ccov = 2 / (self.dim**2 + 6)\n        self.success_rate = 0.5  # Initial success rate\n        self.success_history = [] #History of success rate\n\n    def __call__(self, func):\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n        c_sigma = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        c_c = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        c_1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        c_mu = min(1 - c_1, 2 * (self.mueff - 1 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        d_sigma = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + c_sigma\n\n        C_evals, C_evecs = np.linalg.eigh(C)\n        C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n        C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n        \n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n        diversity_threshold = 0.01  # Diversity threshold for stagnation\n        no_improvement_streak = 0 # Streak of no improvement\n        improvement_count = 0\n\n        while eval_count < self.budget:\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            x = mean + sigma * (C_sqrt @ z.T).T\n\n            # Mirrored Sampling\n            num_mirrored = int(self.popsize * self.mirror_ratio)\n            z_mirrored = -z[:num_mirrored]\n            x_mirrored = mean + sigma * (C_sqrt @ z_mirrored.T).T\n            x = np.concatenate((x, x_mirrored), axis=0)\n            z = np.concatenate((z, z_mirrored), axis=0)\n\n            f = np.array([func(xi) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n            eval_count += len(x)\n\n            idx = np.argsort(f)\n            x = x[idx]\n            z = z[idx]\n            f = f[idx]\n\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[0]\n                self.stagnation_counter = 0\n                self.last_f_opt = f[0] #Store for stagnation criterion\n                no_improvement_streak = 0  # Reset no improvement streak\n                improvement_count +=1\n            else:\n                self.stagnation_counter += 1\n                no_improvement_streak += 1 # Increment no improvement streak\n            \n            # Update success rate\n            if eval_count > self.popsize: # Start adapting after initial population\n                self.success_rate = improvement_count / (eval_count/self.popsize)\n                self.success_history.append(self.success_rate)\n\n                # Adjust population size based on success rate\n                if self.success_rate > 0.25 and self.popsize < self.max_popsize:\n                    self.target_popsize = min(self.popsize + 1, self.max_popsize)\n                    self.popsize = self.target_popsize\n                    self.mu = self.popsize // 2\n                    self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n                    self.weights = self.weights / np.sum(self.weights)\n                    self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n                elif self.success_rate < 0.15 and self.popsize > self.min_popsize:\n                    self.target_popsize = max(self.popsize - 1, self.min_popsize)\n                    self.popsize = self.target_popsize\n                    self.mu = self.popsize // 2\n                    self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n                    self.weights = self.weights / np.sum(self.weights)\n                    self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n\n            # Restart strategy\n            if self.stagnation_counter > self.restart_iterations or no_improvement_streak > 2 * self.restart_iterations:\n                if self.restart_strategy == \"random\":\n                    mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                elif self.restart_strategy == \"jitter\":\n                     mean = mean + 0.1 * np.random.normal(0, 1, size=self.dim) #Jitter the mean\n                     mean = np.clip(mean, func.bounds.lb, func.bounds.ub) #Ensure bounds\n                else:\n                    raise ValueError(\"Invalid restart strategy.\")\n\n                sigma = self.sigma0\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                self.stagnation_counter = 0\n                no_improvement_streak = 0 #reset streak\n\n            x_mu = x[:self.mu]\n            z_mu = z[:self.mu]\n\n            mean_new = np.sum(x_mu * self.weights[:, None], axis=0)\n            z_w = np.sum(z_mu * self.weights[:, None], axis=0)\n            \n            ps = (1 - self.cs) * ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * (C_invsqrt @ (mean_new - mean) / sigma)\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - self.cs)**(2 * eval_count / self.popsize)) < chiN * (1.4 + 2 / (self.dim + 1))\n            pc = (1 - c_c) * pc + hsig * np.sqrt(c_c * (2 - c_c) * self.mueff) * (mean_new - mean) / sigma\n\n            C = (1 - c_1 - c_mu) * C + c_1 * (pc[:, None] @ pc[None, :])\n            for i in range(self.mu):\n                C += c_mu * self.weights[i] * (z_mu[i, :, None] @ z_mu[i, None, :])\n            \n            sigma *= np.exp((c_sigma / d_sigma) * (np.linalg.norm(ps) / chiN - 1))\n            \n            # Dynamically Adjust Damping\n            step_norm = np.linalg.norm(mean_new - mean) / sigma\n            self.step_history.append(step_norm)\n            if len(self.step_history) > self.history_length:\n                self.step_history.pop(0)\n\n            damping_factor = 1 + 0.1 * (np.mean(self.step_history) - 1)\n            self.damps = max(0.1, self.damps * damping_factor) # Ensure damping is not too small.\n\n            sigma *= self.damps # Apply damping\n\n            mean = mean_new\n            \n            if eval_count % (self.popsize * 5) == 0:\n                C = np.triu(C) + np.triu(C, 1).T\n                try:\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_evals = np.maximum(C_evals, 1e-10)\n                    C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n\n                     # Condition number based regularization\n                    condition_number = np.max(C_evals) / np.min(C_evals)\n                    if condition_number > self.condition_number_target:\n                        regularization_factor = 1e-8 * (condition_number / self.condition_number_target)\n                        C = C + regularization_factor * np.eye(self.dim)\n\n                     # Spectral clipping regularization\n                    C_evals = np.clip(C_evals, np.max(C_evals) / self.condition_number_target, np.max(C_evals))\n                    C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n\n                except np.linalg.LinAlgError:\n                    C = np.eye(self.dim)\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n\n                # Regularize Covariance Matrix\n                C = C + 1e-8 * np.eye(self.dim)\n                \n            if np.any(np.isnan(mean)) or np.any(np.isnan(C)):\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                sigma = self.sigma0\n\n        return f_opt, x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm RobustCMAES_ProPlusAdaptivePop scored 0.495 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["24bca9ca-66d4-46c6-940e-5e4b89072caa"], "operator": null, "metadata": {"aucs": [0.2098317381872411, 0.18301395662182773, 0.2685993674058824, 0.9271367092160131, 0.8793734277808003, 0.9132728742010756, 0.26058944214927915, 0.20223021633605298, 0.8010242751088906, 0.2201781417177786, 0.4546232759844906, 0.9854663235329783, 0.2184830187506248, 0.5617529203331912, 0.5579067384968394, 0.3313618280401184, 0.2899855220807426, 0.9127127378046107, 0.23536885145411812, 0.48700820346725127]}}
{"id": "a768949f-2134-4e3b-81d4-efff6a4cf961", "fitness": 0.6979350254078521, "name": "AdvancedCMAES_OrthoPopSizeDynSigma", "description": "CMA-ES with adaptive population sizing, spectral decay covariance matrix adaptation, orthogonal learning to improve exploration, and dynamic step size adaptation based on objective similarity.", "code": "import numpy as np\n\nclass AdvancedCMAES_OrthoPopSizeDynSigma(object):\n    def __init__(self, budget=10000, dim=10, sigma0=0.5, history_length=5, mirror_ratio=0.5, restart_strategy=\"random\"):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.history_length = history_length\n        self.min_popsize = 4\n        self.max_popsize = 50\n        self.target_popsize = min(self.max_popsize, self.min_popsize + int(np.floor(3 * np.log(self.dim))))\n        self.popsize = self.target_popsize\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.step_history = []\n        self.last_f_opt = np.inf\n        self.stagnation_counter = 0\n        self.restart_iterations = 10 * self.dim  # More aggressive stagnation detection\n        self.mirror_ratio = mirror_ratio\n        self.restart_strategy = restart_strategy # \"random\" or \"jitter\"\n        self.condition_number_target = 1e14\n        self.damps = 1.0 # Damping factor for step size\n        self.cs = 0.3  # Step-size learning rate\n        self.cc = 0.4  # Cumulation for covariance matrix rank-one update\n        self.c1 = 1.5 / (self.dim + 2)**2  # Learning rate for rank-one update\n        self.cmu = 1.5 * self.mueff / (self.dim + 2)**2  # Learning rate for rank-mu update\n        self.init_lambda = 2 + np.floor(3 * np.log(self.dim))\n        self.lambda_factor = 2\n        self.orthogonal_basis = np.random.randn(self.dim, self.dim)\n        self.orthogonal_basis, _ = np.linalg.qr(self.orthogonal_basis)\n        self.sigma_threshold = 1e-12 # Minimum step size\n\n    def __call__(self, func):\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n        C_evals, C_evecs = np.linalg.eigh(C)\n        C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n        C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n        \n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n        \n        past_objectives = [] #Stores last N objective values\n\n        while eval_count < self.budget:\n            # Adapt population size\n            self.target_popsize = min(self.max_popsize, self.min_popsize + int(np.floor(3 * np.log(self.dim))))\n            if self.target_popsize != self.popsize:\n                self.popsize = self.target_popsize\n                self.mu = self.popsize // 2\n                self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n                self.weights = self.weights / np.sum(self.weights)\n                self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n                self.cmu = 1.5 * self.mueff / (self.dim + 2)**2\n\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            x = mean + sigma * (C_sqrt @ z.T).T\n\n            # Mirrored Sampling\n            num_mirrored = int(self.popsize * self.mirror_ratio)\n            z_mirrored = -z[:num_mirrored]\n            x_mirrored = mean + sigma * (C_sqrt @ z_mirrored.T).T\n            x = np.concatenate((x, x_mirrored), axis=0)\n            z = np.concatenate((z, z_mirrored), axis=0)\n\n            f = np.array([func(xi) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n            eval_count += len(x)\n\n            idx = np.argsort(f)\n            x = x[idx]\n            z = z[idx]\n            f = f[idx]\n\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[0]\n                self.stagnation_counter = 0\n                self.last_f_opt = f[0] #Store for stagnation criterion\n            else:\n                self.stagnation_counter += 1\n            \n            # Restart strategy\n            if self.stagnation_counter > self.restart_iterations:\n                if self.restart_strategy == \"random\":\n                    mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                elif self.restart_strategy == \"jitter\":\n                     mean = mean + 0.1 * np.random.normal(0, 1, size=self.dim) #Jitter the mean\n                     mean = np.clip(mean, func.bounds.lb, func.bounds.ub) #Ensure bounds\n                else:\n                    raise ValueError(\"Invalid restart strategy.\")\n\n                sigma = self.sigma0\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                self.stagnation_counter = 0\n                past_objectives = []\n\n            x_mu = x[:self.mu]\n            z_mu = z[:self.mu]\n\n            mean_new = np.sum(x_mu * self.weights[:, None], axis=0)\n            z_w = np.sum(z_mu * self.weights[:, None], axis=0)\n            \n            ps = (1 - self.cs) * ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * (C_invsqrt @ (mean_new - mean) / sigma)\n            \n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - self.cs)**(2 * eval_count / self.popsize)) < chiN * (1.4 + 2 / (self.dim + 1))\n            \n            pc = (1 - self.cc) * pc + hsig * np.sqrt(self.cc * (2 - self.cc) * self.mueff) * (mean_new - mean) / sigma\n\n            # Covariance matrix update with spectral decay\n            C = (1 - self.c1 - self.cmu) * C + self.c1 * (pc[:, None] @ pc[None, :])\n            for i in range(self.mu):\n                C += self.cmu * self.weights[i] * (z_mu[i, :, None] @ z_mu[i, None, :])\n\n            # Ensure symmetry\n            C = np.triu(C) + np.triu(C, 1).T\n            \n            # Dynamic step size adaptation based on objective similarity\n            past_objectives.append(f[0])\n            if len(past_objectives) > self.history_length:\n                past_objectives.pop(0)\n                similarity = np.std(past_objectives)\n                if similarity < 1e-6: #If recent objectives are very similar, reduce step size\n                  sigma *= 0.8\n                else:\n                  sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(ps) / chiN - 1)) #Adapt normally\n            else:\n              sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(ps) / chiN - 1))\n\n            sigma *= self.damps # Apply damping\n\n            # Ensure sigma is not too small\n            sigma = max(sigma, self.sigma_threshold)\n            \n            # Orthogonal subspace learning every 10 generations\n            if eval_count % (10 * self.popsize) == 0:\n                subspace_size = min(self.dim // 2, 10)  # Adjust subspace size as needed\n                Q = self.orthogonal_basis[:, :subspace_size]\n                y = Q.T @ (mean_new - mean) #Project into subspace\n                C_subspace = Q.T @ C @ Q #Project covariance matrix\n                try:\n                    C_evals_sub, C_evecs_sub = np.linalg.eigh(C_subspace) #Eigen-decomposition\n                    Q_update = Q @ C_evecs_sub #Update vectors\n                    self.orthogonal_basis[:, :subspace_size] = Q_update #Update orthogonal basis\n                except np.linalg.LinAlgError:\n                    pass #Do not update\n\n            # Regularize and update covariance matrix\n            if eval_count % (self.popsize * 5) == 0:\n                C = np.triu(C) + np.triu(C, 1).T\n                try:\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_evals = np.maximum(C_evals, 1e-10)\n\n                    # Spectral Decay: Shrink smaller eigenvalues\n                    C_evals = C_evals * (1 - 0.01 * np.exp(-np.arange(self.dim)))\n\n                    C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n\n                     # Condition number based regularization\n                    condition_number = np.max(C_evals) / np.min(C_evals)\n                    if condition_number > self.condition_number_target:\n                        regularization_factor = 1e-8 * (condition_number / self.condition_number_target)\n                        C = C + regularization_factor * np.eye(self.dim)\n\n                except np.linalg.LinAlgError:\n                    C = np.eye(self.dim)\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n\n                # Regularize Covariance Matrix\n                C = C + 1e-8 * np.eye(self.dim)\n                \n            if np.any(np.isnan(mean)) or np.any(np.isnan(C)):\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                sigma = self.sigma0\n\n            mean = mean_new\n\n        return f_opt, x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdvancedCMAES_OrthoPopSizeDynSigma scored 0.698 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e00ef585-ae18-4cad-9648-bf64037b7b97"], "operator": null, "metadata": {"aucs": [0.26205427754836963, 0.9364867842345952, 0.8375184213261914, 0.963517961043608, 0.9366934755178844, 0.945571581873146, 0.3247562235253396, 0.9349170773013543, 0.941349347899638, 0.2700343389718469, 0.960563059916314, 0.9833065626194338, 0.23153592390353428, 0.848216048017876, 0.6329823337624816, 0.5707302992374609, 0.7501437801571046, 0.9591953387259806, 0.16776960792492446, 0.5013580646499595]}}
{"id": "34b7c91d-60fb-4fef-a767-9e53f449331a", "fitness": 0.7198288336284111, "name": "CMAES_DynamicPopSizeMirrorSigmoid_SelfAdaptC1Cmu", "description": "CMA-ES with dynamic population size adjustment, adaptive mirrored sampling using a sigmoid function, rank-based covariance update, conjugate evolution path step size adaptation, and a self-adaptation mechanism for c_1 and c_mu based on covariance matrix eigenvalue distribution.", "code": "import numpy as np\n\nclass CMAES_DynamicPopSizeMirrorSigmoid_SelfAdaptC1Cmu:\n    def __init__(self, budget=10000, dim=10, sigma0=0.5, history_length=10, initial_mirrored_fraction=0.5, mirrored_decay=0.99, initial_c1=None, initial_cmu=None, popsize_multiplier=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.history_length = history_length\n        self.mirrored_fraction = initial_mirrored_fraction  # Initial fraction of population to mirror\n        self.mirrored_decay = mirrored_decay  # Decay rate for mirrored fraction\n        self.popsize_multiplier = popsize_multiplier # Multiplier for adjusting popsize\n\n        self.base_popsize = 4 + int(np.floor(3 * np.log(self.dim)))\n        self.popsize = int(self.base_popsize * self.popsize_multiplier)\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.step_history = []\n\n        # Initialize c_1 and c_mu\n        if initial_c1 is None:\n            self.c_1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        else:\n            self.c_1 = initial_c1\n        if initial_cmu is None:\n            self.c_mu = min(1 - self.c_1, 2 * (self.mueff - 1 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        else:\n            self.c_mu = initial_cmu\n\n        self.damp_C = 0.2 + self.c_1 + self.c_mu  # Damping factor for covariance matrix update\n        self.c_sigma = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.c_c = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        self.d_sigma = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1)\n\n    def __call__(self, func):\n        # Initialize variables\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n        # Eigen decomposition of C\n        C_evals, C_evecs = np.linalg.eigh(C)\n        C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n        C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n        \n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n        stagnation_counter = 0\n        previous_f_opt = np.Inf\n\n        while eval_count < self.budget:\n            # Sample population\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            x = mean + sigma * (C_sqrt @ z.T).T\n\n            # Mirrored sampling with sigmoid adaptation\n            mirrored_fraction = self.mirrored_fraction / (1 + np.exp(5 - 10*(eval_count / self.budget)))  # Sigmoid decay\n            num_mirrored = int(self.popsize * mirrored_fraction)\n            x_mirrored = mean - sigma * (C_sqrt @ z[:num_mirrored].T).T\n            x = np.vstack((x, x_mirrored))\n            z_mirrored = -z[:num_mirrored]\n            z = np.vstack((z, z_mirrored))\n            \n            # Clipping to bounds\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            \n            # Evaluate the new points\n            f = np.array([func(xi) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n            eval_count += len(x) \n\n            # Sort by fitness\n            idx = np.argsort(f)\n            x = x[idx]\n            z = z[idx]\n            f = f[idx]\n\n            # Update optimal solution\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[0]\n                stagnation_counter = 0 # Reset stagnation counter\n            else:\n                stagnation_counter += 1 # increment stagnation counter\n\n            # Adjust population size based on stagnation\n            if stagnation_counter > 5 * self.dim:\n                self.popsize_multiplier = max(0.5, self.popsize_multiplier * 0.9)\n                stagnation_counter = 0\n            elif f_opt < previous_f_opt:\n                 self.popsize_multiplier = min(2.0, self.popsize_multiplier * 1.1)\n            \n            self.popsize = int(self.base_popsize * self.popsize_multiplier)\n            self.mu = self.popsize // 2\n            self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n            self.weights = self.weights / np.sum(self.weights)\n            self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n            previous_f_opt = f_opt\n\n            # Selection and recombination\n            x_mu = x[:self.mu]\n            z_mu = z[:self.mu]\n\n            mean_new = np.sum(x_mu * self.weights[:,None], axis=0)\n            z_w = np.sum(z_mu * self.weights[:,None], axis=0)\n            \n            # Covariance matrix adaptation using rank-mu update\n            ps = (1 - self.c_sigma) * ps + np.sqrt(self.c_sigma * (2 - self.c_sigma) * self.mueff) * (C_invsqrt @ (mean_new - mean) / sigma)\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - self.c_sigma)**(2 * eval_count / self.popsize)) < chiN * (1.4 + 2 / (self.dim + 1))\n            \n            pc = (1 - self.c_c) * pc + hsig * np.sqrt(self.c_c * (2 - self.c_c) * self.mueff) * (mean_new - mean) / sigma\n\n            C = (1 - self.c_1 - self.c_mu + self.damp_C * self.c_1) * C + self.c_1 * (pc[:, None] @ pc[None, :])\n\n            # Rank-mu update\n            for i in range(self.mu):\n                C += self.c_mu * self.weights[i] * (z_mu[i, :, None] @ z_mu[i, None, :])\n            \n            # Update step size using conjugate evolution path\n            step = (mean_new - mean) / sigma\n            self.step_history.append(step)\n            if len(self.step_history) > self.history_length:\n                self.step_history.pop(0)\n\n            # Adapt step size based on step history\n            step_correlation = 0\n            for h_step in self.step_history:\n                step_correlation += np.dot(step, h_step)\n            \n            sigma_factor = np.exp(0.1 * step_correlation / self.dim)\n            sigma *= np.exp((self.c_sigma / self.d_sigma) * (np.linalg.norm(ps) / chiN - 1)) * sigma_factor\n\n            # Update mean\n            mean = mean_new\n\n            # Adaptive Mirrored fraction (decay moved to start)\n            self.mirrored_fraction *= self.mirrored_decay\n\n            # Eigen decomposition of C\n            if eval_count % (self.popsize * 5) == 0:\n                C = np.triu(C) + np.triu(C, 1).T\n                try:\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_evals = np.maximum(C_evals, 1e-10)\n                    C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                except np.linalg.LinAlgError:\n                    print(\"LinAlgError encountered, resetting C\")\n                    C = np.eye(self.dim)\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n\n                # Self-adaptation of c_1 and c_mu based on eigenvalue distribution of C\n                eigenvalues = np.linalg.eigvalsh(C)\n                condition_number = np.max(eigenvalues) / np.min(eigenvalues)\n                learning_rate = 0.1  # Adjust as needed\n                self.c_1 *= np.exp(learning_rate * (condition_number - 1))  # Reduce c_1 when C is ill-conditioned\n                self.c_mu *= np.exp(-learning_rate * (condition_number - 1)) # Increase c_mu to trust population\n\n                # Ensure c_1 and c_mu remain within reasonable bounds\n                self.c_1 = np.clip(self.c_1, 1e-8, 1.0)\n                self.c_mu = np.clip(self.c_mu, 1e-8, 1.0)\n\n                self.damp_C = 0.2 + self.c_1 + self.c_mu  # Update damping factor\n\n            if np.any(np.isnan(mean)) or np.any(np.isnan(C)):\n                print(\"NaN detected, resetting...\")\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                sigma = self.sigma0\n        \n        return f_opt, x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm CMAES_DynamicPopSizeMirrorSigmoid_SelfAdaptC1Cmu scored 0.720 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["41e310bc-fcf5-4b15-9cbf-b4a3ad61eace"], "operator": null, "metadata": {"aucs": [0.4244909278426997, 0.8926142466181306, 0.8746187600807194, 0.9552934347076101, 0.37987192915427437, 0.9279257630359958, 0.3709294381076339, 0.9007697917174838, 0.910918094876982, 0.8663890198529486, 0.3994134443555679, 0.9943406565503929, 0.23642616323568022, 0.9070536209191353, 0.8257280234318296, 0.8950987783398879, 0.7113027717378406, 0.9387460526077415, 0.46113976022388903, 0.5235059951717775]}}
{"id": "6c81c2ea-d612-4783-aa00-b0b15faac25e", "fitness": -Infinity, "name": "CMAES_SimpleRankOneMirrorHyperbolic_Restart", "description": "CMA-ES with rank-one update, mirrored sampling with dynamic decay, hyperbolic decay factor, adaptive step size, and a restart mechanism triggered by stagnation and a CMA matrix adaptation speed control using eigenvalue decomposition.", "code": "import numpy as np\n\nclass CMAES_SimpleRankOneMirrorHyperbolic_Restart:\n    def __init__(self, budget=10000, dim=10, sigma0=0.5, history_length=5, initial_mirrored_fraction=0.5, mirrored_decay=0.99, hyperbolic_decay_factor=0.1, step_size_adaptation_rate=0.2, stagnation_threshold=1e-6, stagnation_generations=50, cma_adaptation_speed=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.history_length = history_length\n        self.mirrored_fraction = initial_mirrored_fraction\n        self.mirrored_decay = mirrored_decay\n        self.hyperbolic_decay_factor = hyperbolic_decay_factor\n        self.step_size_adaptation_rate = step_size_adaptation_rate\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_generations = stagnation_generations\n        self.cma_adaptation_speed = cma_adaptation_speed\n\n        self.popsize = 4 + int(np.floor(3 * np.log(self.dim)))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n        self.step_history = []\n        self.best_f_history = []\n\n    def __call__(self, func):\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)\n        pc = np.zeros(self.dim)\n        \n        c_sigma = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        c_c = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        c_1 = self.cma_adaptation_speed / ((self.dim + 1.3)**2 + self.mueff)\n        \n        chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n        generation = 0\n\n        while eval_count < self.budget:\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            x = mean + sigma * (np.linalg.cholesky(C) @ z.T).T\n\n            # Mirrored sampling\n            mirrored_fraction = self.mirrored_fraction / (1 + self.hyperbolic_decay_factor * (eval_count / self.budget))\n            num_mirrored = int(self.popsize * mirrored_fraction)\n            x_mirrored = mean - sigma * (np.linalg.cholesky(C) @ z[:num_mirrored].T).T\n            x = np.vstack((x, x_mirrored))\n            z_mirrored = -z[:num_mirrored]\n            z = np.vstack((z, z_mirrored))\n            \n            # Enhanced bound handling - clipping and penalty\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            \n            f = np.array([func(xi) + 1e10 * (np.sum((xi < func.bounds.lb) | (xi > func.bounds.ub))) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n            eval_count += len(x)\n\n            idx = np.argsort(f)\n            x = x[idx]\n            z = z[idx]\n            f = f[idx]\n\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[0]\n                self.best_f_history.append(f_opt)\n            else:\n                 self.best_f_history.append(self.best_f_history[-1] if self.best_f_history else f_opt)\n\n            x_mu = x[:self.mu]\n            mean_new = np.sum(x_mu * self.weights[:,None], axis=0)\n\n            # Simplified Rank-One Update\n            pc = (1 - c_c) * pc + np.sqrt(c_c * (2 - c_c) * self.mueff) * (mean_new - mean) / sigma\n            C = (1 - c_1) * C + c_1 * np.outer(pc, pc)\n            \n            # Step size adaptation using moving average\n            step = (mean_new - mean) / sigma\n            self.step_history.append(np.linalg.norm(step))\n            if len(self.step_history) > self.history_length:\n                self.step_history.pop(0)\n\n            avg_step_size = np.mean(self.step_history) if self.step_history else 1\n            sigma *= np.exp(self.step_size_adaptation_rate * (avg_step_size - 1))\n\n            mean = mean_new\n            self.mirrored_fraction *= self.mirrored_decay\n\n            # Ensure C is positive definite and adapt it if needed\n            C = np.triu(C) + np.triu(C, 1).T\n            try:\n                D, B = np.linalg.eigh(C)\n                D = np.maximum(D, 1e-10)\n                C = B @ np.diag(D) @ B.T\n            except np.linalg.LinAlgError:\n                C = np.eye(self.dim)\n                D = np.ones(self.dim)\n\n            if np.any(np.isnan(mean)) or np.any(np.isnan(C)):\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                sigma = self.sigma0\n\n            # Restart mechanism\n            if generation > self.stagnation_generations and abs(self.best_f_history[-1] - self.best_f_history[-self.stagnation_generations]) < self.stagnation_threshold:\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                sigma = self.sigma0\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                self.step_history = []\n                self.mirrored_fraction = self.initial_mirrored_fraction\n                print(\"Restarting CMA-ES\")\n                self.best_f_history = []\n\n            generation += 1\n\n        return f_opt, x_opt", "configspace": "", "generation": 8, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 144, in evaluate\n    algorithm(f_new)\n  File \"<string>\", line 115, in __call__\nAttributeError: 'CMAES_SimpleRankOneMirrorHyperbolic_Restart' object has no attribute 'initial_mirrored_fraction'\n.", "error": "", "parent_ids": ["496ac514-845a-411c-896f-ee0f392b4057"], "operator": null, "metadata": {}}
{"id": "d897334d-c2ec-4b42-af93-511b3a2db673", "fitness": 0.625781266966744, "name": "CMAES_AdaptiveMirrorRankHyperbolic_SelfAdaptC1Cmu_ActiveSubspace", "description": "CMA-ES with dynamic mirrored sampling controlled by fitness variance, rank-based covariance update, conjugate evolution path step size adaptation, self-adaptation of covariance matrix learning rates, and active subspace adaptation based on eigenvalue decomposition.", "code": "import numpy as np\n\nclass CMAES_AdaptiveMirrorRankHyperbolic_SelfAdaptC1Cmu_ActiveSubspace:\n    def __init__(self, budget=10000, dim=10, sigma0=0.5, history_length=10, initial_mirrored_fraction=0.5, mirrored_decay=0.99, hyperbolic_decay_factor=0.1, initial_c1=None, initial_cmu=None, subspace_dimension=None):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.history_length = history_length\n        self.mirrored_fraction = initial_mirrored_fraction  # Initial fraction of population to mirror\n        self.mirrored_decay = mirrored_decay  # Decay rate for mirrored fraction\n        self.hyperbolic_decay_factor = hyperbolic_decay_factor\n        \n        if subspace_dimension is None:\n            self.subspace_dimension = max(1, dim // 2)  # Default subspace dimension\n        else:\n            self.subspace_dimension = subspace_dimension\n        \n        self.popsize = 4 + int(np.floor(3 * np.log(self.dim)))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.step_history = []\n\n        # Initialize c_1 and c_mu\n        if initial_c1 is None:\n            self.c_1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        else:\n            self.c_1 = initial_c1\n        if initial_cmu is None:\n            self.c_mu = min(1 - self.c_1, 2 * (self.mueff - 1 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        else:\n            self.c_mu = initial_cmu\n\n        self.damp_C = 0.2 + self.c_1 + self.c_mu  # Damping factor for covariance matrix update\n        self.c_sigma = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.c_c = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        self.d_sigma = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1)\n        \n        self.B = np.eye(self.dim, self.subspace_dimension) # Basis for active subspace\n\n    def __call__(self, func):\n        # Initialize variables\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n        # Eigen decomposition of C\n        C_evals, C_evecs = np.linalg.eigh(C)\n        C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n        C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n        \n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n\n        while eval_count < self.budget:\n            # Sample population\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            x = mean + sigma * (C_sqrt @ z.T).T\n\n            # Adaptive Mirrored Sampling based on fitness variance\n            f_var = np.var(f) if eval_count > 0 else 1.0\n            mirrored_fraction = self.mirrored_fraction / (1 + self.hyperbolic_decay_factor * (eval_count / self.budget)) * (1 - np.exp(-f_var)) # scale mirrored fraction by fitness variance\n\n            num_mirrored = int(self.popsize * mirrored_fraction)\n            x_mirrored = mean - sigma * (C_sqrt @ z[:num_mirrored].T).T\n            x = np.vstack((x, x_mirrored))\n            z_mirrored = -z[:num_mirrored]\n            z = np.vstack((z, z_mirrored))\n            \n            # Clipping to bounds\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            \n            # Evaluate the new points\n            f = np.array([func(xi) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n            eval_count += len(x) \n\n            # Sort by fitness\n            idx = np.argsort(f)\n            x = x[idx]\n            z = z[idx]\n            f = f[idx]\n\n            # Update optimal solution\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[0]\n                \n            # Selection and recombination\n            x_mu = x[:self.mu]\n            z_mu = z[:self.mu]\n\n            mean_new = np.sum(x_mu * self.weights[:,None], axis=0)\n            z_w = np.sum(z_mu * self.weights[:,None], axis=0)\n            \n            # Covariance matrix adaptation using rank-mu update\n            ps = (1 - self.c_sigma) * ps + np.sqrt(self.c_sigma * (2 - self.c_sigma) * self.mueff) * (C_invsqrt @ (mean_new - mean) / sigma)\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - self.c_sigma)**(2 * eval_count / self.popsize)) < chiN * (1.4 + 2 / (self.dim + 1))\n            \n            pc = (1 - self.c_c) * pc + hsig * np.sqrt(self.c_c * (2 - self.c_c) * self.mueff) * (mean_new - mean) / sigma\n\n            C = (1 - self.c_1 - self.c_mu + self.damp_C * self.c_1) * C + self.c_1 * (pc[:, None] @ pc[None, :])\n\n            # Rank-mu update\n            for i in range(self.mu):\n                C += self.c_mu * self.weights[i] * (z_mu[i, :, None] @ z_mu[i, None, :])\n            \n            # Update step size using conjugate evolution path\n            step = (mean_new - mean) / sigma\n            self.step_history.append(step)\n            if len(self.step_history) > self.history_length:\n                self.step_history.pop(0)\n\n            # Adapt step size based on step history\n            step_correlation = 0\n            for h_step in self.step_history:\n                step_correlation += np.dot(step, h_step)\n            \n            sigma_factor = np.exp(0.1 * step_correlation / self.dim)\n            sigma *= np.exp((self.c_sigma / self.d_sigma) * (np.linalg.norm(ps) / chiN - 1)) * sigma_factor\n\n            # Update mean\n            mean = mean_new\n\n            # Adaptive Mirrored fraction (decay moved to start)\n            self.mirrored_fraction *= self.mirrored_decay\n\n            # Eigen decomposition of C\n            if eval_count % (self.popsize * 5) == 0:\n                C = np.triu(C) + np.triu(C, 1).T\n                try:\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_evals = np.maximum(C_evals, 1e-10)\n                    C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                    \n                    # Active Subspace Adaptation\n                    idx = np.argsort(C_evals)[::-1]  # Sort eigenvalues in descending order\n                    self.B = C_evecs[:, idx[:self.subspace_dimension]] # Select eigenvectors corresponding to top eigenvalues as basis\n\n                except np.linalg.LinAlgError:\n                    print(\"LinAlgError encountered, resetting C\")\n                    C = np.eye(self.dim)\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n\n                # Self-adaptation of c_1 and c_mu based on trace of C\n                trace_C = np.trace(C)\n                learning_rate = 0.1  # Adjust as needed\n                self.c_1 *= np.exp(learning_rate * (trace_C - self.dim))\n                self.c_mu *= np.exp(-learning_rate * (trace_C - self.dim))\n\n                # Ensure c_1 and c_mu remain within reasonable bounds\n                self.c_1 = np.clip(self.c_1, 1e-8, 1.0)\n                self.c_mu = np.clip(self.c_mu, 1e-8, 1.0)\n\n                self.damp_C = 0.2 + self.c_1 + self.c_mu  # Update damping factor\n\n            if np.any(np.isnan(mean)) or np.any(np.isnan(C)):\n                print(\"NaN detected, resetting...\")\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1/np.sqrt(C_evals)) @ C_evecs.T\n                sigma = self.sigma0\n        \n        return f_opt, x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm CMAES_AdaptiveMirrorRankHyperbolic_SelfAdaptC1Cmu_ActiveSubspace scored 0.626 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["41e310bc-fcf5-4b15-9cbf-b4a3ad61eace"], "operator": null, "metadata": {"aucs": [0.5158116072026351, 0.1725687454014353, 0.913787580909299, 0.17294858633642385, 0.938509882989734, 0.953686972346227, 0.32497854164764794, 0.6286614150966865, 0.9447304116476972, 0.12215974170028754, 0.9589892048903694, 0.9999290840975815, 0.2715625256447731, 0.9229993137555397, 0.8560208227646, 0.7967477953235264, 0.41462026807900987, 0.9548924942279081, 0.156648583711108, 0.4953717615623907]}}
{"id": "08afd1f0-4e6c-40af-b1ea-968823ab61ee", "fitness": 0.22579199956842438, "name": "RobustCMAES_ProPlus_DynamicPop", "description": "Robust CMA-ES with dynamic population size adjustment based on stagnation and diversity, spectral regularization, and adaptive step size control using a trust region approach.", "code": "import numpy as np\n\nclass RobustCMAES_ProPlus_DynamicPop(object):\n    def __init__(self, budget=10000, dim=10, sigma0=0.5, history_length=5, mirror_ratio=0.5, restart_strategy=\"random\", initial_popsize=None):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.history_length = history_length\n        self.min_popsize = 4\n        self.max_popsize = 50\n        if initial_popsize is None:\n            self.target_popsize = min(self.max_popsize, self.min_popsize + int(np.floor(3 * np.log(self.dim))))\n        else:\n            self.target_popsize = initial_popsize\n        self.popsize = self.target_popsize\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.step_history = []\n        self.last_f_opt = np.inf\n        self.stagnation_counter = 0\n        self.restart_iterations = 10 * self.dim  # More aggressive stagnation detection\n        self.mirror_ratio = mirror_ratio\n        self.restart_strategy = restart_strategy # \"random\" or \"jitter\"\n        self.condition_number_target = 1e14\n        self.damps = 1.0 # Damping factor for step size\n        self.cs = 0.2 # cumulation factor for step-size\n        self.ccov = 2 / (self.dim**2 + 6)\n        self.trust_region_radius = 1.0 # Initial trust region radius\n        self.trust_region_shrink = 0.1 # Trust region shrink factor\n        self.trust_region_expand = 2.0 # Trust region expand factor\n        self.success_threshold = 0.2 # Success threshold for trust region adaptation\n        self.min_sigma = 1e-10 # Minimum step size\n        self.max_sigma = 10 # Maximum step size\n        self.popsize_increase_factor = 1.2 # Population increase factor\n        self.popsize_decrease_factor = 0.8 # Population decrease factor\n        self.diversity_threshold = 0.01 # Diversity threshold for stagnation\n\n\n    def __call__(self, func):\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n        c_sigma = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        c_c = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        c_1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        c_mu = min(1 - c_1, 2 * (self.mueff - 1 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        d_sigma = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + c_sigma\n\n        C_evals, C_evecs = np.linalg.eigh(C)\n        C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n        C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n        \n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n        no_improvement_streak = 0 # Streak of no improvement\n        successes = 0\n        \n        while eval_count < self.budget:\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            x = mean + sigma * (C_sqrt @ z.T).T\n\n            # Mirrored Sampling\n            num_mirrored = int(self.popsize * self.mirror_ratio)\n            z_mirrored = -z[:num_mirrored]\n            x_mirrored = mean + sigma * (C_sqrt @ z_mirrored.T).T\n            x = np.concatenate((x, x_mirrored), axis=0)\n            z = np.concatenate((z, z_mirrored), axis=0)\n\n            # Clip to trust region\n            diff = x - mean\n            radii = np.linalg.norm(diff, axis=1)\n            too_far = radii > self.trust_region_radius\n            x[too_far] = mean + (diff[too_far] * self.trust_region_radius / radii[too_far, None])\n\n            f = np.array([func(xi) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n            eval_count += len(x)\n\n            idx = np.argsort(f)\n            x = x[idx]\n            z = z[idx]\n            f = f[idx]\n\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[0]\n                self.stagnation_counter = 0\n                self.last_f_opt = f[0] #Store for stagnation criterion\n                no_improvement_streak = 0  # Reset no improvement streak\n                successes +=1\n            else:\n                self.stagnation_counter += 1\n                no_improvement_streak += 1 # Increment no improvement streak\n                successes = max(0, successes -1)\n            \n            # Restart strategy\n            if self.stagnation_counter > self.restart_iterations or no_improvement_streak > 2 * self.restart_iterations:\n                if self.restart_strategy == \"random\":\n                    mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                elif self.restart_strategy == \"jitter\":\n                     mean = mean + 0.1 * np.random.normal(0, 1, size=self.dim) #Jitter the mean\n                     mean = np.clip(mean, func.bounds.lb, func.bounds.ub) #Ensure bounds\n                else:\n                    raise ValueError(\"Invalid restart strategy.\")\n\n                sigma = self.sigma0\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                self.stagnation_counter = 0\n                no_improvement_streak = 0 #reset streak\n\n            x_mu = x[:self.mu]\n            z_mu = z[:self.mu]\n\n            mean_new = np.sum(x_mu * self.weights[:, None], axis=0)\n            z_w = np.sum(z_mu * self.weights[:, None], axis=0)\n            \n            ps = (1 - self.cs) * ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * (C_invsqrt @ (mean_new - mean) / sigma)\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - self.cs)**(2 * eval_count / self.popsize)) < chiN * (1.4 + 2 / (self.dim + 1))\n            pc = (1 - c_c) * pc + hsig * np.sqrt(c_c * (2 - c_c) * self.mueff) * (mean_new - mean) / sigma\n\n            C = (1 - c_1 - c_mu) * C + c_1 * (pc[:, None] @ pc[None, :])\n            for i in range(self.mu):\n                C += c_mu * self.weights[i] * (z_mu[i, :, None] @ z_mu[i, None, :])\n            \n            sigma *= np.exp((c_sigma / d_sigma) * (np.linalg.norm(ps) / chiN - 1))\n            sigma *= self.damps # Apply damping\n            sigma = np.clip(sigma, self.min_sigma, self.max_sigma)\n\n            # Simplified Step history\n            step = (mean_new - mean) / sigma\n            self.step_history.append(step)\n            if len(self.step_history) > self.history_length:\n                self.step_history.pop(0)\n\n            # Adapt C based on step history, simplified\n            for h_step in self.step_history:\n                C += 0.1 * c_mu * (h_step[:, None] @ h_step[None, :])\n\n            mean = mean_new\n            \n            if eval_count % (self.popsize * 5) == 0:\n                C = np.triu(C) + np.triu(C, 1).T\n                try:\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_evals = np.maximum(C_evals, 1e-10)\n                    C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n\n                     # Condition number based regularization\n                    condition_number = np.max(C_evals) / np.min(C_evals)\n                    if condition_number > self.condition_number_target:\n                        regularization_factor = 1e-8 * (condition_number / self.condition_number_target)\n                        C = C + regularization_factor * np.eye(self.dim)\n\n                     # Spectral clipping regularization\n                    C_evals = np.clip(C_evals, np.max(C_evals) / self.condition_number_target, np.max(C_evals))\n                    C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n\n                except np.linalg.LinAlgError:\n                    C = np.eye(self.dim)\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n\n                # Regularize Covariance Matrix\n                C = C + 1e-8 * np.eye(self.dim)\n                \n            if np.any(np.isnan(mean)) or np.any(np.isnan(C)):\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                sigma = self.sigma0\n\n            # Trust region adaptation\n            if successes > self.popsize * self.success_threshold:\n                self.trust_region_radius *= self.trust_region_expand\n            elif successes < self.popsize * (1 - self.success_threshold):\n                self.trust_region_radius *= self.trust_region_shrink\n\n            # Dynamic population size adjustment\n            if self.stagnation_counter > self.restart_iterations and self.popsize < self.max_popsize:\n                self.popsize = min(int(self.popsize * self.popsize_increase_factor), self.max_popsize)\n                self.mu = self.popsize // 2\n                self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n                self.weights = self.weights / np.sum(self.weights)\n                self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n                self.cs = 0.2  # Adjust cumulation factor based on popsize\n            elif no_improvement_streak > 2 * self.restart_iterations and self.popsize > self.min_popsize:\n                self.popsize = max(int(self.popsize * self.popsize_decrease_factor), self.min_popsize)\n                self.mu = self.popsize // 2\n                self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n                self.weights = self.weights / np.sum(self.weights)\n                self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n                self.cs = 0.2  # Adjust cumulation factor based on popsize\n\n        return f_opt, x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm RobustCMAES_ProPlus_DynamicPop scored 0.226 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["24bca9ca-66d4-46c6-940e-5e4b89072caa"], "operator": null, "metadata": {"aucs": [0.09755509755802128, 0.3239671099420788, 0.6609557293789414, 0.10168166469947848, 0.06133801267055694, 0.2191889503959057, 0.14521549079869334, 0.27455014295601843, 0.007250593332471134, 0.11642453872580172, 0.5053673932092023, 0.12777308808616872, 0.02832167768715843, 0.3269372507352247, 0.5843737750286562, 0.13058805815184948, 0.11092356924159807, 0.09680297835964735, 0.12300998895421922, 0.47361488145679576]}}
{"id": "95c63ae8-101f-43c8-9285-a9675aaf8b0e", "fitness": 0.7019206320063962, "name": "AdvancedCMAES_OrthoPopSize_SlidingWindow", "description": "CMA-ES with orthogonal subspace learning using a sliding window of past search directions to guide subspace selection, enhancing adaptation to ridge-like structures.", "code": "import numpy as np\n\nclass AdvancedCMAES_OrthoPopSize_SlidingWindow(object):\n    def __init__(self, budget=10000, dim=10, sigma0=0.5, history_length=5, mirror_ratio=0.5, restart_strategy=\"random\", subspace_size=None):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.history_length = history_length\n        self.min_popsize = 4\n        self.max_popsize = 50\n        self.target_popsize = min(self.max_popsize, self.min_popsize + int(np.floor(3 * np.log(self.dim))))\n        self.popsize = self.target_popsize\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.step_history = []\n        self.last_f_opt = np.inf\n        self.stagnation_counter = 0\n        self.restart_iterations = 10 * self.dim  # More aggressive stagnation detection\n        self.mirror_ratio = mirror_ratio\n        self.restart_strategy = restart_strategy # \"random\" or \"jitter\"\n        self.condition_number_target = 1e14\n        self.damps = 1.0 # Damping factor for step size\n        self.cs = 0.3  # Step-size learning rate\n        self.cc = 0.4  # Cumulation for covariance matrix rank-one update\n        self.c1 = 1.5 / (self.dim + 2)**2  # Learning rate for rank-one update\n        self.cmu = 1.5 * self.mueff / (self.dim + 2)**2  # Learning rate for rank-mu update\n        self.init_lambda = 2 + np.floor(3 * np.log(self.dim))\n        self.lambda_factor = 2\n        self.orthogonal_basis = np.random.randn(self.dim, self.dim)\n        self.orthogonal_basis, _ = np.linalg.qr(self.orthogonal_basis)\n        self.mean_history = []\n        self.subspace_size = subspace_size if subspace_size is not None else min(self.dim // 2, 10)\n\n    def __call__(self, func):\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n        C_evals, C_evecs = np.linalg.eigh(C)\n        C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n        C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n        \n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n\n        while eval_count < self.budget:\n            # Adapt population size\n            self.target_popsize = min(self.max_popsize, self.min_popsize + int(np.floor(3 * np.log(self.dim))))\n            if self.target_popsize != self.popsize:\n                self.popsize = self.target_popsize\n                self.mu = self.popsize // 2\n                self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n                self.weights = self.weights / np.sum(self.weights)\n                self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n                self.cmu = 1.5 * self.mueff / (self.dim + 2)**2\n\n            z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n            x = mean + sigma * (C_sqrt @ z.T).T\n\n            # Mirrored Sampling\n            num_mirrored = int(self.popsize * self.mirror_ratio)\n            z_mirrored = -z[:num_mirrored]\n            x_mirrored = mean + sigma * (C_sqrt @ z_mirrored.T).T\n            x = np.concatenate((x, x_mirrored), axis=0)\n            z = np.concatenate((z, z_mirrored), axis=0)\n\n            f = np.array([func(xi) if eval_count + i < self.budget else np.inf for i, xi in enumerate(x)])\n            eval_count += len(x)\n\n            idx = np.argsort(f)\n            x = x[idx]\n            z = z[idx]\n            f = f[idx]\n\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[0]\n                self.stagnation_counter = 0\n                self.last_f_opt = f[0] #Store for stagnation criterion\n            else:\n                self.stagnation_counter += 1\n            \n            # Restart strategy\n            if self.stagnation_counter > self.restart_iterations:\n                if self.restart_strategy == \"random\":\n                    mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                elif self.restart_strategy == \"jitter\":\n                     mean = mean + 0.1 * np.random.normal(0, 1, size=self.dim) #Jitter the mean\n                     mean = np.clip(mean, func.bounds.lb, func.bounds.ub) #Ensure bounds\n                else:\n                    raise ValueError(\"Invalid restart strategy.\")\n\n                sigma = self.sigma0\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                self.stagnation_counter = 0\n\n            x_mu = x[:self.mu]\n            z_mu = z[:self.mu]\n\n            mean_new = np.sum(x_mu * self.weights[:, None], axis=0)\n            z_w = np.sum(z_mu * self.weights[:, None], axis=0)\n            \n            ps = (1 - self.cs) * ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * (C_invsqrt @ (mean_new - mean) / sigma)\n            \n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - self.cs)**(2 * eval_count / self.popsize)) < chiN * (1.4 + 2 / (self.dim + 1))\n            \n            pc = (1 - self.cc) * pc + hsig * np.sqrt(self.cc * (2 - self.cc) * self.mueff) * (mean_new - mean) / sigma\n\n            # Covariance matrix update with spectral decay\n            C = (1 - self.c1 - self.cmu) * C + self.c1 * (pc[:, None] @ pc[None, :])\n            for i in range(self.mu):\n                C += self.cmu * self.weights[i] * (z_mu[i, :, None] @ z_mu[i, None, :])\n\n            # Ensure symmetry\n            C = np.triu(C) + np.triu(C, 1).T\n            \n            sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(ps) / chiN - 1))\n            sigma *= self.damps # Apply damping\n\n            # Store mean and update orthogonal subspace\n            self.mean_history.append(mean_new)\n            if len(self.mean_history) > self.history_length:\n                self.mean_history.pop(0)\n\n            # Orthogonal subspace learning every 10 generations\n            if eval_count % (10 * self.popsize) == 0 and len(self.mean_history) > 1:\n                # Construct a matrix of recent search steps\n                search_directions = np.array(self.mean_history[1:]) - np.array(self.mean_history[:-1])\n                \n                # Perform SVD to find the dominant directions\n                try:\n                    U, S, V = np.linalg.svd(search_directions.T)\n                    dominant_directions = U[:, :self.subspace_size]  # Select top subspace_size directions\n\n                    # Update orthogonal basis with the dominant directions\n                    self.orthogonal_basis[:, :self.subspace_size] = dominant_directions\n\n                except np.linalg.LinAlgError:\n                    pass # Handle potential SVD issues\n\n                # Project and update (same as before)\n                subspace_size = self.subspace_size\n                Q = self.orthogonal_basis[:, :subspace_size]\n                y = Q.T @ (mean_new - mean) #Project into subspace\n                C_subspace = Q.T @ C @ Q #Project covariance matrix\n                try:\n                    C_evals_sub, C_evecs_sub = np.linalg.eigh(C_subspace) #Eigen-decomposition\n                    Q_update = Q @ C_evecs_sub #Update vectors\n                    self.orthogonal_basis[:, :subspace_size] = Q_update #Update orthogonal basis\n                except np.linalg.LinAlgError:\n                    pass #Do not update\n\n            # Regularize and update covariance matrix\n            if eval_count % (self.popsize * 5) == 0:\n                C = np.triu(C) + np.triu(C, 1).T\n                try:\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_evals = np.maximum(C_evals, 1e-10)\n\n                    # Spectral Decay: Shrink smaller eigenvalues\n                    C_evals = C_evals * (1 - 0.01 * np.exp(-np.arange(self.dim)))\n\n                    C = C_evecs @ np.diag(C_evals) @ C_evecs.T\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n\n                     # Condition number based regularization\n                    condition_number = np.max(C_evals) / np.min(C_evals)\n                    if condition_number > self.condition_number_target:\n                        regularization_factor = 1e-8 * (condition_number / self.condition_number_target)\n                        C = C + regularization_factor * np.eye(self.dim)\n\n                except np.linalg.LinAlgError:\n                    C = np.eye(self.dim)\n                    C_evals, C_evecs = np.linalg.eigh(C)\n                    C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                    C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n\n                # Regularize Covariance Matrix\n                C = C + 1e-8 * np.eye(self.dim)\n                \n            if np.any(np.isnan(mean)) or np.any(np.isnan(C)):\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                C_evals, C_evecs = np.linalg.eigh(C)\n                C_sqrt = C_evecs @ np.diag(np.sqrt(C_evals)) @ C_evecs.T\n                C_invsqrt = C_evecs @ np.diag(1 / np.sqrt(C_evals)) @ C_evecs.T\n                sigma = self.sigma0\n\n            mean = mean_new\n\n        return f_opt, x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdvancedCMAES_OrthoPopSize_SlidingWindow scored 0.702 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e00ef585-ae18-4cad-9648-bf64037b7b97"], "operator": null, "metadata": {"aucs": [0.2610048612027589, 0.9296441679276445, 0.9260234601330061, 0.9678238292948255, 0.930287483011742, 0.9412915391189955, 0.8243925959984373, 0.9342408435906668, 0.9391263950116331, 0.22900317657094538, 0.9665444110214562, 0.9943833284457846, 0.276051237929796, 0.9225574776740363, 0.636498065642901, 0.3620599820981969, 0.3413435312141251, 0.9505015798501344, 0.1979704099392855, 0.5076642644515541]}}
