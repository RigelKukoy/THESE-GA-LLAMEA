{"id": "058d1560-fbdf-4c5c-8f01-64eb6442c127", "fitness": -Infinity, "name": "CMAES", "description": "Covariance Matrix Adaptation Evolution Strategy with a simple repair mechanism to handle bounds.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, sigma0=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.sigma0 = sigma0\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        # Initialize variables\n        mu = np.random.uniform(self.lb, self.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)  # Covariance matrix\n        \n        # Parameters for CMA-ES\n        c_s = 0.3  # Learning rate for step size\n        d_s = 1.0 + 2 * max(0, np.sqrt((self.mu_eff - 1) / (self.dim + 1)) - 1)\n        c_mu = 0.3 # Learning rate for mean\n        c_1 = 0.3# Learning rate for covariance rank-one update\n        \n        alpha_mu = 2\n        c_mu = min(1-c_1, alpha_mu * (self.mu_eff - 2 + 1/self.mu_eff) / ((self.dim+2)**2 + alpha_mu * self.mu_eff / 2))\n        \n        c_c = 0.3\n        \n        mu_weights = np.log(self.popsize+1) - np.log(np.arange(1, self.popsize+1))\n        mu_weights = mu_weights / np.sum(mu_weights)\n        self.mu_eff = np.sum(mu_weights)**2 / np.sum(mu_weights**2)\n        \n        c_1 = 2 / ((self.dim+1.3)**2 + self.mu_eff)\n        c_mu = min(1-c_1, 2 * (self.mu_eff - 2 + 1/self.mu_eff) / ((self.dim+2)**2 + 2 * self.mu_eff/2))\n        c_c = (4 + self.mu_eff/self.dim) / (self.dim + 4 + 2*self.mu_eff/self.dim)\n\n        p_s = np.zeros(self.dim)\n        p_c = np.zeros(self.dim)\n        \n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n\n        while eval_count < self.budget:\n            # Generate population\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            \n            try:\n                C_sqrt = np.linalg.cholesky(C)\n            except np.linalg.LinAlgError:\n                C = C + 1e-6 * np.eye(self.dim)\n                C_sqrt = np.linalg.cholesky(C)\n            \n            x = mu[:, np.newaxis] + sigma * np.dot(C_sqrt, z)\n            \n            # Repair individuals outside the bounds\n            x = np.clip(x, self.lb, self.ub)\n\n            # Evaluate population\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            eval_count += self.popsize\n\n            # Sort by fitness\n            idx = np.argsort(f)\n            x = x[:, idx]\n            f = f[idx]\n\n            # Update the mean\n            mu_old = mu\n            mu = np.dot(x[:, :self.popsize], mu_weights)\n\n            # Update evolution path for step size\n            p_s = (1 - c_s) * p_s + np.sqrt(c_s * (2 - c_s) * self.mu_eff) * np.dot(C_sqrt, (mu - mu_old)) / sigma\n            \n            # Update step size\n            sigma = sigma * np.exp((c_s / d_s) * (np.linalg.norm(p_s) / np.sqrt(self.dim) - 1))\n            \n            # Update evolution path for covariance matrix\n            p_c = (1 - c_c) * p_c + np.sqrt(c_c * (2 - c_c) * self.mu_eff) * (mu - mu_old) / sigma\n            \n            # Update covariance matrix\n            C = (1 - c_1 - c_mu) * C + c_1 * np.outer(p_c, p_c)\n\n            for i in range(self.popsize):\n                C = C + c_mu * mu_weights[i] * np.outer((x[:,i] - mu_old) / sigma, (x[:,i] - mu_old) / sigma)\n            \n            # Store best solution\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[:, 0].copy()\n\n        return f_opt, x_opt", "configspace": "", "generation": 0, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 20, in __call__\nAttributeError: 'CMAES' object has no attribute 'mu_eff'\n.", "error": "", "parent_ids": [], "operator": null, "metadata": {}}
{"id": "e408772a-7a0d-42f4-a489-45e8567bf10e", "fitness": -Infinity, "name": "AdaptiveDEAR", "description": "Adaptive Differential Evolution with Archive and Restart: Uses a differential evolution strategy with an archive of past solutions to enhance exploration and exploitation, restarting the population if convergence stagnates.", "code": "import numpy as np\n\nclass AdaptiveDEAR:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, Cr=0.9, archive_size=10, restart_trigger=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.Cr = Cr  # Crossover rate\n        self.archive_size = archive_size\n        self.archive = []\n        self.restart_trigger = restart_trigger\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count = self.pop_size\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                if len(self.archive) > 0 and np.random.rand() < 0.1:\n                    # Use archive with a probability of 0.1\n                    idx = np.random.choice(len(self.archive), 2, replace=False)\n                    x_r1 = self.archive[idx[0]]\n                    x_r2 = self.archive[idx[1]]\n\n                else:\n                    idx = np.random.choice(self.pop_size, 3, replace=False)\n                    while i in idx:\n                        idx = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1 = self.population[idx[0]]\n                    x_r2 = self.population[idx[1]]\n                    x_r3 = self.population[idx[2]]\n\n                    \n                x_mutated = self.population[i] + self.F * (x_r1 - x_r2)\n\n                # Crossover\n                x_trial = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.Cr or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n                \n                # Selection\n                f_trial = func(x_trial)\n                self.eval_count += 1\n\n                if f_trial < self.fitness[i]:\n                    # Update population and archive\n                    self.fitness[i] = f_trial\n                    self.population[i] = x_trial\n\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i])\n                    else:\n                        # Replace a random element in the archive\n                        idx_replace = np.random.randint(self.archive_size)\n                        self.archive[idx_replace] = self.population[i]\n\n                    # Update best solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n\n                else:\n                    # Add parent to archive if trial is worse\n                     if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i])\n                     else:\n                        # Replace a random element in the archive\n                        idx_replace = np.random.randint(self.archive_size)\n                        self.archive[idx_replace] = self.population[i]\n\n            # Restart mechanism: Checks for stagnation and restarts the population if necessary\n            if self.eval_count > self.budget * 0.1 and np.std(self.fitness) < self.restart_trigger: # Check for stagnation after 10% budget is used\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.archive = []\n                self.eval_count += self.pop_size\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 144, in evaluate\n    algorithm(f_new)\n  File \"<string>\", line 26, in __call__\n  File \"mtrand.pyx\", line 984, in numpy.random.mtrand.RandomState.choice\nValueError: Cannot take a larger sample than population when 'replace=False'\n.", "error": "", "parent_ids": [], "operator": null, "metadata": {}}
{"id": "616e90ef-29bc-40ee-aa96-7b17f0c2d49e", "fitness": 0.5760827054306763, "name": "AdaptiveDEAR", "description": "Adaptive Differential Evolution with Archive and Restart: A population-based algorithm that evolves solutions through mutation, crossover, and selection, incorporating an archive of past solutions to enhance exploration and a restart mechanism to escape local optima.", "code": "import numpy as np\n\nclass AdaptiveDEAR:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, Cr=0.9, archive_size=10, restart_trigger=0.001):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.Cr = Cr\n        self.archive_size = archive_size\n        self.restart_trigger = restart_trigger\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        archive = []\n        \n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n        \n        iteration = 0\n        \n        while self.budget > self.pop_size:\n            iteration += 1\n            \n            # Mutation and Crossover\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Mutation\n                indices = [j for j in range(self.pop_size) if j != i]\n                if len(archive) > 0 and np.random.rand() < 0.1: # Use archive sometimes\n                    use_archive = True\n                    a_idx = np.random.randint(len(archive))\n                    x_r1 = archive[a_idx]\n                    idxs = np.random.choice(indices, size=2, replace=False)\n                    x_r2, x_r3 = population[idxs[0]], population[idxs[1]]\n                    \n                else:\n                    use_archive = False\n                    idxs = np.random.choice(indices, size=3, replace=False)\n                    x_r1, x_r2, x_r3 = population[idxs[0]], population[idxs[1]], population[idxs[2]]\n\n                mutant = population[i] + self.F * (x_r1 - x_r2) + self.F * (x_r3 - population[i]) if not use_archive else population[i] + self.F * (x_r1 - x_r2)\n                \n                # Crossover\n                for j in range(self.dim):\n                    if np.random.rand() < self.Cr:\n                        new_population[i, j] = mutant[j]\n                    else:\n                        new_population[i, j] = population[i, j]\n                \n                new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n            \n            # Evaluation\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n            \n            # Selection\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    \n                    # Update archive\n                    if len(archive) < self.archive_size:\n                        archive.append(population[i].copy())\n                    else:\n                        idx_to_replace = np.random.randint(self.archive_size)\n                        archive[idx_to_replace] = population[i].copy()\n                    \n                    population[i] = new_population[i]\n                    fitness[i] = new_fitness[i]\n                    \n                    if new_fitness[i] < self.f_opt:\n                        self.f_opt = new_fitness[i]\n                        self.x_opt = new_population[i]\n                        \n            # Restart Mechanism\n            if np.std(fitness) < self.restart_trigger:\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n\n                if np.min(fitness) < self.f_opt:\n                    self.f_opt = np.min(fitness)\n                    self.x_opt = population[np.argmin(fitness)]\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDEAR scored 0.576 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.2550563591378008, 0.53294782647898, 0.4422639271544816, 0.894519205000661, 0.5116786046612649, 0.5108448262657219, 0.62835798161087, 0.46002844527403297, 0.811962541812433, 0.5626575913238482, 0.9022995334232866, 0.9991692039405679, 0.47507216969257937, 0.6862657100376142, 0.7226727533596822, 0.3812020848507356, 0.4815374531538933, 0.5666005771429838, 0.21970492348299409, 0.47681239080909577]}}
{"id": "e6f27bc4-098e-496a-9c03-f1890a407edb", "fitness": 0.7132677563121383, "name": "AdaptiveDE", "description": "An adaptive differential evolution strategy that adjusts its parameters based on the function evaluations and maintains a diverse population.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        \n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index].copy()\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            # Mutation\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[idxs]\n            x_mutated = x_r1 + self.F * (x_r2 - x_r3)\n            x_mutated = np.clip(x_mutated, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            x_trial = self.population[i].copy()\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.CR or j == j_rand:\n                    x_trial[j] = x_mutated[j]\n\n            # Selection\n            f_trial = func(x_trial)\n            self.eval_count += 1\n\n            if f_trial < self.fitness[i]:\n                self.population[i] = x_trial\n                self.fitness[i] = f_trial\n                \n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = x_trial.copy()\n\n            # Adaptive parameter adjustment\n            if self.eval_count % (self.budget // 10) == 0:  # Adjust every 10% of budget\n                self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n                self.CR = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0)\n\n            if self.eval_count >= self.budget:\n                break\n                \n    def __call__(self, func):\n        self.initialize_population(func)\n        while self.eval_count < self.budget:\n            self.evolve(func)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDE scored 0.713 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.45258581579555035, 0.6761640663906656, 0.6789647808678401, 0.8659454379407654, 0.7224857697069544, 0.7994429681722723, 0.5887905844987715, 0.6431034348124101, 0.7429008554386459, 0.7009195347291302, 0.8589655817221933, 0.998917964381844, 0.5067156915059832, 0.7640805233942942, 0.9312525175072195, 0.7849221708524158, 0.5499467540114216, 0.8403210247825007, 0.544255440617992, 0.6146742091138974]}}
{"id": "4a92677b-50a0-4544-87dd-f1a0bc7d847d", "fitness": 0.0, "name": "SOSBee", "description": "A self-organizing scout bee algorithm that dynamically adjusts search parameters based on the fitness landscape.", "code": "import numpy as np\n\nclass SOSBee:\n    def __init__(self, budget=10000, dim=10, colony_size=50, scout_bees=5, neighborhood_size=3, reduction_factor=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.colony_size = colony_size\n        self.scout_bees = scout_bees\n        self.neighborhood_size = neighborhood_size  # Number of neighbors to consider for adaptation\n        self.reduction_factor = reduction_factor # Reduction factor for step size adaptation\n\n        self.population = None\n        self.fitness = None\n        self.step_sizes = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.colony_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.step_sizes = np.full((self.colony_size, self.dim), 0.1 * (func.bounds.ub - func.bounds.lb))  # Initialize step sizes\n        self.eval_count += self.colony_size\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n\n\n    def scout_phase(self, func):\n        for _ in range(self.scout_bees):\n            index = np.random.randint(self.colony_size)  # Randomly select a bee to become a scout\n            x_new = np.random.uniform(func.bounds.lb, func.bounds.ub)\n            f_new = func(x_new)\n            self.eval_count += 1\n\n            if f_new < self.fitness[index]:\n                self.fitness[index] = f_new\n                self.population[index] = x_new\n                self.step_sizes[index] = np.full(self.dim, 0.1 * (func.bounds.ub - func.bounds.lb)) # Reset step size for scout\n\n                if f_new < self.f_opt:\n                    self.f_opt = f_new\n                    self.x_opt = x_new\n\n\n    def employed_bee_phase(self, func):\n        for i in range(self.colony_size):\n            neighbor_indices = np.random.choice(self.colony_size, self.neighborhood_size, replace=False)\n            \n            # Adaptive step size adjustment based on neighbors\n            delta = np.mean(self.population[neighbor_indices] - self.population[i], axis=0)\n            x_new = self.population[i] + self.step_sizes[i] * delta\n            x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n            \n            f_new = func(x_new)\n            self.eval_count += 1\n\n            if f_new < self.fitness[i]:\n                self.fitness[i] = f_new\n                self.population[i] = x_new\n\n                # Reduce step size if improvement is found\n                self.step_sizes[i] *= self.reduction_factor\n\n                if f_new < self.f_opt:\n                    self.f_opt = f_new\n                    self.x_opt = x_new\n            else:\n                # Increase step size if no improvement is found, to explore more\n                self.step_sizes[i] /= self.reduction_factor # Increase exploration\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            self.employed_bee_phase(func)\n            self.scout_phase(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm SOSBee scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e408772a-7a0d-42f4-a489-45e8567bf10e"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "a3801960-e454-4a89-9c7c-6bcc726f3912", "fitness": 0.0, "name": "SANE", "description": "Self-Adaptive Differential Evolution with Neighborhood Search and Cauchy Mutation: Adapts DE parameters, uses neighborhood search for local exploitation, and employs Cauchy mutation to escape local optima more effectively.", "code": "import numpy as np\n\nclass SANE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, Cr_init=0.9, neighborhood_size=5, cauchy_scale=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_init = F_init\n        self.Cr_init = Cr_init\n        self.neighborhood_size = neighborhood_size\n        self.cauchy_scale = cauchy_scale\n        self.F = np.full(pop_size, F_init)\n        self.Cr = np.full(pop_size, Cr_init)\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n        \n        while self.budget > self.pop_size:\n            # Mutation and Crossover\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Adaptive F and Cr\n                self.F[i] = np.clip(self.F[i] + 0.1 * np.random.normal(0, 1), 0.1, 1.0)\n                self.Cr[i] = np.clip(self.Cr[i] + 0.1 * np.random.normal(0, 1), 0.1, 1.0)\n                \n                # Mutation\n                indices = [j for j in range(self.pop_size) if j != i]\n                idxs = np.random.choice(indices, size=3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs[0]], population[idxs[1]], population[idxs[2]]\n\n                mutant = population[i] + self.F[i] * (x_r1 - x_r2) + self.F[i] * (x_r3 - population[i])\n\n                # Crossover\n                for j in range(self.dim):\n                    if np.random.rand() < self.Cr[i]:\n                        new_population[i, j] = mutant[j]\n                    else:\n                        new_population[i, j] = population[i, j]\n                \n                new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n\n                # Neighborhood Search\n                for _ in range(self.neighborhood_size):\n                    neighbor = new_population[i] + self.cauchy_scale * np.random.standard_cauchy(size=self.dim)\n                    neighbor = np.clip(neighbor, func.bounds.lb, func.bounds.ub)\n                    neighbor_fitness = func(neighbor)\n                    self.budget -= 1\n                    if neighbor_fitness < func(new_population[i]):\n                        new_population[i] = neighbor\n                    if self.budget <= self.pop_size:\n                        break\n            \n            # Evaluation\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n            \n            # Selection\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    population[i] = new_population[i]\n                    fitness[i] = new_fitness[i]\n                    \n                    if new_fitness[i] < self.f_opt:\n                        self.f_opt = new_fitness[i]\n                        self.x_opt = new_population[i]\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm SANE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["616e90ef-29bc-40ee-aa96-7b17f0c2d49e"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "67af55c2-ee58-42e9-a1b3-637b54688b3b", "fitness": 0.0, "name": "SelfAdaptiveDE", "description": "Self-Adaptive Differential Evolution with Neighborhood Search, dynamically adjusting mutation and crossover rates based on individual success and exploring the neighborhood of the best solution.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_init=0.5, CR_init=0.9, neighborhood_size=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = np.full(self.pop_size, F_init)  # Individual mutation factors\n        self.CR = np.full(self.pop_size, CR_init)  # Individual crossover rates\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.neighborhood_size = neighborhood_size # Size of the neighborhood for local search\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        \n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index].copy()\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            # Parameter adaptation\n            F_i = self.F[i]\n            CR_i = self.CR[i]\n\n            # Mutation\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[idxs]\n            x_mutated = x_r1 + F_i * (x_r2 - x_r3)\n            x_mutated = np.clip(x_mutated, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            x_trial = self.population[i].copy()\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < CR_i or j == j_rand:\n                    x_trial[j] = x_mutated[j]\n\n            # Selection\n            f_trial = func(x_trial)\n            self.eval_count += 1\n\n            if f_trial < self.fitness[i]:\n                # Update population and fitness\n                self.population[i] = x_trial\n                self.fitness[i] = f_trial\n\n                # Update F and CR based on success\n                self.F[i] = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n                self.CR[i] = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0)\n                \n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = x_trial.copy()\n            else:\n                # If unsuccessful, revert F and CR towards initial values\n                self.F[i] = 0.9 * self.F[i] + 0.1 * 0.5\n                self.CR[i] = 0.9 * self.CR[i] + 0.1 * 0.9\n\n        # Neighborhood search around the best solution\n        if self.eval_count % (self.budget // 10) == 0:\n            x_neighbor = self.x_opt + np.random.uniform(-self.neighborhood_size, self.neighborhood_size, size=self.dim)\n            x_neighbor = np.clip(x_neighbor, func.bounds.lb, func.bounds.ub)\n            f_neighbor = func(x_neighbor)\n            self.eval_count += 1\n\n            if f_neighbor < self.f_opt:\n                self.f_opt = f_neighbor\n                self.x_opt = x_neighbor.copy()\n\n            # Perturb population around the best solution\n            for i in range(self.pop_size):\n                if np.random.rand() < 0.1:  # Perturb 10% of the population\n                    self.population[i] = self.x_opt + np.random.uniform(-self.neighborhood_size, self.neighborhood_size, size=self.dim)\n                    self.population[i] = np.clip(self.population[i], func.bounds.lb, func.bounds.ub)\n                    self.fitness[i] = func(self.population[i])\n                    self.eval_count += 1\n                    if self.fitness[i] < self.f_opt:\n                         self.f_opt = self.fitness[i]\n                         self.x_opt = self.population[i].copy()\n        if self.eval_count >= self.budget:\n            return\n                \n    def __call__(self, func):\n        self.initialize_population(func)\n        while self.eval_count < self.budget:\n            self.evolve(func)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm SelfAdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e6f27bc4-098e-496a-9c03-f1890a407edb"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "a1d06e1e-97cc-4450-bd97-a658aaea0d4b", "fitness": -Infinity, "name": "AdaptiveDEOL", "description": "Adaptive Differential Evolution with orthogonal learning, archive, and dynamic parameter adaptation for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDEOL:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, Cr=0.9, archive_size=10, restart_trigger=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.Cr = Cr  # Crossover rate\n        self.archive_size = archive_size\n        self.archive = []\n        self.restart_trigger = restart_trigger\n        self.min_F = 0.1\n        self.max_F = 0.9\n        self.min_Cr = 0.1\n        self.max_Cr = 0.9\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count = self.pop_size\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Parameter adaptation\n                self.F = np.clip(np.random.normal(self.F, 0.1), self.min_F, self.max_F)\n                self.Cr = np.clip(np.random.normal(self.Cr, 0.1), self.min_Cr, self.max_Cr)\n                \n                # Mutation\n                if len(self.archive) > 0 and np.random.rand() < 0.1:\n                    # Use archive with a probability of 0.1\n                    idx = np.random.choice(len(self.archive), 2, replace=False)\n                    x_r1 = self.archive[idx[0]]\n                    x_r2 = self.archive[idx[1]]\n\n                else:\n                    idx = np.random.choice(self.pop_size, 3, replace=False)\n                    while i in idx:\n                        idx = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1 = self.population[idx[0]]\n                    x_r2 = self.population[idx[1]]\n                    x_r3 = self.population[idx[2]]\n\n                x_mutated = self.population[i] + self.F * (x_r1 - x_r2)\n\n                # Orthogonal Learning\n                levels = 3\n                ol_vector = np.zeros(self.dim)\n                for j in range(self.dim):\n                    level_idx = np.random.randint(levels)\n                    ol_vector[j] = x_r1[j] + (level_idx - 1) * (x_r2[j] - x_r3[j])\n\n                # Crossover\n                x_trial = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.Cr or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n                    else:\n                        x_trial[j] = ol_vector[j] # Incorporate orthogonal learning\n\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n                \n                # Selection\n                f_trial = func(x_trial)\n                self.eval_count += 1\n\n                if f_trial < self.fitness[i]:\n                    # Update population and archive\n                    self.fitness[i] = f_trial\n                    self.population[i] = x_trial\n\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i])\n                    else:\n                        # Replace a random element in the archive\n                        idx_replace = np.random.randint(self.archive_size)\n                        self.archive[idx_replace] = self.population[i]\n\n                    # Update best solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n\n                else:\n                    # Add parent to archive if trial is worse\n                     if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i])\n                     else:\n                        # Replace a random element in the archive\n                        idx_replace = np.random.randint(self.archive_size)\n                        self.archive[idx_replace] = self.population[i]\n\n            # Restart mechanism: Checks for stagnation and restarts the population if necessary\n            if self.eval_count > self.budget * 0.1 and np.std(self.fitness) < self.restart_trigger: # Check for stagnation after 10% budget is used\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.archive = []\n                self.eval_count += self.pop_size\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 144, in evaluate\n    algorithm(f_new)\n  File \"<string>\", line 34, in __call__\n  File \"mtrand.pyx\", line 984, in numpy.random.mtrand.RandomState.choice\nValueError: Cannot take a larger sample than population when 'replace=False'\n.", "error": "", "parent_ids": ["e408772a-7a0d-42f4-a489-45e8567bf10e"], "operator": null, "metadata": {}}
{"id": "bcdf7c89-c898-48da-90a3-3aba72df981e", "fitness": 0.0, "name": "AdaptiveDESEnsembleLocal", "description": "Adaptive Differential Evolution with a Self-Adjusting Mutation Strategy and Ensemble Crossover, incorporating a local search step and a diversity maintenance mechanism.", "code": "import numpy as np\n\nclass AdaptiveDESEnsembleLocal:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_base=0.5, Cr_base=0.9, local_search_prob=0.1, diversity_threshold=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_base = F_base\n        self.Cr_base = Cr_base\n        self.local_search_prob = local_search_prob\n        self.diversity_threshold = diversity_threshold\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n        \n        iteration = 0\n        \n        while self.budget > self.pop_size:\n            iteration += 1\n            \n            # Adaptive Mutation and Ensemble Crossover\n            new_population = np.copy(population)\n            new_fitness = np.zeros(self.pop_size)\n            \n            for i in range(self.pop_size):\n                # Adaptive F (Mutation factor)\n                F = self.F_base * np.random.uniform(0.5, 1.5)  # Self-adjusting F\n                \n                # Mutation\n                indices = [j for j in range(self.pop_size) if j != i]\n                idxs = np.random.choice(indices, size=3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs[0]], population[idxs[1]], population[idxs[2]]\n                mutant = population[i] + F * (x_r1 - x_r2) + F * (x_r3 - population[i])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Ensemble Crossover: Choose between different Cr values\n                Cr = self.Cr_base * np.random.uniform(0.5, 1.5)\n                crossover_strategy = np.random.choice(['bin', 'exp'])\n                \n                if crossover_strategy == 'bin':  # Binomial Crossover\n                    for j in range(self.dim):\n                        if np.random.rand() < Cr or j == np.random.randint(self.dim):\n                            new_population[i, j] = mutant[j]\n                        else:\n                            new_population[i, j] = population[i, j]\n                else:  # Exponential Crossover\n                    L = 0\n                    while L < self.dim and np.random.rand() < Cr:\n                        L += 1\n                    for j in range(L):\n                        new_population[i, (np.random.randint(self.dim) + j) % self.dim] = mutant[(np.random.randint(self.dim) + j) % self.dim]\n                \n                new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                \n                # Local Search\n                if np.random.rand() < self.local_search_prob:\n                    step_size = 0.01 * (func.bounds.ub - func.bounds.lb)\n                    new_x = new_population[i] + np.random.uniform(-step_size, step_size, size=self.dim)\n                    new_x = np.clip(new_x, func.bounds.lb, func.bounds.ub)\n                    \n                    new_f = func(new_x)\n                    self.budget -= 1\n                    \n                    if new_f < func(new_population[i]):\n                        new_population[i] = new_x\n                        new_fitness[i] = new_f\n                    else:\n                        new_fitness[i] = func(new_population[i])\n                        self.budget -= 1\n\n                else:\n                     new_fitness[i] = func(new_population[i])\n                     self.budget -=1\n                \n            # Selection\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    population[i] = new_population[i]\n                    fitness[i] = new_fitness[i]\n                    \n                    if new_fitness[i] < self.f_opt:\n                        self.f_opt = new_fitness[i]\n                        self.x_opt = new_population[i]\n            \n            # Diversity Maintenance\n            if np.std(fitness) < self.diversity_threshold:\n                # Introduce new random solutions to increase diversity\n                num_to_replace = int(self.pop_size * 0.2)  # Replace 20% of the population\n                replace_indices = np.random.choice(self.pop_size, size=num_to_replace, replace=False)\n                \n                population[replace_indices] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(num_to_replace, self.dim))\n                fitness[replace_indices] = np.array([func(x) for x in population[replace_indices]])\n                self.budget -= num_to_replace\n\n                if np.min(fitness) < self.f_opt:\n                    self.f_opt = np.min(fitness)\n                    self.x_opt = population[np.argmin(fitness)]\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDESEnsembleLocal scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["616e90ef-29bc-40ee-aa96-7b17f0c2d49e"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "7f9af33b-1ac7-4655-972e-2b7b2cfe913c", "fitness": 0.19624449624704463, "name": "AdaptiveStepSizeRandomSearch", "description": "Self-Adaptive Step Size Random Search: Randomly searches the space while adapting the step size based on the success rate of finding better solutions.", "code": "import numpy as np\n\nclass AdaptiveStepSizeRandomSearch:\n    def __init__(self, budget=10000, dim=10, initial_step_size=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.step_size = initial_step_size\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        x = np.random.uniform(self.lb, self.ub, size=self.dim)\n        f_best = func(x)\n        eval_count = 1\n        x_best = x.copy()\n\n        success_count = 0\n        total_trials = 0\n\n        while eval_count < self.budget:\n            # Generate a new candidate solution by adding a random step\n            x_new = x + np.random.normal(0, self.step_size, size=self.dim)\n\n            # Clip the solution to stay within bounds\n            x_new = np.clip(x_new, self.lb, self.ub)\n\n            f_new = func(x_new)\n            eval_count += 1\n\n            if f_new < f_best:\n                f_best = f_new\n                x_best = x_new.copy()\n                x = x_new\n                success_count += 1\n            else:\n                # Keep the old solution\n                pass\n            \n            total_trials += 1\n\n            # Adjust step size based on success rate\n            if total_trials % 100 == 0:\n                success_rate = success_count / total_trials\n                if success_rate > 0.2:\n                    self.step_size *= 1.1  # Increase step size\n                elif success_rate < 0.1:\n                    self.step_size *= 0.9  # Decrease step size\n\n                # Reset counters\n                success_count = 0\n                total_trials = 0\n                \n            if eval_count >= self.budget:\n                 break\n\n        return f_best, x_best", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveStepSizeRandomSearch scored 0.196 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["058d1560-fbdf-4c5c-8f01-64eb6442c127"], "operator": null, "metadata": {"aucs": [0.026408910357397453, 0.08423073892459743, 0.24042286905203658, 0.16638365466392846, 0.10292003212864731, 0.14324919107998524, 0.16302111089852278, 0.13397110661898404, 0.1491543993858595, 0.12049012155694161, 0.9474172397262248, 0.14470956655965694, 0.22416698189266404, 0.166013956622046, 0.17620529615273095, 0.31976272754759316, 0.22463738619629292, 0.15408907088102353, 0.11185148618233853, 0.12578407851342144]}}
{"id": "2834f1ed-a461-463a-b6d1-bb2d2278575b", "fitness": 0.14043527165912123, "name": "GradientEstimationRestart", "description": "Gradient Estimation with Random Restarts: Estimates the gradient using random sampling and adapts the step size based on success, restarting the search from a new random location if stagnation is detected.", "code": "import numpy as np\n\nclass GradientEstimationRestart:\n    def __init__(self, budget=10000, dim=10, step_size=0.1, num_samples=10, restart_patience=1000):\n        self.budget = budget\n        self.dim = dim\n        self.step_size = step_size\n        self.num_samples = num_samples\n        self.restart_patience = restart_patience\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        x = np.random.uniform(self.lb, self.ub, size=self.dim)\n        f_current = func(x)\n        eval_count = 1\n        f_opt = f_current\n        x_opt = x.copy()\n        no_improvement_count = 0\n\n        while eval_count < self.budget:\n            # Estimate gradient\n            gradient = np.zeros(self.dim)\n            for _ in range(self.num_samples):\n                direction = np.random.normal(0, 1, size=self.dim)\n                direction /= np.linalg.norm(direction)  # Normalize\n                x_perturbed = x + self.step_size * direction\n                x_perturbed = np.clip(x_perturbed, self.lb, self.ub)\n                f_perturbed = func(x_perturbed)\n                eval_count += 1\n                gradient += (f_perturbed - f_current) * direction\n\n                if eval_count >= self.budget:\n                    break\n\n            gradient /= self.num_samples * self.step_size\n\n            # Update position\n            x_new = x - self.step_size * gradient\n            x_new = np.clip(x_new, self.lb, self.ub)\n            f_new = func(x_new)\n            eval_count += 1\n\n            if f_new < f_current:\n                x = x_new\n                f_current = f_new\n                no_improvement_count = 0\n                if f_new < f_opt:\n                    f_opt = f_new\n                    x_opt = x.copy()\n            else:\n                no_improvement_count += 1\n                self.step_size *= 0.9  # Reduce step size if no improvement\n\n            # Restart if no improvement for too long\n            if no_improvement_count > self.restart_patience:\n                x = np.random.uniform(self.lb, self.ub, size=self.dim)\n                f_current = func(x)\n                eval_count += 1\n                self.step_size = 0.1 # Reinitialize stepsize\n                no_improvement_count = 0\n\n\n            if eval_count >= self.budget:\n                break\n\n        return f_opt, x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm GradientEstimationRestart scored 0.140 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["058d1560-fbdf-4c5c-8f01-64eb6442c127"], "operator": null, "metadata": {"aucs": [0.06136979700162315, 0.06250869220137667, 0.1568046209919599, 0.08637498310508451, 0.12304921880381825, 0.09513866383887959, 0.17128798316464056, 0.13315717932616244, 0.12357738211577085, 0.10614777142892085, 0.1239155296894473, 0.11333714123738314, 0.22848415456053106, 0.1169108702682804, 0.35139746164171937, 0.21711610669670545, 0.13864599161167623, 0.15662722349645153, 0.10256621010798317, 0.14028845189400962]}}
{"id": "d597ebc8-ab19-4213-82a6-59b48c9b9ce3", "fitness": 0.3477457525327498, "name": "SimplifiedCMAES", "description": "A simplified CMA-ES variant with adaptive step size and mean adaptation, but without covariance matrix adaptation, focusing on faster computation and exploration.", "code": "import numpy as np\n\nclass SimplifiedCMAES:\n    def __init__(self, budget=10000, dim=10, sigma0=0.5, step_size_factor=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.lb = -5.0\n        self.ub = 5.0\n        self.step_size_factor = step_size_factor\n\n    def __call__(self, func):\n        # Initialize variables\n        mu = np.random.uniform(self.lb, self.ub, size=self.dim)\n        sigma = self.sigma0  # Overall standard deviation\n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n\n        while eval_count < self.budget:\n            # Generate a single candidate solution\n            z = np.random.normal(0, 1, size=self.dim)\n            x = mu + sigma * z\n\n            # Repair individual outside the bounds\n            x = np.clip(x, self.lb, self.ub)\n\n            # Evaluate candidate\n            f = func(x)\n            eval_count += 1\n\n            # Update the mean and step size\n            if f < f_opt:\n                f_opt = f\n                x_opt = x.copy()\n                mu = x.copy() # Move mean towards the better solution\n                sigma *= self.step_size_factor  # Reduce step size\n\n            else:\n                sigma /= self.step_size_factor # increase step size\n\n            sigma = np.clip(sigma, 1e-6, 1)  # Keep sigma within reasonable bounds\n\n        return f_opt, x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm SimplifiedCMAES scored 0.348 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["058d1560-fbdf-4c5c-8f01-64eb6442c127"], "operator": null, "metadata": {"aucs": [0.16650106810711518, 0.2575019222149627, 0.3366736924313265, 0.3607829186179283, 0.26681812129589355, 0.34005812014876213, 0.2752588515357366, 0.2844733997313714, 0.27365824857693166, 0.15935903068593726, 0.3234651515614957, 0.9967731718898276, 0.27788798124175795, 0.28251476530210284, 0.701524707193325, 0.32328090501440987, 0.2945778236841228, 0.3815343993135879, 0.1606863282009826, 0.4915844439074185]}}
{"id": "e230bfdd-67ce-4422-b332-c5968396f503", "fitness": 0.36405656789744273, "name": "SADE_NS", "description": "Self-Adaptive Differential Evolution with Neighborhood Search and a Cauchy mutation operator to enhance exploration and exploitation.", "code": "import numpy as np\n\nclass SADE_NS:\n    def __init__(self, budget=10000, dim=10, pop_size=40, archive_size=10, neighborhood_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.neighborhood_size = neighborhood_size\n        self.F = 0.5\n        self.Cr = 0.9\n        self.F_adapt_rate = 0.1\n        self.Cr_adapt_rate = 0.1\n\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        archive = []\n        \n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n        \n        iteration = 0\n        \n        while self.budget > self.pop_size:\n            iteration += 1\n            \n            # Mutation and Crossover\n            new_population = np.copy(population)\n            new_fitness = np.zeros(self.pop_size)\n            for i in range(self.pop_size):\n                # Mutation\n                indices = [j for j in range(self.pop_size) if j != i]\n                idxs = np.random.choice(indices, size=3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs[0]], population[idxs[1]], population[idxs[2]]\n\n                # Cauchy mutation\n                mutant = population[i] + self.F * (x_r1 - x_r2) + np.random.standard_cauchy(size=self.dim) * self.F * (x_r3 - population[i])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                for j in range(self.dim):\n                    if np.random.rand() < self.Cr:\n                        new_population[i, j] = mutant[j]\n                    #else:  # No need to explicitly copy, already copied in line 35\n                        #new_population[i, j] = population[i, j]\n\n                new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                new_fitness[i] = func(new_population[i])\n                self.budget -= 1\n                \n            # Selection and Adaptation\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    # Update archive\n                    if len(archive) < self.archive_size:\n                        archive.append(population[i].copy())\n                    else:\n                        idx_to_replace = np.random.randint(self.archive_size)\n                        archive[idx_to_replace] = population[i].copy()\n                    \n                    population[i] = new_population[i]\n                    fitness[i] = new_fitness[i]\n                    \n                    if new_fitness[i] < self.f_opt:\n                        self.f_opt = new_fitness[i]\n                        self.x_opt = new_population[i]\n                        \n                    # Adaptive F and Cr\n                    self.F = self.F * (1 - self.F_adapt_rate) + np.random.rand() * self.F_adapt_rate\n                    self.Cr = self.Cr * (1 - self.Cr_adapt_rate) + np.random.rand() * self.Cr_adapt_rate\n\n            # Neighborhood Search\n            best_index = np.argmin(fitness)\n            neighborhood_indices = np.random.choice(self.pop_size, size=self.neighborhood_size, replace=False)\n            for idx in neighborhood_indices:\n              x_neighbor = population[idx] + np.random.normal(0, 0.05, self.dim)  # Small perturbation\n              x_neighbor = np.clip(x_neighbor, func.bounds.lb, func.bounds.ub)\n              f_neighbor = func(x_neighbor)\n              self.budget -= 1\n              if f_neighbor < fitness[idx]:\n                population[idx] = x_neighbor\n                fitness[idx] = f_neighbor\n                if f_neighbor < self.f_opt:\n                  self.f_opt = f_neighbor\n                  self.x_opt = x_neighbor\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm SADE_NS scored 0.364 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["616e90ef-29bc-40ee-aa96-7b17f0c2d49e"], "operator": null, "metadata": {"aucs": [0.14925849437666716, 0.2162883082167738, 0.37356463082547064, 0.35183461658319914, 0.2947920375251707, 0.38173791631695864, 0.30116685504327556, 0.32325746159465296, 0.26998940150176587, 0.20973997501209973, 0.3177857803775944, 0.9974905634446533, 0.2814606277376712, 0.2918578710919243, 0.7081990442521698, 0.38212232499100607, 0.3066888843008615, 0.452375992052525, 0.1888530365493477, 0.48266753615506763]}}
{"id": "0929134e-94e3-4a24-b5fb-b838ae98b0fb", "fitness": 0.32225230742944433, "name": "AdaptiveDEENM", "description": "Adaptive Differential Evolution with Elitism and Euclidean Neighborhood Mutation, enhancing exploitation and exploration via elitism and local search.", "code": "import numpy as np\n\nclass AdaptiveDEENM:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, Cr=0.9, elite_count=2, neighborhood_size=3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.Cr = Cr  # Crossover rate\n        self.elite_count = elite_count\n        self.neighborhood_size = neighborhood_size\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count = self.pop_size\n\n        while self.eval_count < self.budget:\n            # Sort population by fitness\n            sorted_indices = np.argsort(self.fitness)\n            self.population = self.population[sorted_indices]\n            self.fitness = self.fitness[sorted_indices]\n            \n            if self.fitness[0] < self.f_opt:\n                self.f_opt = self.fitness[0]\n                self.x_opt = self.population[0]\n\n            for i in range(self.pop_size):\n                # Mutation - Euclidean Neighborhood Mutation\n                neighbors_indices = np.random.choice(self.pop_size, self.neighborhood_size, replace=False)\n                \n                # Calculate the Euclidean distances\n                distances = np.linalg.norm(self.population[neighbors_indices] - self.population[i], axis=1)\n                \n                # Select two different neighbors based on distances\n                idx = np.argsort(distances)[:2]\n                x_r1 = self.population[neighbors_indices[idx[0]]]\n                x_r2 = self.population[neighbors_indices[idx[1]]]\n\n                x_mutated = self.population[i] + self.F * (x_r1 - x_r2)\n\n                # Crossover\n                x_trial = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.Cr or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n                \n                # Selection\n                f_trial = func(x_trial)\n                self.eval_count += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = x_trial\n                \n                # Elitism: Preserve the best solutions\n                sorted_indices = np.argsort(self.fitness)\n                self.population = self.population[sorted_indices]\n                self.fitness = self.fitness[sorted_indices]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDEENM scored 0.322 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e408772a-7a0d-42f4-a489-45e8567bf10e"], "operator": null, "metadata": {"aucs": [0.142994502791422, 0.21226590595065664, 0.26341952248026623, 0.2690423245456959, 0.2490886753997983, 0.29575717092424836, 0.2538652155410004, 0.24940019276960357, 0.2246929223182198, 0.1927224857176303, 0.2444053437604674, 0.9992706098016021, 0.3015660926137582, 0.2604685705248847, 0.6820143691831393, 0.33698753071609866, 0.2677591194531841, 0.34440911796663276, 0.17557013659793663, 0.4793463395326405]}}
{"id": "57d948e2-0b4f-483f-a55c-d7c8a0af2618", "fitness": 0.6693177965502215, "name": "SelfAdaptiveDE", "description": "An adaptive differential evolution strategy with self-adaptive mutation factor and crossover rate, dynamically adjusting based on the success of previous generations.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_init=0.5, CR_init=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_init = F_init  # Initial mutation factor\n        self.CR_init = CR_init  # Initial crossover rate\n        self.population = None\n        self.fitness = None\n        self.F = None\n        self.CR = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        self.F = np.full(self.pop_size, self.F_init)\n        self.CR = np.full(self.pop_size, self.CR_init)\n        \n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index].copy()\n\n    def evolve(self, func):\n        successful_F = []\n        successful_CR = []\n\n        for i in range(self.pop_size):\n            # Parameter adaptation\n            self.F[i] = np.clip(np.random.normal(self.F[i], 0.1), 0.1, 1.0)\n            self.CR[i] = np.clip(np.random.normal(self.CR[i], 0.1), 0.1, 1.0)\n            \n            # Mutation\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[idxs]\n            x_mutated = x_r1 + self.F[i] * (x_r2 - x_r3)\n            x_mutated = np.clip(x_mutated, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            x_trial = self.population[i].copy()\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.CR[i] or j == j_rand:\n                    x_trial[j] = x_mutated[j]\n\n            # Selection\n            f_trial = func(x_trial)\n            self.eval_count += 1\n\n            if f_trial < self.fitness[i]:\n                successful_F.append(self.F[i])\n                successful_CR.append(self.CR[i])\n                \n                self.population[i] = x_trial\n                self.fitness[i] = f_trial\n                \n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = x_trial.copy()\n\n            if self.eval_count >= self.budget:\n                break\n        \n        # Update F and CR based on successful values from the generation\n        if successful_F:\n            self.F = np.full(self.pop_size, np.mean(successful_F))\n        if successful_CR:\n            self.CR = np.full(self.pop_size, np.mean(successful_CR))\n                \n    def __call__(self, func):\n        self.initialize_population(func)\n        while self.eval_count < self.budget:\n            self.evolve(func)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm SelfAdaptiveDE scored 0.669 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e6f27bc4-098e-496a-9c03-f1890a407edb"], "operator": null, "metadata": {"aucs": [0.33969162422574095, 0.292452172117203, 0.4857980113631578, 0.8800134487487903, 0.7733419637390303, 0.816026511142028, 0.7820339213754879, 0.4526674101942002, 0.7803107770696811, 0.7447155570626112, 0.8791377632119377, 0.9901967979473189, 0.3748629293060478, 0.7879949093370479, 0.9073213919769628, 0.8326189487923996, 0.5484826694174341, 0.8689464065368186, 0.3194015224865322, 0.5303411949540029]}}
{"id": "c8c94de2-a36a-48c7-9e66-1857ea3615f0", "fitness": 0.6249138894346234, "name": "AdaptiveDELocalSearch", "description": "Adaptive Differential Evolution with a decaying exploration rate and a local search component.", "code": "import numpy as np\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, Cr=0.9, exploration_decay=0.995, local_search_prob=0.1, local_search_radius=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.Cr = Cr\n        self.exploration_decay = exploration_decay\n        self.local_search_prob = local_search_prob\n        self.local_search_radius = local_search_radius\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n        \n        exploration_rate = 1.0 \n        \n        while self.budget > self.pop_size:\n            # Mutation and Crossover\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Mutation\n                indices = [j for j in range(self.pop_size) if j != i]\n                idxs = np.random.choice(indices, size=3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs[0]], population[idxs[1]], population[idxs[2]]\n                \n                mutant = population[i] + self.F * (x_r1 - x_r2) + self.F * exploration_rate * (x_r3 - population[i])\n                \n                # Crossover\n                for j in range(self.dim):\n                    if np.random.rand() < self.Cr:\n                        new_population[i, j] = mutant[j]\n                    else:\n                        new_population[i, j] = population[i, j]\n                \n                new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                \n                # Local Search\n                if np.random.rand() < self.local_search_prob:\n                    new_x = new_population[i] + np.random.uniform(-self.local_search_radius, self.local_search_radius, size=self.dim)\n                    new_x = np.clip(new_x, func.bounds.lb, func.bounds.ub)\n                    new_population[i] = new_x\n            \n            # Evaluation\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n            \n            # Selection\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    population[i] = new_population[i]\n                    fitness[i] = new_fitness[i]\n                    \n                    if new_fitness[i] < self.f_opt:\n                        self.f_opt = new_fitness[i]\n                        self.x_opt = new_population[i]\n            \n            # Decay exploration rate\n            exploration_rate *= self.exploration_decay\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDELocalSearch scored 0.625 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["616e90ef-29bc-40ee-aa96-7b17f0c2d49e"], "operator": null, "metadata": {"aucs": [0.2026568246557484, 0.6709079698794277, 0.4919992723516441, 0.8973343623999125, 0.7638622118315028, 0.8119613454233989, 0.40860673521451685, 0.7087982146874672, 0.813370247173296, 0.22307957419883762, 0.8698219480191817, 0.9990062394133348, 0.35310123674030225, 0.6991727960562659, 0.8464981650995418, 0.5743287977324045, 0.6037794045385091, 0.8753006429467756, 0.18889437015082677, 0.49579743017957445]}}
{"id": "3e3488c4-96e5-4cb3-bba9-9ab7521ec4e5", "fitness": -Infinity, "name": "SobolNelderMead", "description": "Hybrid optimization algorithm combining a global exploration phase using a Sobol sequence with a local search phase using a Nelder-Mead simplex.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass SobolNelderMead:\n    def __init__(self, budget=10000, dim=10, num_sobol_points=100):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.num_sobol_points = num_sobol_points\n\n    def __call__(self, func):\n        # Generate Sobol sequence points\n        sobol_points = self.generate_sobol(self.num_sobol_points, self.dim)\n        sobol_points = self.lb + sobol_points * (self.ub - self.lb)\n\n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n\n        # Evaluate Sobol points\n        for i in range(self.num_sobol_points):\n            f = func(sobol_points[i])\n            eval_count += 1\n            if f < f_opt:\n                f_opt = f\n                x_opt = sobol_points[i].copy()\n\n            if eval_count >= self.budget:\n                return f_opt, x_opt\n\n\n        # Local search using Nelder-Mead from the best Sobol point\n        remaining_budget = self.budget - eval_count\n\n        if remaining_budget > 0:\n            result = minimize(func, x_opt, method='Nelder-Mead',\n                                options={'maxiter': remaining_budget, 'maxfev': remaining_budget})\n            \n            if result.fun < f_opt:\n                f_opt = result.fun\n                x_opt = result.x\n                \n        return f_opt, x_opt\n\n    def generate_sobol(self, n, dim):\n        \"\"\"Generate Sobol sequence points.\"\"\"\n        try:\n            import sobol_seq\n            points = sobol_seq.i4_sobol_generate(dim, n)\n            return points\n        except ImportError:\n            print(\"Sobol sequence library not found. Please install 'sobol_seq'. Returning random samples instead.\")\n            return np.random.rand(n, dim)", "configspace": "", "generation": 2, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 144, in evaluate\n    algorithm(f_new)\n  File \"<string>\", line 37, in __call__\nNameError: name 'minimize' is not defined\n.", "error": "", "parent_ids": ["d597ebc8-ab19-4213-82a6-59b48c9b9ce3"], "operator": null, "metadata": {}}
{"id": "0a1909f8-8c6e-448a-8c43-05849413fcfc", "fitness": -Infinity, "name": "CooperativeDE", "description": "Cooperative Differential Evolution with stochastic ranking and adaptive population sizing.", "code": "import numpy as np\n\nclass CooperativeDE:\n    def __init__(self, budget=10000, dim=10, pop_size_min=10, pop_size_max=80, F=0.5, Cr=0.9, good_fitness_share=0.25, stagnation_threshold=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_min = pop_size_min\n        self.pop_size_max = pop_size_max\n        self.pop_size = pop_size_max # Initial population size\n        self.F = F\n        self.Cr = Cr\n        self.good_fitness_share = good_fitness_share\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.inf\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n        \n        while self.budget > self.pop_size_min:\n            # Mutation and Crossover\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Mutation\n                indices = [j for j in range(self.pop_size) if j != i]\n                idxs = np.random.choice(indices, size=3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs[0]], population[idxs[1]], population[idxs[2]]\n                \n                mutant = population[i] + self.F * (x_r1 - x_r2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                for j in range(self.dim):\n                    if np.random.rand() < self.Cr:\n                        new_population[i, j] = mutant[j]\n                    else:\n                        new_population[i, j] = population[i, j]\n                \n                new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n            \n            # Evaluation\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n            \n            # Stochastic Ranking\n            combined_fitness = np.concatenate((fitness, new_fitness))\n            combined_population = np.vstack((population, new_population))\n            \n            ranked_indices = np.argsort(combined_fitness)\n            \n            # Adaptive Population Sizing\n            good_count = int(self.good_fitness_share * self.pop_size)\n            \n            population = combined_population[ranked_indices[:self.pop_size]]\n            fitness = combined_fitness[ranked_indices[:self.pop_size]]\n        \n            # Best solution update\n            if np.min(fitness) < self.f_opt:\n                self.f_opt = np.min(fitness)\n                self.x_opt = population[np.argmin(fitness)]\n                self.stagnation_counter = 0\n            else:\n                self.stagnation_counter += 1\n\n            # Adjust population size based on stagnation\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.pop_size = max(self.pop_size_min, int(self.pop_size * 0.8)) # Reduce population size\n                self.stagnation_counter = 0\n            elif self.pop_size < self.pop_size_max and np.random.rand() < 0.1:  # Increase population size occasionally\n                self.pop_size = min(self.pop_size_max, int(self.pop_size * 1.2))\n\n            # Re-initialize population if stagnation is too high and budget allows\n            if self.stagnation_counter > 3 * self.stagnation_threshold and self.budget > self.pop_size:\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                self.stagnation_counter = 0\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 144, in evaluate\n    algorithm(f_new)\n  File \"<string>\", line 33, in __call__\nIndexError: index 72 is out of bounds for axis 0 with size 64\n.", "error": "", "parent_ids": ["c8c94de2-a36a-48c7-9e66-1857ea3615f0"], "operator": null, "metadata": {}}
{"id": "951270d2-327e-4c04-98fe-338d09b1f241", "fitness": -Infinity, "name": "DODE", "description": "Differential Evolution with a dynamic population size, orthogonal learning, and a restart mechanism to escape local optima.", "code": "import numpy as np\n\nclass DODE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, min_pop_size=10, max_pop_size=100, restart_factor=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = initial_pop_size\n        self.min_pop_size = min_pop_size\n        self.max_pop_size = max_pop_size\n        self.restart_factor = restart_factor\n        self.pop_size = initial_pop_size\n        self.F = 0.5\n        self.Cr = 0.9\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n        \n        iteration = 0\n        no_improvement_count = 0\n        \n        while self.budget > self.min_pop_size:\n            iteration += 1\n            \n            # Mutation and Crossover\n            new_population = np.copy(population)\n            new_fitness = np.zeros(self.pop_size)\n            for i in range(self.pop_size):\n                # Mutation\n                indices = [j for j in range(self.pop_size) if j != i]\n                idxs = np.random.choice(indices, size=3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs[0]], population[idxs[1]], population[idxs[2]]\n                mutant = population[i] + self.F * (x_r1 - x_r2) + self.F * (x_r3 - population[i])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                for j in range(self.dim):\n                    if np.random.rand() < self.Cr:\n                        new_population[i, j] = mutant[j]\n                        \n                new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                new_fitness[i] = func(new_population[i])\n                self.budget -= 1\n                \n            # Selection\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    population[i] = new_population[i]\n                    fitness[i] = new_fitness[i]\n                    \n                    if new_fitness[i] < self.f_opt:\n                        self.f_opt = new_fitness[i]\n                        self.x_opt = new_population[i]\n                        no_improvement_count = 0  # Reset counter\n                else:\n                    no_improvement_count += 1\n            \n            # Dynamic Population Size Adjustment\n            if no_improvement_count > 50:\n                self.pop_size = max(self.min_pop_size, int(self.pop_size * 0.9)) # Reduce population size if no improvement\n            elif len(np.unique(fitness)) > self.pop_size * 0.8:\n                self.pop_size = min(self.max_pop_size, int(self.pop_size * 1.1)) # Increase population size if diversity is high\n                \n            self.pop_size = int(np.clip(self.pop_size, self.min_pop_size, self.max_pop_size))\n            \n            # Orthogonal Learning (OL) - Applied probabilistically\n            if np.random.rand() < 0.1:\n                best_index = np.argmin(fitness)\n                x_best = population[best_index].copy()\n                \n                # Generate orthogonal design points around the best solution\n                orthogonal_points = self.generate_orthogonal_design(x_best, func.bounds.lb, func.bounds.ub, num_points=5)\n                \n                for x_ol in orthogonal_points:\n                    f_ol = func(x_ol)\n                    self.budget -= 1\n                    if f_ol < self.f_opt:\n                        self.f_opt = f_ol\n                        self.x_opt = x_ol\n                        \n            # Restart mechanism\n            if no_improvement_count > 200:  # If still no improvement after many iterations, restart\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                self.f_opt = np.min(fitness)\n                self.x_opt = population[np.argmin(fitness)]\n                no_improvement_count = 0\n        return self.f_opt, self.x_opt\n\n    def generate_orthogonal_design(self, x_center, lb, ub, num_points=5):\n        design = []\n        for _ in range(num_points):\n            x = x_center + np.random.uniform(-0.1, 0.1, self.dim) * (ub - lb)  # Small perturbation\n            x = np.clip(x, lb, ub)\n            design.append(x)\n        return np.array(design)", "configspace": "", "generation": 2, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 144, in evaluate\n    algorithm(f_new)\n  File \"<string>\", line 37, in __call__\nIndexError: index 52 is out of bounds for axis 0 with size 50\n.", "error": "", "parent_ids": ["e230bfdd-67ce-4422-b332-c5968396f503"], "operator": null, "metadata": {}}
{"id": "0c927273-e49c-4617-a1b5-fca18bf8bd4a", "fitness": 0.23481954075160635, "name": "AgingDE", "description": "Differential Evolution with a novel mutation strategy that combines information from the best solution and a randomly selected individual, alongside an aging mechanism to promote exploration.", "code": "import numpy as np\n\nclass AgingDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, aging_rate=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.aging_rate = aging_rate\n        self.F = 0.5\n        self.Cr = 0.9\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        age = np.zeros(self.pop_size)  # Initialize age for each individual\n\n        while self.budget > self.pop_size:\n            # Mutation and Crossover\n            new_population = np.copy(population)\n            new_fitness = np.zeros(self.pop_size)\n\n            for i in range(self.pop_size):\n                # Mutation: Use best individual info + random individual\n                best_idx = np.argmin(fitness)\n                random_idx = np.random.randint(self.pop_size)\n                while random_idx == i:\n                    random_idx = np.random.randint(self.pop_size)\n                \n                indices = [j for j in range(self.pop_size) if j != i and j != best_idx and j!= random_idx]\n                if len(indices) < 1: \n                    mutant = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                else:\n                    idx_r1 = np.random.choice(indices, size=1, replace=False)[0]\n                    x_r1 = population[idx_r1]\n\n                    mutant = population[i] + self.F * (population[best_idx] - population[i]) + self.F * (x_r1 - population[random_idx]) # Novel Mutation\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n\n                # Crossover\n                for j in range(self.dim):\n                    if np.random.rand() < self.Cr:\n                        new_population[i, j] = mutant[j]\n\n                new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                new_fitness[i] = func(new_population[i])\n                self.budget -= 1\n\n            # Selection and Aging\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    population[i] = new_population[i]\n                    fitness[i] = new_fitness[i]\n                    age[i] = 0  # Reset age\n                    if new_fitness[i] < self.f_opt:\n                        self.f_opt = new_fitness[i]\n                        self.x_opt = new_population[i]\n                else:\n                    age[i] += 1 # Increment age\n\n\n                # Aging mechanism: replace old individuals\n                if age[i] > (self.budget/self.pop_size * self.aging_rate): #Age is relative to budget.\n                    population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                    fitness[i] = func(population[i])\n                    self.budget -= 1 # Account for new function evaluation\n                    age[i] = 0 # Reset age of the new individual\n                    if fitness[i] < self.f_opt:\n                        self.f_opt = fitness[i]\n                        self.x_opt = population[i]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AgingDE scored 0.235 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e230bfdd-67ce-4422-b332-c5968396f503"], "operator": null, "metadata": {"aucs": [0.14912849607556478, 0.26280575088827285, 0.3311905588143219, 0.3817329437492838, 0.28405949498219496, 0]}}
{"id": "a012adab-e6ab-4ae8-b201-2852c8f14f15", "fitness": 0.5447318493185979, "name": "AdaptivePSO", "description": "An adaptive step-size Particle Swarm Optimization (PSO) algorithm with velocity clamping and dynamic inertia weight adjustment based on particle success.", "code": "import numpy as np\n\nclass AdaptivePSO:\n    def __init__(self, budget=10000, dim=10, num_particles=20, inertia_max=0.9, inertia_min=0.2, c1=2.0, c2=2.0, v_max_ratio=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.num_particles = num_particles\n        self.inertia_max = inertia_max\n        self.inertia_min = inertia_min\n        self.c1 = c1\n        self.c2 = c2\n        self.lb = -5.0\n        self.ub = 5.0\n        self.v_max = v_max_ratio * (self.ub - self.lb)  # Velocity clamping\n        self.particles = np.random.uniform(self.lb, self.ub, size=(self.num_particles, self.dim))\n        self.velocities = np.random.uniform(-self.v_max, self.v_max, size=(self.num_particles, self.dim))  # Initialize velocities\n        self.personal_best_positions = self.particles.copy()\n        self.personal_best_values = np.full(self.num_particles, np.inf)\n        self.global_best_position = None\n        self.global_best_value = np.inf\n        self.eval_count = 0\n\n    def __call__(self, func):\n        while self.eval_count < self.budget:\n            # Evaluate particles\n            fitness_values = np.zeros(self.num_particles)\n            for i in range(self.num_particles):\n                if self.eval_count < self.budget:\n                    fitness_values[i] = func(self.particles[i])\n                    self.eval_count += 1\n                else:\n                    fitness_values[i] = np.inf  # Or a very large number\n\n            # Update personal bests\n            for i in range(self.num_particles):\n                if fitness_values[i] < self.personal_best_values[i]:\n                    self.personal_best_values[i] = fitness_values[i]\n                    self.personal_best_positions[i] = self.particles[i].copy()\n\n            # Update global best\n            best_index = np.argmin(fitness_values)\n            if fitness_values[best_index] < self.global_best_value:\n                self.global_best_value = fitness_values[best_index]\n                self.global_best_position = self.particles[best_index].copy()\n\n            # Update inertia weight (linearly decreasing)\n            inertia = self.inertia_max - (self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)\n\n            # Update velocities and positions\n            for i in range(self.num_particles):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n\n                self.velocities[i] = (inertia * self.velocities[i] +\n                                      self.c1 * r1 * (self.personal_best_positions[i] - self.particles[i]) +\n                                      self.c2 * r2 * (self.global_best_position - self.particles[i]))\n\n                # Velocity clamping\n                self.velocities[i] = np.clip(self.velocities[i], -self.v_max, self.v_max)\n                self.particles[i] = self.particles[i] + self.velocities[i]\n\n                # Boundary handling (clip or bounce)\n                self.particles[i] = np.clip(self.particles[i], self.lb, self.ub)\n\n        return self.global_best_value, self.global_best_position", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptivePSO scored 0.545 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d597ebc8-ab19-4213-82a6-59b48c9b9ce3"], "operator": null, "metadata": {"aucs": [0.17731140756181363, 0.6666453208907619, 0.690118973693409, 0.8648859408687826, 0.25850563348953703, 0.7546032142946673, 0.3110826624013683, 0.5559432460050622, 0.6760253030850668, 0.2001149674024223, 0.7717714891802493, 0.9966766643627819, 0.23931397560351675, 0.2911986301480397, 0.7335839326319303, 0.756438638678327, 0.5723366448200449, 0.38022633980456344, 0.3296970637492219, 0.6681569377003902]}}
{"id": "b6933206-e848-41ff-a985-e5a4e8d99d40", "fitness": 0.6946113409949316, "name": "PopulationCMAES", "description": "Population-based CMA-ES with adaptive covariance matrix, step size adaptation and restart mechanism.", "code": "import numpy as np\n\nclass PopulationCMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, sigma0=0.5, cs=0.3, c_cov=0.1, mu_ratio=0.25, restart_factor=2.0):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.sigma0 = sigma0\n        self.cs = cs\n        self.c_cov = c_cov\n        self.restart_factor = restart_factor\n        if pop_size is None:\n            self.pop_size = 4 + int(3 * np.log(self.dim))\n        else:\n            self.pop_size = pop_size\n        self.mu = int(self.pop_size * mu_ratio)\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.c_sigma = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.d_sigma = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.c_sigma\n        self.chiN = self.dim**0.5 * (1 - 1/(4*self.dim) + 1/(21*self.dim**2))\n\n    def __call__(self, func):\n        mean = np.random.uniform(self.lb, self.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)\n        p_sigma = np.zeros(self.dim)\n        p_c = np.zeros(self.dim)\n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n        restarts = 0\n\n        while eval_count < self.budget:\n            # Generate population\n            Z = np.random.multivariate_normal(np.zeros(self.dim), C, size=self.pop_size)\n            X = mean + sigma * Z\n            # Repair individuals outside the bounds\n            X = np.clip(X, self.lb, self.ub)\n            \n            F = np.array([func(x) for x in X])\n            eval_count += self.pop_size\n            if eval_count > self.budget:\n                eval_count = self.budget\n                F = F[:self.budget - (eval_count - self.pop_size)]\n                X = X[:self.budget - (eval_count - self.pop_size)]\n                \n            if np.min(F) < f_opt:\n                f_opt = np.min(F)\n                x_opt = X[np.argmin(F)].copy()\n\n            # Selection and recombination\n            idx = np.argsort(F)\n            X_mu = X[idx[:self.mu]]\n            Z_mu = Z[idx[:self.mu]]\n            mean_new = np.sum(self.weights[:, None] * X_mu, axis=0)\n            z_mean = np.sum(self.weights[:, None] * Z_mu, axis=0)\n\n            # Update evolution paths\n            p_sigma = (1 - self.c_sigma) * p_sigma + np.sqrt(self.c_sigma * (2 - self.c_sigma) * self.mueff) * z_mean\n            p_c = (1 - self.cs) * p_c + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * (mean_new - mean) / sigma\n\n            # Update covariance matrix\n            C = (1 - self.c_cov) * C + self.c_cov * (p_c[:, None] @ p_c[None, :]) + self.c_cov * (1 - self.c_cov) * self.c_sigma * (2 - self.c_sigma) * C\n\n            # Update step size\n            sigma *= np.exp((self.c_sigma / self.d_sigma) * (np.linalg.norm(p_sigma) / self.chiN - 1))\n            sigma = np.clip(sigma, 1e-6, 1)\n            \n            mean = mean_new\n\n            # Restart mechanism\n            if np.max(np.diag(C)) > 1e7 * sigma**2:\n                restarts += 1\n                mean = np.random.uniform(self.lb, self.ub, size=self.dim)\n                sigma = self.sigma0 * self.restart_factor**restarts\n                C = np.eye(self.dim)\n                p_sigma = np.zeros(self.dim)\n                p_c = np.zeros(self.dim)\n\n        return f_opt, x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm PopulationCMAES scored 0.695 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d597ebc8-ab19-4213-82a6-59b48c9b9ce3"], "operator": null, "metadata": {"aucs": [0.24123380248385629, 0.8419108809253799, 0.6894962781380995, 0.963245772773836, 0.6605489858189815, 0.8336462551380842, 0.38052024117745886, 0.6128349230359063, 0.7592702229900232, 0.5251155783605862, 0.9630173774464204, 0.9936460941959911, 0.7585119495944884, 0.748532770421024, 0.9615491071143804, 0.7271822265889794, 0.5531543888880928, 0.8871184376952483, 0.2815319918215189, 0.5101595352902772]}}
{"id": "fcb3768f-bdd4-4a6e-86fc-ddb91bfed549", "fitness": 0.6460614396624949, "name": "AdaptiveDERestart", "description": "An adaptive differential evolution strategy with a probabilistic restart mechanism and a dynamic population size adjustment based on stagnation detection.", "code": "import numpy as np\n\nclass AdaptiveDERestart:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, Cr=0.9, restart_prob=0.05, stagnation_threshold=100, pop_size_reduction_factor=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.Cr = Cr\n        self.restart_prob = restart_prob\n        self.stagnation_threshold = stagnation_threshold\n        self.pop_size_reduction_factor = pop_size_reduction_factor\n        self.best_fitness_history = []\n        self.last_improvement = 0\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n        self.best_fitness_history.append(self.f_opt)\n        \n        generation = 0\n\n        while self.budget > self.pop_size:\n            # Mutation and Crossover\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Mutation\n                indices = [j for j in range(self.pop_size) if j != i]\n                idxs = np.random.choice(indices, size=3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs[0]], population[idxs[1]], population[idxs[2]]\n                \n                mutant = population[i] + self.F * (x_r1 - x_r2) + self.F * (x_r3 - population[i])\n                \n                # Crossover\n                for j in range(self.dim):\n                    if np.random.rand() < self.Cr:\n                        new_population[i, j] = mutant[j]\n                    else:\n                        new_population[i, j] = population[i, j]\n                \n                new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n            \n            # Evaluation\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n            \n            # Selection\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    population[i] = new_population[i]\n                    fitness[i] = new_fitness[i]\n                    \n                    if new_fitness[i] < self.f_opt:\n                        self.f_opt = new_fitness[i]\n                        self.x_opt = new_population[i]\n                        self.last_improvement = generation\n                        \n            self.best_fitness_history.append(self.f_opt)\n            \n            # Stagnation check and restart\n            if (generation - self.last_improvement) > self.stagnation_threshold:\n                if np.random.rand() < self.restart_prob:\n                    # Restart: Re-initialize a portion of the population\n                    num_restart = int(self.pop_size * 0.2)\n                    restart_indices = np.random.choice(self.pop_size, size=num_restart, replace=False)\n                    population[restart_indices] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(num_restart, self.dim))\n                    fitness[restart_indices] = np.array([func(x) for x in population[restart_indices]])\n                    self.budget -= num_restart # Adjust budget for new evaluations\n\n                    current_best_idx = np.argmin(fitness)\n                    if fitness[current_best_idx] < self.f_opt:\n                        self.f_opt = fitness[current_best_idx]\n                        self.x_opt = population[current_best_idx]\n                    \n                    self.last_improvement = generation\n\n                #Dynamic Population Size Reduction:\n                else:\n                    self.pop_size = max(int(self.pop_size * self.pop_size_reduction_factor), 10) #Ensure minimum population of 10\n                    population = population[:self.pop_size]\n                    fitness = fitness[:self.pop_size]\n\n            generation += 1\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDERestart scored 0.646 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c8c94de2-a36a-48c7-9e66-1857ea3615f0"], "operator": null, "metadata": {"aucs": [0.35316180651096796, 0.44978262318218276, 0.40123084604334414, 0.8615215185170052, 0.8017103160084643, 0.8563552217177031, 0.3108249216840764, 0.7601999090855887, 0.8167586375195037, 0.25341273416539933, 0.9152889122601877, 0.9902807761036593, 0.7312017068223563, 0.7895513551064159, 0.8490039443522766, 0.676174562178455, 0.577279070453508, 0.8189466337413237, 0.20904851141872616, 0.49949478637875244]}}
{"id": "c3114246-0d30-429b-ad2a-3bf2d26efb24", "fitness": 0.529810452797936, "name": "DynamicPopulationDE", "description": "A differential evolution strategy with a dynamically adjusted population size and a restart mechanism based on stagnation detection.", "code": "import numpy as np\n\nclass DynamicPopulationDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, min_pop_size=10, max_pop_size=100, stagnation_tolerance=500):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.min_pop_size = min_pop_size\n        self.max_pop_size = max_pop_size\n        self.stagnation_tolerance = stagnation_tolerance\n        self.F = 0.5\n        self.Cr = 0.9\n        self.stagnation_counter = 0\n        self.last_improvement = 0\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n        \n        iteration = 0\n        \n        while self.budget > self.min_pop_size:\n            iteration += 1\n            \n            # Mutation and Crossover\n            new_population = np.copy(population)\n            new_fitness = np.zeros(self.pop_size)\n            for i in range(self.pop_size):\n                # Mutation\n                indices = [j for j in range(self.pop_size) if j != i]\n                idxs = np.random.choice(indices, size=3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs[0]], population[idxs[1]], population[idxs[2]]\n                \n                mutant = population[i] + self.F * (x_r1 - x_r2) + self.F * (x_r3 - population[i])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                for j in range(self.dim):\n                    if np.random.rand() < self.Cr:\n                        new_population[i, j] = mutant[j]\n\n                new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                new_fitness[i] = func(new_population[i])\n                self.budget -= 1\n                \n            # Selection\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    population[i] = new_population[i]\n                    fitness[i] = new_fitness[i]\n                    \n                    if new_fitness[i] < self.f_opt:\n                        self.f_opt = new_fitness[i]\n                        self.x_opt = new_population[i]\n                        self.stagnation_counter = 0 # Reset stagnation counter\n                        self.last_improvement = iteration\n                else:\n                    self.stagnation_counter +=1\n            \n            # Population size adaptation\n            if self.stagnation_counter > self.stagnation_tolerance:\n                # Reduce population size if stagnating\n                if self.pop_size > self.min_pop_size:\n                    self.pop_size = max(self.min_pop_size, int(self.pop_size * 0.8))\n                    population = population[np.argsort(fitness)[:self.pop_size]]\n                    fitness = fitness[np.argsort(fitness)[:self.pop_size]]\n                    print(f\"Reducing population size to {self.pop_size} at iteration {iteration}\")\n                    self.stagnation_counter = 0  # Reset counter after reduction\n                    # Potentially restart with new individuals\n                    new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                    new_fitnesses = np.array([func(x) for x in new_individuals])\n                    self.budget -= self.pop_size\n                    \n                    if np.min(new_fitnesses) < self.f_opt:\n                        self.f_opt = np.min(new_fitnesses)\n                        self.x_opt = new_individuals[np.argmin(new_fitnesses)]\n                        \n                    population = new_individuals\n                    fitness = new_fitnesses\n\n                else:\n                    # Restart if at minimum population size\n                    print(\"Restarting population at iteration\", iteration)\n                    population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                    fitness = np.array([func(x) for x in population])\n                    self.budget -= self.pop_size\n                    self.f_opt = np.min(fitness)\n                    self.x_opt = population[np.argmin(fitness)]\n                    self.stagnation_counter = 0  # Reset counter after restart\n                    self.last_improvement = iteration\n\n\n            elif iteration - self.last_improvement > 2 * self.stagnation_tolerance and self.pop_size < self.max_pop_size:\n                # Increase population size if improvement is rare\n                 if self.pop_size < self.max_pop_size:\n                    increase_amount = min(int(self.pop_size * 0.2), self.max_pop_size - self.pop_size)\n                    if increase_amount > 0:\n                        self.pop_size += increase_amount\n                        new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(increase_amount, self.dim))\n                        new_fitnesses = np.array([func(x) for x in new_individuals])\n                        self.budget -= increase_amount\n                        population = np.vstack((population, new_individuals))\n                        fitness = np.concatenate((fitness, new_fitnesses))\n                        \n                        if np.min(new_fitnesses) < self.f_opt:\n                            self.f_opt = np.min(new_fitnesses)\n                            self.x_opt = new_individuals[np.argmin(new_fitnesses)]\n                        print(f\"Increasing population size to {self.pop_size} at iteration {iteration}\")\n                        \n                        self.last_improvement = iteration\n                        self.stagnation_counter = 0\n                        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm DynamicPopulationDE scored 0.530 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e230bfdd-67ce-4422-b332-c5968396f503"], "operator": null, "metadata": {"aucs": [0.21307652853052783, 0.6753175255309836, 0.5065130976585377, 0.8704711068543536, 0.7679169287873867, 0.7950126495646406, 0.34258314913362264, 0.6602192647003491, 0.8044227180045606, 0.19238201201233474, 0]}}
{"id": "9e5761ab-1d53-40a3-b4fa-4b86592367a4", "fitness": 0.626598951607683, "name": "CooperativeSelfAdaptiveDEArchive", "description": "Cooperative Self-Adaptive Differential Evolution with Archive, using an external archive to store promising solutions and cooperative strategies to enhance exploration and exploitation.", "code": "import numpy as np\n\nclass CooperativeSelfAdaptiveDEArchive:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, F_init=0.5, CR_init=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F_init = F_init\n        self.CR_init = CR_init\n        self.population = None\n        self.fitness = None\n        self.F = None\n        self.CR = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.archive = []  # Archive to store promising solutions\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        self.F = np.full(self.pop_size, self.F_init)\n        self.CR = np.full(self.pop_size, self.CR_init)\n        \n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index].copy()\n\n    def evolve(self, func):\n        successful_F = []\n        successful_CR = []\n\n        for i in range(self.pop_size):\n            # Parameter adaptation\n            self.F[i] = np.clip(np.random.normal(self.F[i], 0.1), 0.1, 1.0)\n            self.CR[i] = np.clip(np.random.normal(self.CR[i], 0.1), 0.1, 1.0)\n            \n            # Mutation - Cooperative strategy: use archive if available\n            if len(self.archive) > 0 and np.random.rand() < 0.2:  # 20% chance to use archive\n                x_r1 = self.archive[np.random.randint(len(self.archive))]\n                idxs = np.random.choice(self.pop_size, 2, replace=False)\n                x_r2, x_r3 = self.population[idxs]\n            else:\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[idxs]\n            \n            x_mutated = x_r1 + self.F[i] * (x_r2 - x_r3)\n            x_mutated = np.clip(x_mutated, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            x_trial = self.population[i].copy()\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.CR[i] or j == j_rand:\n                    x_trial[j] = x_mutated[j]\n\n            # Selection\n            f_trial = func(x_trial)\n            self.eval_count += 1\n\n            if f_trial < self.fitness[i]:\n                successful_F.append(self.F[i])\n                successful_CR.append(self.CR[i])\n                \n                # Update archive\n                if len(self.archive) < self.archive_size:\n                    self.archive.append(x_trial)\n                else:\n                    # Replace a random element in the archive\n                    self.archive[np.random.randint(self.archive_size)] = x_trial\n                    \n                self.population[i] = x_trial\n                self.fitness[i] = f_trial\n                \n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = x_trial.copy()\n\n            if self.eval_count >= self.budget:\n                break\n        \n        # Update F and CR based on successful values from the generation\n        if successful_F:\n            self.F = np.full(self.pop_size, np.mean(successful_F))\n        if successful_CR:\n            self.CR = np.full(self.pop_size, np.mean(successful_CR))\n                \n    def __call__(self, func):\n        self.initialize_population(func)\n        while self.eval_count < self.budget:\n            self.evolve(func)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm CooperativeSelfAdaptiveDEArchive scored 0.627 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["57d948e2-0b4f-483f-a55c-d7c8a0af2618"], "operator": null, "metadata": {"aucs": [0.32536567651381465, 0.5281415139730337, 0.47943299953650453, 0.5841164769635676, 0.8055680434798335, 0.8149668635967579, 0.6052870232836052, 0.42191098708956065, 0.8086637860485218, 0.750463682538926, 0.88439498976368, 0.9904618628815388, 0.2664381337155426, 0.6409482312282021, 0.7480540529873763, 0.813105254146772, 0.42446546493709625, 0.8637752759863869, 0.2468494687260332, 0.5295692447569069]}}
{"id": "df7f77e7-ac62-4e2c-9d9c-11eec84d14de", "fitness": 0.5885046498780679, "name": "AdaptiveDEStrategyPool", "description": "A differential evolution strategy with a pool of mutation strategies and adaptive selection among them, along with a self-adjusting crossover rate based on the success rate of each individual.", "code": "import numpy as np\n\nclass AdaptiveDEStrategyPool:\n    def __init__(self, budget=10000, dim=10, pop_size=50, strategy_pool_size=4, CR_init=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.strategy_pool_size = strategy_pool_size\n        self.CR_init = CR_init\n        self.population = None\n        self.fitness = None\n        self.CR = None\n        self.strategies = [self.mutation_strategy_1, self.mutation_strategy_2, self.mutation_strategy_3, self.mutation_strategy_4]  # Pool of mutation strategies\n        self.strategy_selection_probs = np.ones(self.strategy_pool_size) / self.strategy_pool_size\n        self.success_counts = np.zeros(self.strategy_pool_size)\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        self.CR = np.full(self.pop_size, self.CR_init)\n        \n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index].copy()\n\n    def mutation_strategy_1(self, x, population): # DE/rand/1\n        idxs = np.random.choice(self.pop_size, 3, replace=False)\n        x_r1, x_r2, x_r3 = population[idxs]\n        return x_r1 + 0.8 * (x_r2 - x_r3)\n\n    def mutation_strategy_2(self, x, population): # DE/current-to-rand/1\n         idxs = np.random.choice(self.pop_size, 2, replace=False)\n         x_r1, x_r2 = population[idxs]\n         return x + 0.5 * (x_r1 - x_r2)\n    \n    def mutation_strategy_3(self, x, population): # DE/best/1\n        best_index = np.argmin(self.fitness)\n        x_best = population[best_index]\n        idxs = np.random.choice(self.pop_size, 2, replace=False)\n        x_r1, x_r2 = population[idxs]\n        return x_best + 0.8 * (x_r1 - x_r2)\n    \n    def mutation_strategy_4(self, x, population): # DE/current-to-best/1\n        best_index = np.argmin(self.fitness)\n        x_best = population[best_index]\n        idxs = np.random.choice(self.pop_size, 1, replace=False)\n        x_r1 = population[idxs[0]]\n        return x + 0.5 * (x_best - x) + 0.5 * (x_r1 - x)\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            # Strategy selection\n            strategy_index = np.random.choice(self.strategy_pool_size, p=self.strategy_selection_probs)\n            mutation_strategy = self.strategies[strategy_index]\n\n            # Mutation\n            x_mutated = mutation_strategy(self.population[i], self.population)\n            x_mutated = np.clip(x_mutated, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            x_trial = self.population[i].copy()\n            j_rand = np.random.randint(self.dim)\n            CR = np.clip(np.random.normal(self.CR[i], 0.1), 0.1, 1.0)  # Self-adjusting CR\n            for j in range(self.dim):\n                if np.random.rand() < CR or j == j_rand:\n                    x_trial[j] = x_mutated[j]\n\n            # Selection\n            f_trial = func(x_trial)\n            self.eval_count += 1\n\n            if f_trial < self.fitness[i]:\n                self.success_counts[strategy_index] += 1\n                self.CR[i] = CR  # Update CR of individual\n\n                self.population[i] = x_trial\n                self.fitness[i] = f_trial\n                \n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = x_trial.copy()\n            \n            if self.eval_count >= self.budget:\n                break\n\n        # Update strategy selection probabilities\n        self.strategy_selection_probs = (1 - 0.1) * self.strategy_selection_probs + 0.1 * (self.success_counts / np.sum(self.success_counts) if np.sum(self.success_counts) > 0 else np.ones(self.strategy_pool_size) / self.strategy_pool_size)\n        self.strategy_selection_probs /= np.sum(self.strategy_selection_probs) # Normalize\n        self.success_counts = np.zeros(self.strategy_pool_size)  # Reset success counts\n                \n    def __call__(self, func):\n        self.initialize_population(func)\n        while self.eval_count < self.budget:\n            self.evolve(func)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDEStrategyPool scored 0.589 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["57d948e2-0b4f-483f-a55c-d7c8a0af2618"], "operator": null, "metadata": {"aucs": [0.14363548859740727, 0.29014550650632953, 0.5202849258226308, 0.9123763522035692, 0.764070218057576, 0.7163165929394772, 0.36803756029580914, 0.5665683927475373, 0.5939244207376635, 0.406046961295068, 0.8551838489955353, 0.9885680253696582, 0.3756191975142832, 0.6936029783576529, 0.8109280278504639, 0.8293498331761062, 0.39642867464578013, 0.7971562401562957, 0.2266613953444877, 0.5151883569480282]}}
{"id": "37fec3fe-592a-4f76-b75e-9f26013d4310", "fitness": 0.54759692813682, "name": "DistanceAdaptiveDE", "description": "An adaptive differential evolution strategy with a distance-based mutation factor and orthogonal crossover.", "code": "import numpy as np\n\nclass DistanceAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_init=0.5, CR_init=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_init = F_init  # Initial mutation factor\n        self.CR_init = CR_init  # Initial crossover rate\n        self.population = None\n        self.fitness = None\n        self.F = None\n        self.CR = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        self.F = np.full(self.pop_size, self.F_init)\n        self.CR = np.full(self.pop_size, self.CR_init)\n        \n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index].copy()\n\n    def distance_based_mutation_factor(self, x, population):\n        distances = np.linalg.norm(population - x, axis=1)\n        distances = distances / np.sum(distances)  # Normalize to create a probability distribution\n        #Weight F by the inverse distance from current solution to rest of population\n        return np.clip(np.sum(distances * self.F), 0.1, 1.0)\n\n    def orthogonal_crossover(self, x_mutated, x_target):\n        # Generate orthogonal matrix (e.g., using Hadamard matrix if dim is a power of 2)\n        if (self.dim & (self.dim - 1) == 0) and self.dim > 1:  # Check if dim is a power of 2\n            H = self.hadamard(self.dim)\n            # Select one row of Hadamard matrix for crossover\n            idx = np.random.randint(self.dim)\n            crossover_pattern = (H[idx] + 1) / 2  # Convert -1/1 to 0/1\n        else:\n            crossover_pattern = np.random.choice([0, 1], size=self.dim)\n        \n        x_trial = x_target.copy()\n        for j in range(self.dim):\n            if crossover_pattern[j] == 1:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def hadamard(self, n):\n        if n == 1:\n            return np.array([[1]])\n        H = self.hadamard(n // 2)\n        return np.vstack((\n            np.hstack((H, H)),\n            np.hstack((H, -H))\n        ))\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            # Adaptive F based on distance to other solutions\n            self.F[i] = self.distance_based_mutation_factor(self.population[i], self.population)\n            self.CR[i] = np.clip(np.random.normal(self.CR[i], 0.1), 0.1, 1.0)\n\n            # Mutation\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[idxs]\n            x_mutated = x_r1 + self.F[i] * (x_r2 - x_r3)\n            x_mutated = np.clip(x_mutated, func.bounds.lb, func.bounds.ub)\n\n            # Orthogonal Crossover\n            x_trial = self.orthogonal_crossover(x_mutated, self.population[i])\n            x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n\n            # Selection\n            f_trial = func(x_trial)\n            self.eval_count += 1\n\n            if f_trial < self.fitness[i]:\n                self.population[i] = x_trial\n                self.fitness[i] = f_trial\n                \n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = x_trial.copy()\n\n            if self.eval_count >= self.budget:\n                break\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        while self.eval_count < self.budget:\n            self.evolve(func)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm DistanceAdaptiveDE scored 0.548 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["57d948e2-0b4f-483f-a55c-d7c8a0af2618"], "operator": null, "metadata": {"aucs": [0.19191840473362598, 0.2654750541907457, 0.5274019393879517, 0.7188474999520507, 0.6143342285200959, 0.7198906623818375, 0.44577203309178937, 0.48717364113088546, 0.5818857909634221, 0.43098469519332105, 0.6736477762894673, 0.9915206940960817, 0.272752376196991, 0.580117589638294, 0.7875240831835638, 0.7234522725401367, 0.43725730098427207, 0.7736894549889769, 0.218160974264475, 0.510132091008417]}}
{"id": "ca04d731-4675-48ae-a67b-f03cacddba25", "fitness": 0.3932081842541554, "name": "BlendedAdaptiveDE", "description": "An adaptive differential evolution strategy using a blended crossover operator and a diversity maintenance mechanism based on crowding distance.", "code": "import numpy as np\n\nclass BlendedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, archive_size=10, crowding_threshold=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.crowding_threshold = crowding_threshold\n        self.F = 0.5\n        self.Cr = 0.9\n        self.F_adapt_rate = 0.1\n        self.Cr_adapt_rate = 0.1\n\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        archive = []\n        \n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n        \n        iteration = 0\n        \n        while self.budget > self.pop_size:\n            iteration += 1\n            \n            # Mutation and Crossover\n            new_population = np.copy(population)\n            new_fitness = np.zeros(self.pop_size)\n            for i in range(self.pop_size):\n                # Mutation\n                indices = [j for j in range(self.pop_size) if j != i]\n                idxs = np.random.choice(indices, size=3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs[0]], population[idxs[1]], population[idxs[2]]\n\n                mutant = population[i] + self.F * (x_r1 - x_r2) + self.F * (x_r3 - population[i])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                \n                # Blended Crossover\n                for j in range(self.dim):\n                    if np.random.rand() < self.Cr:\n                        alpha = np.random.rand()\n                        new_population[i, j] = alpha * mutant[j] + (1 - alpha) * population[i, j]\n                    else:\n                        new_population[i, j] = population[i, j]\n\n                new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                new_fitness[i] = func(new_population[i])\n                self.budget -= 1\n                \n            # Selection and Adaptation\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    # Update archive\n                    if len(archive) < self.archive_size:\n                        archive.append(population[i].copy())\n                    else:\n                        idx_to_replace = np.random.randint(self.archive_size)\n                        archive[idx_to_replace] = population[i].copy()\n                    \n                    population[i] = new_population[i]\n                    fitness[i] = new_fitness[i]\n                    \n                    if new_fitness[i] < self.f_opt:\n                        self.f_opt = new_fitness[i]\n                        self.x_opt = new_population[i]\n                        \n                    # Adaptive F and Cr\n                    self.F = self.F * (1 - self.F_adapt_rate) + np.random.rand() * self.F_adapt_rate\n                    self.Cr = self.Cr * (1 - self.Cr_adapt_rate) + np.random.rand() * self.Cr_adapt_rate\n\n            # Diversity Maintenance (Crowding Distance)\n            distances = np.zeros(self.pop_size)\n            for i in range(self.pop_size):\n                for j in range(self.pop_size):\n                    if i != j:\n                        distances[i] += np.linalg.norm(population[i] - population[j])\n\n            # Remove individuals with low crowding distance if necessary\n            if np.std(fitness) < self.crowding_threshold:\n                worst_index = np.argmin(distances)\n                population[worst_index] = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                fitness[worst_index] = func(population[worst_index])\n                self.budget -= 1\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm BlendedAdaptiveDE scored 0.393 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e230bfdd-67ce-4422-b332-c5968396f503"], "operator": null, "metadata": {"aucs": [0.16663066892043132, 0.2661142972552891, 0.40810829461261744, 0.24638622947361677, 0.2593218985163016, 0.5402973375562466, 0.2785917378574112, 0.3951344138919949, 0.44841888406569064, 0.16951456850111624, 0.2874786528347484, 0.9992308746040232, 0.32884701838575114, 0.27163425870247415, 0.6963212614631328, 0.34815919086261204, 0.33487962877278277, 0.7281066652936955, 0.1961808525025277, 0.494806951010644]}}
{"id": "216d5736-d9f1-4ba1-9852-7b0c2c3b5b7b", "fitness": -Infinity, "name": "OrthogonalCMAES", "description": "CMA-ES with orthogonal sampling to reduce correlations in the population and adaptive step size based on success rate.", "code": "import numpy as np\n\nclass OrthogonalCMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, sigma0=0.5, cs=0.3, c_cov=0.1, mu_ratio=0.25, restart_factor=2.0, success_history=10):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.sigma0 = sigma0\n        self.cs = cs\n        self.c_cov = c_cov\n        self.restart_factor = restart_factor\n        if pop_size is None:\n            self.pop_size = 4 + int(3 * np.log(self.dim))\n        else:\n            self.pop_size = pop_size\n        self.mu = int(self.pop_size * mu_ratio)\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.c_sigma = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.d_sigma = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.c_sigma\n        self.chiN = self.dim**0.5 * (1 - 1/(4*self.dim) + 1/(21*self.dim**2))\n        self.success_history = success_history\n        self.success_rate = 0.5\n        self.successes = []\n\n\n    def __call__(self, func):\n        mean = np.random.uniform(self.lb, self.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)\n        p_sigma = np.zeros(self.dim)\n        p_c = np.zeros(self.dim)\n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n        restarts = 0\n\n        while eval_count < self.budget:\n            # Generate orthogonal matrix\n            H = np.random.normal(size=(self.pop_size, self.dim))\n            Q, R = np.linalg.qr(H)\n\n            # Generate population\n            Z = Q @ np.random.multivariate_normal(np.zeros(self.dim), C, size=self.dim).T\n            Z = Z.T # Back to pop_size x dim\n            X = mean + sigma * Z\n            # Repair individuals outside the bounds\n            X = np.clip(X, self.lb, self.ub)\n            \n            F = np.array([func(x) for x in X])\n            eval_count += self.pop_size\n            if eval_count > self.budget:\n                eval_count = self.budget\n                F = F[:self.budget - (eval_count - self.pop_size)]\n                X = X[:self.budget - (eval_count - self.pop_size)]\n                \n            if np.min(F) < f_opt:\n                f_opt = np.min(F)\n                x_opt = X[np.argmin(F)].copy()\n\n            # Selection and recombination\n            idx = np.argsort(F)\n            X_mu = X[idx[:self.mu]]\n            Z_mu = Z[idx[:self.mu]]\n            mean_new = np.sum(self.weights[:, None] * X_mu, axis=0)\n            z_mean = np.sum(self.weights[:, None] * Z_mu, axis=0)\n\n            # Update evolution paths\n            p_sigma = (1 - self.c_sigma) * p_sigma + np.sqrt(self.c_sigma * (2 - self.c_sigma) * self.mueff) * z_mean\n            p_c = (1 - self.cs) * p_c + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * (mean_new - mean) / sigma\n\n            # Update covariance matrix\n            C = (1 - self.c_cov) * C + self.c_cov * (p_c[:, None] @ p_c[None, :]) + self.c_cov * (1 - self.c_cov) * self.c_sigma * (2 - self.c_sigma) * C\n\n            # Adapt step size based on success rate\n            success = np.mean(F < np.median(F))\n            self.successes.append(success)\n            if len(self.successes) > self.success_history:\n                self.successes.pop(0)\n            self.success_rate = np.mean(self.successes)\n\n            if self.success_rate > 0.6:\n                sigma *= 1.1\n            elif self.success_rate < 0.4:\n                sigma *= 0.9\n            \n            sigma = np.clip(sigma, 1e-6, 1)\n            \n            mean = mean_new\n\n            # Restart mechanism\n            if np.max(np.diag(C)) > 1e7 * sigma**2:\n                restarts += 1\n                mean = np.random.uniform(self.lb, self.ub, size=self.dim)\n                sigma = self.sigma0 * self.restart_factor**restarts\n                C = np.eye(self.dim)\n                p_sigma = np.zeros(self.dim)\n                p_c = np.zeros(self.dim)\n\n        return f_opt, x_opt", "configspace": "", "generation": 3, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 48, in __call__\nValueError: operands could not be broadcast together with shapes (2,) (2,6) \n.", "error": "", "parent_ids": ["b6933206-e848-41ff-a985-e5a4e8d99d40"], "operator": null, "metadata": {}}
{"id": "c077592a-84a0-412a-ba34-07465d9c4e89", "fitness": 0.4890281797212243, "name": "AdaptiveDEAggressiveReduction", "description": "An adaptive DE algorithm with a simplified adaptive mutation strategy and a more aggressive population size reduction upon stagnation.", "code": "import numpy as np\n\nclass AdaptiveDEAggressiveReduction:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_initial=0.5, Cr=0.9, stagnation_threshold=50, pop_size_reduction_factor=0.7, min_pop_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_initial = F_initial\n        self.Cr = Cr\n        self.stagnation_threshold = stagnation_threshold\n        self.pop_size_reduction_factor = pop_size_reduction_factor\n        self.min_pop_size = min_pop_size\n        self.best_fitness_history = []\n        self.last_improvement = 0\n        self.F = self.F_initial  # Initialize F\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n        self.best_fitness_history.append(self.f_opt)\n        \n        generation = 0\n\n        while self.budget > self.min_pop_size:  # Ensure budget is sufficient for min_pop_size evaluations\n            # Mutation and Crossover\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Simplified Mutation: Adapt F based on recent success. No separate F selection.\n                if generation > 0 and len(self.best_fitness_history) > 1:\n                    if self.best_fitness_history[-1] < self.best_fitness_history[-2]:  # Improvement\n                        self.F = max(0.1, self.F * 0.95)  # Reduce F if improving\n                    else:\n                        self.F = min(0.9, self.F * 1.05) #Increase F if stagnating\n\n                indices = [j for j in range(self.pop_size) if j != i]\n                idxs = np.random.choice(indices, size=3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs[0]], population[idxs[1]], population[idxs[2]]\n                \n                mutant = population[i] + self.F * (x_r1 - x_r2) + self.F * (x_r3 - population[i])\n                \n                # Crossover\n                for j in range(self.dim):\n                    if np.random.rand() < self.Cr:\n                        new_population[i, j] = mutant[j]\n                \n                new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n            \n            # Evaluation\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n            \n            # Selection\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    population[i] = new_population[i]\n                    fitness[i] = new_fitness[i]\n                    \n                    if new_fitness[i] < self.f_opt:\n                        self.f_opt = new_fitness[i]\n                        self.x_opt = new_population[i]\n                        self.last_improvement = generation\n                        \n            self.best_fitness_history.append(self.f_opt)\n            \n            # Stagnation check and aggressive population size reduction\n            if (generation - self.last_improvement) > self.stagnation_threshold:\n                #Aggressive Population Size Reduction:\n                self.pop_size = max(int(self.pop_size * self.pop_size_reduction_factor), self.min_pop_size)\n                population = population[:self.pop_size]\n                fitness = fitness[:self.pop_size]\n                #print(f\"Population reduced to {self.pop_size}\")\n                self.last_improvement = generation # Reset last improvement to avoid repeated reduction\n\n            generation += 1\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDEAggressiveReduction scored 0.489 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["fcb3768f-bdd4-4a6e-86fc-ddb91bfed549"], "operator": null, "metadata": {"aucs": [0.18702001714081418, 0.5069212211649774, 0.5553199300687764, 0.8193176702648131, 0.6125420676505295, 0.7420763517586593, 0]}}
{"id": "87b0c31f-e2a2-4a7a-bc89-197adc7ebcc8", "fitness": 0.2538080162675635, "name": "SelfOrganizingScoutBees", "description": "A self-organizing scout bee algorithm with dynamic search radius and adaptive step sizes, balancing exploration and exploitation by varying scout bee numbers based on fitness landscape assessment.", "code": "import numpy as np\n\nclass SelfOrganizingScoutBees:\n    def __init__(self, budget=10000, dim=10, num_bees=50, scout_ratio=0.1, initial_radius=1.0, radius_decay=0.95, step_size=0.1, step_size_decay=0.98, stagnation_threshold=50):\n        self.budget = budget\n        self.dim = dim\n        self.num_bees = num_bees\n        self.scout_ratio = scout_ratio\n        self.initial_radius = initial_radius\n        self.radius_decay = radius_decay\n        self.step_size = step_size\n        self.step_size_decay = step_size_decay\n        self.stagnation_threshold = stagnation_threshold\n        self.best_fitness_history = []\n        self.last_improvement = 0\n        self.radius = initial_radius\n        self.num_scouts = int(self.num_bees * self.scout_ratio)\n\n    def __call__(self, func):\n        # Initialization\n        bees = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.num_bees, self.dim))\n        fitness = np.array([func(x) for x in bees])\n        self.budget -= self.num_bees\n        \n        self.f_opt = np.min(fitness)\n        self.x_opt = bees[np.argmin(fitness)]\n        self.best_fitness_history.append(self.f_opt)\n        \n        generation = 0\n\n        while self.budget > self.num_bees:\n            # Employed Bees Phase\n            for i in range(self.num_bees):\n                neighbor_index = np.random.choice([j for j in range(self.num_bees) if j != i])\n                \n                new_bee = bees[i] + self.step_size * (bees[i] - bees[neighbor_index])\n                new_bee = np.clip(new_bee, func.bounds.lb, func.bounds.ub)\n                \n                new_fitness = func(new_bee)\n                self.budget -= 1\n                \n                if new_fitness < fitness[i]:\n                    bees[i] = new_bee\n                    fitness[i] = new_fitness\n                    \n                    if new_fitness < self.f_opt:\n                        self.f_opt = new_fitness\n                        self.x_opt = new_bee\n                        self.last_improvement = generation\n\n            # Scout Bees Phase\n            worst_indices = np.argsort(fitness)[-self.num_scouts:]  # Replace worst bees with scouts\n\n            for i in worst_indices:\n                # Conduct a broader search within a radius\n                new_bee = np.random.uniform(\n                    np.maximum(func.bounds.lb, self.x_opt - self.radius),\n                    np.minimum(func.bounds.ub, self.x_opt + self.radius),\n                    size=self.dim\n                )\n                new_fitness = func(new_bee)\n                self.budget -= 1\n                \n                if new_fitness < fitness[i]:\n                    bees[i] = new_bee\n                    fitness[i] = new_fitness\n                    \n                    if new_fitness < self.f_opt:\n                        self.f_opt = new_fitness\n                        self.x_opt = new_bee\n                        self.last_improvement = generation\n            \n            self.best_fitness_history.append(self.f_opt)\n            \n            # Stagnation Check and Adaptation\n            if (generation - self.last_improvement) > self.stagnation_threshold:\n                # Reduce search radius and step size if stagnating\n                self.radius *= self.radius_decay\n                self.step_size *= self.step_size_decay\n\n                # Increase scouts to promote exploration if stagnating\n                self.num_scouts = min(int(self.num_bees * 0.5), self.num_scouts + 1)  #Increase scout ratio gradually\n                \n                #Reset the worst scout bee indexes to the updated number of scout bees.\n                worst_indices = np.argsort(fitness)[-self.num_scouts:] #Recalculate the worst bees with updated num_scouts\n\n                #Explore around the best bee.\n                for i in worst_indices:\n                    new_bee = np.random.uniform(\n                        np.maximum(func.bounds.lb, self.x_opt - self.radius),\n                        np.minimum(func.bounds.ub, self.x_opt + self.radius),\n                        size=self.dim\n                    )\n                    new_fitness = func(new_bee)\n                    self.budget -= 1\n                    \n                    if new_fitness < fitness[i]:\n                        bees[i] = new_bee\n                        fitness[i] = new_fitness\n                        \n                        if new_fitness < self.f_opt:\n                            self.f_opt = new_fitness\n                            self.x_opt = new_bee\n                            self.last_improvement = generation\n            else:\n                # Decrease scout ratio for exploitation\n                 self.num_scouts = max(int(self.num_bees * 0.05), int(self.num_scouts * 0.95))\n\n            generation += 1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm SelfOrganizingScoutBees scored 0.254 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["fcb3768f-bdd4-4a6e-86fc-ddb91bfed549"], "operator": null, "metadata": {"aucs": [0.17050602524309455, 0.2173969103559965, 0.3744431757237968, 0.34377717195960544, 0.26409269551929315, 0.3206725726961861, 0.25200176441193956, 0.3363299953639569, 0.25885985140176604, 0]}}
{"id": "da83c584-07ed-4920-8140-2191a7bd60eb", "fitness": 0.12532819243926469, "name": "AdaptiveNeighborhoodSearch", "description": "Neighborhood Search with Adaptive Step Size, dynamically adjusting the step size based on success and failure in finding better solutions in the neighborhood.", "code": "import numpy as np\n\nclass AdaptiveNeighborhoodSearch:\n    def __init__(self, budget=10000, dim=10, initial_step_size=0.1, reduction_factor=0.9, expansion_factor=1.1):\n        self.budget = budget\n        self.dim = dim\n        self.step_size = initial_step_size\n        self.reduction_factor = reduction_factor\n        self.expansion_factor = expansion_factor\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize with a random solution\n        x = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        f = func(x)\n        self.eval_count += 1\n        self.f_opt = f\n        self.x_opt = x.copy()\n        \n        while self.eval_count < self.budget:\n            # Generate a neighbor by adding a random displacement\n            x_new = x + np.random.normal(0, self.step_size, size=self.dim)\n            x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n            \n            f_new = func(x_new)\n            self.eval_count += 1\n            \n            if f_new < self.f_opt:\n                # Accept the new solution\n                self.f_opt = f_new\n                self.x_opt = x_new.copy()\n                x = x_new.copy()\n                # Increase the step size (expansion)\n                self.step_size *= self.expansion_factor\n            else:\n                # Reduce the step size (contraction)\n                self.step_size *= self.reduction_factor\n            \n            # Limit the step size\n            self.step_size = np.clip(self.step_size, 1e-6, 1.0)  # Ensure step size stays within reasonable bounds\n\n            if self.eval_count >= self.budget:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveNeighborhoodSearch scored 0.125 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9e5761ab-1d53-40a3-b4fa-4b86592367a4"], "operator": null, "metadata": {"aucs": [9.999999999998899e-05, 0.07468149341186525, 0.15250352339867734, 0.09897983672531374, 0.04314780593764156, 0.0931762278292434, 0.17541607155431715, 0.15250388498506784, 0.16353386548539672, 0.07648335764713043, 0.12142776183357817, 0.1623175206345726, 0.22174083859733063, 0.0889589531496987, 0.17016611801636405, 0.1995330773429489, 0.0904872612896841, 0.143960053654287, 0.13557992796889717, 0.14186626932327906]}}
{"id": "304dd10a-5780-4400-bdcf-2b56637c2af3", "fitness": 0.08280842813102646, "name": "SelfOrganizingPSO", "description": "A self-organizing particle swarm optimization with velocity clamping and adaptive inertia weight, incorporating a local search strategy for improved exploitation and diversity.", "code": "import numpy as np\n\nclass SelfOrganizingPSO:\n    def __init__(self, budget=10000, dim=10, pop_size=50, w_init=0.9, c1=2.0, c2=2.0, v_max_ratio=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w_init = w_init\n        self.w_min = 0.4  # Minimum inertia weight\n        self.c1 = c1\n        self.c2 = c2\n        self.v_max = None  # Calculated dynamically\n        self.population = None\n        self.velocities = None\n        self.fitness = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = None\n        self.global_best_position = None\n        self.global_best_fitness = np.Inf\n        self.eval_count = 0\n        self.v_max_ratio = v_max_ratio\n\n    def initialize_population(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        self.velocities = np.random.uniform(-abs(ub-lb) * self.v_max_ratio, abs(ub-lb) * self.v_max_ratio, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        self.personal_best_positions = self.population.copy()\n        self.personal_best_fitness = self.fitness.copy()\n        self.global_best_position = self.population[np.argmin(self.fitness)].copy()\n        self.global_best_fitness = np.min(self.fitness)\n\n    def update_velocity(self, i, w):\n        r1 = np.random.rand(self.dim)\n        r2 = np.random.rand(self.dim)\n        cognitive_velocity = self.c1 * r1 * (self.personal_best_positions[i] - self.population[i])\n        social_velocity = self.c2 * r2 * (self.global_best_position - self.population[i])\n        self.velocities[i] = w * self.velocities[i] + cognitive_velocity + social_velocity\n        \n        # Velocity clamping\n        for j in range(self.dim):\n            self.velocities[i, j] = np.clip(self.velocities[i, j], -abs(self.population[i, j] * self.v_max_ratio), abs(self.population[i, j] * self.v_max_ratio)) #vmax depends on current particle position\n\n    def update_position(self, i, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.population[i] = self.population[i] + self.velocities[i]\n        self.population[i] = np.clip(self.population[i], lb, ub)  # Clamp to bounds\n        \n        # Local search with small probability to improve exploitation\n        if np.random.rand() < 0.1:\n            direction = np.random.uniform(-0.01, 0.01, self.dim)\n            x_local = np.clip(self.population[i] + direction, lb, ub)\n            f_local = func(x_local)\n            self.eval_count += 1\n            if f_local < self.fitness[i]:\n                self.population[i] = x_local\n                self.fitness[i] = f_local\n                \n        self.fitness[i] = func(self.population[i])\n        self.eval_count += 1\n        \n        if self.fitness[i] < self.personal_best_fitness[i]:\n            self.personal_best_fitness[i] = self.fitness[i]\n            self.personal_best_positions[i] = self.population[i].copy()\n            \n            if self.fitness[i] < self.global_best_fitness:\n                self.global_best_fitness = self.fitness[i]\n                self.global_best_position = self.population[i].copy()\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        w = self.w_init\n        while self.eval_count < self.budget:\n            # Adaptive inertia weight\n            w = self.w_init - (self.w_init - self.w_min) * (self.eval_count / self.budget)\n\n            for i in range(self.pop_size):\n                self.update_velocity(i, w)\n                self.update_position(i, func)\n                if self.eval_count >= self.budget:\n                    break\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 3, "feedback": "The algorithm SelfOrganizingPSO scored 0.083 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9e5761ab-1d53-40a3-b4fa-4b86592367a4"], "operator": null, "metadata": {"aucs": [0.16561685626205291, 0]}}
{"id": "c8920c0b-9334-4217-9668-4fe0a2b4fdc2", "fitness": 0.29385122164075633, "name": "SelfOrganizingScoutBee", "description": "A self-organizing scout bee algorithm that dynamically adjusts scout bee frequency based on fitness landscape exploration and exploitation.", "code": "import numpy as np\n\nclass SelfOrganizingScoutBee:\n    def __init__(self, budget=10000, dim=10, pop_size=50, scout_bees=5, scout_frequency_initial=0.1, scout_frequency_decay=0.95, elite_fraction=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.scout_bees = scout_bees\n        self.scout_frequency = scout_frequency_initial\n        self.scout_frequency_decay = scout_frequency_decay\n        self.elite_fraction = elite_fraction\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        while self.budget > self.pop_size:\n            # Sort population by fitness\n            sorted_indices = np.argsort(fitness)\n            population = population[sorted_indices]\n            fitness = fitness[sorted_indices]\n\n            # Employed bees phase (exploitation)\n            for i in range(self.pop_size):\n                # Select a random neighbor\n                neighbor_index = np.random.randint(0, self.pop_size)\n                while neighbor_index == i:\n                    neighbor_index = np.random.randint(0, self.pop_size)\n\n                # Create a new solution by modifying the current solution\n                phi = np.random.uniform(-1, 1, size=self.dim)\n                new_solution = population[i] + phi * (population[i] - population[neighbor_index])\n                new_solution = np.clip(new_solution, func.bounds.lb, func.bounds.ub)\n\n                # Evaluate the new solution\n                new_fitness = func(new_solution)\n                self.budget -= 1\n                if self.budget <= 0:\n                    return self.f_opt, self.x_opt\n\n                # Greedy selection\n                if new_fitness < fitness[i]:\n                    population[i] = new_solution\n                    fitness[i] = new_fitness\n\n                    if new_fitness < self.f_opt:\n                        self.f_opt = new_fitness\n                        self.x_opt = new_solution\n\n            # Scout bees phase (exploration) - Adaptive frequency\n            num_elite = int(self.elite_fraction * self.pop_size)\n            mean_fitness_elite = np.mean(fitness[:num_elite])\n            mean_fitness_non_elite = np.mean(fitness[num_elite:])\n\n            # Adjust scout bee frequency based on elite vs non-elite fitness\n            if mean_fitness_non_elite > mean_fitness_elite:\n                self.scout_frequency *= (2 - self.scout_frequency_decay) # Increase scout frequency\n            else:\n                self.scout_frequency *= self.scout_frequency_decay # Decrease scout frequency\n\n            self.scout_frequency = np.clip(self.scout_frequency, 0.01, 0.5) # limit the scout frequency\n\n            for i in range(self.pop_size):\n                 if np.random.rand() < self.scout_frequency:\n                    # Replace with a new random solution (scout bee)\n                    population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                    fitness[i] = func(population[i])\n                    self.budget -= 1\n                    if self.budget <= 0:\n                        return self.f_opt, self.x_opt\n                    \n                    if fitness[i] < self.f_opt:\n                        self.f_opt = fitness[i]\n                        self.x_opt = population[i]\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm SelfOrganizingScoutBee scored 0.294 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["fcb3768f-bdd4-4a6e-86fc-ddb91bfed549"], "operator": null, "metadata": {"aucs": [0.13139430004939734, 0.17283706762012074, 0.2884546269606766, 0.21770654885090757, 0.22083868466829892, 0.2420341397604654, 0.23251608993154071, 0.22639484470205473, 0.22607908083847872, 0.16637046322696203, 0.24568533897970668, 0.9915156730504296, 0.2311249492686318, 0.2214522209078783, 0.6230469661099212, 0.2683155944016369, 0.22035746354637187, 0.33291174096962683, 0.1555422207622943, 0.46244641820972643]}}
{"id": "a59b5a02-9d06-4478-89d9-39a31672db22", "fitness": 0.3144205369242258, "name": "MirroredCMAES", "description": "CMA-ES with a mirrored sampling strategy to improve exploration and a simplified covariance update rule.", "code": "import numpy as np\n\nclass MirroredCMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, sigma0=0.5, mu_ratio=0.25, mirrored_sampling=True):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.sigma0 = sigma0\n        self.mirrored_sampling = mirrored_sampling\n\n        if pop_size is None:\n            self.pop_size = 4 + int(3 * np.log(self.dim))\n        else:\n            self.pop_size = pop_size\n        self.mu = int(self.pop_size * mu_ratio)\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.c_cov = 2 / (self.dim + np.sqrt(2))**2 + self.mueff / self.pop_size  # Simplified c_cov\n        self.c_sigma = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.d_sigma = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.c_sigma\n        self.chiN = self.dim**0.5 * (1 - 1/(4*self.dim) + 1/(21*self.dim**2))\n\n    def __call__(self, func):\n        mean = np.random.uniform(self.lb, self.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)\n        p_sigma = np.zeros(self.dim)\n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n\n        while eval_count < self.budget:\n            # Generate population\n            Z = np.random.multivariate_normal(np.zeros(self.dim), C, size=self.pop_size // (1 + self.mirrored_sampling)) if self.mirrored_sampling else np.random.multivariate_normal(np.zeros(self.dim), C, size=self.pop_size)\n            \n            if self.mirrored_sampling:\n                Z = np.concatenate([Z, -Z])\n            \n            X = mean + sigma * Z\n\n            # Repair individuals outside the bounds\n            X = np.clip(X, self.lb, self.ub)\n            \n            F = np.array([func(x) for x in X])\n            eval_count += self.pop_size\n            if eval_count > self.budget:\n                eval_count = self.budget\n                F = F[:self.budget - (eval_count - self.pop_size)]\n                X = X[:self.budget - (eval_count - self.pop_size)]\n                \n            if np.min(F) < f_opt:\n                f_opt = np.min(F)\n                x_opt = X[np.argmin(F)].copy()\n\n            # Selection and recombination\n            idx = np.argsort(F)\n            X_mu = X[idx[:self.mu]]\n            Z_mu = Z[idx[:self.mu]]\n            mean_new = np.sum(self.weights[:, None] * X_mu, axis=0)\n            z_mean = np.sum(self.weights[:, None] * Z_mu, axis=0)\n\n            # Update evolution paths\n            p_sigma = (1 - self.c_sigma) * p_sigma + np.sqrt(self.c_sigma * (2 - self.c_sigma) * self.mueff) * z_mean\n\n            # Simplified Covariance matrix adaptation\n            C = (1 - self.c_cov) * C + self.c_cov * (z_mean[:, None] @ z_mean[None, :])  # Simplified update\n\n            # Update step size\n            sigma *= np.exp((self.c_sigma / self.d_sigma) * (np.linalg.norm(p_sigma) / self.chiN - 1))\n            sigma = np.clip(sigma, 1e-6, 1)\n            \n            mean = mean_new\n\n        return f_opt, x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm MirroredCMAES scored 0.314 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b6933206-e848-41ff-a985-e5a4e8d99d40"], "operator": null, "metadata": {"aucs": [0.053761085664362884, 0.21641040656213661, 0.3741303212819096, 0.18784351502790497, 0.25377401299347613, 0.3491364702160855, 0.22572832984852564, 0.4012149980297012, 0.17222888051327212, 0.14285547654357633, 0.1906736300219405, 0.9967245211620002, 0.3709806363823872, 0.2616203926140144, 0.6703305334327345, 0.3053019274598726, 0.26426358656082116, 0.2515660275164152, 0.12261024300829726, 0.4772557436450826]}}
{"id": "42116618-fc18-44e4-b6aa-ee0473321764", "fitness": 0.0, "name": "ImprovedCMAESLocalSearch", "description": "An improved CMA-ES variant that incorporates a simplified adaptation of the covariance matrix and step size, along with a local search component triggered based on stagnation.", "code": "import numpy as np\n\nclass ImprovedCMAESLocalSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=None, sigma0=0.5, mu_ratio=0.25, stagnation_threshold=100):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.sigma0 = sigma0\n        if pop_size is None:\n            self.pop_size = 4 + int(3 * np.log(self.dim))\n        else:\n            self.pop_size = pop_size\n        self.mu = int(self.pop_size * mu_ratio)\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.stagnation_threshold = stagnation_threshold\n\n    def local_search(self, func, x_opt, sigma, num_iterations=10):\n        \"\"\"Performs a simple local search around the current best solution.\"\"\"\n        for _ in range(num_iterations):\n            x_new = x_opt + np.random.normal(0, sigma, self.dim)\n            x_new = np.clip(x_new, self.lb, self.ub)\n            f_new = func(x_new)\n            if f_new < self.f_opt:\n                self.f_opt = f_new\n                self.x_opt = x_new.copy()\n\n\n    def __call__(self, func):\n        mean = np.random.uniform(self.lb, self.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)\n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n        stagnation_counter = 0\n        previous_f_opt = np.Inf\n\n\n        while eval_count < self.budget:\n            # Generate population\n            Z = np.random.multivariate_normal(np.zeros(self.dim), C, size=self.pop_size)\n            X = mean + sigma * Z\n            # Repair individuals outside the bounds\n            X = np.clip(X, self.lb, self.ub)\n            \n            F = np.array([func(x) for x in X])\n            eval_count += self.pop_size\n            if eval_count > self.budget:\n                eval_count = self.budget\n                F = F[:self.budget - (eval_count - self.pop_size)]\n                X = X[:self.budget - (eval_count - self.pop_size)]\n                \n            if np.min(F) < f_opt:\n                f_opt = np.min(F)\n                x_opt = X[np.argmin(F)].copy()\n                if f_opt < previous_f_opt:\n                    stagnation_counter = 0\n                    previous_f_opt = f_opt\n            else:\n                stagnation_counter += 1\n                \n            self.f_opt = f_opt\n            self.x_opt = x_opt\n\n            # Selection and recombination\n            idx = np.argsort(F)\n            X_mu = X[idx[:self.mu]]\n            mean_new = np.sum(self.weights[:, None] * X_mu, axis=0)\n            \n            # Simplified covariance matrix adaptation (rank-one update)\n            z_mu = (mean_new - mean) / sigma\n            C = (1 - 0.1) * C + 0.1 * np.outer(z_mu, z_mu)\n            \n            # Ensure C is positive definite\n            C = np.triu(C)\n            C += C.T - np.diag(C.diagonal())\n            try:\n                np.linalg.cholesky(C)  # Check if positive definite\n            except np.linalg.LinAlgError:\n                C = np.eye(self.dim)  # Reset if not positive definite\n\n            # Update step size\n            sigma *= np.exp(0.2 * (np.linalg.norm(z_mu) - np.sqrt(self.dim)))\n            sigma = np.clip(sigma, 1e-6, 1)\n\n            mean = mean_new\n\n            if stagnation_counter > self.stagnation_threshold:\n                self.local_search(func, self.x_opt, sigma/2)\n                stagnation_counter = 0  # Reset stagnation counter\n\n            f_opt = self.f_opt\n            x_opt = self.x_opt\n                \n        return f_opt, x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm ImprovedCMAESLocalSearch scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b6933206-e848-41ff-a985-e5a4e8d99d40"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "5df86769-c606-4caa-9331-00bd94fb6415", "fitness": 0.2729320580789973, "name": "OrthogonalDE", "description": "Adaptive Differential Evolution with orthogonal learning, using orthogonal experimental design to improve the search efficiency of differential evolution.", "code": "import numpy as np\n\nclass OrthogonalDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, CR=0.7, F=0.5, orthogonal_levels=3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.F = F\n        self.orthogonal_levels = orthogonal_levels\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        \n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index].copy()\n\n    def generate_orthogonal_array(self):\n        # Generate an orthogonal array based on the specified levels and dimensions.\n        # This is a simplified version; in practice, use a library or pre-computed array.\n        # L9(3^4) array is used here\n        if self.orthogonal_levels == 3:\n            array = np.array([\n                [1, 1, 1, 1],\n                [1, 2, 2, 2],\n                [1, 3, 3, 3],\n                [2, 1, 2, 3],\n                [2, 2, 3, 1],\n                [2, 3, 1, 2],\n                [3, 1, 3, 2],\n                [3, 2, 1, 3],\n                [3, 3, 2, 1]\n            ])\n            return array - 1 # Convert to 0-based indexing\n\n    def evolve(self, func):\n        orthogonal_array = self.generate_orthogonal_array()\n        num_factors = orthogonal_array.shape[1]\n        num_experiments = orthogonal_array.shape[0]\n\n        for i in range(self.pop_size):\n            # Differential Evolution Mutation\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[idxs]\n            x_mutated = x_r1 + self.F * (x_r2 - x_r3)\n            x_mutated = np.clip(x_mutated, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            x_trial = self.population[i].copy()\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.CR or j == j_rand:\n                    x_trial[j] = x_mutated[j]\n\n            # Orthogonal Learning\n            if self.dim >= num_factors:\n                # Select a subset of dimensions for orthogonal learning\n                dims_to_explore = np.random.choice(self.dim, num_factors, replace=False)\n\n                # Create a temporary population based on orthogonal array\n                temp_pop = np.zeros((num_experiments, num_factors))\n                for k in range(num_experiments):\n                    for l, dim_index in enumerate(dims_to_explore):\n                        level = orthogonal_array[k, l]\n                        # Divide the range into levels\n                        level_size = (func.bounds.ub[dim_index] - func.bounds.lb[dim_index]) / self.orthogonal_levels\n                        temp_pop[k, l] = func.bounds.lb[dim_index] + level * level_size + level_size / 2 # Middle of the level\n\n                # Evaluate the temporary population\n                temp_fitness = np.zeros(num_experiments)\n                for k in range(num_experiments):\n                    x_temp = x_trial.copy()\n                    for l, dim_index in enumerate(dims_to_explore):\n                        x_temp[dim_index] = temp_pop[k, l]\n                    temp_fitness[k] = func(x_temp)\n                    self.eval_count += 1\n                    if self.eval_count >= self.budget:\n                        break\n                if self.eval_count >= self.budget:\n                    break\n\n                # Select the best point from orthogonal learning\n                best_experiment = np.argmin(temp_fitness)\n                for l, dim_index in enumerate(dims_to_explore):\n                    x_trial[dim_index] = temp_pop[best_experiment, l]\n\n            # Selection\n            f_trial = func(x_trial)\n            self.eval_count += 1\n\n            if f_trial < self.fitness[i]:\n                self.population[i] = x_trial\n                self.fitness[i] = f_trial\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = x_trial.copy()\n\n            if self.eval_count >= self.budget:\n                break\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        while self.eval_count < self.budget:\n            self.evolve(func)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm OrthogonalDE scored 0.273 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["df7f77e7-ac62-4e2c-9d9c-11eec84d14de"], "operator": null, "metadata": {"aucs": [0.11056931753609756, 0.16150397215210943, 0.25199985911752487, 0.24320053940844766, 0.18058707300329735, 0.18019168404374686, 0.24624900995935628, 0.17315084500198774, 0.21715194901199553, 0.15501374014989067, 0.23291663732294154, 0.9970687788619572, 0.2415877499280693, 0.2030431134916023, 0.5308223671664327, 0.2359620245896582, 0.22600796743047058, 0.26865612427175967, 0.15371997254715697, 0.4492384365854428]}}
{"id": "e0bea45d-45ae-4525-8472-f0a3003bf604", "fitness": 0.6930059238205194, "name": "EnhancedDE", "description": "An enhanced differential evolution strategy with a self-adaptive mutation factor and a local search operator triggered based on stagnation detection.", "code": "import numpy as np\n\nclass EnhancedDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_min=0.1, F_max=0.9, Cr=0.9, stagnation_threshold=100, local_search_prob=0.1, local_search_radius=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_min = F_min\n        self.F_max = F_max\n        self.Cr = Cr\n        self.stagnation_threshold = stagnation_threshold\n        self.local_search_prob = local_search_prob\n        self.local_search_radius = local_search_radius\n        self.best_fitness_history = []\n        self.last_improvement = 0\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n        self.best_fitness_history.append(self.f_opt)\n        \n        generation = 0\n\n        while self.budget > self.pop_size:\n            # Mutation and Crossover\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Self-adaptive Mutation Factor\n                F = np.random.uniform(self.F_min, self.F_max)\n\n                # Mutation\n                indices = [j for j in range(self.pop_size) if j != i]\n                idxs = np.random.choice(indices, size=3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs[0]], population[idxs[1]], population[idxs[2]]\n                \n                mutant = population[i] + F * (x_r1 - x_r2) + F * (x_r3 - population[i])\n                \n                # Crossover\n                for j in range(self.dim):\n                    if np.random.rand() < self.Cr:\n                        new_population[i, j] = mutant[j]\n                    else:\n                        new_population[i, j] = population[i, j]\n                \n                new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n            \n            # Evaluation\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n            \n            # Selection\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    population[i] = new_population[i]\n                    fitness[i] = new_fitness[i]\n                    \n                    if new_fitness[i] < self.f_opt:\n                        self.f_opt = new_fitness[i]\n                        self.x_opt = new_population[i]\n                        self.last_improvement = generation\n                        \n            self.best_fitness_history.append(self.f_opt)\n            \n            # Stagnation check and local search\n            if (generation - self.last_improvement) > self.stagnation_threshold:\n                if np.random.rand() < self.local_search_prob:\n                    # Apply local search to the best individual\n                    x_local_search = self.x_opt + np.random.uniform(-self.local_search_radius, self.local_search_radius, size=self.dim)\n                    x_local_search = np.clip(x_local_search, func.bounds.lb, func.bounds.ub)\n                    f_local_search = func(x_local_search)\n                    self.budget -= 1\n                    \n                    if f_local_search < self.f_opt:\n                        self.f_opt = f_local_search\n                        self.x_opt = x_local_search\n                        self.last_improvement = generation\n                        \n            generation += 1\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm EnhancedDE scored 0.693 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["fcb3768f-bdd4-4a6e-86fc-ddb91bfed549"], "operator": null, "metadata": {"aucs": [0.26758694077392964, 0.718102250812195, 0.5446451933592061, 0.8491293982573754, 0.7413763887634044, 0.8320858366741646, 0.6390714249096348, 0.7173686384097318, 0.7944999200976932, 0.5826948892213437, 0.8849163473589874, 1.0, 0.693077203253124, 0.73213475230598, 0.869219896291475, 0.8114845142720033, 0.6126629881873082, 0.875422090969237, 0.19998773601556674, 0.4946520664780307]}}
{"id": "b4c9cfdd-13af-46b7-aaff-873f5b30c748", "fitness": 0.4132153542486023, "name": "MirroredAdaptiveDE", "description": "An adaptive differential evolution algorithm with a mirrored sampling technique to enhance exploration around the current best solution.", "code": "import numpy as np\n\nclass MirroredAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, CR_init=0.5, F=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR_init = CR_init\n        self.F = F  # Mutation factor\n        self.population = None\n        self.fitness = None\n        self.CR = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        self.CR = np.full(self.pop_size, self.CR_init)\n        \n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index].copy()\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            # Mutation\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[idxs]\n            x_mutated = x_r1 + self.F * (x_r2 - x_r3)\n            x_mutated = np.clip(x_mutated, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            x_trial = self.population[i].copy()\n            j_rand = np.random.randint(self.dim)\n            CR = np.clip(np.random.normal(self.CR[i], 0.1), 0.1, 1.0)  # Self-adjusting CR\n            for j in range(self.dim):\n                if np.random.rand() < CR or j == j_rand:\n                    x_trial[j] = x_mutated[j]\n\n            # Mirrored Sampling\n            best_index = np.argmin(self.fitness)\n            x_best = self.population[best_index]\n            x_mirrored = 2 * x_best - x_trial\n            x_mirrored = np.clip(x_mirrored, func.bounds.lb, func.bounds.ub)\n\n            # Selection\n            f_trial = func(x_trial)\n            f_mirrored = func(x_mirrored)\n            self.eval_count += 2\n\n            if f_trial < self.fitness[i] and f_trial <= f_mirrored:\n                self.CR[i] = CR  # Update CR of individual\n                self.population[i] = x_trial\n                self.fitness[i] = f_trial\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = x_trial.copy()\n            elif f_mirrored < self.fitness[i] and f_mirrored < f_trial:\n                self.CR[i] = CR\n                self.population[i] = x_mirrored\n                self.fitness[i] = f_mirrored\n                if f_mirrored < self.f_opt:\n                    self.f_opt = f_mirrored\n                    self.x_opt = x_mirrored.copy()\n            \n            if self.eval_count >= self.budget:\n                break\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        while self.eval_count < self.budget:\n            self.evolve(func)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm MirroredAdaptiveDE scored 0.413 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["df7f77e7-ac62-4e2c-9d9c-11eec84d14de"], "operator": null, "metadata": {"aucs": [0.1571246563333749, 0.23215615117526145, 0.39592935120665784, 0.625837422054274, 0.2809383408226791, 0.50212904077346, 0.29995039194960227, 0.3826064510128857, 0.3932785729068907, 0.23722063728919174, 0.5260934605863613, 0.9991657103654293, 0.26656485993953516, 0.2799393064043719, 0.7298421357846059, 0.5471503501167041, 0.3372773483044641, 0.3709159305004307, 0.19930214938197255, 0.5008848180638941]}}
{"id": "b6ec3a3c-06ad-4135-a2b5-ab6a026e095e", "fitness": 0.30240215338706566, "name": "SelfOrganizingMigratingAlgorithm", "description": "Self-Organizing Migrating Algorithm with probabilistic neighborhood selection and adaptive step size control using successful mutation information.", "code": "import numpy as np\n\nclass SelfOrganizingMigratingAlgorithm:\n    def __init__(self, budget=10000, dim=10, pop_size=50, migration_probability=0.1, initial_step_size=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.migration_probability = migration_probability\n        self.initial_step_size = initial_step_size\n        self.population = None\n        self.fitness = None\n        self.step_size = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.success_rate = 0.0\n        self.success_history = []\n        self.success_history_size = 10\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        self.step_size = np.full(self.pop_size, self.initial_step_size)\n        \n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index].copy()\n\n    def migrate(self, func):\n        for i in range(self.pop_size):\n            # Select a neighbor - probabilistic selection based on fitness\n            probabilities = np.exp(-self.fitness / self.f_opt)  # Higher fitness -> lower probability\n            probabilities /= np.sum(probabilities)\n            \n            neighbors = np.arange(self.pop_size)\n            neighbor_index = np.random.choice(neighbors, p=probabilities)\n\n            if np.random.rand() < self.migration_probability:\n                # Adaptive step size\n                mutation = self.step_size[i] * np.random.normal(0, 1, self.dim)\n                x_new = self.population[i] + mutation\n                x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n                \n                f_new = func(x_new)\n                self.eval_count += 1\n\n                if f_new < self.fitness[i]:\n                    self.success_history.append(1)\n                    self.population[i] = x_new\n                    self.fitness[i] = f_new\n                    \n                    if f_new < self.f_opt:\n                        self.f_opt = f_new\n                        self.x_opt = x_new.copy()\n                    self.step_size[i] *= 1.1 # Increase step size if successful\n                else:\n                    self.success_history.append(0)\n                    self.step_size[i] *= 0.9 # Decrease step size if unsuccessful\n\n                # Keep track of success rate for global step size adjustment\n                if len(self.success_history) > self.success_history_size:\n                    self.success_history = self.success_history[-self.success_history_size:]\n\n                self.success_rate = np.mean(self.success_history)\n                \n                # Global step size adaptation based on success rate.\n                if self.success_rate > 0.6:\n                     self.step_size *= 1.05\n                elif self.success_rate < 0.2:\n                    self.step_size *= 0.95\n\n            if self.eval_count >= self.budget:\n                break\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        while self.eval_count < self.budget:\n            self.migrate(func)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm SelfOrganizingMigratingAlgorithm scored 0.302 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9e5761ab-1d53-40a3-b4fa-4b86592367a4"], "operator": null, "metadata": {"aucs": [0.10009314224837385, 0.22367091621839785, 0.27741840221956915, 0.22435174220441267, 0.16130759187149213, 0.1661767793948591, 0.22918329641405122, 0.17196017784599693, 0.19626741431438954, 0.13963091834886177, 0.21931078671521098, 0.9899306148101878, 0.2736216951886542, 0.24346868812019162, 0.8156506634973563, 0.3039193212361472, 0.27040653442633966, 0.46421750483165947, 0.1431298628830111, 0.4343270149521501]}}
{"id": "bdf023cd-685b-409e-8198-074928660e97", "fitness": -Infinity, "name": "AdaptiveDECauchyRestart", "description": "An adaptive differential evolution algorithm that uses a Cauchy mutation operator for enhanced exploration and a restart mechanism based on population diversity.", "code": "import numpy as np\n\nclass AdaptiveDECauchyRestart:\n    def __init__(self, budget=10000, dim=10, pop_size=50, CR_init=0.5, F_init=0.7, cauchy_scale=0.1, diversity_threshold=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR_init = CR_init\n        self.F_init = F_init\n        self.cauchy_scale = cauchy_scale\n        self.diversity_threshold = diversity_threshold\n        self.population = None\n        self.fitness = None\n        self.CR = None\n        self.F = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.stagnation_counter = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        self.CR = np.full(self.pop_size, self.CR_init)\n        self.F = np.full(self.pop_size, self.F_init)\n        \n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index].copy()\n\n    def cauchy_mutation(self, x_r1, x_r2, x_r3):\n        delta = x_r2 - x_r3\n        cauchy_noise = np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n        return x_r1 + self.F * delta + cauchy_noise\n\n    def evolve(self, func):\n        f_opt_old = self.f_opt\n        for i in range(self.pop_size):\n            # Mutation\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[idxs]\n            x_mutated = self.cauchy_mutation(x_r1, x_r2, x_r3) # Use Cauchy mutation\n            x_mutated = np.clip(x_mutated, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            x_trial = self.population[i].copy()\n            j_rand = np.random.randint(self.dim)\n            CR = np.clip(np.random.normal(self.CR[i], 0.1), 0.0, 1.0)  # Self-adjusting CR\n            F = np.clip(np.random.normal(self.F[i], 0.1), 0.1, 2.0) # Self-adjusting F\n            \n            for j in range(self.dim):\n                if np.random.rand() < CR or j == j_rand:\n                    x_trial[j] = x_mutated[j]\n\n            # Selection\n            f_trial = func(x_trial)\n            self.eval_count += 1\n\n            if f_trial < self.fitness[i]:\n                self.CR[i] = CR  # Update CR of individual\n                self.F[i] = F # Update F of individual\n                self.population[i] = x_trial\n                self.fitness[i] = f_trial\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = x_trial.copy()\n            \n            if self.eval_count >= self.budget:\n                break\n        \n        # Restart mechanism based on population diversity\n        diversity = np.std(self.fitness)\n        if diversity < self.diversity_threshold:\n            self.stagnation_counter +=1\n        else:\n            self.stagnation_counter = 0\n        \n        if self.stagnation_counter > 20:\n             self.initialize_population(func) # restart population\n             self.stagnation_counter = 0\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        while self.eval_count < self.budget:\n            self.evolve(func)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 87, in __call__\n  File \"<string>\", line 44, in evolve\n  File \"<string>\", line 36, in cauchy_mutation\nValueError: operands could not be broadcast together with shapes (50,) (2,) \n.", "error": "", "parent_ids": ["b4c9cfdd-13af-46b7-aaff-873f5b30c748"], "operator": null, "metadata": {}}
{"id": "cf60f64e-d63f-4750-a500-af1c1a38efcd", "fitness": 0.0, "name": "RingTopologyAdaptiveDE", "description": "An adaptive differential evolution with a ring topology for population interaction and a restarting mechanism based on fitness improvement stagnation.", "code": "import numpy as np\n\nclass RingTopologyAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, CR_init=0.5, F=0.7, stagnation_threshold=1000):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR_init = CR_init\n        self.F = F\n        self.population = None\n        self.fitness = None\n        self.CR = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.stagnation_threshold = stagnation_threshold\n        self.last_improvement_eval = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        self.CR = np.full(self.pop_size, self.CR_init)\n        \n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index].copy()\n            self.last_improvement_eval = self.eval_count\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            # Ring Topology: Select neighbors\n            neighbor_left = (i - 1) % self.pop_size\n            neighbor_right = (i + 1) % self.pop_size\n\n            # Mutation using neighbors\n            x_r1 = self.population[neighbor_left]\n            x_r2 = self.population[neighbor_right]\n            x_r3 = self.population[np.random.choice(self.pop_size)] #random individual\n\n            x_mutated = x_r1 + self.F * (x_r2 - x_r3)\n            x_mutated = np.clip(x_mutated, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            x_trial = self.population[i].copy()\n            j_rand = np.random.randint(self.dim)\n            CR = np.clip(np.random.normal(self.CR[i], 0.1), 0.1, 1.0)  # Self-adjusting CR\n            for j in range(self.dim):\n                if np.random.rand() < CR or j == j_rand:\n                    x_trial[j] = x_mutated[j]\n\n            # Selection\n            f_trial = func(x_trial)\n            self.eval_count += 1\n\n            if f_trial < self.fitness[i]:\n                self.CR[i] = CR  # Update CR of individual\n                self.population[i] = x_trial\n                self.fitness[i] = f_trial\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = x_trial.copy()\n                    self.last_improvement_eval = self.eval_count\n            \n            if self.eval_count >= self.budget:\n                break\n\n        # Restart mechanism if stagnation occurs\n        if self.eval_count - self.last_improvement_eval > self.stagnation_threshold:\n            self.initialize_population(func)  # Restart the population\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        while self.eval_count < self.budget:\n            self.evolve(func)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm RingTopologyAdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b4c9cfdd-13af-46b7-aaff-873f5b30c748"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "1c7b01fd-f37d-4e07-ac47-06ed4ed17f39", "fitness": 0.0, "name": "DynamicPopMirroredDE", "description": "An adaptive Differential Evolution strategy with a simplified mirrored sampling technique and a dynamic population size that decreases over time based on performance, favoring exploitation in later stages.", "code": "import numpy as np\n\nclass DynamicPopMirroredDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=100, CR=0.5, F=0.7, reduction_factor=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.initial_pop_size = initial_pop_size\n        self.CR = CR\n        self.F = F\n        self.reduction_factor = reduction_factor\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 50  # Number of iterations without improvement to trigger reduction\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index].copy()\n\n    def evolve(self, func):\n        new_population = np.zeros_like(self.population)\n        new_fitness = np.zeros_like(self.fitness)\n\n        for i in range(self.pop_size):\n            # Mutation\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[idxs]\n            x_mutated = x_r1 + self.F * (x_r2 - x_r3)\n            x_mutated = np.clip(x_mutated, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            x_trial = self.population[i].copy()\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.CR or j == j_rand:\n                    x_trial[j] = x_mutated[j]\n\n            # Simplified Mirrored Sampling\n            x_mirrored = 2 * self.x_opt - x_trial  # Mirror around current best\n            x_mirrored = np.clip(x_mirrored, func.bounds.lb, func.bounds.ub)\n\n            # Selection: Compare trial and mirrored vectors\n            f_trial = func(x_trial)\n            f_mirrored = func(x_mirrored)\n            self.eval_count += 2\n\n            if f_trial < f_mirrored:\n                new_population[i] = x_trial\n                new_fitness[i] = f_trial\n            else:\n                new_population[i] = x_mirrored\n                new_fitness[i] = f_mirrored\n\n            if new_fitness[i] < self.f_opt:\n                self.f_opt = new_fitness[i]\n                self.x_opt = new_population[i].copy()\n                self.stagnation_counter = 0 # Reset stagnation counter\n        \n        # Update population and fitness\n        self.population = new_population\n        self.fitness = new_fitness\n\n        # Stagnation check and population reduction\n        self.stagnation_counter += 1\n        if self.stagnation_counter > self.stagnation_threshold and self.pop_size > 10:  # Minimum pop size of 10\n            self.reduce_population()\n            self.stagnation_counter = 0\n\n    def reduce_population(self):\n        new_pop_size = int(self.pop_size * self.reduction_factor)\n        if new_pop_size < 10:\n            new_pop_size = 10\n            \n        # Sort population by fitness\n        sorted_indices = np.argsort(self.fitness)\n        \n        # Keep the best individuals\n        self.population = self.population[sorted_indices[:new_pop_size]]\n        self.fitness = self.fitness[sorted_indices[:new_pop_size]]\n        self.pop_size = new_pop_size\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        while self.eval_count < self.budget:\n            self.evolve(func)\n            if self.eval_count >= self.budget:\n                break\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm DynamicPopMirroredDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b4c9cfdd-13af-46b7-aaff-873f5b30c748"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "d36331cd-c0b4-4bb9-ad8d-0627c63eadc8", "fitness": 0.0, "name": "AdaptiveDECauchyArchiveMirror", "description": "An adaptive Differential Evolution strategy with a Cauchy mutation operator, archive for past bad solutions and combined mirrored sampling.", "code": "import numpy as np\n\nclass AdaptiveDECauchyArchiveMirror:\n    def __init__(self, budget=10000, dim=10, pop_size=50, CR_init=0.5, F_init=0.7, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR_init = CR_init\n        self.F_init = F_init\n        self.archive_size = archive_size\n        self.population = None\n        self.fitness = None\n        self.CR = None\n        self.F = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.archive = []  # Archive for storing solutions that are worse than their parents\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        self.CR = np.full(self.pop_size, self.CR_init)\n        self.F = np.full(self.pop_size, self.F_init)\n        \n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index].copy()\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            # Mutation (Cauchy)\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[idxs]\n            \n            # Adaptive F\n            self.F[i] = np.clip(np.random.normal(self.F[i], 0.1), 0.1, 1.0)\n            \n            x_mutated = x_r1 + self.F[i] * (x_r2 - x_r3) + np.random.standard_cauchy(size=self.dim) # Adding Cauchy noise\n            x_mutated = np.clip(x_mutated, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            x_trial = self.population[i].copy()\n            j_rand = np.random.randint(self.dim)\n            CR = np.clip(np.random.normal(self.CR[i], 0.1), 0.1, 1.0)  # Self-adjusting CR\n            for j in range(self.dim):\n                if np.random.rand() < CR or j == j_rand:\n                    x_trial[j] = x_mutated[j]\n\n            # Mirrored Sampling\n            best_index = np.argmin(self.fitness)\n            x_best = self.population[best_index]\n            x_mirrored = 2 * x_best - x_trial\n            x_mirrored = np.clip(x_mirrored, func.bounds.lb, func.bounds.ub)\n\n            # Selection\n            f_trial = func(x_trial)\n            f_mirrored = func(x_mirrored)\n            self.eval_count += 2\n\n            if f_trial < self.fitness[i] and f_trial <= f_mirrored:\n                self.CR[i] = CR  # Update CR of individual\n                self.population[i] = x_trial\n                self.fitness[i] = f_trial\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = x_trial.copy()\n            elif f_mirrored < self.fitness[i] and f_mirrored < f_trial:\n                self.CR[i] = CR\n                self.population[i] = x_mirrored\n                self.fitness[i] = f_mirrored\n                if f_mirrored < self.f_opt:\n                    self.f_opt = f_mirrored\n                    self.x_opt = x_mirrored.copy()\n            else:\n                # Archive the parent solution\n                if len(self.archive) < self.archive_size:\n                    self.archive.append(self.population[i].copy())\n                else:\n                    # Replace a random element in the archive\n                    idx_to_replace = np.random.randint(self.archive_size)\n                    self.archive[idx_to_replace] = self.population[i].copy()\n                    \n            if self.eval_count >= self.budget:\n                break\n            \n            # Use information from archive: restart if necessary\n            if len(self.archive) > 0 and np.random.rand() < 0.05:  # Small probability to use the archive\n                archived_idx = np.random.randint(len(self.archive))\n                self.population[i] = self.archive[archived_idx].copy()\n                self.fitness[i] = func(self.population[i])\n                self.eval_count += 1\n                if self.fitness[i] < self.f_opt:\n                    self.f_opt = self.fitness[i]\n                    self.x_opt = self.population[i].copy()\n                \n    def __call__(self, func):\n        self.initialize_population(func)\n        while self.eval_count < self.budget:\n            self.evolve(func)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDECauchyArchiveMirror scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b4c9cfdd-13af-46b7-aaff-873f5b30c748"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "0baebfba-ced2-42e7-838e-7ca4ac1ff3ac", "fitness": -Infinity, "name": "AdaptiveCMAES", "description": "An adaptive CMA-ES variant that dynamically adjusts its population size and step size based on the function landscape's ruggedness.", "code": "import numpy as np\n\nclass AdaptiveCMAES:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=None, sigma0=0.5, mu_ratio=0.25, ruggedness_detection_window=50):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.sigma0 = sigma0\n        self.ruggedness_detection_window = ruggedness_detection_window\n\n        if initial_pop_size is None:\n            self.pop_size = 4 + int(3 * np.log(self.dim))\n        else:\n            self.pop_size = initial_pop_size\n        self.min_pop_size = 4  # Minimum population size\n        self.max_pop_size = 4 + int(6 * np.log(self.dim)) # Maximum population size\n        \n        self.mu = int(self.pop_size * mu_ratio)\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.c_sigma = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.d_sigma = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.c_sigma\n        self.chiN = self.dim**0.5 * (1 - 1/(4*self.dim) + 1/(21*self.dim**2))\n        self.c_cov = 2 / (self.dim + np.sqrt(2))**2 + self.mueff / self.pop_size #Simplified Covariance matrix adaptation\n\n        self.fitness_history = []\n        self.sigma_history = []\n\n    def __call__(self, func):\n        mean = np.random.uniform(self.lb, self.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)\n        p_sigma = np.zeros(self.dim)\n        f_opt = np.Inf\n        x_opt = None\n        eval_count = 0\n\n        while eval_count < self.budget:\n            # Generate population\n            Z = np.random.multivariate_normal(np.zeros(self.dim), C, size=self.pop_size)\n            X = mean + sigma * Z\n\n            # Repair individuals outside the bounds\n            X = np.clip(X, self.lb, self.ub)\n            \n            F = np.array([func(x) for x in X])\n            eval_count += self.pop_size\n            if eval_count > self.budget:\n                eval_count = self.budget\n                F = F[:self.budget - (eval_count - self.pop_size)]\n                X = X[:self.budget - (eval_count - self.pop_size)]\n            \n            if np.min(F) < f_opt:\n                f_opt = np.min(F)\n                x_opt = X[np.argmin(F)].copy()\n            \n            self.fitness_history.append(np.min(F))\n            self.sigma_history.append(sigma)\n\n            # Selection and recombination\n            idx = np.argsort(F)\n            X_mu = X[idx[:self.mu]]\n            Z_mu = Z[idx[:self.mu]]\n            mean_new = np.sum(self.weights[:, None] * X_mu, axis=0)\n            z_mean = np.sum(self.weights[:, None] * Z_mu, axis=0)\n\n            # Update evolution paths\n            p_sigma = (1 - self.c_sigma) * p_sigma + np.sqrt(self.c_sigma * (2 - self.c_sigma) * self.mueff) * z_mean\n\n            # Simplified Covariance matrix adaptation\n            C = (1 - self.c_cov) * C + self.c_cov * (z_mean[:, None] @ z_mean[None, :])  # Simplified update\n\n            # Update step size\n            sigma *= np.exp((self.c_sigma / self.d_sigma) * (np.linalg.norm(p_sigma) / self.chiN - 1))\n            sigma = np.clip(sigma, 1e-6, 1)\n\n            # Ruggedness detection and adaptation\n            if len(self.fitness_history) > self.ruggedness_detection_window:\n                fitness_window = self.fitness_history[-self.ruggedness_detection_window:]\n                sigma_window = self.sigma_history[-self.ruggedness_detection_window:]\n                fitness_std = np.std(fitness_window)\n                sigma_mean = np.mean(sigma_window)\n\n                if fitness_std < 1e-8 and sigma_mean < 1e-3: #Stagnation detected\n                    self.pop_size = max(self.min_pop_size, self.pop_size // 2)\n                    sigma = min(self.sigma0, sigma * 1.5) #Increase the step size.\n                elif fitness_std > 1e-3: # Rugged landscape\n                    self.pop_size = min(self.max_pop_size, self.pop_size * 2) #Increase the population size\n                    sigma = max(1e-6, sigma / 1.1) #Decrease the step size.\n                else:\n                    self.pop_size = min(self.max_pop_size, max(self.min_pop_size, self.pop_size)) #Keep pop size as is\n                    \n                self.mu = int(self.pop_size * (self.mu / (self.pop_size + 1e-9))) #Adapt mu to the current pop size.\n\n            mean = mean_new\n\n        return f_opt, x_opt", "configspace": "", "generation": 4, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 144, in evaluate\n    algorithm(f_new)\n  File \"<string>\", line 66, in __call__\nValueError: operands could not be broadcast together with shapes (2,1) (0,5) \n.", "error": "", "parent_ids": ["a59b5a02-9d06-4478-89d9-39a31672db22"], "operator": null, "metadata": {}}
{"id": "3340f124-340d-43e6-b5af-ca74ca6f6500", "fitness": -Infinity, "name": "OrthogonalAdaptiveDE", "description": "An adaptive differential evolution strategy with orthogonal learning to enhance population diversity and convergence speed.", "code": "import numpy as np\n\nclass OrthogonalAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_min=0.1, F_max=0.9, Cr=0.9, stagnation_threshold=50, orthogonal_sample_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_min = F_min\n        self.F_max = F_max\n        self.Cr = Cr\n        self.stagnation_threshold = stagnation_threshold\n        self.orthogonal_sample_size = orthogonal_sample_size\n        self.best_fitness_history = []\n        self.last_improvement = 0\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n        self.best_fitness_history.append(self.f_opt)\n        \n        generation = 0\n\n        while self.budget > self.pop_size:\n            # Mutation and Crossover\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Self-adaptive Mutation Factor\n                F = np.random.uniform(self.F_min, self.F_max)\n\n                # Mutation\n                indices = [j for j in range(self.pop_size) if j != i]\n                idxs = np.random.choice(indices, size=3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs[0]], population[idxs[1]], population[idxs[2]]\n                \n                mutant = population[i] + F * (x_r1 - x_r2) + F * (x_r3 - population[i])\n                \n                # Crossover\n                for j in range(self.dim):\n                    if np.random.rand() < self.Cr:\n                        new_population[i, j] = mutant[j]\n                    else:\n                        new_population[i, j] = population[i, j]\n                \n                new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n            \n            # Evaluation\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n            \n            # Selection\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    population[i] = new_population[i]\n                    fitness[i] = new_fitness[i]\n                    \n                    if new_fitness[i] < self.f_opt:\n                        self.f_opt = new_fitness[i]\n                        self.x_opt = new_population[i]\n                        self.last_improvement = generation\n                        \n            self.best_fitness_history.append(self.f_opt)\n            \n            # Stagnation check and orthogonal learning\n            if (generation - self.last_improvement) > self.stagnation_threshold:\n                # Orthogonal experimental design for diversification\n                orthogonal_matrix = self._generate_orthogonal_matrix(self.orthogonal_sample_size)\n                \n                for i in range(self.pop_size):\n                    # Select a dimension to perturb based on the orthogonal matrix\n                    dim_index = np.random.randint(0, self.dim)\n                    \n                    # Generate orthogonal samples around the current solution\n                    for j in range(self.orthogonal_sample_size):\n                        x_orthogonal = np.copy(population[i])\n                        \n                        # Calculate perturbation based on orthogonal matrix\n                        perturbation = orthogonal_matrix[j, dim_index] * (func.bounds.ub - func.bounds.lb) / 2.0\n\n                        x_orthogonal[dim_index] += perturbation\n                        x_orthogonal = np.clip(x_orthogonal, func.bounds.lb, func.bounds.ub)\n\n                        f_orthogonal = func(x_orthogonal)\n                        self.budget -= 1\n\n                        if f_orthogonal < fitness[i]:\n                            population[i] = x_orthogonal\n                            fitness[i] = f_orthogonal\n\n                            if f_orthogonal < self.f_opt:\n                                self.f_opt = f_orthogonal\n                                self.x_opt = x_orthogonal\n                                self.last_improvement = generation\n                                \n            generation += 1\n        \n        return self.f_opt, self.x_opt\n\n    def _generate_orthogonal_matrix(self, size):\n         # A simplified version, replace with a proper OAV if needed\n         matrix = np.random.rand(size, self.dim)\n         return matrix", "configspace": "", "generation": 4, "feedback": "An exception occured: TypeError: only size-1 arrays can be converted to Python scalars\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 144, in evaluate\n    algorithm(f_new)\n  File \"<string>\", line 84, in __call__\nValueError: setting an array element with a sequence.\n.", "error": "", "parent_ids": ["e0bea45d-45ae-4525-8472-f0a3003bf604"], "operator": null, "metadata": {}}
{"id": "3070cae7-c5b7-4718-bc5a-ef9a6d1d352e", "fitness": 0.3473626336159007, "name": "RestartDE", "description": "A differential evolution strategy with a restart mechanism triggered by stagnation and a dynamically adjusted crossover rate based on population diversity.", "code": "import numpy as np\n\nclass RestartDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, Cr_initial=0.9, stagnation_threshold=100, diversity_threshold=0.1, restart_probability=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.Cr_initial = Cr_initial\n        self.Cr = Cr_initial  # Initialize Cr\n        self.stagnation_threshold = stagnation_threshold\n        self.diversity_threshold = diversity_threshold\n        self.restart_probability = restart_probability\n        self.best_fitness_history = []\n        self.last_improvement = 0\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n        self.best_fitness_history.append(self.f_opt)\n        \n        generation = 0\n\n        while self.budget > self.pop_size:\n            # Calculate population diversity (variance of each dimension)\n            diversity = np.mean(np.var(population, axis=0))\n            \n            # Adjust crossover rate based on diversity\n            if diversity < self.diversity_threshold:\n                self.Cr = min(1.0, self.Cr + 0.05)  # Increase Cr if diversity is low\n            else:\n                self.Cr = max(0.1, self.Cr - 0.025)  # Decrease Cr if diversity is high\n\n            # Mutation and Crossover\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                indices = [j for j in range(self.pop_size) if j != i]\n                idxs = np.random.choice(indices, size=3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs[0]], population[idxs[1]], population[idxs[2]]\n                \n                mutant = population[i] + self.F * (x_r1 - x_r2)\n                \n                # Crossover\n                for j in range(self.dim):\n                    if np.random.rand() < self.Cr:\n                        new_population[i, j] = mutant[j]\n                    \n                new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n\n            # Evaluation\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n\n            # Selection\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    population[i] = new_population[i]\n                    fitness[i] = new_fitness[i]\n                    \n                    if new_fitness[i] < self.f_opt:\n                        self.f_opt = new_fitness[i]\n                        self.x_opt = new_population[i]\n                        self.last_improvement = generation\n                        \n            self.best_fitness_history.append(self.f_opt)\n\n            # Stagnation check and restart\n            if (generation - self.last_improvement) > self.stagnation_threshold:\n                if np.random.rand() < self.restart_probability:\n                    # Restart: Reinitialize the population\n                    population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                    fitness = np.array([func(x) for x in population])\n                    self.budget -= self.pop_size\n                    self.f_opt = np.min(fitness)\n                    self.x_opt = population[np.argmin(fitness)]\n                    self.last_improvement = generation  # Reset last improvement\n                    self.Cr = self.Cr_initial # Reset Cr\n                    print(\"Restarting population\")\n\n            generation += 1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm RestartDE scored 0.347 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c077592a-84a0-412a-ba34-07465d9c4e89"], "operator": null, "metadata": {"aucs": [0.14436161914890344, 0.20155212589470828, 0.3539114387910245, 0.30696265318002525, 0.2653897697200448, 0.3296990895008549, 0.2787631976589814, 0.30328867798923664, 0.28212795969602644, 0.19393737592755256, 0.30152929640255954, 0.9985354744430103, 0.27166589585255885, 0.27742704943052554, 0.6979193306387441, 0.3659390423000992, 0.27288491865137865, 0.4118870055294558, 0.1853324351374428, 0.5041383164248808]}}
{"id": "bfcd1c5a-80d5-4ac1-aef0-7fe3be106781", "fitness": 0.2893770167099851, "name": "AntColonyOptimization", "description": "An adaptive population-based algorithm inspired by the foraging behavior of ants, using pheromone trails to guide search and dynamically adjusting trail evaporation rates.", "code": "import numpy as np\n\nclass AntColonyOptimization:\n    def __init__(self, budget=10000, dim=10, n_ants=50, rho=0.1, alpha=1, beta=2, q=1):\n        self.budget = budget\n        self.dim = dim\n        self.n_ants = n_ants\n        self.rho = rho  # Evaporation rate\n        self.alpha = alpha  # Pheromone influence\n        self.beta = beta  # Heuristic influence (fitness)\n        self.q = q # Pheromone deposit constant\n        self.lb = -5.0\n        self.ub = 5.0\n        self.pheromone = np.ones(dim) * 1e-6  # Initialize pheromone levels\n        self.best_solution = None\n        self.best_fitness = np.inf\n        self.eval_count = 0\n\n    def construct_solution(self):\n        solution = np.zeros(self.dim)\n        for i in range(self.dim):\n            probabilities = (self.pheromone[i]**self.alpha) * ((self.ub - self.lb)**-self.beta)  # Simplified heuristic\n            probabilities /= np.sum(probabilities)\n            solution[i] = np.random.uniform(self.lb, self.ub)\n        return solution\n\n    def deposit_pheromone(self, solution, fitness):\n        delta_pheromone = self.q / (fitness + 1e-9)  # Avoid division by zero\n        return delta_pheromone\n    \n    def evaporate_pheromone(self):\n        self.pheromone *= (1 - self.rho)\n        self.pheromone = np.clip(self.pheromone, 1e-6, 1e6)\n\n    def __call__(self, func):\n        while self.eval_count < self.budget:\n            solutions = []\n            fitnesses = []\n\n            # Ant colony constructs solutions\n            for _ in range(self.n_ants):\n                solution = self.construct_solution()\n                fitness = func(solution)\n                self.eval_count += 1\n\n                solutions.append(solution)\n                fitnesses.append(fitness)\n                \n                if fitness < self.best_fitness:\n                    self.best_fitness = fitness\n                    self.best_solution = solution.copy()\n                \n                if self.eval_count >= self.budget:\n                    break\n                    \n            if self.eval_count >= self.budget:\n                break\n            \n            # Pheromone update\n            for i in range(self.n_ants):\n                delta_pheromone = self.deposit_pheromone(solutions[i], fitnesses[i])\n                \n            self.pheromone += delta_pheromone\n            self.pheromone = np.clip(self.pheromone, 1e-6, 1e6) # Clip for stability\n                \n            self.evaporate_pheromone()\n\n        return self.best_fitness, self.best_solution", "configspace": "", "generation": 4, "feedback": "The algorithm AntColonyOptimization scored 0.289 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a59b5a02-9d06-4478-89d9-39a31672db22"], "operator": null, "metadata": {"aucs": [0.12041005406104954, 0.2021658020753616, 0.27278170767277976, 0.23396919597102483, 0.20973196383593617, 0.23515953197934147, 0.2371465286961114, 0.22843494416449817, 0.2176768104599336, 0.15853477568197227, 0.2451646238399231, 0.9990610772413291, 0.26339371058900174, 0.1960993384214832, 0.568884208916773, 0.2776511093110273, 0.219126735470053, 0.2684382797777035, 0.17880674159902288, 0.45490319443537686]}}
{"id": "b80c3dd2-4d19-43e3-84f7-1c95b0b2b2ff", "fitness": 0.3356438763361621, "name": "CauchyAdaptiveDE", "description": "Population-based algorithm with a simplified adaptive strategy and a Cauchy mutation operator to enhance exploration, combined with a restart mechanism upon stagnation.", "code": "import numpy as np\n\nclass CauchyAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, Cr=0.9, F_initial=0.5, stagnation_threshold=100, restart_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.Cr = Cr\n        self.F = F_initial\n        self.stagnation_threshold = stagnation_threshold\n        self.restart_prob = restart_prob\n        self.best_fitness_history = []\n        self.last_improvement = 0\n        self.generation = 0\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n        self.best_fitness_history.append(self.f_opt)\n        self.last_improvement = 0\n        self.generation = 0\n\n        while self.budget > self.pop_size:\n            # Mutation and Crossover\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Cauchy Mutation\n                indices = [j for j in range(self.pop_size) if j != i]\n                idxs = np.random.choice(indices, size=3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs[0]], population[idxs[1]], population[idxs[2]]\n                \n                # F = np.random.uniform(self.F_min, self.F_max)\n                mutant = population[i] + self.F * (x_r1 - x_r2) + self.F * np.random.standard_cauchy(size=self.dim) #Cauchy dist\n\n                # Crossover\n                for j in range(self.dim):\n                    if np.random.rand() < self.Cr:\n                        new_population[i, j] = mutant[j]\n                    else:\n                        new_population[i, j] = population[i, j]\n                \n                new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n            \n            # Evaluation\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n            \n            # Selection\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    population[i] = new_population[i]\n                    fitness[i] = new_fitness[i]\n                    \n                    if new_fitness[i] < self.f_opt:\n                        self.f_opt = new_fitness[i]\n                        self.x_opt = new_population[i]\n                        self.last_improvement = self.generation\n                        \n            self.best_fitness_history.append(self.f_opt)\n            \n            # Stagnation check and restart\n            if (self.generation - self.last_improvement) > self.stagnation_threshold:\n                if np.random.rand() < self.restart_prob:\n                    # Restart the population\n                    population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                    fitness = np.array([func(x) for x in population])\n                    self.budget -= self.pop_size\n                    self.f_opt = np.min(fitness)\n                    self.x_opt = population[np.argmin(fitness)]\n                    self.last_improvement = self.generation\n                    self.F = 0.5 #reset F\n                else:\n                    # Adaptive F: Reduce mutation strength upon stagnation\n                    self.F *= 0.9  # Reduce F, but prevent it from becoming zero.\n                    self.F = max(self.F, 0.1)\n\n            self.generation += 1\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm CauchyAdaptiveDE scored 0.336 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e0bea45d-45ae-4525-8472-f0a3003bf604"], "operator": null, "metadata": {"aucs": [0.15360253244822364, 0.24686888369550386, 0.31219655962212023, 0.3027460001852106, 0.25837793010523313, 0.2934922769437188, 0.26936058369221805, 0.26570880598405233, 0.24481695960200345, 0.18693619841500886, 0.29795824822148376, 0.9959605028362211, 0.2806799975865232, 0.2906007310418933, 0.6596270853525764, 0.3333267041917658, 0.2816995447609518, 0.38051219833905203, 0.18333284827429763, 0.47507293542518303]}}
{"id": "5f69fa46-56fc-495f-beba-61dcb6765840", "fitness": 0.4209339104610092, "name": "LandscapeAwareDE", "description": "A differential evolution strategy that uses a combination of global and local search with dynamically adjusted parameters based on the function landscape.", "code": "import numpy as np\n\nclass LandscapeAwareDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, Cr_init=0.7, local_search_prob=0.1, local_search_radius=0.1, exploration_factor=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.Cr = Cr_init\n        self.local_search_prob = local_search_prob\n        self.local_search_radius = local_search_radius\n        self.exploration_factor = exploration_factor\n        self.best_fitness_history = []\n        self.stagnation_counter = 0\n        self.last_improvement = 0\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)].copy()\n        self.best_fitness_history.append(self.f_opt)\n        \n        generation = 0\n\n        while self.budget > self.pop_size:\n            # Adaptive Parameter Adjustment\n            if generation > 0:\n                if self.f_opt == self.best_fitness_history[-1]:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0\n            \n            # Dynamically adjust F and Cr\n            if self.stagnation_counter > 50:\n                self.F = min(1.0, self.F * 1.1)  # Increase F for more exploration\n                self.Cr = max(0.0, self.Cr * 0.9)  # Decrease Cr for focused search\n            else:\n                self.F = max(0.1, self.F * 0.99)  # Decrease F for exploitation\n                self.Cr = min(0.9, self.Cr * 1.01)  # Increase Cr for exploration\n\n            # Mutation and Crossover\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Mutation\n                indices = [j for j in range(self.pop_size) if j != i]\n                idxs = np.random.choice(indices, size=3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs[0]], population[idxs[1]], population[idxs[2]]\n                \n                mutant = population[i] + self.F * (x_r1 - x_r2) + self.F * (x_r3 - population[i])\n                \n                # Crossover\n                for j in range(self.dim):\n                    if np.random.rand() < self.Cr:\n                        new_population[i, j] = mutant[j]\n                    else:\n                        new_population[i, j] = population[i, j]\n                \n                new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n            \n            # Evaluation\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n            \n            # Selection and Local Search\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    population[i] = new_population[i]\n                    fitness[i] = new_fitness[i]\n                    \n                    if new_fitness[i] < self.f_opt:\n                        self.f_opt = new_fitness[i]\n                        self.x_opt = new_population[i].copy()  # Deep copy\n                        self.last_improvement = generation\n                        \n            self.best_fitness_history.append(self.f_opt)\n\n            # Local Search around the best solution with probability\n            if np.random.rand() < self.local_search_prob:\n                # Explore around the current best solution\n                exploration_vector = np.random.uniform(-self.exploration_factor, self.exploration_factor, size=self.dim) * (func.bounds.ub - func.bounds.lb)\n                x_local_search = self.x_opt + exploration_vector\n                x_local_search = np.clip(x_local_search, func.bounds.lb, func.bounds.ub)\n                f_local_search = func(x_local_search)\n                self.budget -= 1\n\n                if f_local_search < self.f_opt:\n                    self.f_opt = f_local_search\n                    self.x_opt = x_local_search.copy()\n                    self.last_improvement = generation\n                \n            generation += 1\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm LandscapeAwareDE scored 0.421 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e0bea45d-45ae-4525-8472-f0a3003bf604"], "operator": null, "metadata": {"aucs": [0.13983139842690995, 0.36662730353854933, 0.44598689088820664, 0.8359891273519159, 0.2627237249917652, 0.40552145187777655, 0.3195173085572246, 0.38674397474914923, 0.4894381228656206, 0.178597402356243, 0.33835530281138215, 0.9921157262546627, 0.2993892511179238, 0.24408317730546714, 0.7041147348729383, 0.5364695365294803, 0.365663053797433, 0.4190850721841206, 0.18878306736133788, 0.4996425813820766]}}
{"id": "e2489bcc-feb0-46d2-bc47-f80d76739cde", "fitness": 0.6007912531976787, "name": "RingTopologyAdaptiveDE", "description": "An adaptive differential evolution strategy with a ring topology for population interactions and a self-adjusting mutation factor based on the success rate of the individuals within their neighborhood.", "code": "import numpy as np\n\nclass RingTopologyAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, Cr=0.9, neighborhood_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.Cr = Cr\n        self.neighborhood_size = neighborhood_size\n        self.F = np.full(pop_size, 0.5)  # Individual F values\n        self.success_rate = np.zeros(pop_size)\n        self.best_fitness_history = []\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n        self.best_fitness_history.append(self.f_opt)\n\n        generation = 0\n        while self.budget > 0:\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n            success = np.zeros(self.pop_size)\n\n            for i in range(self.pop_size):\n                # Ring Topology Neighborhood Selection\n                neighbors = [(i + j) % self.pop_size for j in range(-self.neighborhood_size // 2, self.neighborhood_size // 2 + 1)]\n                neighbors.remove(i)\n\n                # Mutation\n                idxs = np.random.choice(neighbors, size=3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs[0]], population[idxs[1]], population[idxs[2]]\n\n                mutant = population[i] + self.F[i] * (x_r1 - x_r2) + self.F[i] * (x_r3 - population[i])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                for j in range(self.dim):\n                    if np.random.rand() < self.Cr:\n                        new_population[i, j] = mutant[j]\n\n                new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n\n                # Evaluation\n                new_fitness_i = func(new_population[i])\n                self.budget -= 1\n                \n                if new_fitness_i < fitness[i]:\n                    population[i] = new_population[i]\n                    fitness[i] = new_fitness_i\n                    success[i] = 1  # Mark as successful\n                    \n                    if new_fitness_i < self.f_opt:\n                        self.f_opt = new_fitness_i\n                        self.x_opt = new_population[i]\n\n            # Adjust F based on local success rate\n            for i in range(self.pop_size):\n                neighbor_success_rate = np.mean(success[[(i + j) % self.pop_size for j in range(-self.neighborhood_size // 2, self.neighborhood_size // 2 + 1)]])\n                if neighbor_success_rate > 0.5:\n                    self.F[i] = max(0.1, self.F[i] * 0.95)  # Reduce F if neighborhood is improving\n                else:\n                    self.F[i] = min(0.9, self.F[i] * 1.05)  # Increase F if neighborhood is stagnating\n\n            self.best_fitness_history.append(self.f_opt)\n            generation += 1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm RingTopologyAdaptiveDE scored 0.601 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c077592a-84a0-412a-ba34-07465d9c4e89"], "operator": null, "metadata": {"aucs": [0.18388714366268688, 0.6044068835225449, 0.6035939360308291, 0.8250898796810278, 0.5857534154048228, 0.7223529772168812, 0.38998699840213036, 0.5254934248683057, 0.6651219148415235, 0.5328935957681566, 0.7876066528005231, 0.9805925618913339, 0.500949529202166, 0.4807723506059033, 0.9189479409210675, 0.6916408481681453, 0.4998149782913799, 0.7632957839352075, 0.23047731971205798, 0.5231469290268808]}}
{"id": "2a7d3481-41b1-4846-ae22-b932eef56bfb", "fitness": 0.2819701620845525, "name": "OrthogonalDE", "description": "Differential Evolution with orthogonal learning to enhance population diversity and convergence.", "code": "import numpy as np\n\nclass OrthogonalDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, CR=0.7, F=0.5, orthogonal_samples=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.F = F\n        self.orthogonal_samples = orthogonal_samples # Number of orthogonal samples\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index].copy()\n            \n    def generate_orthogonal_array(self, n, k, l):\n        \"\"\"Generates an orthogonal array using the method described in Taguchi's Orthogonal Arrays.\n        n: Number of runs (samples)\n        k: Number of factors (variables)\n        l: Number of levels (values per variable)\n        Note that this simplified version requires n = l**m, where m is an integer.\n        \"\"\"\n        if n != l**int(np.log(n)/np.log(l)):\n            raise ValueError(\"n must be a power of l\")\n\n        array = np.zeros((n, k), dtype=int)\n\n        # First column\n        for i in range(n):\n            array[i, 0] = i % l\n\n        # Subsequent columns\n        for j in range(1, k):\n            for i in range(n):\n                array[i, j] = (array[i, 0] + i // (l**(int(np.log(n)/np.log(l)) - int(np.log(j+1)/np.log(l))))) % l\n        return array\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            # Mutation\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[idxs]\n            x_mutated = x_r1 + self.F * (x_r2 - x_r3)\n            x_mutated = np.clip(x_mutated, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            x_trial = self.population[i].copy()\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.CR or j == j_rand:\n                    x_trial[j] = x_mutated[j]\n\n            # Orthogonal Learning\n            levels = self.orthogonal_samples\n            if levels <= 1:\n                f_trial = func(x_trial)\n                self.eval_count +=1\n                if f_trial < self.fitness[i]:\n                    self.population[i] = x_trial\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial.copy()\n            else:\n                n_samples = levels**2 if levels <=5 else levels\n                if n_samples > self.budget-self.eval_count:\n                    n_samples = max(1, self.budget-self.eval_count)\n                \n                if n_samples > 1:\n                  \n                    # Only generate orthogonal array if dimension is suitable for the array size\n                    if n_samples >= levels and self.dim <= levels: \n                        try:\n                            orthogonal_array = self.generate_orthogonal_array(n_samples, self.dim, levels)\n                        except ValueError:\n                            orthogonal_array = np.random.randint(0, levels, size=(n_samples, self.dim))\n\n                        candidates = np.zeros((n_samples, self.dim))\n                        for k in range(n_samples):\n                            for d in range(self.dim):\n                                candidates[k, d] = x_trial[d] + (func.bounds.ub[d] - func.bounds.lb[d]) * (orthogonal_array[k, d] / (levels-1) - 0.5)  # Map orthogonal points to search space\n                                candidates[k, d] = np.clip(candidates[k, d], func.bounds.lb[d], func.bounds.ub[d])\n                                \n                        fitness_candidates = np.array([func(x) for x in candidates])\n                        self.eval_count += n_samples\n\n                        best_candidate_idx = np.argmin(fitness_candidates)\n                        if fitness_candidates[best_candidate_idx] < self.fitness[i]:\n                            self.population[i] = candidates[best_candidate_idx]\n                            self.fitness[i] = fitness_candidates[best_candidate_idx]\n\n                            if fitness_candidates[best_candidate_idx] < self.f_opt:\n                                self.f_opt = fitness_candidates[best_candidate_idx]\n                                self.x_opt = candidates[best_candidate_idx].copy()\n                    else:\n                        f_trial = func(x_trial)\n                        self.eval_count +=1\n                        if f_trial < self.fitness[i]:\n                            self.population[i] = x_trial\n                            self.fitness[i] = f_trial\n                            if f_trial < self.f_opt:\n                                self.f_opt = f_trial\n                                self.x_opt = x_trial.copy()\n\n            if self.eval_count >= self.budget:\n                break\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        while self.eval_count < self.budget:\n            self.evolve(func)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm OrthogonalDE scored 0.282 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b4c9cfdd-13af-46b7-aaff-873f5b30c748"], "operator": null, "metadata": {"aucs": [0.11511495397548466, 0.20256950822413577, 0.2888692862350778, 0.21412785149583047, 0.20954553683436927, 0.23845918056338034, 0.23465078498856407, 0.21801429163928943, 0.1944027654379522, 0.1562685008310737, 0.22209964616922118, 0.8953219478522618, 0.2758603258467477, 0.21837367892638626, 0.5973749265359656, 0.2729780599000423, 0.2271133263513193, 0.24165604920421868, 0.16527302829016044, 0.45132959238957004]}}
{"id": "bb29defb-cdc6-4b08-918c-fac3b8758c86", "fitness": -Infinity, "name": "DynamicPopulationDE", "description": "A differential evolution strategy with a dynamically adjusted population size and a distance-based mutation operator, aiming to balance exploration and exploitation by controlling population diversity.", "code": "import numpy as np\n\nclass DynamicPopulationDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, Cr=0.9, F=0.5, pop_size_reduction_factor=0.9, pop_size_increase_trigger=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.Cr = Cr\n        self.F = F\n        self.pop_size_reduction_factor = pop_size_reduction_factor\n        self.pop_size_increase_trigger = pop_size_increase_trigger\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.generation = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n\n    def adjust_population_size(self):\n        # Calculate population diversity (average distance to centroid)\n        centroid = np.mean(self.population, axis=0)\n        distances = np.linalg.norm(self.population - centroid, axis=1)\n        avg_distance = np.mean(distances)\n\n        # Reduce population size if diversity is low\n        if avg_distance < self.pop_size_increase_trigger: # self.dim * 0.1\n            new_pop_size = int(self.pop_size * self.pop_size_reduction_factor)\n            if new_pop_size > 10: # Minimum population size\n                self.pop_size = new_pop_size\n                # Select the best individuals to keep\n                indices = np.argsort(self.fitness)[:self.pop_size]\n                self.population = self.population[indices]\n                self.fitness = self.fitness[indices]\n                print(f\"Reduced population size to {self.pop_size}\")\n        elif self.generation % 50 == 0 and self.pop_size < 100: #Increase population slowly if not already too high\n            self.pop_size = min(self.pop_size+5, 100)\n            print(f\"Increasing population size to {self.pop_size}\")\n            \n\n    def distance_based_mutation(self, i):\n         # Select two random indices, ensuring they are different from i\n        idxs = np.random.choice(self.population.shape[0], 2, replace=False)\n        while i in idxs:\n            idxs = np.random.choice(self.population.shape[0], 2, replace=False)\n        \n        x_r1, x_r2 = self.population[idxs[0]], self.population[idxs[1]]\n        \n        # Calculate the distance between the individual and the best individual\n        distance = np.linalg.norm(self.population[i] - self.x_opt)\n        \n        # Scale the mutation factor based on the distance\n        scaled_F = self.F * (1 + distance)\n        \n        # Create the mutant vector\n        mutant = self.population[i] + scaled_F * (x_r1 - x_r2)\n        mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n        return mutant\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            new_population = np.copy(self.population)\n            new_fitness = np.copy(self.fitness)\n\n            for i in range(self.pop_size):\n                # Mutation\n                mutant = self.distance_based_mutation(i)\n                \n                # Crossover\n                for j in range(self.dim):\n                    if np.random.rand() < self.Cr:\n                        new_population[i, j] = mutant[j]\n                    else:\n                        new_population[i, j] = self.population[i, j]\n\n                new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n\n                # Evaluation\n                new_fitness_i = func(new_population[i])\n                self.budget -= 1\n\n                if new_fitness_i < self.fitness[i]:\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness_i\n\n                    if new_fitness_i < self.f_opt:\n                        self.f_opt = new_fitness_i\n                        self.x_opt = new_population[i]\n            \n            self.generation += 1\n            if self.generation % 10 == 0:\n                self.adjust_population_size()\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 75, in __call__\n  File \"<string>\", line 63, in distance_based_mutation\nNameError: name 'func' is not defined\n.", "error": "", "parent_ids": ["e2489bcc-feb0-46d2-bc47-f80d76739cde"], "operator": null, "metadata": {}}
{"id": "bf6b2251-d3ab-44d4-8d66-658f62f6533a", "fitness": -Infinity, "name": "SelfOrganizingPSO", "description": "A self-organizing particle swarm optimization with velocity clamping and adaptive exploration-exploitation balance based on swarm diversity.", "code": "import numpy as np\n\nclass SelfOrganizingPSO:\n    def __init__(self, budget=10000, dim=10, pop_size=40, inertia=0.7, c1=1.5, c2=1.5, velocity_clamp=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.inertia = inertia\n        self.c1 = c1\n        self.c2 = c2\n        self.velocity_clamp = velocity_clamp\n        self.swarm = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = None\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n\n    def initialize_swarm(self, func):\n        self.swarm = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.random.uniform(-self.velocity_clamp, self.velocity_clamp, size=(self.pop_size, self.dim))\n        self.personal_best_positions = np.copy(self.swarm)\n        self.personal_best_fitness = np.array([func(x) for x in self.swarm])\n        self.budget -= self.pop_size\n\n        self.global_best_position = self.personal_best_positions[np.argmin(self.personal_best_fitness)]\n        self.global_best_fitness = np.min(self.personal_best_fitness)\n        self.x_opt = self.global_best_position\n        self.f_opt = self.global_best_fitness\n\n    def update_velocities(self):\n        r1 = np.random.rand(self.pop_size, self.dim)\n        r2 = np.random.rand(self.pop_size, self.dim)\n        cognitive_component = self.c1 * r1 * (self.personal_best_positions - self.swarm)\n        social_component = self.c2 * r2 * (self.global_best_position - self.swarm)\n        self.velocities = self.inertia * self.velocities + cognitive_component + social_component\n        self.velocities = np.clip(self.velocities, -self.velocity_clamp, self.velocity_clamp)\n\n    def update_positions(self, func):\n        self.swarm += self.velocities\n        self.swarm = np.clip(self.swarm, func.bounds.lb, func.bounds.ub)\n\n        fitness = np.array([func(x) for x in self.swarm])\n        self.budget -= self.pop_size\n\n        for i in range(self.pop_size):\n            if fitness[i] < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = fitness[i]\n                self.personal_best_positions[i] = self.swarm[i]\n\n                if fitness[i] < self.global_best_fitness:\n                    self.global_best_fitness = fitness[i]\n                    self.global_best_position = self.swarm[i]\n                    self.x_opt = self.global_best_position\n                    self.f_opt = self.global_best_fitness\n\n    def calculate_diversity(self):\n        centroid = np.mean(self.swarm, axis=0)\n        distances = np.linalg.norm(self.swarm - centroid, axis=1)\n        diversity = np.mean(distances)\n        return diversity\n\n    def adjust_parameters(self, diversity):\n        # Dynamically adjust inertia and exploration/exploitation balance\n        if diversity > 0.1 * (func.bounds.ub[0] - func.bounds.lb[0]):  # Arbitrary threshold\n            self.inertia = min(0.9, self.inertia + 0.05)  # Increase inertia for exploration\n            self.c1 = max(1.0, self.c1 - 0.05)          # Reduce cognitive component\n            self.c2 = min(2.0, self.c2 + 0.05)          # Increase social component\n        else:\n            self.inertia = max(0.4, self.inertia - 0.05)  # Decrease inertia for exploitation\n            self.c1 = min(2.0, self.c1 + 0.05)          # Increase cognitive component\n            self.c2 = max(1.0, self.c2 - 0.05)          # Reduce social component\n\n    def __call__(self, func):\n        self.initialize_swarm(func)\n\n        while self.budget > 0:\n            diversity = self.calculate_diversity()\n            self.adjust_parameters(diversity)\n            n_evaluations_before = self.budget\n            self.update_velocities()\n            self.update_positions(func)\n            n_evaluations_after = self.budget\n\n            if n_evaluations_after == n_evaluations_before:\n              break # no more budget left, so stop optimization\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 79, in __call__\n  File \"<string>\", line 65, in adjust_parameters\nNameError: name 'func' is not defined\n.", "error": "", "parent_ids": ["e2489bcc-feb0-46d2-bc47-f80d76739cde"], "operator": null, "metadata": {}}
{"id": "7b744087-5b18-4b54-99a4-f280e8e29ddb", "fitness": -Infinity, "name": "SOMAdaptiveDE", "description": "An adaptive differential evolution strategy that uses a self-organizing map (SOM) to cluster individuals and adjusts mutation and crossover rates based on cluster performance and diversity.", "code": "import numpy as np\nfrom minisom import MiniSom\n\nclass SOMAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, som_grid_size=5, F_init=0.5, Cr_init=0.7, stagnation_threshold=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.som_grid_size = som_grid_size\n        self.F = F_init\n        self.Cr = Cr_init\n        self.stagnation_threshold = stagnation_threshold\n        self.best_fitness_history = []\n        self.stagnation_counter = 0\n        self.som = MiniSom(som_grid_size, som_grid_size, dim, sigma=0.3, learning_rate=0.5)\n        self.last_improvement = 0\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)].copy()\n        self.best_fitness_history.append(self.f_opt)\n        \n        generation = 0\n\n        while self.budget > self.pop_size:\n            # Train SOM\n            self.som.train_random(population, 10)\n\n            # Assign individuals to SOM nodes\n            node_assignments = [self.som.winner(x) for x in population]\n\n            # Calculate cluster performance and diversity\n            cluster_fitnesses = {}\n            cluster_diversities = {}\n            for i in range(self.som_grid_size):\n                for j in range(self.som_grid_size):\n                    cluster_individuals = [k for k, assignment in enumerate(node_assignments) if assignment == (i, j)]\n                    if cluster_individuals:\n                        cluster_fitnesses[(i, j)] = np.mean(fitness[cluster_individuals])\n                        cluster_diversities[(i, j)] = np.std(population[cluster_individuals])\n\n            # Adaptive Parameter Adjustment based on SOM clusters\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                cluster = node_assignments[i]\n                if cluster in cluster_fitnesses:\n                    # Adjust F and Cr based on cluster performance\n                    if cluster_fitnesses[cluster] < np.mean(fitness): #Good cluster\n                        self.F = max(0.1, self.F * 0.95)  #Exploitation\n                        self.Cr = min(0.9, self.Cr * 1.05)\n                    else: # Bad cluster\n                        self.F = min(1.0, self.F * 1.05) # Exploration\n                        self.Cr = max(0.1, self.Cr * 0.95)\n                    \n                    # Adjust F and Cr based on cluster diversity\n                    if cluster_diversities.get(cluster,0) > np.mean(np.std(population, axis=0)):\n                         self.F = min(1.0, self.F * 1.02) # Exploration\n                         self.Cr = max(0.1, self.Cr * 0.98)\n                    else:\n                         self.F = max(0.1, self.F * 0.98)  #Exploitation\n                         self.Cr = min(0.9, self.Cr * 1.02)\n\n            # Mutation and Crossover\n                indices = [j for j in range(self.pop_size) if j != i]\n                idxs = np.random.choice(indices, size=3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs[0]], population[idxs[1]], population[idxs[2]]\n                \n                mutant = population[i] + self.F * (x_r1 - x_r2) + self.F * (x_r3 - population[i])\n                \n                # Crossover\n                for j in range(self.dim):\n                    if np.random.rand() < self.Cr:\n                        new_population[i, j] = mutant[j]\n                    else:\n                        new_population[i, j] = population[i, j]\n                \n                new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n\n            # Evaluation\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n            \n            # Selection\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    population[i] = new_population[i]\n                    fitness[i] = new_fitness[i]\n                    \n                    if new_fitness[i] < self.f_opt:\n                        self.f_opt = new_fitness[i]\n                        self.x_opt = new_population[i].copy()\n                        self.last_improvement = generation\n                        \n            self.best_fitness_history.append(self.f_opt)\n\n            #Stagnation check\n            if generation > 0:\n                if self.f_opt == self.best_fitness_history[-1]:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0\n            \n            if self.stagnation_counter > self.stagnation_threshold:\n                # Reset population if stagnated\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size  # Adjust budget after re-evaluation\n                self.stagnation_counter = 0\n            \n            generation += 1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 111, in evaluate\n    exec(code, safe_globals, local_env)\n  File \"<string>\", line 2, in <module>\nModuleNotFoundError: No module named 'minisom'\n.", "error": "", "parent_ids": ["5f69fa46-56fc-495f-beba-61dcb6765840"], "operator": null, "metadata": {}}
{"id": "fe89ace4-9df8-4f82-9187-80165fcfbb79", "fitness": 0.0, "name": "AdaptivePopulationDE", "description": "An adaptive differential evolution strategy with a dynamic population size and a local search operator triggered upon stagnation to refine the best solution.", "code": "import numpy as np\n\nclass AdaptivePopulationDE:\n    def __init__(self, budget=10000, dim=10, pop_size_initial=40, pop_size_min=10, pop_size_max=100, F=0.5, Cr=0.9, stagnation_threshold=100, local_search_probability=0.1, local_search_radius=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_initial = pop_size_initial\n        self.pop_size = pop_size_initial\n        self.pop_size_min = pop_size_min\n        self.pop_size_max = pop_size_max\n        self.F = F\n        self.Cr = Cr\n        self.stagnation_threshold = stagnation_threshold\n        self.local_search_probability = local_search_probability\n        self.local_search_radius = local_search_radius\n        self.best_fitness_history = []\n        self.last_improvement = 0\n        self.generation = 0\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n        self.best_fitness_history.append(self.f_opt)\n        \n        while self.budget > self.pop_size_min:\n            # Mutation and Crossover\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                indices = [j for j in range(self.pop_size) if j != i]\n                idxs = np.random.choice(indices, size=3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs[0]], population[idxs[1]], population[idxs[2]]\n                \n                mutant = population[i] + self.F * (x_r1 - x_r2)\n                \n                # Crossover\n                for j in range(self.dim):\n                    if np.random.rand() < self.Cr:\n                        new_population[i, j] = mutant[j]\n                    \n                new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n\n            # Evaluation\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n\n            # Selection\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    population[i] = new_population[i]\n                    fitness[i] = new_fitness[i]\n                    \n                    if new_fitness[i] < self.f_opt:\n                        self.f_opt = new_fitness[i]\n                        self.x_opt = new_population[i]\n                        self.last_improvement = self.generation\n                        \n            self.best_fitness_history.append(self.f_opt)\n\n            # Stagnation check and Local Search\n            if (self.generation - self.last_improvement) > self.stagnation_threshold:\n                if np.random.rand() < self.local_search_probability:\n                    # Perform local search around the best solution\n                    x_local = self.x_opt + np.random.uniform(-self.local_search_radius, self.local_search_radius, size=self.dim)\n                    x_local = np.clip(x_local, func.bounds.lb, func.bounds.ub)\n                    f_local = func(x_local)\n                    self.budget -= 1\n\n                    if f_local < self.f_opt:\n                        self.f_opt = f_local\n                        self.x_opt = x_local\n                        self.last_improvement = self.generation\n                        print(\"Local search improved the solution\")\n            \n            # Population size adaptation\n            if self.f_opt == self.best_fitness_history[-1] and len(self.best_fitness_history) > 1:  #Stagnation\n                self.pop_size = max(self.pop_size_min, int(self.pop_size * 0.9)) #Reduce the population size\n            else:\n                 self.pop_size = min(self.pop_size_max, int(self.pop_size * 1.1)) #Increase the population size\n\n            #Ensure pop_size never goes below the minimal population size.\n            self.pop_size = max(self.pop_size, self.pop_size_min)\n            #Regenerate the population:\n            population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n            fitness = np.array([func(x) for x in population])\n            self.budget -= self.pop_size\n            self.f_opt = np.min(fitness)\n            self.x_opt = population[np.argmin(fitness)]\n            self.last_improvement = self.generation \n            \n            self.generation += 1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptivePopulationDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3070cae7-c5b7-4718-bc5a-ef9a6d1d352e"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "67c00127-1841-4b1c-889c-35396e9f6442", "fitness": 0.0, "name": "VoronoiAdaptiveDE", "description": "An adaptive differential evolution strategy with a self-adjusting population size based on performance, using a Voronoi neighborhood mutation.", "code": "import numpy as np\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\n\nclass VoronoiAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, min_pop_size=10, max_pop_size=100, Cr=0.7, F=0.5, voronoi_samples=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.min_pop_size = min_pop_size\n        self.max_pop_size = max_pop_size\n        self.Cr = Cr\n        self.F = F\n        self.voronoi_samples = voronoi_samples # Samples per Voronoi cell\n        self.best_fitness_history = []\n        self.vor = None\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n        self.best_fitness_history.append(self.f_opt)\n        \n        generation = 0\n\n        while self.budget > self.min_pop_size:\n            # Mutation using Voronoi neighborhood\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n\n            try:\n                if self.dim > 1:\n                    self.vor = Voronoi(population)\n                else:\n                    # Handle 1D case by creating a 2D representation with identical x and y\n                    population_2d = np.column_stack((population.flatten(), population.flatten()))\n                    self.vor = Voronoi(population_2d)\n            except Exception as e:\n                # Fallback to random mutation in case of Voronoi issues\n                print(f\"Voronoi error: {e}. Using random mutation.\")\n                for i in range(self.pop_size):\n                    indices = np.random.choice(self.pop_size, size=3, replace=False)\n                    x_r1, x_r2, x_r3 = population[indices[0]], population[indices[1]], population[indices[2]]\n                    mutant = population[i] + self.F * (x_r1 - x_r2)\n                    for j in range(self.dim):\n                        if np.random.rand() < self.Cr:\n                            new_population[i, j] = mutant[j]\n                    new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                new_fitness = np.array([func(x) for x in new_population])\n                self.budget -= self.pop_size\n\n                for i in range(self.pop_size):\n                    if new_fitness[i] < fitness[i]:\n                        population[i] = new_population[i]\n                        fitness[i] = new_fitness[i]\n                        if new_fitness[i] < self.f_opt:\n                            self.f_opt = new_fitness[i]\n                            self.x_opt = new_population[i]\n\n                self.best_fitness_history.append(self.f_opt)\n                generation +=1\n                \n                # Population size adjustment (simplified)\n                if np.random.rand() < 0.1:\n                    self.pop_size = int(np.clip(self.pop_size * (1 + np.random.normal(0, 0.1)), self.min_pop_size, self.max_pop_size))\n                    population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                    fitness = np.array([func(x) for x in population])\n                    self.budget -= self.pop_size\n                continue\n\n            for i in range(self.pop_size):\n                # Sample neighbors within Voronoi cell\n                neighbors = []\n                if self.dim > 1:\n                  region = self.vor.point_region[i]\n                  if region != -1 and len(self.vor.regions[region]) > 0:  # Voronoi region exists and is not empty\n                      vertices_indices = self.vor.regions[region]\n                      vertices = self.vor.vertices[vertices_indices]\n\n                      # Sample points within the Voronoi cell based on its vertices\n                      for _ in range(self.voronoi_samples):\n                          weights = np.random.rand(len(vertices))\n                          weights /= np.sum(weights)\n                          neighbor = np.sum(vertices * weights[:, np.newaxis], axis=0) #weighted average\n                          neighbors.append(neighbor)\n                else:\n                    # If Voronoi fails, fall back to using other random individuals.\n                    indices = np.random.choice(self.pop_size, size=3, replace=False)\n                    x_r1, x_r2, x_r3 = population[indices[0]], population[indices[1]], population[indices[2]]\n                    mutant = population[i] + self.F * (x_r1 - x_r2)\n                    neighbors = [mutant]\n                    \n\n                if neighbors:\n                    # Select best neighbor\n                    neighbor_fitnesses = [func(np.clip(n, func.bounds.lb, func.bounds.ub)) for n in neighbors]\n                    self.budget -= len(neighbors)\n                    best_neighbor_idx = np.argmin(neighbor_fitnesses)\n                    best_neighbor = np.clip(neighbors[best_neighbor_idx], func.bounds.lb, func.bounds.ub)\n                    best_neighbor_fitness = neighbor_fitnesses[best_neighbor_idx]\n\n\n                    # Crossover and Selection\n                    for j in range(self.dim):\n                        if np.random.rand() < self.Cr:\n                            new_population[i, j] = best_neighbor[j]\n                        else:\n                            new_population[i, j] = population[i, j]\n                    new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                    new_fitness[i] = func(new_population[i])\n                    self.budget -=1\n\n                    if new_fitness[i] < fitness[i]:\n                        population[i] = new_population[i]\n                        fitness[i] = new_fitness[i]\n\n                        if new_fitness[i] < self.f_opt:\n                            self.f_opt = new_fitness[i]\n                            self.x_opt = new_population[i]\n\n            self.best_fitness_history.append(self.f_opt)\n            generation +=1\n            \n            # Population size adjustment\n            if np.random.rand() < 0.1:\n                self.pop_size = int(np.clip(self.pop_size * (1 + np.random.normal(0, 0.1)), self.min_pop_size, self.max_pop_size))\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm VoronoiAdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b80c3dd2-4d19-43e3-84f7-1c95b0b2b2ff"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "415b5a97-623e-4699-8461-e97521e27d87", "fitness": 0.4188589724043565, "name": "AdaptiveLocalSearchDE", "description": "A differential evolution strategy with a local search component triggered adaptively based on the stagnation and diversity of the population.", "code": "import numpy as np\n\nclass AdaptiveLocalSearchDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, Cr=0.9, stagnation_threshold=50, diversity_threshold=0.1, local_search_probability=0.1, local_search_radius=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.Cr = Cr\n        self.stagnation_threshold = stagnation_threshold\n        self.diversity_threshold = diversity_threshold\n        self.local_search_probability = local_search_probability\n        self.local_search_radius = local_search_radius\n        self.best_fitness_history = []\n        self.last_improvement = 0\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n        self.best_fitness_history.append(self.f_opt)\n        \n        generation = 0\n\n        while self.budget > self.pop_size:\n            # Calculate population diversity (variance of each dimension)\n            diversity = np.mean(np.var(population, axis=0))\n            \n            # Mutation and Crossover\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                indices = [j for j in range(self.pop_size) if j != i]\n                idxs = np.random.choice(indices, size=3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs[0]], population[idxs[1]], population[idxs[2]]\n                \n                mutant = population[i] + self.F * (x_r1 - x_r2)\n                \n                # Crossover\n                for j in range(self.dim):\n                    if np.random.rand() < self.Cr:\n                        new_population[i, j] = mutant[j]\n                    \n                new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n\n            # Evaluation\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n\n            # Selection\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    population[i] = new_population[i]\n                    fitness[i] = new_fitness[i]\n                    \n                    if new_fitness[i] < self.f_opt:\n                        self.f_opt = new_fitness[i]\n                        self.x_opt = new_population[i]\n                        self.last_improvement = generation\n                        \n            self.best_fitness_history.append(self.f_opt)\n\n            # Adaptive Local Search\n            if (generation - self.last_improvement) > self.stagnation_threshold and diversity < self.diversity_threshold:\n                for i in range(self.pop_size):\n                    if np.random.rand() < self.local_search_probability:\n                        # Perform local search around individual i\n                        x_current = population[i].copy()\n                        f_current = fitness[i]\n                        \n                        for _ in range(5):  # Small budget for local search\n                            x_new = x_current + np.random.uniform(-self.local_search_radius, self.local_search_radius, size=self.dim)\n                            x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n                            f_new = func(x_new)\n                            self.budget -= 1\n                            if self.budget <= 0:\n                                return self.f_opt, self.x_opt\n                                \n                            if f_new < f_current:\n                                x_current = x_new\n                                f_current = f_new\n                        \n                        # Update population if local search finds a better solution\n                        if f_current < fitness[i]:\n                            population[i] = x_current\n                            fitness[i] = f_current\n                            if f_current < self.f_opt:\n                                self.f_opt = f_current\n                                self.x_opt = x_current\n                                self.last_improvement = generation\n\n\n            generation += 1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveLocalSearchDE scored 0.419 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3070cae7-c5b7-4718-bc5a-ef9a6d1d352e"], "operator": null, "metadata": {"aucs": [0.16426338407177077, 0.2782452648409266, 0.39330806055359735, 0.512236599520641, 0.31635329700659187, 0.4665068319946336, 0.29618619112654054, 0.3325025506521303, 0.32128401611076374, 0.1899337557457944, 0.618534471056008, 0.9973566520839079, 0.3903953123941938, 0.30952769603908736, 0.7810687313153215, 0.45691673846713143, 0.33249210637615534, 0.5419568402408057, 0.1953586200447468, 0.4827523284463824]}}
{"id": "fc29000f-e32e-4d16-9a85-819f85fe7566", "fitness": 0.3616471406286156, "name": "ScaleFreeAdaptiveDE", "description": "Self-organizing differential evolution with a scale-free network topology for information sharing and adaptive parameter control based on individual success.", "code": "import numpy as np\n\nclass ScaleFreeAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, Cr=0.9, F_initial=0.5, stagnation_threshold=100, restart_prob=0.1, sf_connectivity=3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.Cr = Cr\n        self.F = F_initial\n        self.stagnation_threshold = stagnation_threshold\n        self.restart_prob = restart_prob\n        self.sf_connectivity = sf_connectivity # Parameter for scale-free network\n        self.best_fitness_history = []\n        self.last_improvement = 0\n        self.generation = 0\n        self.network = self.create_scale_free_network()\n\n    def create_scale_free_network(self):\n        \"\"\"Creates a scale-free network using the Barabsi-Albert model.\"\"\"\n        # Initialize with a fully connected network of sf_connectivity nodes\n        network = {i: list(range(self.sf_connectivity)) for i in range(self.sf_connectivity)}\n        for i in range(self.sf_connectivity):\n            network[i].remove(i)  # Remove self-loops\n        \n        for new_node in range(self.sf_connectivity, self.pop_size):\n            # Connect new node to existing nodes with probability proportional to their degree\n            degrees = [len(network[node]) for node in network]\n            probabilities = np.array(degrees) / np.sum(degrees)\n            \n            # Choose nodes without replacement\n            connections = np.random.choice(list(network.keys()), size=self.sf_connectivity, replace=False, p=probabilities)\n            \n            network[new_node] = list(connections)\n            for connected_node in connections:\n                network[connected_node].append(new_node)\n                \n        return network\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n        self.best_fitness_history.append(self.f_opt)\n        self.last_improvement = 0\n        self.generation = 0\n\n        while self.budget > self.pop_size:\n            # Mutation and Crossover\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Scale-Free Network based Mutation\n                neighbors = self.network[i]\n                if len(neighbors) < 2:\n                  # Handle edge cases where degree < 2 (rare, but possible)\n                  indices = [j for j in range(self.pop_size) if j != i]\n                  idxs = np.random.choice(indices, size=2, replace=False)\n                  x_r1, x_r2 = population[idxs[0]], population[idxs[1]]\n                  mutant = population[i] + self.F * (x_r1 - x_r2)\n                else:\n                  idxs = np.random.choice(neighbors, size=2, replace=False)\n                  x_r1, x_r2 = population[idxs[0]], population[idxs[1]]\n                  mutant = population[i] + self.F * (x_r1 - x_r2)\n\n                # Crossover\n                for j in range(self.dim):\n                    if np.random.rand() < self.Cr:\n                        new_population[i, j] = mutant[j]\n                    else:\n                        new_population[i, j] = population[i, j]\n                \n                new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n            \n            # Evaluation\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n            \n            # Selection\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    population[i] = new_population[i]\n                    fitness[i] = new_fitness[i]\n                    \n                    if new_fitness[i] < self.f_opt:\n                        self.f_opt = new_fitness[i]\n                        self.x_opt = new_population[i]\n                        self.last_improvement = self.generation\n                        \n            self.best_fitness_history.append(self.f_opt)\n            \n            # Stagnation check and restart\n            if (self.generation - self.last_improvement) > self.stagnation_threshold:\n                if np.random.rand() < self.restart_prob:\n                    # Restart the population and network\n                    population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                    fitness = np.array([func(x) for x in population])\n                    self.budget -= self.pop_size\n                    self.f_opt = np.min(fitness)\n                    self.x_opt = population[np.argmin(fitness)]\n                    self.last_improvement = self.generation\n                    self.F = 0.5 #reset F\n                    self.network = self.create_scale_free_network()  # Recreate the network\n                else:\n                    # Adaptive F: Reduce mutation strength upon stagnation\n                    self.F *= 0.9  # Reduce F, but prevent it from becoming zero.\n                    self.F = max(self.F, 0.1)\n\n            self.generation += 1\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm ScaleFreeAdaptiveDE scored 0.362 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b80c3dd2-4d19-43e3-84f7-1c95b0b2b2ff"], "operator": null, "metadata": {"aucs": [0.14364609098510905, 0.2653579131629563, 0.3374165025349499, 0.32403352869590596, 0.2808479630645234, 0.3877593407048575, 0.2860719067232066, 0.3026339769621964, 0.2643258774177467, 0.18167941648469588, 0.39340047617895435, 0.9985790724834493, 0.3531650502565067, 0.27276272743403407, 0.699825018565613, 0.3512916616473869, 0.30398363413647567, 0.43479611090337267, 0.1660155875747381, 0.48535095665563255]}}
{"id": "e1fc997b-677b-4fc7-b2a1-078867c26133", "fitness": 0.3230734321850765, "name": "CauchyLocalSearchDE", "description": "Differential Evolution with a dynamically adjusted Cauchy mutation step size and a local search operator triggered by stagnation.", "code": "import numpy as np\n\nclass CauchyLocalSearchDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, Cr=0.9, F_initial=0.5, stagnation_threshold=100, local_search_prob=0.1, cauchy_scale_initial=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.Cr = Cr\n        self.F = F_initial\n        self.stagnation_threshold = stagnation_threshold\n        self.local_search_prob = local_search_prob\n        self.best_fitness_history = []\n        self.last_improvement = 0\n        self.generation = 0\n        self.cauchy_scale = cauchy_scale_initial # Initial scale for Cauchy distribution\n\n    def local_search(self, x, func, scale=0.1):\n        \"\"\"Performs a local search around x using Cauchy mutations.\"\"\"\n        x_new = x + scale * np.random.standard_cauchy(size=self.dim)\n        x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n        f_new = func(x_new)\n        self.budget -=1\n        if f_new < func(x):\n            return x_new, f_new\n        else:\n            return x, func(x)\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n        self.best_fitness_history.append(self.f_opt)\n        self.last_improvement = 0\n        self.generation = 0\n\n        while self.budget > self.pop_size:\n            # Mutation and Crossover\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Cauchy Mutation with adaptive step size\n                indices = [j for j in range(self.pop_size) if j != i]\n                idxs = np.random.choice(indices, size=3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs[0]], population[idxs[1]], population[idxs[2]]\n                \n                mutant = population[i] + self.F * (x_r1 - x_r2) + self.cauchy_scale * np.random.standard_cauchy(size=self.dim) #Cauchy dist\n\n                # Crossover\n                for j in range(self.dim):\n                    if np.random.rand() < self.Cr:\n                        new_population[i, j] = mutant[j]\n                    else:\n                        new_population[i, j] = population[i, j]\n                \n                new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n            \n            # Evaluation\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n            \n            # Selection\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    population[i] = new_population[i]\n                    fitness[i] = new_fitness[i]\n                    \n                    if new_fitness[i] < self.f_opt:\n                        self.f_opt = new_fitness[i]\n                        self.x_opt = new_population[i]\n                        self.last_improvement = self.generation\n                        \n            self.best_fitness_history.append(self.f_opt)\n            \n            # Stagnation check and local search\n            if (self.generation - self.last_improvement) > self.stagnation_threshold:\n                # Reduce Cauchy scale upon stagnation\n                self.cauchy_scale *= 0.8\n                self.cauchy_scale = max(self.cauchy_scale, 0.01) # Ensure it does not become zero\n\n                # Local search around the best solution with probability\n                if np.random.rand() < self.local_search_prob and self.budget > 0:\n                    self.x_opt, self.f_opt = self.local_search(self.x_opt, func)\n                    self.last_improvement = self.generation\n\n            self.generation += 1\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm CauchyLocalSearchDE scored 0.323 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b80c3dd2-4d19-43e3-84f7-1c95b0b2b2ff"], "operator": null, "metadata": {"aucs": [0.13771802972351777, 0.23414109493945778, 0.3042880674481233, 0.29551930914427527, 0.24377968379981463, 0.2908947484607003, 0.26961210803111213, 0.24946362448752224, 0.22712957285008095, 0.17689965526053197, 0.27400590565517347, 0.9997447202760473, 0.2542417592678705, 0.2642311928875706, 0.6714733085694776, 0.30986734014720974, 0.2642688750451685, 0.35387378399653147, 0.17200423728424252, 0.4683116264271021]}}
{"id": "d3415a25-1bee-4eae-8710-c1e72ae3e1da", "fitness": 0.328645825447497, "name": "RepulsiveDE", "description": "A differential evolution strategy that incorporates a repulsive force from the worst performing individuals to guide the population away from unfavorable regions, combined with adaptive parameter control.", "code": "import numpy as np\n\nclass RepulsiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, Cr_init=0.7, repulsion_factor=0.1, adaptation_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.Cr = Cr_init\n        self.repulsion_factor = repulsion_factor\n        self.adaptation_rate = adaptation_rate\n        self.best_fitness_history = []\n        self.stagnation_counter = 0\n        self.last_improvement = 0\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)].copy()\n        self.best_fitness_history.append(self.f_opt)\n        \n        generation = 0\n\n        while self.budget > self.pop_size:\n            # Adaptive Parameter Adjustment\n            if generation > 0:\n                if self.f_opt == self.best_fitness_history[-1]:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0\n            \n            # Dynamically adjust F and Cr\n            if self.stagnation_counter > 50:\n                self.F = min(1.0, self.F * (1 + self.adaptation_rate))  # Increase F for more exploration\n                self.Cr = max(0.0, self.Cr * (1 - self.adaptation_rate))  # Decrease Cr for focused search\n            else:\n                self.F = max(0.1, self.F * (1 - self.adaptation_rate/2))  # Decrease F for exploitation\n                self.Cr = min(0.9, self.Cr * (1 + self.adaptation_rate/2))  # Increase Cr for exploration\n\n            # Mutation and Crossover\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Mutation\n                indices = [j for j in range(self.pop_size) if j != i]\n                idxs = np.random.choice(indices, size=3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs[0]], population[idxs[1]], population[idxs[2]]\n\n                # Repulsion from the worst individual\n                worst_index = np.argmax(fitness)\n                x_worst = population[worst_index]\n\n                mutant = population[i] + self.F * (x_r1 - x_r2) + self.repulsion_factor * (population[i] - x_worst)\n                \n                # Crossover\n                for j in range(self.dim):\n                    if np.random.rand() < self.Cr:\n                        new_population[i, j] = mutant[j]\n                    else:\n                        new_population[i, j] = population[i, j]\n                \n                new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n            \n            # Evaluation\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n            \n            # Selection\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    population[i] = new_population[i]\n                    fitness[i] = new_fitness[i]\n                    \n                    if new_fitness[i] < self.f_opt:\n                        self.f_opt = new_fitness[i]\n                        self.x_opt = new_population[i].copy()  # Deep copy\n                        self.last_improvement = generation\n                        \n            self.best_fitness_history.append(self.f_opt)\n                \n            generation += 1\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm RepulsiveDE scored 0.329 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5f69fa46-56fc-495f-beba-61dcb6765840"], "operator": null, "metadata": {"aucs": [0.1401121702074739, 0.2401449835274494, 0.3105711860923236, 0.2901683277173126, 0.2506913699514697, 0.30783179124249005, 0.28011098927630806, 0.2389285022803631, 0.26453878584247537, 0.1957118993518352, 0.26427598519115814, 0.9980226108947756, 0.28873300813425895, 0.24803674458475589, 0.67249486218771, 0.33528719892043113, 0.2183869375876718, 0.38585539814079306, 0.17694888733671632, 0.4660648704821667]}}
{"id": "a69bd5b5-94b9-48dd-9770-5ddd6f0cb15c", "fitness": 0.4760807515496671, "name": "SelfOrganizingDE", "description": "A self-organizing differential evolution with a dynamic population size and a tournament-based selection mechanism adjusted by success history.", "code": "import numpy as np\n\nclass SelfOrganizingDE:\n    def __init__(self, budget=10000, dim=10, pop_size_init=40, F_init=0.5, Cr_init=0.7, reduction_factor=0.9, expansion_factor=1.1, tournament_size=3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size_init\n        self.F = F_init\n        self.Cr = Cr_init\n        self.reduction_factor = reduction_factor\n        self.expansion_factor = expansion_factor\n        self.tournament_size = tournament_size\n        self.success_history = []\n        self.best_fitness_history = []\n        self.stagnation_counter = 0\n        self.last_improvement = 0\n        self.min_pop_size = 10\n        self.max_pop_size = 100\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)].copy()\n        self.best_fitness_history.append(self.f_opt)\n        \n        generation = 0\n\n        while self.budget > self.min_pop_size:\n            # Adaptive Parameter Adjustment\n            if generation > 0:\n                if self.f_opt == self.best_fitness_history[-1]:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0\n            \n            # Adjust population size dynamically based on success history\n            success_rate = np.mean(self.success_history[-100:]) if len(self.success_history) > 0 else 0.5  # Use 0.5 as default if no history\n            \n            if success_rate > 0.6 and self.pop_size * self.expansion_factor <= self.max_pop_size:\n                self.pop_size = int(min(self.max_pop_size, self.pop_size * self.expansion_factor))\n                new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size - population.shape[0], self.dim))\n                population = np.vstack((population, new_individuals))\n                new_fitness = np.array([func(x) for x in new_individuals])\n                fitness = np.concatenate((fitness, new_fitness))\n                self.budget -= new_individuals.shape[0]\n                \n            elif success_rate < 0.3 and self.pop_size * self.reduction_factor >= self.min_pop_size:\n                self.pop_size = int(max(self.min_pop_size, self.pop_size * self.reduction_factor))\n                indices_to_keep = np.argsort(fitness)[:self.pop_size]\n                population = population[indices_to_keep]\n                fitness = fitness[indices_to_keep]\n\n            # Mutation and Crossover\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n            for i in range(self.pop_size):\n                # Mutation\n                indices = [j for j in range(self.pop_size) if j != i]\n                idxs = np.random.choice(indices, size=3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs[0]], population[idxs[1]], population[idxs[2]]\n                \n                mutant = population[i] + self.F * (x_r1 - x_r2) + self.F * (x_r3 - population[i])\n                \n                # Crossover\n                for j in range(self.dim):\n                    if np.random.rand() < self.Cr:\n                        new_population[i, j] = mutant[j]\n                    else:\n                        new_population[i, j] = population[i, j]\n                \n                new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n            \n            # Evaluation\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n            \n            # Tournament Selection\n            for i in range(self.pop_size):\n                # Tournament selection\n                participants = np.random.choice(self.pop_size, size=self.tournament_size, replace=False)\n                winner = participants[np.argmin(fitness[participants])]\n                \n                if new_fitness[i] < fitness[winner]:\n                    population[winner] = new_population[i]\n                    fitness[winner] = new_fitness[i]\n                    self.success_history.append(1)\n                    \n                    if new_fitness[i] < self.f_opt:\n                        self.f_opt = new_fitness[i]\n                        self.x_opt = new_population[i].copy()\n                        self.last_improvement = generation\n                else:\n                    self.success_history.append(0)\n            \n            self.best_fitness_history.append(self.f_opt)            \n            generation += 1\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm SelfOrganizingDE scored 0.476 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5f69fa46-56fc-495f-beba-61dcb6765840"], "operator": null, "metadata": {"aucs": [0.13450202067751438, 0.31371727506748825, 0.4059674240022252, 0.9057046500965873, 0.30967264390748017, 0.6472590653531998, 0.3569929459401874, 0.361514625222101, 0.24091852513814693, 0.18885732592770588, 0.8940763683980815, 0.9986564414864187, 0.2608866125192305, 0.38013623868420876, 0.9140280019354057, 0.33256765552349665, 0.3238954891608071, 0.8377263483630328, 0.21882387779503687, 0.49571149579498475]}}
{"id": "2acca4fa-9b1e-40a8-94cb-5af51ae00c30", "fitness": 0.40385801343545075, "name": "SelfOrganizingDE", "description": "A differential evolution strategy with a self-organizing population and adaptive mutation based on individual performance, favoring exploration in early stages and exploitation later.", "code": "import numpy as np\n\nclass SelfOrganizingDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_initial=0.7, F_final=0.3, Cr=0.9, adaptation_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_initial = F_initial\n        self.F_final = F_final\n        self.F = F_initial\n        self.Cr = Cr\n        self.adaptation_rate = adaptation_rate\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.individual_successes = np.zeros(self.pop_size) # Track individual successes\n        self.F_values = np.full(self.pop_size, self.F_initial)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            # Adaptive F schedule (linear annealing)\n            remaining_evals = self.budget - self.eval_count\n            progress = 1.0 - (remaining_evals / self.budget)\n            self.F = self.F_initial + (self.F_final - self.F_initial) * progress\n\n            new_population = np.copy(self.population)\n            new_fitness = np.copy(self.fitness)\n\n            for i in range(self.pop_size):\n                # Adaptive F per individual\n                F_i = self.F_values[i]\n\n                indices = [j for j in range(self.pop_size) if j != i]\n                idxs = np.random.choice(indices, size=3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[idxs[0]], self.population[idxs[1]], self.population[idxs[2]]\n\n                mutant = self.population[i] + F_i * (x_r1 - x_r2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                for j in range(self.dim):\n                    if np.random.rand() < self.Cr:\n                        new_population[i, j] = mutant[j]\n                    else:\n                        new_population[i, j] = self.population[i, j]\n\n                new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n\n                # Evaluation\n                f_new = func(new_population[i])\n                self.eval_count += 1\n\n                # Selection\n                if f_new < self.fitness[i]:\n                    new_fitness[i] = f_new\n                    self.individual_successes[i] += 1  # Increment success counter\n                else:\n                    new_population[i] = self.population[i] # Revert if not better\n                    new_fitness[i] = self.fitness[i]\n\n                # Update global best\n                if f_new < self.f_opt:\n                    self.f_opt = f_new\n                    self.x_opt = new_population[i]\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Adapt F values based on individual success\n            for i in range(self.pop_size):\n                if self.individual_successes[i] > 0:\n                    # If successful, reduce F to promote exploitation (but not too much)\n                    self.F_values[i] = max(self.F_final, self.F_values[i] * (1 - self.adaptation_rate))\n                    self.individual_successes[i] = 0 #reset counter\n                else:\n                    # If unsuccessful, increase F to promote exploration\n                    self.F_values[i] = min(self.F_initial, self.F_values[i] * (1 + self.adaptation_rate))\n\n            self.population = new_population\n            self.fitness = new_fitness\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm SelfOrganizingDE scored 0.404 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3070cae7-c5b7-4718-bc5a-ef9a6d1d352e"], "operator": null, "metadata": {"aucs": [0.14585072175253877, 0.261638122308826, 0.35628437604289265, 0.4837568066688068, 0.3010031493959634, 0.447547446584767, 0.2890474854365487, 0.34434057600180634, 0.32636533957353875, 0.19798397183066996, 0.5529390916669283, 0.9931568346507739, 0.3598943996524945, 0.32484734616666033, 0.742546142568044, 0.4052308271002176, 0.32433387425902316, 0.5311991929787576, 0.2007054549035835, 0.4884891091661747]}}
{"id": "ec6d3964-0b6f-4284-9243-9c766bd371f8", "fitness": 0.33615667959532974, "name": "SelfOrganizingDE", "description": "A self-organizing differential evolution algorithm that adaptively adjusts its control parameters and population structure based on individual success rates and a niching strategy to maintain diversity.", "code": "import numpy as np\n\nclass SelfOrganizingDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_initial=0.5, Cr_initial=0.9, lr_F=0.1, lr_Cr=0.1, niche_radius=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_initial = F_initial\n        self.Cr_initial = Cr_initial\n        self.lr_F = lr_F # Learning rate for F\n        self.lr_Cr = lr_Cr # Learning rate for Cr\n        self.niche_radius = niche_radius\n        self.population = None\n        self.fitness = None\n        self.F = None\n        self.Cr = None\n        self.success_F = None\n        self.success_Cr = None\n        self.archive = [] # Archive for storing successful solutions\n\n    def initialize(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.F = np.full(self.pop_size, self.F_initial)\n        self.Cr = np.full(self.pop_size, self.Cr_initial)\n        self.success_F = np.zeros(self.pop_size)\n        self.success_Cr = np.zeros(self.pop_size)\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n    def niching(self):\n        # Simplified niching: penalize individuals that are too close to each other\n        for i in range(self.pop_size):\n            for j in range(i + 1, self.pop_size):\n                if np.linalg.norm(self.population[i] - self.population[j]) < self.niche_radius:\n                    # Penalize the worse individual\n                    if self.fitness[i] > self.fitness[j]:\n                        self.fitness[i] += 0.01 * (self.fitness[i] - self.fitness[j])\n                    else:\n                        self.fitness[j] += 0.01 * (self.fitness[j] - self.fitness[i])\n\n    def __call__(self, func):\n        self.initialize(func)\n\n        while self.budget > self.pop_size:\n            new_population = np.copy(self.population)\n            new_fitness = np.copy(self.fitness)\n\n            for i in range(self.pop_size):\n                # Mutation\n                indices = [j for j in range(self.pop_size) if j != i]\n                idxs = np.random.choice(indices, size=3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[idxs[0]], self.population[idxs[1]], self.population[idxs[2]]\n                mutant = self.population[i] + self.F[i] * (x_r1 - x_r2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                for j in range(self.dim):\n                    if np.random.rand() > self.Cr[i]:\n                        mutant[j] = self.population[i, j]\n\n                # Evaluation\n                f_mutant = func(mutant)\n                self.budget -= 1\n\n                # Selection\n                if f_mutant < self.fitness[i]:\n                    new_population[i] = mutant\n                    new_fitness[i] = f_mutant\n                    # Update success history\n                    self.success_F[i] = 0.9 * self.success_F[i] + 0.1 # Exponential smoothing\n                    self.success_Cr[i] = 0.9 * self.success_Cr[i] + 0.1\n                    \n                    # Archive successful solutions\n                    self.archive.append(self.population[i].copy())  # Store the old solution\n                    if len(self.archive) > 2 * self.pop_size:\n                        self.archive.pop(0) # Maintain archive size\n                        \n                else:\n                    self.success_F[i] = 0.9 * self.success_F[i]\n                    self.success_Cr[i] = 0.9 * self.success_Cr[i]\n\n            # Update population and fitness\n            self.population = new_population\n            self.fitness = new_fitness\n\n            # Self-organizing adaptation of F and Cr\n            self.F = np.clip(self.F + self.lr_F * (self.success_F - 0.5), 0.1, 1.0)\n            self.Cr = np.clip(self.Cr + self.lr_Cr * (self.success_Cr - 0.5), 0.1, 1.0)\n            \n            # Niching strategy (optional but recommended)\n            self.niching()\n            \n            # Update best solution\n            if np.min(self.fitness) < self.f_opt:\n                self.f_opt = np.min(self.fitness)\n                self.x_opt = self.population[np.argmin(self.fitness)]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm SelfOrganizingDE scored 0.336 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3070cae7-c5b7-4718-bc5a-ef9a6d1d352e"], "operator": null, "metadata": {"aucs": [0.10168076904600165, 0.18990846765305136, 0.39193567331712764, 0.324287310239015, 0.2266636889101573, 0.3328968874676972, 0.2615826037718604, 0.3012089278447403, 0.26360040596861034, 0.17111258858590717, 0.3000626455221914, 0.986634313427715, 0.298078440651819, 0.21397437326561142, 0.6788733729432421, 0.33123088967420267, 0.2869624495332982, 0.3695135933975964, 0.2228525509084659, 0.47007363977828387]}}
{"id": "c0023713-f809-4807-9a40-a7f03af83b9b", "fitness": -Infinity, "name": "CooperativeSwarm", "description": "A cooperative swarm optimization algorithm with dynamic sub-swarms and adaptive radius-based exploration.", "code": "import numpy as np\n\nclass CooperativeSwarm:\n    def __init__(self, budget=10000, dim=10, num_swarms=5, swarm_size=10, radius_initial=1.0, radius_final=0.1, inertia=0.7, cognitive_coeff=1.4, social_coeff=1.4):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = num_swarms\n        self.swarm_size = swarm_size\n        self.radius_initial = radius_initial\n        self.radius_final = radius_final\n        self.inertia = inertia\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.swarms = None\n        self.swarm_fitness = None\n        self.swarm_best_positions = None\n        self.swarm_best_fitness = None\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.eval_count = 0\n        self.velocities = None\n\n    def initialize_swarms(self, func):\n        self.swarms = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.num_swarms, self.swarm_size, self.dim))\n        self.velocities = np.zeros_like(self.swarms)\n        self.swarm_fitness = np.zeros((self.num_swarms, self.swarm_size))\n        self.swarm_best_positions = np.copy(self.swarms)\n        self.swarm_best_fitness = np.full((self.num_swarms, self.swarm_size), np.inf)\n\n        for i in range(self.num_swarms):\n            for j in range(self.swarm_size):\n                self.swarm_fitness[i, j] = func(self.swarms[i, j])\n                self.eval_count += 1\n                self.swarm_best_fitness[i, j] = self.swarm_fitness[i, j]\n                if self.swarm_fitness[i, j] < self.global_best_fitness:\n                    self.global_best_fitness = self.swarm_fitness[i, j]\n                    self.global_best_position = np.copy(self.swarms[i, j])\n\n    def __call__(self, func):\n        self.initialize_swarms(func)\n\n        while self.eval_count < self.budget:\n            # Adaptive Radius\n            remaining_evals = self.budget - self.eval_count\n            progress = 1.0 - (remaining_evals / self.budget)\n            radius = self.radius_initial + (self.radius_final - self.radius_initial) * progress\n\n            for i in range(self.num_swarms):\n                # Find local best in the swarm\n                local_best_index = np.argmin(self.swarm_best_fitness[i])\n                local_best_position = self.swarm_best_positions[i, local_best_index]\n\n                for j in range(self.swarm_size):\n                    # Update velocity\n                    r1 = np.random.rand(self.dim)\n                    r2 = np.random.rand(self.dim)\n                    self.velocities[i, j] = (self.inertia * self.velocities[i, j] +\n                                            self.cognitive_coeff * r1 * (self.swarm_best_positions[i, j] - self.swarms[i, j]) +\n                                            self.social_coeff * r2 * (local_best_position - self.swarms[i, j]))\n\n                    # Update position with radius-based exploration\n                    self.swarms[i, j] = self.swarms[i, j] + self.velocities[i, j]\n                    \n                    # Radius-based exploration: occasionally explore within a shrinking radius\n                    if np.random.rand() < 0.1:  # Probability of exploration\n                        exploration_vector = np.random.uniform(-radius, radius, size=self.dim)\n                        self.swarms[i, j] = self.swarm_best_positions[i, j] + exploration_vector\n                    \n                    self.swarms[i, j] = np.clip(self.swarms[i, j], func.bounds.lb, func.bounds.ub)\n\n\n                    # Evaluate new position\n                    fitness = func(self.swarms[i, j])\n                    self.eval_count += 1\n\n                    # Update personal best\n                    if fitness < self.swarm_best_fitness[i, j]:\n                        self.swarm_best_fitness[i, j] = fitness\n                        self.swarm_best_positions[i, j] = np.copy(self.swarms[i, j])\n\n                    # Update global best\n                    if fitness < self.global_best_fitness:\n                        self.global_best_fitness = fitness\n                        self.global_best_position = np.copy(self.swarms[i, j])\n                    \n                    if self.eval_count >= self.budget:\n                        break\n                if self.eval_count >= self.budget:\n                    break\n\n            # Dynamic sub-swarm merging (optional, but might help)\n            if self.num_swarms > 1 and self.eval_count % (self.budget // 10) == 0:\n                # Periodically merge the two closest swarms based on distance of their best particles\n                swarm_distances = np.zeros((self.num_swarms, self.num_swarms))\n                for s1 in range(self.num_swarms):\n                    for s2 in range(s1 + 1, self.num_swarms):\n                        swarm_distances[s1, s2] = np.linalg.norm(self.swarm_best_positions[s1, np.argmin(self.swarm_best_fitness[s1])] - self.swarm_best_positions[s2, np.argmin(self.swarm_best_fitness[s2])])\n                        swarm_distances[s2, s1] = swarm_distances[s1, s2]\n                \n                s1, s2 = np.unravel_index(np.argmin(swarm_distances, axis=None), swarm_distances.shape)\n                \n                # Merge swarm s2 into s1: append s2 particles to s1 and remove s2\n                self.swarms[s1] = np.concatenate((self.swarms[s1], self.swarms[s2]), axis=0)\n                self.swarm_fitness[s1] = np.concatenate((self.swarm_fitness[s1], self.swarm_fitness[s2]), axis=0)\n                self.swarm_best_positions[s1] = np.concatenate((self.swarm_best_positions[s1], self.swarm_best_positions[s2]), axis=0)\n                self.swarm_best_fitness[s1] = np.concatenate((self.swarm_best_fitness[s1], self.swarm_best_fitness[s2]), axis=0)\n                \n                # Re-evaluate merged swarm\n                for k in range(self.swarms[s1].shape[0]):\n                    fitness = func(self.swarms[s1][k])\n                    self.eval_count += 1\n                    self.swarm_fitness[s1][k] = fitness\n                    if fitness < self.swarm_best_fitness[s1][k]:\n                        self.swarm_best_fitness[s1][k] = fitness\n                        self.swarm_best_positions[s1][k] = np.copy(self.swarms[s1][k])\n                    if fitness < self.global_best_fitness:\n                        self.global_best_fitness = fitness\n                        self.global_best_position = np.copy(self.swarms[s1][k])\n                    if self.eval_count >= self.budget:\n                        break\n                if self.eval_count >= self.budget:\n                    break\n\n                # Reduce number of swarms\n                self.num_swarms -=1\n                # Remove swarm s2 from other swarm arrays\n                indices = [x for x in range(self.swarms.shape[0]) if x != s2]\n                self.swarms = self.swarms[indices]\n                self.swarm_fitness = self.swarm_fitness[indices]\n                self.swarm_best_positions = self.swarm_best_positions[indices]\n                self.swarm_best_fitness = self.swarm_best_fitness[indices]\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 6, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 103, in __call__\nValueError: could not broadcast input array from shape (20,2) into shape (10,2)\n.", "error": "", "parent_ids": ["2acca4fa-9b1e-40a8-94cb-5af51ae00c30"], "operator": null, "metadata": {}}
{"id": "c0023713-f809-4807-9a40-a7f03af83b9b", "fitness": -Infinity, "name": "CooperativeSwarm", "description": "A cooperative swarm optimization algorithm with dynamic sub-swarms and adaptive radius-based exploration.", "code": "import numpy as np\n\nclass CooperativeSwarm:\n    def __init__(self, budget=10000, dim=10, num_swarms=5, swarm_size=10, radius_initial=1.0, radius_final=0.1, inertia=0.7, cognitive_coeff=1.4, social_coeff=1.4):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = num_swarms\n        self.swarm_size = swarm_size\n        self.radius_initial = radius_initial\n        self.radius_final = radius_final\n        self.inertia = inertia\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.swarms = None\n        self.swarm_fitness = None\n        self.swarm_best_positions = None\n        self.swarm_best_fitness = None\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.eval_count = 0\n        self.velocities = None\n\n    def initialize_swarms(self, func):\n        self.swarms = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.num_swarms, self.swarm_size, self.dim))\n        self.velocities = np.zeros_like(self.swarms)\n        self.swarm_fitness = np.zeros((self.num_swarms, self.swarm_size))\n        self.swarm_best_positions = np.copy(self.swarms)\n        self.swarm_best_fitness = np.full((self.num_swarms, self.swarm_size), np.inf)\n\n        for i in range(self.num_swarms):\n            for j in range(self.swarm_size):\n                self.swarm_fitness[i, j] = func(self.swarms[i, j])\n                self.eval_count += 1\n                self.swarm_best_fitness[i, j] = self.swarm_fitness[i, j]\n                if self.swarm_fitness[i, j] < self.global_best_fitness:\n                    self.global_best_fitness = self.swarm_fitness[i, j]\n                    self.global_best_position = np.copy(self.swarms[i, j])\n\n    def __call__(self, func):\n        self.initialize_swarms(func)\n\n        while self.eval_count < self.budget:\n            # Adaptive Radius\n            remaining_evals = self.budget - self.eval_count\n            progress = 1.0 - (remaining_evals / self.budget)\n            radius = self.radius_initial + (self.radius_final - self.radius_initial) * progress\n\n            for i in range(self.num_swarms):\n                # Find local best in the swarm\n                local_best_index = np.argmin(self.swarm_best_fitness[i])\n                local_best_position = self.swarm_best_positions[i, local_best_index]\n\n                for j in range(self.swarm_size):\n                    # Update velocity\n                    r1 = np.random.rand(self.dim)\n                    r2 = np.random.rand(self.dim)\n                    self.velocities[i, j] = (self.inertia * self.velocities[i, j] +\n                                            self.cognitive_coeff * r1 * (self.swarm_best_positions[i, j] - self.swarms[i, j]) +\n                                            self.social_coeff * r2 * (local_best_position - self.swarms[i, j]))\n\n                    # Update position with radius-based exploration\n                    self.swarms[i, j] = self.swarms[i, j] + self.velocities[i, j]\n                    \n                    # Radius-based exploration: occasionally explore within a shrinking radius\n                    if np.random.rand() < 0.1:  # Probability of exploration\n                        exploration_vector = np.random.uniform(-radius, radius, size=self.dim)\n                        self.swarms[i, j] = self.swarm_best_positions[i, j] + exploration_vector\n                    \n                    self.swarms[i, j] = np.clip(self.swarms[i, j], func.bounds.lb, func.bounds.ub)\n\n\n                    # Evaluate new position\n                    fitness = func(self.swarms[i, j])\n                    self.eval_count += 1\n\n                    # Update personal best\n                    if fitness < self.swarm_best_fitness[i, j]:\n                        self.swarm_best_fitness[i, j] = fitness\n                        self.swarm_best_positions[i, j] = np.copy(self.swarms[i, j])\n\n                    # Update global best\n                    if fitness < self.global_best_fitness:\n                        self.global_best_fitness = fitness\n                        self.global_best_position = np.copy(self.swarms[i, j])\n                    \n                    if self.eval_count >= self.budget:\n                        break\n                if self.eval_count >= self.budget:\n                    break\n\n            # Dynamic sub-swarm merging (optional, but might help)\n            if self.num_swarms > 1 and self.eval_count % (self.budget // 10) == 0:\n                # Periodically merge the two closest swarms based on distance of their best particles\n                swarm_distances = np.zeros((self.num_swarms, self.num_swarms))\n                for s1 in range(self.num_swarms):\n                    for s2 in range(s1 + 1, self.num_swarms):\n                        swarm_distances[s1, s2] = np.linalg.norm(self.swarm_best_positions[s1, np.argmin(self.swarm_best_fitness[s1])] - self.swarm_best_positions[s2, np.argmin(self.swarm_best_fitness[s2])])\n                        swarm_distances[s2, s1] = swarm_distances[s1, s2]\n                \n                s1, s2 = np.unravel_index(np.argmin(swarm_distances, axis=None), swarm_distances.shape)\n                \n                # Merge swarm s2 into s1: append s2 particles to s1 and remove s2\n                self.swarms[s1] = np.concatenate((self.swarms[s1], self.swarms[s2]), axis=0)\n                self.swarm_fitness[s1] = np.concatenate((self.swarm_fitness[s1], self.swarm_fitness[s2]), axis=0)\n                self.swarm_best_positions[s1] = np.concatenate((self.swarm_best_positions[s1], self.swarm_best_positions[s2]), axis=0)\n                self.swarm_best_fitness[s1] = np.concatenate((self.swarm_best_fitness[s1], self.swarm_best_fitness[s2]), axis=0)\n                \n                # Re-evaluate merged swarm\n                for k in range(self.swarms[s1].shape[0]):\n                    fitness = func(self.swarms[s1][k])\n                    self.eval_count += 1\n                    self.swarm_fitness[s1][k] = fitness\n                    if fitness < self.swarm_best_fitness[s1][k]:\n                        self.swarm_best_fitness[s1][k] = fitness\n                        self.swarm_best_positions[s1][k] = np.copy(self.swarms[s1][k])\n                    if fitness < self.global_best_fitness:\n                        self.global_best_fitness = fitness\n                        self.global_best_position = np.copy(self.swarms[s1][k])\n                    if self.eval_count >= self.budget:\n                        break\n                if self.eval_count >= self.budget:\n                    break\n\n                # Reduce number of swarms\n                self.num_swarms -=1\n                # Remove swarm s2 from other swarm arrays\n                indices = [x for x in range(self.swarms.shape[0]) if x != s2]\n                self.swarms = self.swarms[indices]\n                self.swarm_fitness = self.swarm_fitness[indices]\n                self.swarm_best_positions = self.swarm_best_positions[indices]\n                self.swarm_best_fitness = self.swarm_best_fitness[indices]\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 6, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 103, in __call__\nValueError: could not broadcast input array from shape (20,2) into shape (10,2)\n.", "error": "", "parent_ids": ["2acca4fa-9b1e-40a8-94cb-5af51ae00c30"], "operator": null, "metadata": {}}
{"id": "c0023713-f809-4807-9a40-a7f03af83b9b", "fitness": -Infinity, "name": "CooperativeSwarm", "description": "A cooperative swarm optimization algorithm with dynamic sub-swarms and adaptive radius-based exploration.", "code": "import numpy as np\n\nclass CooperativeSwarm:\n    def __init__(self, budget=10000, dim=10, num_swarms=5, swarm_size=10, radius_initial=1.0, radius_final=0.1, inertia=0.7, cognitive_coeff=1.4, social_coeff=1.4):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = num_swarms\n        self.swarm_size = swarm_size\n        self.radius_initial = radius_initial\n        self.radius_final = radius_final\n        self.inertia = inertia\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.swarms = None\n        self.swarm_fitness = None\n        self.swarm_best_positions = None\n        self.swarm_best_fitness = None\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.eval_count = 0\n        self.velocities = None\n\n    def initialize_swarms(self, func):\n        self.swarms = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.num_swarms, self.swarm_size, self.dim))\n        self.velocities = np.zeros_like(self.swarms)\n        self.swarm_fitness = np.zeros((self.num_swarms, self.swarm_size))\n        self.swarm_best_positions = np.copy(self.swarms)\n        self.swarm_best_fitness = np.full((self.num_swarms, self.swarm_size), np.inf)\n\n        for i in range(self.num_swarms):\n            for j in range(self.swarm_size):\n                self.swarm_fitness[i, j] = func(self.swarms[i, j])\n                self.eval_count += 1\n                self.swarm_best_fitness[i, j] = self.swarm_fitness[i, j]\n                if self.swarm_fitness[i, j] < self.global_best_fitness:\n                    self.global_best_fitness = self.swarm_fitness[i, j]\n                    self.global_best_position = np.copy(self.swarms[i, j])\n\n    def __call__(self, func):\n        self.initialize_swarms(func)\n\n        while self.eval_count < self.budget:\n            # Adaptive Radius\n            remaining_evals = self.budget - self.eval_count\n            progress = 1.0 - (remaining_evals / self.budget)\n            radius = self.radius_initial + (self.radius_final - self.radius_initial) * progress\n\n            for i in range(self.num_swarms):\n                # Find local best in the swarm\n                local_best_index = np.argmin(self.swarm_best_fitness[i])\n                local_best_position = self.swarm_best_positions[i, local_best_index]\n\n                for j in range(self.swarm_size):\n                    # Update velocity\n                    r1 = np.random.rand(self.dim)\n                    r2 = np.random.rand(self.dim)\n                    self.velocities[i, j] = (self.inertia * self.velocities[i, j] +\n                                            self.cognitive_coeff * r1 * (self.swarm_best_positions[i, j] - self.swarms[i, j]) +\n                                            self.social_coeff * r2 * (local_best_position - self.swarms[i, j]))\n\n                    # Update position with radius-based exploration\n                    self.swarms[i, j] = self.swarms[i, j] + self.velocities[i, j]\n                    \n                    # Radius-based exploration: occasionally explore within a shrinking radius\n                    if np.random.rand() < 0.1:  # Probability of exploration\n                        exploration_vector = np.random.uniform(-radius, radius, size=self.dim)\n                        self.swarms[i, j] = self.swarm_best_positions[i, j] + exploration_vector\n                    \n                    self.swarms[i, j] = np.clip(self.swarms[i, j], func.bounds.lb, func.bounds.ub)\n\n\n                    # Evaluate new position\n                    fitness = func(self.swarms[i, j])\n                    self.eval_count += 1\n\n                    # Update personal best\n                    if fitness < self.swarm_best_fitness[i, j]:\n                        self.swarm_best_fitness[i, j] = fitness\n                        self.swarm_best_positions[i, j] = np.copy(self.swarms[i, j])\n\n                    # Update global best\n                    if fitness < self.global_best_fitness:\n                        self.global_best_fitness = fitness\n                        self.global_best_position = np.copy(self.swarms[i, j])\n                    \n                    if self.eval_count >= self.budget:\n                        break\n                if self.eval_count >= self.budget:\n                    break\n\n            # Dynamic sub-swarm merging (optional, but might help)\n            if self.num_swarms > 1 and self.eval_count % (self.budget // 10) == 0:\n                # Periodically merge the two closest swarms based on distance of their best particles\n                swarm_distances = np.zeros((self.num_swarms, self.num_swarms))\n                for s1 in range(self.num_swarms):\n                    for s2 in range(s1 + 1, self.num_swarms):\n                        swarm_distances[s1, s2] = np.linalg.norm(self.swarm_best_positions[s1, np.argmin(self.swarm_best_fitness[s1])] - self.swarm_best_positions[s2, np.argmin(self.swarm_best_fitness[s2])])\n                        swarm_distances[s2, s1] = swarm_distances[s1, s2]\n                \n                s1, s2 = np.unravel_index(np.argmin(swarm_distances, axis=None), swarm_distances.shape)\n                \n                # Merge swarm s2 into s1: append s2 particles to s1 and remove s2\n                self.swarms[s1] = np.concatenate((self.swarms[s1], self.swarms[s2]), axis=0)\n                self.swarm_fitness[s1] = np.concatenate((self.swarm_fitness[s1], self.swarm_fitness[s2]), axis=0)\n                self.swarm_best_positions[s1] = np.concatenate((self.swarm_best_positions[s1], self.swarm_best_positions[s2]), axis=0)\n                self.swarm_best_fitness[s1] = np.concatenate((self.swarm_best_fitness[s1], self.swarm_best_fitness[s2]), axis=0)\n                \n                # Re-evaluate merged swarm\n                for k in range(self.swarms[s1].shape[0]):\n                    fitness = func(self.swarms[s1][k])\n                    self.eval_count += 1\n                    self.swarm_fitness[s1][k] = fitness\n                    if fitness < self.swarm_best_fitness[s1][k]:\n                        self.swarm_best_fitness[s1][k] = fitness\n                        self.swarm_best_positions[s1][k] = np.copy(self.swarms[s1][k])\n                    if fitness < self.global_best_fitness:\n                        self.global_best_fitness = fitness\n                        self.global_best_position = np.copy(self.swarms[s1][k])\n                    if self.eval_count >= self.budget:\n                        break\n                if self.eval_count >= self.budget:\n                    break\n\n                # Reduce number of swarms\n                self.num_swarms -=1\n                # Remove swarm s2 from other swarm arrays\n                indices = [x for x in range(self.swarms.shape[0]) if x != s2]\n                self.swarms = self.swarms[indices]\n                self.swarm_fitness = self.swarm_fitness[indices]\n                self.swarm_best_positions = self.swarm_best_positions[indices]\n                self.swarm_best_fitness = self.swarm_best_fitness[indices]\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 6, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 103, in __call__\nValueError: could not broadcast input array from shape (20,2) into shape (10,2)\n.", "error": "", "parent_ids": ["2acca4fa-9b1e-40a8-94cb-5af51ae00c30"], "operator": null, "metadata": {}}
{"id": "c0023713-f809-4807-9a40-a7f03af83b9b", "fitness": -Infinity, "name": "CooperativeSwarm", "description": "A cooperative swarm optimization algorithm with dynamic sub-swarms and adaptive radius-based exploration.", "code": "import numpy as np\n\nclass CooperativeSwarm:\n    def __init__(self, budget=10000, dim=10, num_swarms=5, swarm_size=10, radius_initial=1.0, radius_final=0.1, inertia=0.7, cognitive_coeff=1.4, social_coeff=1.4):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = num_swarms\n        self.swarm_size = swarm_size\n        self.radius_initial = radius_initial\n        self.radius_final = radius_final\n        self.inertia = inertia\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.swarms = None\n        self.swarm_fitness = None\n        self.swarm_best_positions = None\n        self.swarm_best_fitness = None\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.eval_count = 0\n        self.velocities = None\n\n    def initialize_swarms(self, func):\n        self.swarms = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.num_swarms, self.swarm_size, self.dim))\n        self.velocities = np.zeros_like(self.swarms)\n        self.swarm_fitness = np.zeros((self.num_swarms, self.swarm_size))\n        self.swarm_best_positions = np.copy(self.swarms)\n        self.swarm_best_fitness = np.full((self.num_swarms, self.swarm_size), np.inf)\n\n        for i in range(self.num_swarms):\n            for j in range(self.swarm_size):\n                self.swarm_fitness[i, j] = func(self.swarms[i, j])\n                self.eval_count += 1\n                self.swarm_best_fitness[i, j] = self.swarm_fitness[i, j]\n                if self.swarm_fitness[i, j] < self.global_best_fitness:\n                    self.global_best_fitness = self.swarm_fitness[i, j]\n                    self.global_best_position = np.copy(self.swarms[i, j])\n\n    def __call__(self, func):\n        self.initialize_swarms(func)\n\n        while self.eval_count < self.budget:\n            # Adaptive Radius\n            remaining_evals = self.budget - self.eval_count\n            progress = 1.0 - (remaining_evals / self.budget)\n            radius = self.radius_initial + (self.radius_final - self.radius_initial) * progress\n\n            for i in range(self.num_swarms):\n                # Find local best in the swarm\n                local_best_index = np.argmin(self.swarm_best_fitness[i])\n                local_best_position = self.swarm_best_positions[i, local_best_index]\n\n                for j in range(self.swarm_size):\n                    # Update velocity\n                    r1 = np.random.rand(self.dim)\n                    r2 = np.random.rand(self.dim)\n                    self.velocities[i, j] = (self.inertia * self.velocities[i, j] +\n                                            self.cognitive_coeff * r1 * (self.swarm_best_positions[i, j] - self.swarms[i, j]) +\n                                            self.social_coeff * r2 * (local_best_position - self.swarms[i, j]))\n\n                    # Update position with radius-based exploration\n                    self.swarms[i, j] = self.swarms[i, j] + self.velocities[i, j]\n                    \n                    # Radius-based exploration: occasionally explore within a shrinking radius\n                    if np.random.rand() < 0.1:  # Probability of exploration\n                        exploration_vector = np.random.uniform(-radius, radius, size=self.dim)\n                        self.swarms[i, j] = self.swarm_best_positions[i, j] + exploration_vector\n                    \n                    self.swarms[i, j] = np.clip(self.swarms[i, j], func.bounds.lb, func.bounds.ub)\n\n\n                    # Evaluate new position\n                    fitness = func(self.swarms[i, j])\n                    self.eval_count += 1\n\n                    # Update personal best\n                    if fitness < self.swarm_best_fitness[i, j]:\n                        self.swarm_best_fitness[i, j] = fitness\n                        self.swarm_best_positions[i, j] = np.copy(self.swarms[i, j])\n\n                    # Update global best\n                    if fitness < self.global_best_fitness:\n                        self.global_best_fitness = fitness\n                        self.global_best_position = np.copy(self.swarms[i, j])\n                    \n                    if self.eval_count >= self.budget:\n                        break\n                if self.eval_count >= self.budget:\n                    break\n\n            # Dynamic sub-swarm merging (optional, but might help)\n            if self.num_swarms > 1 and self.eval_count % (self.budget // 10) == 0:\n                # Periodically merge the two closest swarms based on distance of their best particles\n                swarm_distances = np.zeros((self.num_swarms, self.num_swarms))\n                for s1 in range(self.num_swarms):\n                    for s2 in range(s1 + 1, self.num_swarms):\n                        swarm_distances[s1, s2] = np.linalg.norm(self.swarm_best_positions[s1, np.argmin(self.swarm_best_fitness[s1])] - self.swarm_best_positions[s2, np.argmin(self.swarm_best_fitness[s2])])\n                        swarm_distances[s2, s1] = swarm_distances[s1, s2]\n                \n                s1, s2 = np.unravel_index(np.argmin(swarm_distances, axis=None), swarm_distances.shape)\n                \n                # Merge swarm s2 into s1: append s2 particles to s1 and remove s2\n                self.swarms[s1] = np.concatenate((self.swarms[s1], self.swarms[s2]), axis=0)\n                self.swarm_fitness[s1] = np.concatenate((self.swarm_fitness[s1], self.swarm_fitness[s2]), axis=0)\n                self.swarm_best_positions[s1] = np.concatenate((self.swarm_best_positions[s1], self.swarm_best_positions[s2]), axis=0)\n                self.swarm_best_fitness[s1] = np.concatenate((self.swarm_best_fitness[s1], self.swarm_best_fitness[s2]), axis=0)\n                \n                # Re-evaluate merged swarm\n                for k in range(self.swarms[s1].shape[0]):\n                    fitness = func(self.swarms[s1][k])\n                    self.eval_count += 1\n                    self.swarm_fitness[s1][k] = fitness\n                    if fitness < self.swarm_best_fitness[s1][k]:\n                        self.swarm_best_fitness[s1][k] = fitness\n                        self.swarm_best_positions[s1][k] = np.copy(self.swarms[s1][k])\n                    if fitness < self.global_best_fitness:\n                        self.global_best_fitness = fitness\n                        self.global_best_position = np.copy(self.swarms[s1][k])\n                    if self.eval_count >= self.budget:\n                        break\n                if self.eval_count >= self.budget:\n                    break\n\n                # Reduce number of swarms\n                self.num_swarms -=1\n                # Remove swarm s2 from other swarm arrays\n                indices = [x for x in range(self.swarms.shape[0]) if x != s2]\n                self.swarms = self.swarms[indices]\n                self.swarm_fitness = self.swarm_fitness[indices]\n                self.swarm_best_positions = self.swarm_best_positions[indices]\n                self.swarm_best_fitness = self.swarm_best_fitness[indices]\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 6, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 103, in __call__\nValueError: could not broadcast input array from shape (20,2) into shape (10,2)\n.", "error": "", "parent_ids": ["2acca4fa-9b1e-40a8-94cb-5af51ae00c30"], "operator": null, "metadata": {}}
{"id": "c0023713-f809-4807-9a40-a7f03af83b9b", "fitness": -Infinity, "name": "CooperativeSwarm", "description": "A cooperative swarm optimization algorithm with dynamic sub-swarms and adaptive radius-based exploration.", "code": "import numpy as np\n\nclass CooperativeSwarm:\n    def __init__(self, budget=10000, dim=10, num_swarms=5, swarm_size=10, radius_initial=1.0, radius_final=0.1, inertia=0.7, cognitive_coeff=1.4, social_coeff=1.4):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = num_swarms\n        self.swarm_size = swarm_size\n        self.radius_initial = radius_initial\n        self.radius_final = radius_final\n        self.inertia = inertia\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.swarms = None\n        self.swarm_fitness = None\n        self.swarm_best_positions = None\n        self.swarm_best_fitness = None\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.eval_count = 0\n        self.velocities = None\n\n    def initialize_swarms(self, func):\n        self.swarms = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.num_swarms, self.swarm_size, self.dim))\n        self.velocities = np.zeros_like(self.swarms)\n        self.swarm_fitness = np.zeros((self.num_swarms, self.swarm_size))\n        self.swarm_best_positions = np.copy(self.swarms)\n        self.swarm_best_fitness = np.full((self.num_swarms, self.swarm_size), np.inf)\n\n        for i in range(self.num_swarms):\n            for j in range(self.swarm_size):\n                self.swarm_fitness[i, j] = func(self.swarms[i, j])\n                self.eval_count += 1\n                self.swarm_best_fitness[i, j] = self.swarm_fitness[i, j]\n                if self.swarm_fitness[i, j] < self.global_best_fitness:\n                    self.global_best_fitness = self.swarm_fitness[i, j]\n                    self.global_best_position = np.copy(self.swarms[i, j])\n\n    def __call__(self, func):\n        self.initialize_swarms(func)\n\n        while self.eval_count < self.budget:\n            # Adaptive Radius\n            remaining_evals = self.budget - self.eval_count\n            progress = 1.0 - (remaining_evals / self.budget)\n            radius = self.radius_initial + (self.radius_final - self.radius_initial) * progress\n\n            for i in range(self.num_swarms):\n                # Find local best in the swarm\n                local_best_index = np.argmin(self.swarm_best_fitness[i])\n                local_best_position = self.swarm_best_positions[i, local_best_index]\n\n                for j in range(self.swarm_size):\n                    # Update velocity\n                    r1 = np.random.rand(self.dim)\n                    r2 = np.random.rand(self.dim)\n                    self.velocities[i, j] = (self.inertia * self.velocities[i, j] +\n                                            self.cognitive_coeff * r1 * (self.swarm_best_positions[i, j] - self.swarms[i, j]) +\n                                            self.social_coeff * r2 * (local_best_position - self.swarms[i, j]))\n\n                    # Update position with radius-based exploration\n                    self.swarms[i, j] = self.swarms[i, j] + self.velocities[i, j]\n                    \n                    # Radius-based exploration: occasionally explore within a shrinking radius\n                    if np.random.rand() < 0.1:  # Probability of exploration\n                        exploration_vector = np.random.uniform(-radius, radius, size=self.dim)\n                        self.swarms[i, j] = self.swarm_best_positions[i, j] + exploration_vector\n                    \n                    self.swarms[i, j] = np.clip(self.swarms[i, j], func.bounds.lb, func.bounds.ub)\n\n\n                    # Evaluate new position\n                    fitness = func(self.swarms[i, j])\n                    self.eval_count += 1\n\n                    # Update personal best\n                    if fitness < self.swarm_best_fitness[i, j]:\n                        self.swarm_best_fitness[i, j] = fitness\n                        self.swarm_best_positions[i, j] = np.copy(self.swarms[i, j])\n\n                    # Update global best\n                    if fitness < self.global_best_fitness:\n                        self.global_best_fitness = fitness\n                        self.global_best_position = np.copy(self.swarms[i, j])\n                    \n                    if self.eval_count >= self.budget:\n                        break\n                if self.eval_count >= self.budget:\n                    break\n\n            # Dynamic sub-swarm merging (optional, but might help)\n            if self.num_swarms > 1 and self.eval_count % (self.budget // 10) == 0:\n                # Periodically merge the two closest swarms based on distance of their best particles\n                swarm_distances = np.zeros((self.num_swarms, self.num_swarms))\n                for s1 in range(self.num_swarms):\n                    for s2 in range(s1 + 1, self.num_swarms):\n                        swarm_distances[s1, s2] = np.linalg.norm(self.swarm_best_positions[s1, np.argmin(self.swarm_best_fitness[s1])] - self.swarm_best_positions[s2, np.argmin(self.swarm_best_fitness[s2])])\n                        swarm_distances[s2, s1] = swarm_distances[s1, s2]\n                \n                s1, s2 = np.unravel_index(np.argmin(swarm_distances, axis=None), swarm_distances.shape)\n                \n                # Merge swarm s2 into s1: append s2 particles to s1 and remove s2\n                self.swarms[s1] = np.concatenate((self.swarms[s1], self.swarms[s2]), axis=0)\n                self.swarm_fitness[s1] = np.concatenate((self.swarm_fitness[s1], self.swarm_fitness[s2]), axis=0)\n                self.swarm_best_positions[s1] = np.concatenate((self.swarm_best_positions[s1], self.swarm_best_positions[s2]), axis=0)\n                self.swarm_best_fitness[s1] = np.concatenate((self.swarm_best_fitness[s1], self.swarm_best_fitness[s2]), axis=0)\n                \n                # Re-evaluate merged swarm\n                for k in range(self.swarms[s1].shape[0]):\n                    fitness = func(self.swarms[s1][k])\n                    self.eval_count += 1\n                    self.swarm_fitness[s1][k] = fitness\n                    if fitness < self.swarm_best_fitness[s1][k]:\n                        self.swarm_best_fitness[s1][k] = fitness\n                        self.swarm_best_positions[s1][k] = np.copy(self.swarms[s1][k])\n                    if fitness < self.global_best_fitness:\n                        self.global_best_fitness = fitness\n                        self.global_best_position = np.copy(self.swarms[s1][k])\n                    if self.eval_count >= self.budget:\n                        break\n                if self.eval_count >= self.budget:\n                    break\n\n                # Reduce number of swarms\n                self.num_swarms -=1\n                # Remove swarm s2 from other swarm arrays\n                indices = [x for x in range(self.swarms.shape[0]) if x != s2]\n                self.swarms = self.swarms[indices]\n                self.swarm_fitness = self.swarm_fitness[indices]\n                self.swarm_best_positions = self.swarm_best_positions[indices]\n                self.swarm_best_fitness = self.swarm_best_fitness[indices]\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 6, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 103, in __call__\nValueError: could not broadcast input array from shape (20,2) into shape (10,2)\n.", "error": "", "parent_ids": ["2acca4fa-9b1e-40a8-94cb-5af51ae00c30"], "operator": null, "metadata": {}}
{"id": "c0023713-f809-4807-9a40-a7f03af83b9b", "fitness": -Infinity, "name": "CooperativeSwarm", "description": "A cooperative swarm optimization algorithm with dynamic sub-swarms and adaptive radius-based exploration.", "code": "import numpy as np\n\nclass CooperativeSwarm:\n    def __init__(self, budget=10000, dim=10, num_swarms=5, swarm_size=10, radius_initial=1.0, radius_final=0.1, inertia=0.7, cognitive_coeff=1.4, social_coeff=1.4):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = num_swarms\n        self.swarm_size = swarm_size\n        self.radius_initial = radius_initial\n        self.radius_final = radius_final\n        self.inertia = inertia\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.swarms = None\n        self.swarm_fitness = None\n        self.swarm_best_positions = None\n        self.swarm_best_fitness = None\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.eval_count = 0\n        self.velocities = None\n\n    def initialize_swarms(self, func):\n        self.swarms = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.num_swarms, self.swarm_size, self.dim))\n        self.velocities = np.zeros_like(self.swarms)\n        self.swarm_fitness = np.zeros((self.num_swarms, self.swarm_size))\n        self.swarm_best_positions = np.copy(self.swarms)\n        self.swarm_best_fitness = np.full((self.num_swarms, self.swarm_size), np.inf)\n\n        for i in range(self.num_swarms):\n            for j in range(self.swarm_size):\n                self.swarm_fitness[i, j] = func(self.swarms[i, j])\n                self.eval_count += 1\n                self.swarm_best_fitness[i, j] = self.swarm_fitness[i, j]\n                if self.swarm_fitness[i, j] < self.global_best_fitness:\n                    self.global_best_fitness = self.swarm_fitness[i, j]\n                    self.global_best_position = np.copy(self.swarms[i, j])\n\n    def __call__(self, func):\n        self.initialize_swarms(func)\n\n        while self.eval_count < self.budget:\n            # Adaptive Radius\n            remaining_evals = self.budget - self.eval_count\n            progress = 1.0 - (remaining_evals / self.budget)\n            radius = self.radius_initial + (self.radius_final - self.radius_initial) * progress\n\n            for i in range(self.num_swarms):\n                # Find local best in the swarm\n                local_best_index = np.argmin(self.swarm_best_fitness[i])\n                local_best_position = self.swarm_best_positions[i, local_best_index]\n\n                for j in range(self.swarm_size):\n                    # Update velocity\n                    r1 = np.random.rand(self.dim)\n                    r2 = np.random.rand(self.dim)\n                    self.velocities[i, j] = (self.inertia * self.velocities[i, j] +\n                                            self.cognitive_coeff * r1 * (self.swarm_best_positions[i, j] - self.swarms[i, j]) +\n                                            self.social_coeff * r2 * (local_best_position - self.swarms[i, j]))\n\n                    # Update position with radius-based exploration\n                    self.swarms[i, j] = self.swarms[i, j] + self.velocities[i, j]\n                    \n                    # Radius-based exploration: occasionally explore within a shrinking radius\n                    if np.random.rand() < 0.1:  # Probability of exploration\n                        exploration_vector = np.random.uniform(-radius, radius, size=self.dim)\n                        self.swarms[i, j] = self.swarm_best_positions[i, j] + exploration_vector\n                    \n                    self.swarms[i, j] = np.clip(self.swarms[i, j], func.bounds.lb, func.bounds.ub)\n\n\n                    # Evaluate new position\n                    fitness = func(self.swarms[i, j])\n                    self.eval_count += 1\n\n                    # Update personal best\n                    if fitness < self.swarm_best_fitness[i, j]:\n                        self.swarm_best_fitness[i, j] = fitness\n                        self.swarm_best_positions[i, j] = np.copy(self.swarms[i, j])\n\n                    # Update global best\n                    if fitness < self.global_best_fitness:\n                        self.global_best_fitness = fitness\n                        self.global_best_position = np.copy(self.swarms[i, j])\n                    \n                    if self.eval_count >= self.budget:\n                        break\n                if self.eval_count >= self.budget:\n                    break\n\n            # Dynamic sub-swarm merging (optional, but might help)\n            if self.num_swarms > 1 and self.eval_count % (self.budget // 10) == 0:\n                # Periodically merge the two closest swarms based on distance of their best particles\n                swarm_distances = np.zeros((self.num_swarms, self.num_swarms))\n                for s1 in range(self.num_swarms):\n                    for s2 in range(s1 + 1, self.num_swarms):\n                        swarm_distances[s1, s2] = np.linalg.norm(self.swarm_best_positions[s1, np.argmin(self.swarm_best_fitness[s1])] - self.swarm_best_positions[s2, np.argmin(self.swarm_best_fitness[s2])])\n                        swarm_distances[s2, s1] = swarm_distances[s1, s2]\n                \n                s1, s2 = np.unravel_index(np.argmin(swarm_distances, axis=None), swarm_distances.shape)\n                \n                # Merge swarm s2 into s1: append s2 particles to s1 and remove s2\n                self.swarms[s1] = np.concatenate((self.swarms[s1], self.swarms[s2]), axis=0)\n                self.swarm_fitness[s1] = np.concatenate((self.swarm_fitness[s1], self.swarm_fitness[s2]), axis=0)\n                self.swarm_best_positions[s1] = np.concatenate((self.swarm_best_positions[s1], self.swarm_best_positions[s2]), axis=0)\n                self.swarm_best_fitness[s1] = np.concatenate((self.swarm_best_fitness[s1], self.swarm_best_fitness[s2]), axis=0)\n                \n                # Re-evaluate merged swarm\n                for k in range(self.swarms[s1].shape[0]):\n                    fitness = func(self.swarms[s1][k])\n                    self.eval_count += 1\n                    self.swarm_fitness[s1][k] = fitness\n                    if fitness < self.swarm_best_fitness[s1][k]:\n                        self.swarm_best_fitness[s1][k] = fitness\n                        self.swarm_best_positions[s1][k] = np.copy(self.swarms[s1][k])\n                    if fitness < self.global_best_fitness:\n                        self.global_best_fitness = fitness\n                        self.global_best_position = np.copy(self.swarms[s1][k])\n                    if self.eval_count >= self.budget:\n                        break\n                if self.eval_count >= self.budget:\n                    break\n\n                # Reduce number of swarms\n                self.num_swarms -=1\n                # Remove swarm s2 from other swarm arrays\n                indices = [x for x in range(self.swarms.shape[0]) if x != s2]\n                self.swarms = self.swarms[indices]\n                self.swarm_fitness = self.swarm_fitness[indices]\n                self.swarm_best_positions = self.swarm_best_positions[indices]\n                self.swarm_best_fitness = self.swarm_best_fitness[indices]\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 6, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 103, in __call__\nValueError: could not broadcast input array from shape (20,2) into shape (10,2)\n.", "error": "", "parent_ids": ["2acca4fa-9b1e-40a8-94cb-5af51ae00c30"], "operator": null, "metadata": {}}
{"id": "c0023713-f809-4807-9a40-a7f03af83b9b", "fitness": -Infinity, "name": "CooperativeSwarm", "description": "A cooperative swarm optimization algorithm with dynamic sub-swarms and adaptive radius-based exploration.", "code": "import numpy as np\n\nclass CooperativeSwarm:\n    def __init__(self, budget=10000, dim=10, num_swarms=5, swarm_size=10, radius_initial=1.0, radius_final=0.1, inertia=0.7, cognitive_coeff=1.4, social_coeff=1.4):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = num_swarms\n        self.swarm_size = swarm_size\n        self.radius_initial = radius_initial\n        self.radius_final = radius_final\n        self.inertia = inertia\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.swarms = None\n        self.swarm_fitness = None\n        self.swarm_best_positions = None\n        self.swarm_best_fitness = None\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.eval_count = 0\n        self.velocities = None\n\n    def initialize_swarms(self, func):\n        self.swarms = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.num_swarms, self.swarm_size, self.dim))\n        self.velocities = np.zeros_like(self.swarms)\n        self.swarm_fitness = np.zeros((self.num_swarms, self.swarm_size))\n        self.swarm_best_positions = np.copy(self.swarms)\n        self.swarm_best_fitness = np.full((self.num_swarms, self.swarm_size), np.inf)\n\n        for i in range(self.num_swarms):\n            for j in range(self.swarm_size):\n                self.swarm_fitness[i, j] = func(self.swarms[i, j])\n                self.eval_count += 1\n                self.swarm_best_fitness[i, j] = self.swarm_fitness[i, j]\n                if self.swarm_fitness[i, j] < self.global_best_fitness:\n                    self.global_best_fitness = self.swarm_fitness[i, j]\n                    self.global_best_position = np.copy(self.swarms[i, j])\n\n    def __call__(self, func):\n        self.initialize_swarms(func)\n\n        while self.eval_count < self.budget:\n            # Adaptive Radius\n            remaining_evals = self.budget - self.eval_count\n            progress = 1.0 - (remaining_evals / self.budget)\n            radius = self.radius_initial + (self.radius_final - self.radius_initial) * progress\n\n            for i in range(self.num_swarms):\n                # Find local best in the swarm\n                local_best_index = np.argmin(self.swarm_best_fitness[i])\n                local_best_position = self.swarm_best_positions[i, local_best_index]\n\n                for j in range(self.swarm_size):\n                    # Update velocity\n                    r1 = np.random.rand(self.dim)\n                    r2 = np.random.rand(self.dim)\n                    self.velocities[i, j] = (self.inertia * self.velocities[i, j] +\n                                            self.cognitive_coeff * r1 * (self.swarm_best_positions[i, j] - self.swarms[i, j]) +\n                                            self.social_coeff * r2 * (local_best_position - self.swarms[i, j]))\n\n                    # Update position with radius-based exploration\n                    self.swarms[i, j] = self.swarms[i, j] + self.velocities[i, j]\n                    \n                    # Radius-based exploration: occasionally explore within a shrinking radius\n                    if np.random.rand() < 0.1:  # Probability of exploration\n                        exploration_vector = np.random.uniform(-radius, radius, size=self.dim)\n                        self.swarms[i, j] = self.swarm_best_positions[i, j] + exploration_vector\n                    \n                    self.swarms[i, j] = np.clip(self.swarms[i, j], func.bounds.lb, func.bounds.ub)\n\n\n                    # Evaluate new position\n                    fitness = func(self.swarms[i, j])\n                    self.eval_count += 1\n\n                    # Update personal best\n                    if fitness < self.swarm_best_fitness[i, j]:\n                        self.swarm_best_fitness[i, j] = fitness\n                        self.swarm_best_positions[i, j] = np.copy(self.swarms[i, j])\n\n                    # Update global best\n                    if fitness < self.global_best_fitness:\n                        self.global_best_fitness = fitness\n                        self.global_best_position = np.copy(self.swarms[i, j])\n                    \n                    if self.eval_count >= self.budget:\n                        break\n                if self.eval_count >= self.budget:\n                    break\n\n            # Dynamic sub-swarm merging (optional, but might help)\n            if self.num_swarms > 1 and self.eval_count % (self.budget // 10) == 0:\n                # Periodically merge the two closest swarms based on distance of their best particles\n                swarm_distances = np.zeros((self.num_swarms, self.num_swarms))\n                for s1 in range(self.num_swarms):\n                    for s2 in range(s1 + 1, self.num_swarms):\n                        swarm_distances[s1, s2] = np.linalg.norm(self.swarm_best_positions[s1, np.argmin(self.swarm_best_fitness[s1])] - self.swarm_best_positions[s2, np.argmin(self.swarm_best_fitness[s2])])\n                        swarm_distances[s2, s1] = swarm_distances[s1, s2]\n                \n                s1, s2 = np.unravel_index(np.argmin(swarm_distances, axis=None), swarm_distances.shape)\n                \n                # Merge swarm s2 into s1: append s2 particles to s1 and remove s2\n                self.swarms[s1] = np.concatenate((self.swarms[s1], self.swarms[s2]), axis=0)\n                self.swarm_fitness[s1] = np.concatenate((self.swarm_fitness[s1], self.swarm_fitness[s2]), axis=0)\n                self.swarm_best_positions[s1] = np.concatenate((self.swarm_best_positions[s1], self.swarm_best_positions[s2]), axis=0)\n                self.swarm_best_fitness[s1] = np.concatenate((self.swarm_best_fitness[s1], self.swarm_best_fitness[s2]), axis=0)\n                \n                # Re-evaluate merged swarm\n                for k in range(self.swarms[s1].shape[0]):\n                    fitness = func(self.swarms[s1][k])\n                    self.eval_count += 1\n                    self.swarm_fitness[s1][k] = fitness\n                    if fitness < self.swarm_best_fitness[s1][k]:\n                        self.swarm_best_fitness[s1][k] = fitness\n                        self.swarm_best_positions[s1][k] = np.copy(self.swarms[s1][k])\n                    if fitness < self.global_best_fitness:\n                        self.global_best_fitness = fitness\n                        self.global_best_position = np.copy(self.swarms[s1][k])\n                    if self.eval_count >= self.budget:\n                        break\n                if self.eval_count >= self.budget:\n                    break\n\n                # Reduce number of swarms\n                self.num_swarms -=1\n                # Remove swarm s2 from other swarm arrays\n                indices = [x for x in range(self.swarms.shape[0]) if x != s2]\n                self.swarms = self.swarms[indices]\n                self.swarm_fitness = self.swarm_fitness[indices]\n                self.swarm_best_positions = self.swarm_best_positions[indices]\n                self.swarm_best_fitness = self.swarm_best_fitness[indices]\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 6, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 103, in __call__\nValueError: could not broadcast input array from shape (20,2) into shape (10,2)\n.", "error": "", "parent_ids": ["2acca4fa-9b1e-40a8-94cb-5af51ae00c30"], "operator": null, "metadata": {}}
{"id": "c0023713-f809-4807-9a40-a7f03af83b9b", "fitness": -Infinity, "name": "CooperativeSwarm", "description": "A cooperative swarm optimization algorithm with dynamic sub-swarms and adaptive radius-based exploration.", "code": "import numpy as np\n\nclass CooperativeSwarm:\n    def __init__(self, budget=10000, dim=10, num_swarms=5, swarm_size=10, radius_initial=1.0, radius_final=0.1, inertia=0.7, cognitive_coeff=1.4, social_coeff=1.4):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = num_swarms\n        self.swarm_size = swarm_size\n        self.radius_initial = radius_initial\n        self.radius_final = radius_final\n        self.inertia = inertia\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.swarms = None\n        self.swarm_fitness = None\n        self.swarm_best_positions = None\n        self.swarm_best_fitness = None\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.eval_count = 0\n        self.velocities = None\n\n    def initialize_swarms(self, func):\n        self.swarms = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.num_swarms, self.swarm_size, self.dim))\n        self.velocities = np.zeros_like(self.swarms)\n        self.swarm_fitness = np.zeros((self.num_swarms, self.swarm_size))\n        self.swarm_best_positions = np.copy(self.swarms)\n        self.swarm_best_fitness = np.full((self.num_swarms, self.swarm_size), np.inf)\n\n        for i in range(self.num_swarms):\n            for j in range(self.swarm_size):\n                self.swarm_fitness[i, j] = func(self.swarms[i, j])\n                self.eval_count += 1\n                self.swarm_best_fitness[i, j] = self.swarm_fitness[i, j]\n                if self.swarm_fitness[i, j] < self.global_best_fitness:\n                    self.global_best_fitness = self.swarm_fitness[i, j]\n                    self.global_best_position = np.copy(self.swarms[i, j])\n\n    def __call__(self, func):\n        self.initialize_swarms(func)\n\n        while self.eval_count < self.budget:\n            # Adaptive Radius\n            remaining_evals = self.budget - self.eval_count\n            progress = 1.0 - (remaining_evals / self.budget)\n            radius = self.radius_initial + (self.radius_final - self.radius_initial) * progress\n\n            for i in range(self.num_swarms):\n                # Find local best in the swarm\n                local_best_index = np.argmin(self.swarm_best_fitness[i])\n                local_best_position = self.swarm_best_positions[i, local_best_index]\n\n                for j in range(self.swarm_size):\n                    # Update velocity\n                    r1 = np.random.rand(self.dim)\n                    r2 = np.random.rand(self.dim)\n                    self.velocities[i, j] = (self.inertia * self.velocities[i, j] +\n                                            self.cognitive_coeff * r1 * (self.swarm_best_positions[i, j] - self.swarms[i, j]) +\n                                            self.social_coeff * r2 * (local_best_position - self.swarms[i, j]))\n\n                    # Update position with radius-based exploration\n                    self.swarms[i, j] = self.swarms[i, j] + self.velocities[i, j]\n                    \n                    # Radius-based exploration: occasionally explore within a shrinking radius\n                    if np.random.rand() < 0.1:  # Probability of exploration\n                        exploration_vector = np.random.uniform(-radius, radius, size=self.dim)\n                        self.swarms[i, j] = self.swarm_best_positions[i, j] + exploration_vector\n                    \n                    self.swarms[i, j] = np.clip(self.swarms[i, j], func.bounds.lb, func.bounds.ub)\n\n\n                    # Evaluate new position\n                    fitness = func(self.swarms[i, j])\n                    self.eval_count += 1\n\n                    # Update personal best\n                    if fitness < self.swarm_best_fitness[i, j]:\n                        self.swarm_best_fitness[i, j] = fitness\n                        self.swarm_best_positions[i, j] = np.copy(self.swarms[i, j])\n\n                    # Update global best\n                    if fitness < self.global_best_fitness:\n                        self.global_best_fitness = fitness\n                        self.global_best_position = np.copy(self.swarms[i, j])\n                    \n                    if self.eval_count >= self.budget:\n                        break\n                if self.eval_count >= self.budget:\n                    break\n\n            # Dynamic sub-swarm merging (optional, but might help)\n            if self.num_swarms > 1 and self.eval_count % (self.budget // 10) == 0:\n                # Periodically merge the two closest swarms based on distance of their best particles\n                swarm_distances = np.zeros((self.num_swarms, self.num_swarms))\n                for s1 in range(self.num_swarms):\n                    for s2 in range(s1 + 1, self.num_swarms):\n                        swarm_distances[s1, s2] = np.linalg.norm(self.swarm_best_positions[s1, np.argmin(self.swarm_best_fitness[s1])] - self.swarm_best_positions[s2, np.argmin(self.swarm_best_fitness[s2])])\n                        swarm_distances[s2, s1] = swarm_distances[s1, s2]\n                \n                s1, s2 = np.unravel_index(np.argmin(swarm_distances, axis=None), swarm_distances.shape)\n                \n                # Merge swarm s2 into s1: append s2 particles to s1 and remove s2\n                self.swarms[s1] = np.concatenate((self.swarms[s1], self.swarms[s2]), axis=0)\n                self.swarm_fitness[s1] = np.concatenate((self.swarm_fitness[s1], self.swarm_fitness[s2]), axis=0)\n                self.swarm_best_positions[s1] = np.concatenate((self.swarm_best_positions[s1], self.swarm_best_positions[s2]), axis=0)\n                self.swarm_best_fitness[s1] = np.concatenate((self.swarm_best_fitness[s1], self.swarm_best_fitness[s2]), axis=0)\n                \n                # Re-evaluate merged swarm\n                for k in range(self.swarms[s1].shape[0]):\n                    fitness = func(self.swarms[s1][k])\n                    self.eval_count += 1\n                    self.swarm_fitness[s1][k] = fitness\n                    if fitness < self.swarm_best_fitness[s1][k]:\n                        self.swarm_best_fitness[s1][k] = fitness\n                        self.swarm_best_positions[s1][k] = np.copy(self.swarms[s1][k])\n                    if fitness < self.global_best_fitness:\n                        self.global_best_fitness = fitness\n                        self.global_best_position = np.copy(self.swarms[s1][k])\n                    if self.eval_count >= self.budget:\n                        break\n                if self.eval_count >= self.budget:\n                    break\n\n                # Reduce number of swarms\n                self.num_swarms -=1\n                # Remove swarm s2 from other swarm arrays\n                indices = [x for x in range(self.swarms.shape[0]) if x != s2]\n                self.swarms = self.swarms[indices]\n                self.swarm_fitness = self.swarm_fitness[indices]\n                self.swarm_best_positions = self.swarm_best_positions[indices]\n                self.swarm_best_fitness = self.swarm_best_fitness[indices]\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 6, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 103, in __call__\nValueError: could not broadcast input array from shape (20,2) into shape (10,2)\n.", "error": "", "parent_ids": ["2acca4fa-9b1e-40a8-94cb-5af51ae00c30"], "operator": null, "metadata": {}}
{"id": "c0023713-f809-4807-9a40-a7f03af83b9b", "fitness": -Infinity, "name": "CooperativeSwarm", "description": "A cooperative swarm optimization algorithm with dynamic sub-swarms and adaptive radius-based exploration.", "code": "import numpy as np\n\nclass CooperativeSwarm:\n    def __init__(self, budget=10000, dim=10, num_swarms=5, swarm_size=10, radius_initial=1.0, radius_final=0.1, inertia=0.7, cognitive_coeff=1.4, social_coeff=1.4):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = num_swarms\n        self.swarm_size = swarm_size\n        self.radius_initial = radius_initial\n        self.radius_final = radius_final\n        self.inertia = inertia\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.swarms = None\n        self.swarm_fitness = None\n        self.swarm_best_positions = None\n        self.swarm_best_fitness = None\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.eval_count = 0\n        self.velocities = None\n\n    def initialize_swarms(self, func):\n        self.swarms = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.num_swarms, self.swarm_size, self.dim))\n        self.velocities = np.zeros_like(self.swarms)\n        self.swarm_fitness = np.zeros((self.num_swarms, self.swarm_size))\n        self.swarm_best_positions = np.copy(self.swarms)\n        self.swarm_best_fitness = np.full((self.num_swarms, self.swarm_size), np.inf)\n\n        for i in range(self.num_swarms):\n            for j in range(self.swarm_size):\n                self.swarm_fitness[i, j] = func(self.swarms[i, j])\n                self.eval_count += 1\n                self.swarm_best_fitness[i, j] = self.swarm_fitness[i, j]\n                if self.swarm_fitness[i, j] < self.global_best_fitness:\n                    self.global_best_fitness = self.swarm_fitness[i, j]\n                    self.global_best_position = np.copy(self.swarms[i, j])\n\n    def __call__(self, func):\n        self.initialize_swarms(func)\n\n        while self.eval_count < self.budget:\n            # Adaptive Radius\n            remaining_evals = self.budget - self.eval_count\n            progress = 1.0 - (remaining_evals / self.budget)\n            radius = self.radius_initial + (self.radius_final - self.radius_initial) * progress\n\n            for i in range(self.num_swarms):\n                # Find local best in the swarm\n                local_best_index = np.argmin(self.swarm_best_fitness[i])\n                local_best_position = self.swarm_best_positions[i, local_best_index]\n\n                for j in range(self.swarm_size):\n                    # Update velocity\n                    r1 = np.random.rand(self.dim)\n                    r2 = np.random.rand(self.dim)\n                    self.velocities[i, j] = (self.inertia * self.velocities[i, j] +\n                                            self.cognitive_coeff * r1 * (self.swarm_best_positions[i, j] - self.swarms[i, j]) +\n                                            self.social_coeff * r2 * (local_best_position - self.swarms[i, j]))\n\n                    # Update position with radius-based exploration\n                    self.swarms[i, j] = self.swarms[i, j] + self.velocities[i, j]\n                    \n                    # Radius-based exploration: occasionally explore within a shrinking radius\n                    if np.random.rand() < 0.1:  # Probability of exploration\n                        exploration_vector = np.random.uniform(-radius, radius, size=self.dim)\n                        self.swarms[i, j] = self.swarm_best_positions[i, j] + exploration_vector\n                    \n                    self.swarms[i, j] = np.clip(self.swarms[i, j], func.bounds.lb, func.bounds.ub)\n\n\n                    # Evaluate new position\n                    fitness = func(self.swarms[i, j])\n                    self.eval_count += 1\n\n                    # Update personal best\n                    if fitness < self.swarm_best_fitness[i, j]:\n                        self.swarm_best_fitness[i, j] = fitness\n                        self.swarm_best_positions[i, j] = np.copy(self.swarms[i, j])\n\n                    # Update global best\n                    if fitness < self.global_best_fitness:\n                        self.global_best_fitness = fitness\n                        self.global_best_position = np.copy(self.swarms[i, j])\n                    \n                    if self.eval_count >= self.budget:\n                        break\n                if self.eval_count >= self.budget:\n                    break\n\n            # Dynamic sub-swarm merging (optional, but might help)\n            if self.num_swarms > 1 and self.eval_count % (self.budget // 10) == 0:\n                # Periodically merge the two closest swarms based on distance of their best particles\n                swarm_distances = np.zeros((self.num_swarms, self.num_swarms))\n                for s1 in range(self.num_swarms):\n                    for s2 in range(s1 + 1, self.num_swarms):\n                        swarm_distances[s1, s2] = np.linalg.norm(self.swarm_best_positions[s1, np.argmin(self.swarm_best_fitness[s1])] - self.swarm_best_positions[s2, np.argmin(self.swarm_best_fitness[s2])])\n                        swarm_distances[s2, s1] = swarm_distances[s1, s2]\n                \n                s1, s2 = np.unravel_index(np.argmin(swarm_distances, axis=None), swarm_distances.shape)\n                \n                # Merge swarm s2 into s1: append s2 particles to s1 and remove s2\n                self.swarms[s1] = np.concatenate((self.swarms[s1], self.swarms[s2]), axis=0)\n                self.swarm_fitness[s1] = np.concatenate((self.swarm_fitness[s1], self.swarm_fitness[s2]), axis=0)\n                self.swarm_best_positions[s1] = np.concatenate((self.swarm_best_positions[s1], self.swarm_best_positions[s2]), axis=0)\n                self.swarm_best_fitness[s1] = np.concatenate((self.swarm_best_fitness[s1], self.swarm_best_fitness[s2]), axis=0)\n                \n                # Re-evaluate merged swarm\n                for k in range(self.swarms[s1].shape[0]):\n                    fitness = func(self.swarms[s1][k])\n                    self.eval_count += 1\n                    self.swarm_fitness[s1][k] = fitness\n                    if fitness < self.swarm_best_fitness[s1][k]:\n                        self.swarm_best_fitness[s1][k] = fitness\n                        self.swarm_best_positions[s1][k] = np.copy(self.swarms[s1][k])\n                    if fitness < self.global_best_fitness:\n                        self.global_best_fitness = fitness\n                        self.global_best_position = np.copy(self.swarms[s1][k])\n                    if self.eval_count >= self.budget:\n                        break\n                if self.eval_count >= self.budget:\n                    break\n\n                # Reduce number of swarms\n                self.num_swarms -=1\n                # Remove swarm s2 from other swarm arrays\n                indices = [x for x in range(self.swarms.shape[0]) if x != s2]\n                self.swarms = self.swarms[indices]\n                self.swarm_fitness = self.swarm_fitness[indices]\n                self.swarm_best_positions = self.swarm_best_positions[indices]\n                self.swarm_best_fitness = self.swarm_best_fitness[indices]\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 6, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 103, in __call__\nValueError: could not broadcast input array from shape (20,2) into shape (10,2)\n.", "error": "", "parent_ids": ["2acca4fa-9b1e-40a8-94cb-5af51ae00c30"], "operator": null, "metadata": {}}
{"id": "1265d49a-2a53-4b2e-beb2-01766b66734a", "fitness": 0.36429165779178013, "name": "RingCauchyAdaptiveDE", "description": "An adaptive differential evolution with a ring topology and Cauchy mutation, combined with a migration strategy based on fitness rank and periodic local search to enhance exploitation.", "code": "import numpy as np\n\nclass RingCauchyAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, Cr=0.9, F_initial=0.5, stagnation_threshold=100, restart_prob=0.1, local_search_prob=0.1, migration_interval=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.Cr = Cr\n        self.F = F_initial\n        self.stagnation_threshold = stagnation_threshold\n        self.restart_prob = restart_prob\n        self.local_search_prob = local_search_prob\n        self.migration_interval = migration_interval\n        self.best_fitness_history = []\n        self.last_improvement = 0\n        self.generation = 0\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n        self.best_fitness_history.append(self.f_opt)\n        self.last_improvement = 0\n        self.generation = 0\n\n        while self.budget > self.pop_size:\n            # Mutation and Crossover\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Ring Topology based Mutation (Cauchy Mutation)\n                idx_prev = (i - 1) % self.pop_size\n                idx_next = (i + 1) % self.pop_size\n                \n                # Cauchy mutation\n                cauchy_noise = self.F * np.random.standard_cauchy(size=self.dim)\n                mutant = population[i] + cauchy_noise\n\n                # Crossover\n                for j in range(self.dim):\n                    if np.random.rand() < self.Cr:\n                        new_population[i, j] = mutant[j]\n                    else:\n                        new_population[i, j] = population[i, j]\n                \n                new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n            \n            # Evaluation\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n            \n            # Selection\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    population[i] = new_population[i]\n                    fitness[i] = new_fitness[i]\n                    \n                    if new_fitness[i] < self.f_opt:\n                        self.f_opt = new_fitness[i]\n                        self.x_opt = new_population[i]\n                        self.last_improvement = self.generation\n                        \n            self.best_fitness_history.append(self.f_opt)\n            \n            # Stagnation check and restart\n            if (self.generation - self.last_improvement) > self.stagnation_threshold:\n                if np.random.rand() < self.restart_prob:\n                    # Restart the population\n                    population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                    fitness = np.array([func(x) for x in population])\n                    self.budget -= self.pop_size\n                    self.f_opt = np.min(fitness)\n                    self.x_opt = population[np.argmin(fitness)]\n                    self.last_improvement = self.generation\n                    self.F = 0.5 #reset F\n                else:\n                    # Adaptive F: Reduce mutation strength upon stagnation\n                    self.F *= 0.9  # Reduce F, but prevent it from becoming zero.\n                    self.F = max(self.F, 0.1)\n            \n            # Migration strategy\n            if self.generation % self.migration_interval == 0:\n                ranked_indices = np.argsort(fitness)\n                # Replace worst individuals with slightly perturbed best individuals\n                for i in range(self.pop_size // 4): #Migrate 25% of population\n                    worst_idx = ranked_indices[self.pop_size - 1 - i]\n                    best_idx = ranked_indices[0]\n                    population[worst_idx] = population[best_idx] + 0.05 * np.random.normal(0, 1, self.dim)\n                    population[worst_idx] = np.clip(population[worst_idx], func.bounds.lb, func.bounds.ub)\n                    fitness[worst_idx] = func(population[worst_idx])\n                    self.budget -= 1\n\n                    if fitness[worst_idx] < self.f_opt:\n                        self.f_opt = fitness[worst_idx]\n                        self.x_opt = population[worst_idx]\n                        self.last_improvement = self.generation\n\n            # Local Search\n            if np.random.rand() < self.local_search_prob:\n                idx = np.random.randint(0, self.pop_size)\n                # Apply small perturbation to the selected individual\n                population[idx] = population[idx] + 0.01 * np.random.normal(0, 1, self.dim)\n                population[idx] = np.clip(population[idx], func.bounds.lb, func.bounds.ub)\n                fitness[idx] = func(population[idx])\n                self.budget -= 1\n\n                if fitness[idx] < self.f_opt:\n                    self.f_opt = fitness[idx]\n                    self.x_opt = population[idx]\n                    self.last_improvement = self.generation\n                    \n            self.generation += 1\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm RingCauchyAdaptiveDE scored 0.364 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["fc29000f-e32e-4d16-9a85-819f85fe7566"], "operator": null, "metadata": {"aucs": [0.14807799907194608, 0.26368125677616416, 0.33962699003645735, 0.40696733286916276, 0.2595355487603034, 0.3282409910513019, 0.2805096495104046, 0.31511739303850383, 0.26393054250661363, 0.20408294044857134, 0.48215524334568693, 0.999733637772526, 0.28808010718528787, 0.27199880377421937, 0.7100673733991318, 0.33907920794337754, 0.2936419589325867, 0.37455830682798774, 0.227095669856632, 0.48965220272873855]}}
{"id": "3b12931a-ae0c-4f5c-8819-429d06372e0e", "fitness": 0.4189979015239647, "name": "DynamicRestartDE", "description": "A differential evolution strategy with a dynamically adjusted mutation factor and a restart mechanism when stagnation is detected, aiming to escape local optima.", "code": "import numpy as np\n\nclass DynamicRestartDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_initial=0.5, Cr=0.9, stagnation_threshold=50, restart_probability=0.1, F_decay=0.99):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_initial\n        self.Cr = Cr\n        self.stagnation_threshold = stagnation_threshold\n        self.restart_probability = restart_probability\n        self.F_decay = F_decay\n        self.best_fitness_history = []\n        self.last_improvement = 0\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n        self.best_fitness_history.append(self.f_opt)\n        \n        generation = 0\n\n        while self.budget > self.pop_size:\n            # Mutation and Crossover\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                indices = [j for j in range(self.pop_size) if j != i]\n                idxs = np.random.choice(indices, size=3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs[0]], population[idxs[1]], population[idxs[2]]\n                \n                mutant = population[i] + self.F * (x_r1 - x_r2)\n                \n                # Crossover\n                for j in range(self.dim):\n                    if np.random.rand() < self.Cr:\n                        new_population[i, j] = mutant[j]\n                    \n                new_population[i] = np.clip(new_population[i], self.lb, self.ub)\n\n            # Evaluation\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n\n            # Selection\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    population[i] = new_population[i]\n                    fitness[i] = new_fitness[i]\n                    \n                    if new_fitness[i] < self.f_opt:\n                        self.f_opt = new_fitness[i]\n                        self.x_opt = new_population[i]\n                        self.last_improvement = generation\n                        \n            self.best_fitness_history.append(self.f_opt)\n\n            # Stagnation Check and Restart Mechanism\n            if (generation - self.last_improvement) > self.stagnation_threshold:\n                if np.random.rand() < self.restart_probability:\n                    # Restart a portion of the population\n                    num_to_restart = int(self.pop_size * 0.25)  # Restart 25% of the population\n                    indices_to_restart = np.random.choice(self.pop_size, size=num_to_restart, replace=False)\n                    population[indices_to_restart] = np.random.uniform(self.lb, self.ub, size=(num_to_restart, self.dim))\n                    fitness[indices_to_restart] = np.array([func(x) for x in population[indices_to_restart]])\n                    self.budget -= num_to_restart\n                    \n                    # Decay mutation factor\n                    self.F *= self.F_decay\n                    \n                    # Update best solution\n                    min_fitness_index = np.argmin(fitness)\n                    if fitness[min_fitness_index] < self.f_opt:\n                        self.f_opt = fitness[min_fitness_index]\n                        self.x_opt = population[min_fitness_index]\n                    \n                    self.last_improvement = generation # Reset last improvement to current generation\n\n\n            generation += 1\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm DynamicRestartDE scored 0.419 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["415b5a97-623e-4699-8461-e97521e27d87"], "operator": null, "metadata": {"aucs": [0.1542734198871083, 0.3001909330309702, 0.38056944017448957, 0.5733408032266001, 0.3189627329997551, 0.4432443109056021, 0.2974184416084168, 0.339020161589364, 0.31287199358189566, 0.19681979657822368, 0.5796231870454194, 0.9903558473940939, 0.3985386829885804, 0.3146216241296135, 0.7793071897139634, 0.4496530351445326, 0.33177896929988826, 0.5589821413411362, 0.17665124518624253, 0.48373407465339757]}}
{"id": "de67fb59-bb06-43e9-b59d-5e706467c835", "fitness": 0.6006599201115286, "name": "RingTopologyAdaptiveDE", "description": "An adaptive differential evolution with a ring topology and self-adaptive mutation and crossover rates based on the success of recent generations.", "code": "import numpy as np\n\nclass RingTopologyAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, initial_Cr=0.5, initial_F=0.7, learning_rate=0.1, ring_neighborhood=3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.Cr = np.full(pop_size, initial_Cr)  # Individual crossover rates\n        self.F = np.full(pop_size, initial_F)  # Individual mutation factors\n        self.learning_rate = learning_rate\n        self.ring_neighborhood = ring_neighborhood\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.generation = 0\n        self.success_Cr = np.zeros(pop_size)\n        self.success_F = np.zeros(pop_size)\n        self.success_count = np.zeros(pop_size)\n        self.min_F = 0.1\n        self.max_F = 1.0\n        self.epsilon = 1e-6\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n    def evolve(self, func):\n        new_population = np.copy(self.population)\n        new_fitness = np.copy(self.fitness)\n\n        for i in range(self.pop_size):\n            # Ring Topology Selection: Select neighbors\n            neighbors = [(i + j) % self.pop_size for j in range(-self.ring_neighborhood, self.ring_neighborhood + 1) if j != 0]\n            \n            # Mutation: Use the best neighbor within the ring\n            best_neighbor_idx = neighbors[np.argmin(self.fitness[neighbors])]\n            \n            available_indices = [idx for idx in range(self.pop_size) if idx not in [i, best_neighbor_idx]]\n            if len(available_indices) < 2:\n                # Handle edge cases (very small population)\n                r1, r2 = np.random.choice(self.pop_size, 2, replace=False)\n                while r1 == i or r2 == i:\n                     r1, r2 = np.random.choice(self.pop_size, 2, replace=False)\n            else:    \n                r1, r2 = np.random.choice(available_indices, 2, replace=False)  # Ensure r1 and r2 are different\n            \n            mutant = self.population[i] + self.F[i] * (self.population[best_neighbor_idx] - self.population[i]) + self.F[i] * (self.population[r1] - self.population[r2])\n\n            # Crossover\n            for j in range(self.dim):\n                if np.random.rand() < self.Cr[i]:\n                    new_population[i, j] = mutant[j]\n                else:\n                    new_population[i, j] = self.population[i, j]\n\n            new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n\n            # Evaluation\n            new_fitness[i] = func(new_population[i])\n            self.budget -= 1\n\n            # Selection and Parameter Adaptation\n            if new_fitness[i] < self.fitness[i]:\n                self.success_Cr[i] = self.Cr[i]\n                self.success_F[i] = self.F[i]\n                self.success_count[i] += 1\n                self.population[i] = new_population[i]\n                self.fitness[i] = new_fitness[i]\n\n                if new_fitness[i] < self.f_opt:\n                    self.f_opt = new_fitness[i]\n                    self.x_opt = new_population[i]\n\n        # Update Cr and F values: Adaptation based on success history\n        for i in range(self.pop_size):\n            if self.success_count[i] > 0:\n                # Update Cr and F based on successful values\n                self.Cr[i] = (1 - self.learning_rate) * self.Cr[i] + self.learning_rate * self.success_Cr[i]\n                self.F[i] = (1 - self.learning_rate) * self.F[i] + self.learning_rate * self.success_F[i]\n\n                # Reset success counters\n                self.success_Cr[i] = 0.0\n                self.success_F[i] = 0.0\n                self.success_count[i] = 0\n\n            # Apply bounds\n            self.F[i] = np.clip(self.F[i], self.min_F, self.max_F)\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        while self.budget > self.pop_size:\n            self.evolve(func)\n            self.generation += 1\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm RingTopologyAdaptiveDE scored 0.601 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["fc29000f-e32e-4d16-9a85-819f85fe7566"], "operator": null, "metadata": {"aucs": [0.19533988254251167, 0.3197936114833835, 0.6411139976240007, 0.840884922669986, 0.6983891125774527, 0.7937788824018276, 0.47973459297486354, 0.5723433594614686, 0.7394787805728309, 0.38638668681828603, 0.8074835202484136, 0.9988036659752226, 0.2742264715338011, 0.511672292145938, 0.8904999923016459, 0.8082664760283326, 0.4967016103738666, 0.8323295788367159, 0.21047395796880264, 0.5154970076912209]}}
{"id": "002a62e5-8897-4562-aa85-9bbec42fe24b", "fitness": -Infinity, "name": "SOMDifferentialEvolution", "description": "A differential evolution strategy with a self-organizing map (SOM) to cluster individuals and apply different mutation strategies based on cluster performance.", "code": "import numpy as np\n\nclass SOMDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, Cr=0.9, som_grid_size=5, learning_rate=0.1, sigma=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.Cr = Cr\n        self.som_grid_size = som_grid_size\n        self.learning_rate = learning_rate\n        self.sigma = sigma\n        self.som = np.random.rand(som_grid_size, som_grid_size, dim)  # SOM grid\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n    def find_closest_node(self, x):\n        distances = np.sum((self.som - x)**2, axis=2)\n        best_node = np.unravel_index(np.argmin(distances), distances.shape)\n        return best_node\n\n    def update_som(self, x, best_node):\n        for i in range(self.som_grid_size):\n            for j in range(self.som_grid_size):\n                distance = np.sqrt((i - best_node[0])**2 + (j - best_node[1])**2)\n                influence = np.exp(-distance**2 / (2 * self.sigma**2))\n                self.som[i, j] += self.learning_rate * influence * (x - self.som[i, j])\n\n    def __call__(self, func):\n        # Initialization\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        while self.budget > self.pop_size:\n            new_population = np.copy(self.population)\n            for i in range(self.pop_size):\n                # SOM node selection\n                best_node = self.find_closest_node(self.population[i])\n\n                # Mutation strategy based on SOM node (example: different F)\n                if best_node[0] < self.som_grid_size // 2:\n                    F = self.F  # Standard DE mutation\n                else:\n                    F = self.F * 1.5  # Enhanced exploration\n\n                indices = [j for j in range(self.pop_size) if j != i]\n                idxs = np.random.choice(indices, size=3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[idxs[0]], self.population[idxs[1]], self.population[idxs[2]]\n\n                mutant = self.population[i] + F * (x_r1 - x_r2)\n\n                # Crossover\n                for j in range(self.dim):\n                    if np.random.rand() < self.Cr:\n                        new_population[i, j] = mutant[j]\n\n                new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n\n            # Evaluation\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n\n            # Selection and SOM update\n            for i in range(self.pop_size):\n                if new_fitness[i] < self.fitness[i]:\n                    # Update SOM using the successful individual\n                    best_node = self.find_closest_node(self.new_population[i])\n                    self.update_som(self.new_population[i], best_node)\n\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n\n                    if new_fitness[i] < self.f_opt:\n                        self.f_opt = new_fitness[i]\n                        self.x_opt = new_population[i]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 73, in __call__\nAttributeError: 'SOMDifferentialEvolution' object has no attribute 'new_population'. Did you mean: 'population'?\n.", "error": "", "parent_ids": ["515b85bb-935b-4faf-9062-867def7467a0"], "operator": null, "metadata": {}}
{"id": "ffd051d6-808d-453a-b1d3-d1beea55c0e4", "fitness": -Infinity, "name": "SOMDifferentialEvolution", "description": "A differential evolution strategy with a self-organizing map (SOM) to cluster individuals and apply different mutation strategies based on cluster characteristics.", "code": "import numpy as np\nfrom minisom import MiniSom\n\nclass SOMDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=40, som_grid_size=5, initial_Cr=0.5, initial_F=0.7, learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.som_grid_size = som_grid_size\n        self.Cr = np.full(pop_size, initial_Cr)  # Individual crossover rates\n        self.F = np.full(pop_size, initial_F)  # Individual mutation factors\n        self.learning_rate = learning_rate\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.generation = 0\n        self.som = None\n        self.cluster_labels = None\n        self.min_F = 0.1\n        self.max_F = 1.0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n    def initialize_som(self):\n        self.som = MiniSom(self.som_grid_size, self.som_grid_size, self.dim, sigma=0.3, learning_rate=0.5)\n        self.som.random_weights_init(self.population)\n        self.som.train_random(self.population, 100)  # Train SOM for a few iterations\n\n    def assign_clusters(self):\n         self.cluster_labels = [self.som.winner(x) for x in self.population]\n\n    def evolve(self, func):\n        new_population = np.copy(self.population)\n        new_fitness = np.copy(self.fitness)\n\n        for i in range(self.pop_size):\n            # Get cluster label for the individual\n            cluster = self.cluster_labels[i]\n            \n            # Mutation strategy based on cluster (example: different F values)\n            if cluster[0] % 2 == 0:  # Example: even rows in SOM grid\n                F = self.F[i]  # Use individual F\n                mutation_strategy = 1\n            else:  # Example: odd rows in SOM grid\n                F = np.random.uniform(0.5, 1.0) # Use different F\n                mutation_strategy = 2\n\n            # Mutation\n            if mutation_strategy == 1:\n                r1, r2, r3 = np.random.choice(self.pop_size, 3, replace=False)\n                mutant = self.population[r1] + F * (self.population[r2] - self.population[r3])\n            elif mutation_strategy == 2:\n                # Using current best individual\n                best_idx = np.argmin(self.fitness)\n                r1, r2 = np.random.choice(self.pop_size, 2, replace=False)\n                mutant = self.population[i] + F * (self.population[best_idx] - self.population[i]) + F * (self.population[r1] - self.population[r2])\n            \n            # Crossover\n            for j in range(self.dim):\n                if np.random.rand() < self.Cr[i]:\n                    new_population[i, j] = mutant[j]\n                else:\n                    new_population[i, j] = self.population[i, j]\n\n            new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n\n            # Evaluation\n            new_fitness[i] = func(new_population[i])\n            self.budget -= 1\n\n            # Selection\n            if new_fitness[i] < self.fitness[i]:\n                self.population[i] = new_population[i]\n                self.fitness[i] = new_fitness[i]\n\n                if new_fitness[i] < self.f_opt:\n                    self.f_opt = new_fitness[i]\n                    self.x_opt = new_population[i]\n\n        # Adapt SOM\n        self.som.train_random(self.population, 10)\n        self.assign_clusters()\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.initialize_som()\n        self.assign_clusters()\n\n        while self.budget > self.pop_size:\n            self.evolve(func)\n            self.generation += 1\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 111, in evaluate\n    exec(code, safe_globals, local_env)\n  File \"<string>\", line 2, in <module>\nModuleNotFoundError: No module named 'minisom'\n.", "error": "", "parent_ids": ["de67fb59-bb06-43e9-b59d-5e706467c835"], "operator": null, "metadata": {}}
{"id": "d174d7e1-ec78-48cd-b185-8895d4e4d625", "fitness": -Infinity, "name": "SpiralAdaptiveDE", "description": "An adaptive differential evolution strategy employing a spiral dynamic inspired mutation, combined with a self-adjusting exploration-exploitation balance.", "code": "import numpy as np\n\nclass SpiralAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, Cr=0.9, F_initial=0.5, stagnation_threshold=100, restart_prob=0.1, spiral_factor=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.Cr = Cr\n        self.F = F_initial\n        self.stagnation_threshold = stagnation_threshold\n        self.restart_prob = restart_prob\n        self.spiral_factor = spiral_factor\n        self.best_fitness_history = []\n        self.last_improvement = 0\n        self.generation = 0\n        self.exploration_rate = 0.7  # Initial exploration rate\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n        self.best_fitness_history.append(self.f_opt)\n        self.last_improvement = 0\n        self.generation = 0\n\n        while self.budget > self.pop_size:\n            # Mutation and Crossover\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                if np.random.rand() < self.exploration_rate:\n                    # Exploration: Spiral Dynamic inspired mutation\n                    r1, r2 = np.random.choice(self.pop_size, 2, replace=False)\n                    center = self.x_opt  # Spiral towards the current best\n                    radius = np.linalg.norm(population[i] - center)\n                    angle = np.random.uniform(0, 2 * np.pi)\n                    mutant = center + radius * np.exp(self.spiral_factor * angle) * np.array([np.cos(angle), np.sin(angle)])[:self.dim]\n\n                else:\n                    # Exploitation: Standard DE mutation\n                    r1, r2, r3 = np.random.choice(self.pop_size, 3, replace=False)\n                    mutant = population[r1] + self.F * (population[r2] - population[r3])\n\n                # Crossover\n                for j in range(self.dim):\n                    if np.random.rand() < self.Cr:\n                        new_population[i, j] = mutant[j]\n                    else:\n                        new_population[i, j] = population[i, j]\n                \n                new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n            \n            # Evaluation\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n            \n            # Selection\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    population[i] = new_population[i]\n                    fitness[i] = new_fitness[i]\n                    \n                    if new_fitness[i] < self.f_opt:\n                        self.f_opt = new_fitness[i]\n                        self.x_opt = new_population[i]\n                        self.last_improvement = self.generation\n                        \n            self.best_fitness_history.append(self.f_opt)\n            \n            # Stagnation check and restart\n            if (self.generation - self.last_improvement) > self.stagnation_threshold:\n                if np.random.rand() < self.restart_prob:\n                    # Restart the population\n                    population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                    fitness = np.array([func(x) for x in population])\n                    self.budget -= self.pop_size\n                    self.f_opt = np.min(fitness)\n                    self.x_opt = population[np.argmin(fitness)]\n                    self.last_improvement = self.generation\n                    self.F = 0.5 #reset F\n                else:\n                    # Adaptive F: Reduce mutation strength upon stagnation\n                    self.F *= 0.9  # Reduce F, but prevent it from becoming zero.\n                    self.F = max(self.F, 0.1)\n                    \n            # Dynamically adjust exploration rate\n            if self.generation % 50 == 0:\n                if (self.generation - self.last_improvement) < self.stagnation_threshold // 2:\n                     self.exploration_rate = min(1.0, self.exploration_rate + 0.05)  # Increase exploration\n                else:\n                     self.exploration_rate = max(0.1, self.exploration_rate - 0.05)  # Decrease exploration\n                    \n            self.generation += 1\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 144, in evaluate\n    algorithm(f_new)\n  File \"<string>\", line 40, in __call__\nValueError: operands could not be broadcast together with shapes (5,) (2,) \n.", "error": "", "parent_ids": ["1265d49a-2a53-4b2e-beb2-01766b66734a"], "operator": null, "metadata": {}}
{"id": "1ce111ca-130c-486d-a75e-31d8f943fdf0", "fitness": -Infinity, "name": "DynamicPopSizeAdaptiveDE", "description": "A differential evolution with a dynamically adjusted population size, mutation and crossover rates based on fitness diversity and improvement stagnation, aiming to balance exploration and exploitation.", "code": "import numpy as np\n\nclass DynamicPopSizeAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size_initial=40, pop_size_min=10, pop_size_max=100, Cr_initial=0.9, F_initial=0.5, stagnation_threshold=100, pop_size_adjust_interval=50, diversity_threshold=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_initial\n        self.pop_size_initial = pop_size_initial\n        self.pop_size_min = pop_size_min\n        self.pop_size_max = pop_size_max\n        self.Cr = Cr_initial\n        self.F = F_initial\n        self.stagnation_threshold = stagnation_threshold\n        self.pop_size_adjust_interval = pop_size_adjust_interval\n        self.diversity_threshold = diversity_threshold\n        self.best_fitness_history = []\n        self.last_improvement = 0\n        self.generation = 0\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n        self.best_fitness_history.append(self.f_opt)\n        self.last_improvement = 0\n        self.generation = 0\n\n        while self.budget > self.pop_size_min:\n            # Mutation and Crossover\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # DE mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n                \n                mutant = population[i] + self.F * (x_r1 - x_r2)\n\n                # Crossover\n                for j in range(self.dim):\n                    if np.random.rand() < self.Cr:\n                        new_population[i, j] = mutant[j]\n                    else:\n                        new_population[i, j] = population[i, j]\n                \n                new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n            \n            # Evaluation\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n            \n            # Selection\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    population[i] = new_population[i]\n                    fitness[i] = new_fitness[i]\n                    \n                    if new_fitness[i] < self.f_opt:\n                        self.f_opt = new_fitness[i]\n                        self.x_opt = new_population[i]\n                        self.last_improvement = self.generation\n                        \n            self.best_fitness_history.append(self.f_opt)\n            \n            # Stagnation check\n            if (self.generation - self.last_improvement) > self.stagnation_threshold:\n                # Adaptive F and Cr: Reduce mutation strength and increase crossover rate upon stagnation\n                self.F *= 0.8\n                self.Cr = min(self.Cr * 1.2, 1.0)\n                self.F = max(self.F, 0.1)\n\n            # Population size adjustment\n            if self.generation % self.pop_size_adjust_interval == 0:\n                # Calculate fitness diversity\n                fitness_range = np.max(fitness) - np.min(fitness)\n                if fitness_range < self.diversity_threshold:\n                    # Low diversity, increase population size to explore more\n                    self.pop_size = min(self.pop_size + 10, self.pop_size_max)\n                else:\n                    # High diversity, decrease population size to exploit more\n                    self.pop_size = max(self.pop_size - 5, self.pop_size_min)\n\n                # Resize population\n                if self.pop_size != population.shape[0]:\n                    if self.pop_size > population.shape[0]:\n                        # Add new random individuals\n                        new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size - population.shape[0], self.dim))\n                        population = np.vstack((population, new_individuals))\n                        new_fitness = np.array([func(x) for x in new_individuals])\n                        fitness = np.concatenate((fitness, new_fitness))\n                        self.budget -= (self.pop_size - population.shape[0])\n\n                    else:\n                        # Remove worst individuals\n                        ranked_indices = np.argsort(fitness)\n                        population = population[ranked_indices[:self.pop_size]]\n                        fitness = fitness[ranked_indices[:self.pop_size]]\n            \n            self.generation += 1\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 120, in evaluate\n    algorithm = local_env[algorithm_name](budget=100, dim=2)\n  File \"<string>\", line 7, in __init__\nNameError: name 'pop_initial' is not defined. Did you mean: 'Cr_initial'?\n.", "error": "", "parent_ids": ["1265d49a-2a53-4b2e-beb2-01766b66734a"], "operator": null, "metadata": {}}
{"id": "383fbfa3-e324-47c7-b9e1-97cd2bcd7008", "fitness": 0.3717302776248002, "name": "ProbabilisticRingOrthoDE", "description": "An adaptive differential evolution with a probabilistic ring topology, orthogonal learning, and a combined Cauchy-Gaussian mutation to balance exploration and exploitation.", "code": "import numpy as np\n\nclass ProbabilisticRingOrthoDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, Cr=0.9, F_initial=0.5, stagnation_threshold=100, restart_prob=0.1, ortho_prob=0.1, migration_interval=50, cauchy_prob=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.Cr = Cr\n        self.F = F_initial\n        self.stagnation_threshold = stagnation_threshold\n        self.restart_prob = restart_prob\n        self.ortho_prob = ortho_prob\n        self.migration_interval = migration_interval\n        self.cauchy_prob = cauchy_prob\n        self.best_fitness_history = []\n        self.last_improvement = 0\n        self.generation = 0\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n        self.best_fitness_history.append(self.f_opt)\n        self.last_improvement = 0\n        self.generation = 0\n\n        while self.budget > self.pop_size:\n            # Mutation and Crossover\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Probabilistic Ring Topology\n                neighbors = []\n                if np.random.rand() < 0.7:  # 70% chance to include left neighbor\n                    neighbors.append((i - 1) % self.pop_size)\n                if np.random.rand() < 0.7:  # 70% chance to include right neighbor\n                    neighbors.append((i + 1) % self.pop_size)\n                if not neighbors:\n                    neighbors.append(np.random.randint(0, self.pop_size)) # Ensure at least one neighbor\n\n                # Combined Cauchy-Gaussian Mutation\n                if np.random.rand() < self.cauchy_prob:\n                    mutation_noise = self.F * np.random.standard_cauchy(size=self.dim)  # Cauchy\n                else:\n                    mutation_noise = self.F * np.random.normal(0, 1, size=self.dim)  # Gaussian\n\n                mutant = population[i] + mutation_noise\n\n                # Crossover\n                for j in range(self.dim):\n                    if np.random.rand() < self.Cr:\n                        new_population[i, j] = mutant[j]\n                    else:\n                        new_population[i, j] = population[i, j]\n                \n                new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n            \n            # Evaluation\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n            \n            # Selection\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    population[i] = new_population[i]\n                    fitness[i] = new_fitness[i]\n                    \n                    if new_fitness[i] < self.f_opt:\n                        self.f_opt = new_fitness[i]\n                        self.x_opt = new_population[i]\n                        self.last_improvement = self.generation\n                        \n            self.best_fitness_history.append(self.f_opt)\n            \n            # Stagnation check and restart\n            if (self.generation - self.last_improvement) > self.stagnation_threshold:\n                if np.random.rand() < self.restart_prob:\n                    # Restart the population\n                    population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                    fitness = np.array([func(x) for x in population])\n                    self.budget -= self.pop_size\n                    self.f_opt = np.min(fitness)\n                    self.x_opt = population[np.argmin(fitness)]\n                    self.last_improvement = self.generation\n                    self.F = 0.5 #reset F\n                else:\n                    # Adaptive F: Reduce mutation strength upon stagnation\n                    self.F *= 0.9  # Reduce F, but prevent it from becoming zero.\n                    self.F = max(self.F, 0.1)\n            \n            # Migration strategy\n            if self.generation % self.migration_interval == 0:\n                ranked_indices = np.argsort(fitness)\n                # Replace worst individuals with slightly perturbed best individuals\n                for i in range(self.pop_size // 4): #Migrate 25% of population\n                    worst_idx = ranked_indices[self.pop_size - 1 - i]\n                    best_idx = ranked_indices[0]\n                    population[worst_idx] = population[best_idx] + 0.05 * np.random.normal(0, 1, self.dim)\n                    population[worst_idx] = np.clip(population[worst_idx], func.bounds.lb, func.bounds.ub)\n                    fitness[worst_idx] = func(population[worst_idx])\n                    self.budget -= 1\n\n                    if fitness[worst_idx] < self.f_opt:\n                        self.f_opt = fitness[worst_idx]\n                        self.x_opt = population[worst_idx]\n                        self.last_improvement = self.generation\n            \n            # Orthogonal Learning\n            if np.random.rand() < self.ortho_prob:\n                idx = np.random.randint(0, self.pop_size)\n                \n                # Generate orthogonal array (simplified - random sampling)\n                levels = 3  # Number of levels for each factor\n                factors = self.dim  # Number of factors (dimensions)\n                orthogonal_array = np.random.randint(0, levels, size=(levels**2, factors)) # L9 array\n\n                # Evaluate all points in the orthogonal array around the selected individual\n                best_fitness_oa = fitness[idx]\n                best_oa_point = population[idx]\n\n                for oa_point in orthogonal_array:\n                    # Map the levels to a perturbation around the current individual\n                    perturbation = (oa_point - (levels - 1) / 2) * 0.05 # small perturbation\n                    new_point = population[idx] + perturbation\n                    new_point = np.clip(new_point, func.bounds.lb, func.bounds.ub)\n\n                    new_fitness_oa = func(new_point)\n                    self.budget -= 1\n\n                    if new_fitness_oa < best_fitness_oa:\n                        best_fitness_oa = new_fitness_oa\n                        best_oa_point = new_point\n\n                        if new_fitness_oa < self.f_opt:\n                            self.f_opt = new_fitness_oa\n                            self.x_opt = new_point\n                            self.last_improvement = self.generation\n                \n                # Replace the individual with the best point found in the orthogonal array\n                population[idx] = best_oa_point\n                fitness[idx] = best_fitness_oa\n            \n            self.generation += 1\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm ProbabilisticRingOrthoDE scored 0.372 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["1265d49a-2a53-4b2e-beb2-01766b66734a"], "operator": null, "metadata": {"aucs": [0.17792765463279703, 0.3085880328024374, 0.37996957662095787, 0.39481392332635756, 0.28128618891577595, 0.3777107339188841, 0.2808598182410975, 0.3388300174357959, 0.29195729319851205, 0.20466829889077343, 0.38442476115751, 0.9892779300093687, 0.25801972686103525, 0.30009249374021296, 0.6938297410313934, 0.3368679435913946, 0.29948007732096293, 0.41024297356968, 0.22248769227819132, 0.503270674952866]}}
{"id": "2c2ff8d7-bec5-49f5-a7ff-2beb84b823c0", "fitness": 0.39085828888259944, "name": "AdaptiveDE_CMAES", "description": "A differential evolution strategy with self-adaptive parameters and a covariance matrix adaptation local search, triggered based on stagnation detection.", "code": "import numpy as np\n\nclass AdaptiveDE_CMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=40, stagnation_threshold=50, cmaes_probability=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.stagnation_threshold = stagnation_threshold\n        self.cmaes_probability = cmaes_probability\n        self.best_fitness_history = []\n        self.last_improvement = 0\n        self.F = 0.5\n        self.Cr = 0.9\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n        self.best_fitness_history.append(self.f_opt)\n        \n        generation = 0\n\n        while self.budget > self.pop_size:\n            # Adaptive F and Cr\n            if np.random.rand() < 0.1:\n                self.F = np.random.uniform(0.1, 0.9)\n            if np.random.rand() < 0.1:\n                self.Cr = np.random.uniform(0.1, 0.9)\n            \n            # Mutation and Crossover\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                indices = [j for j in range(self.pop_size) if j != i]\n                idxs = np.random.choice(indices, size=3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs[0]], population[idxs[1]], population[idxs[2]]\n                \n                mutant = population[i] + self.F * (x_r1 - x_r2)\n                \n                # Crossover\n                for j in range(self.dim):\n                    if np.random.rand() < self.Cr:\n                        new_population[i, j] = mutant[j]\n                    \n                new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n\n            # Evaluation\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n\n            # Selection\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    population[i] = new_population[i]\n                    fitness[i] = new_fitness[i]\n                    \n                    if new_fitness[i] < self.f_opt:\n                        self.f_opt = new_fitness[i]\n                        self.x_opt = new_population[i]\n                        self.last_improvement = generation\n                        \n            self.best_fitness_history.append(self.f_opt)\n\n            # CMA-ES Local Search\n            if (generation - self.last_improvement) > self.stagnation_threshold:\n                for i in range(self.pop_size):\n                    if np.random.rand() < self.cmaes_probability:\n                        # Perform CMA-ES local search around individual i\n                        x_current = population[i].copy()\n                        sigma = 0.1  # Initial step size\n                        C = np.eye(self.dim)  # Initial covariance matrix\n\n                        for _ in range(5):  # Limited budget for local search\n                            z = np.random.randn(self.dim)\n                            x_new = x_current + sigma * np.dot(C, z)\n                            x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n                            f_new = func(x_new)\n                            self.budget -= 1\n                            if self.budget <= 0:\n                                return self.f_opt, self.x_opt\n\n                            if f_new < fitness[i]:\n                                population[i] = x_new\n                                fitness[i] = f_new\n                                x_current = x_new\n                                if f_new < self.f_opt:\n                                    self.f_opt = f_new\n                                    self.x_opt = x_new\n                                    self.last_improvement = generation\n\n            generation += 1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveDE_CMAES scored 0.391 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["515b85bb-935b-4faf-9062-867def7467a0"], "operator": null, "metadata": {"aucs": [0.13503049507234066, 0.27116684402539737, 0.40062653466365217, 0.3937556512902959, 0.31531776500561404, 0.4434121890333821, 0.2991684415131384, 0.32775235095999355, 0.30638032286355654, 0.17815614776616295, 0.48754599400715415, 0.9994924181705711, 0.3178510826408506, 0.3038141325919116, 0.6868938017417829, 0.47860178070179404, 0.32096046573075065, 0.4661926618713633, 0.1994272246941733, 0.4856194733081035]}}
{"id": "8f402b2c-a363-4549-a6e1-a29c21c3192d", "fitness": 0.47490085472437193, "name": "RankBasedAdaptiveDE", "description": "A differential evolution strategy with a self-adjusting mutation factor based on the rank of the individual and an aging mechanism that diversifies the population.", "code": "import numpy as np\n\nclass RankBasedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, Cr=0.9, aging_rate=0.02):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.Cr = Cr\n        self.aging_rate = aging_rate\n        self.best_fitness_history = []\n        self.population = None\n        self.fitness = None\n        self.ages = None\n\n    def __call__(self, func):\n        # Initialization\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        \n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.best_fitness_history.append(self.f_opt)\n        self.ages = np.zeros(self.pop_size)\n        \n        generation = 0\n\n        while self.budget > self.pop_size:\n            # Sort population based on fitness\n            ranked_indices = np.argsort(self.fitness)\n            ranked_population = self.population[ranked_indices]\n\n            new_population = np.copy(self.population)\n            for i in range(self.pop_size):\n                # Rank-based mutation factor\n                rank = np.where(ranked_indices == i)[0][0]  # Find the rank of individual i\n                F = 0.1 + 0.9 * (rank / (self.pop_size - 1))  # F increases with rank\n\n                indices = [j for j in range(self.pop_size) if j != i]\n                idxs = np.random.choice(indices, size=3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[idxs[0]], self.population[idxs[1]], self.population[idxs[2]]\n                \n                mutant = self.population[i] + F * (x_r1 - x_r2)\n                \n                # Crossover\n                for j in range(self.dim):\n                    if np.random.rand() < self.Cr:\n                        new_population[i, j] = mutant[j]\n                    \n                new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n\n            # Evaluation\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n\n            # Selection\n            for i in range(self.pop_size):\n                if new_fitness[i] < self.fitness[i]:\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n                    self.ages[i] = 0  # Reset age if improved\n                    \n                    if new_fitness[i] < self.f_opt:\n                        self.f_opt = new_fitness[i]\n                        self.x_opt = new_population[i]\n                else:\n                    self.ages[i] += 1 # Increment age if not improved\n\n            self.best_fitness_history.append(self.f_opt)\n\n            # Aging mechanism: Replace old individuals with new random ones\n            for i in range(self.pop_size):\n                if self.ages[i] > (1 / self.aging_rate):  # Age threshold\n                    self.population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                    self.fitness[i] = func(self.population[i])\n                    self.budget -= 1\n                    self.ages[i] = 0\n                    if self.fitness[i] < self.f_opt:\n                        self.f_opt = self.fitness[i]\n                        self.x_opt = self.population[i]\n\n            generation += 1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm RankBasedAdaptiveDE scored 0.475 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["515b85bb-935b-4faf-9062-867def7467a0"], "operator": null, "metadata": {"aucs": [0.1631897031475783, 0.3416338714942544, 0.4377782831388758, 0.7465905124364736, 0.4067366164018109, 0.5345884551528772, 0.29854457604324325, 0.39518500651409816, 0.3651849314720892, 0.21587765626307842, 0.7675989565357868, 1.0, 0.44219767568706037, 0.32206764917666497, 0.8456271502983922, 0.5033357108411737, 0.3815963852027374, 0.6294259809741614, 0.2037376993049521, 0.49712027440212947]}}
{"id": "664174e0-1753-41ff-bc0f-80323b424622", "fitness": 0.456122524927767, "name": "DiversityCrossoverMirroredDE", "description": "A differential evolution strategy that dynamically adjusts its crossover rate based on population diversity and uses a mirrored sampling technique to enhance boundary exploration.", "code": "import numpy as np\n\nclass DiversityCrossoverMirroredDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, diversity_threshold=0.1, mirrored_sampling_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.diversity_threshold = diversity_threshold\n        self.mirrored_sampling_prob = mirrored_sampling_prob\n        self.best_fitness_history = []\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n        self.best_fitness_history.append(self.f_opt)\n        \n        while self.budget > self.pop_size:\n            # Calculate population diversity\n            diversity = np.std(population)\n            \n            # Adjust crossover rate based on diversity\n            if diversity > self.diversity_threshold:\n                Cr = 0.9  # High diversity, high crossover\n            else:\n                Cr = 0.3  # Low diversity, low crossover\n                \n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n                mutant = population[i] + self.F * (x_r2 - x_r3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                for j in range(self.dim):\n                    if np.random.rand() < Cr:\n                        new_population[i, j] = mutant[j]\n                    else:\n                        new_population[i, j] = population[i, j]\n                        \n                # Mirrored Sampling\n                if np.random.rand() < self.mirrored_sampling_prob:\n                    for j in range(self.dim):\n                        if new_population[i,j] < func.bounds.lb[j]:\n                            new_population[i,j] = func.bounds.lb[j] + (func.bounds.lb[j] - new_population[i,j])\n                        elif new_population[i,j] > func.bounds.ub[j]:\n                            new_population[i,j] = func.bounds.ub[j] - (new_population[i,j] - func.bounds.ub[j])\n                    new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n\n            # Evaluation\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n\n            # Selection\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    population[i] = new_population[i]\n                    fitness[i] = new_fitness[i]\n                    if new_fitness[i] < self.f_opt:\n                        self.f_opt = new_fitness[i]\n                        self.x_opt = new_population[i]\n            self.best_fitness_history.append(self.f_opt)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm DiversityCrossoverMirroredDE scored 0.456 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["1265d49a-2a53-4b2e-beb2-01766b66734a"], "operator": null, "metadata": {"aucs": [0.15607998455133132, 0.3085674085551958, 0.42931016391557963, 0.702161634670197, 0.3648331307245809, 0.5051303739283675, 0.2995684173187624, 0.395203932376752, 0.36672115327204025, 0.19214838990867944, 0.6792381801730618, 0.9936815668049275, 0.4354117313303437, 0.3475776541860639, 0.7969150764253308, 0.47872179447785546, 0.34708681167880284, 0.6311472183741292, 0.20010021093417696, 0.49284566494915893]}}
{"id": "e9a531ba-15eb-46f7-ac14-202dd28ed3f9", "fitness": 0.4579819491144814, "name": "ShiftingMomentumAdaptiveDE", "description": "An adaptive differential evolution with a shifting ring topology, momentum-based mutation, and adaptive parameter control based on objective function curvature.", "code": "import numpy as np\n\nclass ShiftingMomentumAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, Cr=0.9, F_initial=0.5, stagnation_threshold=100, restart_prob=0.1, curvature_window=50, momentum=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.Cr = Cr\n        self.F = F_initial\n        self.stagnation_threshold = stagnation_threshold\n        self.restart_prob = restart_prob\n        self.curvature_window = curvature_window\n        self.momentum = momentum\n        self.best_fitness_history = []\n        self.last_improvement = 0\n        self.generation = 0\n        self.fitness_trend = [] # Store recent fitness values to estimate curvature\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n        self.best_fitness_history.append(self.f_opt)\n        self.fitness_trend.append(self.f_opt)\n        self.last_improvement = 0\n        self.generation = 0\n        self.previous_mutation = np.zeros((self.pop_size, self.dim)) # Initialize previous mutation direction\n\n        while self.budget > self.pop_size:\n            # Mutation and Crossover\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Shifting Ring Topology\n                shift = np.random.randint(1, self.pop_size // 4 + 1)  # Shift by a random amount\n                idx_prev = (i - shift) % self.pop_size\n                idx_next = (i + shift) % self.pop_size\n                \n                # Momentum-based Mutation\n                r1, r2 = np.random.choice(self.pop_size, 2, replace=False)\n                \n                # Calculate mutation vector with momentum\n                mutation_vector = self.F * (population[r1] - population[r2])\n                mutation_vector = self.momentum * self.previous_mutation[i] + (1 - self.momentum) * mutation_vector\n                \n                mutant = population[i] + mutation_vector\n                \n                # Store the mutation vector for the next iteration\n                self.previous_mutation[i] = mutation_vector\n                \n                # Crossover\n                for j in range(self.dim):\n                    if np.random.rand() < self.Cr:\n                        new_population[i, j] = mutant[j]\n                    else:\n                        new_population[i, j] = population[i, j]\n                \n                new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n            \n            # Evaluation\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n            \n            # Selection\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    population[i] = new_population[i]\n                    fitness[i] = new_fitness[i]\n                    \n                    if new_fitness[i] < self.f_opt:\n                        self.f_opt = new_fitness[i]\n                        self.x_opt = new_population[i]\n                        self.last_improvement = self.generation\n                        \n            self.best_fitness_history.append(self.f_opt)\n            self.fitness_trend.append(self.f_opt)\n            if len(self.fitness_trend) > self.curvature_window:\n                self.fitness_trend.pop(0)\n            \n            # Stagnation check and restart\n            if (self.generation - self.last_improvement) > self.stagnation_threshold:\n                if np.random.rand() < self.restart_prob:\n                    # Restart the population\n                    population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                    fitness = np.array([func(x) for x in population])\n                    self.budget -= self.pop_size\n                    self.f_opt = np.min(fitness)\n                    self.x_opt = population[np.argmin(fitness)]\n                    self.last_improvement = self.generation\n                    self.F = 0.5 #reset F\n                else:\n                    # Adaptive F: Reduce mutation strength upon stagnation\n                    self.F *= 0.9  # Reduce F, but prevent it from becoming zero.\n                    self.F = max(self.F, 0.1)\n\n            # Adaptive Parameter Control based on Curvature\n            if len(self.fitness_trend) >= self.curvature_window:\n                # Estimate curvature (simplified as the difference between the first and last fitness values)\n                curvature = self.fitness_trend[-1] - self.fitness_trend[0]\n\n                # Adjust Cr and F based on curvature\n                if curvature > 0:  # Positive curvature indicates slow progress\n                    self.Cr *= 0.9  # Reduce crossover rate to promote exploration\n                    self.F *= 1.1   # Increase mutation rate to escape local optima\n                else:  # Negative curvature indicates good progress\n                    self.Cr *= 1.1  # Increase crossover rate to promote exploitation\n                    self.F *= 0.9   # Reduce mutation rate to refine the solution\n                \n                self.Cr = np.clip(self.Cr, 0.1, 0.95)\n                self.F = np.clip(self.F, 0.1, 2.0) #prevent F from getting too high.\n\n            self.generation += 1\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm ShiftingMomentumAdaptiveDE scored 0.458 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["1265d49a-2a53-4b2e-beb2-01766b66734a"], "operator": null, "metadata": {"aucs": [0.1678951113058883, 0.328112298600725, 0.4199767687454359, 0.6980413971233148, 0.35502973047120556, 0.49965542869767254, 0.3067107895809542, 0.37337379929238024, 0.3255287495026371, 0.20647131502432725, 0.7485199314707647, 0.9927282053238243, 0.45039513249816854, 0.3225936377712365, 0.8102345968682718, 0.4972024872036014, 0.35615056537679635, 0.6051695194195781, 0.2128131076431402, 0.4830364103697068]}}
{"id": "b388b19d-ebda-454b-b92c-5cbdbcf83822", "fitness": 0.32130022696489, "name": "VarianceAdaptiveDE", "description": "An adaptive differential evolution with a modified Cauchy mutation based on the fitness landscape variance, combined with a local search that adapts its step size based on recent success.", "code": "import numpy as np\n\nclass VarianceAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, Cr=0.9, F_initial=0.5, stagnation_threshold=100, restart_prob=0.1, local_search_prob=0.1, local_search_step_size=0.1, step_size_adaptation_rate=0.9, success_threshold=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.Cr = Cr\n        self.F = F_initial\n        self.stagnation_threshold = stagnation_threshold\n        self.restart_prob = restart_prob\n        self.local_search_prob = local_search_prob\n        self.local_search_step_size = local_search_step_size\n        self.step_size_adaptation_rate = step_size_adaptation_rate\n        self.success_threshold = success_threshold\n        self.best_fitness_history = []\n        self.last_improvement = 0\n        self.generation = 0\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n        self.best_fitness_history.append(self.f_opt)\n        self.last_improvement = 0\n        self.generation = 0\n\n        while self.budget > self.pop_size:\n            # Mutation and Crossover\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Calculate variance of the fitness landscape around the individual\n                neighborhood_size = min(self.pop_size // 5, 5)  #Adjust neighborhood size as needed\n                neighbor_indices = np.random.choice(self.pop_size, size=neighborhood_size, replace=False)\n                neighborhood_fitness = fitness[neighbor_indices]\n                fitness_variance = np.var(neighborhood_fitness)\n\n                # Modified Cauchy mutation: scale Cauchy noise by fitness variance\n                cauchy_noise = self.F * np.random.standard_cauchy(size=self.dim) * (1 + fitness_variance) # Scale by variance\n                mutant = population[i] + cauchy_noise\n\n                # Crossover\n                for j in range(self.dim):\n                    if np.random.rand() < self.Cr:\n                        new_population[i, j] = mutant[j]\n                    else:\n                        new_population[i, j] = population[i, j]\n                \n                new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n            \n            # Evaluation\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n            \n            # Selection\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    population[i] = new_population[i]\n                    fitness[i] = new_fitness[i]\n                    \n                    if new_fitness[i] < self.f_opt:\n                        self.f_opt = new_fitness[i]\n                        self.x_opt = new_population[i]\n                        self.last_improvement = self.generation\n                        \n            self.best_fitness_history.append(self.f_opt)\n            \n            # Stagnation check and restart\n            if (self.generation - self.last_improvement) > self.stagnation_threshold:\n                if np.random.rand() < self.restart_prob:\n                    # Restart the population\n                    population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                    fitness = np.array([func(x) for x in population])\n                    self.budget -= self.pop_size\n                    self.f_opt = np.min(fitness)\n                    self.x_opt = population[np.argmin(fitness)]\n                    self.last_improvement = self.generation\n                    self.F = 0.5 #reset F\n                else:\n                    # Adaptive F: Reduce mutation strength upon stagnation\n                    self.F *= 0.9  # Reduce F, but prevent it from becoming zero.\n                    self.F = max(self.F, 0.1)\n            \n            # Local Search\n            if np.random.rand() < self.local_search_prob:\n                idx = np.random.randint(0, self.pop_size)\n                current_fitness = fitness[idx]\n                # Apply small perturbation to the selected individual\n                new_individual = population[idx] + self.local_search_step_size * np.random.normal(0, 1, self.dim)\n                new_individual = np.clip(new_individual, func.bounds.lb, func.bounds.ub)\n                new_fitness_local = func(new_individual)\n                self.budget -= 1\n\n                if new_fitness_local < current_fitness:\n                    population[idx] = new_individual\n                    fitness[idx] = new_fitness_local\n                    # Adapt step size: increase if successful\n                    self.local_search_step_size /= self.step_size_adaptation_rate #increase step size\n                    if fitness[idx] < self.f_opt:\n                        self.f_opt = fitness[idx]\n                        self.x_opt = population[idx]\n                        self.last_improvement = self.generation\n\n                else:\n                     # Adapt step size: decrease if unsuccessful\n                     self.local_search_step_size *= self.step_size_adaptation_rate #decrease step size\n\n            self.local_search_step_size = np.clip(self.local_search_step_size, 0.0001, 0.5) #Bound step size\n            self.generation += 1\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm VarianceAdaptiveDE scored 0.321 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["1265d49a-2a53-4b2e-beb2-01766b66734a"], "operator": null, "metadata": {"aucs": [0.10992308543438134, 0.15858731024181216, 0.3367427987172228, 0.3015496189754564, 0.23691260445082662, 0.30376011834495753, 0.2548670095362444, 0.26682812410787315, 0.2452134439883754, 0.19040061028871913, 0.2594507200410763, 0.993308014165922, 0.20283435643481817, 0.2521464165206696, 0.6696272647931665, 0.3537848713774271, 0.2845955778711212, 0.3628663700329069, 0.1640885682312504, 0.47851765574357297]}}
{"id": "70a904aa-a695-4067-94cf-0019e7049cd0", "fitness": 0.42383483145144474, "name": "SOM_DE", "description": "A differential evolution strategy with a self-organizing map (SOM) to cluster individuals and apply different mutation strategies based on the cluster.", "code": "import numpy as np\n\nclass SOM_DE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, Cr=0.9, som_grid_size=5, som_learning_rate=0.1, som_sigma=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.Cr = Cr\n        self.som_grid_size = som_grid_size\n        self.som_learning_rate = som_learning_rate\n        self.som_sigma = som_sigma\n        self.som_weights = np.random.rand(som_grid_size, som_grid_size, dim)  # SOM weights\n        self.best_fitness_history = []\n        self.clusters = np.zeros(pop_size, dtype=int) # Cluster assignment for each individual\n\n    def find_closest_node(self, x):\n        \"\"\"Finds the closest SOM node to the input vector x.\"\"\"\n        distances = np.sum((self.som_weights - x)**2, axis=2)\n        return np.unravel_index(np.argmin(distances), distances.shape[:2])\n\n    def update_som(self, x, winning_node):\n        \"\"\"Updates the SOM weights based on the winning node and neighborhood.\"\"\"\n        for i in range(self.som_grid_size):\n            for j in range(self.som_grid_size):\n                distance = np.sqrt((i - winning_node[0])**2 + (j - winning_node[1])**2)\n                influence = np.exp(-distance**2 / (2 * self.som_sigma**2))\n                self.som_weights[i, j] += self.som_learning_rate * influence * (x - self.som_weights[i, j])\n                \n    def assign_clusters(self, population):\n        \"\"\"Assigns each individual to its closest SOM node.\"\"\"\n        for i, x in enumerate(population):\n            self.clusters[i] = np.ravel_multi_index(self.find_closest_node(x), (self.som_grid_size, self.som_grid_size))\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n        self.best_fitness_history.append(self.f_opt)\n        \n        generation = 0\n\n        while self.budget > self.pop_size:\n            # Update SOM and assign clusters\n            for x in population:\n                winning_node = self.find_closest_node(x)\n                self.update_som(x, winning_node)\n            self.assign_clusters(population)\n            \n            # Mutation and Crossover\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Cluster-based mutation strategy (example: different F values)\n                cluster = self.clusters[i]\n                if cluster % 3 == 0:\n                    F = self.F  # Standard F\n                elif cluster % 3 == 1:\n                    F = self.F * 0.8  # Smaller F\n                else:\n                    F = self.F * 1.2  # Larger F\n                \n                indices = [j for j in range(self.pop_size) if j != i]\n                idxs = np.random.choice(indices, size=3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs[0]], population[idxs[1]], population[idxs[2]]\n                \n                mutant = population[i] + F * (x_r1 - x_r2)\n                \n                # Crossover\n                for j in range(self.dim):\n                    if np.random.rand() < self.Cr:\n                        new_population[i, j] = mutant[j]\n                    \n                new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n\n            # Evaluation\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n\n            # Selection\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    population[i] = new_population[i]\n                    fitness[i] = new_fitness[i]\n                    \n                    if new_fitness[i] < self.f_opt:\n                        self.f_opt = new_fitness[i]\n                        self.x_opt = new_population[i]\n                        \n            self.best_fitness_history.append(self.f_opt)\n            generation += 1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm SOM_DE scored 0.424 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["515b85bb-935b-4faf-9062-867def7467a0"], "operator": null, "metadata": {"aucs": [0.14349324052425383, 0.2965537006297331, 0.3858735329505364, 0.6085928628240207, 0.31838207446810574, 0.4562257346766415, 0.2925098358824332, 0.34444354107801756, 0.3225073844251989, 0.1889005660769031, 0.6749194264635952, 0.9992302599948001, 0.38919961239486567, 0.28693039992169866, 0.7790021890046325, 0.4344597053142103, 0.33073099339710754, 0.553922221488685, 0.1870168694436013, 0.4838024780698553]}}
{"id": "4218b067-d0ef-45dc-ad81-fd3f1a657bbe", "fitness": 0.6960627220616141, "name": "SOMGuidedDE", "description": "A differential evolution strategy incorporating a self-organizing map (SOM) to guide the mutation and crossover operations by adapting to the fitness landscape.", "code": "import numpy as np\n\nclass SOMGuidedDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, Cr=0.9, F=0.5, som_grid_size=10, learning_rate=0.1, sigma_initial=1.0, sigma_decay=0.995):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.Cr = Cr\n        self.F = F\n        self.som_grid_size = som_grid_size\n        self.learning_rate = learning_rate\n        self.sigma = sigma_initial\n        self.sigma_decay = sigma_decay\n        self.som = np.random.uniform(0, 1, size=(som_grid_size, som_grid_size, dim))  # SOM nodes initialized randomly\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def find_best_matching_unit(self, vector):\n        \"\"\"Find the best matching unit (BMU) in the SOM grid.\"\"\"\n        distances = np.sum((self.som - vector)**2, axis=2)\n        bmu_index = np.unravel_index(np.argmin(distances), distances.shape)\n        return bmu_index\n\n    def update_som(self, vector, bmu_index):\n        \"\"\"Update the SOM based on the input vector and BMU.\"\"\"\n        for i in range(self.som_grid_size):\n            for j in range(self.som_grid_size):\n                distance = np.sqrt((i - bmu_index[0])**2 + (j - bmu_index[1])**2)\n                influence = np.exp(-distance**2 / (2 * self.sigma**2))\n                self.som[i, j] += self.learning_rate * influence * (vector - self.som[i, j])\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)].copy()\n        \n        while self.budget > self.pop_size:\n            new_population = np.copy(population)\n            \n            for i in range(self.pop_size):\n                # Mutation guided by SOM\n                bmu_index = self.find_best_matching_unit(population[i])\n                bmu = self.som[bmu_index[0], bmu_index[1]]\n                \n                # Select three random indices, excluding the current index 'i'\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                \n                mutant = population[idxs[0]] + self.F * (population[idxs[1]] - population[idxs[2]]) + 0.1 * (bmu - population[i])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                for j in range(self.dim):\n                    if np.random.rand() < self.Cr:\n                        new_population[i, j] = mutant[j]\n                    \n            # Evaluation\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n            \n            # Selection and SOM update\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    fitness_diff = fitness[i] - new_fitness[i]\n\n                    bmu_index = self.find_best_matching_unit(population[i])\n                    self.update_som(population[i], bmu_index)  # Update SOM with old position\n\n                    population[i] = new_population[i].copy()\n                    fitness[i] = new_fitness[i]\n\n                    bmu_index = self.find_best_matching_unit(population[i])\n                    self.update_som(population[i], bmu_index)  # Update SOM with new position\n                    \n                    if new_fitness[i] < self.f_opt:\n                        self.f_opt = new_fitness[i]\n                        self.x_opt = new_population[i].copy()\n\n            # Decay SOM parameters\n            self.sigma *= self.sigma_decay\n            self.learning_rate *= 0.99\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm SOMGuidedDE scored 0.696 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["1265d49a-2a53-4b2e-beb2-01766b66734a"], "operator": null, "metadata": {"aucs": [0.3323519846247748, 0.6894509850035468, 0.6774129401666722, 0.8825597889801264, 0.7400127563599748, 0.8026362866671578, 0.6241470836483339, 0.6811334958570558, 0.7419234163980315, 0.713067175801463, 0.8578957579886439, 0.998103737958978, 0.37934550464599437, 0.7266980202946987, 0.9292156241806343, 0.789370972002541, 0.5973832620751786, 0.8443262720279294, 0.29794498247288215, 0.6162743940776645]}}
{"id": "806f9cc5-479e-4eef-b0c3-ae756f863acf", "fitness": -Infinity, "name": "RepulsiveClusteringDE", "description": "A differential evolution strategy that uses a repulsive force from the worst performing individuals to drive exploration and a clustering mechanism to encourage exploitation around promising regions.", "code": "import numpy as np\nfrom sklearn.cluster import MiniBatchKMeans\n\nclass RepulsiveClusteringDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, Cr=0.9, F=0.5, num_clusters=5, repulsion_strength=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.Cr = Cr\n        self.F = F\n        self.num_clusters = num_clusters\n        self.repulsion_strength = repulsion_strength\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)].copy()\n        \n        while self.budget > self.pop_size:\n            new_population = np.copy(population)\n            \n            # Clustering\n            kmeans = MiniBatchKMeans(n_clusters=self.num_clusters, random_state=0, n_init=5).fit(population)\n            clusters = [[] for _ in range(self.num_clusters)]\n            for i in range(self.pop_size):\n                clusters[kmeans.labels_[i]].append(i)\n\n            # Find worst individual\n            worst_index = np.argmax(fitness)\n\n            for i in range(self.pop_size):\n                # Mutation with repulsive force from the worst\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                mutant = population[idxs[0]] + self.F * (population[idxs[1]] - population[idxs[2]]) + self.repulsion_strength * (population[i] - population[worst_index])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                for j in range(self.dim):\n                    if np.random.rand() < self.Cr:\n                        new_population[i, j] = mutant[j]\n                    \n            # Evaluation\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n            \n            # Selection\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    population[i] = new_population[i].copy()\n                    fitness[i] = new_fitness[i]\n                    \n                    if new_fitness[i] < self.f_opt:\n                        self.f_opt = new_fitness[i]\n                        self.x_opt = new_population[i].copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 29, in __call__\nNameError: name 'MiniBatchKMeans' is not defined\n.", "error": "", "parent_ids": ["4218b067-d0ef-45dc-ad81-fd3f1a657bbe"], "operator": null, "metadata": {}}
{"id": "dff4bd64-1fc9-425e-85b2-6a435af8f986", "fitness": -Infinity, "name": "WaveletMutationDE", "description": "A differential evolution strategy employing a wavelet-based mutation operator and a local search refinement step to enhance exploitation of promising regions.", "code": "import numpy as np\nimport pywt\n\nclass WaveletMutationDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, Cr=0.7, wavelet='db1', local_search_iterations=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.Cr = Cr\n        self.wavelet = wavelet\n        self.local_search_iterations = local_search_iterations\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)].copy()\n        \n        while self.budget > self.pop_size:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n                mutant = population[i] + self.F * (x_r2 - x_r3)\n\n                # Wavelet Mutation\n                coeffs = pywt.wavedec(mutant, self.wavelet, level=min(4, pywt.dwt_max_level(len(mutant), pywt.Wavelet(self.wavelet).dec_len)))\n                for j in range(1, len(coeffs)):  # Skip the coarse approximation coefficients\n                    coeffs[j] = np.random.normal(0, self.F * np.std(coeffs[j]), size=coeffs[j].shape)\n                mutant = pywt.waverec(coeffs, self.wavelet)\n                \n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                for j in range(self.dim):\n                    if np.random.rand() < self.Cr:\n                        new_population[i, j] = mutant[j]\n                    else:\n                        new_population[i, j] = population[i, j]\n\n            # Evaluation\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n\n            # Selection and Local Search\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    population[i] = new_population[i]\n                    fitness[i] = new_fitness[i]\n                    if new_fitness[i] < self.f_opt:\n                        self.f_opt = new_fitness[i]\n                        self.x_opt = new_population[i].copy()\n                else:\n                    # Local search around the better solution\n                    x_local = population[i].copy()\n                    f_local = fitness[i]\n                    for _ in range(self.local_search_iterations):\n                        x_neighbor = x_local + np.random.normal(0, 0.05, size=self.dim)\n                        x_neighbor = np.clip(x_neighbor, func.bounds.lb, func.bounds.ub)\n                        f_neighbor = func(x_neighbor)\n                        self.budget -= 1\n                        if f_neighbor < f_local:\n                            x_local = x_neighbor.copy()\n                            f_local = f_neighbor\n                            if f_local < self.f_opt:\n                                self.f_opt = f_local\n                                self.x_opt = x_local.copy()\n                    population[i] = x_local\n                    fitness[i] = f_local\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 111, in evaluate\n    exec(code, safe_globals, local_env)\n  File \"<string>\", line 2, in <module>\nModuleNotFoundError: No module named 'pywt'\n.", "error": "", "parent_ids": ["664174e0-1753-41ff-bc0f-80323b424622"], "operator": null, "metadata": {}}
{"id": "517ed724-d409-4e8b-b2b7-343cadaae8d2", "fitness": -Infinity, "name": "CMAES_AdaptiveDE", "description": "An adaptive differential evolution strategy with a decaying exploration rate, covariance matrix adaptation for mutation, and a restart mechanism to escape local optima.", "code": "import numpy as np\n\nclass CMAES_AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=None, initial_sigma=0.1, restart_trigger=1e-9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 4 * dim if pop_size is None else pop_size  # Adaptive pop size\n        self.initial_sigma = initial_sigma\n        self.sigma = initial_sigma\n        self.mean = None\n        self.C = None\n        self.restart_trigger = restart_trigger\n        self.best_fitness_history = []\n        self.exploration_rate = 1.0\n\n    def initialize(self, func):\n        self.mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def sample_population(self, func):\n        z = np.random.multivariate_normal(np.zeros(self.dim), self.C, size=self.pop_size)\n        population = self.mean + self.sigma * z\n        population = np.clip(population, func.bounds.lb, func.bounds.ub)\n        return population\n\n    def update_distribution(self, population, fitness):\n        # Weighted recombination\n        weights = np.maximum(0, np.log(self.pop_size / 2 + 1) - np.log(np.arange(1, self.pop_size + 1)))\n        weights /= np.sum(weights)\n\n        sorted_indices = np.argsort(fitness)\n        best_individuals = population[sorted_indices]\n\n        new_mean = np.sum(weights[:, np.newaxis] * best_individuals, axis=0)\n\n        # Rank-one update of covariance matrix\n        y = best_individuals[0] - self.mean\n        self.C = (1 - 0.1) * self.C + 0.1 * np.outer(y / self.sigma, y / self.sigma)\n\n        self.mean = new_mean\n\n    def __call__(self, func):\n        self.initialize(func)\n        \n        evals = 0\n        while self.budget - evals > self.pop_size:\n            # Sample population\n            population = self.sample_population(func)\n            \n            # Evaluation\n            fitness = np.array([func(x) for x in population])\n            evals += self.pop_size\n\n            # Update best solution\n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n            self.best_fitness_history.append(self.f_opt)\n\n            # Update distribution parameters\n            self.update_distribution(population, fitness)\n\n            # Adapt step size (sigma)\n            self.sigma *= np.exp(0.5 * (np.mean(fitness) - self.f_opt) / np.std(fitness))\n\n            # Adjust exploration rate\n            self.exploration_rate *= 0.995\n            self.sigma *= self.exploration_rate\n            \n            # Restart mechanism\n            if self.f_opt < self.restart_trigger:\n                self.initialize(func)\n                self.sigma = self.initial_sigma # reset sigma after restart\n                \n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 144, in evaluate\n    algorithm(f_new)\n  File \"<string>\", line 50, in __call__\n  File \"<string>\", line 23, in sample_population\n  File \"mtrand.pyx\", line 4219, in numpy.random.mtrand.RandomState.multivariate_normal\n  File \"<__array_function__ internals>\", line 200, in svd\n  File \"/home/neocortex/.cache/pypoetry/virtualenvs/blade-xXF9vABH-py3.10/lib/python3.10/site-packages/numpy/linalg/linalg.py\", line 1642, in svd\n    u, s, vh = gufunc(a, signature=signature, extobj=extobj)\n  File \"/home/neocortex/.cache/pypoetry/virtualenvs/blade-xXF9vABH-py3.10/lib/python3.10/site-packages/numpy/linalg/linalg.py\", line 98, in _raise_linalgerror_svd_nonconvergence\n    raise LinAlgError(\"SVD did not converge\")\nnumpy.linalg.LinAlgError: SVD did not converge\n.", "error": "", "parent_ids": ["664174e0-1753-41ff-bc0f-80323b424622"], "operator": null, "metadata": {}}
{"id": "fdf69229-be45-4527-b8d3-00b3945831a9", "fitness": 0.09719942550236556, "name": "CauchyAdaptiveDE", "description": "A differential evolution strategy employing a Cauchy mutation operator, adaptive population sizing, and a restart mechanism to escape local optima.", "code": "import numpy as np\n\nclass CauchyAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, F=0.5, Cr=0.7, cauchy_scale=0.1, adaptive_pop_factor=0.1, restart_trigger=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.F = F\n        self.Cr = Cr\n        self.cauchy_scale = cauchy_scale\n        self.adaptive_pop_factor = adaptive_pop_factor\n        self.restart_trigger = restart_trigger\n        self.best_fitness_history = []\n        self.no_improvement_counter = 0\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n        self.best_fitness_history.append(self.f_opt)\n\n        while self.budget > self.pop_size:\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Mutation with Cauchy distribution\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n                cauchy_noise = self.cauchy_scale * np.random.standard_cauchy(size=self.dim)\n                mutant = population[i] + self.F * (x_r2 - x_r3) + cauchy_noise\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                for j in range(self.dim):\n                    if np.random.rand() < self.Cr:\n                        new_population[i, j] = mutant[j]\n                    else:\n                        new_population[i, j] = population[i, j]\n\n            # Evaluation\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n\n            # Selection\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    population[i] = new_population[i]\n                    fitness[i] = new_fitness[i]\n                    if new_fitness[i] < self.f_opt:\n                        self.f_opt = new_fitness[i]\n                        self.x_opt = new_population[i]\n                        self.no_improvement_counter = 0  # Reset counter if improvement found\n                else:\n                    self.no_improvement_counter +=1\n\n            self.best_fitness_history.append(self.f_opt)\n            \n            # Adaptive Population Sizing\n            if self.no_improvement_counter > self.restart_trigger // 2:\n                new_pop_size = int(self.pop_size * (1 + self.adaptive_pop_factor))\n                new_pop_size = min(new_pop_size, self.budget) # Limit pop size by remaining budget\n                if new_pop_size > self.pop_size:\n                    additional_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(new_pop_size - self.pop_size, self.dim))\n                    population = np.vstack((population, additional_individuals))\n                    fitness = np.concatenate((fitness, np.array([func(x) for x in additional_individuals])))\n                    self.budget -= (new_pop_size - self.pop_size)\n                    self.pop_size = new_pop_size\n\n            # Restart Mechanism\n            if self.no_improvement_counter > self.restart_trigger:\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                self.f_opt = np.min(fitness)\n                self.x_opt = population[np.argmin(fitness)]\n                self.no_improvement_counter = 0  # Reset counter after restart\n                self.best_fitness_history.append(self.f_opt)\n                \n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm CauchyAdaptiveDE scored 0.097 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["664174e0-1753-41ff-bc0f-80323b424622"], "operator": null, "metadata": {"aucs": [0.11131218952332766, 0.18028608698376902, 0]}}
{"id": "785bf1b0-ca3b-4d0a-a2f4-07c5491f7e9c", "fitness": -Infinity, "name": "AdaptiveMutationPoolDE", "description": "A differential evolution strategy that utilizes a pool of mutation strategies and adaptively selects them based on their recent success rates, combined with a local search operator to fine-tune promising solutions.", "code": "import numpy as np\n\nclass AdaptiveMutationPoolDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, Cr=0.9, ls_prob=0.1, mutation_pool_size=5, success_memory=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.Cr = Cr\n        self.ls_prob = ls_prob  # Probability of applying local search\n        self.mutation_pool_size = mutation_pool_size\n        self.success_memory = success_memory # Number of generations to track mutation success\n        self.best_fitness_history = []\n        self.population = None\n        self.fitness = None\n        self.mutation_strategies = [\n            self._mutation_rand1,\n            self._mutation_best1,\n            self._mutation_current_to_rand1,\n            self._mutation_current_to_best1,\n            self._mutation_rand2,\n        ]\n        self.mutation_success_counts = np.zeros(len(self.mutation_strategies))\n        self.mutation_usage_counts = np.ones(len(self.mutation_strategies)) #Initialize to 1 to avoid division by zero\n        self.mutation_success_history = [[] for _ in range(len(self.mutation_strategies))]\n\n    def __call__(self, func):\n        # Initialization\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        \n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.best_fitness_history.append(self.f_opt)\n\n        while self.budget > self.pop_size:\n            new_population = np.copy(self.population)\n            new_fitness = np.copy(self.fitness)\n            \n            mutation_probs = self.mutation_success_counts / self.mutation_usage_counts\n            mutation_probs /= np.sum(mutation_probs)\n            \n            for i in range(self.pop_size):\n                # Adaptive Mutation Strategy Selection\n                mutation_index = np.random.choice(len(self.mutation_strategies), p=mutation_probs)\n                self.mutation_usage_counts[mutation_index] +=1\n                mutant = self.mutation_strategies[mutation_index](i)\n\n                # Crossover\n                for j in range(self.dim):\n                    if np.random.rand() < self.Cr:\n                        new_population[i, j] = mutant[j]\n                    else:\n                         new_population[i, j] = self.population[i, j]\n                \n                new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n\n            # Evaluation\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n\n            # Selection\n            for i in range(self.pop_size):\n                if new_fitness[i] < self.fitness[i]:\n                    if (new_fitness[i] - self.fitness[i]) / abs(self.fitness[i]) < 0.01: #Success criteria\n                        self.mutation_success_counts[mutation_index] += 1\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n\n                    if new_fitness[i] < self.f_opt:\n                        self.f_opt = new_fitness[i]\n                        self.x_opt = new_population[i]\n                \n            # Local Search\n            for i in range(self.pop_size):\n                if np.random.rand() < self.ls_prob:\n                    x_ls = self._local_search(self.population[i], func)\n                    f_ls = func(x_ls)\n                    self.budget -=1\n                    if f_ls < self.fitness[i]:\n                        self.population[i] = x_ls\n                        self.fitness[i] = f_ls\n                        if f_ls < self.f_opt:\n                            self.f_opt = f_ls\n                            self.x_opt = x_ls\n\n            self.best_fitness_history.append(self.f_opt)\n            self.mutation_success_counts *= 0.95 #Discount success\n            self.mutation_success_counts += 0.05\n\n\n        return self.f_opt, self.x_opt\n\n    def _mutation_rand1(self, i):\n        indices = [j for j in range(self.pop_size) if j != i]\n        idxs = np.random.choice(indices, size=3, replace=False)\n        x_r1, x_r2, x_r3 = self.population[idxs[0]], self.population[idxs[1]], self.population[idxs[2]]\n        F = np.random.uniform(0.5, 1.0)\n        return self.population[i] + F * (x_r1 - x_r2 + x_r3 - self.population[i])\n    \n    def _mutation_best1(self, i):\n        indices = [j for j in range(self.pop_size) if j != i]\n        idxs = np.random.choice(indices, size=2, replace=False)\n        x_r1, x_r2 = self.population[idxs[0]], self.population[idxs[1]]\n        F = np.random.uniform(0.5, 1.0)\n        return self.x_opt + F * (x_r1 - x_r2)\n\n    def _mutation_current_to_rand1(self, i):\n        indices = [j for j in range(self.pop_size) if j != i]\n        idxs = np.random.choice(indices, size=3, replace=False)\n        x_r1, x_r2, x_r3 = self.population[idxs[0]], self.population[idxs[1]], self.population[idxs[2]]\n        F = np.random.uniform(0.5, 1.0)\n        return self.population[i] + F * (x_r1 - self.population[i] + x_r2 - x_r3)\n\n    def _mutation_current_to_best1(self, i):\n         F = np.random.uniform(0.5, 1.0)\n         return self.population[i] + F * (self.x_opt - self.population[i])\n\n    def _mutation_rand2(self, i):\n        indices = [j for j in range(self.pop_size) if j != i]\n        idxs = np.random.choice(indices, size=5, replace=False)\n        x_r1, x_r2, x_r3, x_r4, x_r5 = self.population[idxs[0]], self.population[idxs[1]], self.population[idxs[2]], self.population[idxs[3]], self.population[idxs[4]]\n        F = np.random.uniform(0.5, 1.0)\n        return self.population[i] + F * (x_r1 - x_r2 + x_r3 - x_r4)\n\n    def _local_search(self, x, func, radius=0.1, num_samples=5):\n        best_x = x\n        best_f = func(x)\n        for _ in range(num_samples):\n            x_new = x + np.random.uniform(-radius, radius, size=self.dim)\n            x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n            f_new = func(x_new)\n            if f_new < best_f:\n                best_f = f_new\n                best_x = x_new\n        return best_x", "configspace": "", "generation": 8, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 45, in __call__\n  File \"mtrand.pyx\", line 954, in numpy.random.mtrand.RandomState.choice\nValueError: probabilities contain NaN\n.", "error": "", "parent_ids": ["8f402b2c-a363-4549-a6e1-a29c21c3192d"], "operator": null, "metadata": {}}
{"id": "0aa6b103-7702-4f9c-8c19-1bd61fec24f8", "fitness": 0.41935811912058263, "name": "VoronoiAdaptiveDE", "description": "A differential evolution strategy that uses a Voronoi tessellation to identify promising search regions and adapt mutation parameters based on the density of individuals within each Voronoi cell.", "code": "import numpy as np\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\n\nclass VoronoiAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, Cr=0.9, F=0.5, voronoi_refresh_rate=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.Cr = Cr\n        self.F = F\n        self.voronoi_refresh_rate = voronoi_refresh_rate\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.generation = 0\n        self.voronoi = None\n        self.regions = None\n        self.vertices = None\n        self.point_region = None\n\n    def __call__(self, func):\n        # Initialization\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        \n        while self.budget > self.pop_size:\n            if self.generation % self.voronoi_refresh_rate == 0:\n                try:\n                  self.voronoi = Voronoi(self.population)\n                  self.regions = self.voronoi.regions\n                  self.vertices = self.voronoi.vertices\n                  self.point_region = self.voronoi.point_region\n                except Exception as e:\n                  # Handle cases where Voronoi computation fails (e.g., due to identical points)\n                  # Fallback strategy: small random perturbation\n                  self.population += np.random.normal(0, 1e-6, size=self.population.shape)\n                  try:\n                    self.voronoi = Voronoi(self.population)\n                    self.regions = self.voronoi.regions\n                    self.vertices = self.voronoi.vertices\n                    self.point_region = self.voronoi.point_region\n                  except:\n                    pass\n\n            new_population = np.copy(self.population)\n            for i in range(self.pop_size):\n                # Mutation\n                indices = [j for j in range(self.pop_size) if j != i]\n                idxs = np.random.choice(indices, size=3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[idxs[0]], self.population[idxs[1]], self.population[idxs[2]]\n\n                # Voronoi-based F adaptation\n                if self.voronoi is not None and self.point_region[i] != -1 and self.regions[self.point_region[i]] and all(v >= 0 for v in self.regions[self.point_region[i]]):\n                    region_index = self.point_region[i]\n                    num_vertices = len(self.regions[region_index]) #Density of voronoi cell\n                    F = 0.1 + 0.9 * np.exp(-num_vertices/10) #Denser Voronoi cell, smaller F\n                else:\n                    F = self.F\n                \n                mutant = self.population[i] + F * (x_r1 - x_r2)\n\n                # Crossover\n                for j in range(self.dim):\n                    if np.random.rand() < self.Cr:\n                        new_population[i, j] = mutant[j]\n                    else:\n                        new_population[i, j] = self.population[i, j]\n\n                new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n\n            # Evaluation\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n\n            # Selection\n            for i in range(self.pop_size):\n                if new_fitness[i] < self.fitness[i]:\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n\n                    if new_fitness[i] < self.f_opt:\n                        self.f_opt = new_fitness[i]\n                        self.x_opt = new_population[i]\n\n            self.generation += 1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm VoronoiAdaptiveDE scored 0.419 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["8f402b2c-a363-4549-a6e1-a29c21c3192d"], "operator": null, "metadata": {"aucs": [0.15532049013894866, 0.2852855744720013, 0.3938711420072306, 0.5617766343604254, 0.30191935496384004, 0.45153245137992626, 0.2952433361532151, 0.34613313571349513, 0.32896653812720944, 0.1852321879938741, 0.6334103784175006, 0.9946097915784944, 0.3722898072399339, 0.2915932173505763, 0.7981669463211121, 0.43738089522287016, 0.3304100218314948, 0.548182814364521, 0.18846052078178765, 0.487377143993196]}}
{"id": "87701173-2f38-4965-be67-156b283dd77a", "fitness": 0.42246413630802493, "name": "SuccessHistoryAdaptiveDE", "description": "A differential evolution strategy that adaptively adjusts mutation strength and crossover rate based on the success history of previous generations, while also incorporating a Cauchy mutation operator for enhanced exploration.", "code": "import numpy as np\n\nclass SuccessHistoryAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, memory_size=10, initial_F=0.5, initial_Cr=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.memory_size = memory_size\n        self.F_memory = np.full(memory_size, initial_F)\n        self.Cr_memory = np.full(memory_size, initial_Cr)\n        self.memory_index = 0\n        self.best_fitness_history = []\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n        self.best_fitness_history.append(self.f_opt)\n\n        success_F_list = []\n        success_Cr_list = []\n        \n        while self.budget > self.pop_size:\n            new_population = np.copy(population)\n            new_fitness = np.zeros(self.pop_size)\n            \n            for i in range(self.pop_size):\n                # Adaptation of F and Cr\n                F = self.F_memory[np.random.randint(self.memory_size)]\n                Cr = self.Cr_memory[np.random.randint(self.memory_size)]\n                \n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n                \n                # Cauchy mutation with probability 0.1, otherwise standard DE mutation\n                if np.random.rand() < 0.1:\n                   mutant = population[i] + F * (x_r1 - population[i]) + 0.1 * np.random.standard_cauchy(size=self.dim)\n                else:\n                   mutant = population[i] + F * (x_r2 - x_r3)\n                \n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                for j in range(self.dim):\n                    if np.random.rand() < Cr:\n                        new_population[i, j] = mutant[j]\n                    else:\n                        new_population[i, j] = population[i, j]\n                        \n            # Evaluation\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n\n            # Selection and update success history\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    success_F_list.append(F)\n                    success_Cr_list.append(Cr)\n                    population[i] = new_population[i]\n                    fitness[i] = new_fitness[i]\n                    if new_fitness[i] < self.f_opt:\n                        self.f_opt = new_fitness[i]\n                        self.x_opt = new_population[i]\n            self.best_fitness_history.append(self.f_opt)\n            \n            # Update memory\n            if success_F_list:\n                self.F_memory[self.memory_index] = np.mean(success_F_list)\n                self.Cr_memory[self.memory_index] = np.mean(success_Cr_list)\n                self.memory_index = (self.memory_index + 1) % self.memory_size\n\n            success_F_list = []\n            success_Cr_list = []\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm SuccessHistoryAdaptiveDE scored 0.422 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["664174e0-1753-41ff-bc0f-80323b424622"], "operator": null, "metadata": {"aucs": [0.1556580766239145, 0.24665709042953665, 0.42007447682774257, 0.6059510263373715, 0.31763850015125783, 0.49514706474736403, 0.30470914505515934, 0.373860892024113, 0.3759358904727721, 0.1981705089425534, 0.5392861854886699, 0.9995834779108326, 0.28171246972108843, 0.31519832441677464, 0.7538506407579201, 0.47972881589661287, 0.35293439337776045, 0.5405032885594808, 0.1980803264855977, 0.49460213193397784]}}
{"id": "54d6759f-a36a-4370-891f-7da1edd003cd", "fitness": 0.40859060991489926, "name": "SuccessRateAdaptiveDE", "description": "A differential evolution strategy that dynamically adjusts its mutation and crossover rates based on the success rate of recent parameter settings, while incorporating orthogonal learning to enhance search efficiency.", "code": "import numpy as np\n\nclass SuccessRateAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, Cr_initial=0.5, F_initial=0.7, success_rate_window=20, orthogonal_learning_prob=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.Cr = Cr_initial\n        self.F = F_initial\n        self.success_rate_window = success_rate_window\n        self.orthogonal_learning_prob = orthogonal_learning_prob\n        self.success_rates_Cr = []\n        self.success_rates_F = []\n        self.archive = []\n        self.archive_fitness = []\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        while self.budget > self.pop_size:\n            # Mutation and Crossover\n            new_population = np.copy(population)\n            successful_Cr = []\n            successful_F = []\n\n            for i in range(self.pop_size):\n                # Mutation\n                r1, r2, r3 = np.random.choice(self.pop_size, 3, replace=False)\n                mutant = population[r1] + self.F * (population[r2] - population[r3])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                for j in range(self.dim):\n                    if np.random.rand() < self.Cr:\n                        new_population[i, j] = mutant[j]\n                    else:\n                        new_population[i, j] = population[i, j]\n\n                new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n            \n            # Evaluation\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n\n            # Selection\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    successful_Cr.append(self.Cr)\n                    successful_F.append(self.F)\n                    \n                    population[i] = new_population[i]\n                    fitness[i] = new_fitness[i]\n                    \n                    if new_fitness[i] < self.f_opt:\n                        self.f_opt = new_fitness[i]\n                        self.x_opt = new_population[i]\n                        \n                        #Archive successful solutions\n                        if len(self.archive) < self.pop_size:\n                           self.archive.append(new_population[i])\n                           self.archive_fitness.append(new_fitness[i])\n                        else:\n                            max_archive_fitness_index = np.argmax(self.archive_fitness)\n                            if new_fitness[i] < self.archive_fitness[max_archive_fitness_index]:\n                                self.archive[max_archive_fitness_index] = new_population[i]\n                                self.archive_fitness[max_archive_fitness_index] = new_fitness[i]\n                            \n            # Adaptive Parameter Control based on Success Rate\n            if successful_Cr:\n                self.success_rates_Cr.append(np.mean(successful_Cr))\n            else:\n                self.success_rates_Cr.append(0)  # No successful Cr value\n\n            if successful_F:\n                self.success_rates_F.append(np.mean(successful_F))\n            else:\n                self.success_rates_F.append(0)\n\n            if len(self.success_rates_Cr) > self.success_rate_window:\n                self.success_rates_Cr.pop(0)\n            if len(self.success_rates_F) > self.success_rate_window:\n                self.success_rates_F.pop(0)\n\n            # Adjust Cr and F based on average success rate over the window\n            if self.success_rates_Cr:\n                avg_success_Cr = np.mean(self.success_rates_Cr)\n                self.Cr = min(0.95, max(0.1, avg_success_Cr * 1.2))  # Adjust Cr, prevent too high or low\n            if self.success_rates_F:\n                avg_success_F = np.mean(self.success_rates_F)\n                self.F = min(1.2, max(0.2, avg_success_F * 1.1)) # Adjust F\n\n            # Orthogonal Learning\n            if np.random.rand() < self.orthogonal_learning_prob and len(self.archive) >= 2:\n                # Select two parents from the archive\n                parent1_idx, parent2_idx = np.random.choice(len(self.archive), 2, replace=False)\n                parent1 = self.archive[parent1_idx]\n                parent2 = self.archive[parent2_idx]\n\n                # Generate a new individual based on orthogonal array design\n                orthogonal_individual = np.zeros(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < 0.5:\n                        orthogonal_individual[j] = parent1[j]\n                    else:\n                        orthogonal_individual[j] = parent2[j]\n\n                orthogonal_individual = np.clip(orthogonal_individual, func.bounds.lb, func.bounds.ub)\n                orthogonal_fitness = func(orthogonal_individual)\n                self.budget -= 1\n\n                # Replace the worst individual in the population with the orthogonal individual if it's better\n                worst_idx = np.argmax(fitness)\n                if orthogonal_fitness < fitness[worst_idx]:\n                    population[worst_idx] = orthogonal_individual\n                    fitness[worst_idx] = orthogonal_fitness\n\n                    if orthogonal_fitness < self.f_opt:\n                        self.f_opt = orthogonal_fitness\n                        self.x_opt = orthogonal_individual\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm SuccessRateAdaptiveDE scored 0.409 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e9a531ba-15eb-46f7-ac14-202dd28ed3f9"], "operator": null, "metadata": {"aucs": [0.18137366080355832, 0.2344760646782975, 0.3565809267003469, 0.4751763358810378, 0.3596159799278843, 0.3875913394227849, 0.29329098317574487, 0.3255156174993077, 0.32392490618735104, 0.23996824982619236, 0.4899072336714151, 0.9978985473287078, 0.37963513074430333, 0.3448803124579539, 0.8470439644111303, 0.4198367503714755, 0.3059449959707107, 0.48569880335491666, 0.23929202959434626, 0.4841603662905206]}}
{"id": "7025c43a-ff4b-4206-a104-993d013ec565", "fitness": 0.619500942791239, "name": "EnsembleAdaptiveDE", "description": "A differential evolution strategy using a dynamic ensemble of mutation strategies and a success-history based parameter adaptation.", "code": "import numpy as np\n\nclass EnsembleAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, memory_size=10, strategy_probabilities=[0.3, 0.3, 0.4]):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.memory_size = memory_size\n        self.strategy_probabilities = strategy_probabilities\n        self.F_memory = np.ones(self.memory_size) * 0.5\n        self.Cr_memory = np.ones(self.memory_size) * 0.5\n        self.archive_rate = 0.1\n        self.archive = None\n        self.strategy_successes = np.zeros(len(strategy_probabilities))\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Initialization\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.archive = np.copy(self.population[:int(self.pop_size * self.archive_rate)])\n\n        memory_index = 0\n\n        while self.budget > self.pop_size:\n            new_population = np.copy(self.population)\n            new_fitness = np.zeros(self.pop_size)\n\n            # Sample F and Cr from memory\n            F = np.random.choice(self.F_memory)\n            Cr = np.random.choice(self.Cr_memory)\n\n            for i in range(self.pop_size):\n                # Choose a mutation strategy based on probabilities\n                strategy_choice = np.random.choice(len(self.strategy_probabilities), p=self.strategy_probabilities)\n\n                if strategy_choice == 0:  # DE/rand/1\n                    indices = np.random.choice(self.pop_size, size=3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices[0]], self.population[indices[1]], self.population[indices[2]]\n                    mutant = x_r1 + F * (x_r2 - x_r3)\n                elif strategy_choice == 1:  # DE/current-to-best/1\n                    indices = np.random.choice(self.pop_size, size=2, replace=False)\n                    x_r1, x_r2 = self.population[indices[0]], self.population[indices[1]]\n                    mutant = self.population[i] + F * (self.x_opt - self.population[i]) + F * (x_r1 - x_r2)\n                else:  # DE/rand/2\n                    indices = np.random.choice(self.pop_size, size=5, replace=False)\n                    x_r1, x_r2, x_r3, x_r4, x_r5 = self.population[indices[0]], self.population[indices[1]], self.population[indices[2]], self.population[indices[4]], self.population[indices[4]]\n                    mutant = x_r1 + F * (x_r2 - x_r3) + F * (x_r4-x_r5)\n\n                # Crossover\n                for j in range(self.dim):\n                    if np.random.rand() < Cr:\n                        new_population[i, j] = mutant[j]\n                    else:\n                        new_population[i, j] = self.population[i, j]\n                \n                new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                new_fitness[i] = func(new_population[i])\n                self.budget -= 1\n\n            # Selection\n            for i in range(self.pop_size):\n                if new_fitness[i] < self.fitness[i]:\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n                    if new_fitness[i] < self.f_opt:\n                        self.f_opt = new_fitness[i]\n                        self.x_opt = new_population[i]\n                \n            # Update memory (simplified - replace oldest)\n            self.F_memory[memory_index] = F\n            self.Cr_memory[memory_index] = Cr\n            memory_index = (memory_index + 1) % self.memory_size\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm EnsembleAdaptiveDE scored 0.620 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["8f402b2c-a363-4549-a6e1-a29c21c3192d"], "operator": null, "metadata": {"aucs": [0.21304170399825817, 0.3410166946221286, 0.6410815463765305, 0.8370708278176597, 0.7067012260968364, 0.7818059177602739, 0.5859081525458065, 0.6150123500527875, 0.7173294785425322, 0.3775539252642419, 0.7992595970996665, 0.9992403582673014, 0.3038715455614638, 0.6577850901757822, 0.8612832180446397, 0.7932866457846075, 0.5648058328786153, 0.8519929093383662, 0.21821868188204785, 0.5237531537152335]}}
{"id": "a67dd86d-1ea7-4b38-a668-6c5099459b6a", "fitness": 0.39002170328057584, "name": "CauchyCrossoverAdaptiveDE", "description": "A differential evolution strategy that leverages a Cauchy mutation operator for enhanced exploration and adapts the crossover rate based on the success rate of previous generations.", "code": "import numpy as np\n\nclass CauchyCrossoverAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, Cr_init=0.5, F=0.5, learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.Cr = Cr_init\n        self.F = F\n        self.learning_rate = learning_rate\n        self.best_fitness_history = []\n        self.population = None\n        self.fitness = None\n        self.success_Cr = []\n        self.archive = []\n\n    def __call__(self, func):\n        # Initialization\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        \n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.best_fitness_history.append(self.f_opt)\n        \n        generation = 0\n\n        while self.budget > self.pop_size:\n            new_population = np.copy(self.population)\n            new_fitness = np.zeros(self.pop_size)\n            successful_offspring = 0\n            Cr_sum = 0\n\n            for i in range(self.pop_size):\n                # Mutation using Cauchy distribution\n                indices = [j for j in range(self.pop_size) if j != i]\n                idxs = np.random.choice(indices, size=3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[idxs[0]], self.population[idxs[1]], self.population[idxs[2]]\n                \n                # Cauchy mutation\n                cauchy_rand = np.random.standard_cauchy(size=self.dim)\n                mutant = self.population[i] + self.F * (x_r1 - x_r2) + 0.01 * cauchy_rand  # Added Cauchy noise\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.Cr or j == j_rand:\n                        new_population[i, j] = mutant[j]\n                    \n                new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                new_fitness[i] = func(new_population[i])\n                self.budget -= 1\n\n                # Selection\n                if new_fitness[i] < self.fitness[i]:\n                    self.archive.append(self.population[i].copy())\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n                    successful_offspring += 1\n                    Cr_sum += self.Cr\n                    if new_fitness[i] < self.f_opt:\n                        self.f_opt = new_fitness[i]\n                        self.x_opt = new_population[i]\n\n\n            # Adapt Cr\n            if successful_offspring > 0:\n                mean_Cr = Cr_sum / successful_offspring\n                self.Cr = (1 - self.learning_rate) * self.Cr + self.learning_rate * mean_Cr\n            else:\n                self.Cr = (1 - self.learning_rate) * self.Cr + self.learning_rate * np.random.rand() # Randomize if no success\n                \n            self.Cr = np.clip(self.Cr, 0.1, 0.9)\n\n            self.best_fitness_history.append(self.f_opt)\n            generation += 1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm CauchyCrossoverAdaptiveDE scored 0.390 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["8f402b2c-a363-4549-a6e1-a29c21c3192d"], "operator": null, "metadata": {"aucs": [0.14151779628493655, 0.23516722148003044, 0.3846889581242644, 0.3988982051197558, 0.3227055657022534, 0.4120690668887945, 0.3010088628486558, 0.342651552607807, 0.3039605535246297, 0.1985149965038524, 0.46330079191058116, 0.9884529844249188, 0.3253764164494215, 0.2955981033005606, 0.7199683213051326, 0.42332771574829053, 0.335361147264299, 0.5142570381646028, 0.20226623237703922, 0.49134253558169116]}}
{"id": "afe60a59-bc32-4d9f-99ee-c3738b858056", "fitness": 0.4834584499783289, "name": "MultiStrategyAdaptiveDE", "description": "A differential evolution strategy using a multi-strategy approach with dynamic switching between mutation strategies based on population diversity and success rate.", "code": "import numpy as np\n\nclass MultiStrategyAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, Cr=0.9, F_initial=0.5, strategy_prob=[0.3, 0.3, 0.4], diversity_threshold=0.1, success_memory=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.Cr = Cr\n        self.F = F_initial\n        self.strategy_prob = strategy_prob  # Probabilities for each mutation strategy\n        self.diversity_threshold = diversity_threshold\n        self.success_memory = success_memory\n        self.success_rates = [0.0] * len(self.strategy_prob)  # Track success rates of each strategy\n        self.strategy_counts = [0] * len(self.strategy_prob)\n        self.best_fitness_history = []\n        self.last_improvement = 0\n        self.generation = 0\n\n    def calculate_diversity(self, population):\n        \"\"\"Calculates the diversity of the population based on the mean pairwise distance.\"\"\"\n        distances = np.sum((population[:, np.newaxis, :] - population[np.newaxis, :, :]) ** 2, axis=2)\n        diversity = np.mean(distances)\n        return diversity\n\n    def mutation_strategy(self, population, i, strategy_index):\n        \"\"\"Applies different mutation strategies based on the strategy index.\"\"\"\n        if strategy_index == 0:  # DE/rand/1\n            indices = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = population[indices]\n            return x_r1 + self.F * (x_r2 - x_r3)\n        elif strategy_index == 1:  # DE/current-to-rand/1\n            indices = np.random.choice(self.pop_size, 2, replace=False)\n            x_r1, x_r2 = population[indices]\n            return population[i] + self.F * (x_r1 - population[i]) + self.F * (x_r2 - population[i])\n        elif strategy_index == 2:  # DE/current-to-best/1\n            best_index = np.argmin(self.fitness)\n            indices = np.random.choice(self.pop_size, 2, replace=False)\n            x_r1, x_r2 = population[indices]\n            return population[i] + self.F * (population[best_index] - population[i]) + self.F * (x_r1 - x_r2)\n        else:\n            raise ValueError(\"Invalid strategy index.\")\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        self.f_opt = np.min(self.fitness)\n        self.x_opt = population[np.argmin(self.fitness)]\n        self.best_fitness_history.append(self.f_opt)\n        self.last_improvement = 0\n        self.generation = 0\n        success_memory = []\n\n        while self.budget > self.pop_size:\n            # Dynamic adjustment of strategy probabilities based on success\n            if len(success_memory) >= self.success_memory:\n                success_memory.pop(0)\n\n            new_population = np.copy(population)\n            new_fitness = np.copy(self.fitness)\n\n            for i in range(self.pop_size):\n                # Select mutation strategy based on probabilities\n                strategy_index = np.random.choice(len(self.strategy_prob), p=self.strategy_prob)\n                self.strategy_counts[strategy_index] += 1\n\n                # Mutation\n                mutant = self.mutation_strategy(population, i, strategy_index)\n\n                # Crossover\n                for j in range(self.dim):\n                    if np.random.rand() < self.Cr:\n                        new_population[i, j] = mutant[j]\n                    else:\n                        new_population[i, j] = population[i, j]\n                \n                new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n\n            # Evaluation\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n\n            # Selection and Success tracking\n            for i in range(self.pop_size):\n                if new_fitness[i] < self.fitness[i]:\n                    # Strategy success\n                    success_memory.append(strategy_index)\n                    population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n\n                    if new_fitness[i] < self.f_opt:\n                        self.f_opt = new_fitness[i]\n                        self.x_opt = new_population[i]\n                        self.last_improvement = self.generation\n\n            self.best_fitness_history.append(self.f_opt)\n\n            # Calculate and adjust strategy probabilities based on recent successes\n            success_counts = [success_memory.count(k) for k in range(len(self.strategy_prob))]\n            total_successes = sum(success_counts)\n            if total_successes > 0:\n                self.strategy_prob = [(count / total_successes) for count in success_counts]\n            else: # If there are no success, keep initial probabilities.\n                self.strategy_prob = [p for p in self.strategy_prob]\n                self.strategy_prob = [p / sum(self.strategy_prob) for p in self.strategy_prob] # Normalize\n\n            # Diversity check\n            diversity = self.calculate_diversity(population)\n            if diversity < self.diversity_threshold:\n                # Increase exploration by increasing mutation rate\n                self.F = min(self.F * 1.1, 1.0)\n            else:\n                # Decrease exploration if diversity is high\n                self.F = max(self.F * 0.9, 0.1)\n            \n            self.generation += 1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm MultiStrategyAdaptiveDE scored 0.483 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e9a531ba-15eb-46f7-ac14-202dd28ed3f9"], "operator": null, "metadata": {"aucs": [0.15428869429282366, 0.2830292629568746, 0.6694300409585863, 0.7688218336707014, 0.5874944940770196, 0.18797428938547345, 0.542847582337718, 0.5168558671107574, 0.20212704721557972, 0.1843244974591619, 0.6830337069642551, 0.9968154387221019, 0.27561857556067904, 0.2603718588953743, 0.7519003364342914, 0.7106911320608926, 0.32798960176669467, 0.7481733351102691, 0.30242582890446, 0.5149555756828654]}}
{"id": "719dd1e7-51de-4f02-ae8b-111eb9e505bb", "fitness": 0.2926784548176455, "name": "NeighborhoodAdaptiveDE", "description": "A differential evolution strategy that dynamically adjusts its mutation factor and population diversity based on a neighborhood-based fitness landscape analysis, incorporating a fitness-dependent reproduction probability.", "code": "import numpy as np\n\nclass NeighborhoodAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, Cr=0.9, F_initial=0.5, neighborhood_size=5, diversity_threshold=0.1, fitness_scaling=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.Cr = Cr\n        self.F = F_initial\n        self.neighborhood_size = neighborhood_size\n        self.diversity_threshold = diversity_threshold\n        self.fitness_scaling = fitness_scaling\n        self.generation = 0\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n        \n        while self.budget > self.pop_size:\n            new_population = np.copy(population)\n            new_fitness = np.zeros(self.pop_size)\n\n            for i in range(self.pop_size):\n                # Neighborhood Selection\n                neighborhood_indices = np.random.choice(self.pop_size, self.neighborhood_size, replace=False)\n                neighborhood = population[neighborhood_indices]\n                neighborhood_fitness = fitness[neighborhood_indices]\n                \n                # Fitness Landscape Analysis (Neighborhood Diversity)\n                diversity = np.std(neighborhood)\n                \n                # Adaptive F\n                if diversity < self.diversity_threshold:\n                    F = self.F * 1.5  # Increase mutation strength if neighborhood is too homogeneous\n                else:\n                    F = self.F * 0.8  # Decrease mutation strength if neighborhood is diverse\n                F = np.clip(F, 0.1, 2.0)\n                \n                # Mutation\n                r1, r2, r3 = np.random.choice(self.pop_size, 3, replace=False)\n                mutant = population[i] + F * (population[r1] - population[r2])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                for j in range(self.dim):\n                    if np.random.rand() < self.Cr:\n                        new_population[i, j] = mutant[j]\n                    # else: new_population[i,j] = population[i,j] #no change\n\n                new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                \n                # Evaluation\n                new_fitness[i] = func(new_population[i])\n                self.budget -= 1\n\n            # Selection with Fitness-Dependent Reproduction Probability\n            for i in range(self.pop_size):\n                # Scale fitness values to create probabilities\n                scaled_fitness = np.exp(-self.fitness_scaling * (new_fitness - np.min(new_fitness)))\n                probabilities = scaled_fitness / np.sum(scaled_fitness)\n\n                # Randomly choose whether to replace based on the fitness-dependent probabilities\n                if np.random.rand() < probabilities[i]: # Probability is proportional to fitness\n                  if new_fitness[i] < fitness[i]:\n                    population[i] = new_population[i]\n                    fitness[i] = new_fitness[i]\n\n                    if new_fitness[i] < self.f_opt:\n                        self.f_opt = new_fitness[i]\n                        self.x_opt = new_population[i]\n\n            self.generation += 1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm NeighborhoodAdaptiveDE scored 0.293 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e9a531ba-15eb-46f7-ac14-202dd28ed3f9"], "operator": null, "metadata": {"aucs": [0.12646425487620472, 0.20814517251570464, 0.30227562056757185, 0.20123958426147848, 0.21866440813117127, 0.22422374005440837, 0.2503442380807194, 0.21828322676633638, 0.21064229606544171, 0.17746413265341965, 0.3107816929284798, 0.9978926848024406, 0.2708721521940415, 0.21838214175164017, 0.5708217676649128, 0.26156355701404455, 0.23640051888262448, 0.23972566476408486, 0.16058455880306355, 0.44879768357512184]}}
