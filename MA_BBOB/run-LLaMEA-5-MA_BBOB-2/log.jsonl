{"id": "4ac09956-f476-42ea-97ce-0b1aa5673b53", "fitness": 0.0, "name": "AdaptiveDE", "description": "An adaptive Differential Evolution strategy that adjusts mutation and crossover rates based on success, combined with a local search to refine promising solutions.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, mutation_factor=0.5, crossover_rate=0.7, local_search_iterations=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.mutation_factor = mutation_factor\n        self.crossover_rate = crossover_rate\n        self.local_search_iterations = local_search_iterations\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.eval_count += self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index].copy()\n        \n        generation = 0\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = a + self.mutation_factor * (b - c)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.crossover_rate\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    # Local Search\n                    for _ in range(self.local_search_iterations):\n                        perturbation = np.random.normal(0, 0.01, size=self.dim)\n                        local_trial = trial + perturbation\n                        local_trial = np.clip(local_trial, func.bounds.lb, func.bounds.ub)\n                        f_local = func(local_trial)\n                        self.eval_count += 1\n                        if f_local < f_trial:\n                            f_trial = f_local\n                            trial = local_trial\n                            fitness[i] = f_trial\n                            population[i] = trial\n                        else:\n                            break\n\n                # Update optimal solution\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial.copy()\n                \n                if self.eval_count >= self.budget:\n                    break\n            \n            # Adaptive parameter adjustment\n            if generation % 10 == 0:\n                success_indices = np.where(fitness < np.mean(fitness))[0]\n                if len(success_indices) > 0:\n                    self.mutation_factor *= np.random.uniform(0.8, 1.2)\n                    self.crossover_rate *= np.random.uniform(0.8, 1.2)\n                    self.mutation_factor = np.clip(self.mutation_factor, 0.1, 1.0)\n                    self.crossover_rate = np.clip(self.crossover_rate, 0.1, 1.0)\n            \n            generation += 1\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0]}}
{"id": "6dbf843f-a357-471a-a757-f847d2551854", "fitness": -Infinity, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with a population-wide learning rate and stochastic universal sampling for parent selection.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7):\n        \"\"\"\n        Initialize the Adaptive Differential Evolution algorithm.\n\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size (int): The population size.\n            F (float): The initial scaling factor.\n            CR (float): The initial crossover rate.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n\n    def initialize_population(self, func):\n        \"\"\"Initialize the population randomly within the bounds.\"\"\"\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.pop[np.argmin(self.fitness)]\n\n    def stochastic_universal_sampling(self):\n        \"\"\"Select parents using Stochastic Universal Sampling.\"\"\"\n        \n        probabilities = (np.max(self.fitness) - self.fitness) / np.sum(np.max(self.fitness) - self.fitness) # Fitness proportionate selection (minimize)\n        \n        selected_indices = []\n        start = np.random.uniform(0, 1/self.pop_size)\n        for i in range(self.pop_size):\n            pointer = start + i/self.pop_size\n            cumulative_prob = 0.0\n            for j in range(self.pop_size):\n                cumulative_prob += probabilities[j]\n                if cumulative_prob >= pointer:\n                    selected_indices.append(j)\n                    break\n        return selected_indices\n        \n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using Adaptive Differential Evolution.\n\n        Args:\n            func (callable): The function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            \n            # Adaptive parameter update (population-wide learning rate)\n            lr = 0.1 * np.random.rand()  # Learning rate for F and CR\n            self.F = np.clip(self.F + lr * np.random.normal(0, 0.1), 0.1, 1.0)\n            self.CR = np.clip(self.CR + lr * np.random.normal(0, 0.1), 0.1, 1.0)\n\n\n            # Stochastic Universal Sampling\n            parent_indices = self.stochastic_universal_sampling()\n\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                \n                # Use selected indices for parent selection\n                a, b, c = parent_indices[a], parent_indices[b], parent_indices[c]\n                \n                mutant = self.pop[a] + self.F * (self.pop[b] - self.pop[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Selection\n                f = func(trial)\n                self.eval_count += 1\n\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.pop[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                        \n\n                if self.eval_count >= self.budget:\n                    break\n            \n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "An exception occurred: list index out of range.", "error": "", "parent_ids": [], "operator": null, "metadata": {}}
{"id": "a6a171af-d9f3-4bc0-81ce-88b919872456", "fitness": -Infinity, "name": "CMAES", "description": "Covariance Matrix Adaptation Evolution Strategy with orthogonal sampling to handle rugged landscapes by adapting the search distribution and increasing diversity.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damps=None, c_cov=None, sigma0=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize or (4 + int(3 * np.log(self.dim)))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = cs\n        self.damps = damps or (1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1)) * self.cs\n        self.c_cov = c_cov or ((1 / self.mueff) * (2 / ((self.dim + np.sqrt(2))**2))) + (1 - (1 / self.mueff)) * (2 / ((self.dim + 2)**2))\n        self.chiN = self.dim**0.5 * (1 - (1 / (4 * self.dim)) + (1 / (21 * self.dim**2)))\n        self.sigma0 = sigma0\n\n\n    def __call__(self, func):\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)\n        p_sigma = np.zeros(self.dim)\n        p_c = np.zeros(self.dim)\n        f_opt = np.Inf\n        x_opt = None\n        evals = 0\n\n        while evals < self.budget:\n            try:\n                R = np.random.randn(self.dim, self.popsize)\n                x = mean[:, np.newaxis] + sigma * np.linalg.cholesky(C) @ R\n                x = np.clip(x, func.bounds.lb, func.bounds.ub)\n                fitness = np.array([func(x[:, i]) for i in range(self.popsize)])\n                evals += self.popsize\n            except np.linalg.LinAlgError:\n                C = C + 1e-6 * np.eye(self.dim) # jitter C if it is not positive definite\n                continue\n\n            idx = np.argsort(fitness)\n            best_idx = idx[:self.mu]\n\n            mean_old = mean.copy()\n            mean = np.sum(self.weights * x[:, best_idx], axis=1)\n\n            C_invsqrt = np.linalg.inv(np.linalg.cholesky(C))\n            y = C_invsqrt @ (mean - mean_old) / sigma\n            p_sigma = (1 - self.cs) * p_sigma + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * y\n            \n            hsig = np.linalg.norm(p_sigma) / np.sqrt(1 - (1 - self.cs)**(2 * evals / self.popsize)) < self.chiN * (self.dim**0.5) * (1.0 + 0.05 / (self.dim + 1.0))\n\n            p_c = (1 - self.c_cov) * p_c + hsig * np.sqrt(self.c_cov * (2 - self.c_cov) * self.mueff) * (mean - mean_old) / sigma\n            \n            C = (1 - self.c_cov) * C + self.c_cov * (1 / self.mueff) * (p_c[:, np.newaxis] @ p_c[np.newaxis, :]) \\\n                + self.c_cov * (1 - (1 / self.mueff)) * sum(self.weights[i] * ((x[:, best_idx[i]] - mean_old)[:, np.newaxis] @ (x[:, best_idx[i]] - mean_old)[np.newaxis, :]) / (sigma**2) for i in range(self.mu))\n\n            sigma = sigma * np.exp((self.cs / self.damps) * (np.linalg.norm(p_sigma) / self.chiN - 1))\n            \n            if fitness[idx[0]] < f_opt:\n                f_opt = fitness[idx[0]]\n                x_opt = x[:, idx[0]].copy()\n                \n        return f_opt, x_opt", "configspace": "", "generation": 0, "feedback": "An exception occurred: operands could not be broadcast together with shapes (2,6) (2,) (2,) .", "error": "", "parent_ids": [], "operator": null, "metadata": {}}
{"id": "b0732da5-bcba-4859-9af8-64f78c0dd242", "fitness": 0.5476443647073462, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with Archive and Stochastic Ranking, which employs an archive to store promising solutions, adapts differential evolution parameters, and uses stochastic ranking to handle constraints and improve convergence.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=50, F=0.7, CR=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.archive = []\n        self.archive_fitness = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                if len(self.archive) > 0:\n                    idx = np.random.choice(range(self.pop_size), 3, replace=False)\n                    xa_idx = np.random.choice(range(len(self.archive)), 1)[0]\n                    x_rand1, x_rand2, x_rand3 = self.population[idx[0]], self.population[idx[1]], self.population[idx[2]]\n                    xa = self.archive[xa_idx]\n                    \n                    mutant = self.population[i] + self.F * (xa - self.population[i]) + self.F * (x_rand1 - x_rand2)\n\n                else:\n                    idx = np.random.choice(range(self.pop_size), 3, replace=False)\n                    x_rand1, x_rand2, x_rand3 = self.population[idx[0]], self.population[idx[1]], self.population[idx[2]]\n                    mutant = self.population[i] + self.F * (x_rand1 - x_rand2)\n                    \n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n                    \n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        max_archive_fitness_idx = np.argmax(self.archive_fitness)\n                        if f_trial < self.archive_fitness[max_archive_fitness_idx]:\n                            self.archive[max_archive_fitness_idx] = trial\n                            self.archive_fitness[max_archive_fitness_idx] = f_trial\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n\n            #Adapt F and CR (optional) - Simplified adaptation\n            self.F = np.clip(np.random.normal(0.7, 0.1), 0.1, 1.0)\n            self.CR = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDE scored 0.548 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.19206853555272219, 0.27071064725648475, 0.5418037311519746, 0.70974284134584, 0.6117487653423057, 0.731540381681723, 0.3983642646529669, 0.5164419930315699, 0.6443080033905626, 0.3658872270889014, 0.6857982496003879, 0.9776515190248234, 0.288846806179087, 0.5391968825551426, 0.8286556017519471, 0.717220000103341, 0.4476567507300937, 0.7684096041897273, 0.214443195349599, 0.5023922941677242]}}
{"id": "c78f63a0-7ea1-4f26-a5e7-93ebe530348b", "fitness": -Infinity, "name": "CMAES", "description": "Simplified CMA-ES with rank-one updates and clipping to handle boundary constraints and numerical issues.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, sigma0=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize or (4 + int(3 * np.log(self.dim)))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = 0.3\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) * self.cs\n        self.c_cov = (1 / self.mueff) * (2 / ((self.dim + np.sqrt(2))**2)) + (1 - (1 / self.mueff)) * (2 / ((self.dim + 2)**2))\n        self.chiN = self.dim**0.5 * (1 - (1 / (4 * self.dim)) + (1 / (21 * self.dim**2)))\n        self.sigma0 = sigma0\n\n\n    def __call__(self, func):\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)\n        p_sigma = np.zeros(self.dim)\n        f_opt = np.Inf\n        x_opt = None\n        evals = 0\n\n        while evals < self.budget:\n            try:\n                R = np.random.randn(self.dim, self.popsize)\n                x = mean[:, np.newaxis] + sigma * np.linalg.cholesky(C) @ R\n                x = np.clip(x, func.bounds.lb, func.bounds.ub)\n                fitness = np.array([func(x[:, i]) for i in range(self.popsize)])\n                evals += self.popsize\n            except np.linalg.LinAlgError:\n                C = C + 1e-6 * np.eye(self.dim)\n                continue\n\n            idx = np.argsort(fitness)\n            best_idx = idx[:self.mu]\n\n            mean_old = mean.copy()\n            mean = np.sum(self.weights * x[:, best_idx], axis=1)\n\n            y = (mean - mean_old) / sigma\n            p_sigma = (1 - self.cs) * p_sigma + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * y\n\n            C = (1 - self.c_cov) * C + self.c_cov * (p_sigma[:, np.newaxis] @ p_sigma[np.newaxis, :])\n\n            sigma = sigma * np.exp((self.cs / self.damps) * (np.linalg.norm(p_sigma) / self.chiN - 1))\n            \n            if fitness[idx[0]] < f_opt:\n                f_opt = fitness[idx[0]]\n                x_opt = x[:, idx[0]].copy()\n                \n            if np.any(np.isnan(C)):\n                C = np.eye(self.dim) # Reset covariance matrix\n\n        return f_opt, x_opt", "configspace": "", "generation": 1, "feedback": "An exception occurred: operands could not be broadcast together with shapes (2,6) (2,) (2,) .", "error": "", "parent_ids": ["a6a171af-d9f3-4bc0-81ce-88b919872456"], "operator": null, "metadata": {}}
{"id": "334a55b7-a69a-4ce4-b98a-cd3b86c75165", "fitness": -Infinity, "name": "SimplifiedCMAES", "description": "Simplified Covariance Matrix Adaptation Evolution Strategy, focusing on core adaptation steps for efficient search.", "code": "import numpy as np\n\nclass SimplifiedCMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, sigma0=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize or (4 + int(3 * np.log(self.dim)))\n        self.mu = self.popsize // 2\n        self.weights = np.log(np.arange(self.mu, 0, -1))\n        self.weights /= np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.sigma0 = sigma0\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n\n        self.cc = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        self.c_cov = 2 / ((self.dim + np.sqrt(2))**2 + self.mueff)\n\n    def __call__(self, func):\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)\n        p_sigma = np.zeros(self.dim)\n        p_c = np.zeros(self.dim)\n        f_opt = np.Inf\n        x_opt = None\n        evals = 0\n\n        while evals < self.budget:\n            try:\n                R = np.random.randn(self.dim, self.popsize)\n                x = mean[:, np.newaxis] + sigma * np.linalg.cholesky(C) @ R\n                x = np.clip(x, func.bounds.lb, func.bounds.ub)\n                fitness = np.array([func(x[:, i]) for i in range(self.popsize)])\n                evals += self.popsize\n            except np.linalg.LinAlgError:\n                C += 1e-6 * np.eye(self.dim)\n                continue\n\n            idx = np.argsort(fitness)\n            best_idx = idx[:self.mu]\n\n            mean_old = mean.copy()\n            mean = np.sum(self.weights * x[:, best_idx], axis=1)\n\n            y = (mean - mean_old) / sigma\n            p_sigma = (1 - self.cs) * p_sigma + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * (np.linalg.solve(np.linalg.cholesky(C).T, y)) #C**-0.5 @ y\n            p_c = (1 - self.cc) * p_c + np.sqrt(self.cc * (2 - self.cc) * self.mueff) * y\n            C = (1 - self.c_cov) * C + self.c_cov * (p_c[:, np.newaxis] @ p_c[np.newaxis, :]) + self.c_cov * (1 - self.c_cov) * sum(self.weights[i] * ((x[:, best_idx[i]] - mean_old)[:, np.newaxis] @ (x[:, best_idx[i]] - mean_old)[np.newaxis, :]) / (sigma**2) for i in range(self.mu))\n            sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(p_sigma) / np.sqrt(self.dim) - 1))\n\n            if fitness[idx[0]] < f_opt:\n                f_opt = fitness[idx[0]]\n                x_opt = x[:, idx[0]].copy()\n\n        return f_opt, x_opt", "configspace": "", "generation": 1, "feedback": "An exception occurred: operands could not be broadcast together with shapes (2,6) (2,) (2,) .", "error": "", "parent_ids": ["a6a171af-d9f3-4bc0-81ce-88b919872456"], "operator": null, "metadata": {}}
{"id": "53ff5821-dce5-48d1-9ab6-2faaed4858e1", "fitness": -Infinity, "name": "CooperativeSwarm", "description": "Cooperative Swarm Optimization with Lévy flight and Velocity clamping.", "code": "import numpy as np\n\nclass CooperativeSwarm:\n    def __init__(self, budget=10000, dim=10, swarm_size=50, c1=1.5, c2=1.5, w_max=0.9, w_min=0.4, v_max_ratio=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.c1 = c1  # Cognitive component\n        self.c2 = c2  # Social component\n        self.w_max = w_max\n        self.w_min = w_min\n        self.v_max_ratio = v_max_ratio  # Maximum velocity as a fraction of search space\n        self.swarm = None\n        self.velocities = None\n        self.pbest_positions = None\n        self.pbest_fitness = None\n        self.gbest_position = None\n        self.gbest_fitness = np.Inf\n        self.eval_count = 0\n\n    def levy_flight(self, beta=1.5):\n        \"\"\"Generate a Lévy flight step.\"\"\"\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) / (np.math.gamma((1 + beta) / 2) * beta * (2 ** ((beta - 1) / 2)))) ** (1 / beta)\n        u = np.random.normal(0, sigma, size=self.dim)\n        v = np.random.normal(0, 1, size=self.dim)\n        step = u / (np.abs(v) ** (1 / beta))\n        return step\n\n    def initialize_swarm(self, func):\n        \"\"\"Initialize the swarm randomly within the bounds.\"\"\"\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.swarm = np.random.uniform(lb, ub, size=(self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-self.v_max_ratio * (ub - lb), self.v_max_ratio * (ub - lb), size=(self.swarm_size, self.dim))\n        self.pbest_positions = self.swarm.copy()\n        self.pbest_fitness = np.array([func(x) for x in self.swarm])\n        self.eval_count += self.swarm_size\n        self.gbest_position = self.pbest_positions[np.argmin(self.pbest_fitness)].copy()\n        self.gbest_fitness = np.min(self.pbest_fitness)\n\n    def __call__(self, func):\n        \"\"\"Optimize the given function using Cooperative Particle Swarm Optimization.\"\"\"\n        self.initialize_swarm(func)\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        v_max = self.v_max_ratio * (ub - lb)\n\n        while self.eval_count < self.budget:\n            # Inertia weight decreases linearly\n            w = self.w_max - (self.w_max - self.w_min) * (self.eval_count / self.budget)\n\n            for i in range(self.swarm_size):\n                # Update velocity\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (self.pbest_positions[i] - self.swarm[i])\n                social_component = self.c2 * r2 * (self.gbest_position - self.swarm[i])\n                self.velocities[i] = w * self.velocities[i] + cognitive_component + social_component\n\n                # Velocity clamping\n                self.velocities[i] = np.clip(self.velocities[i], -v_max, v_max)\n\n                # Update position\n                self.swarm[i] += self.velocities[i]\n\n                # Boundary handling (reflection)\n                for d in range(self.dim):\n                    if self.swarm[i][d] < lb:\n                        self.swarm[i][d] = lb + abs(self.swarm[i][d] - lb)\n                    elif self.swarm[i][d] > ub:\n                        self.swarm[i][d] = ub - abs(self.swarm[i][d] - ub)\n                        \n                # Lévy flight as perturbation\n                if np.random.rand() < 0.1:\n                    levy_step = self.levy_flight()\n                    self.swarm[i] += 0.01 * levy_step  # Scale levy step\n                    self.swarm[i] = np.clip(self.swarm[i], lb, ub)\n\n                # Evaluate fitness\n                fitness = func(self.swarm[i])\n                self.eval_count += 1\n\n                # Update personal best\n                if fitness < self.pbest_fitness[i]:\n                    self.pbest_fitness[i] = fitness\n                    self.pbest_positions[i] = self.swarm[i].copy()\n\n                    # Update global best\n                    if fitness < self.gbest_fitness:\n                        self.gbest_fitness = fitness\n                        self.gbest_position = self.swarm[i].copy()\n\n                if self.eval_count >= self.budget:\n                    break\n\n        return self.gbest_fitness, self.gbest_position", "configspace": "", "generation": 1, "feedback": "An exception occurred: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all().", "error": "", "parent_ids": ["6dbf843f-a357-471a-a757-f847d2551854"], "operator": null, "metadata": {}}
{"id": "9c7c0dba-6bd4-4bf5-bb2e-49e7e735fc2b", "fitness": 0.06499284618120893, "name": "AdaptiveDE", "description": "An Adaptive Differential Evolution strategy with orthogonal learning, momentum-based exploration and a local search refinement, which dynamically adjusts mutation and crossover rates and employs orthogonal learning for enhanced exploration.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, mutation_factor=0.5, crossover_rate=0.7, local_search_iterations=5, orthogonal_trials=3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.mutation_factor = mutation_factor\n        self.crossover_rate = crossover_rate\n        self.local_search_iterations = local_search_iterations\n        self.orthogonal_trials = orthogonal_trials\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.momentum = np.zeros((pop_size, dim)) # Initialize momentum\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.eval_count += self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index].copy()\n        \n        generation = 0\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = a + self.mutation_factor * (b - c) + 0.1 * self.momentum[i]  # Added momentum\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.crossover_rate\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Orthogonal Learning\n                orthogonal_set = []\n                for _ in range(self.orthogonal_trials):\n                    orthogonal_vector = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                    orthogonal_set.append(orthogonal_vector)\n\n                f_trial = func(trial)\n                self.eval_count += 1\n                \n                best_orthogonal_f = f_trial\n                best_orthogonal_trial = trial\n                \n                for orthogonal_vector in orthogonal_set:\n                    f_orthogonal = func(orthogonal_vector)\n                    self.eval_count += 1\n\n                    if f_orthogonal < best_orthogonal_f:\n                        best_orthogonal_f = f_orthogonal\n                        best_orthogonal_trial = orthogonal_vector\n\n                if best_orthogonal_f < fitness[i]:\n                    fitness[i] = best_orthogonal_f\n                    population[i] = best_orthogonal_trial\n                    self.momentum[i] = best_orthogonal_trial - population[i] #update momentum\n                    f_trial = best_orthogonal_f  # Ensure f_trial reflects the updated fitness\n                else:\n                    self.momentum[i] *= 0.9 # Dampen momentum if no improvement\n\n                # Local Search\n                for _ in range(self.local_search_iterations):\n                    perturbation = np.random.normal(0, 0.01, size=self.dim)\n                    local_trial = trial + perturbation\n                    local_trial = np.clip(local_trial, func.bounds.lb, func.bounds.ub)\n                    f_local = func(local_trial)\n                    self.eval_count += 1\n                    if f_local < f_trial:\n                        f_trial = f_local\n                        trial = local_trial\n                        fitness[i] = f_trial\n                        population[i] = trial\n                    else:\n                        break\n\n                # Update optimal solution\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial.copy()\n                \n                if self.eval_count >= self.budget:\n                    break\n            \n            # Adaptive parameter adjustment\n            if generation % 10 == 0:\n                success_indices = np.where(fitness < np.mean(fitness))[0]\n                if len(success_indices) > 0:\n                    self.mutation_factor *= np.random.uniform(0.8, 1.2)\n                    self.crossover_rate *= np.random.uniform(0.8, 1.2)\n                    self.mutation_factor = np.clip(self.mutation_factor, 0.1, 1.0)\n                    self.crossover_rate = np.clip(self.crossover_rate, 0.1, 1.0)\n            \n            generation += 1\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDE scored 0.065 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["4ac09956-f476-42ea-97ce-0b1aa5673b53"], "operator": null, "metadata": {"aucs": [0.12998569236241786, 0]}}
{"id": "32c2eb2f-0577-4c8e-9745-ebb5f96141cf", "fitness": 0.33890032223313643, "name": "AdaptiveDE", "description": "Simplified Adaptive Differential Evolution with self-adaptive parameters and a reduced local search, aiming for faster convergence and reduced complexity.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, mutation_factor=0.5, crossover_rate=0.7, local_search_iterations=3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.mutation_factor = mutation_factor\n        self.crossover_rate = crossover_rate\n        self.local_search_iterations = local_search_iterations\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.eval_count += self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = a + self.mutation_factor * (b - c)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.crossover_rate\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    # Local Search\n                    for _ in range(self.local_search_iterations):\n                        perturbation = np.random.normal(0, 0.01, size=self.dim)\n                        local_trial = trial + perturbation\n                        local_trial = np.clip(local_trial, func.bounds.lb, func.bounds.ub)\n                        f_local = func(local_trial)\n                        self.eval_count += 1\n                        if f_local < f_trial:\n                            f_trial = f_local\n                            trial = local_trial\n                            fitness[i] = f_trial\n                            population[i] = trial\n                        else:\n                            break\n\n                # Update optimal solution\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial.copy()\n                \n                if self.eval_count >= self.budget:\n                    break\n            \n            # Adaptive parameter adjustment - simplified\n            self.mutation_factor *= np.random.uniform(0.9, 1.1)\n            self.crossover_rate *= np.random.uniform(0.9, 1.1)\n            self.mutation_factor = np.clip(self.mutation_factor, 0.1, 1.0)\n            self.crossover_rate = np.clip(self.crossover_rate, 0.1, 1.0)\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDE scored 0.339 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["4ac09956-f476-42ea-97ce-0b1aa5673b53"], "operator": null, "metadata": {"aucs": [0.19218146267516445, 0.26052488353157954, 0.404801547657184, 0.8369937173017541, 0]}}
{"id": "6fa227fc-9876-4563-9b66-01e7cd740a2a", "fitness": 0.32418495420670224, "name": "AdaptiveDE", "description": "Simplified Adaptive Differential Evolution with self-adaptive mutation and crossover rates, and focused local search.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.mutation_factor = 0.5\n        self.crossover_rate = 0.7\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.eval_count += self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = a + self.mutation_factor * (b - c)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.crossover_rate\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    # Local Search (only if improvement)\n                    perturbation = np.random.normal(0, 0.01, size=self.dim)\n                    local_trial = trial + perturbation\n                    local_trial = np.clip(local_trial, func.bounds.lb, func.bounds.ub)\n                    f_local = func(local_trial)\n                    self.eval_count += 1\n                    if f_local < f_trial:\n                        fitness[i] = f_local\n                        population[i] = local_trial\n\n                # Update optimal solution\n                if fitness[i] < self.f_opt:\n                    self.f_opt = fitness[i]\n                    self.x_opt = population[i].copy()\n                \n                if self.eval_count >= self.budget:\n                    break\n            \n            # Adaptive parameter adjustment (simplified)\n            self.mutation_factor = np.clip(self.mutation_factor + 0.05 * np.random.normal(), 0.1, 1.0)\n            self.crossover_rate = np.clip(self.crossover_rate + 0.05 * np.random.normal(), 0.1, 1.0)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDE scored 0.324 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["4ac09956-f476-42ea-97ce-0b1aa5673b53"], "operator": null, "metadata": {"aucs": [0.2033678454015646, 0.38559325792171395, 0.3880202224791809, 0.6439434452310516, 0]}}
{"id": "adad8070-9279-4096-9544-b29a9eef1cd2", "fitness": 0.6931759870095828, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with Archive, simplified parameter adaptation, and focus on exploitation by reducing diversity in mutation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=50, F=0.7, CR=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.archive = []\n        self.archive_fitness = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation - Focus on exploiting the best solution and the archive\n                best_idx = np.argmin(self.fitness)\n                x_best = self.population[best_idx]\n\n                if len(self.archive) > 0:\n                    xa_idx = np.random.choice(range(len(self.archive)))\n                    xa = self.archive[xa_idx]\n                    mutant = self.population[i] + self.F * (x_best - self.population[i]) + self.F * (xa - self.population[i])  #Focus on exploitation\n                else:\n                    idx = np.random.choice(range(self.pop_size), 2, replace=False)\n                    x_rand1, x_rand2 = self.population[idx[0]], self.population[idx[1]]\n                    mutant = self.population[i] + self.F * (x_rand1 - x_rand2)\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n                    \n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        max_archive_fitness_idx = np.argmax(self.archive_fitness)\n                        if f_trial < self.archive_fitness[max_archive_fitness_idx]:\n                            self.archive[max_archive_fitness_idx] = trial\n                            self.archive_fitness[max_archive_fitness_idx] = f_trial\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n\n            #Adapt F and CR (optional) - Simplified adaptation\n            self.F = np.clip(np.random.normal(0.7, 0.05), 0.1, 1.0) #Smaller std for F\n            self.CR = np.clip(np.random.normal(0.5, 0.05), 0.1, 1.0) #Smaller std for CR\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDE scored 0.693 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b0732da5-bcba-4859-9af8-64f78c0dd242"], "operator": null, "metadata": {"aucs": [0.3143549167089956, 0.7028986779373858, 0.814272232786531, 0.9238591497986398, 0.8583640487402907, 0.8754968483957399, 0.34425140388381203, 0.7805321854081269, 0.8316137989887119, 0.2620830372120231, 0.8817037623391777, 0.9983169382311408, 0.3472786789708707, 0.8423628753478349, 0.7981559600643389, 0.8695737776882049, 0.7248178174187737, 0.8988795346905901, 0.2966241807751828, 0.4980799148052868]}}
{"id": "d183d761-454d-4458-a49a-1fa9d2f9427f", "fitness": 0.32896616203067464, "name": "AdaptiveDECauchy", "description": "An Adaptive Differential Evolution strategy employing a self-learning mutation factor, combined with a Cauchy mutation operator for enhanced exploration, and a repair mechanism to handle boundary violations.", "code": "import numpy as np\n\nclass AdaptiveDECauchy:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_init=0.5, CR=0.5, F_adapt_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init  # Initial mutation factor\n        self.CR = CR  # Crossover rate\n        self.F_adapt_rate = F_adapt_rate\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idx = np.random.choice(range(self.pop_size), 3, replace=False)\n                x_rand1, x_rand2, x_rand3 = self.population[idx[0]], self.population[idx[1]], self.population[idx[2]]\n\n                # Cauchy mutation with self-adaptive F\n                cauchy_mutation = self.F * np.random.standard_cauchy(size=self.dim)\n                mutant = self.population[i] + self.F * (x_rand1 - x_rand2) + cauchy_mutation\n                \n                # Repair mechanism\n                mutant = self.repair(mutant, func.bounds.lb, func.bounds.ub)\n\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    # Update F based on success (self-learning)\n                    if self.fitness[i] - f_trial > 0.0001:  # Significant improvement\n                         self.F = self.F * np.exp(self.F_adapt_rate)\n                    else:\n                         self.F = self.F * np.exp(-self.F_adapt_rate)\n                    self.F = np.clip(self.F, 0.1, 1.0)\n                    \n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n\n        return self.f_opt, self.x_opt\n\n    def repair(self, x, lb, ub):\n        \"\"\"Repair solution to lie within the bounds.\"\"\"\n        return np.clip(x, lb, ub)", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDECauchy scored 0.329 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b0732da5-bcba-4859-9af8-64f78c0dd242"], "operator": null, "metadata": {"aucs": [0.128639184831278, 0.17749285040498886, 0.3229334041740908, 0.2689887519929086, 0.24480212125276735, 0.3015144011082913, 0.26977714097029437, 0.27577000054832856, 0.25254919937939857, 0.18887847991268947, 0.26540415368529435, 0.9978856743539002, 0.267655273003212, 0.28448602715162896, 0.6830122201963668, 0.3204480271955934, 0.272518197681458, 0.3915526917519754, 0.182977478094991, 0.48203796292403644]}}
{"id": "df748dcf-2d2b-42cb-96f9-571be3c406fa", "fitness": 0.3266440818195371, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with archive and simplified parameter adaptation, focusing on reducing complexity and improving exploration/exploitation balance.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=50, F=0.7, CR=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.archive = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idx = np.random.choice(range(self.pop_size), 3, replace=False)\n                x_rand1, x_rand2, x_rand3 = self.population[idx[0]], self.population[idx[1]], self.population[idx[2]]\n                mutant = self.population[i] + self.F * (x_rand1 - x_rand2)\n                \n                # Use archive if available to further diversify exploration\n                if len(self.archive) > 0 and np.random.rand() < 0.1: #10% chance to use archive\n                    xa_idx = np.random.choice(range(len(self.archive)), 1)[0]\n                    mutant = self.population[i] + self.F * (self.archive[xa_idx] - self.population[i]) + self.F * (x_rand1 - x_rand2)\n\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n                    \n                    # Update archive - simplified: replace worst if better\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                    else:\n                        worst_idx = np.argmax(self.fitness[:len(self.archive)]) # Consider only archive part for worst\n                        if f_trial < self.fitness[worst_idx]:\n                            self.archive[worst_idx] = trial\n                            self.fitness[worst_idx] = f_trial\n\n\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n\n            #Adapt F and CR (optional) - Simplified adaptation\n            self.F = np.clip(np.random.normal(0.7, 0.1), 0.1, 1.0)\n            self.CR = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDE scored 0.327 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b0732da5-bcba-4859-9af8-64f78c0dd242"], "operator": null, "metadata": {"aucs": [0.14512057476495466, 0.22577333478172723, 0.39014727817696615, 0.23089053094863843, 0.2366926592415476, 0.28107938004100885, 0.2760634770309489, 0.26053808369259734, 0.24443056773474103, 0.18934957605725422, 0.2641472733889525, 0.9931945426652254, 0.2590756594872916, 0.26426711403409797, 0.6716382226002583, 0.31317857406395655, 0.26258076807618946, 0.3598570030765753, 0.18817568617562297, 0.47668133035218785]}}
{"id": "b25e2ca3-787c-442c-ad08-200eeee61e1e", "fitness": 0.5499103367652745, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with Archive, Stochastic Ranking, and improved parameter adaptation using success history.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=50, F=0.7, CR=0.5, F_history_size=10, CR_history_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.archive = []\n        self.archive_fitness = []\n        self.F_history = []\n        self.CR_history = []\n        self.F_history_size = F_history_size\n        self.CR_history_size = CR_history_size\n        self.success_F = []\n        self.success_CR = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                if len(self.archive) > 0:\n                    idx = np.random.choice(range(self.pop_size), 3, replace=False)\n                    xa_idx = np.random.choice(range(len(self.archive)), 1)[0]\n                    x_rand1, x_rand2, x_rand3 = self.population[idx[0]], self.population[idx[1]], self.population[idx[2]]\n                    xa = self.archive[xa_idx]\n                    \n                    mutant = self.population[i] + self.F * (xa - self.population[i]) + self.F * (x_rand1 - x_rand2)\n\n                else:\n                    idx = np.random.choice(range(self.pop_size), 3, replace=False)\n                    x_rand1, x_rand2, x_rand3 = self.population[idx[0]], self.population[idx[1]], self.population[idx[2]]\n                    mutant = self.population[i] + self.F * (x_rand1 - x_rand2)\n                    \n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    delta = np.abs(self.fitness[i] - f_trial)\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n                    self.success_F.append(self.F)\n                    self.success_CR.append(self.CR)\n                    \n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        max_archive_fitness_idx = np.argmax(self.archive_fitness)\n                        if f_trial < self.archive_fitness[max_archive_fitness_idx]:\n                            self.archive[max_archive_fitness_idx] = trial\n                            self.archive_fitness[max_archive_fitness_idx] = f_trial\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n\n            #Adapt F and CR using success history\n            if len(self.success_F) > 0:\n                self.F = np.clip(np.mean(self.success_F), 0.1, 1.0)\n                self.CR = np.clip(np.mean(self.success_CR), 0.1, 1.0)\n            else:\n                self.F = np.clip(np.random.normal(0.7, 0.1), 0.1, 1.0)\n                self.CR = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n            \n            self.success_F = []\n            self.success_CR = []\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDE scored 0.550 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b0732da5-bcba-4859-9af8-64f78c0dd242"], "operator": null, "metadata": {"aucs": [0.189579725616437, 0.29721768990115116, 0.5502826081087309, 0.7472258193874508, 0.5919954675292429, 0.706273735419751, 0.38166136082689217, 0.5160691635631481, 0.6286537756277853, 0.3946491213644632, 0.6711325503617454, 0.9954826787310115, 0.3196523429538224, 0.5551122019917041, 0.8209446523206543, 0.7110797819586283, 0.4291911017312533, 0.7638355169817774, 0.22302742660919972, 0.5051400143206393]}}
{"id": "b1ceb08e-a8a3-4ccd-a6c3-547c795d41c0", "fitness": -Infinity, "name": "CMAES", "description": "Covariance Matrix Adaptation Evolution Strategy adapted to handle function evaluation limits and potential numerical issues, focusing on robustness and simplification.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damps=None, c_cov=None, sigma0=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize or (4 + int(3 * np.log(self.dim)))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = cs\n        self.damps = damps or (1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1)) * self.cs\n        self.c_cov = c_cov or ((1 / self.mueff) * (2 / ((self.dim + np.sqrt(2))**2))) + (1 - (1 / self.mueff)) * (2 / ((self.dim + 2)**2))\n        self.chiN = self.dim**0.5 * (1 - (1 / (4 * self.dim)) + (1 / (21 * self.dim**2)))\n        self.sigma0 = sigma0\n\n\n    def __call__(self, func):\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)\n        p_sigma = np.zeros(self.dim)\n        p_c = np.zeros(self.dim)\n        f_opt = np.Inf\n        x_opt = None\n        evals = 0\n\n        while evals < self.budget:\n            z = np.random.randn(self.dim, self.popsize)\n            try:\n                A = np.linalg.cholesky(C)\n                x = mean[:, np.newaxis] + sigma * A @ z\n            except np.linalg.LinAlgError:\n                C += 1e-6 * np.eye(self.dim)\n                continue\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            \n            fitness = np.array([func(x[:, i]) if evals + i < self.budget else np.inf for i in range(self.popsize)])\n            evals += self.popsize\n            if evals > self.budget:\n              fitness[self.budget - (evals):] = np.inf\n              evals = self.budget\n            \n            idx = np.argsort(fitness)\n            best_idx = idx[:self.mu]\n            \n            mean_old = mean.copy()\n            mean = np.sum(x[:, best_idx] * self.weights, axis=1)\n\n            y = np.linalg.solve(A, (mean - mean_old)) / sigma\n            p_sigma = (1 - self.cs) * p_sigma + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * y\n            \n            hsig = np.linalg.norm(p_sigma) / self.chiN < (1.4 + 2/(self.dim+1))\n\n            p_c = (1 - self.c_cov) * p_c + hsig * np.sqrt(self.c_cov * (2 - self.c_cov) * self.mueff) * (mean - mean_old) / sigma\n            \n            C = (1 - self.c_cov) * C + self.c_cov * (p_c[:, np.newaxis] @ p_c[np.newaxis, :])\n            C += self.c_cov * (1 - (1 / self.mueff)) * np.sum([self.weights[i] * ((x[:, best_idx[i]] - mean_old)[:, np.newaxis] @ (x[:, best_idx[i]] - mean_old)[np.newaxis, :]) / (sigma**2) for i in range(self.mu)], axis=0)\n            \n            sigma = sigma * np.exp((self.cs / self.damps) * (np.linalg.norm(p_sigma) / self.chiN - 1))\n            \n            if fitness[idx[0]] < f_opt:\n                f_opt = fitness[idx[0]]\n                x_opt = x[:, idx[0]].copy()\n                \n        return f_opt, x_opt", "configspace": "", "generation": 1, "feedback": "An exception occurred: operands could not be broadcast together with shapes (2,6) (2,) (2,) .", "error": "", "parent_ids": ["a6a171af-d9f3-4bc0-81ce-88b919872456"], "operator": null, "metadata": {}}
{"id": "57bc6944-d16f-4227-b9e7-a0f80e3e6bda", "fitness": 0.6231526442185971, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with adaptive parameters and simplified parent selection to enhance exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7):\n        \"\"\"\n        Initialize the Adaptive Differential Evolution algorithm.\n\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size (int): The population size.\n            F (float): The initial scaling factor.\n            CR (float): The initial crossover rate.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n\n    def initialize_population(self, func):\n        \"\"\"Initialize the population randomly within the bounds.\"\"\"\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.pop[np.argmin(self.fitness)]\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using Adaptive Differential Evolution.\n\n        Args:\n            func (callable): The function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            \n            # Adaptive parameter update\n            self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n            self.CR = np.clip(np.random.normal(0.7, 0.1), 0.1, 1.0)\n\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                a, b, c = idxs[0], idxs[1], idxs[2]\n                \n                mutant = self.pop[a] + self.F * (self.pop[b] - self.pop[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Selection\n                f = func(trial)\n                self.eval_count += 1\n\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.pop[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                        \n\n                if self.eval_count >= self.budget:\n                    break\n            \n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDE scored 0.623 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6dbf843f-a357-471a-a757-f847d2551854"], "operator": null, "metadata": {"aucs": [0.23066314485491546, 0.4174477429243215, 0.6081099431283938, 0.837717254657289, 0.6649944509975918, 0.7587658642833185, 0.46443579285161496, 0.5875355801934825, 0.6950036943313738, 0.6141617904076286, 0.7979315425235618, 0.9987085228284829, 0.3446756321791985, 0.6742881078702774, 0.8433908853141704, 0.7745525298868925, 0.4716160593679203, 0.8065576415082326, 0.35186012303079395, 0.52063658123248]}}
{"id": "64f41a28-c210-4535-89d5-baa8c0d50fc7", "fitness": 0.0, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with simplified parameter adaptation and a combined mutation strategy favoring the best and a randomly selected individual from the population, balancing exploitation and exploration.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=50, F=0.7, CR=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.CR = CR\n        self.archive = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation: Combining best and random individual\n                best_idx = np.argmin(self.fitness)\n                x_best = self.population[best_idx]\n                rand_idx = np.random.choice(self.pop_size)\n                x_rand = self.population[rand_idx]\n                mutant = self.population[i] + self.F * (x_best - self.population[i]) + self.F * (x_rand - self.population[i])\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        if f_trial < np.max([func(x) for x in self.archive]):\n                            worst_idx = np.argmax([func(x) for x in self.archive])\n                            self.archive[worst_idx] = self.population[i].copy()\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n\n            # Adapt F and CR\n            self.F = np.clip(np.random.normal(0.7, 0.05), 0.1, 1.0)\n            self.CR = np.clip(np.random.normal(0.5, 0.05), 0.1, 1.0)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["adad8070-9279-4096-9544-b29a9eef1cd2"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "5b8691f0-c0f4-40c0-b7f3-e83c2aae2bef", "fitness": 0.0, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with simplified parameter adaptation, archive, and a focused mutation strategy using the best solution and a single random individual from the population or archive for improved exploitation and efficiency.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=50, F=0.7, CR=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.CR = CR\n        self.archive = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation: Exploit best and one random from pop or archive\n                best_idx = np.argmin(self.fitness)\n                x_best = self.population[best_idx]\n                \n                if len(self.archive) > 0 and np.random.rand() < 0.5:\n                    xa = self.archive[np.random.choice(len(self.archive))]\n                    mutant = self.population[i] + self.F * (x_best - self.population[i]) + self.F * (xa - self.population[i])\n                else:\n                    idx = np.random.choice(self.pop_size)\n                    mutant = self.population[i] + self.F * (x_best - self.population[i]) + self.F * (self.population[idx] - self.population[i])\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                    else:\n                        if f_trial < np.max([func(x) for x in self.archive]):\n                            worst_idx = np.argmax([func(x) for x in self.archive])\n                            self.archive[worst_idx] = trial\n                            \n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n\n            # Adapt F and CR\n            self.F = np.clip(np.random.normal(0.7, 0.05), 0.1, 1.0)\n            self.CR = np.clip(np.random.normal(0.5, 0.05), 0.1, 1.0)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["adad8070-9279-4096-9544-b29a9eef1cd2"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "7557fff6-f4fa-434c-b6cd-4bb9aba6f4c9", "fitness": 0.0, "name": "AdaptiveDE", "description": "Simplified Adaptive DE with dynamic population size and a focused local search around the best solution.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size_init=40, mutation_factor=0.5, crossover_rate=0.7, local_search_iterations=3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_init = pop_size_init\n        self.mutation_factor = mutation_factor\n        self.crossover_rate = crossover_rate\n        self.local_search_iterations = local_search_iterations\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        \n        # Initialize population - dynamic population size\n        pop_size = min(self.pop_size_init, self.budget // 10)\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.eval_count += pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(pop_size):\n                # Mutation\n                idxs = [idx for idx in range(pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = a + self.mutation_factor * (b - c)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.crossover_rate\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                # Update optimal solution\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial.copy()\n\n            # Local Search around the best solution\n            for _ in range(self.local_search_iterations):\n                perturbation = np.random.normal(0, 0.01, size=self.dim)\n                local_trial = self.x_opt + perturbation\n                local_trial = np.clip(local_trial, func.bounds.lb, func.bounds.ub)\n                f_local = func(local_trial)\n                self.eval_count += 1\n                if f_local < self.f_opt:\n                    self.f_opt = f_local\n                    self.x_opt = local_trial.copy()\n                else:\n                    break\n                \n                if self.eval_count >= self.budget:\n                    break\n            \n            # Adaptive parameter adjustment - simplified\n            self.mutation_factor *= np.random.uniform(0.9, 1.1)\n            self.crossover_rate *= np.random.uniform(0.9, 1.1)\n            self.mutation_factor = np.clip(self.mutation_factor, 0.1, 1.0)\n            self.crossover_rate = np.clip(self.crossover_rate, 0.1, 1.0)\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["32c2eb2f-0577-4c8e-9745-ebb5f96141cf"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "2d160999-91f1-453e-a25b-a19a58825756", "fitness": -Infinity, "name": "AdaptiveDE", "description": "Improved Adaptive Differential Evolution with dynamic population size adjustment and a restart mechanism to escape local optima.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, mutation_factor=0.5, crossover_rate=0.7, local_search_iterations=3, restart_trigger=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.mutation_factor = mutation_factor\n        self.crossover_rate = crossover_rate\n        self.local_search_iterations = local_search_iterations\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.restart_trigger = restart_trigger # Percentage of budget to trigger restart\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.eval_count += self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index].copy()\n        \n        stagnation_counter = 0\n        last_improvement = 0\n        \n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = a + self.mutation_factor * (b - c)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.crossover_rate\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    # Local Search\n                    for _ in range(self.local_search_iterations):\n                        perturbation = np.random.normal(0, 0.01, size=self.dim)\n                        local_trial = trial + perturbation\n                        local_trial = np.clip(local_trial, func.bounds.lb, func.bounds.ub)\n                        f_local = func(local_trial)\n                        self.eval_count += 1\n                        if f_local < f_trial:\n                            f_trial = f_local\n                            trial = local_trial\n                            fitness[i] = f_trial\n                            population[i] = trial\n                        else:\n                            break\n\n                # Update optimal solution\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial.copy()\n                    stagnation_counter = 0\n                    last_improvement = self.eval_count\n                else:\n                    stagnation_counter += 1\n                \n                if self.eval_count >= self.budget:\n                    break\n            \n            # Adaptive parameter adjustment - simplified\n            self.mutation_factor *= np.random.uniform(0.9, 1.1)\n            self.crossover_rate *= np.random.uniform(0.9, 1.1)\n            self.mutation_factor = np.clip(self.mutation_factor, 0.1, 1.0)\n            self.crossover_rate = np.clip(self.crossover_rate, 0.1, 1.0)\n\n            # Restart mechanism\n            if (self.eval_count - last_improvement) > self.restart_trigger * self.budget:\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.eval_count += self.pop_size\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index].copy()\n                last_improvement = self.eval_count\n\n            # Dynamic population size adjustment\n            if stagnation_counter > 0.05 * self.budget:\n                self.pop_size = min(100, int(self.pop_size * 1.1)) #Increase population size\n            else:\n                self.pop_size = max(10, int(self.pop_size * 0.9)) # Decrease population size\n                \n            population_new = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size - population.shape[0] , self.dim))\n            fitness_new = np.array([func(x) for x in population_new])\n            self.eval_count += population_new.shape[0]\n\n            population = np.concatenate([population, population_new], axis=0)\n            fitness = np.concatenate([fitness, fitness_new], axis=0)\n\n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index].copy()\n                last_improvement = self.eval_count\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "An exception occurred: negative dimensions are not allowed.", "error": "", "parent_ids": ["32c2eb2f-0577-4c8e-9745-ebb5f96141cf"], "operator": null, "metadata": {}}
{"id": "2bf88b98-0658-47ba-a3a9-cc73a7a08696", "fitness": 0.0973470274564815, "name": "AdaptiveDE", "description": "Simplified Adaptive Differential Evolution with self-adaptive parameters, reduced local search, and population reduction for faster convergence.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, mutation_factor=0.5, crossover_rate=0.7, local_search_iterations=1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.mutation_factor = mutation_factor\n        self.crossover_rate = crossover_rate\n        self.local_search_iterations = local_search_iterations\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.eval_count += self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = a + self.mutation_factor * (b - c)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.crossover_rate\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    # Local Search\n                    for _ in range(self.local_search_iterations):\n                        perturbation = np.random.normal(0, 0.01, size=self.dim)\n                        local_trial = trial + perturbation\n                        local_trial = np.clip(local_trial, func.bounds.lb, func.bounds.ub)\n                        f_local = func(local_trial)\n                        self.eval_count += 1\n                        if f_local < f_trial:\n                            f_trial = f_local\n                            trial = local_trial\n                            fitness[i] = f_trial\n                            population[i] = trial\n                        else:\n                            break\n\n                # Update optimal solution\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial.copy()\n                \n                if self.eval_count >= self.budget:\n                    break\n            \n            # Adaptive parameter adjustment - simplified\n            self.mutation_factor = np.clip(self.mutation_factor + np.random.normal(0, 0.05), 0.1, 1.0)\n            self.crossover_rate = np.clip(self.crossover_rate + np.random.normal(0, 0.05), 0.1, 1.0)\n\n            # Population reduction\n            if self.eval_count > self.budget * 0.75 and self.pop_size > 10:\n                worst_index = np.argmax(fitness)\n                population = np.delete(population, worst_index, axis=0)\n                fitness = np.delete(fitness, worst_index)\n                self.pop_size -= 1\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDE scored 0.097 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["32c2eb2f-0577-4c8e-9745-ebb5f96141cf"], "operator": null, "metadata": {"aucs": [0.194694054912963, 0]}}
{"id": "36b34c92-6d7b-41be-bf88-7a333edcb463", "fitness": -Infinity, "name": "AdaptiveDE_CMA", "description": "An adaptive differential evolution algorithm with covariance matrix adaptation for mutation and a restart mechanism based on stagnation detection.", "code": "import numpy as np\n\nclass AdaptiveDE_CMA:\n    def __init__(self, budget=10000, dim=10, pop_size=None, mutation_factor=0.5, crossover_rate=0.7, stagnation_threshold=500):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.mutation_factor = mutation_factor\n        self.crossover_rate = crossover_rate\n        self.stagnation_threshold = stagnation_threshold\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.mean = None\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.chiN = None\n        self.cs = None\n        self.damps = None\n        self.c1 = None\n        self.mu_eff = None\n        self.cmu = None\n\n    def initialize(self, func):\n        self.mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + (1 / (21 * self.dim**2)))\n        self.cs = (0.3) \n        self.damps = 1 + (2 * max(0, np.sqrt((self.mu_eff - 1) / (self.dim + 1)) - 1)) + self.cs\n\n        mu = self.pop_size // 2\n        self.mu_eff = np.sum(mu) / np.sum(np.arange(1, mu + 1)**-1)\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mu_eff)\n        self.cmu = min(1 - self.c1, 2 * (self.mu_eff - 2 + (1 / self.mu_eff)) / ((self.dim + 2)**2 + self.mu_eff))\n\n    def sample_population(self, func):\n        z = np.random.multivariate_normal(np.zeros(self.dim), np.eye(self.dim), size=self.pop_size)\n        A = np.linalg.cholesky(self.C)\n        population = self.mean + self.mutation_factor * A @ z.T\n        population = np.clip(population.T, func.bounds.lb, func.bounds.ub)\n        return population, z\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.last_improvement = 0\n\n        self.initialize(func)\n        \n        while self.eval_count < self.budget:\n            population, z = self.sample_population(func)\n            fitness = np.array([func(x) for x in population])\n            self.eval_count += self.pop_size\n\n            if np.min(fitness) < self.f_opt:\n                self.f_opt = np.min(fitness)\n                self.x_opt = population[np.argmin(fitness)].copy()\n                self.last_improvement = self.eval_count\n\n            # Sort the population and z according to fitness\n            indices = np.argsort(fitness)\n            population = population[indices]\n            z = z[indices]\n            \n            # Selection and Recombination\n            mu = self.pop_size // 2\n            weights = np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))\n            weights /= np.sum(weights)\n\n            delta_mean = np.sum(weights[:, None] * z[:mu], axis=0)\n            \n            # Update evolution path\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mu_eff) * delta_mean\n            \n            if np.linalg.norm(self.ps) / self.chiN < 1.4 + 2 / (self.dim + 1):\n                hsig = 1\n            else:\n                hsig = 0\n\n            self.pc = (1 - self.c1) * self.pc + hsig * np.sqrt(self.c1 * (2 - self.c1) * self.mu_eff) * np.linalg.cholesky(self.C) @ delta_mean\n            \n            # Update Covariance Matrix\n            self.C = (1 - self.c1 - self.cmu + self.c1 * self.cmu * (2 - hsig)**2) * self.C \\\n                     + self.c1 * np.outer(self.pc, self.pc) \\\n                     + self.cmu * np.sum(weights[:, None, None] * (z[:mu, :, None] * z[:mu, None, :]), axis=0)\n            \n            self.mean += self.mutation_factor * np.linalg.cholesky(self.C) @ delta_mean\n\n            if self.eval_count - self.last_improvement > self.stagnation_threshold:\n                self.initialize(func) # Restart\n                self.mutation_factor *= 0.8 # reduce step size after stagnation\n\n            self.mutation_factor *= np.random.uniform(0.9, 1.1)\n            self.mutation_factor = np.clip(self.mutation_factor, 0.1, 1.0)\n            \n            if self.eval_count >= self.budget:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "An exception occurred: unsupported operand type(s) for -: 'NoneType' and 'int'.", "error": "", "parent_ids": ["32c2eb2f-0577-4c8e-9745-ebb5f96141cf"], "operator": null, "metadata": {}}
{"id": "31c5f757-63cc-425a-8823-c093cc12f08a", "fitness": 0.7024383754053336, "name": "AdaptiveDE", "description": "Simplified Adaptive DE with population-based mutation and adaptive parameters tuned for exploration and exploitation balance.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, mutation_factor=0.5, crossover_rate=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.mutation_factor = mutation_factor\n        self.crossover_rate = crossover_rate\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.eval_count += self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Mutation - use best individual in the population\n                best_individual = population[np.argmin(fitness)]\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b = population[np.random.choice(idxs, 2, replace=False)]\n                mutant = best_individual + self.mutation_factor * (a - b)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.crossover_rate\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.eval_count += 1\n                \n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                # Update optimal solution\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial.copy()\n                \n                if self.eval_count >= self.budget:\n                    break\n            \n            # Adaptive parameter adjustment - simplified and less aggressive\n            self.mutation_factor *= np.random.uniform(0.95, 1.05)\n            self.crossover_rate *= np.random.uniform(0.95, 1.05)\n            self.mutation_factor = np.clip(self.mutation_factor, 0.1, 1.0)\n            self.crossover_rate = np.clip(self.crossover_rate, 0.1, 1.0)\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDE scored 0.702 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["32c2eb2f-0577-4c8e-9745-ebb5f96141cf"], "operator": null, "metadata": {"aucs": [0.2802202112417177, 0.706287211669779, 0.9056698920962678, 0.9697600236002148, 0.9053977397071867, 0.9351344194102893, 0.8678985021896571, 0.9259546154583542, 0.8769481718583267, 0.20042052840796798, 0.9510891320576282, 0.9970131926127428, 0.3528508031417007, 0.3584528526610917, 0.5873291136180517, 0.9300237789225552, 0.45349779724509276, 0.9525452415568445, 0.3508540157116974, 0.5414202649395057]}}
{"id": "80002dc8-9d65-4ae6-a6cf-c590c2bc51a2", "fitness": 0.599566397164269, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with simplified parameter adaptation, archive, and a blend of exploitation and exploration by adjusting mutation based on success.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=50, F=0.7, CR=0.5, adapt_freq=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.archive = []\n        self.archive_fitness = []\n        self.adapt_freq = adapt_freq\n        self.success_F = []\n        self.success_CR = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Mutation - Blend exploitation and exploration\n                best_idx = np.argmin(self.fitness)\n                x_best = self.population[best_idx]\n\n                if len(self.archive) > 0 and np.random.rand() < 0.7:  # Prefer archive\n                    xa_idx = np.random.choice(range(len(self.archive)))\n                    xa = self.archive[xa_idx]\n                    mutant = self.population[i] + self.F * (x_best - self.population[i]) + self.F * (xa - self.population[i])\n                else:\n                    idx = np.random.choice(range(self.pop_size), 2, replace=False)\n                    x_rand1, x_rand2 = self.population[idx[0]], self.population[idx[1]]\n                    mutant = self.population[i] + self.F * (x_rand1 - x_rand2)\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    self.success_F.append(self.F)\n                    self.success_CR.append(self.CR)\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n                    \n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        max_archive_fitness_idx = np.argmax(self.archive_fitness)\n                        if f_trial < self.archive_fitness[max_archive_fitness_idx]:\n                            self.archive[max_archive_fitness_idx] = trial\n                            self.archive_fitness[max_archive_fitness_idx] = f_trial\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n\n            #Adapt F and CR (optional) - Simplified adaptation\n            if generation % self.adapt_freq == 0:\n                if self.success_F:\n                    self.F = np.mean(self.success_F)\n                    self.CR = np.mean(self.success_CR)\n                self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n                self.CR = np.clip(np.random.normal(self.CR, 0.1), 0.1, 1.0)\n                self.success_F = []\n                self.success_CR = []\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDE scored 0.600 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["adad8070-9279-4096-9544-b29a9eef1cd2"], "operator": null, "metadata": {"aucs": [0.2660459411583923, 0.3344772426529149, 0.4303010018577381, 0.903734288712237, 0.7835448864109114, 0.7295617700646031, 0.5322865194560724, 0.5818819808181346, 0.6289296405737234, 0.47845911853891643, 0.7317403078928084, 0.9957113496288329, 0.3064186239859161, 0.6367655441785055, 0.7332034338089822, 0.7519544247006755, 0.4352337248882979, 0.8239256016743489, 0.40391989960854413, 0.5032326426748246]}}
{"id": "f85c5ca1-8cde-4853-97fa-b4f71dafeb81", "fitness": 0.5568419436945768, "name": "AdaptiveDE", "description": "Simplified Adaptive Differential Evolution with Archive and adaptive F/CR based on success, using a single list for both F and CR success history to streamline adaptation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=50, F=0.7, CR=0.5, history_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.CR = CR\n        self.archive = []\n        self.archive_fitness = []\n        self.history_size = history_size\n        self.success_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                if len(self.archive) > 0:\n                    idx = np.random.choice(range(self.pop_size), 3, replace=False)\n                    xa_idx = np.random.choice(range(len(self.archive)), 1)[0]\n                    x_rand1, x_rand2, x_rand3 = self.population[idx[0]], self.population[idx[1]], self.population[idx[2]]\n                    xa = self.archive[xa_idx]\n                    mutant = self.population[i] + self.F * (xa - self.population[i]) + self.F * (x_rand1 - x_rand2)\n                else:\n                    idx = np.random.choice(range(self.pop_size), 3, replace=False)\n                    x_rand1, x_rand2, x_rand3 = self.population[idx[0]], self.population[idx[1]], self.population[idx[2]]\n                    mutant = self.population[i] + self.F * (x_rand1 - x_rand2)\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n                    self.success_history.append((self.F, self.CR))\n                    \n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        max_archive_fitness_idx = np.argmax(self.archive_fitness)\n                        if f_trial < self.archive_fitness[max_archive_fitness_idx]:\n                            self.archive[max_archive_fitness_idx] = trial\n                            self.archive_fitness[max_archive_fitness_idx] = f_trial\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n\n            # Adapt F and CR\n            if len(self.success_history) > 0:\n                successful_F = [f for f, cr in self.success_history]\n                successful_CR = [cr for f, cr in self.success_history]\n\n                self.F = np.clip(np.mean(successful_F), 0.1, 1.0) if successful_F else np.clip(np.random.normal(0.7, 0.1), 0.1, 1.0)\n                self.CR = np.clip(np.mean(successful_CR), 0.1, 1.0) if successful_CR else np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n                \n            else:\n                self.F = np.clip(np.random.normal(0.7, 0.1), 0.1, 1.0)\n                self.CR = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n\n            self.success_history = []\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDE scored 0.557 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b25e2ca3-787c-442c-ad08-200eeee61e1e"], "operator": null, "metadata": {"aucs": [0.19076343480945634, 0.3178765764242669, 0.5465117750746933, 0.7674606545048972, 0.6160492336039265, 0.710177656910093, 0.41948227575265373, 0.5283890326538776, 0.6026359594062647, 0.4524154684158578, 0.7010293991407532, 0.9965280072294411, 0.3174145056871581, 0.5251395199920902, 0.8147272466048103, 0.725546400962688, 0.4067746981461762, 0.7828178135659827, 0.21342657681161814, 0.5016726381948324]}}
{"id": "06932a9c-3902-4608-8134-cd84203034b2", "fitness": 0.7459364390486837, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with a dynamically adjusted population size and learning rate, enhancing exploration and exploitation balance.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size_factor=5, F=0.5, CR=0.7, lr_F=0.1, lr_CR=0.1):\n        \"\"\"\n        Initialize the Adaptive Differential Evolution algorithm.\n\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size_factor (int): Factor to determine the population size (pop_size = pop_size_factor * dim).\n            F (float): The initial scaling factor.\n            CR (float): The initial crossover rate.\n            lr_F (float): Learning rate for adapting F.\n            lr_CR (float): Learning rate for adapting CR.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = int(pop_size_factor * dim)\n        self.F = F\n        self.CR = CR\n        self.lr_F = lr_F\n        self.lr_CR = lr_CR\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.success_F = []\n        self.success_CR = []\n        self.archive = []\n        self.archive_size = self.pop_size\n\n    def initialize_population(self, func):\n        \"\"\"Initialize the population randomly within the bounds.\"\"\"\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.pop[np.argmin(self.fitness)]\n\n    def update_archive(self, trial):\n        if len(self.archive) < self.archive_size:\n            self.archive.append(trial)\n        else:\n            index = np.random.randint(0, self.archive_size)\n            self.archive[index] = trial\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using Adaptive Differential Evolution.\n\n        Args:\n            func (callable): The function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            \n            for i in range(self.pop_size):\n                # Parameter adaptation based on success history\n                if self.success_F:\n                    self.F = np.clip(np.random.choice(self.success_F), 0.1, 1.0)\n                else:\n                    self.F = np.clip(np.random.normal(0.5, self.lr_F), 0.1, 1.0)\n\n                if self.success_CR:\n                    self.CR = np.clip(np.random.choice(self.success_CR), 0.1, 1.0)\n                else:\n                    self.CR = np.clip(np.random.normal(0.7, self.lr_CR), 0.1, 1.0)\n\n\n                # Mutation using archive\n                if len(self.archive) > 0 and np.random.rand() < 0.1:  # Use archive occasionally\n                    xa = np.random.choice(len(self.archive))\n                    arch_member = self.archive[xa]\n                    idxs = np.random.choice(self.pop_size, 2, replace=False)\n                    b, c = idxs[0], idxs[1]\n                    mutant = self.pop[i] + self.F * (arch_member - self.pop[b]) + self.F * (self.pop[c] - self.pop[i])  # Using current population\n                else:\n                    idxs = np.random.choice(self.pop_size, 3, replace=False)\n                    a, b, c = idxs[0], idxs[1], idxs[2]\n                    mutant = self.pop[a] + self.F * (self.pop[b] - self.pop[c])\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Selection\n                f = func(trial)\n                self.eval_count += 1\n\n                if f < self.fitness[i]:\n                    self.success_F.append(self.F)\n                    self.success_CR.append(self.CR)\n                    self.fitness[i] = f\n                    self.pop[i] = trial\n                    self.update_archive(self.pop[i].copy()) # Store successful individuals\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n\n                if len(self.success_F) > 10:\n                    self.success_F.pop(0)\n                if len(self.success_CR) > 10:\n                    self.success_CR.pop(0)\n\n                if self.eval_count >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDE scored 0.746 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["57bc6944-d16f-4227-b9e7-a0f80e3e6bda"], "operator": null, "metadata": {"aucs": [0.43871874551788315, 0.745363737534473, 0.6997142373633207, 0.8879363216343765, 0.859631575576879, 0.8773510558293469, 0.7996165095314691, 0.7519789745152138, 0.8294335666840853, 0.7784169054865165, 0.8958749887689703, 0.9995046350461588, 0.349839276635881, 0.8715790135064637, 0.9205743903063723, 0.8912952026323427, 0.6878951350170289, 0.8671651502011992, 0.24497008815512644, 0.5218692710305648]}}
{"id": "046874a4-b44a-40e1-91a2-5e87f5db1c4d", "fitness": 0.1515742668823338, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with orthogonal learning and dynamic local search adjustment based on success rate.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, mutation_factor=0.5, crossover_rate=0.7, local_search_iterations=3, ls_reduction_factor=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.mutation_factor = mutation_factor\n        self.crossover_rate = crossover_rate\n        self.local_search_iterations = local_search_iterations\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.success_rate = 0.0\n        self.ls_reduction_factor = ls_reduction_factor\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        success_count = 0\n        total_trials = 0\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.eval_count += self.pop_size\n\n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index].copy()\n        \n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = a + self.mutation_factor * (b - c)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.crossover_rate\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.eval_count += 1\n                total_trials += 1\n                \n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = trial\n                    success_count += 1\n\n                    # Local Search with orthogonal learning\n                    best_local_fitness = f_trial\n                    best_local_trial = trial.copy()\n\n                    for _ in range(int(self.local_search_iterations)):  # Dynamic local search iterations\n                        # Generate orthogonal array (simplified for example, choose 2 random dimensions)\n                        dims_to_perturb = np.random.choice(self.dim, min(2, self.dim), replace=False)\n                        levels = [-0.01, 0.01]  # Perturbation levels\n                        \n                        for level_1 in levels:\n                            temp_trial = trial.copy()\n                            temp_trial[dims_to_perturb[0]] += level_1\n                            temp_trial = np.clip(temp_trial, func.bounds.lb, func.bounds.ub)\n\n                            if len(dims_to_perturb) > 1:\n                                for level_2 in levels:\n                                    local_trial = temp_trial.copy()\n                                    local_trial[dims_to_perturb[1]] += level_2\n                                    local_trial = np.clip(local_trial, func.bounds.lb, func.bounds.ub)\n\n                                    f_local = func(local_trial)\n                                    self.eval_count += 1\n                                    if f_local < best_local_fitness:\n                                        best_local_fitness = f_local\n                                        best_local_trial = local_trial.copy()\n                            else:\n                                f_local = func(temp_trial)\n                                self.eval_count += 1\n                                if f_local < best_local_fitness:\n                                    best_local_fitness = f_local\n                                    best_local_trial = temp_trial.copy()\n\n\n                    if best_local_fitness < f_trial:\n                        fitness[i] = best_local_fitness\n                        population[i] = best_local_trial\n                        f_trial = best_local_fitness\n                        trial = best_local_trial\n\n                # Update optimal solution\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial.copy()\n                \n                if self.eval_count >= self.budget:\n                    break\n            \n            # Adaptive parameter adjustment - simplified\n            self.mutation_factor *= np.random.uniform(0.9, 1.1)\n            self.crossover_rate *= np.random.uniform(0.9, 1.1)\n            self.mutation_factor = np.clip(self.mutation_factor, 0.1, 1.0)\n            self.crossover_rate = np.clip(self.crossover_rate, 0.1, 1.0)\n\n            # Adjust local search iterations based on success rate\n            if total_trials > 0:\n                self.success_rate = success_count / total_trials\n                if self.success_rate < 0.2:\n                    self.local_search_iterations *= self.ls_reduction_factor  # Reduce if success rate is low\n                elif self.success_rate > 0.5:\n                    self.local_search_iterations /= self.ls_reduction_factor # Increase if success rate is high\n                \n                self.local_search_iterations = np.clip(self.local_search_iterations, 1, 5) #Bound the local search iterations\n\n            success_count = 0\n            total_trials = 0\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDE scored 0.152 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["32c2eb2f-0577-4c8e-9745-ebb5f96141cf"], "operator": null, "metadata": {"aucs": [0.1853082497623625, 0.26941455088463895, 0]}}
{"id": "7be47c02-acfa-4b9d-a35a-5e72cc757ff0", "fitness": 0.46347914799751877, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with success-history based parameter adaptation and a larger population for better exploration.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=100, F=0.5, CR=0.7, archive_size=10):\n        \"\"\"\n        Initialize the Adaptive Differential Evolution algorithm.\n\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size (int): The population size.\n            F (float): The initial scaling factor.\n            CR (float): The initial crossover rate.\n            archive_size (int): The size of the archive for storing successful F and CR values.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.archive_F = []\n        self.archive_CR = []\n        self.archive_size = archive_size\n\n    def initialize_population(self, func):\n        \"\"\"Initialize the population randomly within the bounds.\"\"\"\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.pop[np.argmin(self.fitness)]\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using Adaptive Differential Evolution.\n\n        Args:\n            func (callable): The function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                a, b, c = idxs[0], idxs[1], idxs[2]\n                \n                mutant = self.pop[a] + self.F * (self.pop[b] - self.pop[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Selection\n                f = func(trial)\n                self.eval_count += 1\n\n                if f < self.fitness[i]:\n                    # Archive successful F and CR\n                    if len(self.archive_F) > self.archive_size:\n                        self.archive_F.pop(0)\n                        self.archive_CR.pop(0)\n                    self.archive_F.append(self.F)\n                    self.archive_CR.append(self.CR)\n\n                    self.fitness[i] = f\n                    self.pop[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                        \n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Adaptive parameter update based on success history\n            if self.archive_F:\n                self.F = np.clip(np.mean(self.archive_F), 0.1, 1.0)\n                self.CR = np.clip(np.mean(self.archive_CR), 0.1, 1.0)\n            else:\n                 self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n                 self.CR = np.clip(np.random.normal(0.7, 0.1), 0.1, 1.0)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDE scored 0.463 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["57bc6944-d16f-4227-b9e7-a0f80e3e6bda"], "operator": null, "metadata": {"aucs": [0.176743982985183, 0.2800696646877444, 0.44135562356477853, 0.6267587878098885, 0.4435170365961997, 0.5582939099297006, 0.33798858866703074, 0.4137752966700813, 0.4385695688040303, 0.27866501832153756, 0.6122119166667521, 0.9876282327837399, 0.27715612144262936, 0.39875025562084687, 0.7758608484485884, 0.5408121080187529, 0.3740916371827999, 0.622568195661992, 0.18605321749216386, 0.4987129485959362]}}
{"id": "01dd6b4e-25ce-491b-800d-909e9a80854d", "fitness": -Infinity, "name": "AdaptiveDEOrtho", "description": "Adaptive Differential Evolution with orthogonal learning, dynamically adjusting F and CR based on the rank of improvements and population diversity, promoting both convergence and exploration.", "code": "import numpy as np\n\nclass AdaptiveDEOrtho:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=50, F=0.7, CR=0.5, history_size=10, ortho_group_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.CR = CR\n        self.archive = []\n        self.archive_fitness = []\n        self.history_size = history_size\n        self.success_history = []\n        self.ortho_group_size = ortho_group_size  # Size of orthogonal groups\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        while self.budget > 0:\n            ranks = np.argsort(self.fitness) # Rank individuals based on fitness\n            diversity = np.std(self.population, axis=0).mean() # Calculate population diversity\n            \n            for i in range(self.pop_size):\n                # Mutation\n                if len(self.archive) > 0:\n                    idx = np.random.choice(range(self.pop_size), 3, replace=False)\n                    xa_idx = np.random.choice(range(len(self.archive)), 1)[0]\n                    x_rand1, x_rand2, x_rand3 = self.population[idx[0]], self.population[idx[1]], self.population[idx[2]]\n                    xa = self.archive[xa_idx]\n                    mutant = self.population[i] + self.F * (xa - self.population[i]) + self.F * (x_rand1 - x_rand2)\n                else:\n                    idx = np.random.choice(range(self.pop_size), 3, replace=False)\n                    x_rand1, x_rand2, x_rand3 = self.population[idx[0]], self.population[idx[1]], self.population[idx[2]]\n                    mutant = self.population[i] + self.F * (x_rand1 - x_rand2)\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Orthogonal Learning: Sample around the individual and its mutant\n                ortho_samples = np.random.uniform(min(self.population[i], mutant), max(self.population[i], mutant), size=(self.ortho_group_size, self.dim))\n                ortho_fitness = np.array([func(x) for x in ortho_samples])\n                self.budget -= self.ortho_group_size\n                best_ortho_idx = np.argmin(ortho_fitness)\n                best_ortho = ortho_samples[best_ortho_idx]\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, best_ortho, self.population[i]) # Use best ortho sample in crossover\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n                    self.success_history.append((self.F, self.CR, ranks[i]))  # Store rank\n\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        max_archive_fitness_idx = np.argmax(self.archive_fitness)\n                        if f_trial < self.archive_fitness[max_archive_fitness_idx]:\n                            self.archive[max_archive_fitness_idx] = trial\n                            self.archive_fitness[max_archive_fitness_idx] = f_trial\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n            \n            # Adapt F and CR based on ranks and diversity\n            if len(self.success_history) > 0:\n                successful_F = [f for f, cr, rank in self.success_history]\n                successful_CR = [cr for f, cr, rank in self.success_history]\n                successful_ranks = [rank for f, cr, rank in self.success_history]\n                \n                # Weigh F and CR update based on rank\n                rank_weights = np.exp(-np.array(successful_ranks) / self.pop_size) # Higher rank gets smaller weight\n                rank_weights /= rank_weights.sum() # Normalize weights\n\n                self.F = np.clip(np.average(successful_F, weights=rank_weights), 0.1, 1.0) if successful_F else np.clip(np.random.normal(0.7, 0.1), 0.1, 1.0)\n                self.CR = np.clip(np.average(successful_CR, weights=rank_weights), 0.1, 1.0) if successful_CR else np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n\n                # Adjust F and CR based on diversity\n                self.F *= (1 + diversity) # Increase F when diversity is high to promote exploration\n                self.CR *= (1 - diversity) # Decrease CR when diversity is low to promote exploitation\n                self.F = np.clip(self.F, 0.1, 1.0)\n                self.CR = np.clip(self.CR, 0.1, 1.0)\n            \n            else:\n                self.F = np.clip(np.random.normal(0.7, 0.1), 0.1, 1.0)\n                self.CR = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n\n            self.success_history = []\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "An exception occurred: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all().", "error": "", "parent_ids": ["f85c5ca1-8cde-4853-97fa-b4f71dafeb81"], "operator": null, "metadata": {}}
{"id": "17c47507-9af5-4a54-a1dc-a8f7119bddc7", "fitness": 0.0, "name": "SelfAdaptiveDE", "description": "Self-Adaptive Differential Evolution with multiple mutation strategies and a dynamically adjusted population size based on success rate, improving exploration and exploitation.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, mutation_strategies = None):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.success_rates = np.zeros(4)  # Success rate for each mutation strategy\n        self.mutation_strategy_weights = np.ones(4) / 4  # Initial weights for each strategy\n\n        if mutation_strategies is None:\n            self.mutation_strategies = [\n                self._mutation_strategy_1,  # \"DE/rand/1\"\n                self._mutation_strategy_2,  # \"DE/best/1\"\n                self._mutation_strategy_3,  # \"DE/rand/2\"\n                self._mutation_strategy_4   # \"DE/current-to-rand/1\"\n            ]\n        else:\n            self.mutation_strategies = mutation_strategies\n\n\n    def _mutation_strategy_1(self, population, i, mutation_factor, func):  # DE/rand/1\n        idxs = [idx for idx in range(self.pop_size) if idx != i]\n        a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n        return a + mutation_factor * (b - c)\n\n    def _mutation_strategy_2(self, population, i, mutation_factor, func):  # DE/best/1\n         best_individual = population[np.argmin([func(x) for x in population])]\n         idxs = [idx for idx in range(self.pop_size) if idx != i]\n         a, b = population[np.random.choice(idxs, 2, replace=False)]\n         return best_individual + mutation_factor * (a - b)\n\n    def _mutation_strategy_3(self, population, i, mutation_factor, func):  # DE/rand/2\n        idxs = [idx for idx in range(self.pop_size) if idx != i]\n        a, b, c, d, e = population[np.random.choice(idxs, 5, replace=False)]\n        return a + mutation_factor * (b - c) + mutation_factor * (d-e)\n    \n    def _mutation_strategy_4(self, population, i, mutation_factor, func):  #DE/current-to-rand/1\n        idxs = [idx for idx in range(self.pop_size) if idx != i]\n        a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n        return population[i] + mutation_factor * (a - population[i]) + mutation_factor * (b - c)\n    \n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.eval_count += self.pop_size\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        archive = []\n        archive_fitness = []\n\n        success_count = np.zeros(len(self.mutation_strategies))\n        trial_vector_fitness_values = []\n\n        while self.eval_count < self.budget:\n            mutation_factor = np.random.uniform(0.4, 0.9)\n            crossover_rate = np.random.uniform(0.1, 0.9)\n            for i in range(self.pop_size):\n                strategy_index = np.random.choice(len(self.mutation_strategies), p=self.mutation_strategy_weights)\n                mutant = self.mutation_strategies[strategy_index](population, i, mutation_factor, func)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                cross_points = np.random.rand(self.dim) < crossover_rate\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                f_trial = func(trial)\n                self.eval_count += 1\n                trial_vector_fitness_values.append(f_trial)\n                \n                if f_trial < fitness[i]:\n                    success_count[strategy_index] += 1\n                    fitness[i] = f_trial\n                    population[i] = trial\n                    \n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial.copy()\n                        \n                if self.eval_count >= self.budget:\n                    break\n\n            # Update strategy weights based on success\n            self.success_rates = success_count / self.pop_size\n            self.mutation_strategy_weights = self.success_rates / np.sum(self.success_rates) # Normalize\n\n            # Adjust population size dynamically based on the average improvement\n            avg_improvement = np.mean(fitness) - np.mean(trial_vector_fitness_values)\n            if avg_improvement > 0.001 and self.pop_size < 200:\n                    self.pop_size = min(self.pop_size + 5, 200)  # Increase if improvement is good\n                    new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(5, self.dim))\n                    population = np.vstack((population, new_individuals))\n                    fitness = np.concatenate((fitness, [func(x) for x in new_individuals]))\n                    self.eval_count += 5\n            elif avg_improvement < -0.001 and self.pop_size > 20:\n                    self.pop_size = max(self.pop_size - 5, 20)  # Decrease if improvement is poor\n                    population = population[:-5]\n                    fitness = fitness[:-5]\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm SelfAdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["31c5f757-63cc-425a-8823-c093cc12f08a"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "f3adfd6b-0e13-44e5-8ad3-226c34ac8083", "fitness": 0.5712474788160613, "name": "AdaptiveDE", "description": "Simplified Adaptive Differential Evolution with a streamlined archive update and parameter adaptation based on successful generations, reducing complexity while maintaining performance.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=50, F=0.7, CR=0.5, adapt_freq=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.CR = CR\n        self.archive = []\n        self.adapt_freq = adapt_freq\n        self.success_F = []\n        self.success_CR = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Mutation\n                best_idx = np.argmin(self.fitness)\n                x_best = self.population[best_idx]\n\n                if len(self.archive) > 0 and np.random.rand() < 0.7:\n                    xa = self.archive[np.random.randint(len(self.archive))]\n                    mutant = self.population[i] + self.F * (x_best - self.population[i]) + self.F * (xa - self.population[i])\n                else:\n                    idx = np.random.choice(range(self.pop_size), 2, replace=False)\n                    mutant = self.population[i] + self.F * (self.population[idx[0]] - self.population[idx[1]])\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    self.success_F.append(self.F)\n                    self.success_CR.append(self.CR)\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n                    \n                    # Simplified Archive Update: Replace a random archive member\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                    else:\n                        self.archive[np.random.randint(self.archive_size)] = trial\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n\n            # Adapt F and CR - Simplified adaptation: Mean of successful values\n            if generation % self.adapt_freq == 0:\n                if self.success_F:\n                    self.F = np.mean(self.success_F)\n                    self.CR = np.mean(self.success_CR)\n                self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n                self.CR = np.clip(np.random.normal(self.CR, 0.1), 0.1, 1.0)\n                self.success_F = []\n                self.success_CR = []\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDE scored 0.571 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["80002dc8-9d65-4ae6-a6cf-c590c2bc51a2"], "operator": null, "metadata": {"aucs": [0.23952027292956035, 0.23174502359246396, 0.5289655437038728, 0.7800854875115034, 0.7613055133723665, 0.7986590591787092, 0.31597750051159124, 0.4412661080243443, 0.7070567125442604, 0.6472745340997385, 0.5432337498255211, 0.9927357099345601, 0.27072477749456025, 0.7571724434272857, 0.7185610705191317, 0.8037401533236641, 0.4838076794236327, 0.6835071631646361, 0.20422985348422462, 0.5153812202555996]}}
{"id": "1c0e22b0-c40e-47a0-aa18-48091b5bdcaf", "fitness": 0.5763895074844536, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with simplified parameter adaptation and memory-based mutation using the best solution and a random archive member.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=50, F=0.7, CR=0.5, adapt_freq=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.archive = []\n        self.adapt_freq = adapt_freq\n        self.success_F = []\n        self.success_CR = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Mutation - Blend exploitation and exploration using archive and best\n                best_idx = np.argmin(self.fitness)\n                x_best = self.population[best_idx]\n\n                if self.archive:\n                    xa = self.archive[np.random.choice(len(self.archive))]\n                    mutant = self.population[i] + self.F * (x_best - self.population[i]) + self.F * (xa - self.population[i])\n                else:\n                    idx = np.random.choice(range(self.pop_size), 2, replace=False)\n                    x_rand1, x_rand2 = self.population[idx[0]], self.population[idx[1]]\n                    mutant = self.population[i] + self.F * (x_rand1 - x_rand2)\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    self.success_F.append(self.F)\n                    self.success_CR.append(self.CR)\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n                    \n                    # Update archive - always add improving solutions\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                    else:\n                        # Replace a random archive member\n                        self.archive[np.random.randint(0, self.archive_size)] = trial\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n\n            #Adapt F and CR (optional) - Simplified adaptation\n            if generation % self.adapt_freq == 0:\n                if self.success_F:\n                    self.F = np.mean(self.success_F)\n                    self.CR = np.mean(self.success_CR)\n                self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n                self.CR = np.clip(np.random.normal(self.CR, 0.1), 0.1, 1.0)\n                self.success_F = []\n                self.success_CR = []\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDE scored 0.576 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["80002dc8-9d65-4ae6-a6cf-c590c2bc51a2"], "operator": null, "metadata": {"aucs": [0.20745307775749966, 0.3090035166609081, 0.8191416186034535, 0.9005364645904304, 0.6805387866128803, 0.7940507541374033, 0.4298739306179947, 0.34395535721216075, 0.6321584520203176, 0.43849073513522885, 0.6851309108562375, 0.9961254869058729, 0.4181680244370243, 0.670474817636225, 0.756584776282925, 0.7425719046977901, 0.48318681420551934, 0.4349714684853173, 0.26541518182151835, 0.5199580710123655]}}
{"id": "df265f0f-5076-48a4-99db-ab90bc5213d5", "fitness": 0.6703007637675251, "name": "HybridDE", "description": "Hybrid Differential Evolution with self-adaptive parameters, orthogonal learning, and a ring topology for population interaction, balancing exploration and exploitation.", "code": "import numpy as np\n\nclass HybridDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, adapt_freq=10, ring_neighbors=3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.adapt_freq = adapt_freq\n        self.ring_neighbors = ring_neighbors\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Ring Topology Selection\n                neighbors = [(i - j) % self.pop_size for j in range(1, self.ring_neighbors + 1)]\n                neighbor_fitnesses = self.fitness[neighbors]\n                best_neighbor_idx = neighbors[np.argmin(neighbor_fitnesses)]\n                \n                # Mutation - Use best neighbor in ring\n                idx = np.random.choice(range(self.pop_size), 2, replace=False)\n                x_rand1, x_rand2 = self.population[idx[0]], self.population[idx[1]]\n                mutant = self.population[i] + self.F * (self.population[best_neighbor_idx] - self.population[i]) + self.F * (x_rand1 - x_rand2)\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Orthogonal Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n\n            #Adapt F and CR\n            if generation % self.adapt_freq == 0:\n                self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n                self.CR = np.clip(np.random.normal(self.CR, 0.1), 0.1, 1.0)\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm HybridDE scored 0.670 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["80002dc8-9d65-4ae6-a6cf-c590c2bc51a2"], "operator": null, "metadata": {"aucs": [0.22493234672096662, 0.6271670349993907, 0.6970520080475189, 0.8798001056497406, 0.7245279544916872, 0.8728465530431988, 0.4339042544035525, 0.7714527540972372, 0.8378402888741572, 0.3464550201144241, 0.8991850511933793, 0.9928886139989775, 0.3905562915406594, 0.7489993188032096, 0.9383444879748957, 0.8110896164317974, 0.5579558351030436, 0.8980763038496065, 0.23447355657790936, 0.5184678794351516]}}
{"id": "cfe1f0c2-021e-4138-bca4-af3c0962bb72", "fitness": 0.3552667447433796, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with simplified adaptation and enhanced exploration using a combined archive and current population for mutation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=50, F=0.7, CR=0.5, adapt_freq=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.CR = CR\n        self.archive = []\n        self.adapt_freq = adapt_freq\n        self.success_F = []\n        self.success_CR = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Mutation\n                pool = np.concatenate((self.population, self.archive)) if len(self.archive) > 0 else self.population\n                idxs = np.random.choice(range(len(pool)), 3, replace=False)\n                x_r0, x_r1, x_r2 = pool[idxs[0]], pool[idxs[1]], pool[idxs[2]]\n                mutant = self.population[i] + self.F * (x_r1 - x_r2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.success_F.append(self.F)\n                    self.success_CR.append(self.CR)\n                    \n                    # Update archive before updating population\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())  # Add the replaced vector\n                    else:\n                        if self.fitness[i] > np.mean(self.fitness): #Replace if the parent is worse than the mean population fitness\n                            idx_to_replace = np.random.randint(0, self.archive_size)\n                            self.archive[idx_to_replace] = self.population[i].copy()\n\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    \n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n\n            # Adaptation\n            if generation % self.adapt_freq == 0:\n                if self.success_F:\n                    self.F = np.mean(self.success_F)\n                    self.CR = np.mean(self.success_CR)\n                self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n                self.CR = np.clip(np.random.normal(self.CR, 0.1), 0.1, 1.0)\n                self.success_F = []\n                self.success_CR = []\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDE scored 0.355 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["80002dc8-9d65-4ae6-a6cf-c590c2bc51a2"], "operator": null, "metadata": {"aucs": [0.1435447486946334, 0.19663665739668967, 0.3449108781271859, 0.31004874663335136, 0.29001191837428764, 0.38719434404629793, 0.293879076364181, 0.3140396912282989, 0.29352358005008905, 0.21655138569792887, 0.3399767750316207, 0.9981570043902168, 0.26070647175333217, 0.2789497580095909, 0.7094167552378028, 0.3611882792891452, 0.27350909286753944, 0.4348088629422272, 0.17452660455791214, 0.48375426417526146]}}
{"id": "a2230d5f-2ba9-4fd6-88f0-6c3aa4f9e57a", "fitness": 0.7437647601391248, "name": "AdaptiveDE", "description": "Simplified Adaptive Differential Evolution with self-adaptive parameters and reduced memory, balancing exploration and exploitation more efficiently.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size_factor=5, F=0.5, CR=0.7, lr_F=0.1, lr_CR=0.1):\n        \"\"\"\n        Initialize the Adaptive Differential Evolution algorithm.\n\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size_factor (int): Factor to determine the population size (pop_size = pop_size_factor * dim).\n            F (float): The initial scaling factor.\n            CR (float): The initial crossover rate.\n            lr_F (float): Learning rate for adapting F.\n            lr_CR (float): Learning rate for adapting CR.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = int(pop_size_factor * dim)\n        self.F = F\n        self.CR = CR\n        self.lr_F = lr_F\n        self.lr_CR = lr_CR\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.archive = []  # simplified: no separate success_F/CR lists\n        self.archive_size = self.pop_size // 2 # Reduced archive size\n\n    def initialize_population(self, func):\n        \"\"\"Initialize the population randomly within the bounds.\"\"\"\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.pop[np.argmin(self.fitness)]\n\n    def update_archive(self, x):\n        \"\"\"Update the archive with a new solution.\"\"\"\n        if len(self.archive) < self.archive_size:\n            self.archive.append(x)\n        else:\n            idx = np.random.randint(0, self.archive_size)\n            self.archive[idx] = x\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using Adaptive Differential Evolution.\n\n        Args:\n            func (callable): The function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Self-adaptive parameters\n                self.F = np.clip(np.random.normal(self.F, self.lr_F), 0.1, 1.0)\n                self.CR = np.clip(np.random.normal(self.CR, self.lr_CR), 0.1, 1.0)\n\n                # Mutation with archive\n                if len(self.archive) > 0 and np.random.rand() < 0.1:  # Reduced archive usage\n                    xa = np.random.randint(0, len(self.archive))\n                    arch_member = self.archive[xa]\n                    \n                    idx_r1 = np.random.randint(0, self.pop_size)\n                    mutant = self.pop[i] + self.F * (arch_member - self.pop[i]) + self.F * (self.pop[idx_r1] - self.pop[i])\n                else:\n                    idxs = np.random.choice(self.pop_size, 3, replace=False)\n                    a, b, c = idxs[0], idxs[1], idxs[2]\n                    mutant = self.pop[a] + self.F * (self.pop[b] - self.pop[c])\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Selection\n                f = func(trial)\n                self.eval_count += 1\n\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.pop[i] = trial\n                    self.update_archive(self.pop[i].copy())\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n\n                if self.eval_count >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDE scored 0.744 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["06932a9c-3902-4608-8134-cd84203034b2"], "operator": null, "metadata": {"aucs": [0.30629861608052866, 0.6742248096536829, 0.7953921021172816, 0.8858038879930712, 0.8511994525854969, 0.8359144348684578, 0.7620147018224119, 0.8156708088296589, 0.822565094306624, 0.8048601056976963, 0.8795596443826258, 0.9955890842700474, 0.37602015084784657, 0.7903910828368761, 0.9088960209303053, 0.8630517817952092, 0.7237065595132606, 0.8896013474249259, 0.2405627806116757, 0.6539727362148164]}}
{"id": "c49fb3e4-0ec9-44f0-b553-8e1c3fb534ba", "fitness": 0.3976475101485682, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with Archive, adaptive F/CR, and a memory-based adaptation of F and CR using weighted averages favoring recent successes, plus selective pressure on archive members based on diversity.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=50, F=0.7, CR=0.5, history_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.CR = CR\n        self.archive = []\n        self.archive_fitness = []\n        self.history_size = history_size\n        self.success_history_F = []\n        self.success_history_CR = []\n        self.archive_selection_rate = 0.1 # Probability of selecting archive member\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                if len(self.archive) > 0 and np.random.rand() < self.archive_selection_rate:\n                    idx = np.random.choice(range(self.pop_size), 2, replace=False)\n                    xa_idx = np.random.choice(range(len(self.archive)), 1)[0]\n                    x_rand1, x_rand2 = self.population[idx[0]], self.population[idx[1]]\n                    xa = self.archive[xa_idx]\n                    mutant = self.population[i] + self.F * (xa - self.population[i]) + self.F * (x_rand1 - x_rand2)\n                else:\n                    idx = np.random.choice(range(self.pop_size), 3, replace=False)\n                    x_rand1, x_rand2, x_rand3 = self.population[idx[0]], self.population[idx[1]], self.population[idx[2]]\n                    mutant = self.population[i] + self.F * (x_rand1 - x_rand2)\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n                    self.success_history_F.append(self.F)\n                    self.success_history_CR.append(self.CR)\n                    \n                    # Update archive with diversity consideration\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        # Replace the *most similar* archive member.\n                        distances = [np.linalg.norm(trial - archive_member) for archive_member in self.archive]\n                        max_distance_idx = np.argmax(distances) # Replace least diverse\n                        \n                        if f_trial < self.archive_fitness[max_distance_idx]:\n                            self.archive[max_distance_idx] = trial\n                            self.archive_fitness[max_distance_idx] = f_trial\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n\n            # Adapt F and CR using weighted average\n            if len(self.success_history_F) > 0:\n                weights = np.linspace(0.1, 1.0, len(self.success_history_F)) # Recency-weighted average\n                weights /= np.sum(weights)  # Normalize weights\n\n                self.F = np.clip(np.average(self.success_history_F, weights=weights), 0.1, 1.0)\n                self.CR = np.clip(np.average(self.success_history_CR, weights=weights), 0.1, 1.0)\n            else:\n                self.F = np.clip(np.random.normal(0.7, 0.1), 0.1, 1.0)\n                self.CR = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n                \n            self.success_history_F = []\n            self.success_history_CR = []\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDE scored 0.398 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f85c5ca1-8cde-4853-97fa-b4f71dafeb81"], "operator": null, "metadata": {"aucs": [0.14434468151654456, 0.2385855235562505, 0.39255505348861697, 0.49876810073512756, 0.34324890479925574, 0.45242315905731934, 0.3042272698730799, 0.3592876263478304, 0.3354321444791796, 0.2020911062650732, 0.3759630305905699, 0.9944588213471646, 0.2626910749508311, 0.3257239819830289, 0.7383320267949012, 0.4696320904843828, 0.3218107148611348, 0.5252352370643099, 0.18101113735362473, 0.4871285174231378]}}
{"id": "c306c897-d516-466a-9b5f-6bc88d16dd33", "fitness": 0.5482636474574496, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with Archive, adaptive F/CR based on Lehmer mean of successful values, and simplified parameter adaptation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=50, F=0.7, CR=0.5, history_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.CR = CR\n        self.archive = []\n        self.archive_fitness = []\n        self.history_size = history_size\n        self.success_history_F = []\n        self.success_history_CR = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                if len(self.archive) > 0:\n                    idx = np.random.choice(range(self.pop_size), 3, replace=False)\n                    xa_idx = np.random.choice(range(len(self.archive)), 1)[0]\n                    x_rand1, x_rand2, x_rand3 = self.population[idx[0]], self.population[idx[1]], self.population[idx[2]]\n                    xa = self.archive[xa_idx]\n                    mutant = self.population[i] + self.F * (xa - self.population[i]) + self.F * (x_rand1 - x_rand2)\n                else:\n                    idx = np.random.choice(range(self.pop_size), 3, replace=False)\n                    x_rand1, x_rand2, x_rand3 = self.population[idx[0]], self.population[idx[1]], self.population[idx[2]]\n                    mutant = self.population[i] + self.F * (x_rand1 - x_rand2)\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    delta_fitness = self.fitness[i] - f_trial\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n                    self.success_history_F.append(self.F)\n                    self.success_history_CR.append(self.CR)\n                    \n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        max_archive_fitness_idx = np.argmax(self.archive_fitness)\n                        if f_trial < self.archive_fitness[max_archive_fitness_idx]:\n                            self.archive[max_archive_fitness_idx] = trial\n                            self.archive_fitness[max_archive_fitness_idx] = f_trial\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n\n            # Adapt F and CR using Lehmer mean\n            if len(self.success_history_F) > 0:\n                self.F = np.sum(np.array(self.success_history_F)**2) / np.sum(np.array(self.success_history_F)) if np.sum(np.array(self.success_history_F)) != 0 else np.clip(np.random.normal(0.7, 0.1), 0.1, 1.0)\n                self.CR = np.sum(np.array(self.success_history_CR)**2) / np.sum(np.array(self.success_history_CR)) if np.sum(np.array(self.success_history_CR)) != 0 else np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n                self.F = np.clip(self.F, 0.1, 1.0)\n                self.CR = np.clip(self.CR, 0.1, 1.0)\n            else:\n                self.F = np.clip(np.random.normal(0.7, 0.1), 0.1, 1.0)\n                self.CR = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n\n            self.success_history_F = []\n            self.success_history_CR = []\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDE scored 0.548 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f85c5ca1-8cde-4853-97fa-b4f71dafeb81"], "operator": null, "metadata": {"aucs": [0.18533007038631688, 0.35836019137308495, 0.5442669742152948, 0.7967746316172678, 0.596044942491716, 0.6954507192498947, 0.3705015646413864, 0.511078535345778, 0.6362784337408256, 0.25733835987796094, 0.7044648595236298, 0.9979261145463066, 0.32634976512355496, 0.5385841578984657, 0.833422760597262, 0.7129994595811513, 0.4262035393432071, 0.7694767906689697, 0.20031765919319944, 0.5041034197337206]}}
{"id": "870da4c8-2145-4234-b7a9-147bc187ca24", "fitness": 0.6485964437139394, "name": "AdaptiveDE", "description": "Enhanced Adaptive Differential Evolution with a decaying archive, adaptive population size, and improved F/CR adaptation based on quantiles.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=50, F=0.7, CR=0.5, history_size=10, decay_rate=0.99):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.CR = CR\n        self.archive = []\n        self.archive_fitness = []\n        self.history_size = history_size\n        self.success_history_F = []\n        self.success_history_CR = []\n        self.decay_rate = decay_rate  # Decay rate for archive fitness\n        self.population_size_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        while self.budget > 0:\n            # Adaptive Population Size\n            if len(self.population_size_history) > 5:\n                recent_sizes = self.population_size_history[-5:]\n                if np.std(recent_sizes) < 5 and np.mean(recent_sizes) > 10:  # Stabilized population size\n                    self.pop_size = int(max(10, self.pop_size * 0.95)) # Reduce population size\n                elif np.std(recent_sizes) < 5 and np.mean(recent_sizes) < 10:\n                     self.pop_size = int(min(50, self.pop_size * 1.05)) # Increase population size\n            self.population_size_history.append(self.pop_size)\n\n\n            for i in range(self.pop_size):\n                # Mutation\n                if len(self.archive) > 0:\n                    idx = np.random.choice(range(self.pop_size), 3, replace=False)\n                    xa_idx = np.random.choice(range(len(self.archive)), 1)[0]\n                    x_rand1, x_rand2, x_rand3 = self.population[idx[0]], self.population[idx[1]], self.population[idx[2]]\n                    xa = self.archive[xa_idx]\n                    mutant = self.population[i] + self.F * (xa - self.population[i]) + self.F * (x_rand1 - x_rand2)\n                else:\n                    idx = np.random.choice(range(self.pop_size), 3, replace=False)\n                    x_rand1, x_rand2, x_rand3 = self.population[idx[0]], self.population[idx[1]], self.population[idx[2]]\n                    mutant = self.population[i] + self.F * (x_rand1 - x_rand2)\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n                    self.success_history_F.append(self.F)\n                    self.success_history_CR.append(self.CR)\n                    \n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        max_archive_fitness_idx = np.argmax(self.archive_fitness)\n                        if f_trial < self.archive_fitness[max_archive_fitness_idx]:\n                            self.archive[max_archive_fitness_idx] = trial\n                            self.archive_fitness[max_archive_fitness_idx] = f_trial\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n\n            # Adapt F and CR using quantiles\n            if len(self.success_history_F) > 0:\n                self.F = np.quantile(self.success_history_F, 0.5)  # Median F\n                self.CR = np.quantile(self.success_history_CR, 0.5) # Median CR\n            else:\n                self.F = np.clip(np.random.normal(0.7, 0.1), 0.1, 1.0)\n                self.CR = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n            \n            self.success_history_F = []\n            self.success_history_CR = []\n\n            # Decay Archive Fitness\n            self.archive_fitness = [f * self.decay_rate for f in self.archive_fitness]\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDE scored 0.649 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f85c5ca1-8cde-4853-97fa-b4f71dafeb81"], "operator": null, "metadata": {"aucs": [0.20473337054932395, 0.2964364865668523, 0.4752703472826544, 0.8921257275374508, 0.7838694307788222, 0.8704763547389031, 0.377063999528394, 0.7392643298925412, 0.8280437493175503, 0.6735479388954753, 0.8581403606672701, 0.9878713443848272, 0.2713403283244138, 0.7448409519239707, 0.8310749073517066, 0.8540909353791436, 0.6611030841440269, 0.8815635460695272, 0.2509723631875628, 0.490099317758371]}}
{"id": "f40388b5-c60d-45dd-a4b4-4e6c79387be6", "fitness": -Infinity, "name": "EnhancedAdaptiveDE", "description": "Enhanced Adaptive Differential Evolution with orthogonal design-based mutation, improved archive handling, and adaptive parameter tuning using success history and Cauchy distributions.", "code": "import numpy as np\nfrom scipy.stats import cauchy\n\nclass EnhancedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=50, F=0.7, CR=0.5, adapt_freq=10, orthogonal_design_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.archive = []\n        self.archive_fitness = []\n        self.adapt_freq = adapt_freq\n        self.success_F = []\n        self.success_CR = []\n        self.orthogonal_design_size = orthogonal_design_size\n\n    def generate_orthogonal_design(self):\n        # A simple orthogonal design for demonstration.  For higher dimensions, use a library like pyDOE.\n        design = np.array([\n            [-1, -1],\n            [1, -1],\n            [-1, 1],\n            [1, 1],\n            [0,0] # Add a center point\n        ])\n        # Map -1 to 0, 1 to 1 to be used as indices\n        design = (design + 1) / 2\n        return design[:self.orthogonal_design_size]\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Mutation - Blend exploitation and exploration with orthogonal design\n                best_idx = np.argmin(self.fitness)\n                x_best = self.population[best_idx]\n\n                if len(self.archive) > 0 and np.random.rand() < 0.7:  # Prefer archive\n                    xa_idx = np.random.choice(range(len(self.archive)))\n                    xa = self.archive[xa_idx]\n                    mutant = self.population[i] + self.F * (x_best - self.population[i]) + self.F * (xa - self.population[i])\n                else:\n                    # Orthogonal design-based mutation\n                    design = self.generate_orthogonal_design()\n                    indices = np.random.choice(range(self.pop_size), self.orthogonal_design_size, replace=False)\n                    mutant = np.zeros(self.dim)\n                    for j in range(self.orthogonal_design_size):\n                        mutant += design[j,0] * self.population[indices[j]]  # Use only first column for now\n\n                    mutant = self.population[i] + self.F * (mutant - self.population[i]) #Scale the effect\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    self.success_F.append(self.F)\n                    self.success_CR.append(self.CR)\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n                    \n                    # Update archive - Use a better archive update strategy\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        # Replace worst in archive only if new solution is better than worst\n                        worst_archive_fitness_idx = np.argmax(self.archive_fitness)\n                        if f_trial < self.archive_fitness[worst_archive_fitness_idx]:\n                            self.archive[worst_archive_fitness_idx] = trial\n                            self.archive_fitness[worst_archive_fitness_idx] = f_trial\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n\n            #Adapt F and CR (optional) - Improved adaptation using Cauchy distribution\n            if generation % self.adapt_freq == 0:\n                if self.success_F:\n                    # Use Cauchy distribution for generating new F and CR\n                    self.F = cauchy.rvs(loc=np.mean(self.success_F), scale=0.1)\n                    self.CR = cauchy.rvs(loc=np.mean(self.success_CR), scale=0.1)\n                self.F = np.clip(self.F, 0.1, 1.0)\n                self.CR = np.clip(self.CR, 0.1, 1.0)\n                self.success_F = []\n                self.success_CR = []\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "An exception occurred: name 'cauchy' is not defined.", "error": "", "parent_ids": ["80002dc8-9d65-4ae6-a6cf-c590c2bc51a2"], "operator": null, "metadata": {}}
{"id": "4a4fad6b-1faf-4351-9605-faf215345e43", "fitness": 0.5228052626314257, "name": "CMAES", "description": "Covariance matrix adaptation evolution strategy (CMA-ES) with population size adaptation and restarts.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, sigma=0.5, cs=0.3, damps=1.0, c_cov_mu=0.1, c_cov_one=0.1):\n        \"\"\"\n        Initialize the CMA-ES algorithm.\n\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size (int): The population size. If None, it's set to 4 + int(3 * np.log(dim)).\n            sigma (float): The initial step size.\n            cs (float): Cumulation factor for step-size evolution.\n            damps (float): Damping for step-size.\n            c_cov_mu (float): Learning rate for rank-mu update of covariance matrix.\n            c_cov_one (float): Learning rate for rank-one update of covariance matrix.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.mu = self.pop_size // 2\n        self.sigma = sigma\n        self.cs = cs\n        self.damps = damps\n        self.c_cov_mu = c_cov_mu\n        self.c_cov_one = c_cov_one\n        self.eval_count = 0\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.mean = None\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.weights = None\n        self.restart_trigger = False\n        self.restart_iter = 0\n        self.max_restart_iter = 100\n\n    def initialize(self, func):\n        \"\"\"Initialize the CMA-ES parameters.\"\"\"\n        self.mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.damps = 1 + self.damps * (self.mu / self.dim)\n        self.damps = max(self.damps, 1.0)\n        self.chiN = self.dim**0.5 * (1 - (1/(4*self.dim)) + 1/(21*self.dim**2))\n\n\n    def sample_population(self):\n        \"\"\"Sample a population from the multivariate normal distribution.\"\"\"\n        z = np.random.randn(self.dim, self.pop_size)\n        x = self.mean[:, np.newaxis] + self.sigma * np.dot(np.linalg.cholesky(self.C), z)\n        return x.T\n\n    def update_parameters(self, x, fitness):\n        \"\"\"Update the CMA-ES parameters based on the fitness of the population.\"\"\"\n        idx = np.argsort(fitness)\n        x_mu = x[idx[:self.mu]]\n        z = np.linalg.solve(np.linalg.cholesky(self.C), (x_mu - self.mean).T / self.sigma)\n        \n        # Cumulation for covariance matrix\n        self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs)) * np.dot(z, self.weights)\n        self.pc = (1 - self.cs) * self.pc + np.sqrt(self.cs * (2 - self.cs)) * (x_mu[0] - self.mean) / self.sigma\n\n\n        # Covariance matrix adaptation\n        self.C = (1 - self.c_cov_one - self.c_cov_mu) * self.C + self.c_cov_one * np.outer(self.ps, self.ps)\n        \n        for k in range(self.mu):\n            self.C += self.c_cov_mu * self.weights[k] * np.outer(z[:, k], z[:, k])\n\n        # Adaptation of step size\n        self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n        self.mean = np.dot(x_mu.T, self.weights)\n\n    def check_restart(self):\n        \"\"\"Check if a restart is needed based on covariance matrix properties.\"\"\"\n        condition_number = np.max(np.linalg.eigvals(self.C)) / np.min(np.linalg.eigvals(self.C))\n\n        if condition_number > 1e14 or np.isnan(condition_number) or np.isinf(condition_number):\n            self.restart_trigger = True\n            return True\n        \n        if self.restart_iter > self.max_restart_iter:\n            self.restart_trigger = True\n            return True\n        \n        return False\n\n    def __call__(self, func):\n        \"\"\"Optimize the given function using CMA-ES.\"\"\"\n        self.initialize(func)\n        \n        while self.eval_count < self.budget:\n            x = self.sample_population()\n            \n            # Clip x to boundaries\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            \n            fitness = np.array([func(xi) for xi in x])\n            self.eval_count += self.pop_size\n\n            if np.min(fitness) < self.f_opt:\n                self.f_opt = np.min(fitness)\n                self.x_opt = x[np.argmin(fitness)]\n            \n            self.update_parameters(x, fitness)\n            \n            if self.check_restart():\n                self.initialize(func)\n                self.restart_iter += 1\n                self.restart_trigger = False\n            else:\n                self.restart_iter = 0\n\n            if self.eval_count >= self.budget:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm CMAES scored 0.523 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["06932a9c-3902-4608-8134-cd84203034b2"], "operator": null, "metadata": {"aucs": [0.08279590299949391, 0.19347416506773418, 0.9275256336792188, 0.19465750147294347, 0.9454732476198133, 0.9610377765075584, 0.2974681214566799, 0.939320562316133, 0.9484953800449236, 0.17775658263842198, 0.9343716020669092, 0.20408155699786135, 0.25642909550546267, 0.41439306264505094, 0.9340535727092073, 0.31602489396649336, 0.42944063715716374, 0.9663284649870355, 0.15493110563434564, 0.1780463871560627]}}
{"id": "1df5eb5d-805c-45b0-b130-9bccb449facd", "fitness": -Infinity, "name": "MultiStrategyDE", "description": "Differential Evolution with a multi-strategy mutation and probabilistic parameter adaptation guided by the success rate of each strategy.", "code": "import numpy as np\n\nclass MultiStrategyDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=50, F=0.7, CR=0.5, strategy_probabilities=[0.3, 0.3, 0.4]):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.CR = CR\n        self.archive = []\n        self.strategy_probabilities = strategy_probabilities  # Probabilities for each mutation strategy\n        self.success_counts = np.zeros(3)  # Success counter for each strategy\n        self.strategy_usage = np.zeros(3) # Usage counter for each strategy\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Strategy Selection\n                strategy_index = np.random.choice(len(self.strategy_probabilities), p=self.strategy_probabilities)\n                self.strategy_usage[strategy_index] += 1\n\n                # Mutation Strategies\n                if strategy_index == 0:\n                    # DE/rand/1\n                    idx = np.random.choice(range(self.pop_size), 3, replace=False)\n                    x_rand1, x_rand2, x_rand3 = self.population[idx[0]], self.population[idx[1]], self.population[idx[2]]\n                    mutant = x_rand1 + self.F * (x_rand2 - x_rand3)\n                elif strategy_index == 1:\n                    # DE/current-to-best/1\n                    best_idx = np.argmin(self.fitness)\n                    x_best = self.population[best_idx]\n                    idx = np.random.choice(range(self.pop_size), 2, replace=False)\n                    x_rand1, x_rand2 = self.population[idx[0]], self.population[idx[1]]\n                    mutant = self.population[i] + self.F * (x_best - self.population[i]) + self.F * (x_rand1 - x_rand2)\n                else:\n                    # DE/rand/2\n                    idx = np.random.choice(range(self.pop_size), 5, replace=False)\n                    x_rand1, x_rand2, x_rand3, x_rand4, x_rand5 = self.population[idx[0]], self.population[idx[1]], self.population[idx[2]], self.population[idx[3]], self.population[idx[4]]\n                    mutant = x_rand1 + self.F * (x_rand2 - x_rand3) + self.F * (x_rand4 - x_rand5)\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n                    self.success_counts[strategy_index] += 1\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n\n            # Adapt strategy probabilities based on success\n            for k in range(len(self.strategy_probabilities)):\n                if self.strategy_usage[k] > 0:\n                    success_rate = self.success_counts[k] / self.strategy_usage[k]\n                    self.strategy_probabilities[k] *= (1 + 0.1 * (success_rate - 0.5))  # Adjust probability\n                    self.strategy_probabilities[k] = max(0.05, min(0.95, self.strategy_probabilities[k])) # clip probabilities\n                \n            self.strategy_probabilities /= np.sum(self.strategy_probabilities)  # Normalize probabilities\n\n            # Reset success counts and usage\n            self.success_counts = np.zeros(3)\n            self.strategy_usage = np.zeros(3)\n            \n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "An exception occurred: probabilities do not sum to 1.", "error": "", "parent_ids": ["870da4c8-2145-4234-b7a9-147bc187ca24"], "operator": null, "metadata": {}}
{"id": "5289f9a4-3edc-4e4b-98f0-428e72ad9d9d", "fitness": 0.6561661387718122, "name": "AdaptiveDE", "description": "Simplified Adaptive Differential Evolution with improved parameter adaptation, reduced archive interaction, and vectorized operations for faster convergence.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size_factor=5, F=0.5, CR=0.7, lr_F=0.1, lr_CR=0.1, archive_prob=0.1):\n        \"\"\"\n        Initialize the Adaptive Differential Evolution algorithm.\n\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size_factor (int): Factor to determine the population size (pop_size = pop_size_factor * dim).\n            F (float): The initial scaling factor.\n            CR (float): The initial crossover rate.\n            lr_F (float): Learning rate for adapting F.\n            lr_CR (float): Learning rate for adapting CR.\n            archive_prob (float): Probability of using an archive member for mutation.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = int(pop_size_factor * dim)\n        self.F = F\n        self.CR = CR\n        self.lr_F = lr_F\n        self.lr_CR = lr_CR\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.archive = []\n        self.archive_size = self.pop_size // 2\n        self.archive_prob = archive_prob\n\n    def initialize_population(self, func):\n        \"\"\"Initialize the population randomly within the bounds.\"\"\"\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.pop[np.argmin(self.fitness)]\n\n    def update_archive(self, x):\n        \"\"\"Update the archive with a new solution.\"\"\"\n        if len(self.archive) < self.archive_size:\n            self.archive.append(x)\n        else:\n            idx = np.random.randint(0, self.archive_size)\n            self.archive[idx] = x\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using Adaptive Differential Evolution.\n\n        Args:\n            func (callable): The function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            # Adaptive parameters: Simplified adaptation\n            self.F = np.clip(self.F + self.lr_F * np.random.normal(0, 1), 0.1, 1.0)\n            self.CR = np.clip(self.CR + self.lr_CR * np.random.normal(0, 1), 0.1, 1.0)\n\n            # Mutation\n            idxs = np.random.randint(0, self.pop_size, size=(self.pop_size, 3))\n            a, b, c = idxs[:, 0], idxs[:, 1], idxs[:, 2]\n            mutant = self.pop[a] + self.F * (self.pop[b] - self.pop[c])\n\n            # Archive interaction\n            if len(self.archive) > 0 and np.random.rand() < self.archive_prob:\n                xa = np.random.randint(0, len(self.archive), size=self.pop_size)\n                arch_member = np.array([self.archive[i] for i in xa])\n                mutant = np.where(np.random.rand(self.pop_size, self.dim) < 0.5, mutant, arch_member + self.F * (mutant - self.pop[a]))\n\n            mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            cross_points = np.random.rand(self.pop_size, self.dim) < self.CR\n            trial = np.where(cross_points, mutant, self.pop)\n\n            # Selection\n            fitness_trial = np.array([func(x) for x in trial])\n            self.eval_count += self.pop_size\n\n            improved = fitness_trial < self.fitness\n            self.fitness[improved] = fitness_trial[improved]\n            self.pop[improved] = trial[improved]\n\n            # Update archive\n            for x in self.pop[improved]:\n                self.update_archive(x.copy())\n                \n            #Global best update\n            best_idx = np.argmin(self.fitness)\n            if self.fitness[best_idx] < self.f_opt:\n                self.f_opt = self.fitness[best_idx]\n                self.x_opt = self.pop[best_idx]\n            \n            if self.eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDE scored 0.656 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a2230d5f-2ba9-4fd6-88f0-6c3aa4f9e57a"], "operator": null, "metadata": {"aucs": [0.1985883494598356, 0.3144513626843636, 0.3679747368784553, 0.8920685752509829, 0.7673546551486531, 0.7751444516960411, 0.6803936135230915, 0.6082005705925186, 0.751524442092401, 0.7725383103180943, 0.8605269988857434, 0.9986024841016332, 0.43646827595213356, 0.7927594771591366, 0.8995812573333566, 0.7882386958447292, 0.5004405856768025, 0.6470272045159369, 0.5450496775882891, 0.5263890507340463]}}
{"id": "8d2672f7-8798-4b63-9624-f6152046cf06", "fitness": 0.5549579282359531, "name": "AdaptiveDE", "description": "Simplified Adaptive Differential Evolution with combined archive and best-guided mutation, and simplified adaptation based on success history.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=50, F=0.7, CR=0.5, adapt_freq=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.archive = []\n        self.adapt_freq = adapt_freq\n        self.success_F = []\n        self.success_CR = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Mutation - Blend exploitation and exploration using archive and best\n                best_idx = np.argmin(self.fitness)\n                x_best = self.population[best_idx]\n\n                # Combine archive and random member for mutation\n                if self.archive:\n                    archive_idx = np.random.randint(0, len(self.archive))\n                    mutant = self.population[i] + self.F * (x_best - self.population[i]) + self.F * (self.archive[archive_idx] - self.population[i])\n                else:\n                    idx = np.random.choice(range(self.pop_size), 2, replace=False)\n                    x_rand1, x_rand2 = self.population[idx[0]], self.population[idx[1]]\n                    mutant = self.population[i] + self.F * (x_rand1 - x_rand2)\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n                    self.success_F.append(self.F)\n                    self.success_CR.append(self.CR)\n\n                    # Update archive - always add improving solutions\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                    else:\n                        # Replace a random archive member\n                        self.archive[np.random.randint(0, self.archive_size)] = trial\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n\n            #Adapt F and CR (optional) - Simplified adaptation\n            if generation % self.adapt_freq == 0:\n                if self.success_F:\n                    self.F = np.mean(self.success_F)\n                    self.CR = np.mean(self.success_CR)\n                    self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n                    self.CR = np.clip(np.random.normal(self.CR, 0.1), 0.1, 1.0)\n                else:\n                    self.F = np.clip(np.random.normal(0.7, 0.1), 0.1, 1.0) # Back to defaults if no success\n                    self.CR = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n\n                self.success_F = []\n                self.success_CR = []\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDE scored 0.555 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["1c0e22b0-c40e-47a0-aa18-48091b5bdcaf"], "operator": null, "metadata": {"aucs": [0.11965927471394389, 0.22429256991747992, 0.5393281924849, 0.7580527473911404, 0.448625728764259, 0.855943686891079, 0.3422116080806723, 0.426745616581264, 0.5309915326421775, 0.5744869095663974, 0.8679397913598123, 0.9990164005631432, 0.2910276391146821, 0.48294260265219147, 0.7931286773817966, 0.8411845117011144, 0.43730079820722345, 0.8363077525864513, 0.20806532328000793, 0.5219072008393268]}}
{"id": "2bfc844b-330c-4a03-9044-54c9c7714210", "fitness": 0.7611949348567842, "name": "AdaptiveDE", "description": "Simplified Adaptive Differential Evolution with paired mutation and dynamic F/CR adaptation using population-level statistics, enhancing both convergence and exploration.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size_factor=5, F=0.5, CR=0.7):\n        \"\"\"\n        Initialize the Adaptive Differential Evolution algorithm.\n\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size_factor (int): Factor to determine the population size (pop_size = pop_size_factor * dim).\n            F (float): The initial scaling factor.\n            CR (float): The initial crossover rate.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = int(pop_size_factor * dim)\n        self.F = F\n        self.CR = CR\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n\n    def initialize_population(self, func):\n        \"\"\"Initialize the population randomly within the bounds.\"\"\"\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.pop[np.argmin(self.fitness)]\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using Adaptive Differential Evolution.\n\n        Args:\n            func (callable): The function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            # Adaptive F and CR\n            F = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n            CR = np.clip(np.random.normal(self.CR, 0.1), 0.1, 1.0)\n\n            for i in range(self.pop_size):\n                # Mutation (DE/rand/1)\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                a, b, c = idxs[0], idxs[1], idxs[2]\n                mutant = self.pop[a] + F * (self.pop[b] - self.pop[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Selection\n                f = func(trial)\n                self.eval_count += 1\n\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.pop[i] = trial\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n\n                if self.eval_count >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDE scored 0.761 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a2230d5f-2ba9-4fd6-88f0-6c3aa4f9e57a"], "operator": null, "metadata": {"aucs": [0.2764078106654473, 0.7962218305578039, 0.7284449366375623, 0.9135917808993749, 0.8516876978157658, 0.8769514173602535, 0.7433362636919636, 0.755515417724272, 0.8350968474187358, 0.7749647270216897, 0.892165166707541, 0.997803379893753, 0.30446923762972666, 0.821011198746229, 0.9381609326097317, 0.8740903004805377, 0.7269796867116871, 0.9026721534760505, 0.7070521962910558, 0.5072757147965041]}}
{"id": "ab66b1ff-c3f0-479f-86d5-e41d88faf16a", "fitness": 0.6157374806450389, "name": "AdaptiveDE", "description": "Enhanced Adaptive Differential Evolution with jitter-based exploration and adaptive population reduction based on stagnation detection.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=50, F=0.7, CR=0.5, adapt_freq=10, stagnation_threshold=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.archive = []\n        self.adapt_freq = adapt_freq\n        self.success_F = []\n        self.success_CR = []\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.best_fitness_history.append(self.f_opt)\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Mutation - Blend exploitation and exploration using archive and best with jitter\n                best_idx = np.argmin(self.fitness)\n                x_best = self.population[best_idx]\n\n                if self.archive:\n                    xa = self.archive[np.random.choice(len(self.archive))]\n                    # Jitter: Add small random variation to F\n                    F_jitter = self.F * (0.9 + 0.2 * np.random.rand())\n                    mutant = self.population[i] + F_jitter * (x_best - self.population[i]) + F_jitter * (xa - self.population[i])\n                else:\n                    idx = np.random.choice(range(self.pop_size), 2, replace=False)\n                    x_rand1, x_rand2 = self.population[idx[0]], self.population[idx[1]]\n                    # Jitter: Add small random variation to F\n                    F_jitter = self.F * (0.9 + 0.2 * np.random.rand())\n                    mutant = self.population[i] + F_jitter * (x_rand1 - x_rand2)\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    self.success_F.append(self.F)\n                    self.success_CR.append(self.CR)\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n                    \n                    # Update archive - always add improving solutions\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                    else:\n                        # Replace a random archive member\n                        self.archive[np.random.randint(0, self.archive_size)] = trial\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n\n            #Adapt F and CR (optional) - Simplified adaptation\n            if generation % self.adapt_freq == 0:\n                if self.success_F:\n                    self.F = np.mean(self.success_F)\n                    self.CR = np.mean(self.success_CR)\n                self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n                self.CR = np.clip(np.random.normal(self.CR, 0.1), 0.1, 1.0)\n                self.success_F = []\n                self.success_CR = []\n\n            # Stagnation Check and Population Reduction\n            if generation > 1:\n                if self.f_opt >= self.best_fitness_history[-1]:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0\n\n            if self.stagnation_counter > self.stagnation_threshold and self.pop_size > 10:\n                self.pop_size = max(10, int(self.pop_size * 0.8))  # Reduce population size\n                self.population = self.population[np.argsort(self.fitness)][:self.pop_size]  # Keep best individuals\n                self.fitness = self.fitness[np.argsort(self.fitness)][:self.pop_size]\n                self.stagnation_counter = 0  # Reset stagnation counter\n                print(f\"Population reduced to {self.pop_size}\")\n            \n            self.best_fitness_history.append(self.f_opt)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDE scored 0.616 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["1c0e22b0-c40e-47a0-aa18-48091b5bdcaf"], "operator": null, "metadata": {"aucs": [0.24096627651308244, 0.5134712439563721, 0.47241299423783434, 0.8408434983273995, 0.3680634434775417, 0.7763510878280618, 0.61667323577945, 0.5015179436433854, 0.8312387293649837, 0.6851081535991169, 0.8679205794451614, 0.9987410924230355, 0.25952530485497005, 0.7015694666182404, 0.8893574071134444, 0.7819266471892906, 0.39991479363575855, 0.8460439194656629, 0.21725816301214695, 0.5058456324158355]}}
{"id": "ae6a515c-dc09-407e-9a1a-11cf3d26c06b", "fitness": 0.6671190022887732, "name": "HybridDE", "description": "Hybrid Differential Evolution with dynamic population size reduction, self-adaptive parameters, orthogonal learning, and ring topology, focusing on intensifying the search near promising regions.", "code": "import numpy as np\n\nclass HybridDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, adapt_freq=10, ring_neighbors=3, reduction_factor=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.adapt_freq = adapt_freq\n        self.ring_neighbors = ring_neighbors\n        self.reduction_factor = reduction_factor #Population size reduction factor\n        self.min_pop_size = 10\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Ring Topology Selection\n                neighbors = [(i - j) % self.pop_size for j in range(1, self.ring_neighbors + 1)]\n                neighbor_fitnesses = self.fitness[neighbors]\n                best_neighbor_idx = neighbors[np.argmin(neighbor_fitnesses)]\n                \n                # Mutation - Use best neighbor in ring\n                idx = np.random.choice(range(self.pop_size), 2, replace=False)\n                x_rand1, x_rand2 = self.population[idx[0]], self.population[idx[1]]\n                mutant = self.population[i] + self.F * (self.population[best_neighbor_idx] - self.population[i]) + self.F * (x_rand1 - x_rand2)\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Orthogonal Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n\n            #Adapt F and CR\n            if generation % self.adapt_freq == 0:\n                self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n                self.CR = np.clip(np.random.normal(self.CR, 0.1), 0.1, 1.0)\n            \n            #Population size reduction\n            if generation % (self.adapt_freq*5) == 0 and self.pop_size > self.min_pop_size:\n                num_to_reduce = max(1, int(self.pop_size * (1 - self.reduction_factor)))\n                \n                #Find the worst individuals and remove them\n                worst_indices = np.argsort(self.fitness)[-num_to_reduce:]\n                \n                remaining_indices = np.array([i for i in range(self.pop_size) if i not in worst_indices])\n                \n                self.population = self.population[remaining_indices]\n                self.fitness = self.fitness[remaining_indices]\n                self.pop_size = len(self.population)\n                \n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm HybridDE scored 0.667 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["df265f0f-5076-48a4-99db-ab90bc5213d5"], "operator": null, "metadata": {"aucs": [0.21298758885872981, 0.7425324244467641, 0.49260415347591713, 0.9110234820460732, 0.8167870457116677, 0.8454699942380663, 0.5683376711595933, 0.7961749031501911, 0.8074218843072923, 0.22752970639209857, 0.9344935687975034, 0.9938141238698124, 0.32017720105334213, 0.6052812984221203, 0.8830986835624145, 0.8390063417611612, 0.7101440458093615, 0.8875500228303774, 0.22297191900607938, 0.5249739868768974]}}
{"id": "fcfb7300-3029-4e4d-b443-03c48d251d20", "fitness": 0.5539892749839452, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with orthogonal array-based crossover and enhanced parameter adaptation using a mirrored archive.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=50, F=0.7, CR=0.5, adapt_freq=10, oa_design=None):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.archive = []\n        self.mirrored_archive = []\n        self.adapt_freq = adapt_freq\n        self.success_F = []\n        self.success_CR = []\n        self.oa_design = self._generate_orthogonal_array(dim) if oa_design is None else oa_design # Ensure the design is always generated\n\n    def _generate_orthogonal_array(self, dim):\n        # A simplified approach to create a design with some basic coverage. More sophisticated approaches may be needed\n        # for higher dimensional spaces. This ensures that a default array is available if none is provided.\n        if dim <= 2:\n            return np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n        \n        # Create a minimal covering array for larger dimensions\n        design = np.random.randint(0, 2, size=(2 * dim, dim))  # Ensure it has enough rows to be useful\n        return design\n        \n\n    def orthogonal_crossover(self, mutant, current):\n        trial = np.copy(current)\n        \n        for i in range(self.dim):\n            if self.oa_design is not None and i < self.oa_design.shape[1]:\n                level = self.oa_design[np.random.randint(0, self.oa_design.shape[0]), i] # Select a random row\n\n                if level == 1:\n                    trial[i] = mutant[i]\n            else:\n                if np.random.rand() < self.CR:\n                   trial[i] = mutant[i]\n\n        return trial\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Mutation - Blend exploitation and exploration using archive and best\n                best_idx = np.argmin(self.fitness)\n                x_best = self.population[best_idx]\n\n                if self.archive:\n                    xa = self.archive[np.random.choice(len(self.archive))]\n                    mutant = self.population[i] + self.F * (x_best - self.population[i]) + self.F * (xa - self.population[i])\n                else:\n                    idx = np.random.choice(range(self.pop_size), 2, replace=False)\n                    x_rand1, x_rand2 = self.population[idx[0]], self.population[idx[1]]\n                    mutant = self.population[i] + self.F * (x_rand1 - x_rand2)\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover using Orthogonal Array\n                trial = self.orthogonal_crossover(mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    self.success_F.append(self.F)\n                    self.success_CR.append(self.CR)\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n                    \n                    # Update archive - always add improving solutions\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                        self.mirrored_archive.append(func.bounds.ub - trial + func.bounds.lb)  # Create and store mirrored solution\n                    else:\n                        # Replace a random archive member\n                        replace_idx = np.random.randint(0, self.archive_size)\n                        self.archive[replace_idx] = trial\n                        self.mirrored_archive[replace_idx] = func.bounds.ub - trial + func.bounds.lb # Maintain mirrored solutions\n\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n\n            #Adapt F and CR (optional) - Simplified adaptation\n            if generation % self.adapt_freq == 0:\n                if self.success_F:\n                    self.F = np.mean(self.success_F)\n                    self.CR = np.mean(self.success_CR)\n                self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n                self.CR = np.clip(np.random.normal(self.CR, 0.1), 0.1, 1.0)\n                self.success_F = []\n                self.success_CR = []\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDE scored 0.554 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["1c0e22b0-c40e-47a0-aa18-48091b5bdcaf"], "operator": null, "metadata": {"aucs": [0.23535708701826386, 0.34971612892701387, 0.6274062175066849, 0.7808719865919782, 0.7227044101335098, 0.7712258678807108, 0.48019531131188353, 0.44681755522257205, 0.34042753117602464, 0.4825136790894946, 0.7585170559581257, 0.9983530097084246, 0.2868759370316164, 0.5155541660084941, 0.7205480044226142, 0.3556901202109882, 0.3988736528910303, 0.5340246412468765, 0.7702706522975069, 0.5038424850450931]}}
{"id": "7325c393-fa3b-4716-8ca9-9f05b8f2d851", "fitness": 0.5827289018777979, "name": "AdaptiveDE", "description": "Simplified Adaptive Differential Evolution with a dynamic F and CR based on the median of successful parameters, coupled with a small archive and adaptive population size.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, F=0.5, CR=0.5, history_size=5, decay_rate=0.99):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.CR = CR\n        self.archive = []\n        self.success_history_F = []\n        self.success_history_CR = []\n        self.history_size = history_size\n        self.decay_rate = decay_rate\n        self.population_size_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        while self.budget > 0:\n            # Adaptive Population Size\n            if len(self.population_size_history) > 5:\n                recent_sizes = self.population_size_history[-5:]\n                if np.std(recent_sizes) < 3 and np.mean(recent_sizes) > 10:  # Stabilized population size\n                    self.pop_size = int(max(10, self.pop_size * 0.98)) # Reduce population size\n                elif np.std(recent_sizes) < 3 and np.mean(recent_sizes) < 10:\n                     self.pop_size = int(min(50, self.pop_size * 1.02)) # Increase population size\n            self.population_size_history.append(self.pop_size)\n            \n            for i in range(self.pop_size):\n                # Mutation\n                idx = np.random.choice(range(self.pop_size), 3, replace=False)\n                x_rand1, x_rand2, x_rand3 = self.population[idx[0]], self.population[idx[1]], self.population[idx[2]]\n                if len(self.archive) > 0 and np.random.rand() < 0.1:\n                    xa = self.archive[np.random.choice(len(self.archive))]\n                    mutant = self.population[i] + self.F * (xa - self.population[i]) + self.F * (x_rand1 - x_rand2)\n                else:\n                    mutant = self.population[i] + self.F * (x_rand1 - x_rand2)\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n                    self.success_history_F.append(self.F)\n                    self.success_history_CR.append(self.CR)\n\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                    else:\n                        if f_trial < self.fitness[np.argmax(self.fitness)] :\n                            self.archive[np.random.randint(0,len(self.archive))] = trial\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n\n            # Adapt F and CR using quantiles\n            if len(self.success_history_F) > 0:\n                self.F = np.median(self.success_history_F)\n                self.CR = np.median(self.success_history_CR)\n            else:\n                self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n                self.CR = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n\n            self.success_history_F = []\n            self.success_history_CR = []\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDE scored 0.583 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["870da4c8-2145-4234-b7a9-147bc187ca24"], "operator": null, "metadata": {"aucs": [0.1811037591857867, 0.3039903683894155, 0.727124128196465, 0.7851503906176156, 0.6961876587898488, 0.8228919326320754, 0.35522578431656004, 0.583230996656722, 0.7285940507730746, 0.20762330842856014, 0.789854355200564, 0.9899881890698171, 0.33014603756023486, 0.4821225289356489, 0.8816988170850093, 0.7629617330503289, 0.4883934107965824, 0.814744430279503, 0.22017253923509095, 0.5033736183570566]}}
{"id": "03e40c37-0042-4477-81eb-f9abe9c311c9", "fitness": 0.7187410792562459, "name": "HybridDE", "description": "Enhanced Hybrid Differential Evolution with dynamic F/CR adaptation based on fitness improvement, and a tournament-based selection of neighbors in the ring topology.", "code": "import numpy as np\n\nclass HybridDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, adapt_freq=10, ring_neighbors=3, tourn_size=3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.adapt_freq = adapt_freq\n        self.ring_neighbors = ring_neighbors\n        self.tourn_size = tourn_size\n\n        self.archive = [] # Archive for storing solutions that are replaced\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        generation = 0\n        success_F = []\n        success_CR = []\n\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Tournament Selection of Neighbors\n                neighbor_indices = [(i - j) % self.pop_size for j in range(1, self.ring_neighbors + 1)]\n                tournament_candidates = np.random.choice(neighbor_indices, size=min(self.tourn_size, len(neighbor_indices)), replace=False)\n                best_neighbor_idx = tournament_candidates[np.argmin(self.fitness[tournament_candidates])]\n\n\n                # Mutation - Use best neighbor in ring\n                idx = np.random.choice(range(self.pop_size), 2, replace=False)\n                x_rand1, x_rand2 = self.population[idx[0]], self.population[idx[1]]\n                mutant = self.population[i] + self.F * (self.population[best_neighbor_idx] - self.population[i]) + self.F * (x_rand1 - x_rand2)\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Orthogonal Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    success_F.append(self.F)\n                    success_CR.append(self.CR)\n\n                    # Archive replaced solution\n                    self.archive.append(self.population[i].copy())\n                    \n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n\n                if self.budget <= 0:\n                    break\n\n            #Adapt F and CR using successful values\n            if generation % self.adapt_freq == 0:\n                if success_F:\n                    self.F = np.clip(np.mean(success_F), 0.1, 1.0)\n                    self.CR = np.clip(np.mean(success_CR), 0.1, 1.0)\n                else:\n                    # If no successful F/CR, perturb them randomly\n                    self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n                    self.CR = np.clip(np.random.normal(self.CR, 0.1), 0.1, 1.0)\n                success_F = []\n                success_CR = []\n\n            # Archive management (optional - keep archive size limited)\n            if len(self.archive) > self.pop_size * 2:\n                self.archive = self.archive[-self.pop_size*2:]\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm HybridDE scored 0.719 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["df265f0f-5076-48a4-99db-ab90bc5213d5"], "operator": null, "metadata": {"aucs": [0.30004771986738266, 0.7582624157342409, 0.6999183760782369, 0.9062441330249801, 0.7907431868414359, 0.8566495575036992, 0.7108112209361404, 0.7821206612158843, 0.8485604198236367, 0.21191968588992038, 0.9151127123255282, 0.9931749785043502, 0.7539629450970418, 0.783560987500771, 0.9411831667959745, 0.8382378824013915, 0.6603385574007512, 0.9074778042838814, 0.21277111234904045, 0.5037240615506297]}}
{"id": "771fb1e6-1cbf-4fe3-9960-aacceb033762", "fitness": 0.37720585053285255, "name": "AdaptiveDE_CMA", "description": "Adaptive Differential Evolution with dynamic population size, orthogonal design for initialization, and covariance matrix adaptation for mutation.", "code": "import numpy as np\n\nclass AdaptiveDE_CMA:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, min_pop_size=10, max_pop_size=100, archive_size=50, F=0.7, CR=0.5, adapt_freq=10):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = initial_pop_size\n        self.min_pop_size = min_pop_size\n        self.max_pop_size = max_pop_size\n        self.pop_size = initial_pop_size\n        self.archive_size = archive_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.archive = []\n        self.adapt_freq = adapt_freq\n        self.success_F = []\n        self.success_CR = []\n        self.mean = None\n        self.covariance = None\n\n    def orthogonal_design(self, func, num_points):\n        # Generate an orthogonal array using Latin Hypercube Sampling\n        points = np.zeros((num_points, self.dim))\n        for i in range(self.dim):\n            points[:, i] = np.random.permutation(num_points)\n        points = (points + np.random.rand(num_points, self.dim)) / num_points\n        points = func.bounds.lb + points * (func.bounds.ub - func.bounds.lb)\n        return points\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n        # Orthogonal initialization\n        self.population = self.orthogonal_design(func, self.pop_size)\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.mean = self.x_opt  # Initialize mean for CMA\n        self.covariance = np.eye(self.dim)  # Initialize covariance matrix for CMA\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Mutation - Blend exploitation and exploration using archive and CMA\n                best_idx = np.argmin(self.fitness)\n                x_best = self.population[best_idx]\n                \n                # CMA-ES-like mutation\n                mutant = np.random.multivariate_normal(self.mean, self.covariance)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    self.success_F.append(self.F)\n                    self.success_CR.append(self.CR)\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n                    \n                    # Update archive - always add improving solutions\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                    else:\n                        # Replace a random archive member\n                        self.archive[np.random.randint(0, self.archive_size)] = trial\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n                    self.mean = trial  # Update CMA mean\n\n                if self.budget <= 0:\n                    break\n\n            #Adapt F and CR (optional) - Simplified adaptation\n            if generation % self.adapt_freq == 0:\n                if self.success_F:\n                    self.F = np.mean(self.success_F)\n                    self.CR = np.mean(self.success_CR)\n                self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n                self.CR = np.clip(np.random.normal(self.CR, 0.1), 0.1, 1.0)\n                self.success_F = []\n                self.success_CR = []\n\n                #Adapt population size\n                if len(self.archive) > self.archive_size * 0.75 and self.pop_size < self.max_pop_size:\n                    self.pop_size = min(self.pop_size + 5, self.max_pop_size)\n                    self.population = np.vstack((self.population, self.orthogonal_design(func, 5)))  #Add orthogonal points\n                    new_fitness = np.array([func(x) for x in self.population[-5:]])\n                    self.fitness = np.concatenate((self.fitness, new_fitness))\n                    self.budget -= 5\n                elif len(self.archive) < self.archive_size * 0.25 and self.pop_size > self.min_pop_size:\n                    self.pop_size = max(self.pop_size - 5, self.min_pop_size)\n                    self.population = self.population[:self.pop_size] #Reduce pop size\n                    self.fitness = self.fitness[:self.pop_size]\n                    \n                #Adapt covariance matrix\n                diff = self.population - self.mean\n                self.covariance = np.cov(diff.T)\n                if np.linalg.det(self.covariance) <= 0:\n                    self.covariance = np.eye(self.dim) #Reset if singular\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDE_CMA scored 0.377 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["1c0e22b0-c40e-47a0-aa18-48091b5bdcaf"], "operator": null, "metadata": {"aucs": [0.1861678581414954, 0.24844322640204486, 0.3748867568566544, 0.4104857698614681, 0.2930533851895166, 0.35837460341561533, 0.2886036503630235, 0.3140261504511326, 0.28047549628068047, 0.20564510043048434, 0.42577261622146234, 1.0, 0.32498735121202704, 0.30046100992149727, 0.7353181729011894, 0.3670920035425326, 0.3237614219885464, 0.40870166281240583, 0.2000372038670769, 0.4978235707981973]}}
{"id": "ea4b5074-a6d9-4429-8e70-792e8497cf84", "fitness": -Infinity, "name": "DynamicOrthogonalDE", "description": "Population-based DE with orthogonal learning and dynamic resource allocation based on population diversity, employing Cauchy mutation and Levy flight to enhance exploration and exploitation.", "code": "import numpy as np\n\nclass DynamicOrthogonalDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, ortho_size=10, F=0.5, CR=0.9, levy_exponent=1.5, cauchy_loc=0.0, cauchy_scale=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.ortho_size = ortho_size\n        self.F = F\n        self.CR = CR\n        self.levy_exponent = levy_exponent\n        self.cauchy_loc = cauchy_loc\n        self.cauchy_scale = cauchy_scale\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.diversity_history = []\n\n    def levy_flight(self, size):\n        \"\"\"Generates Levy distribution samples.\"\"\"\n        u = np.random.normal(0, 1, size=size)\n        v = np.random.normal(0, 1, size=size)\n        sigma = (np.math.gamma(1 + self.levy_exponent) * np.sin(np.pi * self.levy_exponent / 2) / (np.math.gamma((1 + self.levy_exponent) / 2) * self.levy_exponent * (2 ** ((self.levy_exponent - 1) / 2)))) ** (1 / self.levy_exponent)\n        step = sigma * u / abs(v)**(1/self.levy_exponent)\n        return step\n\n    def cauchy_mutation(self, size):\n        \"\"\"Generates Cauchy distribution samples.\"\"\"\n        return np.random.standard_cauchy(size=size) * self.cauchy_scale + self.cauchy_loc\n\n    def orthogonal_design(self):\n        \"\"\"Generates orthogonal design matrix.\"\"\"\n        H = np.array([[1, 1], [1, -1]])\n        while H.shape[0] < self.ortho_size:\n            H = np.kron(H, np.array([[1, 1], [1, -1]]))\n        return H[:self.ortho_size, :self.ortho_size]\n\n    def calculate_diversity(self):\n        \"\"\"Calculates population diversity using standard deviation.\"\"\"\n        return np.mean(np.std(self.population, axis=0))\n\n    def __call__(self, func):\n        # Initialization\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        \n        while self.budget > 0:\n            diversity = self.calculate_diversity()\n            self.diversity_history.append(diversity)\n\n            # Dynamic resource allocation: Adjust population size based on diversity\n            if len(self.diversity_history) > 5:\n                diversity_trend = np.mean(np.diff(self.diversity_history[-5:]))\n                if diversity_trend < 0:  # Diversity is decreasing, increase exploration\n                    exploration_rate = 0.7\n                    exploitation_rate = 0.3\n                else:  # Diversity is increasing or stable, focus on exploitation\n                    exploration_rate = 0.3\n                    exploitation_rate = 0.7\n            else:\n                exploration_rate = 0.5\n                exploitation_rate = 0.5\n\n            for i in range(self.pop_size):\n                # Exploration phase: Cauchy mutation and Levy flight\n                if np.random.rand() < exploration_rate:\n                    mutant = self.population[i] + self.cauchy_mutation(self.dim) + self.levy_flight(self.dim) * self.F\n                # Exploitation phase: Standard DE mutation with orthogonal learning\n                else:\n                    idxs = np.random.choice(self.pop_size, 3, replace=False)\n                    x_rand1, x_rand2, x_rand3 = self.population[idxs[0]], self.population[idxs[1]], self.population[idxs[2]]\n                    mutant = x_rand1 + self.F * (x_rand2 - x_rand3)\n\n                    # Orthogonal Learning\n                    orthogonal_matrix = self.orthogonal_design()\n                    best_idx = np.argmin(self.fitness)\n                    best_solution = self.population[best_idx]\n                    for j in range(self.ortho_size):\n                        trial_vector = best_solution + 0.1 * orthogonal_matrix[j] * (func.bounds.ub - func.bounds.lb)\n                        trial_vector = np.clip(trial_vector, func.bounds.lb, func.bounds.ub)\n\n                        f_trial = func(trial_vector)\n                        self.budget -= 1\n                        if f_trial < self.fitness[i]:\n                            mutant = trial_vector # use the result from orthogonal learning instead of the normal DE mutant\n                            break # only use one replacement at a time.\n                            \n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "An exception occurred: operands could not be broadcast together with shapes (10,) (2,) .", "error": "", "parent_ids": ["870da4c8-2145-4234-b7a9-147bc187ca24"], "operator": null, "metadata": {}}
{"id": "24a16bd5-41d7-4fbf-92aa-227b54898eeb", "fitness": 0.6221086945556494, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with simplified parameter adaptation, reduced archive usage, and more aggressive mutation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_rate=0.1, F=0.5, CR=0.9, adapt_freq=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_rate = archive_rate\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.archive = []\n        self.adapt_freq = adapt_freq\n        self.success_F = []\n        self.success_CR = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Mutation - Blend exploitation and exploration\n                best_idx = np.argmin(self.fitness)\n                x_best = self.population[best_idx]\n\n                # Aggressive mutation using multiple difference vectors\n                idx = np.random.choice(range(self.pop_size), 4, replace=False)\n                x_rand1, x_rand2, x_rand3, x_rand4 = self.population[idx[0]], self.population[idx[1]], self.population[idx[2]], self.population[idx[3]]\n                mutant = self.population[i] + self.F * (x_best - self.population[i]) + self.F * (x_rand1 - x_rand2) + self.F * (x_rand3 - x_rand4)\n\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    self.success_F.append(self.F)\n                    self.success_CR.append(self.CR)\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n                    \n                    # Update archive - probabilistic update\n                    if np.random.rand() < self.archive_rate:\n                        if len(self.archive) < self.pop_size:\n                            self.archive.append(trial)\n                        else:\n                            self.archive[np.random.randint(0, self.pop_size)] = trial\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n\n            #Adapt F and CR (optional) - Simplified adaptation\n            if generation % self.adapt_freq == 0:\n                if self.success_F:\n                    self.F = np.mean(self.success_F)\n                    self.CR = np.mean(self.success_CR)\n                self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n                self.CR = np.clip(np.random.normal(self.CR, 0.1), 0.1, 1.0)\n                self.success_F = []\n                self.success_CR = []\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDE scored 0.622 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["1c0e22b0-c40e-47a0-aa18-48091b5bdcaf"], "operator": null, "metadata": {"aucs": [0.215518203317288, 0.46930566517388217, 0.6716594935906901, 0.8963351125549465, 0.6450399496941852, 0.837438248544926, 0.35522497076011184, 0.6559984800415855, 0.7798160744031948, 0.38902281037749975, 0.8034153277818612, 0.9993236774345469, 0.5915583372470616, 0.572599108575223, 0.7760611109103848, 0.7970006542594311, 0.3979086527080279, 0.8643454351294292, 0.20369938284796585, 0.5209031957607484]}}
{"id": "7fd1a316-4865-4537-8091-ff0b2f22883a", "fitness": 0.0, "name": "HybridDE", "description": "Simplified Hybrid DE with adaptive parameters, ring topology, and periodic population size reduction focused on intensification.", "code": "import numpy as np\n\nclass HybridDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, adapt_freq=10, ring_neighbors=3, reduction_factor=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.adapt_freq = adapt_freq\n        self.ring_neighbors = ring_neighbors\n        self.reduction_factor = reduction_factor\n        self.min_pop_size = 10\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Ring Topology Selection\n                neighbors = [(i - j) % self.pop_size for j in range(1, self.ring_neighbors + 1)]\n                best_neighbor_idx = neighbors[np.argmin(self.fitness[neighbors])]\n                \n                # Mutation\n                idx = np.random.choice(self.pop_size, 2, replace=False)\n                mutant = self.population[i] + self.F * (self.population[best_neighbor_idx] - self.population[i]) + self.F * (self.population[idx[0]] - self.population[idx[1]])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                trial = np.where(cross_points, mutant, self.population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                \n\n            # Adapt F and CR\n            if generation % self.adapt_freq == 0:\n                self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0) #Simplified F adaptation\n                self.CR = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0) #Simplified CR adaptation\n            \n            # Population size reduction\n            if generation % (self.adapt_freq * 5) == 0 and self.pop_size > self.min_pop_size:\n                self.pop_size = max(self.min_pop_size, int(self.pop_size * self.reduction_factor))\n                \n                # Keep only the best individuals\n                best_indices = np.argsort(self.fitness)[:self.pop_size]\n                self.population = self.population[best_indices]\n                self.fitness = self.fitness[best_indices]\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm HybridDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ae6a515c-dc09-407e-9a1a-11cf3d26c06b"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "f08f2eab-c926-4e1b-ac99-1e3e8d3f450e", "fitness": 0.09449964585960409, "name": "LevyLocalSearch", "description": "Exploration-Exploitation Balance using Lévy flights for diversification and a local search with shrinking radius for intensification.", "code": "import numpy as np\n\nclass LevyLocalSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=20, levy_exponent=1.5, local_search_radius=0.1):\n        \"\"\"\n        Initialize the Levy Local Search algorithm.\n\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size (int): The population size.\n            levy_exponent (float): Exponent for Levy flight distribution.\n            local_search_radius (float): Initial radius for local search.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.levy_exponent = levy_exponent\n        self.local_search_radius = local_search_radius\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.radius_decay = 0.99\n\n    def initialize_population(self, func):\n        \"\"\"Initialize the population randomly within the bounds.\"\"\"\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.pop[np.argmin(self.fitness)]\n\n    def levy_flight(self, step_size=0.1):\n        \"\"\"Generate a Levy flight step.\"\"\"\n        sigma = (np.math.gamma(1 + self.levy_exponent) * np.sin(np.pi * self.levy_exponent / 2) /\n                 (np.math.gamma((1 + self.levy_exponent) / 2) * self.levy_exponent * (2 ** ((self.levy_exponent - 1) / 2)))) ** (1 / self.levy_exponent)\n        u = np.random.normal(0, sigma, size=self.dim)\n        v = np.random.normal(0, 1, size=self.dim)\n        step = u / (np.abs(v) ** (1 / self.levy_exponent))\n        return step_size * step\n\n    def local_search(self, func, x):\n        \"\"\"Perform local search around a solution.\"\"\"\n        best_x = x.copy()\n        best_f = func(x)\n        self.eval_count += 1\n\n        for _ in range(5):  # Limited local search iterations\n            new_x = x + np.random.uniform(-self.local_search_radius, self.local_search_radius, size=self.dim)\n            new_x = np.clip(new_x, func.bounds.lb, func.bounds.ub)\n            new_f = func(new_x)\n            self.eval_count += 1\n\n            if new_f < best_f:\n                best_f = new_f\n                best_x = new_x\n\n        return best_f, best_x\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using Levy Local Search.\n\n        Args:\n            func (callable): The function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Levy Flight for diversification\n                step = self.levy_flight()\n                new_x = self.pop[i] + step\n                new_x = np.clip(new_x, func.bounds.lb, func.bounds.ub)\n                new_f = func(new_x)\n                self.eval_count += 1\n                \n                if new_f < self.fitness[i]:\n                    self.fitness[i] = new_f\n                    self.pop[i] = new_x\n\n                    # Local Search around the improved solution\n                    local_f, local_x = self.local_search(func, self.pop[i])\n                    if local_f < self.fitness[i]:\n                         self.fitness[i] = local_f\n                         self.pop[i] = local_x\n\n                if self.fitness[i] < self.f_opt:\n                    self.f_opt = self.fitness[i]\n                    self.x_opt = self.pop[i]\n\n                if self.eval_count >= self.budget:\n                    break\n            self.local_search_radius *= self.radius_decay\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm LevyLocalSearch scored 0.094 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2bfc844b-330c-4a03-9044-54c9c7714210"], "operator": null, "metadata": {"aucs": [0.1000310085776569, 0.18346792900115538, 0]}}
{"id": "6235721a-426a-46dc-8e4c-9766b6a2e276", "fitness": -Infinity, "name": "SOMGuidedDE", "description": "Differential Evolution with a self-organizing map (SOM) to guide the search based on the fitness landscape, dynamically adjusting mutation and crossover parameters and diversifying the population using SOM-informed exploration.", "code": "import numpy as np\nfrom minisom import MiniSom\n\nclass SOMGuidedDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, som_grid_size=10, F=0.5, CR=0.9, adapt_freq=10, som_train_freq=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.som_grid_size = som_grid_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.adapt_freq = adapt_freq\n        self.som_train_freq = som_train_freq\n        self.som = None  # Initialize SOM to None\n        self.data_for_som = None\n        self.init_neighborhood_radius = None\n        self.time_constant = None\n\n        self.archive = [] # Archive for storing solutions that are replaced\n\n    def initialize_som(self):\n        self.som = MiniSom(self.som_grid_size, self.som_grid_size, self.dim, sigma=0.3, learning_rate=0.7)\n        self.som.random_weights_init(self.population)\n        self.init_neighborhood_radius = max(self.som_grid_size/2.0, 1) # Ensure radius isn't zero\n        self.time_constant = self.budget / np.log(self.init_neighborhood_radius) # Time constant for decay\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        generation = 0\n        success_F = []\n        success_CR = []\n\n        self.initialize_som() # SOM initialization after population is created\n\n        while self.budget > 0:\n            generation += 1\n\n            # SOM training\n            if generation % self.som_train_freq == 0:\n                # Use only successful solutions for training, and add some random solutions\n                # If the archive is empty, then we don't have anything to train on, so don't train\n                successful_solutions_indices = np.where(self.fitness < np.mean(self.fitness))[0]\n                successful_solutions = self.population[successful_solutions_indices]\n                random_indices = np.random.choice(self.pop_size, size=min(self.pop_size // 4, self.budget), replace=False)\n                random_solutions = self.population[random_indices]\n\n                training_data = np.concatenate([successful_solutions, random_solutions], axis=0) if len(successful_solutions) > 0 else random_solutions\n\n                if len(training_data) > 0:\n                    # decay the learning parameters\n                    current_sigma = self.init_neighborhood_radius * np.exp(-generation/self.time_constant)\n                    current_learning_rate = 0.7 * np.exp(-generation/self.time_constant)\n                    self.som._decay_function = self._gaussian\n                    self.som._neighborhood = 'gaussian'\n                    self.som.train_random(training_data, 100, verbose=False, sigma=current_sigma, learning_rate=current_learning_rate)\n\n\n            for i in range(self.pop_size):\n                # SOM-Guided Mutation\n                if self.som is not None:\n                    winner = self.som.winner(self.population[i])\n                    som_weight = self.som.get_weights()[winner[0], winner[1]]\n                    mutation_factor = self.F * (1 + 0.5 * np.random.randn())  # Introduce some randomness\n                    idx = np.random.choice(range(self.pop_size), 2, replace=False)\n                    x_rand1, x_rand2 = self.population[idx[0]], self.population[idx[1]]\n                    mutant = self.population[i] + mutation_factor * (som_weight - self.population[i]) + self.F * (x_rand1 - x_rand2)\n\n                else:\n                    # Standard DE Mutation if SOM is not initialized\n                    idx = np.random.choice(range(self.pop_size), 2, replace=False)\n                    x_rand1, x_rand2 = self.population[idx[0]], self.population[idx[1]]\n                    mutant = self.population[i] + self.F * (self.population[np.random.randint(self.pop_size)] - self.population[i]) + self.F * (x_rand1 - x_rand2)\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    success_F.append(self.F)\n                    success_CR.append(self.CR)\n\n                    # Archive replaced solution\n                    self.archive.append(self.population[i].copy())\n                    \n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n\n            #Adapt F and CR using successful values\n            if generation % self.adapt_freq == 0:\n                if success_F:\n                    self.F = np.clip(np.mean(success_F), 0.1, 1.0)\n                    self.CR = np.clip(np.mean(success_CR), 0.1, 1.0)\n                else:\n                    # If no successful F/CR, perturb them randomly\n                    self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n                    self.CR = np.clip(np.random.normal(self.CR, 0.1), 0.1, 1.0)\n                success_F = []\n                success_CR = []\n\n            # Archive management (optional - keep archive size limited)\n            if len(self.archive) > self.pop_size * 2:\n                self.archive = self.archive[-self.pop_size*2:]\n\n\n        return self.f_opt, self.x_opt\n\n    def _gaussian(self, pos, sigma):\n        \"\"\"Gaussian function centered on the origin.\n\n        Parameters\n        ----------\n        pos : array_like\n            Position.\n        sigma : float\n            Spread of the Gaussian.\n\n        Returns\n        -------\n        float :\n            Value of the Gaussian at ``pos``.\n\n        \"\"\"\n        return np.exp(-np.sum(pos ** 2) / (2 * sigma ** 2))", "configspace": "", "generation": 5, "feedback": "An exception occurred: No module named 'minisom'.", "error": "", "parent_ids": ["03e40c37-0042-4477-81eb-f9abe9c311c9"], "operator": null, "metadata": {}}
{"id": "7a00507c-9640-437d-a257-480151852911", "fitness": -Infinity, "name": "SelfAdaptiveDE", "description": "Self-adaptive Differential Evolution with orthogonal design for parameter tuning and a local search operator to refine promising solutions.", "code": "import numpy as np\nfrom scipy.stats import norm\nfrom scipy.optimize import minimize\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size_factor=5, F=0.5, CR=0.7, archive_prob=0.1, local_search_prob=0.1):\n        \"\"\"\n        Initialize the Self-Adaptive Differential Evolution algorithm with orthogonal design and local search.\n\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size_factor (int): Factor to determine the population size (pop_size = pop_size_factor * dim).\n            F (float): The initial scaling factor.\n            CR (float): The initial crossover rate.\n            archive_prob (float): Probability of using an archive member for mutation.\n            local_search_prob (float): Probability of applying local search to a solution.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = int(pop_size_factor * dim)\n        self.F = F * np.ones(self.pop_size)  # Individual F values\n        self.CR = CR * np.ones(self.pop_size) # Individual CR values\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.archive = []\n        self.archive_size = self.pop_size // 2\n        self.archive_prob = archive_prob\n        self.local_search_prob = local_search_prob\n        self.success_F = []\n        self.success_CR = []\n\n    def initialize_population(self, func):\n        \"\"\"Initialize the population randomly within the bounds.\"\"\"\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.pop[np.argmin(self.fitness)]\n\n    def update_archive(self, x):\n        \"\"\"Update the archive with a new solution.\"\"\"\n        if len(self.archive) < self.archive_size:\n            self.archive.append(x)\n        else:\n            idx = np.random.randint(0, self.archive_size)\n            self.archive[idx] = x\n\n    def orthogonal_design(self, levels=3):\n        \"\"\"Generate an orthogonal array for parameter tuning.\"\"\"\n        # Simplified L9 orthogonal array (can be extended for higher dimensions)\n        if levels == 3:\n            array = np.array([\n                [1, 1, 1, 1],\n                [1, 2, 2, 2],\n                [1, 3, 3, 3],\n                [2, 1, 2, 3],\n                [2, 2, 3, 1],\n                [2, 3, 1, 2],\n                [3, 1, 3, 2],\n                [3, 2, 1, 3],\n                [3, 3, 2, 1]\n            ]) - 1  # Subtract 1 to make it 0-indexed\n            return array\n        else:\n            raise ValueError(\"Only levels=3 is supported for this simplified orthogonal design.\")\n\n    def local_search(self, x, func):\n        \"\"\"Apply local search to refine a solution.\"\"\"\n        bounds = func.bounds\n        res = minimize(func, x, bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)], method='L-BFGS-B')\n        return res.x, res.fun\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using Self-Adaptive Differential Evolution with orthogonal design and local search.\n\n        Args:\n            func (callable): The function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            # Parameter adaptation using orthogonal design\n            design = self.orthogonal_design()\n            for i in range(self.pop_size):\n                # Sample F and CR from normal distribution based on previous success\n                if self.success_F:\n                    mu_F = np.mean(self.success_F)\n                    sigma_F = np.std(self.success_F) if len(self.success_F) > 1 else 0.1\n                    self.F[i] = np.clip(norm.rvs(loc=mu_F, scale=sigma_F), 0.1, 1.0)\n\n                if self.success_CR:\n                    mu_CR = np.mean(self.success_CR)\n                    sigma_CR = np.std(self.success_CR) if len(self.success_CR) > 1 else 0.1\n                    self.CR[i] = np.clip(norm.rvs(loc=mu_CR, scale=sigma_CR), 0.1, 1.0)\n\n            # Mutation and Crossover\n            for i in range(self.pop_size):\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                a, b, c = idxs[0], idxs[1], idxs[2]\n                mutant = self.pop[a] + self.F[i] * (self.pop[b] - self.pop[c])\n\n                # Archive interaction\n                if len(self.archive) > 0 and np.random.rand() < self.archive_prob:\n                    arch_idx = np.random.randint(0, len(self.archive))\n                    arch_member = self.archive[arch_idx]\n                    mutant = np.where(np.random.rand(self.dim) < 0.5, mutant, arch_member + self.F[i] * (mutant - self.pop[a]))\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                cross_points = np.random.rand(self.dim) < self.CR[i]\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Selection\n                fitness_trial = func(trial)\n                self.eval_count += 1\n\n                if fitness_trial < self.fitness[i]:\n                    # Update success history for F and CR\n                    self.success_F.append(self.F[i])\n                    self.success_CR.append(self.CR[i])\n\n                    # Keep a limited history\n                    self.success_F = self.success_F[-50:]\n                    self.success_CR = self.success_CR[-50:]\n\n                    self.fitness[i] = fitness_trial\n                    self.pop[i] = trial\n                    self.update_archive(trial.copy())\n                    \n                    # Local search\n                    if np.random.rand() < self.local_search_prob:\n                        x_local, f_local = self.local_search(trial, func)\n                        self.eval_count += 1  # Increment for local search evaluation\n\n                        if f_local < self.fitness[i]:\n                            self.fitness[i] = f_local\n                            self.pop[i] = x_local\n                            self.update_archive(x_local.copy())\n\n                #Global best update\n                if self.fitness[i] < self.f_opt:\n                    self.f_opt = self.fitness[i]\n                    self.x_opt = self.pop[i]\n\n                if self.eval_count >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "An exception occurred: name 'minimize' is not defined.", "error": "", "parent_ids": ["5289f9a4-3edc-4e4b-98f0-428e72ad9d9d"], "operator": null, "metadata": {}}
{"id": "eac05687-d954-4375-b5ad-35ffe330dd89", "fitness": 0.17748683828942224, "name": "AdaptiveSampleRefine", "description": "Population-based Adaptive Sample and Refine, where promising regions are identified through sampling and iteratively refined with shrinking bounds.", "code": "import numpy as np\n\nclass AdaptiveSampleRefine:\n    def __init__(self, budget=10000, dim=10, pop_size=20):\n        \"\"\"\n        Initialize the Adaptive Sample and Refine algorithm.\n\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size (int): The size of the population used for sampling.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using Adaptive Sample and Refine.\n\n        Args:\n            func (callable): The function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        while self.eval_count < self.budget:\n            # Sample within the current bounds\n            pop = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n            fitness = np.array([func(x) for x in pop])\n            self.eval_count += self.pop_size\n\n            # Update the best solution found so far\n            best_idx = np.argmin(fitness)\n            if fitness[best_idx] < self.f_opt:\n                self.f_opt = fitness[best_idx]\n                self.x_opt = pop[best_idx]\n\n            if self.eval_count >= self.budget:\n                break\n\n            # Adaptive Refinement: Shrink the bounds around the best solution\n            range_width = ub - lb\n            shrink_factor = 0.1 # Adjust this parameter to control shrinkage rate\n\n            new_lb = np.maximum(self.x_opt - shrink_factor * range_width / 2, func.bounds.lb)\n            new_ub = np.minimum(self.x_opt + shrink_factor * range_width / 2, func.bounds.ub)\n            \n            lb = new_lb\n            ub = new_ub\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveSampleRefine scored 0.177 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2bfc844b-330c-4a03-9044-54c9c7714210"], "operator": null, "metadata": {"aucs": [0.07870137282311296, 0.14767358130984853, 0.22670931199089894, 0.12821286280426358, 0.18941852831098827, 0.153256692438268, 0.2331889285871741, 0.17821199081328543, 0.15003559293467095, 0.13468671729463677, 0.17344763494130022, 0.21911435368528842, 0.2505206231606708, 0.14684450356052425, 0.14266705331516916, 0.24252660703079587, 0.20720912097858313, 0.17760469654916544, 0.1730086363025266, 0.1966979569572732]}}
{"id": "a9ab23b2-ebda-4848-9944-5a17883973d0", "fitness": 0.4886676222983023, "name": "SOMA", "description": "Novel Self-Organizing Migrating Algorithm with dynamic sub-population sizes, adaptive step sizes, and a restart mechanism based on stagnation detection.", "code": "import numpy as np\n\nclass SOMA:\n    def __init__(self, budget=10000, dim=10, pop_size_factor=5, path_length=2.0, step_size=0.1, perturbation=0.1, migration_interval=100, stagnation_threshold=1000):\n        \"\"\"\n        Initialize the Self-Organizing Migrating Algorithm.\n\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size_factor (int): Factor to determine the initial population size (pop_size = pop_size_factor * dim).\n            path_length (float): The maximum length of the migration path.\n            step_size (float): The step size along the migration path.\n            perturbation (float): Probability of perturbing a dimension during migration.\n            migration_interval (int): How often to migrate the population.\n            stagnation_threshold (int): Number of iterations without improvement to trigger a restart.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = int(pop_size_factor * dim)\n        self.path_length = path_length\n        self.step_size = step_size\n        self.perturbation = perturbation\n        self.migration_interval = migration_interval\n        self.stagnation_threshold = stagnation_threshold\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.leader_index = None\n        self.no_improvement_counter = 0\n\n    def initialize_population(self, func):\n        \"\"\"Initialize the population randomly within the bounds.\"\"\"\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.leader_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.leader_index]\n        self.x_opt = self.pop[self.leader_index]\n\n    def migrate(self, func):\n        \"\"\"Migrate the population towards the leader.\"\"\"\n        for i in range(self.pop_size):\n            if i == self.leader_index:\n                continue\n\n            # Calculate the direction vector towards the leader\n            direction = self.pop[self.leader_index] - self.pop[i]\n\n            # Generate migration steps\n            num_steps = int(self.path_length / self.step_size)\n            for step in range(1, num_steps + 1):\n                new_position = self.pop[i] + step * self.step_size * direction\n\n                # Perturb each dimension with a probability\n                perturb_mask = np.random.rand(self.dim) < self.perturbation\n                new_position += perturb_mask * np.random.uniform(-1, 1, size=self.dim)  # Small random perturbation\n\n                # Clip the new position to stay within the bounds\n                new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n\n                # Evaluate the new position\n                fitness_new = func(new_position)\n                self.eval_count += 1\n\n                # Update the individual if the new position is better\n                if fitness_new < self.fitness[i]:\n                    self.fitness[i] = fitness_new\n                    self.pop[i] = new_position\n\n                    # Update leader if this individual is now better than the leader\n                    if fitness_new < self.f_opt:\n                        self.f_opt = fitness_new\n                        self.x_opt = new_position\n                        self.leader_index = i\n                        self.no_improvement_counter = 0  # Reset stagnation counter\n                \n                if self.eval_count >= self.budget:\n                    return\n\n    def restart_population(self, func):\n        \"\"\"Restart the population with new random solutions.\"\"\"\n        print(\"Restarting population...\")\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.leader_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.leader_index]\n        self.x_opt = self.pop[self.leader_index]\n        self.no_improvement_counter = 0 # Reset stagnation counter\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using the Self-Organizing Migrating Algorithm.\n\n        Args:\n            func (callable): The function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            self.migrate(func)\n\n            self.no_improvement_counter += self.migration_interval # Assume no improvement for this interval, unless updated in migrate\n\n            if self.no_improvement_counter > self.stagnation_threshold:\n                self.restart_population(func)\n\n            if self.eval_count >= self.budget:\n                break\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm SOMA scored 0.489 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5289f9a4-3edc-4e4b-98f0-428e72ad9d9d"], "operator": null, "metadata": {"aucs": [0.2276886238736907, 0.2923994065870448, 0.4365523455327982, 0.869582564030162, 0.3451114398049886, 0.43860016391913903, 0.2770902011685755, 0.38211237993189606, 0.5646438407790466, 0.19153198956639483, 0.8649189670975992, 0.9987402566102789, 0.30882894038357767, 0.46043432757594005, 0.803338031828412, 0.5875046183036743, 0.32654688723033054, 0.6941229875517806, 0.19786058933264727, 0.505743884858068]}}
{"id": "4fb17960-26b9-4396-b206-5374a5156745", "fitness": 0.6989586999750319, "name": "DynamicDE", "description": "Differential Evolution with a dynamically adjusted population size based on stagnation detection and a repair mechanism for out-of-bounds individuals.", "code": "import numpy as np\n\nclass DynamicDE:\n    def __init__(self, budget=10000, dim=10, pop_size_factor=5, F=0.5, CR=0.7, stagnation_threshold=100):\n        \"\"\"\n        Initialize the Dynamic Differential Evolution algorithm.\n\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size_factor (int): Initial factor for population size (pop_size = pop_size_factor * dim).\n            F (float): The scaling factor.\n            CR (float): The crossover rate.\n            stagnation_threshold (int): Number of iterations without improvement before reducing population.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = int(pop_size_factor * dim)\n        self.F = F\n        self.CR = CR\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = stagnation_threshold\n        self.best_fitness_history = []\n\n    def initialize_population(self, func):\n        \"\"\"Initialize the population randomly within the bounds.\"\"\"\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.pop[np.argmin(self.fitness)]\n        self.best_fitness_history.append(self.f_opt)\n\n    def adjust_population_size(self):\n        \"\"\"Dynamically adjust the population size based on stagnation.\"\"\"\n        if self.stagnation_counter > self.stagnation_threshold and self.pop_size > self.dim + 3: #Ensure enough individuals for mutation\n            self.pop_size = max(int(self.pop_size * 0.8), self.dim + 3)\n            print(f\"Reducing population size to {self.pop_size}\")\n            # Reduce the population by removing the worst individuals\n            worst_indices = np.argsort(self.fitness)[-int(len(self.fitness) - self.pop_size):] # Keep the best\n            keep_indices = np.setdiff1d(np.arange(len(self.fitness)), worst_indices)\n\n            self.pop = self.pop[keep_indices]\n            self.fitness = self.fitness[keep_indices]\n\n            self.stagnation_counter = 0  # Reset stagnation counter\n            \n    def repair(self, x, lb, ub):\n        \"\"\"Repair function to bring individuals back to the bounds.\"\"\"\n        return np.clip(x, lb, ub)\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using Dynamic Differential Evolution.\n\n        Args:\n            func (callable): The function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            self.adjust_population_size()\n\n            for i in range(self.pop_size):\n                # Mutation (DE/rand/1)\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                a, b, c = idxs[0], idxs[1], idxs[2]\n                mutant = self.pop[a] + self.F * (self.pop[b] - self.pop[c])\n                mutant = self.repair(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Selection\n                f = func(trial)\n                self.eval_count += 1\n\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.pop[i] = trial\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                        self.stagnation_counter = 0 # Reset if improvement\n                else:\n                    self.stagnation_counter +=1 # Increment otherwise\n\n                if self.eval_count >= self.budget:\n                    break\n            \n            if self.f_opt < self.best_fitness_history[-1]:\n                self.best_fitness_history.append(self.f_opt)\n            else:\n                self.best_fitness_history.append(self.best_fitness_history[-1])\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm DynamicDE scored 0.699 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2bfc844b-330c-4a03-9044-54c9c7714210"], "operator": null, "metadata": {"aucs": [0.295066801829001, 0.3314954530992563, 0.7079340394158531, 0.9267184605578663, 0.8773919285872221, 0.8768344698257151, 0.5507917264386303, 0.8417104030017662, 0.863642237675718, 0.8686734818157531, 0.9347853999221245, 0.9989947097921353, 0.2920781552623294, 0.7786732770592943, 0.7377148183073579, 0.899026321083077, 0.46204010958272423, 0.9097954910526664, 0.3102446826163786, 0.5155620325757684]}}
{"id": "0f38255a-72b8-4aef-b60e-01e1e664e452", "fitness": 0.6595660445195708, "name": "HybridDE", "description": "Simplified Hybrid Differential Evolution with self-adaptive parameters, focusing on exploration and exploitation balance through parameter adaptation and a reduced number of ring neighbors.", "code": "import numpy as np\n\nclass HybridDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, adapt_freq=10, ring_neighbors=2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.adapt_freq = adapt_freq\n        self.ring_neighbors = ring_neighbors\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Ring Topology Selection (Simplified)\n                neighbors = [(i - j) % self.pop_size for j in range(1, self.ring_neighbors + 1)]\n                neighbor_fitnesses = self.fitness[neighbors]\n                best_neighbor_idx = neighbors[np.argmin(neighbor_fitnesses)]\n                \n                # Mutation - Use best neighbor in ring\n                idx = np.random.choice(range(self.pop_size), 2, replace=False)\n                x_rand1, x_rand2 = self.population[idx[0]], self.population[idx[1]]\n                mutant = self.population[i] + self.F * (self.population[best_neighbor_idx] - self.population[i]) + self.F * (x_rand1 - x_rand2)\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n\n            #Adapt F and CR (Simplified)\n            if generation % self.adapt_freq == 0:\n                self.F = np.clip(np.random.normal(0.5, 0.3), 0.1, 1.0)\n                self.CR = np.clip(np.random.normal(0.9, 0.2), 0.1, 1.0)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm HybridDE scored 0.660 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ae6a515c-dc09-407e-9a1a-11cf3d26c06b"], "operator": null, "metadata": {"aucs": [0.21860010090304127, 0.5927851602444274, 0.6742200336621571, 0.8987859717080289, 0.7536576586925425, 0.7884780063371404, 0.43276524048076503, 0.6348576051626476, 0.7496996895024022, 0.4460468188685487, 0.8894018165214326, 0.9971054895509364, 0.7193971000522537, 0.6250185287600045, 0.918159258938702, 0.6597501986403256, 0.6029821777540079, 0.8705350854754389, 0.21997128209196704, 0.4991036670446479]}}
{"id": "0d6fa85c-d991-4f21-9f9a-229d5a8f9278", "fitness": 0.522369799426244, "name": "AdaptiveDE", "description": "Enhanced Adaptive Differential Evolution with fitness-aware parameter adaptation and a restart mechanism to escape local optima.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size_factor=5, F=0.5, CR=0.7, restart_trigger=0.1):\n        \"\"\"\n        Initialize the Adaptive Differential Evolution algorithm.\n\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size_factor (int): Factor to determine the population size (pop_size = pop_size_factor * dim).\n            F (float): The initial scaling factor.\n            CR (float): The initial crossover rate.\n            restart_trigger (float): Threshold for fitness improvement to trigger a population restart.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = int(pop_size_factor * dim)\n        self.F = F\n        self.CR = CR\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.restart_trigger = restart_trigger\n        self.no_improvement_count = 0\n        self.max_no_improvement = 100  # Adjust as needed\n\n    def initialize_population(self, func):\n        \"\"\"Initialize the population randomly within the bounds.\"\"\"\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.pop[np.argmin(self.fitness)]\n        self.initial_f_opt = self.f_opt  # Store initial best fitness\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using Adaptive Differential Evolution.\n\n        Args:\n            func (callable): The function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            old_f_opt = self.f_opt\n            F = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n            CR = np.clip(np.random.normal(self.CR, 0.1), 0.1, 1.0)\n            \n            for i in range(self.pop_size):\n                # Mutation (DE/rand/1)\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                a, b, c = idxs[0], idxs[1], idxs[2]\n                mutant = self.pop[a] + F * (self.pop[b] - self.pop[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Selection\n                f = func(trial)\n                self.eval_count += 1\n\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.pop[i] = trial\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                        self.no_improvement_count = 0  # Reset counter on improvement\n                    else:\n                        self.no_improvement_count +=1\n                else:\n                    self.no_improvement_count += 1\n\n                if self.eval_count >= self.budget:\n                    break\n            \n            # Fitness-aware parameter adaptation\n            if self.f_opt < old_f_opt:\n                self.F = 0.9 * self.F + 0.1  # Increase F if improvement\n                self.CR = 0.9 * self.CR + 0.1 # Increase CR if improvement\n            else:\n                self.F = 0.9 * self.F - 0.1 # Decrease F if no improvement\n                self.CR = 0.9 * self.CR - 0.1 # Decrease CR if no improvement\n                self.F = np.clip(self.F, 0.1, 1.0)\n                self.CR = np.clip(self.CR, 0.1, 1.0)\n\n            # Restart mechanism\n            if (self.initial_f_opt - self.f_opt) / self.initial_f_opt < self.restart_trigger and self.no_improvement_count > self.max_no_improvement:\n                self.initialize_population(func)  # Restart population\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDE scored 0.522 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2bfc844b-330c-4a03-9044-54c9c7714210"], "operator": null, "metadata": {"aucs": [0.16315199046978301, 0.21405001168084192, 0.45392915038188686, 0.8239089715869783, 0.7074964771816405, 0.8311260496960227, 0.3985185624634021, 0.5406464699910292, 0.4251383680695847, 0.22148487613001955, 0.4918831749626045, 0.9954532263196106, 0.26971399306265265, 0.2823425172516333, 0.790340674635923, 0.8638018285489224, 0.3941784623008081, 0.8545220891268066, 0.2195363596517267, 0.5061727350130025]}}
{"id": "b2a8e8d4-2d0a-4d76-9e5c-7d605d6bace4", "fitness": 0.6531776749483956, "name": "HybridDE", "description": "Enhanced Hybrid DE with aging individuals, probabilistic repair, and adaptive population size.", "code": "import numpy as np\n\nclass HybridDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, adapt_freq=10, ring_neighbors=3, reduction_factor=0.95, age_limit=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.adapt_freq = adapt_freq\n        self.ring_neighbors = ring_neighbors\n        self.reduction_factor = reduction_factor #Population size reduction factor\n        self.min_pop_size = 10\n        self.age_limit = age_limit #Maximum age before being replaced\n        self.ages = np.zeros(pop_size, dtype=int) #Age of each individual\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Ring Topology Selection\n                neighbors = [(i - j) % self.pop_size for j in range(1, self.ring_neighbors + 1)]\n                neighbor_fitnesses = self.fitness[neighbors]\n                best_neighbor_idx = neighbors[np.argmin(neighbor_fitnesses)]\n                \n                # Mutation - Use best neighbor in ring\n                idx = np.random.choice(range(self.pop_size), 2, replace=False)\n                x_rand1, x_rand2 = self.population[idx[0]], self.population[idx[1]]\n                mutant = self.population[i] + self.F * (self.population[best_neighbor_idx] - self.population[i]) + self.F * (x_rand1 - x_rand2)\n\n                # Probabilistic Repair\n                repair_prob = 0.1 #Probability of repairing a dimension\n                for j in range(self.dim):\n                    if np.random.rand() < repair_prob:\n                        if mutant[j] < func.bounds.lb[j] or mutant[j] > func.bounds.ub[j]:\n                            mutant[j] = np.random.uniform(func.bounds.lb[j], func.bounds.ub[j])\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Orthogonal Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n                    self.ages[i] = 0 #Reset age after improvement\n                else:\n                    self.ages[i] += 1 #Increase age if no improvement\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n\n            #Adapt F and CR\n            if generation % self.adapt_freq == 0:\n                self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n                self.CR = np.clip(np.random.normal(self.CR, 0.1), 0.1, 1.0)\n            \n            # Population size reduction\n            if generation % (self.adapt_freq*5) == 0 and self.pop_size > self.min_pop_size:\n                num_to_reduce = max(1, int(self.pop_size * (1 - self.reduction_factor)))\n                \n                #Find the worst AND oldest individuals and remove them\n                #Combine fitness rank and age rank for selection\n                fitness_rank = np.argsort(self.fitness)\n                age_rank = np.argsort(self.ages)[::-1] #Sort ages in descending order\n                \n                combined_rank = np.concatenate([fitness_rank[-num_to_reduce//2:], age_rank[:num_to_reduce - num_to_reduce//2]])\n                \n                worst_indices = np.unique(combined_rank)\n\n                if len(worst_indices) > num_to_reduce:\n                  worst_indices = worst_indices[:num_to_reduce]\n\n                remaining_indices = np.array([i for i in range(self.pop_size) if i not in worst_indices])\n                \n                self.population = self.population[remaining_indices]\n                self.fitness = self.fitness[remaining_indices]\n                self.ages = self.ages[remaining_indices]\n                self.pop_size = len(self.population)\n                \n            # Re-initialize old individuals:\n            for i in range(self.pop_size):\n                if self.ages[i] > self.age_limit:\n                    self.population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                    self.fitness[i] = func(self.population[i])\n                    self.budget -= 1\n                    self.ages[i] = 0\n\n                    if self.fitness[i] < self.f_opt:\n                        self.f_opt = self.fitness[i]\n                        self.x_opt = self.population[i]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm HybridDE scored 0.653 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ae6a515c-dc09-407e-9a1a-11cf3d26c06b"], "operator": null, "metadata": {"aucs": [0.2198074526816436, 0.7484723543469849, 0.7016956394456547, 0.8548646299649878, 0.7759881394949946, 0.8010123405018014, 0.33428768486817484, 0.7399101392300684, 0.8626167115989266, 0.24255045265064323, 0.8745985986413742, 0.9909208849404918, 0.4949027113212022, 0.5517959215523875, 0.9352565359083869, 0.8288517137504846, 0.5032957206479955, 0.8780213495594884, 0.21158356415733104, 0.51312095370489]}}
{"id": "0002a16b-61e5-4dce-8378-612840fc0ca9", "fitness": 0.0, "name": "BiCMAES", "description": "Covariance matrix adaptation evolution strategy with bi-population and adaptive population size, enhancing exploration and exploitation capabilities.", "code": "import numpy as np\n\nclass BiCMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, mu_factor=0.5, cs=0.3, damps=1.0, c_cov_mean=0.1, c_cov_rank_one=None, c_cov_rank_mu=0.1):\n        \"\"\"\n        Covariance Matrix Adaptation Evolution Strategy with bi-population and adaptive population size.\n\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size (int): Population size. If None, it's set to 4 + int(3 * np.log(dim)).\n            mu_factor (float): Factor to determine the number of parents (mu = mu_factor * pop_size).\n            cs (float): Cumulation factor for step-size control.\n            damps (float): Damping for step-size.\n            c_cov_mean (float): Learning rate for the rank-one update of the covariance matrix.\n            c_cov_rank_one (float): Learning rate for rank-one update, defaults to 2 / ((dim + 1.3)**2 + mu).\n            c_cov_rank_mu (float): Learning rate for the rank-mu update of the covariance matrix.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.mu = int(self.pop_size * mu_factor)\n        self.mu = max(1, self.mu)  # Ensure mu is at least 1\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = cs\n        self.damps = damps + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) * (1 + self.cs)\n        self.c_cov_mean = c_cov_mean\n        self.c_cov_rank_one = 2 / ((self.dim + 1.3)**2 + self.mueff) if c_cov_rank_one is None else c_cov_rank_one\n        self.c_cov_rank_mu = min(1 - self.c_cov_rank_one, self.c_cov_rank_one * (self.mueff - 1 + 1/self.mueff) / ((self.dim + 1.3)**2 + self.mueff))\n        self.mean = None\n        self.sigma = None\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.eval_count = 0\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.lb = None\n        self.ub = None\n        self.adaptive_pop_size = self.pop_size\n        self.success_rate = 0.5 # Initialize success rate\n\n    def initialize(self, func):\n        \"\"\"Initialize the CMA-ES algorithm.\"\"\"\n        self.mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.sigma = 0.1 * (func.bounds.ub - func.bounds.lb)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.lb = func.bounds.lb\n        self.ub = func.bounds.ub\n\n    def sample_population(self):\n        \"\"\"Sample a population from the multivariate normal distribution.\"\"\"\n        A = np.linalg.cholesky(self.C)\n        z = np.random.normal(0, 1, size=(self.adaptive_pop_size, self.dim))\n        x = self.mean + self.sigma * (A @ z.T).T\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def update_parameters(self, x, fitness):\n        \"\"\"Update the parameters of the CMA-ES algorithm.\"\"\"\n        # Sort the population based on fitness\n        idx = np.argsort(fitness)\n        x = x[idx]\n\n        # Selection and weighted recombination\n        x_mu = x[:self.mu]\n        y = (x_mu - self.mean) / self.sigma\n        self.mean = np.sum(self.weights[:, None] * x_mu, axis=0)\n\n        # Cumulation for covariance matrix adaptation\n        self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * (x_mu[0] - self.mean) / self.sigma\n        hsig = np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.eval_count / self.adaptive_pop_size)) < (1.4 + 2 / (self.dim + 1))\n        self.pc = (1 - self.c_cov_mean) * self.pc + hsig * np.sqrt(self.c_cov_mean * (2 - self.c_cov_mean) * self.mueff) * y[0]\n\n        # Covariance matrix adaptation\n        self.C = (1 - self.c_cov_rank_one - self.c_cov_rank_mu) * self.C + self.c_cov_rank_one * np.outer(self.pc, self.pc)\n        for i in range(self.mu):\n            self.C += self.c_cov_rank_mu * self.weights[i] * np.outer(y[i], y[i])\n\n        # Step-size control\n        self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / np.sqrt(self.dim) - 1))\n\n        # Update the covariance matrix to be positive definite\n        try:\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            self.C = np.linalg.eigh(self.C)[1] @ np.diag(np.maximum(1e-10, np.linalg.eigh(self.C)[0])) @ np.linalg.eigh(self.C)[1].T\n        except np.linalg.LinAlgError:\n            self.C = np.eye(self.dim)\n\n        #Adaptive Population Size\n        if fitness[0] < self.f_opt:\n             self.success_rate = 0.9\n        else:\n            self.success_rate *= 0.9\n            \n        self.adaptive_pop_size = int(self.pop_size * (2 - self.success_rate))\n        self.adaptive_pop_size = max(2, min(self.adaptive_pop_size, self.budget // 10))\n\n\n    def __call__(self, func):\n        \"\"\"Optimize the given function using CMA-ES.\"\"\"\n        self.initialize(func)\n\n        while self.eval_count < self.budget:\n            # Sample a population\n            x = self.sample_population()\n\n            # Evaluate the population\n            fitness = np.array([func(xi) for xi in x])\n            self.eval_count += self.adaptive_pop_size\n\n            # Update the global best\n            best_idx = np.argmin(fitness)\n            if fitness[best_idx] < self.f_opt:\n                self.f_opt = fitness[best_idx]\n                self.x_opt = x[best_idx]\n\n            # Update parameters\n            self.update_parameters(x, fitness)\n\n            if self.eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm BiCMAES scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5289f9a4-3edc-4e4b-98f0-428e72ad9d9d"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "8a175fa0-a798-42cf-b2b0-1cd6cd2c3520", "fitness": 0.3990006890147145, "name": "AdaptiveDE", "description": "Simplified Adaptive Differential Evolution with fitness-aware parameter adaptation and reduced archive interaction for enhanced exploitation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size_factor=5, F=0.5, CR=0.7, lr_F=0.1, lr_CR=0.1, archive_prob=0.1):\n        \"\"\"\n        Initialize the Adaptive Differential Evolution algorithm.\n\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size_factor (int): Factor to determine the population size (pop_size = pop_size_factor * dim).\n            F (float): The initial scaling factor.\n            CR (float): The initial crossover rate.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = int(pop_size_factor * dim)\n        self.F = F\n        self.CR = CR\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n\n    def initialize_population(self, func):\n        \"\"\"Initialize the population randomly within the bounds.\"\"\"\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.pop[np.argmin(self.fitness)]\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using Adaptive Differential Evolution.\n\n        Args:\n            func (callable): The function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            # Mutation\n            idxs = np.random.randint(0, self.pop_size, size=(self.pop_size, 3))\n            a, b, c = idxs[:, 0], idxs[:, 1], idxs[:, 2]\n            mutant = self.pop[a] + self.F * (self.pop[b] - self.pop[c])\n            mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            cross_points = np.random.rand(self.pop_size, self.dim) < self.CR\n            trial = np.where(cross_points, mutant, self.pop)\n\n            # Selection\n            fitness_trial = np.array([func(x) for x in trial])\n            self.eval_count += self.pop_size\n\n            improved = fitness_trial < self.fitness\n            \n            # Parameter adaptation based on fitness improvement\n            delta_fitness = np.abs(fitness_trial - self.fitness)\n            self.F = np.mean(delta_fitness[improved]) if np.any(improved) else self.F\n            self.CR = np.mean(np.random.rand(self.pop_size)[improved]) if np.any(improved) else self.CR\n            \n            self.fitness[improved] = fitness_trial[improved]\n            self.pop[improved] = trial[improved]\n\n            #Global best update\n            best_idx = np.argmin(self.fitness)\n            if self.fitness[best_idx] < self.f_opt:\n                self.f_opt = self.fitness[best_idx]\n                self.x_opt = self.pop[best_idx]\n            \n            if self.eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDE scored 0.399 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5289f9a4-3edc-4e4b-98f0-428e72ad9d9d"], "operator": null, "metadata": {"aucs": [0.17641679910330055, 0.19353643849880364, 0.34582276465384143, 0.5262313257037268, 0.34603580993786665, 0.4595105466354382, 0.31179556676402376, 0.33499288072766353, 0.37173298016777856, 0.3895904547419816, 0.5166018211700332, 0.9985162399528008, 0.21636441371481163, 0.28408520393653847, 0.6380612338147873, 0.3618762905491846, 0.27946071656997906, 0.45272676929177114, 0.28758638738837616, 0.48906913697158516]}}
{"id": "1b76b96e-ee56-4c49-8107-00bc6e0517f7", "fitness": -Infinity, "name": "MirroredAdaptiveDE", "description": "Adaptive Differential Evolution with a mirrored sampling strategy to enhance exploration around the boundaries.", "code": "import numpy as np\n\nclass MirroredAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size_factor=5, F_base=0.5, CR_base=0.7, F_range=0.3, CR_range=0.3):\n        \"\"\"\n        Initialize the Mirrored Adaptive Differential Evolution algorithm.\n\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size_factor (int): Initial factor for population size (pop_size = pop_size_factor * dim).\n            F_base (float): Base value for the scaling factor.\n            CR_base (float): Base value for the crossover rate.\n            F_range (float): Range for adapting the scaling factor (F = F_base + random * F_range).\n            CR_range (float): Range for adapting the crossover rate (CR = CR_base + random * CR_range).\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = int(pop_size_factor * dim)\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.F_range = F_range\n        self.CR_range = CR_range\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n\n    def initialize_population(self, func):\n        \"\"\"Initialize the population randomly within the bounds.\"\"\"\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.pop[np.argmin(self.fitness)]\n\n    def generate_mirrored_sample(self, x, lb, ub):\n        \"\"\"Generate a mirrored sample if x is outside bounds.\"\"\"\n        mirrored_x = x.copy()\n        for i in range(self.dim):\n            if x[i] < lb:\n                mirrored_x[i] = lb + (lb - x[i])\n            elif x[i] > ub:\n                mirrored_x[i] = ub - (x[i] - ub)\n        return mirrored_x\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using Mirrored Adaptive Differential Evolution.\n\n        Args:\n            func (callable): The function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Parameter Adaptation\n                F = self.F_base + np.random.uniform(-self.F_range, self.F_range)\n                CR = self.CR_base + np.random.uniform(-self.CR_range, self.CR_range)\n                F = np.clip(F, 0.1, 0.9)\n                CR = np.clip(CR, 0.1, 0.9)\n\n                # Mutation (DE/rand/1)\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                a, b, c = idxs[0], idxs[1], idxs[2]\n                mutant = self.pop[a] + F * (self.pop[b] - self.pop[c])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Handle Out-of-Bounds using mirrored sampling\n                if np.any(trial < lb) or np.any(trial > ub):\n                    trial = self.generate_mirrored_sample(trial, lb, ub)\n                    trial = np.clip(trial, lb, ub)  # Ensure it's still within bounds after mirroring\n\n                # Selection\n                f = func(trial)\n                self.eval_count += 1\n\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.pop[i] = trial\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n\n                if self.eval_count >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "An exception occurred: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all().", "error": "", "parent_ids": ["4fb17960-26b9-4396-b206-5374a5156745"], "operator": null, "metadata": {}}
{"id": "c3ef57eb-5e90-48e5-8a07-53aa649bb582", "fitness": -Infinity, "name": "SOM_DE", "description": "Differential Evolution with a self-organizing map (SOM) to guide exploration and exploitation by clustering solutions and adapting DE parameters based on cluster performance.", "code": "import numpy as np\nfrom minisom import MiniSom\n\nclass SOM_DE:\n    def __init__(self, budget=10000, dim=10, pop_size_factor=5, F=0.5, CR=0.7, som_grid_size=10):\n        \"\"\"\n        Initialize the Self-Organizing Map Differential Evolution algorithm.\n\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size_factor (int): Factor to determine the population size (pop_size = pop_size_factor * dim).\n            F (float): The initial scaling factor.\n            CR (float): The initial crossover rate.\n            som_grid_size (int): Size of the SOM grid (som_grid_size x som_grid_size).\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = int(pop_size_factor * dim)\n        self.F = F\n        self.CR = CR\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.som_grid_size = som_grid_size\n        self.som = None\n\n    def initialize_population(self, func):\n        \"\"\"Initialize the population randomly within the bounds.\"\"\"\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.pop[np.argmin(self.fitness)]\n\n    def initialize_som(self):\n        \"\"\"Initialize the Self-Organizing Map.\"\"\"\n        self.som = MiniSom(self.som_grid_size, self.som_grid_size, self.dim, sigma=0.3, learning_rate=0.5)\n        self.som.random_weights_init(self.pop)\n        self.som.train_random(self.pop, 100)  # Train SOM for a short period\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using Self-Organizing Map Differential Evolution.\n\n        Args:\n            func (callable): The function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        self.initialize_population(func)\n        self.initialize_som()\n\n        while self.eval_count < self.budget:\n            # Assign each individual to a SOM node\n            winning_nodes = [self.som.winner(x) for x in self.pop]\n\n            for i in range(self.pop_size):\n                # Mutation (DE/rand/1) with SOM guidance\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                a, b, c = idxs[0], idxs[1], idxs[2]\n                mutant = self.pop[a] + self.F * (self.pop[b] - self.pop[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Evaluate the trial solution\n                f = func(trial)\n                self.eval_count += 1\n\n                # Selection\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.pop[i] = trial\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n\n                    # Update SOM with the new solution\n                    self.som.update(trial, self.som.winner(trial), learning_rate=0.1)\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Adapt DE parameters based on SOM cluster performance (example: adjust F and CR)\n            cluster_fitness = {}\n            for i, node in enumerate(winning_nodes):\n                if node not in cluster_fitness:\n                    cluster_fitness[node] = []\n                cluster_fitness[node].append(self.fitness[i])\n\n            for node, fitness_values in cluster_fitness.items():\n                avg_fitness = np.mean(fitness_values)\n                if avg_fitness < self.f_opt:\n                    self.F *= 1.05  # Increase F for promising clusters\n                    self.CR *= 1.05  # Increase CR for promising clusters\n                else:\n                    self.F *= 0.95  # Decrease F for less promising clusters\n                    self.CR *= 0.95  # Decrease CR for less promising clusters\n\n                self.F = np.clip(self.F, 0.1, 1.0)\n                self.CR = np.clip(self.CR, 0.1, 1.0)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "An exception occurred: No module named 'minisom'.", "error": "", "parent_ids": ["0d6fa85c-d991-4f21-9f9a-229d5a8f9278"], "operator": null, "metadata": {}}
{"id": "9ea6b98d-fb2d-4b42-8bb7-91d5b9b859bd", "fitness": 0.0, "name": "SHADE", "description": "A differential evolution strategy with a success-history based parameter adaptation and a diversity-preserving selection mechanism.", "code": "import numpy as np\n\nclass SHADE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, memory_size=5, archive_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.memory_size = memory_size\n        self.archive_size = archive_size\n        self.memory_CR = np.full(self.memory_size, 0.5)\n        self.memory_F = np.full(self.memory_size, 0.5)\n        self.archive = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        while self.budget > 0:\n            \n            CRs = np.random.choice(self.memory_CR, size=self.pop_size)\n            Fs = np.random.choice(self.memory_F, size=self.pop_size)\n            Fs = np.clip(Fs, 0.000001, 1.0) #Ensure F is not zero\n\n            new_population = np.copy(self.population)\n            new_fitness = np.copy(self.fitness)\n\n            for i in range(self.pop_size):\n                # Mutation\n                p_best = int(np.random.uniform() * self.pop_size)  #Pick a random value between 0 and pop_size to randomly select the p_best index.\n                \n                indices = np.random.choice(range(self.pop_size), 2, replace=False)\n                x_r1, x_r2 = self.population[indices[0]], self.population[indices[1]]\n                \n                mutant = self.population[i] + Fs[i] * (self.population[p_best] - self.population[i]) + Fs[i] * (x_r1 - x_r2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CRs[i]\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i])\n                    else:\n                        idx = np.random.randint(0, self.archive_size)\n                        self.archive[idx] = self.population[i]\n                    \n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n\n            #Diversity Preserving Selection\n            combined_population = np.concatenate((self.population, self.archive))\n            combined_fitness = np.array([func(x) for x in combined_population]) #Evaluate new population\n\n            self.budget -= len(self.archive)\n\n            if self.budget <=0:\n                break\n\n            #Sort by fitness\n            sorted_indices = np.argsort(combined_fitness)\n            \n            #Pick the best individuals\n            selected_indices = sorted_indices[:self.pop_size]\n            \n            #Update the population and fitness\n            self.population = combined_population[selected_indices]\n            self.fitness = combined_fitness[selected_indices]\n           \n            # Success-History Adaptation\n            successful_CRs = CRs[new_fitness < self.fitness]\n            successful_Fs = Fs[new_fitness < self.fitness]\n            \n            if len(successful_CRs) > 0 and len(successful_Fs) > 0:\n                mean_CR = np.mean(successful_CRs)\n                mean_F = np.mean(successful_Fs)\n\n                #Update memory with new values\n                self.memory_CR[np.random.randint(0, self.memory_size)] = mean_CR\n                self.memory_F[np.random.randint(0, self.memory_size)] = mean_F\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm SHADE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b2a8e8d4-2d0a-4d76-9e5c-7d605d6bace4"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "bbccf3ec-2298-42ba-8d5a-7d6967d1e214", "fitness": 0.23858619392656072, "name": "OrthogonalCauchyDE", "description": "Enhanced Differential Evolution with orthogonal learning and a Cauchy mutation operator for increased exploration.", "code": "import numpy as np\n\nclass OrthogonalCauchyDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, adapt_freq=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.adapt_freq = adapt_freq\n        self.levy_exponent = 1.5\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Mutation - Cauchy Mutation\n                cauchy_vector = np.random.standard_cauchy(size=self.dim) * 0.01 # Scale Cauchy distribution\n                mutant = self.population[i] + self.F * cauchy_vector\n\n                # Orthogonal Learning\n                if np.random.rand() < 0.1: # Apply with a probability\n                    orthogonal_basis = np.random.randn(self.dim, self.dim)\n                    q, r = np.linalg.qr(orthogonal_basis)\n                    orthogonal_vector = np.dot(q, np.random.randn(self.dim))\n                    mutant = self.population[i] + 0.1 * orthogonal_vector # Scale orthogonal vector\n                    \n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n\n            #Adapt F and CR (Simplified)\n            if generation % self.adapt_freq == 0:\n                self.F = np.clip(np.random.normal(0.5, 0.3), 0.1, 1.0)\n                self.CR = np.clip(np.random.normal(0.9, 0.2), 0.1, 1.0)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm OrthogonalCauchyDE scored 0.239 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0f38255a-72b8-4aef-b60e-01e1e664e452"], "operator": null, "metadata": {"aucs": [0.1256860499024206, 0.19162145290043353, 0.2545322011330531, 0.17603656958530012, 0.14736997934296248, 0.1845580861224636, 0.22176767506223782, 0.15980574596810437, 0.16830745433474148, 0.14499436058185833, 0.15340727632128182, 0.7525392630188026, 0.2582811684514069, 0.17398359995205537, 0.3936109782885917, 0.2741041216405422, 0.18782241259049648, 0.22817959234090646, 0.14507866400822544, 0.43003722698533076]}}
{"id": "4c379a64-9dd8-44d8-8b22-a042060a67b3", "fitness": 0.5945042531324033, "name": "HybridDE", "description": "Simplified DE with adaptive F and CR based on fitness improvement rate, plus a simplified ring topology.", "code": "import numpy as np\n\nclass HybridDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, adapt_freq=10, ring_neighbors=2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.adapt_freq = adapt_freq\n        self.ring_neighbors = ring_neighbors\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        generation = 0\n        last_improvement = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Ring Topology Selection (Simplified)\n                neighbor_idx = (i + 1) % self.pop_size  # Simplest ring neighbor\n\n                # Mutation\n                idx = np.random.choice(range(self.pop_size), 2, replace=False)\n                x_rand1, x_rand2 = self.population[idx[0]], self.population[idx[1]]\n                mutant = self.population[i] + self.F * (self.population[neighbor_idx] - self.population[i]) + self.F * (x_rand1 - x_rand2)\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                trial = np.where(cross_points, mutant, self.population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n                    last_improvement = generation\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n\n            #Adapt F and CR based on recent improvement\n            if generation % self.adapt_freq == 0:\n                if generation - last_improvement > self.adapt_freq * 3:  # No improvement\n                    self.F = np.clip(np.random.normal(1.0, 0.2), 0.1, 1.0) # Increase exploration\n                    self.CR = np.clip(np.random.normal(0.2, 0.1), 0.1, 1.0)\n                else:\n                    self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n                    self.CR = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm HybridDE scored 0.595 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0f38255a-72b8-4aef-b60e-01e1e664e452"], "operator": null, "metadata": {"aucs": [0.1934240300443285, 0.5180344643983404, 0.5682235024856941, 0.8073175308271899, 0.6160168257173028, 0.755496643423524, 0.417200613108223, 0.5569863318008104, 0.7123553835368561, 0.22589079029215686, 0.8395207484998999, 0.9913441457654552, 0.6038949935951452, 0.5206378628477415, 0.8707736020730127, 0.7121172005699992, 0.46242462922899985, 0.8109413385125551, 0.2135789226478394, 0.493905503272992]}}
{"id": "9ea084b4-6ae8-4895-9807-8bcbbeb356d4", "fitness": 0.6124367663064951, "name": "HybridDE", "description": "Simplified Differential Evolution with adaptive parameters, focusing on population diversity through periodic random individual replacement and a streamlined parameter adaptation scheme.", "code": "import numpy as np\n\nclass HybridDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, adapt_freq=10, replace_freq=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.adapt_freq = adapt_freq\n        self.replace_freq = replace_freq #Frequency of replacing individuals\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Mutation\n                idx = np.random.choice(range(self.pop_size), 3, replace=False)\n                x_rand1, x_rand2, x_rand3 = self.population[idx[0]], self.population[idx[1]], self.population[idx[2]]\n                mutant = x_rand1 + self.F * (x_rand2 - x_rand3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n            \n            # Adapt F and CR\n            if generation % self.adapt_freq == 0:\n                self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n                self.CR = np.clip(np.random.normal(self.CR, 0.1), 0.1, 1.0)\n            \n            # Replace individuals\n            if generation % self.replace_freq == 0:\n                replace_indices = np.random.choice(range(self.pop_size), self.pop_size // 5, replace=False)\n                for i in replace_indices:\n                    self.population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                    self.fitness[i] = func(self.population[i])\n                    self.budget -= 1\n\n                    if self.fitness[i] < self.f_opt:\n                        self.f_opt = self.fitness[i]\n                        self.x_opt = self.population[i]\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm HybridDE scored 0.612 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b2a8e8d4-2d0a-4d76-9e5c-7d605d6bace4"], "operator": null, "metadata": {"aucs": [0.2202736640310483, 0.673967695839659, 0.4826898125798673, 0.8498927832703097, 0.5521565009406783, 0.6370019806174515, 0.34018075585552265, 0.4439285059964988, 0.44956712427931644, 0.6946563346111192, 0.8705112793233933, 0.9903063501331476, 0.40433982271704694, 0.7222161301743972, 0.9091450453372807, 0.7274183686942184, 0.6182537164744695, 0.8087915791047806, 0.32425581553537997, 0.5291820606143183]}}
{"id": "28eedacc-11d0-48ea-a425-13b8923c6147", "fitness": 0.6203378901572101, "name": "AdaptiveDELocalSearch", "description": "An Adaptive Differential Evolution strategy with a self-adjusting scaling factor and crossover rate based on the success of past generations, combined with a local search operator triggered with a probability to refine promising solutions.", "code": "import numpy as np\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_init=0.5, CR_init=0.7, local_search_prob=0.1, local_search_radius=0.1):\n        \"\"\"\n        Initialize the Adaptive Differential Evolution with Local Search algorithm.\n\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size (int): The population size.\n            F_init (float): Initial scaling factor.\n            CR_init (float): Initial crossover rate.\n            local_search_prob (float): Probability of applying local search.\n            local_search_radius (float): Radius for the local search.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.CR = CR_init\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.success_F = []\n        self.success_CR = []\n        self.local_search_prob = local_search_prob\n        self.local_search_radius = local_search_radius\n\n    def initialize_population(self, func):\n        \"\"\"Initialize the population randomly within the bounds.\"\"\"\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.pop[np.argmin(self.fitness)]\n\n    def adapt_parameters(self):\n        \"\"\"Adapt F and CR based on successful values from previous generations.\"\"\"\n        if self.success_F:\n            self.F = np.mean(self.success_F)\n            if self.F > 1.0:\n                self.F = 1.0\n            elif self.F < 0.1:\n                self.F = 0.1  # keep F within reasonable bounds\n        else:\n            self.F = 0.5  # Reset F if no successful values\n\n        if self.success_CR:\n            self.CR = np.mean(self.success_CR)\n            if self.CR > 1.0:\n                self.CR = 1.0\n            elif self.CR < 0.1:\n                self.CR = 0.1 # keep CR within reasonable bounds\n        else:\n            self.CR = 0.7 # Reset CR if no successful values\n\n        self.success_F = []\n        self.success_CR = []\n\n    def repair(self, x, lb, ub):\n        \"\"\"Repair function to bring individuals back to the bounds.\"\"\"\n        return np.clip(x, lb, ub)\n    \n    def local_search(self, x, func):\n        \"\"\"Perform local search around the given solution.\"\"\"\n        x_new = x + np.random.uniform(-self.local_search_radius, self.local_search_radius, size=self.dim)\n        x_new = self.repair(x_new, func.bounds.lb, func.bounds.ub)\n        f_new = func(x_new)\n        self.eval_count += 1\n        return x_new, f_new\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using Adaptive Differential Evolution with Local Search.\n\n        Args:\n            func (callable): The function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            self.adapt_parameters()\n\n            for i in range(self.pop_size):\n                # Mutation (DE/rand/1)\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                a, b, c = idxs[0], idxs[1], idxs[2]\n                mutant = self.pop[a] + self.F * (self.pop[b] - self.pop[c])\n                mutant = self.repair(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Selection\n                f = func(trial)\n                self.eval_count += 1\n\n                if f < self.fitness[i]:\n                    self.success_F.append(self.F)\n                    self.success_CR.append(self.CR)\n                    self.fitness[i] = f\n                    self.pop[i] = trial\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n\n                    # Local Search\n                    if np.random.rand() < self.local_search_prob:\n                        x_local, f_local = self.local_search(trial, func)\n                        if f_local < self.f_opt:\n                            self.f_opt = f_local\n                            self.x_opt = x_local\n                        if f_local < f:\n                            self.fitness[i] = f_local\n                            self.pop[i] = x_local\n                \n\n                if self.eval_count >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveDELocalSearch scored 0.620 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["4fb17960-26b9-4396-b206-5374a5156745"], "operator": null, "metadata": {"aucs": [0.23605700218471737, 0.45880619200195016, 0.6090949252653819, 0.8333737505253334, 0.6810194938024519, 0.737751957279227, 0.5716980452739968, 0.558464174564933, 0.6755752301880411, 0.6102006442899697, 0.76145361679722, 0.9990101365011469, 0.3644367439032108, 0.6360678423672673, 0.8693387100010816, 0.7375157476358756, 0.506615918319208, 0.8008309104079242, 0.24287008402251975, 0.5165766778127454]}}
{"id": "973b636f-37b9-47f7-b873-cef468f445d3", "fitness": 0.3148928668238424, "name": "AdaptiveCauchyDE", "description": "An adaptive Differential Evolution with a Cauchy mutation operator and a shrinking population based on fitness diversity.", "code": "import numpy as np\n\nclass AdaptiveCauchyDE:\n    def __init__(self, budget=10000, dim=10, pop_size=100, F=0.5, CR=0.9, shrink_freq=50, diversity_threshold=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.shrink_freq = shrink_freq\n        self.diversity_threshold = diversity_threshold\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Mutation (Cauchy)\n                idx = np.random.choice(range(self.pop_size), 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[idx[0]], self.population[idx[1]], self.population[idx[2]]\n\n                # Cauchy mutation\n                cauchy_noise = self.F * np.random.standard_cauchy(size=self.dim)\n                mutant = self.population[i] + cauchy_noise * (x_r1 - x_r2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n\n            # Shrink population based on fitness diversity\n            if generation % self.shrink_freq == 0 and self.pop_size > 10:\n                fitness_std = np.std(self.fitness)\n                if fitness_std < self.diversity_threshold:\n                    # Sort population by fitness and keep only the best half\n                    sorted_indices = np.argsort(self.fitness)\n                    keep_count = max(10, self.pop_size // 2) # Ensure a minimum pop size of 10\n                    keep_indices = sorted_indices[:keep_count]\n                    \n                    self.population = self.population[keep_indices]\n                    self.fitness = self.fitness[keep_indices]\n                    self.pop_size = len(self.population)\n                    # Refill the population with new random individuals\n                    new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                    new_fitness = np.array([func(x) for x in new_individuals])\n                    self.budget -= self.pop_size\n\n                    self.population = new_individuals\n                    self.fitness = new_fitness\n                    \n\n                    self.f_opt = np.min(self.fitness)\n                    self.x_opt = self.population[np.argmin(self.fitness)]\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveCauchyDE scored 0.315 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0f38255a-72b8-4aef-b60e-01e1e664e452"], "operator": null, "metadata": {"aucs": [0.12817076462522803, 0.19928288902977176, 0.2850829694719095, 0.27436089074807446, 0.24374700216816703, 0.28032836451903964, 0.2595109009491149, 0.24027831151740942, 0.22544846370675364, 0.18502880579606673, 0.26021400727270017, 0.9990413036369203, 0.2532282949759391, 0.26368314321367703, 0.6496952920792012, 0.31647441035644663, 0.24891274923731144, 0.3367646768489533, 0.1762429158592118, 0.4723611804649527]}}
{"id": "0b4c9224-f494-441e-91bc-7aaf473e8ad9", "fitness": 0.5346136536281743, "name": "AdaptiveDE", "description": "Differential Evolution with self-adaptive parameters, archive for promising solutions, and a crowding distance-based selection to maintain diversity.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size_factor=5, archive_size_factor=2, F=0.5, CR=0.7, restart_trigger=0.1):\n        \"\"\"\n        Initialize the Adaptive Differential Evolution algorithm.\n\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size_factor (int): Factor to determine the population size (pop_size = pop_size_factor * dim).\n            archive_size_factor (int): Factor to determine the archive size (archive_size = archive_size_factor * dim).\n            F (float): The initial scaling factor.\n            CR (float): The initial crossover rate.\n            restart_trigger (float): Threshold for fitness improvement to trigger a population restart.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = int(pop_size_factor * dim)\n        self.archive_size = int(archive_size_factor * dim)\n        self.F = F\n        self.CR = CR\n        self.pop = None\n        self.fitness = None\n        self.archive = None\n        self.archive_fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.restart_trigger = restart_trigger\n        self.no_improvement_count = 0\n        self.max_no_improvement = 100  # Adjust as needed\n\n    def initialize_population(self, func):\n        \"\"\"Initialize the population randomly within the bounds.\"\"\"\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.pop[np.argmin(self.fitness)]\n        self.initial_f_opt = self.f_opt  # Store initial best fitness\n        \n        # Initialize archive\n        self.archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        self.archive_fitness = np.full(self.archive_size, np.inf) # Initially all archive fitness are infinite\n\n    def update_archive(self, trial, f):\n        \"\"\"Update the archive with the trial vector if it's better than the worst archive member.\"\"\"\n        if f < np.max(self.archive_fitness):\n            worst_index = np.argmax(self.archive_fitness)\n            self.archive[worst_index] = trial\n            self.archive_fitness[worst_index] = f\n\n    def crowding_distance(self, fitness):\n        \"\"\"Calculate crowding distance for each individual.\"\"\"\n        distances = np.zeros_like(fitness)\n        sorted_indices = np.argsort(fitness)\n        distances[sorted_indices[0]] = np.inf\n        distances[sorted_indices[-1]] = np.inf\n\n        for i in range(1, len(fitness) - 1):\n            distances[sorted_indices[i]] = (fitness[sorted_indices[i+1]] - fitness[sorted_indices[i-1]])\n        return distances\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using Adaptive Differential Evolution.\n\n        Args:\n            func (callable): The function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            old_f_opt = self.f_opt\n            F = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n            CR = np.clip(np.random.normal(self.CR, 0.1), 0.1, 1.0)\n            \n            for i in range(self.pop_size):\n                # Mutation (DE/rand/1 with archive)\n                use_archive = np.random.rand() < 0.1  # 10% chance to use archive\n                if use_archive and self.archive.size > 0:\n                    idx_a = np.random.randint(0, self.pop_size)\n                    idx_b = np.random.randint(0, self.archive_size)\n                    idx_c = np.random.randint(0, self.pop_size)\n                    mutant = self.pop[idx_a] + F * (self.archive[idx_b] - self.pop[idx_c])\n                else:\n                    idxs = np.random.choice(self.pop_size, 3, replace=False)\n                    a, b, c = idxs[0], idxs[1], idxs[2]\n                    mutant = self.pop[a] + F * (self.pop[b] - self.pop[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Selection\n                f = func(trial)\n                self.eval_count += 1\n\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.pop[i] = trial\n                    self.update_archive(trial, f)  # Update archive\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                        self.no_improvement_count = 0  # Reset counter on improvement\n                    else:\n                        self.no_improvement_count +=1\n                else:\n                    self.no_improvement_count += 1\n                    self.update_archive(trial, f) # Still update if not better, might kick out bad archive member\n\n                if self.eval_count >= self.budget:\n                    break\n            \n            # Fitness-aware parameter adaptation\n            if self.f_opt < old_f_opt:\n                self.F = 0.9 * self.F + 0.1  # Increase F if improvement\n                self.CR = 0.9 * self.CR + 0.1 # Increase CR if improvement\n            else:\n                self.F = 0.9 * self.F - 0.1 # Decrease F if no improvement\n                self.CR = 0.9 * self.CR - 0.1 # Decrease CR if no improvement\n                self.F = np.clip(self.F, 0.1, 1.0)\n                self.CR = np.clip(self.CR, 0.1, 1.0)\n\n            # Restart mechanism\n            if (self.initial_f_opt - self.f_opt) / self.initial_f_opt < self.restart_trigger and self.no_improvement_count > self.max_no_improvement:\n                self.initialize_population(func)  # Restart population\n                # Reset archive as well after population restart\n                self.archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n                self.archive_fitness = np.full(self.archive_size, np.inf)\n\n            # Crowding distance-based selection to maintain population diversity.\n            distances = self.crowding_distance(self.fitness)\n            min_distance_index = np.argmin(distances)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveDE scored 0.535 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0d6fa85c-d991-4f21-9f9a-229d5a8f9278"], "operator": null, "metadata": {"aucs": [0.16719041113768707, 0.2500013002699647, 0.607778555297439, 0.8769596541672053, 0.24785739167632725, 0.71189870036241, 0.3993676274141259, 0.5510461898335719, 0.773521264206414, 0.22551301028126536, 0.38817662063828584, 0.999157271390911, 0.3149883394185691, 0.3882280254142204, 0.7861785024477692, 0.8657460661621479, 0.4562247969991293, 0.9066850448110206, 0.2693458122359792, 0.5064084883990427]}}
{"id": "bdc76fb9-0748-4af6-90d1-dca39bf21e50", "fitness": 0.7065325945243703, "name": "HybridDE", "description": "Enhanced Hybrid DE with adaptive aging, dynamic F/CR adaptation based on success history, and a dual-strategy population reduction favoring diversity preservation.", "code": "import numpy as np\n\nclass HybridDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, adapt_freq=10, ring_neighbors=3, reduction_factor=0.95, age_limit=50, success_rate_memory=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.adapt_freq = adapt_freq\n        self.ring_neighbors = ring_neighbors\n        self.reduction_factor = reduction_factor #Population size reduction factor\n        self.min_pop_size = 10\n        self.age_limit = age_limit #Maximum age before being replaced\n        self.ages = np.zeros(pop_size, dtype=int) #Age of each individual\n        self.success_rate_memory = success_rate_memory\n        self.successful_F = np.ones(success_rate_memory) * F\n        self.successful_CR = np.ones(success_rate_memory) * CR\n        self.success_idx = 0\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Ring Topology Selection\n                neighbors = [(i - j) % self.pop_size for j in range(1, self.ring_neighbors + 1)]\n                neighbor_fitnesses = self.fitness[neighbors]\n                best_neighbor_idx = neighbors[np.argmin(neighbor_fitnesses)]\n                \n                # Mutation - Use best neighbor in ring\n                idx = np.random.choice(range(self.pop_size), 2, replace=False)\n                x_rand1, x_rand2 = self.population[idx[0]], self.population[idx[1]]\n                mutant = self.population[i] + self.F * (self.population[best_neighbor_idx] - self.population[i]) + self.F * (x_rand1 - x_rand2)\n\n                # Probabilistic Repair\n                repair_prob = 0.1 #Probability of repairing a dimension\n                for j in range(self.dim):\n                    if np.random.rand() < repair_prob:\n                        if mutant[j] < func.bounds.lb[j] or mutant[j] > func.bounds.ub[j]:\n                            mutant[j] = np.random.uniform(func.bounds.lb[j], func.bounds.ub[j])\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Orthogonal Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n                    self.ages[i] = 0 #Reset age after improvement\n                    self.successful_F[self.success_idx] = self.F\n                    self.successful_CR[self.success_idx] = self.CR\n                    self.success_idx = (self.success_idx + 1) % self.success_rate_memory\n                else:\n                    self.ages[i] += 1 #Increase age if no improvement\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n\n            #Adapt F and CR using success history\n            if generation % self.adapt_freq == 0:\n                self.F = np.clip(np.mean(self.successful_F), 0.1, 1.0)\n                self.CR = np.clip(np.mean(self.successful_CR), 0.1, 1.0)\n                #Add noise to prevent stagnation\n                self.F = np.clip(np.random.normal(self.F, 0.05), 0.1, 1.0)\n                self.CR = np.clip(np.random.normal(self.CR, 0.05), 0.1, 1.0)\n            \n            # Population size reduction\n            if generation % (self.adapt_freq*5) == 0 and self.pop_size > self.min_pop_size:\n                num_to_reduce = max(1, int(self.pop_size * (1 - self.reduction_factor)))\n                \n                #Dual strategy for population reduction: remove both old and similar individuals\n                age_rank = np.argsort(self.ages)[::-1] #Sort ages in descending order\n\n                #Calculate diversity (Euclidean distance from the centroid)\n                centroid = np.mean(self.population, axis=0)\n                distances = np.linalg.norm(self.population - centroid, axis=1)\n                diversity_rank = np.argsort(distances)\n\n                #Combine age and diversity ranks: prioritize removing old and undiverse individuals\n                combined_rank = np.concatenate([age_rank[:num_to_reduce//2], diversity_rank[:num_to_reduce - num_to_reduce//2]])\n                worst_indices = np.unique(combined_rank)\n                \n                if len(worst_indices) > num_to_reduce:\n                  worst_indices = worst_indices[:num_to_reduce]\n\n                remaining_indices = np.array([i for i in range(self.pop_size) if i not in worst_indices])\n                \n                self.population = self.population[remaining_indices]\n                self.fitness = self.fitness[remaining_indices]\n                self.ages = self.ages[remaining_indices]\n                self.pop_size = len(self.population)\n                self.ages = np.zeros(self.pop_size, dtype=int) #Reset ages after removing individuals\n\n            # Re-initialize old individuals:\n            for i in range(self.pop_size):\n                if self.ages[i] > self.age_limit:\n                    self.population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                    self.fitness[i] = func(self.population[i])\n                    self.budget -= 1\n                    self.ages[i] = 0\n\n                    if self.fitness[i] < self.f_opt:\n                        self.f_opt = self.fitness[i]\n                        self.x_opt = self.population[i]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm HybridDE scored 0.707 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b2a8e8d4-2d0a-4d76-9e5c-7d605d6bace4"], "operator": null, "metadata": {"aucs": [0.21919424139281152, 0.8324193182289126, 0.673009552640929, 0.8963954223092238, 0.8161790792424908, 0.8583297599394004, 0.6506858547498574, 0.8076431255209141, 0.8497417933758324, 0.24484183281836758, 0.9214686376220321, 0.9894069402072732, 0.6022221813205211, 0.7794426609459534, 0.929060473602513, 0.8465515919587301, 0.5291422115108313, 0.8991712010772193, 0.25990010590672474, 0.5258459061168697]}}
{"id": "69af1366-77c5-44f4-a763-416c7865705b", "fitness": 0.6061187416586175, "name": "HybridDE", "description": "Hybrid Differential Evolution with simplified adaptive parameter control, ring topology, and age-based replacement for diversity.", "code": "import numpy as np\n\nclass HybridDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, ring_neighbors=3, age_limit=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.ring_neighbors = ring_neighbors\n        self.age_limit = age_limit #Maximum age before being replaced\n        self.ages = np.zeros(pop_size, dtype=int) #Age of each individual\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Ring Topology Selection\n                neighbors = [(i - j) % self.pop_size for j in range(1, self.ring_neighbors + 1)]\n                neighbor_fitnesses = self.fitness[neighbors]\n                best_neighbor_idx = neighbors[np.argmin(neighbor_fitnesses)]\n                \n                # Mutation - Use best neighbor in ring\n                idx = np.random.choice(range(self.pop_size), 2, replace=False)\n                x_rand1, x_rand2 = self.population[idx[0]], self.population[idx[1]]\n                mutant = self.population[i] + self.F * (self.population[best_neighbor_idx] - self.population[i]) + self.F * (x_rand1 - x_rand2)\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n                    self.ages[i] = 0 #Reset age after improvement\n                else:\n                    self.ages[i] += 1 #Increase age if no improvement\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n            \n            # Re-initialize old individuals:\n            for i in range(self.pop_size):\n                if self.ages[i] > self.age_limit:\n                    self.population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                    self.fitness[i] = func(self.population[i])\n                    self.budget -= 1\n                    self.ages[i] = 0\n\n                    if self.fitness[i] < self.f_opt:\n                        self.f_opt = self.fitness[i]\n                        self.x_opt = self.population[i]\n\n            # Simplified Adaptive F and CR\n            self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 0.9)\n            self.CR = np.clip(np.random.normal(0.9, 0.1), 0.1, 0.9)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm HybridDE scored 0.606 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b2a8e8d4-2d0a-4d76-9e5c-7d605d6bace4"], "operator": null, "metadata": {"aucs": [0.25414955466984435, 0.6606684359586894, 0.724492490698772, 0.8758714289003209, 0.8226798065705745, 0.8526902355836316, 0.6714404233765707, 0.7802644881529233, 0.8081804432648443, 0.21686885106862064, 0]}}
{"id": "adc30428-73e7-4a0d-9712-f0715c31a5bc", "fitness": 0.6123526461791096, "name": "HybridDE", "description": "Hybrid Differential Evolution with enhanced parameter adaptation using success history and improved exploitation via a smaller ring neighborhood.", "code": "import numpy as np\n\nclass HybridDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, adapt_freq=10, ring_neighbors=1, F_history_size=10, CR_history_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.adapt_freq = adapt_freq\n        self.ring_neighbors = ring_neighbors\n        self.F_history = np.full(F_history_size, F)\n        self.CR_history = np.full(CR_history_size, CR)\n        self.F_history_idx = 0\n        self.CR_history_idx = 0\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Ring Topology Selection (Simplified)\n                neighbors = [(i - j) % self.pop_size for j in range(1, self.ring_neighbors + 1)]\n                neighbor_fitnesses = self.fitness[neighbors]\n                best_neighbor_idx = neighbors[np.argmin(neighbor_fitnesses)]\n                \n                # Mutation - Use best neighbor in ring\n                idx = np.random.choice(range(self.pop_size), 2, replace=False)\n                x_rand1, x_rand2 = self.population[idx[0]], self.population[idx[1]]\n                mutant = self.population[i] + self.F * (self.population[best_neighbor_idx] - self.population[i]) + self.F * (x_rand1 - x_rand2)\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    # Update history\n                    self.F_history[self.F_history_idx] = self.F\n                    self.CR_history[self.CR_history_idx] = self.CR\n                    self.F_history_idx = (self.F_history_idx + 1) % len(self.F_history)\n                    self.CR_history_idx = (self.CR_history_idx + 1) % len(self.CR_history)\n\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n\n            #Adapt F and CR using history\n            if generation % self.adapt_freq == 0:\n                self.F = np.clip(np.random.normal(np.mean(self.F_history), np.std(self.F_history)), 0.1, 1.0) if np.std(self.F_history) > 0 else 0.5\n                self.CR = np.clip(np.random.normal(np.mean(self.CR_history), np.std(self.CR_history)), 0.1, 1.0) if np.std(self.CR_history) > 0 else 0.9\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm HybridDE scored 0.612 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0f38255a-72b8-4aef-b60e-01e1e664e452"], "operator": null, "metadata": {"aucs": [0.20579005803016615, 0.5964074663254006, 0.6272347009997843, 0.8518755287338553, 0.6622583347752147, 0.7826878295501416, 0.3622765187860083, 0.6562550155504769, 0.7417775581754689, 0.29862519723412095, 0.8517672556919853, 0.9989725827596582, 0.6846409051035882, 0.6502196409802848, 0.8919533708007596, 0.3299394770471945, 0.4986429098952436, 0.8485408629375504, 0.20742364436157246, 0.49976406584371647]}}
{"id": "77e75d85-5c1a-465c-99e5-a44bd9042502", "fitness": 0.0, "name": "HybridDE", "description": "Simplified Differential Evolution with self-adaptive F/CR, enhanced exploitation using best individual guidance, and reduced random replacement for diversity.", "code": "import numpy as np\n\nclass HybridDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                F = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n                CR = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0)\n\n                # Mutation with best individual guidance\n                idx = np.random.choice(range(self.pop_size), 2, replace=False)\n                x_rand1, x_rand2 = self.population[idx[0]], self.population[idx[1]]\n                mutant = self.x_opt + F * (x_rand1 - x_rand2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n            \n            # Replace a smaller number of individuals (elitism)\n            replace_indices = np.random.choice(range(self.pop_size), max(1, self.pop_size // 10), replace=False) # Reduced replacement\n            for i in replace_indices:\n                self.population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                self.fitness[i] = func(self.population[i])\n                self.budget -= 1\n\n                if self.fitness[i] < self.f_opt:\n                    self.f_opt = self.fitness[i]\n                    self.x_opt = self.population[i]\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm HybridDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9ea084b4-6ae8-4895-9807-8bcbbeb356d4"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "560ef57e-ba5e-40b2-93ea-9c1531dd9415", "fitness": -Infinity, "name": "HybridDE", "description": "Simplified Differential Evolution with adaptive F/CR and periodic population replacement using a single random element difference.", "code": "import numpy as np\n\nclass HybridDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, adapt_freq=10, replace_freq=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.adapt_freq = adapt_freq\n        self.replace_freq = replace_freq #Frequency of replacing individuals\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Mutation\n                idx = np.random.choice(range(self.pop_size), 3, replace=False)\n                x_rand1, x_rand2, x_rand3 = self.population[idx[0]], self.population[idx[1]], self.population[idx[2]]\n                mutant = x_rand1 + self.F * (x_rand2 - x_rand3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                trial = np.where(cross_points, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n            \n            # Adapt F and CR\n            if generation % self.adapt_freq == 0:\n                self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n                self.CR = np.clip(np.random.normal(self.CR, 0.1), 0.1, 1.0)\n            \n            # Replace individuals - simplified replacement\n            if generation % self.replace_freq == 0:\n                replace_indices = np.random.choice(range(self.pop_size), self.pop_size // 5, replace=False)\n                for i in replace_indices:\n                    j = np.random.randint(self.pop_size)\n                    k = np.random.randint(self.dim)\n                    self.population[i] = self.population[j].copy()\n                    self.population[i][k] = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                    self.fitness[i] = func(self.population[i])\n                    self.budget -= 1\n\n                    if self.fitness[i] < self.f_opt:\n                        self.f_opt = self.fitness[i]\n                        self.x_opt = self.population[i]\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "An exception occurred: setting an array element with a sequence..", "error": "", "parent_ids": ["9ea084b4-6ae8-4895-9807-8bcbbeb356d4"], "operator": null, "metadata": {}}
{"id": "6043904b-2bc1-40be-8593-29b55fe09b31", "fitness": 0.4840018561932468, "name": "AdaptiveDEOrthogonalLocalSearch", "description": "Adaptive Differential Evolution with orthogonal crossover, success-history adaptation of F/CR, and enhanced local search with adaptive radius.", "code": "import numpy as np\n\nclass AdaptiveDEOrthogonalLocalSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_init=0.5, CR_init=0.7, \n                 local_search_prob=0.1, local_search_radius_init=0.1, local_search_radius_decay=0.99):\n        \"\"\"\n        Initialize the Adaptive Differential Evolution with Orthogonal Crossover and Local Search algorithm.\n\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size (int): The population size.\n            F_init (float): Initial scaling factor.\n            CR_init (float): Initial crossover rate.\n            local_search_prob (float): Probability of applying local search.\n            local_search_radius_init (float): Initial radius for the local search.\n            local_search_radius_decay (float): Decay rate for the local search radius.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.CR = CR_init\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.success_F = []\n        self.success_CR = []\n        self.local_search_prob = local_search_prob\n        self.local_search_radius = local_search_radius_init\n        self.local_search_radius_init = local_search_radius_init\n        self.local_search_radius_decay = local_search_radius_decay\n\n    def initialize_population(self, func):\n        \"\"\"Initialize the population randomly within the bounds.\"\"\"\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.pop[np.argmin(self.fitness)]\n\n    def adapt_parameters(self):\n        \"\"\"Adapt F and CR based on successful values from previous generations using Lehmer mean.\"\"\"\n        if self.success_F:\n            self.F = np.mean(self.success_F)\n            if self.F > 1.0:\n                self.F = 1.0\n            elif self.F < 0.1:\n                self.F = 0.1  # keep F within reasonable bounds\n        else:\n            self.F = 0.5  # Reset F if no successful values\n\n        if self.success_CR:\n            self.CR = np.mean(self.success_CR)\n            if self.CR > 1.0:\n                self.CR = 1.0\n            elif self.CR < 0.1:\n                self.CR = 0.1 # keep CR within reasonable bounds\n        else:\n            self.CR = 0.7 # Reset CR if no successful values\n\n        self.success_F = []\n        self.success_CR = []\n\n    def repair(self, x, lb, ub):\n        \"\"\"Repair function to bring individuals back to the bounds.\"\"\"\n        return np.clip(x, lb, ub)\n    \n    def local_search(self, x, func):\n        \"\"\"Perform local search around the given solution with shrinking radius.\"\"\"\n        x_new = x + np.random.uniform(-self.local_search_radius, self.local_search_radius, size=self.dim)\n        x_new = self.repair(x_new, func.bounds.lb, func.bounds.ub)\n        f_new = func(x_new)\n        self.eval_count += 1\n        self.local_search_radius *= self.local_search_radius_decay  # Reduce search radius\n        return x_new, f_new\n\n    def orthogonal_crossover(self, mutant, target):\n        \"\"\"Perform orthogonal crossover between mutant and target vectors.\"\"\"\n        offspring = np.copy(target)\n        for j in range(self.dim):\n            if np.random.rand() < self.CR:\n                offspring[j] = mutant[j]\n        return offspring\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using Adaptive Differential Evolution with Orthogonal Crossover and Local Search.\n\n        Args:\n            func (callable): The function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            self.adapt_parameters()\n\n            for i in range(self.pop_size):\n                # Mutation (DE/rand/1)\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                a, b, c = idxs[0], idxs[1], idxs[2]\n                mutant = self.pop[a] + self.F * (self.pop[b] - self.pop[c])\n                mutant = self.repair(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover (Orthogonal Crossover)\n                trial = self.orthogonal_crossover(mutant, self.pop[i])\n\n\n                # Selection\n                f = func(trial)\n                self.eval_count += 1\n\n                if f < self.fitness[i]:\n                    self.success_F.append(self.F)\n                    self.success_CR.append(self.CR)\n                    self.fitness[i] = f\n                    self.pop[i] = trial\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                        self.local_search_radius = self.local_search_radius_init # Reset radius upon finding a new best\n\n                    # Local Search\n                    if np.random.rand() < self.local_search_prob:\n                        x_local, f_local = self.local_search(trial, func)\n                        if f_local < self.f_opt:\n                            self.f_opt = f_local\n                            self.x_opt = x_local\n                            self.local_search_radius = self.local_search_radius_init # Reset radius upon finding a new best by local search\n                        if f_local < f:\n                            self.fitness[i] = f_local\n                            self.pop[i] = x_local\n                \n\n                if self.eval_count >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveDEOrthogonalLocalSearch scored 0.484 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["28eedacc-11d0-48ea-a425-13b8923c6147"], "operator": null, "metadata": {"aucs": [0.21598847257002707, 0.35472944898227465, 0.5381868769039284, 0.8309170547506398, 0.6898473114286958, 0.7583438287171627, 0]}}
{"id": "96c1a262-eb6d-401f-a0c0-8500d3dfe928", "fitness": 0.6254625955176933, "name": "HybridDE", "description": "Simplified Hybrid DE with success-history based parameter adaptation, probabilistic repair, and age-based re-initialization, focusing on efficiency and robustness.", "code": "import numpy as np\n\nclass HybridDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, CR=0.9, adapt_freq=10, age_limit=50, success_rate_memory=10, repair_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.adapt_freq = adapt_freq\n        self.age_limit = age_limit\n        self.success_rate_memory = success_rate_memory\n        self.repair_prob = repair_prob\n        self.ages = np.zeros(pop_size, dtype=int)\n        self.successful_F = np.ones(success_rate_memory) * F\n        self.successful_CR = np.ones(success_rate_memory) * CR\n        self.success_idx = 0\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Mutation\n                idx = np.random.choice(range(self.pop_size), 3, replace=False)\n                x_rand1, x_rand2, x_rand3 = self.population[idx[0]], self.population[idx[1]], self.population[idx[2]]\n                mutant = x_rand1 + self.F * (x_rand2 - x_rand3)\n\n                # Probabilistic Repair\n                for j in range(self.dim):\n                    if np.random.rand() < self.repair_prob:\n                        if mutant[j] < func.bounds.lb[j] or mutant[j] > func.bounds.ub[j]:\n                            mutant[j] = np.random.uniform(func.bounds.lb[j], func.bounds.ub[j])\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n                    self.ages[i] = 0\n                    self.successful_F[self.success_idx] = self.F\n                    self.successful_CR[self.success_idx] = self.CR\n                    self.success_idx = (self.success_idx + 1) % self.success_rate_memory\n                else:\n                    self.ages[i] += 1\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n\n            # Adapt F and CR\n            if generation % self.adapt_freq == 0:\n                self.F = np.clip(np.mean(self.successful_F), 0.1, 1.0)\n                self.CR = np.clip(np.mean(self.successful_CR), 0.1, 1.0)\n                self.F = np.clip(np.random.normal(self.F, 0.05), 0.1, 1.0)\n                self.CR = np.clip(np.random.normal(self.CR, 0.05), 0.1, 1.0)\n            \n            # Re-initialize old individuals\n            for i in range(self.pop_size):\n                if self.ages[i] > self.age_limit:\n                    self.population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                    self.fitness[i] = func(self.population[i])\n                    self.budget -= 1\n                    self.ages[i] = 0\n\n                    if self.fitness[i] < self.f_opt:\n                        self.f_opt = self.fitness[i]\n                        self.x_opt = self.population[i]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm HybridDE scored 0.625 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["bdc76fb9-0748-4af6-90d1-dca39bf21e50"], "operator": null, "metadata": {"aucs": [0.3618118906879003, 0.7567643244563833, 0.6607784756398658, 0.8602369471355703, 0.8167949751951351, 0.8418311436896513, 0.5967242374317676, 0.571810552471487, 0.7878734084691713, 0]}}
{"id": "a7e82604-797b-487c-8a9d-29758e8baa3f", "fitness": 0.5786510916012457, "name": "AdaptiveDELocalSearch", "description": "Simplified Adaptive Differential Evolution with success-history based parameter adaptation and occasional local search.", "code": "import numpy as np\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_init=0.5, CR_init=0.7, local_search_prob=0.1, local_search_radius=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.CR = CR_init\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.success_F = []\n        self.success_CR = []\n        self.local_search_prob = local_search_prob\n        self.local_search_radius = local_search_radius\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.pop[np.argmin(self.fitness)]\n\n    def adapt_parameters(self):\n        if self.success_F:\n            self.F = np.clip(np.mean(self.success_F), 0.1, 1.0)\n        else:\n            self.F = 0.5\n\n        if self.success_CR:\n            self.CR = np.clip(np.mean(self.success_CR), 0.1, 1.0)\n        else:\n            self.CR = 0.7\n\n        self.success_F = []\n        self.success_CR = []\n\n    def repair(self, x, lb, ub):\n        return np.clip(x, lb, ub)\n    \n    def local_search(self, x, func):\n        x_new = x + np.random.uniform(-self.local_search_radius, self.local_search_radius, size=self.dim)\n        x_new = self.repair(x_new, func.bounds.lb, func.bounds.ub)\n        f_new = func(x_new)\n        self.eval_count += 1\n        return x_new, f_new\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            self.adapt_parameters()\n\n            for i in range(self.pop_size):\n                # Mutation (DE/rand/1)\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                a, b, c = idxs[0], idxs[1], idxs[2]\n                mutant = self.pop[a] + self.F * (self.pop[b] - self.pop[c])\n                mutant = self.repair(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Selection\n                f = func(trial)\n                self.eval_count += 1\n\n                if f < self.fitness[i]:\n                    self.success_F.append(self.F)\n                    self.success_CR.append(self.CR)\n                    self.fitness[i] = f\n                    self.pop[i] = trial\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n\n                # Local Search\n                if np.random.rand() < self.local_search_prob:\n                    x_local, f_local = self.local_search(self.pop[i], func)\n                    if f_local < self.fitness[i]:\n                        self.fitness[i] = f_local\n                        self.pop[i] = x_local\n                        if f_local < self.f_opt:\n                            self.f_opt = f_local\n                            self.x_opt = x_local\n\n                if self.eval_count >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveDELocalSearch scored 0.579 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["28eedacc-11d0-48ea-a425-13b8923c6147"], "operator": null, "metadata": {"aucs": [0.21408085970641955, 0.35538298350624964, 0.5514720803964437, 0.823960680679761, 0.6981649310918943, 0.7314924230944729, 0.5179842078105014, 0.5561337612171453, 0.6809431957254736, 0.5466990168489803, 0.7790574248742423, 0.9987910884932654, 0.3556097822306794, 0.6075070122493204, 0.8411380176950831, 0]}}
{"id": "45977341-c42a-40fa-95a5-a65e5ebad198", "fitness": 0.3940857666341836, "name": "SelfOrganizingPSO", "description": "A self-organizing particle optimization algorithm using a velocity clamping mechanism and a dynamic inertia weight strategy modulated by the population diversity.", "code": "import numpy as np\n\nclass SelfOrganizingPSO:\n    def __init__(self, budget=10000, dim=10, pop_size=50, inertia_max=0.9, inertia_min=0.2, \n                 cognitive_coeff=2.0, social_coeff=2.0, velocity_clamp=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.inertia_max = inertia_max\n        self.inertia_min = inertia_min\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.velocity_clamp = velocity_clamp #clamp percentage of the search space\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize particles and velocities\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.particles = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        self.velocities = np.random.uniform(-(ub - lb) * self.velocity_clamp, (ub - lb) * self.velocity_clamp, size=(self.pop_size, self.dim))\n        \n        # Evaluate initial fitness\n        self.fitness = np.array([func(x) for x in self.particles])\n        self.budget -= self.pop_size\n        \n        # Initialize personal best positions and fitness\n        self.personal_best_positions = self.particles.copy()\n        self.personal_best_fitness = self.fitness.copy()\n        \n        # Find global best\n        self.global_best_index = np.argmin(self.fitness)\n        self.global_best_position = self.particles[self.global_best_index].copy()\n        self.f_opt = self.fitness[self.global_best_index]\n        self.x_opt = self.global_best_position.copy()\n        \n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            \n            # Calculate diversity (standard deviation of particle positions)\n            diversity = np.mean(np.std(self.particles, axis=0))\n            \n            # Adjust inertia weight dynamically based on diversity\n            inertia = self.inertia_max - (self.inertia_max - self.inertia_min) * (diversity / ((ub - lb)/2))\n            inertia = np.clip(inertia, self.inertia_min, self.inertia_max)\n\n            for i in range(self.pop_size):\n                # Update velocity\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                \n                cognitive_component = self.cognitive_coeff * r1 * (self.personal_best_positions[i] - self.particles[i])\n                social_component = self.social_coeff * r2 * (self.global_best_position - self.particles[i])\n                \n                self.velocities[i] = inertia * self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = np.clip(self.velocities[i], -(ub - lb) * self.velocity_clamp, (ub - lb) * self.velocity_clamp)\n                \n                # Update position\n                self.particles[i] = self.particles[i] + self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], lb, ub)\n                \n                # Evaluate fitness\n                fitness = func(self.particles[i])\n                self.budget -= 1\n                \n                # Update personal best\n                if fitness < self.personal_best_fitness[i]:\n                    self.personal_best_fitness[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i].copy()\n                    \n                    # Update global best\n                    if fitness < self.f_opt:\n                        self.f_opt = fitness\n                        self.x_opt = self.particles[i].copy()\n                        self.global_best_position = self.particles[i].copy()\n                        self.global_best_index = i\n\n                if self.budget <= 0:\n                    break\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm SelfOrganizingPSO scored 0.394 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9ea084b4-6ae8-4895-9807-8bcbbeb356d4"], "operator": null, "metadata": {"aucs": [0.15259890854951796, 0.35870463142680264, 0.421825700464197, 0.6009487408247465, 0.2850756425015579, 0.27766303503197465, 0.29898581465567886, 0.35262871776467497, 0.36902678924153853, 0.2310668257679347, 0.4972136050632132, 0.9963180396326743, 0.22873529347927002, 0.2692978026212516, 0.6470419606207448, 0.36414938749932635, 0.32081048183025573, 0.5020277622234512, 0.19903376309519127, 0.5085624303896703]}}
{"id": "363e6e08-08c2-4389-9da4-f4994601ace1", "fitness": 0.6217729028289274, "name": "HybridDE", "description": "Simplified Differential Evolution with adaptive F/CR and periodic population shuffling for diversity, focusing on efficiency and parameter control.", "code": "import numpy as np\n\nclass HybridDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, CR=0.9, adapt_freq=10, shuffle_freq=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.adapt_freq = adapt_freq\n        self.shuffle_freq = shuffle_freq\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in pop])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = pop[np.argmin(fitness)]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Mutation & Crossover\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                mutant = pop[idxs[0]] + self.F * (pop[idxs[1]] - pop[idxs[2]])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                \n                cross_mask = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_mask):\n                    cross_mask[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_mask, mutant, pop[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                if self.budget <= 0:\n                    break\n\n            # Adapt F/CR\n            if generation % self.adapt_freq == 0:\n                self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n                self.CR = np.clip(np.random.normal(self.CR, 0.1), 0.1, 1.0)\n\n            # Shuffle population\n            if generation % self.shuffle_freq == 0:\n                np.random.shuffle(pop)\n                fitness = np.array([func(x) for x in pop])\n                self.budget -= self.pop_size #Recalculating all the fitness after shuffling is costly. Can improve this.\n                \n                min_fitness_idx = np.argmin(fitness)\n                if fitness[min_fitness_idx] < self.f_opt:\n                    self.f_opt = fitness[min_fitness_idx]\n                    self.x_opt = pop[min_fitness_idx]\n                if self.budget <= 0:\n                    break\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm HybridDE scored 0.622 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9ea084b4-6ae8-4895-9807-8bcbbeb356d4"], "operator": null, "metadata": {"aucs": [0.23117611836206187, 0.35813284165992376, 0.489028141809152, 0.8946133968585213, 0.744797116693193, 0.7933008965146535, 0.45818976144755663, 0.6507566826814166, 0.7916190367354481, 0.7161066012848707, 0.903746022564744, 0.994767373315187, 0.3281165150979408, 0.7251624274858898, 0.7681901247232814, 0.5565614629924258, 0.3940977740318229, 0.8610115047889235, 0.24590096523542793, 0.5301832922961061]}}
{"id": "31be346d-952c-4901-a86b-d859149304e2", "fitness": 0.672715847678821, "name": "AdaptiveDELocalSearch", "description": "Simplified Adaptive Differential Evolution with dynamic F/CR adaptation and probabilistic local search for refinement, reducing complexity for efficiency.", "code": "import numpy as np\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, CR_init=0.7, local_search_prob=0.05, local_search_radius=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.CR = CR_init\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.local_search_prob = local_search_prob\n        self.local_search_radius = local_search_radius\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.pop[np.argmin(self.fitness)]\n\n    def adapt_parameters(self, success_F, success_CR):\n        if success_F:\n            self.F = np.mean(success_F)\n            self.F = np.clip(self.F, 0.1, 0.9)\n        if success_CR:\n            self.CR = np.mean(success_CR)\n            self.CR = np.clip(self.CR, 0.1, 0.9)\n\n    def repair(self, x, lb, ub):\n        return np.clip(x, lb, ub)\n    \n    def local_search(self, x, func):\n        x_new = x + np.random.uniform(-self.local_search_radius, self.local_search_radius, size=self.dim)\n        x_new = self.repair(x_new, func.bounds.lb, func.bounds.ub)\n        f_new = func(x_new)\n        self.eval_count += 1\n        return x_new, f_new\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            success_F = []\n            success_CR = []\n\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                a, b, c = self.pop[idxs[0]], self.pop[idxs[1]], self.pop[idxs[2]]\n                mutant = a + self.F * (b - c)\n                mutant = self.repair(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Selection\n                f = func(trial)\n                self.eval_count += 1\n\n                if f < self.fitness[i]:\n                    success_F.append(self.F)\n                    success_CR.append(self.CR)\n                    self.fitness[i] = f\n                    self.pop[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                \n                    if np.random.rand() < self.local_search_prob:\n                        x_local, f_local = self.local_search(trial, func)\n                        if f_local < self.f_opt:\n                            self.f_opt = f_local\n                            self.x_opt = x_local\n                        if f_local < f:\n                            self.fitness[i] = f_local\n                            self.pop[i] = x_local\n\n                if self.eval_count >= self.budget:\n                    break\n            \n            self.adapt_parameters(success_F, success_CR)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveDELocalSearch scored 0.673 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["28eedacc-11d0-48ea-a425-13b8923c6147"], "operator": null, "metadata": {"aucs": [0.23392635168185005, 0.6041146374024909, 0.6741299563644427, 0.8549882690033556, 0.756524934674641, 0.8114251219737836, 0.657114417973425, 0.6554433260151274, 0.7894694538587071, 0.6088717608763896, 0.7847490351491334, 0.9952301304735603, 0.37109249883361173, 0.7007275472913288, 0.9095135430553731, 0.8206994581516182, 0.6227892929434833, 0.8561660586642186, 0.22875666094470215, 0.5185844982451769]}}
{"id": "3bdf392f-c40c-48cf-8d09-f3f5102ea9c6", "fitness": 0.538813837216148, "name": "SelfOrganizingPSO", "description": "A self-organizing particle swarm optimizer that adapts its parameters based on a combination of local and global search success, combined with orthogonal learning for enhanced exploration.", "code": "import numpy as np\n\nclass SelfOrganizingPSO:\n    def __init__(self, budget=10000, dim=10, pop_size=50, w_init=0.9, c1_init=2.0, c2_init=2.0, adapt_freq=10, ortho_freq = 20, exploration_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w_init = w_init  # Inertia weight\n        self.c1_init = c1_init  # Cognitive coefficient\n        self.c2_init = c2_init  # Social coefficient\n        self.adapt_freq = adapt_freq\n        self.ortho_freq = ortho_freq\n        self.exploration_rate = exploration_rate\n        self.particles = np.random.uniform(-5, 5, size=(self.pop_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))\n        self.personal_best_positions = self.particles.copy()\n        self.personal_best_fitnesses = np.full(self.pop_size, np.inf)\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.w = np.full(self.pop_size, w_init)\n        self.c1 = np.full(self.pop_size, c1_init)\n        self.c2 = np.full(self.pop_size, c2_init)\n        self.success_memory = np.zeros(self.pop_size) #Track recent success\n        self.learning_rate = 0.1 #How quickly parameters adapt\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # Evaluate initial population\n        fitness = np.array([func(x) for x in self.particles])\n        self.budget -= self.pop_size\n\n        # Update personal bests\n        for i in range(self.pop_size):\n            if fitness[i] < self.personal_best_fitnesses[i]:\n                self.personal_best_fitnesses[i] = fitness[i]\n                self.personal_best_positions[i] = self.particles[i].copy()\n\n        # Update global best\n        self.global_best_fitness = np.min(self.personal_best_fitnesses)\n        self.global_best_position = self.personal_best_positions[np.argmin(self.personal_best_fitnesses)].copy()\n        self.f_opt = self.global_best_fitness\n        self.x_opt = self.global_best_position\n        \n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Update velocity\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] = (self.w[i] * self.velocities[i] +\n                                      self.c1[i] * r1 * (self.personal_best_positions[i] - self.particles[i]) +\n                                      self.c2[i] * r2 * (self.global_best_position - self.particles[i]))\n                \n                #Velocity clipping\n                v_max = 1.0\n                self.velocities[i] = np.clip(self.velocities[i], -v_max, v_max)\n\n                # Update position\n                new_particle = self.particles[i] + self.velocities[i]\n                \n                #Boundary Handling (random re-initialization within bounds)\n                for j in range(self.dim):\n                    if new_particle[j] < -5 or new_particle[j] > 5:\n                        new_particle[j] = np.random.uniform(-5, 5) #Reinitialize\n\n                self.particles[i] = new_particle\n\n                # Evaluate fitness\n                f = func(self.particles[i])\n                self.budget -= 1\n\n                # Update personal best\n                if f < self.personal_best_fitnesses[i]:\n                    self.personal_best_fitnesses[i] = f\n                    self.personal_best_positions[i] = self.particles[i].copy()\n                    self.success_memory[i] = 1 #Mark success\n                else:\n                    self.success_memory[i] = 0 #Mark failure\n\n                # Update global best\n                if f < self.global_best_fitness:\n                    self.global_best_fitness = f\n                    self.global_best_position = self.particles[i].copy()\n                    self.f_opt = self.global_best_fitness\n                    self.x_opt = self.global_best_position\n                \n                if self.budget <= 0:\n                    break\n\n            # Adapt parameters\n            if generation % self.adapt_freq == 0:\n                for i in range(self.pop_size):\n                    if self.success_memory[i] == 1:\n                        #Increase inertia, reduce cognitive, reduce social\n                        self.w[i] = np.clip(self.w[i] + self.learning_rate, 0.1, 0.9)\n                        self.c1[i] = np.clip(self.c1[i] - self.learning_rate, 0.1, 2.0)\n                        self.c2[i] = np.clip(self.c2[i] - self.learning_rate, 0.1, 2.0)\n                    else:\n                        #Decrease inertia, increase cognitive, increase social\n                        self.w[i] = np.clip(self.w[i] - self.learning_rate, 0.1, 0.9)\n                        self.c1[i] = np.clip(self.c1[i] + self.learning_rate, 0.1, 2.0)\n                        self.c2[i] = np.clip(self.c2[i] + self.learning_rate, 0.1, 2.0)\n                \n                # Add a small random exploration component\n                if np.random.rand() < self.exploration_rate:\n                   idx = np.random.randint(0, self.pop_size)\n                   self.particles[idx] = np.random.uniform(-5, 5, size=self.dim)\n\n            if generation % self.ortho_freq == 0:\n                # Orthogonal learning: select two random particles and orthogonalize\n                idx = np.random.choice(range(self.pop_size), 2, replace=False)\n                x1, x2 = self.particles[idx[0]], self.particles[idx[1]]\n\n                # Calculate the normalized direction vector\n                direction = x2 - x1\n                norm = np.linalg.norm(direction)\n                if norm > 0:\n                    direction = direction / norm\n\n                    # Generate an orthogonal vector\n                    orthogonal_vector = np.random.randn(self.dim)\n                    orthogonal_vector -= np.dot(orthogonal_vector, direction) * direction\n                    orthogonal_vector /= np.linalg.norm(orthogonal_vector)\n\n                    # Move the particles along the orthogonal direction\n                    step_size = np.random.uniform(-1, 1)\n                    self.particles[idx[0]] += step_size * orthogonal_vector\n                    self.particles[idx[1]] -= step_size * orthogonal_vector\n                    \n                    #Boundary Handling after orthogonal learning\n                    self.particles[idx[0]] = np.clip(self.particles[idx[0]], -5, 5)\n                    self.particles[idx[1]] = np.clip(self.particles[idx[1]], -5, 5)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm SelfOrganizingPSO scored 0.539 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["bdc76fb9-0748-4af6-90d1-dca39bf21e50"], "operator": null, "metadata": {"aucs": [0.15288106470382645, 0.22250023780209838, 0.71881199047291, 0.17995571420677048, 0.418719991100188, 0.752858736154318, 0.6918425196488491, 0.5071751952675865, 0.7182012589163039, 0.23227133362422359, 0.841469350731094, 1.0, 0.25534620242196027, 0.5207273843516322, 0.7322172284097301, 0.7520298893623834, 0.6332462235933527, 0.7876426307633307, 0.15171325654359658, 0.5066665362488054]}}
{"id": "a3dd74cb-d024-48d5-9a08-35e1d5e0478a", "fitness": 0.6202988928949763, "name": "AdaptiveDELocalSearch", "description": "Improved Adaptive DE with Local Search featuring archive-based parameter adaptation, enhanced local search, and stagnation detection for parameter adaptation reset.", "code": "import numpy as np\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, F_init=0.5, CR_init=0.7, local_search_prob=0.1, local_search_radius=0.1, stagnation_threshold=1000):\n        \"\"\"\n        Initialize the Adaptive Differential Evolution with Local Search algorithm.\n\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size (int): The population size.\n            archive_size (int): The size of the archive for storing successful F and CR values.\n            F_init (float): Initial scaling factor.\n            CR_init (float): Initial crossover rate.\n            local_search_prob (float): Probability of applying local search.\n            local_search_radius (float): Radius for the local search.\n            stagnation_threshold (int): Number of evaluations without improvement before resetting F/CR.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F_init\n        self.CR = CR_init\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.success_F = []\n        self.success_CR = []\n        self.archive_F = []  # Archive to store successful F values\n        self.archive_CR = []  # Archive to store successful CR values\n        self.local_search_prob = local_search_prob\n        self.local_search_radius = local_search_radius\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.previous_f_opt = np.Inf  # To detect stagnation\n\n    def initialize_population(self, func):\n        \"\"\"Initialize the population randomly within the bounds.\"\"\"\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.pop[np.argmin(self.fitness)]\n        self.previous_f_opt = self.f_opt\n\n    def adapt_parameters(self):\n        \"\"\"Adapt F and CR based on successful values from the archive.\"\"\"\n        if self.archive_F:\n            self.F = np.mean(self.archive_F)\n            self.F = np.clip(self.F, 0.1, 1.0)  # keep F within reasonable bounds\n        else:\n            self.F = 0.5  # Reset F if no successful values in archive\n\n        if self.archive_CR:\n            self.CR = np.mean(self.archive_CR)\n            self.CR = np.clip(self.CR, 0.1, 1.0) # keep CR within reasonable bounds\n        else:\n            self.CR = 0.7 # Reset CR if no successful values in archive\n\n\n    def repair(self, x, lb, ub):\n        \"\"\"Repair function to bring individuals back to the bounds.\"\"\"\n        return np.clip(x, lb, ub)\n    \n    def local_search(self, x, func):\n        \"\"\"Perform local search around the given solution with shrinking radius.\"\"\"\n        radius = self.local_search_radius * (1 - self.eval_count / self.budget)  # Shrinking radius\n        x_new = x + np.random.uniform(-radius, radius, size=self.dim)\n        x_new = self.repair(x_new, func.bounds.lb, func.bounds.ub)\n        f_new = func(x_new)\n        self.eval_count += 1\n        return x_new, f_new\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using Adaptive Differential Evolution with Local Search.\n\n        Args:\n            func (callable): The function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            # Stagnation Check\n            if self.f_opt >= self.previous_f_opt:\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.archive_F = []\n                self.archive_CR = []\n                self.F = 0.5\n                self.CR = 0.7\n                self.stagnation_counter = 0\n\n            self.previous_f_opt = self.f_opt\n            \n            self.adapt_parameters()\n\n            for i in range(self.pop_size):\n                # Mutation (DE/rand/1)\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                a, b, c = idxs[0], idxs[1], idxs[2]\n                mutant = self.pop[a] + self.F * (self.pop[b] - self.pop[c])\n                mutant = self.repair(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Selection\n                f = func(trial)\n                self.eval_count += 1\n\n                if f < self.fitness[i]:\n                    self.success_F.append(self.F)\n                    self.success_CR.append(self.CR)\n                    \n                    # Update Archive\n                    self.archive_F.append(self.F)\n                    self.archive_CR.append(self.CR)\n                    if len(self.archive_F) > self.archive_size:\n                        self.archive_F.pop(0)\n                        self.archive_CR.pop(0)\n                    \n                    self.fitness[i] = f\n                    self.pop[i] = trial\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n\n                    # Local Search\n                    if np.random.rand() < self.local_search_prob:\n                        x_local, f_local = self.local_search(trial, func)\n                        if f_local < self.f_opt:\n                            self.f_opt = f_local\n                            self.x_opt = x_local\n                        if f_local < f:\n                            self.fitness[i] = f_local\n                            self.pop[i] = x_local\n                \n\n                if self.eval_count >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveDELocalSearch scored 0.620 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["28eedacc-11d0-48ea-a425-13b8923c6147"], "operator": null, "metadata": {"aucs": [0.19982451021778325, 0.4195122656679471, 0.6032310087542894, 0.8204736368598429, 0.6809609484112065, 0.749457246947848, 0.5663578209082853, 0.5857267997515295, 0.674263387783987, 0.5537611420950582, 0.7995611488703466, 0.9998217610114766, 0.37277309158945404, 0.6752822654819428, 0.8455883812666265, 0.7658915627745219, 0.5206940242542324, 0.8099609941782315, 0.2416058546917672, 0.5212300063831476]}}
{"id": "d480c02e-20fd-41f7-b066-ad3202deae9f", "fitness": 0.12496648355339529, "name": "HybridDE", "description": "Hybrid DE with success-history adaptation of F/CR, dynamic ring topology size, and a local search phase triggered when diversity is low.", "code": "import numpy as np\n\nclass HybridDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, adapt_freq=10, ring_neighbors=1, F_history_size=10, CR_history_size=10, local_search_prob=0.1, diversity_threshold=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.adapt_freq = adapt_freq\n        self.ring_neighbors = ring_neighbors\n        self.F_history = np.full(F_history_size, F)\n        self.CR_history = np.full(CR_history_size, CR)\n        self.F_history_idx = 0\n        self.CR_history_idx = 0\n        self.local_search_prob = local_search_prob\n        self.diversity_threshold = diversity_threshold\n\n    def __local_search(self, x, func, bounds, radius=0.1, num_steps=5):\n        \"\"\"Performs a local search around a given solution.\"\"\"\n        x_best = x\n        f_best = func(x)\n        self.budget -= 1\n        if self.budget <= 0:\n            return x_best, f_best\n        \n        for _ in range(num_steps):\n            # Generate a random perturbation within the radius\n            delta = np.random.uniform(-radius, radius, size=self.dim)\n            x_new = x + delta\n            x_new = np.clip(x_new, bounds.lb, bounds.ub)\n\n            f_new = func(x_new)\n            self.budget -= 1\n            if self.budget <= 0:\n                return x_best, f_best\n\n            if f_new < f_best:\n                x_best = x_new\n                f_best = f_new\n        return x_best, f_best\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            # Adapt ring topology\n            current_ring_neighbors = min(self.ring_neighbors + generation // 50, self.pop_size // 2)\n\n            for i in range(self.pop_size):\n                # Ring Topology Selection (Simplified)\n                neighbors = [(i - j) % self.pop_size for j in range(1, current_ring_neighbors + 1)]\n                neighbor_fitnesses = self.fitness[neighbors]\n                best_neighbor_idx = neighbors[np.argmin(neighbor_fitnesses)]\n                \n                # Mutation - Use best neighbor in ring\n                idx = np.random.choice(range(self.pop_size), 2, replace=False)\n                x_rand1, x_rand2 = self.population[idx[0]], self.population[idx[1]]\n                mutant = self.population[i] + self.F * (self.population[best_neighbor_idx] - self.population[i]) + self.F * (x_rand1 - x_rand2)\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    # Update history\n                    self.F_history[self.F_history_idx] = self.F\n                    self.CR_history[self.CR_history_idx] = self.CR\n                    self.F_history_idx = (self.F_history_idx + 1) % len(self.F_history)\n                    self.CR_history_idx = (self.CR_history_idx + 1) % len(self.CR_history)\n\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n\n            #Adapt F and CR using history\n            if generation % self.adapt_freq == 0:\n                self.F = np.clip(np.random.normal(np.mean(self.F_history), np.std(self.F_history)), 0.1, 1.0) if np.std(self.F_history) > 0 else 0.5\n                self.CR = np.clip(np.random.normal(np.mean(self.CR_history), np.std(self.CR_history)), 0.1, 1.0) if np.std(self.CR_history) > 0 else 0.9\n            \n            # Local Search if diversity is low\n            if np.std(self.fitness) < self.diversity_threshold:\n                for i in range(self.pop_size):\n                    if np.random.rand() < self.local_search_prob:\n                        self.population[i], self.fitness[i] = self.__local_search(self.population[i], func, func.bounds)\n                        if self.fitness[i] < self.f_opt:\n                            self.f_opt = self.fitness[i]\n                            self.x_opt = self.population[i]\n            \n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm HybridDE scored 0.125 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["adc30428-73e7-4a0d-9712-f0715c31a5bc"], "operator": null, "metadata": {"aucs": [0.24993296710679058, 0]}}
{"id": "47defd8a-ff58-49e2-b846-7ea8e92f6f33", "fitness": 0.39206569458130525, "name": "HybridDE", "description": "Hybrid DE with simplified parameter adaptation, probabilistic repair, and a streamlined aging mechanism for population diversity.", "code": "import numpy as np\n\nclass HybridDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, adapt_freq=10, age_limit=50, repair_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.adapt_freq = adapt_freq\n        self.age_limit = age_limit #Maximum age before being replaced\n        self.ages = np.zeros(pop_size, dtype=int) #Age of each individual\n        self.repair_prob = repair_prob #Probability of repairing a dimension\n        self.archive = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Mutation\n                idx = np.random.choice(range(self.pop_size), 3, replace=False)\n                x_rand1, x_rand2, x_rand3 = self.population[idx[0]], self.population[idx[1]], self.population[idx[2]]\n                mutant = x_rand1 + self.F * (x_rand2 - x_rand3)\n\n                # Probabilistic Repair and Clip\n                for j in range(self.dim):\n                    if np.random.rand() < self.repair_prob:\n                        if mutant[j] < func.bounds.lb[j] or mutant[j] > func.bounds.ub[j]:\n                            mutant[j] = np.random.uniform(func.bounds.lb[j], func.bounds.ub[j])\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n                    self.ages[i] = 0 #Reset age after improvement\n\n                else:\n                    self.ages[i] += 1 #Increase age if no improvement\n                    \n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n\n            #Adapt F and CR \n            if generation % self.adapt_freq == 0:\n                success_indices = self.fitness < np.mean(self.fitness)\n                if np.any(success_indices):\n                    self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n                    self.CR = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0)\n                else:\n                    self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n                    self.CR = np.clip(np.random.normal(self.CR, 0.1), 0.1, 1.0)\n\n\n            # Re-initialize old individuals:\n            for i in range(self.pop_size):\n                if self.ages[i] > self.age_limit:\n                    self.population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                    self.fitness[i] = func(self.population[i])\n                    self.budget -= 1\n                    self.ages[i] = 0\n\n                    if self.fitness[i] < self.f_opt:\n                        self.f_opt = self.fitness[i]\n                        self.x_opt = self.population[i]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm HybridDE scored 0.392 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["bdc76fb9-0748-4af6-90d1-dca39bf21e50"], "operator": null, "metadata": {"aucs": [0.3467551339650473, 0.592961769888247, 0.6285458744719266, 0]}}
{"id": "9420fb11-8d72-4323-8178-dc02c4ff488b", "fitness": 0.0, "name": "EnhancedDE", "description": "An enhanced Differential Evolution strategy incorporating a distance-based mutation scaling factor and a restart mechanism triggered by stagnation detection, aiming for improved exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, CR=0.9, adapt_freq=10, stagnation_threshold=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.adapt_freq = adapt_freq\n        self.stagnation_threshold = stagnation_threshold\n        self.best_fitness_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in pop])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = pop[np.argmin(fitness)]\n        self.best_fitness_history.append(self.f_opt)\n\n        generation = 0\n        stagnation_counter = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Mutation & Crossover\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                \n                # Distance-based scaling factor\n                distance = np.linalg.norm(pop[idxs[1]] - pop[idxs[2]])\n                adaptive_F = self.F * (1 + distance) # Scale F by distance\n                mutant = pop[idxs[0]] + adaptive_F * (pop[idxs[1]] - pop[idxs[2]])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                \n                cross_mask = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_mask):\n                    cross_mask[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_mask, mutant, pop[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        stagnation_counter = 0 # Reset stagnation counter\n                else:\n                    stagnation_counter += 1\n                    \n                if self.budget <= 0:\n                    break\n\n            self.best_fitness_history.append(self.f_opt)\n\n            # Adapt F/CR\n            if generation % self.adapt_freq == 0:\n                self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n                self.CR = np.clip(np.random.normal(self.CR, 0.1), 0.1, 1.0)\n\n            # Stagnation restart\n            if stagnation_counter > self.stagnation_threshold:\n                pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in pop])\n                self.budget -= self.pop_size\n                \n                best_idx = np.argmin(fitness)\n                if fitness[best_idx] < self.f_opt:\n                    self.f_opt = fitness[best_idx]\n                    self.x_opt = pop[best_idx]\n                stagnation_counter = 0\n                self.best_fitness_history.append(self.f_opt)\n\n                if self.budget <= 0:\n                    break\n                    \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm EnhancedDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["363e6e08-08c2-4389-9da4-f4994601ace1"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "77ad30d9-b43f-4e21-a0bc-6b8ef00eae1c", "fitness": -Infinity, "name": "EnhancedAdaptiveDELocalSearch", "description": "Enhanced Adaptive DE with Local Search using Success-Rate based F/CR adaptation, Lévy flight mutation, and adaptive local search probability for improved exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDELocalSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, CR_init=0.7, local_search_prob_init=0.05, local_search_radius=0.05, levy_exponent=1.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.CR = CR_init\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.local_search_prob = local_search_prob_init\n        self.local_search_radius = local_search_radius\n        self.levy_exponent = levy_exponent\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.pop[np.argmin(self.fitness)]\n\n    def adapt_parameters(self, success_rate_F, success_rate_CR):\n        # Adjust F and CR based on recent success rates\n        self.F = 0.9 * self.F + 0.1 * success_rate_F\n        self.CR = 0.9 * self.CR + 0.1 * success_rate_CR\n        self.F = np.clip(self.F, 0.1, 0.9)\n        self.CR = np.clip(self.CR, 0.1, 0.9)\n\n        # Adapt local search probability based on improvement\n        if success_rate_F > 0.2 or success_rate_CR > 0.2:  # Adjust if significant progress\n            self.local_search_prob = min(self.local_search_prob * 1.1, 0.5) # Increase if progress is good.\n        else:\n            self.local_search_prob = max(self.local_search_prob * 0.9, 0.01) # Decrease if progress is slow.\n            \n\n    def repair(self, x, lb, ub):\n        return np.clip(x, lb, ub)\n\n    def levy_flight(self, dim, exponent):\n        num = np.gamma(1 + exponent) * np.sin(np.pi * exponent / 2)\n        den = np.gamma((1 + exponent) / 2) * exponent * (2 ** ((exponent - 1) / 2))\n        sigma = (num / den) ** (1 / exponent)\n        u = np.random.normal(0, sigma, size=dim)\n        v = np.random.normal(0, 1, size=dim)\n        z = u / (np.abs(v) ** (1 / exponent))\n        return z\n    \n    def local_search(self, x, func):\n        x_new = x + np.random.uniform(-self.local_search_radius, self.local_search_radius, size=self.dim)\n        x_new = self.repair(x_new, func.bounds.lb, func.bounds.ub)\n        f_new = func(x_new)\n        self.eval_count += 1\n        return x_new, f_new\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        \n        num_success_F = 0\n        num_success_CR = 0\n        total_trials = 0\n\n        while self.eval_count < self.budget:\n            \n            num_success_F = 0\n            num_success_CR = 0\n            total_trials = 0\n\n            for i in range(self.pop_size):\n                # Mutation (Lévy flight)\n                levy_step = self.levy_flight(self.dim, self.levy_exponent)\n                mutant = self.pop[i] + self.F * levy_step * (self.x_opt - self.pop[i]) # Biased towards best solution\n                mutant = self.repair(mutant, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Selection\n                f = func(trial)\n                self.eval_count += 1\n                total_trials += 1\n\n                if f < self.fitness[i]:\n                    num_success_F += 1\n                    num_success_CR += 1\n                    self.fitness[i] = f\n                    self.pop[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n\n                    if np.random.rand() < self.local_search_prob:\n                        x_local, f_local = self.local_search(trial, func)\n                        if f_local < self.f_opt:\n                            self.f_opt = f_local\n                            self.x_opt = x_local\n                        if f_local < f:\n                            self.fitness[i] = f_local\n                            self.pop[i] = x_local\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Adapt parameters based on success rate\n            success_rate_F = num_success_F / total_trials if total_trials > 0 else 0\n            success_rate_CR = num_success_CR / total_trials if total_trials > 0 else 0\n            self.adapt_parameters(success_rate_F, success_rate_CR)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "An exception occurred: module 'numpy' has no attribute 'gamma'.", "error": "", "parent_ids": ["31be346d-952c-4901-a86b-d859149304e2"], "operator": null, "metadata": {}}
{"id": "75714c97-e03f-479a-aa50-895a9cf13878", "fitness": 0.09998756931921698, "name": "AdaptiveDEOLCMA", "description": "Adaptive Differential Evolution with orthogonal learning and covariance matrix adaptation for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDEOLCMA:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, CR_init=0.7, ol_sample_size=5, cma_learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.CR = CR_init\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.ol_sample_size = ol_sample_size\n        self.cma_learning_rate = cma_learning_rate\n        self.C = np.eye(dim)  # Covariance matrix\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.pop[np.argmin(self.fitness)]\n\n    def adapt_parameters(self, success_F, success_CR):\n        if success_F:\n            self.F = np.mean(success_F)\n            self.F = np.clip(self.F, 0.1, 0.9)\n        if success_CR:\n            self.CR = np.mean(success_CR)\n            self.CR = np.clip(self.CR, 0.1, 0.9)\n\n    def repair(self, x, lb, ub):\n        return np.clip(x, lb, ub)\n\n    def orthogonal_learning(self, func, x):\n        # Generate orthogonal sample points around x\n        ortho_points = []\n        for _ in range(self.ol_sample_size):\n            direction = np.random.randn(self.dim)\n            direction /= np.linalg.norm(direction)\n            step_size = np.random.uniform(0, 0.1)  # Adjust step size as needed\n            x_new = x + step_size * direction\n            x_new = self.repair(x_new, func.bounds.lb, func.bounds.ub)\n            ortho_points.append(x_new)\n\n        # Evaluate orthogonal points\n        fitness_values = [func(xi) for xi in ortho_points]\n        self.eval_count += self.ol_sample_size\n\n        # Select the best point\n        best_index = np.argmin(fitness_values)\n        return ortho_points[best_index], fitness_values[best_index]\n\n    def update_covariance_matrix(self, x, x_opt):\n        diff = x - x_opt\n        self.C = (1 - self.cma_learning_rate) * self.C + self.cma_learning_rate * np.outer(diff, diff)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            success_F = []\n            success_CR = []\n\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                a, b, c = self.pop[idxs[0]], self.pop[idxs[1]], self.pop[idxs[2]]\n                mutant = a + self.F * (b - c)\n                mutant = self.repair(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Selection\n                f = func(trial)\n                self.eval_count += 1\n\n                if f < self.fitness[i]:\n                    success_F.append(self.F)\n                    success_CR.append(self.CR)\n                    self.fitness[i] = f\n                    self.pop[i] = trial\n\n                    # Orthogonal Learning\n                    x_ol, f_ol = self.orthogonal_learning(func, trial)\n                    if f_ol < self.f_opt:\n                        self.f_opt = f_ol\n                        self.x_opt = x_ol\n                    if f_ol < f:\n                        self.fitness[i] = f_ol\n                        self.pop[i] = x_ol\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                        \n\n                    # CMA Update\n                    self.update_covariance_matrix(trial, self.x_opt)\n                \n\n                if self.eval_count >= self.budget:\n                    break\n            \n            self.adapt_parameters(success_F, success_CR)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveDEOLCMA scored 0.100 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["31be346d-952c-4901-a86b-d859149304e2"], "operator": null, "metadata": {"aucs": [0.19997513863843397, 0]}}
{"id": "564c55bf-8634-4155-93f3-775822f92bbb", "fitness": 0.12101248988313877, "name": "DynamicDEMemetic", "description": "A Differential Evolution strategy that dynamically adjusts its mutation strength based on the population diversity and incorporates a gradient-based local search to accelerate convergence.", "code": "import numpy as np\n\nclass DynamicDEMemetic:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, CR_init=0.7, local_search_prob=0.1, lr=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.CR = CR_init\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.local_search_prob = local_search_prob\n        self.lr = lr  # learning rate for gradient descent\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.pop[np.argmin(self.fitness)]\n\n    def adapt_parameters(self):\n        # Dynamic F adaptation based on population diversity\n        diversity = np.std(self.pop)\n        self.F = 0.1 + 0.8 * np.exp(-10 * diversity) # F decreases as diversity decreases\n        self.F = np.clip(self.F, 0.1, 0.9)\n        self.CR = np.clip(self.CR, 0.1, 0.9)\n\n    def repair(self, x, lb, ub):\n        return np.clip(x, lb, ub)\n\n    def gradient_descent(self, x, func, num_steps=5):\n        x_current = x.copy()\n        f_current = self.fitness[np.argmin(self.fitness)]\n        for _ in range(num_steps):\n            # Estimate gradient using finite differences\n            gradient = np.zeros(self.dim)\n            for j in range(self.dim):\n                x_plus = x_current.copy()\n                x_minus = x_current.copy()\n                delta = 1e-4\n                x_plus[j] += delta\n                x_minus[j] -= delta\n                x_plus = self.repair(x_plus, func.bounds.lb, func.bounds.ub)\n                x_minus = self.repair(x_minus, func.bounds.lb, func.bounds.ub)\n                f_plus = func(x_plus)\n                f_minus = func(x_minus)\n                self.eval_count += 2\n                gradient[j] = (f_plus - f_minus) / (2 * delta)\n\n            # Update position\n            x_new = x_current - self.lr * gradient\n            x_new = self.repair(x_new, func.bounds.lb, func.bounds.ub)\n            f_new = func(x_new)\n            self.eval_count += 1\n\n            if f_new < f_current:\n                x_current = x_new\n                f_current = f_new\n            else:\n                break\n\n        return x_current, f_current\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            self.adapt_parameters()\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                a, b, c = self.pop[idxs[0]], self.pop[idxs[1]], self.pop[idxs[2]]\n                mutant = a + self.F * (b - c)\n                mutant = self.repair(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Selection\n                f = func(trial)\n                self.eval_count += 1\n\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.pop[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n\n                # Local search\n                if np.random.rand() < self.local_search_prob:\n                    x_local, f_local = self.gradient_descent(trial.copy(), func) # Apply gradient descent\n                    if f_local < self.f_opt:\n                        self.f_opt = f_local\n                        self.x_opt = x_local\n                    if f_local < self.fitness[i]:\n                        self.fitness[i] = f_local\n                        self.pop[i] = x_local\n                \n\n                if self.eval_count >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm DynamicDEMemetic scored 0.121 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["31be346d-952c-4901-a86b-d859149304e2"], "operator": null, "metadata": {"aucs": [0.15329589437860647, 0.20974157527080983, 0]}}
{"id": "ab8b6271-e262-4a27-96e2-6a4cf7ab6180", "fitness": 0.0, "name": "AdaptiveDEOrthogonalCMA", "description": "Adaptive Differential Evolution with orthogonal learning and covariance matrix adaptation to enhance search efficiency and diversity.", "code": "import numpy as np\n\nclass AdaptiveDEOrthogonalCMA:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, F_init=0.5, CR_init=0.7, local_search_prob=0.1, local_search_radius=0.1, stagnation_threshold=1000, orthogonal_components=5):\n        \"\"\"\n        Initialize the Adaptive Differential Evolution with Orthogonal Learning and CMA.\n\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size (int): The population size.\n            archive_size (int): The size of the archive for storing successful F and CR values.\n            F_init (float): Initial scaling factor.\n            CR_init (float): Initial crossover rate.\n            local_search_prob (float): Probability of applying local search.\n            local_search_radius (float): Radius for the local search.\n            stagnation_threshold (int): Number of evaluations without improvement before resetting F/CR.\n            orthogonal_components (int): Number of orthogonal components for orthogonal learning.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F_init\n        self.CR = CR_init\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.success_F = []\n        self.success_CR = []\n        self.archive_F = []  # Archive to store successful F values\n        self.archive_CR = []  # Archive to store successful CR values\n        self.local_search_prob = local_search_prob\n        self.local_search_radius = local_search_radius\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.previous_f_opt = np.Inf  # To detect stagnation\n        self.orthogonal_components = orthogonal_components\n        self.C = np.eye(dim)  # Covariance matrix for CMA\n        self.learning_rate = 0.1  # Learning rate for CMA\n\n    def initialize_population(self, func):\n        \"\"\"Initialize the population randomly within the bounds.\"\"\"\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.pop[np.argmin(self.fitness)]\n        self.previous_f_opt = self.f_opt\n\n    def adapt_parameters(self):\n        \"\"\"Adapt F and CR based on successful values from the archive.\"\"\"\n        if self.archive_F:\n            self.F = np.mean(self.archive_F)\n            self.F = np.clip(self.F, 0.1, 1.0)  # keep F within reasonable bounds\n        else:\n            self.F = 0.5  # Reset F if no successful values in archive\n\n        if self.archive_CR:\n            self.CR = np.mean(self.archive_CR)\n            self.CR = np.clip(self.CR, 0.1, 1.0) # keep CR within reasonable bounds\n        else:\n            self.CR = 0.7 # Reset CR if no successful values in archive\n\n\n    def repair(self, x, lb, ub):\n        \"\"\"Repair function to bring individuals back to the bounds.\"\"\"\n        return np.clip(x, lb, ub)\n    \n    def local_search(self, x, func):\n        \"\"\"Perform local search around the given solution with shrinking radius.\"\"\"\n        radius = self.local_search_radius * (1 - self.eval_count / self.budget)  # Shrinking radius\n        x_new = x + np.random.normal(0, radius, size=self.dim)\n        x_new = self.repair(x_new, func.bounds.lb, func.bounds.ub)\n        f_new = func(x_new)\n        self.eval_count += 1\n        return x_new, f_new\n\n    def orthogonal_learning(self, func):\n        \"\"\"Perform orthogonal learning to generate new candidate solutions.\"\"\"\n        basis = np.random.randn(self.dim, self.orthogonal_components)  # Generate random basis vectors\n        basis, _ = np.linalg.qr(basis)  # Orthogonalize the basis vectors\n\n        for i in range(self.pop_size):\n            coefficients = np.random.uniform(-1, 1, size=self.orthogonal_components)\n            mutation_vector = np.dot(basis, coefficients)\n            new_x = self.pop[i] + mutation_vector\n            new_x = self.repair(new_x, func.bounds.lb, func.bounds.ub)\n            f_new = func(new_x)\n            self.eval_count += 1\n\n            if f_new < self.fitness[i]:\n                self.fitness[i] = f_new\n                self.pop[i] = new_x\n                if f_new < self.f_opt:\n                    self.f_opt = f_new\n                    self.x_opt = new_x\n\n    def update_covariance_matrix(self):\n        \"\"\"Update the covariance matrix based on successful steps.\"\"\"\n        if self.success_F:\n            diffs = self.pop - np.mean(self.pop, axis=0)\n            self.C = (1 - self.learning_rate) * self.C + self.learning_rate * np.cov(diffs.T)\n            # Ensure C is positive definite\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)  # Reset to identity if not positive definite\n        else:\n            self.C = np.eye(self.dim)\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using Adaptive Differential Evolution with Local Search.\n\n        Args:\n            func (callable): The function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            # Stagnation Check\n            if self.f_opt >= self.previous_f_opt:\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.archive_F = []\n                self.archive_CR = []\n                self.F = 0.5\n                self.CR = 0.7\n                self.C = np.eye(self.dim) # Reset covariance matrix\n                self.stagnation_counter = 0\n\n            self.previous_f_opt = self.f_opt\n            \n            self.adapt_parameters()\n\n            for i in range(self.pop_size):\n                # Mutation (DE/rand/1)\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                a, b, c = idxs[0], idxs[1], idxs[2]\n                mutant = self.pop[a] + self.F * (self.pop[b] - self.pop[c])\n                mutant = self.repair(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Selection\n                f = func(trial)\n                self.eval_count += 1\n\n                if f < self.fitness[i]:\n                    self.success_F.append(self.F)\n                    self.success_CR.append(self.CR)\n                    \n                    # Update Archive\n                    self.archive_F.append(self.F)\n                    self.archive_CR.append(self.CR)\n                    if len(self.archive_F) > self.archive_size:\n                        self.archive_F.pop(0)\n                        self.archive_CR.pop(0)\n                    \n                    self.fitness[i] = f\n                    self.pop[i] = trial\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n\n                    # Local Search\n                    if np.random.rand() < self.local_search_prob:\n                        x_local, f_local = self.local_search(trial, func)\n                        if f_local < self.f_opt:\n                            self.f_opt = f_local\n                            self.x_opt = x_local\n                        if f_local < f:\n                            self.fitness[i] = f_local\n                            self.pop[i] = x_local\n\n            self.orthogonal_learning(func)\n            self.update_covariance_matrix()\n\n            if self.eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveDEOrthogonalCMA scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a3dd74cb-d024-48d5-9a08-35e1d5e0478a"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "5513a512-89e4-4d6e-b5ac-434811806b76", "fitness": 0.6514156084865896, "name": "AdaptiveDELocalSearchRestart", "description": "Adaptive Differential Evolution with periodic local search and restart mechanism to escape local optima.", "code": "import numpy as np\n\nclass AdaptiveDELocalSearchRestart:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, CR=0.9, adapt_freq=10, local_search_freq=50, restart_freq=200, local_search_radius=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.adapt_freq = adapt_freq\n        self.local_search_freq = local_search_freq\n        self.restart_freq = restart_freq\n        self.local_search_radius = local_search_radius\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        pop = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in pop])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = pop[np.argmin(fitness)]\n\n        generation = 0\n        stagnation_counter = 0\n        best_fitness_history = [self.f_opt]\n\n        while self.budget > 0:\n            generation += 1\n            old_f_opt = self.f_opt\n            for i in range(self.pop_size):\n                # Mutation & Crossover\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                mutant = pop[idxs[0]] + self.F * (pop[idxs[1]] - pop[idxs[2]])\n                mutant = np.clip(mutant, lb, ub)\n                \n                cross_mask = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_mask):\n                    cross_mask[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_mask, mutant, pop[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                if self.budget <= 0:\n                    break\n\n            # Adapt F/CR\n            if generation % self.adapt_freq == 0:\n                self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n                self.CR = np.clip(np.random.normal(self.CR, 0.1), 0.1, 1.0)\n\n            # Local Search\n            if generation % self.local_search_freq == 0:\n                best_idx = np.argmin(fitness)\n                x_local = pop[best_idx]\n                \n                #Perturb the best solution\n                trial_local = x_local + np.random.uniform(-self.local_search_radius, self.local_search_radius, size=self.dim)\n                trial_local = np.clip(trial_local, lb, ub)\n                \n                f_local = func(trial_local)\n                self.budget -= 1\n                if f_local < fitness[best_idx]:\n                    fitness[best_idx] = f_local\n                    pop[best_idx] = trial_local\n                    if f_local < self.f_opt:\n                        self.f_opt = f_local\n                        self.x_opt = trial_local\n                \n            #Restart mechanism\n            if generation % self.restart_freq == 0:\n                 if self.f_opt >= old_f_opt: #if there is no improvement\n                     pop = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n                     fitness = np.array([func(x) for x in pop])\n                     self.budget -= self.pop_size\n                     self.f_opt = np.min(fitness)\n                     self.x_opt = pop[np.argmin(fitness)]\n\n            if self.budget <= 0:\n                break\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveDELocalSearchRestart scored 0.651 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["363e6e08-08c2-4389-9da4-f4994601ace1"], "operator": null, "metadata": {"aucs": [0.21517562614227193, 0.5219850591620494, 0.7586563719430099, 0.8976296688532521, 0.7900901114057182, 0.804771085412576, 0.6059417928378535, 0.6118394880201093, 0.7897626452338171, 0.6294788266552367, 0.8458622274110255, 0.99278003893824, 0.34456656844551103, 0.6015789107427508, 0.9206580302316639, 0.7244766892926624, 0.392979449137125, 0.8030998095312208, 0.25991713504357494, 0.5170626352921257]}}
{"id": "324d24d6-86bf-44f2-b7a2-10eced41ec46", "fitness": 0.6681661559603334, "name": "AdaptiveDELocalSearch", "description": "Adaptive Differential Evolution with simplified parameter adaptation and focused local search, balancing exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, CR_init=0.7, local_search_prob=0.05, local_search_radius=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.CR = CR_init\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.local_search_prob = local_search_prob\n        self.local_search_radius = local_search_radius\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.pop[np.argmin(self.fitness)]\n\n    def repair(self, x, lb, ub):\n        return np.clip(x, lb, ub)\n    \n    def local_search(self, x, func):\n        x_new = x + np.random.uniform(-self.local_search_radius, self.local_search_radius, size=self.dim)\n        x_new = self.repair(x_new, func.bounds.lb, func.bounds.ub)\n        f_new = func(x_new)\n        self.eval_count += 1\n        return x_new, f_new\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                a, b, c = self.pop[idxs[0]], self.pop[idxs[1]], self.pop[idxs[2]]\n                mutant = a + self.F * (b - c)\n                mutant = self.repair(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Selection\n                f = func(trial)\n                self.eval_count += 1\n\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.pop[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                \n                    if np.random.rand() < self.local_search_prob:\n                        x_local, f_local = self.local_search(trial, func)\n                        if f_local < self.f_opt:\n                            self.f_opt = f_local\n                            self.x_opt = x_local\n                        if f_local < f:\n                            self.fitness[i] = f_local\n                            self.pop[i] = x_local\n\n                if self.eval_count >= self.budget:\n                    break\n            \n            # Simplified parameter adaptation (fixed adaptation)\n            self.F = np.clip(self.F + np.random.normal(0, 0.01), 0.1, 0.9)\n            self.CR = np.clip(self.CR + np.random.normal(0, 0.01), 0.1, 0.9)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveDELocalSearch scored 0.668 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["31be346d-952c-4901-a86b-d859149304e2"], "operator": null, "metadata": {"aucs": [0.2503622675024829, 0.48418871421836207, 0.6158694018141198, 0.8824729287889562, 0.773739438951805, 0.7902167655608616, 0.6326027842754984, 0.7338377110039878, 0.7121477657512236, 0.6469114156157174, 0.7643457225805034, 0.9916203013079439, 0.49699357818379575, 0.7042994256728219, 0.8937563119257876, 0.7983450337459982, 0.6162986495201783, 0.8446720726387231, 0.20247175500001868, 0.5281710751478812]}}
{"id": "6e2e4a81-587d-403e-9d53-c162e0b5de62", "fitness": 0.4651000560371055, "name": "OrthogonalDE", "description": "Differential Evolution with orthogonal crossover, adaptive mutation and restart based on population diversity.", "code": "import numpy as np\n\nclass OrthogonalDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, CR=0.9, adapt_freq=10, restart_trigger=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.adapt_freq = adapt_freq\n        self.restart_trigger = restart_trigger  # Threshold for diversity-based restart\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in pop])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = pop[np.argmin(fitness)]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                mutant = pop[idxs[0]] + self.F * (pop[idxs[1]] - pop[idxs[2]])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Orthogonal Crossover\n                orthogonal_idx = np.random.choice(self.dim, size=self.dim // 2, replace=False)  # Half the dimensions\n                trial = pop[i].copy()\n                trial[orthogonal_idx] = mutant[orthogonal_idx]\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                if self.budget <= 0:\n                    break\n\n            # Adapt F/CR\n            if generation % self.adapt_freq == 0:\n                self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n                self.CR = np.clip(np.random.normal(self.CR, 0.1), 0.1, 1.0)\n\n            # Diversity Check and Restart\n            if self.population_diversity(pop) < self.restart_trigger:\n                pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in pop])\n                self.budget -= self.pop_size\n                \n                min_fitness_idx = np.argmin(fitness)\n                if fitness[min_fitness_idx] < self.f_opt:\n                    self.f_opt = fitness[min_fitness_idx]\n                    self.x_opt = pop[min_fitness_idx]\n                if self.budget <= 0:\n                    break\n\n        return self.f_opt, self.x_opt\n\n    def population_diversity(self, pop):\n        \"\"\"Calculates the average distance of individuals from the population mean.\"\"\"\n        mean_pos = np.mean(pop, axis=0)\n        distances = np.linalg.norm(pop - mean_pos, axis=1)\n        return np.mean(distances)", "configspace": "", "generation": 8, "feedback": "The algorithm OrthogonalDE scored 0.465 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["363e6e08-08c2-4389-9da4-f4994601ace1"], "operator": null, "metadata": {"aucs": [0.1793788668712556, 0.24228634171163177, 0.41714617457247627, 0.7087902052271059, 0.4145078587777806, 0.49455982615051, 0.3266941756798649, 0.3866445852302346, 0.36388648762256703, 0.38510133607891284, 0.6328270207953413, 0.9998260315197235, 0.2860344024122785, 0.418120691599308, 0.76770202323202, 0.522490634922349, 0.4107082746100944, 0.6020390986336961, 0.24159814075068076, 0.5016589443442803]}}
{"id": "014b3047-b326-48f9-a7b1-cb382b24ce60", "fitness": 0.7052297587613204, "name": "SelfAdaptiveDE", "description": "Differential Evolution with self-adaptive mutation strategy, dynamically adjusting both F and CR based on population diversity and individual success rates.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, CR_init=0.9, adapt_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = np.full(pop_size, F_init)\n        self.CR = np.full(pop_size, CR_init)\n        self.adapt_prob = adapt_prob\n        self.success_F = []\n        self.success_CR = []\n        self.success_fitness_diff = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in pop])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = pop[np.argmin(fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation & Crossover\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                mutant = pop[idxs[0]] + self.F[i] * (pop[idxs[1]] - pop[idxs[2]])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                \n                cross_mask = np.random.rand(self.dim) < self.CR[i]\n                if not np.any(cross_mask):\n                    cross_mask[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_mask, mutant, pop[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    self.success_F.append(self.F[i])\n                    self.success_CR.append(self.CR[i])\n                    self.success_fitness_diff.append(abs(f_trial - fitness[i]))\n                    fitness_diff = fitness[i] - f_trial\n                    fitness[i] = f_trial\n                    pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                \n                    # Adaptation of F and CR based on success\n                    if np.random.rand() < self.adapt_prob:\n                        self.F[i] = np.clip(np.random.normal(self.F[i], 0.1), 0.1, 1.0)\n                        self.CR[i] = np.clip(np.random.normal(self.CR[i], 0.1), 0.1, 1.0)\n                else:\n                     #Adaptation when no improvement\n                    if np.random.rand() < self.adapt_prob:\n                        self.F[i] = np.clip(np.random.normal(self.F[i], 0.3), 0.1, 1.0)\n                        self.CR[i] = np.clip(np.random.normal(self.CR[i], 0.3), 0.1, 1.0)\n\n                if self.budget <= 0:\n                    break\n\n            # Global Adaptation (after each generation)\n            if self.success_F:\n                # Weighted average based on fitness improvement\n                weights = np.array(self.success_fitness_diff) / np.sum(self.success_fitness_diff)\n                \n                mean_F = np.average(self.success_F, weights=weights)\n                mean_CR = np.average(self.success_CR, weights=weights)\n                \n                for i in range(self.pop_size):\n                    self.F[i] = np.clip(np.random.normal(mean_F, 0.1), 0.1, 1.0) if np.random.rand() < 0.1 else self.F[i]\n                    self.CR[i] = np.clip(np.random.normal(mean_CR, 0.1), 0.1, 1.0) if np.random.rand() < 0.1 else self.CR[i]\n                \n                self.success_F = []\n                self.success_CR = []\n                self.success_fitness_diff = []\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm SelfAdaptiveDE scored 0.705 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["363e6e08-08c2-4389-9da4-f4994601ace1"], "operator": null, "metadata": {"aucs": [0.39392279925268314, 0.6760328923986199, 0.7171318357773736, 0.8828305762879757, 0.7917338745419953, 0.7789675257072566, 0.6557783134482472, 0.5594885114595641, 0.7460890305545477, 0.7304114631593112, 0.8859227048492495, 0.9994064018177953, 0.5582880006462072, 0.6930002947147802, 0.9244195908024149, 0.8139527922293545, 0.6149858245355035, 0.8434477735953877, 0.25059432453994246, 0.5881906449081982]}}
{"id": "8cb8f928-b64e-4351-9061-f246973fb5f3", "fitness": 0.27108614563970546, "name": "AdaptiveDEOrthogonalCMA", "description": "Adaptive Differential Evolution with orthogonal design-based mutation and covariance matrix adaptation for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDEOrthogonalCMA:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, CR=0.9, adapt_freq=10, orthogonal_design_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.adapt_freq = adapt_freq\n        self.orthogonal_design_size = min(orthogonal_design_size, dim)\n        self.mean = None  # Initialize mean for CMA-ES\n        self.covariance = None # Initialize covariance matrix\n        self.step_size = 0.1 # Initialize step_size\n        self.eigen_decomposition_updated = False # Flag to check if eigen decomposition needs to be updated\n        self.eigen_values = None # Eigenvalues of covariance matrix\n        self.eigen_vectors = None # Eigenvectors of covariance matrix\n        self.c_sigma = 0.3\n        self.d_sigma = 1\n\n    def initialize_covariance(self):\n        self.covariance = np.eye(self.dim)\n        self.eigen_decomposition_updated = False\n        self.mean = np.zeros(self.dim)\n        \n\n    def update_eigen_decomposition(self):\n        if not self.eigen_decomposition_updated:\n          self.eigen_values, self.eigen_vectors = np.linalg.eigh(self.covariance)\n          self.eigen_decomposition_updated = True\n    \n    def sample_with_CMA(self, num_samples):\n          self.update_eigen_decomposition()\n          z = np.random.normal(0, 1, size=(num_samples, self.dim))\n          samples = self.mean + self.step_size * (self.eigen_vectors @ (np.diag(np.sqrt(self.eigen_values)) @ z.T)).T\n          return samples\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n        # Initialize population\n        pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in pop])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = pop[np.argmin(fitness)]\n\n        # Initialize CMA-ES covariance matrix\n        self.initialize_covariance()\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_pop = []\n            new_fitness = []\n            for i in range(self.pop_size):\n                # Orthogonal Design-based Mutation\n                if self.dim >= 2 and self.orthogonal_design_size > 1:\n                    idxs = np.random.choice(self.pop_size, self.orthogonal_design_size, replace=False)\n                    orthogonal_basis = pop[idxs]\n                    \n                    # Generate orthogonal design (simplified - random combination)\n                    mutant = np.mean(orthogonal_basis, axis=0) # Use the mean of the orthogonal basis\n                    \n                    #Crossover\n                    cross_mask = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_mask):\n                        cross_mask[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_mask, mutant, pop[i])\n                    trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n\n                else: # If dim < 2 or orthogonal_design_size =1, fallback to standard DE\n                    idxs = np.random.choice(self.pop_size, 3, replace=False)\n                    mutant = pop[idxs[0]] + self.F * (pop[idxs[1]] - pop[idxs[2]])\n                    mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                    \n                    cross_mask = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_mask):\n                        cross_mask[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_mask, mutant, pop[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    pop[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n\n            # Adapt F/CR\n            if generation % self.adapt_freq == 0:\n                self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n                self.CR = np.clip(np.random.normal(self.CR, 0.1), 0.1, 1.0)\n\n            # CMA-ES adaptation (simplified)\n            if generation % self.adapt_freq == 0:\n              \n                best_individual = pop[np.argmin(fitness)]\n                \n                # Update mean (simplified - moving average)\n                self.mean = 0.8 * self.mean + 0.2 * best_individual \n\n                #Update covariance matrix\n                diff = pop - self.mean\n                self.covariance = np.cov(diff.T)\n                self.eigen_decomposition_updated = False #Reset flag, so eigen decomposition is recomputed.\n                \n\n                # Update step size\n                c = (self.f_opt - np.min(fitness)) #Reward function\n                self.step_size = self.step_size* np.exp(self.c_sigma/self.d_sigma * (c/np.std(fitness)))\n\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveDEOrthogonalCMA scored 0.271 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["363e6e08-08c2-4389-9da4-f4994601ace1"], "operator": null, "metadata": {"aucs": [0.11796238734567654, 0.18388403574222056, 0.275837138323239, 0.18617341107062102, 0.20024890252597338, 0.20994655702991072, 0.23198016699178947, 0.17011231704364527, 0.18594386712832311, 0.16476347110852563, 0.19235094536244157, 0.9936778077143625, 0.26601668793154587, 0.15778305334906784, 0.5958950881664953, 0.24745290286926025, 0.22081231886586328, 0.18701321078828237, 0.1906754888757105, 0.44319315456115616]}}
{"id": "c094ca98-68a5-4715-a6b7-a72622b2dc74", "fitness": -Infinity, "name": "AdaptiveDELocalSearchEnhanced", "description": "Enhanced Adaptive DE with archive-based parameter adaptation, improved local search using Nelder-Mead, and dynamic scaling of the local search radius.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDELocalSearchEnhanced:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, CR_init=0.7, archive_size=10, local_search_prob=0.05, local_search_radius_init=0.1, local_search_radius_decay=0.99):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.CR = CR_init\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.archive = []\n        self.archive_size = archive_size\n        self.local_search_prob = local_search_prob\n        self.local_search_radius = local_search_radius_init\n        self.local_search_radius_decay = local_search_radius_decay\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.pop[np.argmin(self.fitness)]\n\n    def adapt_parameters(self):\n        if self.archive:\n            archive_F = [entry[0] for entry in self.archive]\n            archive_CR = [entry[1] for entry in self.archive]\n\n            self.F = np.mean(archive_F) if archive_F else self.F\n            self.CR = np.mean(archive_CR) if archive_CR else self.CR\n            self.F = np.clip(self.F, 0.1, 0.9)\n            self.CR = np.clip(self.CR, 0.1, 0.9)\n\n    def repair(self, x, lb, ub):\n        return np.clip(x, lb, ub)\n\n    def local_search(self, x, func):\n        bounds = [(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)]\n        res = minimize(func, x, method='Nelder-Mead', bounds=bounds, options={'maxfev': min(50, self.budget - self.eval_count)}) # Reduced maxfev\n        n_eval = res.nfev\n        self.eval_count += n_eval\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            self.adapt_parameters()\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                a, b, c = self.pop[idxs[0]], self.pop[idxs[1]], self.pop[idxs[2]]\n                mutant = a + self.F * (b - c)\n                mutant = self.repair(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Selection\n                f = func(trial)\n                self.eval_count += 1\n\n                if f < self.fitness[i]:\n                    self.archive.append((self.F, self.CR))\n                    if len(self.archive) > self.archive_size:\n                        self.archive.pop(0)\n                    self.fitness[i] = f\n                    self.pop[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                \n                    if np.random.rand() < self.local_search_prob:\n                        x_local, f_local = self.local_search(self.pop[i], func)\n                        if f_local < self.f_opt:\n                            self.f_opt = f_local\n                            self.x_opt = x_local\n                        if f_local < f:\n                            self.fitness[i] = f_local\n                            self.pop[i] = x_local\n                \n                if self.eval_count >= self.budget:\n                    break\n            self.local_search_radius *= self.local_search_radius_decay # Decay local search radius\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "An exception occurred: name 'minimize' is not defined.", "error": "", "parent_ids": ["31be346d-952c-4901-a86b-d859149304e2"], "operator": null, "metadata": {}}
{"id": "bc5ca081-d76c-4743-b619-4df2a5c5e0b4", "fitness": 0.0, "name": "OrthogonalDE", "description": "Adaptive Differential Evolution with orthogonal learning and a restart mechanism based on population diversity.", "code": "import numpy as np\n\nclass OrthogonalDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, CR=0.9, adapt_freq=10, diversity_threshold=0.01, restart_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.adapt_freq = adapt_freq\n        self.diversity_threshold = diversity_threshold\n        self.restart_prob = restart_prob\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Mutation\n                idx = np.random.choice(range(self.pop_size), 3, replace=False)\n                x_rand1, x_rand2, x_rand3 = self.population[idx[0]], self.population[idx[1]], self.population[idx[2]]\n                mutant = x_rand1 + self.F * (x_rand2 - x_rand3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.population[i])\n                \n                # Orthogonal Learning\n                if np.random.rand() < 0.1: # Apply orthogonal learning with probability 0.1\n                    levels = 3 # Number of levels for orthogonal design\n                    factors = min(self.dim, 5)  # Number of factors (dimensions to consider)\n                    design = self.create_orthogonal_design(levels, factors)\n\n                    best_obj = np.inf\n                    best_point = None\n\n                    for row in design:\n                        new_point = trial.copy()\n                        indices = np.random.choice(range(self.dim), factors, replace=False) # select random dimensions\n                        for j, idx in enumerate(indices):\n                            level_value = func.bounds.lb[idx] + (func.bounds.ub[idx] - func.bounds.lb[idx]) * row[j] / (levels - 1)\n                            new_point[idx] = level_value\n                        \n                        new_point = np.clip(new_point, func.bounds.lb, func.bounds.ub)\n                        obj_val = func(new_point)\n                        self.budget -= 1\n                        if obj_val < best_obj:\n                            best_obj = obj_val\n                            best_point = new_point\n                    \n                    if best_obj < func(trial):\n                        trial = best_point\n                \n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                else:\n                    pass\n\n                if self.budget <= 0:\n                    break\n\n            # Adapt F and CR (simple adaptation)\n            if generation % self.adapt_freq == 0:\n                self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n                self.CR = np.clip(np.random.normal(self.CR, 0.1), 0.1, 1.0)\n\n            # Restart based on population diversity\n            if np.random.rand() < self.restart_prob:\n                diversity = self.calculate_diversity()\n                if diversity < self.diversity_threshold:\n                    self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                    self.fitness = np.array([func(x) for x in self.population])\n                    self.budget -= self.pop_size\n\n                    if np.min(self.fitness) < self.f_opt:\n                        self.f_opt = np.min(self.fitness)\n                        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        return self.f_opt, self.x_opt\n    \n    def create_orthogonal_design(self, levels, factors):\n        # A simple orthogonal design (L9) for 3 levels and up to 4 factors\n        if levels == 3 and factors <= 4:\n            design = np.array([\n                [0, 0, 0, 0],\n                [0, 1, 1, 1],\n                [0, 2, 2, 2],\n                [1, 0, 1, 2],\n                [1, 1, 2, 0],\n                [1, 2, 0, 1],\n                [2, 0, 2, 1],\n                [2, 1, 0, 2],\n                [2, 2, 1, 0]\n            ])\n            return design[:(levels**2), :factors]\n        else:\n            # Return a random design if levels/factors are not supported\n            return np.random.randint(0, levels, size=(levels**2, factors))\n\n\n    def calculate_diversity(self):\n        # Calculate the average distance between individuals in the population\n        distances = []\n        for i in range(self.pop_size):\n            for j in range(i + 1, self.pop_size):\n                distances.append(np.linalg.norm(self.population[i] - self.population[j]))\n        return np.mean(distances) if distances else 1.0", "configspace": "", "generation": 8, "feedback": "The algorithm OrthogonalDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["96c1a262-eb6d-401f-a0c0-8500d3dfe928"], "operator": null, "metadata": {"aucs": [0]}}
