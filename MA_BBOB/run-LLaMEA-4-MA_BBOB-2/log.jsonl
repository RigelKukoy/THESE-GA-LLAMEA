{"id": "c56da915-f1f9-412f-88b9-b6b6c350d730", "fitness": -Infinity, "name": "AdaptiveDE", "description": "An adaptive differential evolution strategy that adjusts its parameters based on the success of previous generations.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR # Crossover rate\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.generation = 0\n        self.archive = []\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.archive = []\n        for i in range(self.pop_size):\n          self.archive.append({\"x\": self.pop[i].copy(), \"f\": self.fitness[i]})\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.pop[np.argmin(self.fitness)].copy()\n        \n\n    def mutate(self):\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.pop[idxs]\n            self.mutant = self.pop[i] + self.F * (x_r2 - x_r3)\n            \n            # Ensure the mutant stays within bounds\n            self.mutant = np.clip(self.mutant, -5.0, 5.0)\n            \n\n    def crossover(self):\n      for i in range(self.pop_size):\n        trial_vector = self.pop[i].copy()\n        for j in range(self.dim):\n            if np.random.rand() <= self.CR or j == np.random.randint(0, self.dim):\n                trial_vector[j] = self.mutant[j]\n        self.pop[i] = trial_vector\n            \n    def selection(self, func):\n        for i in range(self.pop_size):\n            f = func(self.pop[i])\n            if f < self.fitness[i]:\n                self.fitness[i] = f\n                self.archive.append({\"x\": self.pop[i].copy(), \"f\": f})\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = self.pop[i].copy()\n\n    def adapt_parameters(self):\n      # Adaptive F and CR\n      success_indices = np.where(self.fitness < np.mean(self.fitness))[0]\n      if len(success_indices) > 0:\n          self.F = np.random.normal(0.5, 0.3)  # Example of adapting F\n          self.CR = np.random.normal(0.9, 0.1) # Example of adapting CR\n          self.F = np.clip(self.F, 0.1, 1.0)\n          self.CR = np.clip(self.CR, 0.1, 1.0)\n      \n      if self.generation % 10 == 0:\n        #Dynamic population sizing (example)\n        if self.budget - func.evaluations < self.budget * 0.2:\n          self.pop_size = min(self.pop_size + 5, 100)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        while func.evaluations < self.budget:\n            self.generation += 1\n            self.mutate()\n            self.crossover()\n            self.selection(func)\n            self.adapt_parameters()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "An exception occurred: 'ioh.iohcpp.problem.Discus' object has no attribute 'evaluations'.", "error": "", "parent_ids": [], "operator": null, "metadata": {}}
{"id": "b662a24c-0f7d-4880-9956-6d88d53e056c", "fitness": 0.2926525017772739, "name": "AdaptiveNeighborhoodSearch", "description": "Population-based search with adaptive step size and neighborhood exploration based on fitness landscape.", "code": "import numpy as np\n\nclass AdaptiveNeighborhoodSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_step_size=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.initial_step_size = initial_step_size\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Find best initial solution\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n        \n        step_size = self.initial_step_size\n\n        while self.budget > 0:\n            # Generate neighbors for each individual\n            neighbors = population + np.random.normal(0, step_size, size=(self.pop_size, self.dim))\n            \n            # Clip neighbors to bounds\n            neighbors = np.clip(neighbors, self.lb, self.ub)\n            \n            # Evaluate neighbors\n            neighbor_fitness = np.array([func(x) for x in neighbors])\n            self.budget -= self.pop_size\n\n            # Update population based on neighbor fitness\n            for i in range(self.pop_size):\n                if neighbor_fitness[i] < fitness[i]:\n                    population[i] = neighbors[i]\n                    fitness[i] = neighbor_fitness[i]\n\n                    # Update best solution\n                    if neighbor_fitness[i] < self.f_opt:\n                        self.f_opt = neighbor_fitness[i]\n                        self.x_opt = population[i]\n\n            # Adaptive step size adjustment\n            if np.std(fitness) < 1e-6:  # Convergence detected\n                step_size *= 0.5  # Reduce step size\n            elif np.std(fitness) > 1:\n                step_size *= 1.1\n\n            step_size = np.clip(step_size, 1e-6, (self.ub - self.lb)/2)\n            \n            if self.budget <= 0:\n              break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveNeighborhoodSearch scored 0.293 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1263059387082971, 0.18386390950891596, 0.35279364904402943, 0.21778233745973752, 0.21907493396399547, 0.25960563976453566, 0.2338431966115253, 0.2023846574221101, 0.19281221760601508, 0.16414006054416042, 0.22010623523991002, 0.9934791224891989, 0.25548772854051327, 0.22766715400643778, 0.5908000972307925, 0.30785950567671316, 0.223436156332944, 0.2684165468429225, 0.16621685572199807, 0.446974092830726]}}
{"id": "2bf27f2b-7afa-41f0-9213-1db959036489", "fitness": 0.36161955636908044, "name": "AdaptiveExplorationExploitation", "description": "A population-based algorithm that uses a combination of global exploration and local exploitation, with adaptive parameter control based on the success rate of search directions.", "code": "import numpy as np\n\nclass AdaptiveExplorationExploitation:\n    def __init__(self, budget=10000, dim=10, pop_size=20, exploration_rate=0.5, exploitation_rate=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.exploration_rate = exploration_rate\n        self.exploitation_rate = exploitation_rate\n        self.archive = [] # Store successful solutions\n        self.archive_size = 10\n\n    def __call__(self, func):\n        # Initialize population within the bounds\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        while self.budget > 0:\n            new_population = np.zeros_like(population)\n            new_fitness = np.zeros_like(fitness)\n\n            for i in range(self.pop_size):\n                if np.random.rand() < self.exploration_rate:\n                    # Global Exploration: Random perturbation\n                    new_individual = population[i] + np.random.normal(0, 0.5, size=self.dim)\n                    new_individual = np.clip(new_individual, func.bounds.lb, func.bounds.ub)  # Keep within bounds\n\n                else:\n                    # Local Exploitation: Perturbation around the best solution and/or archive\n                    if len(self.archive) > 0 and np.random.rand() < self.exploitation_rate:\n                        #Exploitation using archive\n                        archive_idx = np.random.randint(len(self.archive))\n                        new_individual = population[i] + 0.5 * (self.archive[archive_idx] - population[i]) + np.random.normal(0, 0.1, size=self.dim)\n                        new_individual = np.clip(new_individual, func.bounds.lb, func.bounds.ub)\n\n                    else:\n                        #Exploitation around best\n                        new_individual = population[i] + np.random.normal(0, 0.1, size=self.dim) + 0.1 * (self.x_opt - population[i]) # Add attraction to best\n                        new_individual = np.clip(new_individual, func.bounds.lb, func.bounds.ub)\n\n                new_fitness[i] = func(new_individual)\n                self.budget -= 1\n                if new_fitness[i] < self.f_opt:\n                    self.f_opt = new_fitness[i]\n                    self.x_opt = new_individual\n\n\n                new_population[i] = new_individual\n\n            # Update population\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    population[i] = new_population[i]\n                    fitness[i] = new_fitness[i]\n                    #Archive the solution\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(new_population[i].copy())\n                    else:\n                        # Replace a random archive member\n                        replace_idx = np.random.randint(self.archive_size)\n                        self.archive[replace_idx] = new_population[i].copy()\n\n\n            # Adaptive parameter control (simplified)\n            if np.mean(new_fitness < fitness) > 0.2:\n                self.exploration_rate *= 0.95 #Reduce exploration if exploitation is working\n            else:\n                self.exploration_rate = min(1.0, self.exploration_rate * 1.05) #Increase exploration otherwise\n            \n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveExplorationExploitation scored 0.362 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.13710379434827646, 0.28356342714344684, 0.3571615349176347, 0.3600461547423838, 0.2656921253768092, 0.3500861168517987, 0.27675379348099294, 0.3145490779312855, 0.278220254540156, 0.1734263208070106, 0.37724137748305986, 0.9997434334763975, 0.27952133832350057, 0.2819436672375779, 0.7277348082562758, 0.3381148748740649, 0.3009906814430565, 0.4528488336089479, 0.19354254208883315, 0.48410697045010054]}}
{"id": "db83fb26-fc28-4250-a235-7f50b87f5013", "fitness": 0.5559297650395496, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with a population size that scales with the logarithm of the budget, combined with a Cauchy mutation operator for enhanced exploration.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, F=0.5, Cr=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.F = F  # Mutation factor\n        self.Cr = Cr # Crossover rate\n        self.pop_size = int(4 + 3 * np.log(budget))  # Population size that scales with budget\n        self.population = None\n        self.fitness = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n\n    def initialize_population(self, func):\n         self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n         self.fitness = np.array([func(x) for x in self.population])\n\n    def mutate(self, pop, i):\n        indices = [idx for idx in range(len(pop)) if idx != i]\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        # Cauchy mutation for enhanced exploration\n        mutation = pop[a] + self.F * (pop[b] - pop[c]) + 0.01 * np.random.standard_cauchy(size=self.dim)\n        return mutation\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.Cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n        \n        fevals = self.pop_size # Initial population evaluation\n\n        while fevals < self.budget:\n            for i in range(self.pop_size):\n                mutant = self.mutate(self.population, i)\n                mutant = np.clip(mutant, self.bounds_lb, self.bounds_ub)  # Clip to bounds\n                trial = self.crossover(mutant, self.population[i])\n                \n                f = func(trial)\n                fevals += 1\n\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.population[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                \n                if fevals >= self.budget:\n                    break\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDE scored 0.556 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.24625091265608, 0.497812733962157, 0.5072356340911339, 0.9236524775366068, 0.49889799958959347, 0.5722168469243776, 0.3744702447004884, 0.4751585619436961, 0.5048953007835096, 0.2447221790175491, 0.8730245800316017, 0.9906706547370524, 0.3777409917518828, 0.4971391825846193, 0.9301498130921754, 0.5979450921089222, 0.4393841439115499, 0.708173164338541, 0.3336063498896745, 0.5254484371397816]}}
{"id": "27d65587-b25a-48a9-bb24-fd53e27adc2e", "fitness": -Infinity, "name": "SelfOrganizingMigratingAlgorithm", "description": "A self-organizing migrating algorithm (SOMA) inspired optimization, where individuals migrate towards a leader, adapting their step size based on local fitness landscape.", "code": "import numpy as np\n\nclass SelfOrganizingMigratingAlgorithm:\n    def __init__(self, budget=10000, dim=10, pop_size=40, step_size=0.1, path_length=2.0, perturbation_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.step_size = step_size\n        self.path_length = path_length\n        self.perturbation_rate = perturbation_rate\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.pop[np.argmin(self.fitness)].copy()\n\n    def migrate(self, func):\n        leader_index = np.argmin(self.fitness)\n        leader = self.pop[leader_index].copy()\n\n        for i in range(self.pop_size):\n            if i == leader_index:\n                continue\n\n            for step in np.arange(self.step_size, self.path_length + self.step_size, self.step_size):\n                new_position = self.pop[i] + step * (leader - self.pop[i])\n\n                # Perturbation\n                perturbation = np.random.uniform(-self.perturbation_rate, self.perturbation_rate, size=self.dim)\n                new_position = new_position + perturbation\n\n                # Clip to bounds\n                new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n\n                new_fitness = func(new_position)\n\n                if new_fitness < self.fitness[i]:\n                    self.pop[i] = new_position.copy()\n                    self.fitness[i] = new_fitness\n\n                    if new_fitness < self.f_opt:\n                        self.f_opt = new_fitness\n                        self.x_opt = new_position.copy()\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        while func.evaluations < self.budget:\n            self.migrate(func)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "An exception occurred: 'ioh.iohcpp.problem.Discus' object has no attribute 'evaluations'.", "error": "", "parent_ids": ["c56da915-f1f9-412f-88b9-b6b6c350d730"], "operator": null, "metadata": {}}
{"id": "a4a09c4e-e9fc-47ac-a632-4a07747c5784", "fitness": -Infinity, "name": "HybridDE_NM", "description": "A population-based algorithm that combines Differential Evolution's mutation and crossover with a local search using Nelder-Mead simplex method to refine solutions.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridDE_NM:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, CR=0.7, nm_iters=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.nm_iters = nm_iters\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Find best initial solution\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Differential Evolution Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = population[a] + self.F * (population[b] - population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Differential Evolution Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        trial[j] = mutant[j]\n\n                # Nelder-Mead Local Search around the trial vector\n                nm_result = minimize(func, trial, method='Nelder-Mead',\n                                    bounds=[(self.lb, self.ub)] * self.dim,\n                                    options={'maxiter': self.nm_iters, 'maxfev':self.budget}) # Limit iterations/evaluations\n\n                trial_fitness = nm_result.fun\n                trial_vector = nm_result.x\n                n_evals = nm_result.nfev\n\n                if self.budget - n_evals < 0:\n                    break\n                \n                self.budget -= n_evals\n\n                # Selection\n                if trial_fitness < fitness[i]:\n                    population[i] = trial_vector\n                    fitness[i] = trial_fitness\n\n                    # Update best solution\n                    if trial_fitness < self.f_opt:\n                        self.f_opt = trial_fitness\n                        self.x_opt = trial_vector\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "An exception occurred: name 'minimize' is not defined.", "error": "", "parent_ids": ["b662a24c-0f7d-4880-9956-6d88d53e056c"], "operator": null, "metadata": {}}
{"id": "3e5b6e39-49f2-4d8f-8c0e-3ec89a16d9cf", "fitness": -Infinity, "name": "SelfOrganizingScoutBee", "description": "A self-organizing scout bee algorithm that dynamically adjusts search intensity based on local fitness landscape characteristics.", "code": "import numpy as np\n\nclass SelfOrganizingScoutBee:\n    def __init__(self, budget=10000, dim=10, num_bees=50, scout_rate=0.1, neighborhood_size=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.num_bees = num_bees\n        self.scout_rate = scout_rate # Probability of a bee becoming a scout\n        self.neighborhood_size = neighborhood_size # Size of the neighborhood for local search\n        self.positions = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n    def initialize_population(self, func):\n        self.positions = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.num_bees, self.dim))\n        self.fitness = np.array([func(x) for x in self.positions])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.positions[np.argmin(self.fitness)].copy()\n\n    def local_search(self, func, bee_index):\n        current_position = self.positions[bee_index].copy()\n        current_fitness = self.fitness[bee_index]\n\n        # Generate a random neighbor within the neighborhood\n        neighbor = current_position + np.random.uniform(-self.neighborhood_size, self.neighborhood_size, size=self.dim)\n\n        # Clip the neighbor to stay within bounds\n        neighbor = np.clip(neighbor, -5.0, 5.0)\n        \n        neighbor_fitness = func(neighbor)\n        \n        if neighbor_fitness < current_fitness:\n            self.positions[bee_index] = neighbor\n            self.fitness[bee_index] = neighbor_fitness\n            if neighbor_fitness < self.f_opt:\n                self.f_opt = neighbor_fitness\n                self.x_opt = neighbor.copy()\n\n    def scout_search(self, func):\n        # Replace a random bee with a new random position\n        bee_index = np.random.randint(0, self.num_bees)\n        self.positions[bee_index] = np.random.uniform(-5.0, 5.0, size=self.dim)\n        self.fitness[bee_index] = func(self.positions[bee_index])\n        if self.fitness[bee_index] < self.f_opt:\n            self.f_opt = self.fitness[bee_index]\n            self.x_opt = self.positions[bee_index].copy()\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        while func.evaluations < self.budget:\n            for i in range(self.num_bees):\n                if np.random.rand() < self.scout_rate:\n                    self.scout_search(func)\n                else:\n                    self.local_search(func, i)\n\n            #Adaptive Scout Rate (Example)\n            if func.evaluations > self.budget * 0.75:\n              self.scout_rate = min(self.scout_rate + 0.01, 0.5)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "An exception occurred: 'ioh.iohcpp.problem.Discus' object has no attribute 'evaluations'.", "error": "", "parent_ids": ["c56da915-f1f9-412f-88b9-b6b6c350d730"], "operator": null, "metadata": {}}
{"id": "dbc28305-35a2-4058-b975-d7fabea855ee", "fitness": 0.0, "name": "HybridPSO", "description": "A hybrid algorithm combining a simplified PSO variant with a local search, dynamically adjusting the balance between global and local search.", "code": "import numpy as np\n\nclass HybridPSO:\n    def __init__(self, budget=10000, dim=10, pop_size=20, inertia=0.7, cognitive_coeff=1.5, social_coeff=1.5, local_search_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.inertia = inertia\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.local_search_prob = local_search_prob\n\n        self.velocities = None\n\n    def __local_search(self, func, x, current_f, step_size=0.1):\n        \"\"\"Performs a local search around a given point.\"\"\"\n        best_x = x\n        best_f = current_f\n\n        for _ in range(5):  # Limited local search evaluations\n            neighbor = x + np.random.uniform(-step_size, step_size, size=self.dim)\n            neighbor = np.clip(neighbor, func.bounds.lb, func.bounds.ub)\n            neighbor_f = func(neighbor)\n            self.budget -= 1\n            if self.budget <= 0:\n                break\n\n            if neighbor_f < best_f:\n                best_f = neighbor_f\n                best_x = neighbor\n        return best_x, best_f\n\n    def __call__(self, func):\n        # Initialize population and velocities within the bounds\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, size=(self.pop_size, self.dim)) #Initialize velocities\n\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n        personal_best_positions = population.copy() # Initialize personal best positions\n        personal_best_fitness = fitness.copy()\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Update velocity\n                r1, r2 = np.random.rand(2)\n                self.velocities[i] = (self.inertia * self.velocities[i] +\n                                      self.cognitive_coeff * r1 * (personal_best_positions[i] - population[i]) +\n                                      self.social_coeff * r2 * (self.x_opt - population[i]))\n\n                # Update position\n                new_position = population[i] + self.velocities[i]\n                new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n                \n                # Local Search\n                if np.random.rand() < self.local_search_prob and self.budget > 0:\n                    new_position, new_fitness = self.__local_search(func, new_position, func(new_position))\n                else:\n                    new_fitness = func(new_position)\n                    self.budget -= 1\n\n                if self.budget <= 0:\n                    break\n                \n\n                # Update personal best\n                if new_fitness < personal_best_fitness[i]:\n                    personal_best_fitness[i] = new_fitness\n                    personal_best_positions[i] = new_position.copy()\n\n                #Update global best\n                if new_fitness < self.f_opt:\n                    self.f_opt = new_fitness\n                    self.x_opt = new_position.copy()\n            \n                population[i] = new_position\n            if self.budget <= 0:\n                break\n\n            #Adaptive local search probability (simplified)\n            if np.mean(personal_best_fitness < fitness) > 0.2:\n                self.local_search_prob = min(1.0, self.local_search_prob * 1.05)\n            else:\n                self.local_search_prob *= 0.95\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm HybridPSO scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2bf27f2b-7afa-41f0-9213-1db959036489"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "d649100d-fdf8-4676-96c1-9a7ab54588d7", "fitness": -Infinity, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with a dynamically adjusted population size, mutation, and crossover rates based on success history, and using a repair mechanism for out-of-bounds individuals.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR # Crossover rate\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.generation = 0\n        self.archive = []\n        self.success_history_F = []\n        self.success_history_CR = []\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.archive = []\n        for i in range(self.pop_size):\n            self.archive.append({\"x\": self.pop[i].copy(), \"f\": self.fitness[i]})\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.pop[np.argmin(self.fitness)].copy()\n\n    def repair(self, x, lb, ub):\n        \"\"\"Repair function to keep the solution within bounds.\"\"\"\n        return np.clip(x, lb, ub)\n\n    def mutate(self):\n        self.mutant = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.pop[idxs]\n            self.mutant[i] = self.pop[i] + self.F * (x_r2 - x_r3)\n            \n            # Ensure the mutant stays within bounds using repair\n            self.mutant[i] = self.repair(self.mutant[i], -5.0, 5.0)\n            \n\n    def crossover(self):\n        trial_vectors = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            trial_vector = self.pop[i].copy()\n            j_rand = np.random.randint(0, self.dim)\n            for j in range(self.dim):\n                if np.random.rand() <= self.CR or j == j_rand:\n                    trial_vector[j] = self.mutant[i][j]\n            trial_vectors[i] = trial_vector\n        return trial_vectors\n            \n    def selection(self, func, trial_vectors):\n        improved = False\n        for i in range(self.pop_size):\n            f_trial = func(trial_vectors[i])\n            if f_trial < self.fitness[i]:\n                \n                self.success_history_F.append(self.F)\n                self.success_history_CR.append(self.CR)\n                \n                self.fitness[i] = f_trial\n                self.pop[i] = trial_vectors[i]\n                self.archive.append({\"x\": self.pop[i].copy(), \"f\": f_trial})\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = self.pop[i].copy()\n                    improved = True\n        return improved\n\n    def adapt_parameters(self):\n        # Adaptive F and CR based on success history\n\n        if self.success_history_F:\n            self.F = np.mean(self.success_history_F)\n            self.CR = np.mean(self.success_history_CR)\n\n        #Reset history periodically\n        if self.generation % 20 == 0:\n            self.success_history_F = []\n            self.success_history_CR = []\n            \n        self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n        self.CR = np.clip(np.random.normal(self.CR, 0.1), 0.1, 1.0)\n      \n        if self.generation % 10 == 0:\n            #Dynamic population sizing (example)\n            remaining_budget = self.budget - func.evaluations\n            if remaining_budget < self.budget * 0.2:\n                self.pop_size = min(self.pop_size + 5, 100)\n                new_pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(5, self.dim))\n                new_fitness = np.array([func(x) for x in new_pop])\n\n                self.pop = np.vstack((self.pop, new_pop))\n                self.fitness = np.concatenate((self.fitness, new_fitness))\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        while func.evaluations < self.budget:\n            self.generation += 1\n            self.mutate()\n            trial_vectors = self.crossover()\n            self.selection(func, trial_vectors)\n            self.adapt_parameters()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "An exception occurred: 'ioh.iohcpp.problem.Discus' object has no attribute 'evaluations'.", "error": "", "parent_ids": ["c56da915-f1f9-412f-88b9-b6b6c350d730"], "operator": null, "metadata": {}}
{"id": "df675a7f-9073-4f44-82c6-1d78266ba4d0", "fitness": 0.0, "name": "AdaptiveDESA", "description": "An adaptive differential evolution algorithm that adjusts both the mutation factor and crossover rate based on the success of previous generations, and incorporates a Simulated Annealing-based local search for further refinement.", "code": "import numpy as np\n\nclass AdaptiveDESA:\n    def __init__(self, budget=10000, dim=10, F=0.5, Cr=0.9, initial_temp=1.0, cooling_rate=0.99):\n        self.budget = budget\n        self.dim = dim\n        self.F = F  # Initial mutation factor\n        self.Cr = Cr  # Initial crossover rate\n        self.pop_size = int(4 + 3 * np.log(budget))  # Population size scales with budget\n        self.population = None\n        self.fitness = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.initial_temp = initial_temp\n        self.cooling_rate = cooling_rate\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n\n    def mutate(self, pop, i):\n        indices = [idx for idx in range(len(pop)) if idx != i]\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutation = pop[a] + self.F * (pop[b] - pop[c])\n        return mutation\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.Cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def local_search(self, x, func, temp):\n        \"\"\"Simulated Annealing-based local search.\"\"\"\n        x_new = x + np.random.normal(0, temp, self.dim)\n        x_new = np.clip(x_new, self.bounds_lb, self.bounds_ub)\n        delta_e = func(x_new) - func(x)\n        if delta_e < 0:\n            return x_new\n        elif np.random.rand() < np.exp(-delta_e / temp):\n            return x_new\n        else:\n            return x\n        \n    def adapt_parameters(self, successful_F, successful_Cr):\n        \"\"\"Adapt mutation factor and crossover rate based on successful values.\"\"\"\n        if successful_F:\n            self.F = 0.9 * self.F + 0.1 * np.mean(successful_F)\n        if successful_Cr:\n            self.Cr = 0.9 * self.Cr + 0.1 * np.mean(successful_Cr)\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n        \n        fevals = self.pop_size\n        temp = self.initial_temp\n        successful_Fs = []\n        successful_Crs = []\n\n        while fevals < self.budget:\n            successful_f_values = []\n            successful_cr_values = []\n            for i in range(self.pop_size):\n                mutant = self.mutate(self.population, i)\n                mutant = np.clip(mutant, self.bounds_lb, self.bounds_ub)  # Clip to bounds\n                trial = self.crossover(mutant, self.population[i])\n                \n                f = func(trial)\n                fevals += 1\n\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.population[i] = trial\n                    successful_f_values.append(self.F)\n                    successful_cr_values.append(self.Cr)\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n\n                # Local search with Simulated Annealing\n                self.population[i] = self.local_search(self.population[i], func, temp)\n                f_local = func(self.population[i])\n                fevals += 1\n                if f_local < self.fitness[i]:\n                    self.fitness[i] = f_local\n                    if f_local < self.f_opt:\n                        self.f_opt = f_local\n                        self.x_opt = self.population[i]\n                        \n                if fevals >= self.budget:\n                    break\n\n            if successful_f_values:\n                successful_Fs.append(np.mean(successful_f_values))\n            if successful_cr_values:\n                successful_Crs.append(np.mean(successful_cr_values))\n            if successful_Fs and successful_Crs:\n                self.adapt_parameters(successful_Fs, successful_Crs)\n            temp *= self.cooling_rate # Cool down the temperature\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDESA scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["db83fb26-fc28-4250-a235-7f50b87f5013"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "dd37decc-e4eb-45e2-8b23-464653723767", "fitness": 0.0, "name": "SelfAdaptiveDE", "description": "Self-adaptive Differential Evolution with probabilistic parameter update and a restart mechanism to escape local optima.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=None):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else int(4 + 3 * np.log(budget))\n        self.population = None\n        self.fitness = None\n        self.F = None  # Mutation factor for each individual\n        self.Cr = None # Crossover rate for each individual\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.restart_trigger = 100 # Trigger restart if no improvement after this many iterations\n        self.no_improvement_counter = 0\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.F = np.random.uniform(0.1, 0.9, size=self.pop_size)\n        self.Cr = np.random.uniform(0.1, 0.9, size=self.pop_size)\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n    def mutate(self, pop, i):\n        indices = [idx for idx in range(len(pop)) if idx != i]\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutation = pop[a] + self.F[i] * (pop[b] - pop[c])\n        return mutation\n\n    def crossover(self, mutant, target, i):\n        cross_points = np.random.rand(self.dim) < self.Cr[i]\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n    \n    def update_parameters(self):\n         # Probabilistic update of F and Cr\n        rand_F = np.random.rand(self.pop_size)\n        rand_Cr = np.random.rand(self.pop_size)\n\n        for i in range(self.pop_size):\n            if rand_F[i] < 0.1:\n                self.F[i] = np.random.uniform(0.1, 0.9)\n            if rand_Cr[i] < 0.1:\n                self.Cr[i] = np.random.uniform(0.1, 0.9)\n            self.F[i] = np.clip(self.F[i], 0.1, 0.9)\n            self.Cr[i] = np.clip(self.Cr[i], 0.1, 0.9)\n\n    def restart_population(self, func):\n        # Reinitialize a fraction of the population\n        num_to_restart = int(0.2 * self.pop_size)\n        indices_to_restart = np.random.choice(self.pop_size, num_to_restart, replace=False)\n        \n        self.population[indices_to_restart] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(num_to_restart, self.dim))\n        self.fitness[indices_to_restart] = np.array([func(x) for x in self.population[indices_to_restart]])\n        self.F[indices_to_restart] = np.random.uniform(0.1, 0.9, size=num_to_restart)\n        self.Cr[indices_to_restart] = np.random.uniform(0.1, 0.9, size=num_to_restart)\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        \n        self.initialize_population(func)\n        fevals = self.pop_size\n\n        while fevals < self.budget:\n            best_fitness_before = self.f_opt\n            for i in range(self.pop_size):\n                mutant = self.mutate(self.population, i)\n                mutant = np.clip(mutant, self.bounds_lb, self.bounds_ub)\n                trial = self.crossover(mutant, self.population[i], i)\n                \n                f = func(trial)\n                fevals += 1\n                \n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.population[i] = trial\n                    \n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n            \n            self.update_parameters()\n\n            if self.f_opt < best_fitness_before:\n                self.no_improvement_counter = 0\n            else:\n                self.no_improvement_counter += 1\n\n            if self.no_improvement_counter > self.restart_trigger:\n                self.restart_population(func)\n                self.no_improvement_counter = 0\n\n            if fevals >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm SelfAdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["db83fb26-fc28-4250-a235-7f50b87f5013"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "bbc35178-a913-4580-b812-202bf7339e9f", "fitness": 0.5633981018895036, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with a budget-aware population size, combined with Cauchy mutation, and a self-adaptive mutation factor F.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, F_init=0.5, Cr=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.F_init = F_init  # Initial mutation factor\n        self.Cr = Cr # Crossover rate\n        self.pop_size = int(4 + 3 * np.log(budget))  # Population size that scales with budget\n        self.population = None\n        self.fitness = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.F = np.full(self.pop_size, self.F_init)\n\n    def initialize_population(self, func):\n         self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n         self.fitness = np.array([func(x) for x in self.population])\n\n    def mutate(self, pop, i):\n        indices = [idx for idx in range(len(pop)) if idx != i]\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        # Cauchy mutation for enhanced exploration\n        mutation = pop[a] + self.F[i] * (pop[b] - pop[c]) + 0.01 * np.random.standard_cauchy(size=self.dim)\n        return mutation\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.Cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n    \n    def adapt_mutation_factor(self):\n        \"\"\"\n        Adapt the mutation factor F for each individual based on their success.\n        If an individual has improved its fitness, increase its F value slightly;\n        otherwise, decrease it.\n        \"\"\"\n        for i in range(self.pop_size):\n            if np.random.rand() < 0.1:  # Probabilistically adapt F\n                if np.random.rand() < 0.5:\n                    self.F[i] *= 1.1  # Increase F\n                else:\n                    self.F[i] *= 0.9  # Decrease F\n                self.F[i] = np.clip(self.F[i], 0.1, 1.0)  # Keep F within [0.1, 1.0]\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n        \n        fevals = self.pop_size # Initial population evaluation\n\n        while fevals < self.budget:\n            for i in range(self.pop_size):\n                mutant = self.mutate(self.population, i)\n                mutant = np.clip(mutant, self.bounds_lb, self.bounds_ub)  # Clip to bounds\n                trial = self.crossover(mutant, self.population[i])\n                \n                f = func(trial)\n                fevals += 1\n\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.population[i] = trial\n                    \n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                \n                if fevals >= self.budget:\n                    break\n            \n            self.adapt_mutation_factor()\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDE scored 0.563 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["db83fb26-fc28-4250-a235-7f50b87f5013"], "operator": null, "metadata": {"aucs": [0.2375172687660997, 0.5283900052469694, 0.49657204108140873, 0.884854772763066, 0.49481879723956645, 0.570759454354248, 0.3793476227122936, 0.4415684491635825, 0.4936109025770633, 0.4325387687750738, 0.8799201449212692, 0.9971714397492438, 0.3820796170011356, 0.5029926852783266, 0.8995652668687693, 0.5777726515055747, 0.4388313888165085, 0.6879781097557274, 0.42736057756375034, 0.5143120736503941]}}
{"id": "362496cf-1ef2-41ac-950f-11944911fba6", "fitness": 0.5245984575323893, "name": "AdaptiveDE_Diversity", "description": "An adaptive differential evolution with a dynamically adjusted mutation factor based on the diversity of the population.", "code": "import numpy as np\n\nclass AdaptiveDE_Diversity:\n    def __init__(self, budget=10000, dim=10, F_initial=0.5, Cr=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.F = F_initial  # Initial mutation factor\n        self.Cr = Cr # Crossover rate\n        self.pop_size = int(4 + 3 * np.log(budget))  # Population size that scales with budget\n        self.population = None\n        self.fitness = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.F_min = 0.1\n        self.F_max = 1.0\n\n    def initialize_population(self, func):\n         self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n         self.fitness = np.array([func(x) for x in self.population])\n\n    def calculate_diversity(self):\n        # Calculate the average distance of each individual from the population mean\n        mean_position = np.mean(self.population, axis=0)\n        distances = np.linalg.norm(self.population - mean_position, axis=1)\n        diversity = np.mean(distances)\n        return diversity\n\n    def adjust_mutation_factor(self):\n        # Adjust mutation factor based on population diversity\n        diversity = self.calculate_diversity()\n        # Normalize diversity to [0, 1]\n        max_possible_distance = np.linalg.norm(self.bounds_ub - self.bounds_lb) * np.sqrt(self.dim)\n        normalized_diversity = diversity / max_possible_distance if max_possible_distance > 0 else 0\n\n        # Adapt F: If diversity is low, increase F to explore more.  If diversity is high, decrease F to exploit.\n        self.F = self.F_min + (self.F_max - self.F_min) * (1 - normalized_diversity)\n        self.F = np.clip(self.F, self.F_min, self.F_max)\n\n\n    def mutate(self, pop, i):\n        indices = [idx for idx in range(len(pop)) if idx != i]\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutation = pop[a] + self.F * (pop[b] - pop[c])\n        return mutation\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.Cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n        \n        fevals = self.pop_size # Initial population evaluation\n\n        while fevals < self.budget:\n            self.adjust_mutation_factor() # Adjust F based on diversity\n\n            for i in range(self.pop_size):\n                mutant = self.mutate(self.population, i)\n                mutant = np.clip(mutant, self.bounds_lb, self.bounds_ub)  # Clip to bounds\n                trial = self.crossover(mutant, self.population[i])\n                \n                f = func(trial)\n                fevals += 1\n\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.population[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                \n                if fevals >= self.budget:\n                    break\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDE_Diversity scored 0.525 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["db83fb26-fc28-4250-a235-7f50b87f5013"], "operator": null, "metadata": {"aucs": [0.20567843688787824, 0.36735990509693106, 0.4584602751890229, 0.7670874123930679, 0.4802671290197944, 0.5820189601589099, 0.3554399203986528, 0.425185545231065, 0.483829516659149, 0.441764857127547, 0.7569903264832838, 0.9902973951832582, 0.39848310607482496, 0.47828916179479153, 0.8917885428942248, 0.5951539561049004, 0.36071258233184644, 0.7218807037364038, 0.23273005506751965, 0.49855136281471535]}}
{"id": "fb85ae98-5026-4b8e-8159-54a063aa6489", "fitness": 0.36373956433214893, "name": "DynamicExplorationExploitation", "description": "A population-based algorithm employing a dynamic adaptation of exploration and exploitation rates based on a combination of fitness improvement and diversity maintenance using a distance-based metric.", "code": "import numpy as np\n\nclass DynamicExplorationExploitation:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_exploration_rate=0.5, initial_exploitation_rate=0.2, diversity_threshold=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.exploration_rate = initial_exploration_rate\n        self.exploitation_rate = initial_exploitation_rate\n        self.diversity_threshold = diversity_threshold\n        self.archive = []\n        self.archive_size = 10\n\n    def __call__(self, func):\n        # Initialize population within the bounds\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        while self.budget > 0:\n            new_population = np.zeros_like(population)\n            new_fitness = np.zeros_like(fitness)\n\n            for i in range(self.pop_size):\n                if np.random.rand() < self.exploration_rate:\n                    # Global Exploration: Random perturbation\n                    new_individual = population[i] + np.random.normal(0, 0.5, size=self.dim)\n                    new_individual = np.clip(new_individual, func.bounds.lb, func.bounds.ub)\n\n                else:\n                    # Local Exploitation: Perturbation around the best solution and/or archive\n                    if len(self.archive) > 0 and np.random.rand() < self.exploitation_rate:\n                        # Exploitation using archive\n                        archive_idx = np.random.randint(len(self.archive))\n                        new_individual = population[i] + 0.5 * (self.archive[archive_idx] - population[i]) + np.random.normal(0, 0.1, size=self.dim)\n                        new_individual = np.clip(new_individual, func.bounds.lb, func.bounds.ub)\n                    else:\n                        # Exploitation around best\n                        new_individual = population[i] + np.random.normal(0, 0.1, size=self.dim) + 0.1 * (self.x_opt - population[i])\n                        new_individual = np.clip(new_individual, func.bounds.lb, func.bounds.ub)\n\n                new_fitness[i] = func(new_individual)\n                self.budget -= 1\n                if new_fitness[i] < self.f_opt:\n                    self.f_opt = new_fitness[i]\n                    self.x_opt = new_individual\n\n                new_population[i] = new_individual\n\n            # Update population\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    population[i] = new_population[i]\n                    fitness[i] = new_fitness[i]\n\n                    # Archive the solution\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(new_population[i].copy())\n                    else:\n                        # Replace a random archive member\n                        replace_idx = np.random.randint(self.archive_size)\n                        self.archive[replace_idx] = new_population[i].copy()\n\n            # Adaptive parameter control based on improvement and diversity\n            improvement_ratio = np.mean(new_fitness < fitness)\n            \n            # Calculate average pairwise distance (diversity metric)\n            distances = []\n            for i in range(self.pop_size):\n                for j in range(i + 1, self.pop_size):\n                    distances.append(np.linalg.norm(population[i] - population[j]))\n            avg_distance = np.mean(distances) if distances else 0.0\n\n\n            if improvement_ratio > 0.2 and avg_distance > self.diversity_threshold:\n                self.exploration_rate *= 0.95  # Reduce exploration if exploitation is working and diversity is high\n                self.exploitation_rate = min(1.0, self.exploitation_rate * 1.05) #Increase exploitation\n            elif improvement_ratio <= 0.1 or avg_distance <= self.diversity_threshold:\n                self.exploration_rate = min(1.0, self.exploration_rate * 1.05)  # Increase exploration if not improving or low diversity\n                self.exploitation_rate *= 0.95 # Decrease exploitation\n            \n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm DynamicExplorationExploitation scored 0.364 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2bf27f2b-7afa-41f0-9213-1db959036489"], "operator": null, "metadata": {"aucs": [0.15786463715599963, 0.2715340679518148, 0.35159895240805694, 0.4116129612017456, 0.2671765802946343, 0.35063445412299665, 0.27474991272674254, 0.3143065253187931, 0.28003605156903477, 0.1842122347448888, 0.3835410724922079, 0.9825975069764109, 0.30861751059464193, 0.2711747979546384, 0.7266971590236957, 0.33602013309778545, 0.3047262648589806, 0.4172531428679924, 0.18941330836568626, 0.49102401291623166]}}
{"id": "89c94d9f-24ed-46c5-a3aa-e0a01af3e8d9", "fitness": -Infinity, "name": "PSO_NM", "description": "A population-based algorithm that combines the exploration of a particle swarm optimization with the exploitation of a Nelder-Mead simplex method, adaptively switching between them based on population diversity.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass PSO_NM:\n    def __init__(self, budget=10000, dim=10, pop_size=20, w=0.7, c1=1.5, c2=1.5, switch_prob=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w = w  # Inertia weight\n        self.c1 = c1 # Cognitive coefficient\n        self.c2 = c2 # Social coefficient\n        self.switch_prob = switch_prob #Probability of switching to Nelder-Mead\n        self.lb = -5.0\n        self.ub = 5.0\n\n        self.population = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = None\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))\n        self.personal_best_positions = self.population.copy()\n        self.personal_best_fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(self.personal_best_fitness)\n        self.global_best_position = self.personal_best_positions[best_index].copy()\n        self.global_best_fitness = self.personal_best_fitness[best_index]\n\n    def pso_step(self, func):\n        r1 = np.random.rand(self.pop_size, self.dim)\n        r2 = np.random.rand(self.pop_size, self.dim)\n\n        cognitive_component = self.c1 * r1 * (self.personal_best_positions - self.population)\n        social_component = self.c2 * r2 * (self.global_best_position - self.population)\n\n        self.velocities = self.w * self.velocities + cognitive_component + social_component\n        self.population = self.population + self.velocities\n        self.population = np.clip(self.population, self.lb, self.ub)\n\n        fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        for i in range(self.pop_size):\n            if fitness[i] < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = fitness[i]\n                self.personal_best_positions[i] = self.population[i].copy()\n\n                if fitness[i] < self.global_best_fitness:\n                    self.global_best_fitness = fitness[i]\n                    self.global_best_position = self.population[i].copy()\n    \n    def nelder_mead_optimization(self, func, x0):\n        if self.budget <= 0:\n            return x0, func(x0)\n        \n        bounds = [(self.lb, self.ub)] * self.dim\n        res = minimize(func, x0, method='Nelder-Mead', bounds=bounds, options={'maxfev': self.budget})\n        \n        self.budget -= res.nfev\n        return res.x, res.fun\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        \n        while self.budget > 0:\n            if np.random.rand() < self.switch_prob:\n                # Select a random particle to refine with Nelder-Mead\n                index = np.random.randint(self.pop_size)\n                self.population[index], fitness = self.nelder_mead_optimization(func, self.population[index])\n                \n                if fitness < self.personal_best_fitness[index]:\n                    self.personal_best_fitness[index] = fitness\n                    self.personal_best_positions[index] = self.population[index].copy()\n                    \n                    if fitness < self.global_best_fitness:\n                        self.global_best_fitness = fitness\n                        self.global_best_position = self.population[index].copy()\n            else:\n                self.pso_step(func)\n            \n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 1, "feedback": "An exception occurred: name 'minimize' is not defined.", "error": "", "parent_ids": ["b662a24c-0f7d-4880-9956-6d88d53e056c"], "operator": null, "metadata": {}}
{"id": "4b47faef-5f73-44e5-926f-0569b4fa54a0", "fitness": 0.0, "name": "AdaptiveExplorationExploitation", "description": "Adaptive exploration-exploitation with orthogonal learning, momentum, and dynamic population size.", "code": "import numpy as np\n\nclass AdaptiveExplorationExploitation:\n    def __init__(self, budget=10000, dim=10, pop_size_init=20, exploration_rate=0.5, exploitation_rate=0.2, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_init = pop_size_init\n        self.exploration_rate = exploration_rate\n        self.exploitation_rate = exploitation_rate\n        self.archive = [] # Store successful solutions\n        self.archive_size = archive_size\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.momentum = 0.1  # Momentum for updating positions\n        self.success_history = []\n\n    def __call__(self, func):\n        # Adaptive Population Size\n        pop_size = min(self.pop_size_init, int(np.sqrt(self.budget)))\n        \n        # Initialize population within the bounds\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n        \n        velocities = np.zeros_like(population)  # Initialize velocities for momentum\n\n        while self.budget > 0:\n            new_population = np.zeros_like(population)\n            new_fitness = np.zeros_like(fitness)\n\n            for i in range(pop_size):\n                if np.random.rand() < self.exploration_rate:\n                    # Global Exploration: Orthogonal Learning\n                    basis_vectors = np.random.normal(0, 1, size=(self.dim, self.dim))\n                    Q, _ = np.linalg.qr(basis_vectors)  # Orthogonalize\n                    direction = np.dot(np.random.normal(0, 1, size=self.dim), Q) #random direction sampled from basis\n                    new_individual = population[i] + 0.5 * direction\n                    new_individual = np.clip(new_individual, func.bounds.lb, func.bounds.ub)  # Keep within bounds\n\n                else:\n                    # Local Exploitation: Perturbation around the best solution and/or archive\n                    if len(self.archive) > 0 and np.random.rand() < self.exploitation_rate:\n                        # Exploitation using archive\n                        archive_idx = np.random.randint(len(self.archive))\n                        new_individual = population[i] + 0.5 * (self.archive[archive_idx] - population[i]) + np.random.normal(0, 0.1, size=self.dim)\n                        new_individual = np.clip(new_individual, func.bounds.lb, func.bounds.ub)\n\n                    else:\n                        # Exploitation around best\n                        new_individual = population[i] + np.random.normal(0, 0.1, size=self.dim) + 0.1 * (self.x_opt - population[i]) # Add attraction to best\n                        new_individual = np.clip(new_individual, func.bounds.lb, func.bounds.ub)\n\n                new_fitness[i] = func(new_individual)\n                self.budget -= 1\n\n                # Update best solution\n                if new_fitness[i] < self.f_opt:\n                    self.f_opt = new_fitness[i]\n                    self.x_opt = new_individual\n\n                new_population[i] = new_individual\n                \n            # Momentum update\n            for i in range(pop_size):\n                velocities[i] = self.momentum * velocities[i] + (new_population[i] - population[i])\n                population[i] = new_population[i] + velocities[i]\n                population[i] = np.clip(population[i], func.bounds.lb, func.bounds.ub)\n                \n            fitness = np.array([func(x) for x in population])\n            self.budget -= pop_size\n\n            # Update population - Elitism\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.concatenate((fitness, new_fitness))\n            \n            sorted_indices = np.argsort(combined_fitness)\n            population = combined_population[sorted_indices[:pop_size]]\n            fitness = combined_fitness[sorted_indices[:pop_size]]\n\n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n                #Archive the solution\n                if len(self.archive) < self.archive_size:\n                    self.archive.append(population[best_index].copy())\n                else:\n                    # Replace a random archive member\n                    replace_idx = np.random.randint(self.archive_size)\n                    self.archive[replace_idx] = population[best_index].copy()\n\n            # Adaptive parameter control (simplified)\n            success_rate = np.mean(new_fitness < fitness)\n            self.success_history.append(success_rate)\n            if len(self.success_history) > 10:\n                self.success_history.pop(0) # Keep a moving window\n\n            avg_success = np.mean(self.success_history)\n            if avg_success > 0.2:\n                self.exploration_rate = max(0.05, self.exploration_rate * 0.95) #Reduce exploration if exploitation is working\n                self.exploitation_rate = min(0.95, self.exploitation_rate * 1.05)\n            else:\n                self.exploration_rate = min(0.95, self.exploration_rate * 1.05) #Increase exploration otherwise\n                self.exploitation_rate = max(0.05, self.exploitation_rate * 0.95)\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveExplorationExploitation scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2bf27f2b-7afa-41f0-9213-1db959036489"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "5ab4e0ee-74f3-424b-b36e-c2fb8280319c", "fitness": 0.0, "name": "OrthogonalDE", "description": "An adaptive Differential Evolution strategy that incorporates orthogonal learning to enhance search space exploration and convergence.", "code": "import numpy as np\n\nclass OrthogonalDE:\n    def __init__(self, budget=10000, dim=10, F_init=0.5, Cr=0.9, ol_sample_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.F_init = F_init\n        self.Cr = Cr\n        self.pop_size = int(4 + 3 * np.log(budget))\n        self.population = None\n        self.fitness = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.F = np.full(self.pop_size, self.F_init)\n        self.ol_sample_size = ol_sample_size # Number of samples to generate for orthogonal learning.\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n\n    def mutate(self, pop, i):\n        indices = [idx for idx in range(len(pop)) if idx != i]\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutation = pop[a] + self.F[i] * (pop[b] - pop[c])\n        return mutation\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.Cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def orthogonal_learning(self, func, current_individual):\n        \"\"\"\n        Generate 'ol_sample_size' samples based on orthogonal design around the current individual\n        and select the best one.\n        \"\"\"\n        best_sample = current_individual\n        best_fitness = func(current_individual)\n        \n        for _ in range(self.ol_sample_size):\n            # Generate a random sample within a hypercube around the current individual.\n            sample = current_individual + np.random.uniform(-0.1, 0.1, size=self.dim)  # Small perturbation\n            sample = np.clip(sample, self.bounds_lb, self.bounds_ub)\n            \n            fitness = func(sample)\n            if fitness < best_fitness:\n                best_fitness = fitness\n                best_sample = sample\n        \n        return best_sample, best_fitness\n    \n    def adapt_mutation_factor(self):\n        for i in range(self.pop_size):\n            if np.random.rand() < 0.1:  # Probabilistically adapt F\n                if np.random.rand() < 0.5:\n                    self.F[i] *= 1.1  # Increase F\n                else:\n                    self.F[i] *= 0.9  # Decrease F\n                self.F[i] = np.clip(self.F[i], 0.1, 1.0)  # Keep F within [0.1, 1.0]\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n        \n        fevals = self.pop_size\n\n        while fevals < self.budget:\n            for i in range(self.pop_size):\n                mutant = self.mutate(self.population, i)\n                mutant = np.clip(mutant, self.bounds_lb, self.bounds_ub)\n                trial = self.crossover(mutant, self.population[i])\n                \n                f = func(trial)\n                fevals += 1\n\n                # Orthogonal learning around the trial vector\n                ol_sample, ol_fitness = self.orthogonal_learning(func, trial)\n                fevals += self.ol_sample_size # each orthogonal learning takes ol_sample_size fevals\n                \n                if ol_fitness < self.fitness[i]:\n                    self.fitness[i] = ol_fitness\n                    self.population[i] = ol_sample\n                    \n\n                    if ol_fitness < self.f_opt:\n                        self.f_opt = ol_fitness\n                        self.x_opt = ol_sample\n                \n                if fevals >= self.budget:\n                    break\n            \n            self.adapt_mutation_factor()\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm OrthogonalDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["bbc35178-a913-4580-b812-202bf7339e9f"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "a9d22548-70f3-4a6f-b3fc-ec1979be8e30", "fitness": 0.0, "name": "AdaptiveHybridPSO", "description": "Adaptive Hybrid PSO with aging mechanism, dynamic parameter adjustment, and a restarting strategy to escape local optima.", "code": "import numpy as np\n\nclass AdaptiveHybridPSO:\n    def __init__(self, budget=10000, dim=10, pop_size=20, inertia_start=0.9, inertia_end=0.4,\n                 cognitive_coeff=1.5, social_coeff=1.5, local_search_prob=0.1, age_limit=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.inertia_start = inertia_start\n        self.inertia_end = inertia_end\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.local_search_prob = local_search_prob\n        self.age_limit = age_limit\n        self.velocities = None\n        self.ages = None\n\n\n    def __local_search(self, func, x, current_f, step_size=0.1):\n        \"\"\"Performs a local search around a given point.\"\"\"\n        best_x = x\n        best_f = current_f\n        evals = 0\n\n        for _ in range(5):  # Limited local search evaluations\n            neighbor = x + np.random.uniform(-step_size, step_size, size=self.dim)\n            neighbor = np.clip(neighbor, func.bounds.lb, func.bounds.ub)\n            neighbor_f = func(neighbor)\n            evals += 1\n            if neighbor_f < best_f:\n                best_f = neighbor_f\n                best_x = neighbor\n        return best_x, best_f, evals\n\n    def __call__(self, func):\n        # Initialize population and velocities within the bounds\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, size=(self.pop_size, self.dim)) #Initialize velocities\n        self.ages = np.zeros(self.pop_size)\n\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n        personal_best_positions = population.copy() # Initialize personal best positions\n        personal_best_fitness = fitness.copy()\n        eval_count = 0\n\n        while self.budget > 0:\n            inertia = self.inertia_start - (self.inertia_start - self.inertia_end) * (eval_count / self.budget)\n\n            for i in range(self.pop_size):\n                # Update velocity\n                r1, r2 = np.random.rand(2)\n                self.velocities[i] = (inertia * self.velocities[i] +\n                                      self.cognitive_coeff * r1 * (personal_best_positions[i] - population[i]) +\n                                      self.social_coeff * r2 * (self.x_opt - population[i]))\n\n                # Update position\n                new_position = population[i] + self.velocities[i]\n                new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n                \n                # Local Search\n                local_search_evals = 0\n                if np.random.rand() < self.local_search_prob and self.budget > 0:\n                    new_position, new_fitness, local_search_evals = self.__local_search(func, new_position, func(new_position))\n                else:\n                    new_fitness = func(new_position)\n                    local_search_evals = 1\n\n                self.budget -= local_search_evals\n                eval_count += local_search_evals\n\n                if self.budget <= 0:\n                    break               \n\n                # Update personal best\n                if new_fitness < personal_best_fitness[i]:\n                    personal_best_fitness[i] = new_fitness\n                    personal_best_positions[i] = new_position.copy()\n                    self.ages[i] = 0  # Reset age if improved\n                else:\n                    self.ages[i] += 1 #Increment age\n\n                #Update global best\n                if new_fitness < self.f_opt:\n                    self.f_opt = new_fitness\n                    self.x_opt = new_position.copy()\n                    \n\n                population[i] = new_position\n\n\n            #Aging and Restarting Mechanism:\n            for i in range(self.pop_size):\n                if self.ages[i] > self.age_limit:\n                    population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                    self.velocities[i] = np.random.uniform(-0.1, 0.1, size=self.dim)\n                    personal_best_positions[i] = population[i].copy()\n                    personal_best_fitness[i] = func(population[i])\n                    self.budget -= 1\n                    eval_count +=1\n                    self.ages[i] = 0\n                    if personal_best_fitness[i] < self.f_opt:\n                         self.f_opt = personal_best_fitness[i]\n                         self.x_opt = population[i].copy()\n\n            #Adaptive local search probability (simplified)\n            if np.mean(personal_best_fitness < fitness) > 0.2:\n                self.local_search_prob = min(1.0, self.local_search_prob * 1.05)\n            else:\n                self.local_search_prob *= 0.95\n            \n            if self.budget <= 0:\n                break\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveHybridPSO scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["dbc28305-35a2-4058-b975-d7fabea855ee"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "8b9fb0e5-29a1-48ea-a73d-bd584697db61", "fitness": 0.0, "name": "HybridPSO", "description": "Hybrid PSO with adaptive parameters, elitist archive for global best influence, and enhanced local search.", "code": "import numpy as np\n\nclass HybridPSO:\n    def __init__(self, budget=10000, dim=10, pop_size=20, inertia_start=0.9, inertia_end=0.4, cognitive_coeff=1.5, social_coeff=1.5, local_search_prob=0.1, archive_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.inertia_start = inertia_start\n        self.inertia_end = inertia_end\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.local_search_prob = local_search_prob\n        self.archive_size = archive_size\n        self.archive = []\n        self.archive_fitness = []\n        self.velocities = None\n\n    def __local_search(self, func, x, step_size=0.1, num_evals=5):\n        \"\"\"Performs a local search around a given point with a budget.\"\"\"\n        best_x = x.copy()\n        best_f = func(x)\n        self.budget -= 1\n        if self.budget <= 0:\n            return best_x, best_f\n        \n        for _ in range(num_evals):\n            if self.budget <= 0:\n                break\n            neighbor = x + np.random.uniform(-step_size, step_size, size=self.dim)\n            neighbor = np.clip(neighbor, func.bounds.lb, func.bounds.ub)\n            neighbor_f = func(neighbor)\n            self.budget -= 1\n\n            if neighbor_f < best_f:\n                best_f = neighbor_f\n                best_x = neighbor.copy()\n\n        return best_x, best_f\n\n    def __update_archive(self, x, f):\n        \"\"\"Updates the archive with the new solution if it's better.\"\"\"\n        if len(self.archive) < self.archive_size:\n            self.archive.append(x.copy())\n            self.archive_fitness.append(f)\n        else:\n            max_fitness_index = np.argmax(self.archive_fitness)\n            if f < self.archive_fitness[max_fitness_index]:\n                self.archive[max_fitness_index] = x.copy()\n                self.archive_fitness[max_fitness_index] = f\n\n    def __call__(self, func):\n        # Initialize population and velocities within the bounds\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, size=(self.pop_size, self.dim)) #Initialize velocities\n\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n        personal_best_positions = population.copy() # Initialize personal best positions\n        personal_best_fitness = fitness.copy()\n\n        self.__update_archive(self.x_opt, self.f_opt)\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            inertia = self.inertia_start - (self.inertia_start - self.inertia_end) * generation / (self.budget / self.pop_size + generation) #Linear inertia decay\n            \n            for i in range(self.pop_size):\n                # Update velocity\n                r1, r2 = np.random.rand(2)\n                \n                #Influence from archive\n                if self.archive:\n                    archive_index = np.random.randint(len(self.archive))\n                    global_best = self.archive[archive_index]\n                else:\n                    global_best = self.x_opt\n                    \n                self.velocities[i] = (inertia * self.velocities[i] +\n                                      self.cognitive_coeff * r1 * (personal_best_positions[i] - population[i]) +\n                                      self.social_coeff * r2 * (global_best - population[i]))\n\n                # Update position\n                new_position = population[i] + self.velocities[i]\n                new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n                \n                new_fitness = func(new_position)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n                    \n                # Local Search with probability and budget check\n                if np.random.rand() < self.local_search_prob and self.budget > 0:\n                    new_position, new_fitness = self.__local_search(func, new_position, num_evals=3)\n\n                # Update personal best\n                if new_fitness < personal_best_fitness[i]:\n                    personal_best_fitness[i] = new_fitness\n                    personal_best_positions[i] = new_position.copy()\n\n                #Update global best\n                if new_fitness < self.f_opt:\n                    self.f_opt = new_fitness\n                    self.x_opt = new_position.copy()\n                    self.__update_archive(self.x_opt, self.f_opt)\n            \n                population[i] = new_position\n            if self.budget <= 0:\n                break\n            \n            #Adaptive local search probability (simplified)\n            if np.mean(personal_best_fitness < fitness) > 0.2:\n                self.local_search_prob = min(1.0, self.local_search_prob * 1.05)\n            else:\n                self.local_search_prob *= 0.95\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm HybridPSO scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["dbc28305-35a2-4058-b975-d7fabea855ee"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "2d24fa46-dc87-4ba3-9a25-88129efd948f", "fitness": 0.0, "name": "HybridPSO", "description": "Hybrid PSO with adaptive parameters, velocity clamping, and stagnation detection to enhance exploration and exploitation.", "code": "import numpy as np\n\nclass HybridPSO:\n    def __init__(self, budget=10000, dim=10, pop_size=20, inertia_start=0.9, inertia_end=0.4,\n                 cognitive_coeff=1.5, social_coeff=1.5, local_search_prob=0.1, velocity_clamp=0.5, stagnation_threshold=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.inertia_start = inertia_start\n        self.inertia_end = inertia_end\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.local_search_prob = local_search_prob\n        self.velocity_clamp = velocity_clamp\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n\n        self.velocities = None\n\n    def __local_search(self, func, x, current_f, step_size=0.1):\n        \"\"\"Performs a local search around a given point.\"\"\"\n        best_x = x\n        best_f = current_f\n\n        for _ in range(5):  # Limited local search evaluations\n            neighbor = x + np.random.uniform(-step_size, step_size, size=self.dim)\n            neighbor = np.clip(neighbor, func.bounds.lb, func.bounds.ub)\n            neighbor_f = func(neighbor)\n            self.budget -= 1\n            if self.budget <= 0:\n                break\n\n            if neighbor_f < best_f:\n                best_f = neighbor_f\n                best_x = neighbor\n        return best_x, best_f\n\n    def __call__(self, func):\n        # Initialize population and velocities within the bounds\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, size=(self.pop_size, self.dim)) #Initialize velocities\n\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n        personal_best_positions = population.copy() # Initialize personal best positions\n        personal_best_fitness = fitness.copy()\n\n        while self.budget > 0:\n            # Adaptive Inertia Weight\n            inertia = self.inertia_start - (self.inertia_start - self.inertia_end) * (1 - self.budget / self.budget)\n\n            for i in range(self.pop_size):\n                # Update velocity\n                r1, r2 = np.random.rand(2)\n                self.velocities[i] = (inertia * self.velocities[i] +\n                                      self.cognitive_coeff * r1 * (personal_best_positions[i] - population[i]) +\n                                      self.social_coeff * r2 * (self.x_opt - population[i]))\n                \n                # Velocity clamping\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n\n                # Update position\n                new_position = population[i] + self.velocities[i]\n                new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n                \n                # Local Search\n                if np.random.rand() < self.local_search_prob and self.budget > 0:\n                    new_position, new_fitness = self.__local_search(func, new_position, func(new_position))\n                else:\n                    new_fitness = func(new_position)\n                    self.budget -= 1\n\n                if self.budget <= 0:\n                    break\n\n                # Update personal best\n                if new_fitness < personal_best_fitness[i]:\n                    personal_best_fitness[i] = new_fitness\n                    personal_best_positions[i] = new_position.copy()\n\n                #Update global best\n                if new_fitness < self.f_opt:\n                    self.f_opt = new_fitness\n                    self.x_opt = new_position.copy()\n            \n                population[i] = new_position\n            if self.budget <= 0:\n                break\n\n            #Adaptive local search probability (simplified)\n            if np.mean(personal_best_fitness < fitness) > 0.2:\n                self.local_search_prob = min(1.0, self.local_search_prob * 1.05)\n            else:\n                self.local_search_prob *= 0.95\n\n            # Stagnation Detection\n            self.best_fitness_history.append(self.f_opt)\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                self.best_fitness_history.pop(0)\n                if np.std(self.best_fitness_history) < 1e-6:\n                    self.stagnation_counter += 1\n                    if self.stagnation_counter > 3:\n                        # Reset a portion of the population if stagnating\n                        reset_indices = np.random.choice(self.pop_size, size=self.pop_size // 4, replace=False)\n                        population[reset_indices] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(len(reset_indices), self.dim))\n                        fitness[reset_indices] = [func(x) for x in population[reset_indices]]\n                        self.budget -= len(reset_indices)\n                        personal_best_positions[reset_indices] = population[reset_indices].copy()\n                        personal_best_fitness[reset_indices] = fitness[reset_indices].copy()\n                        best_index = np.argmin(fitness)\n                        self.f_opt = fitness[best_index]\n                        self.x_opt = population[best_index]\n                        self.stagnation_counter = 0  # Reset stagnation counter\n            else:\n                 self.stagnation_counter = 0\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm HybridPSO scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["dbc28305-35a2-4058-b975-d7fabea855ee"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "31213c3b-dc34-4961-b76c-a6d9e6cb98b0", "fitness": 0.0, "name": "HybridPSO", "description": "Enhanced Hybrid PSO with adaptive parameters, velocity clamping, and a restart mechanism for stagnation.", "code": "import numpy as np\n\nclass HybridPSO:\n    def __init__(self, budget=10000, dim=10, pop_size=20, inertia_start=0.9, inertia_end=0.4, cognitive_coeff=2.0, social_coeff=2.0, local_search_prob=0.1, velocity_clamp=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.inertia_start = inertia_start\n        self.inertia_end = inertia_end\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.local_search_prob = local_search_prob\n        self.velocity_clamp = velocity_clamp\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n\n        self.velocities = None\n\n    def __local_search(self, func, x, current_f, step_size=0.1):\n        \"\"\"Performs a local search around a given point.\"\"\"\n        best_x = x\n        best_f = current_f\n        evaluations = 0\n\n        for _ in range(5):  # Limited local search evaluations\n            neighbor = x + np.random.uniform(-step_size, step_size, size=self.dim)\n            neighbor = np.clip(neighbor, func.bounds.lb, func.bounds.ub)\n            neighbor_f = func(neighbor)\n            self.budget -= 1\n            evaluations +=1\n            if self.budget <= 0:\n                break\n\n            if neighbor_f < best_f:\n                best_f = neighbor_f\n                best_x = neighbor\n        return best_x, best_f, evaluations\n\n    def __call__(self, func):\n        # Initialize population and velocities within the bounds\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.random.uniform(-self.velocity_clamp, self.velocity_clamp, size=(self.pop_size, self.dim)) #Initialize velocities\n\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n        personal_best_positions = population.copy() # Initialize personal best positions\n        personal_best_fitness = fitness.copy()\n        evaluations = self.pop_size\n\n        while self.budget > 0:\n            inertia = self.inertia_start - (self.inertia_start - self.inertia_end) * (evaluations / 10000) #Linear inertia decay\n\n            for i in range(self.pop_size):\n                # Update velocity\n                r1, r2 = np.random.rand(2)\n                self.velocities[i] = (inertia * self.velocities[i] +\n                                      self.cognitive_coeff * r1 * (personal_best_positions[i] - population[i]) +\n                                      self.social_coeff * r2 * (self.x_opt - population[i]))\n                # Velocity clamping\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n\n                # Update position\n                new_position = population[i] + self.velocities[i]\n                new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n                \n                # Local Search\n                if np.random.rand() < self.local_search_prob and self.budget > 0:\n                    new_position, new_fitness, ls_evals = self.__local_search(func, new_position, func(new_position))\n                    evaluations += ls_evals\n                else:\n                    new_fitness = func(new_position)\n                    self.budget -= 1\n                    evaluations += 1\n\n                if self.budget <= 0:\n                    break\n                \n\n                # Update personal best\n                if new_fitness < personal_best_fitness[i]:\n                    personal_best_fitness[i] = new_fitness\n                    personal_best_positions[i] = new_position.copy()\n\n                #Update global best\n                if new_fitness < self.f_opt:\n                    self.f_opt = new_fitness\n                    self.x_opt = new_position.copy()\n            \n                population[i] = new_position\n            if self.budget <= 0:\n                break\n\n            # Adaptive local search probability (simplified)\n            if np.mean(personal_best_fitness < fitness) > 0.2:\n                self.local_search_prob = min(1.0, self.local_search_prob * 1.05)\n            else:\n                self.local_search_prob *= 0.95\n            \n            #Stagnation detection and restart\n            self.best_fitness_history.append(self.f_opt)\n            if len(self.best_fitness_history) > 200:\n                self.best_fitness_history.pop(0)\n                if np.std(self.best_fitness_history) < 1e-8:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0\n\n            if self.stagnation_counter > 5: #Restart if stagnated for 5 consecutive times\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.velocities = np.random.uniform(-self.velocity_clamp, self.velocity_clamp, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                evaluations += self.pop_size\n        \n                best_index = np.argmin(fitness)\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n                personal_best_positions = population.copy()\n                personal_best_fitness = fitness.copy()\n                self.stagnation_counter = 0\n                self.best_fitness_history = []\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm HybridPSO scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["dbc28305-35a2-4058-b975-d7fabea855ee"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "d04ca00a-c8d1-4622-8fe7-2e7817e5df76", "fitness": 0.3743324181636426, "name": "AdaptiveClampingPSO", "description": "Particle Swarm Optimization with a self-adaptive velocity clamping mechanism and a restart strategy based on stagnation detection.", "code": "import numpy as np\n\nclass AdaptiveClampingPSO:\n    def __init__(self, budget=10000, dim=10, pop_size=20, inertia=0.7, cognitive_coeff=1.5, social_coeff=1.5, stagnation_threshold=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.inertia = inertia\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.stagnation_threshold = stagnation_threshold\n        self.velocity_clamp = 1.0  # Initial velocity clamp\n\n        self.velocities = None\n        self.stagnation_counter = 0\n        self.last_improvement = 0\n\n    def __restart(self, func):\n        \"\"\"Restarts the population if stagnation is detected.\"\"\"\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n        personal_best_positions = population.copy()\n        personal_best_fitness = fitness.copy()\n        self.stagnation_counter = 0\n        self.last_improvement = 0\n\n        return population, fitness, personal_best_positions, personal_best_fitness\n\n    def __call__(self, func):\n        # Initialize population and velocities within the bounds\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, size=(self.pop_size, self.dim))\n\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n        personal_best_positions = population.copy() # Initialize personal best positions\n        personal_best_fitness = fitness.copy()\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Update velocity\n                r1, r2 = np.random.rand(2)\n                velocity = (self.inertia * self.velocities[i] +\n                            self.cognitive_coeff * r1 * (personal_best_positions[i] - population[i]) +\n                            self.social_coeff * r2 * (self.x_opt - population[i]))\n\n                # Apply velocity clamping\n                norm = np.linalg.norm(velocity)\n                if norm > self.velocity_clamp:\n                    velocity = velocity * (self.velocity_clamp / norm)\n\n                self.velocities[i] = velocity #assign clamped velocity to the particle velocity\n\n                # Update position\n                new_position = population[i] + self.velocities[i]\n                new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n                \n                new_fitness = func(new_position)\n                self.budget -= 1\n\n                if self.budget <= 0:\n                    break\n                \n\n                # Update personal best\n                if new_fitness < personal_best_fitness[i]:\n                    personal_best_fitness[i] = new_fitness\n                    personal_best_positions[i] = new_position.copy()\n\n                #Update global best\n                if new_fitness < self.f_opt:\n                    self.f_opt = new_fitness\n                    self.x_opt = new_position.copy()\n                    self.stagnation_counter = 0 # Reset stagnation counter\n                    self.last_improvement = i\n                else:\n                     self.stagnation_counter += 1\n            \n                population[i] = new_position\n            if self.budget <= 0:\n                break\n            \n            # Stagnation check and restart\n            if self.stagnation_counter > self.stagnation_threshold:\n                population, fitness, personal_best_positions, personal_best_fitness = self.__restart(func)\n                #Reduce velocity clamping\n                self.velocity_clamp = min(5.0, self.velocity_clamp * 1.1) \n                self.stagnation_counter = 0\n\n            #Adaptive velocity clamping\n            if self.stagnation_counter > self.stagnation_threshold / 2:\n                self.velocity_clamp = max(0.1, self.velocity_clamp * 0.95) #reduce velocity clamp\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveClampingPSO scored 0.374 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["dbc28305-35a2-4058-b975-d7fabea855ee"], "operator": null, "metadata": {"aucs": [0.12192929815204956, 0.2408085039195087, 0.4384843120373627, 0.35299896170891565, 0.22544809757145945, 0.4673129596169995, 0.2720566474695948, 0.33522786334857646, 0.2862894145049013, 0.21866322545862038, 0.5689460302508674, 0.9715067340058576, 0.2914601801936565, 0.22810538576363737, 0.6955773451626603, 0.339832003428844, 0.3130155300678844, 0.4288256305320549, 0.19117634461042965, 0.4989838954689718]}}
{"id": "ab65cdb6-aa67-45d6-ac94-9d0d33fdc006", "fitness": -Infinity, "name": "AdaptiveDEMultiMutation", "description": "An adaptive Differential Evolution using a combination of multiple mutation strategies and a self-adjusting population size based on the optimization progress.", "code": "import numpy as np\n\nclass AdaptiveDEMultiMutation:\n    def __init__(self, budget=10000, dim=10, F_init=0.5, Cr=0.9, pop_multiplier=4):\n        self.budget = budget\n        self.dim = dim\n        self.F_init = F_init\n        self.Cr = Cr\n        self.pop_multiplier = pop_multiplier  # Scale initial population size\n        self.pop_size = int(self.pop_multiplier + 3 * np.log(budget))\n        self.population = None\n        self.fitness = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.F = np.full(self.pop_size, self.F_init)\n        self.fevals = 0\n        self.success_history = [[] for _ in range(self.pop_size)] # Track success of each individual\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.fevals += self.pop_size\n\n    def mutate(self, pop, i):\n        indices = [idx for idx in range(len(pop)) if idx != i]\n        a, b, c = np.random.choice(indices, 3, replace=False)\n\n        # Adaptive Strategy Selection\n        strategy = np.random.choice(['current-to-rand', 'rand-1', 'best-1'])\n\n        if strategy == 'current-to-rand':\n            mutation = pop[i] + self.F[i] * (pop[a] - pop[i]) + self.F[i] * (pop[b] - pop[c])\n        elif strategy == 'rand-1':\n            d, e = np.random.choice(indices, 2, replace=False)\n            mutation = pop[a] + self.F[i] * (pop[b] - pop[c]) + self.F[i] * (pop[d] - pop[e])\n        elif strategy == 'best-1':\n            best_index = np.argmin(self.fitness)\n            mutation = self.population[best_index] + self.F[i] * (pop[a] - pop[b])\n        else:  # Default to current-to-rand\n            mutation = pop[i] + self.F[i] * (pop[a] - pop[i]) + self.F[i] * (pop[b] - pop[c])\n           \n        return mutation\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.Cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def adapt_mutation_factor(self):\n        \"\"\"Adapt F based on recent success\"\"\"\n        for i in range(self.pop_size):\n            if self.success_history[i] and np.mean(self.success_history[i]) > 0.2: #If reasonably successful\n                self.F[i] *= 0.9  # Reduce F to exploit\n            else:\n                self.F[i] *= 1.1  # Increase F to explore\n            self.F[i] = np.clip(self.F[i], 0.1, 1.0)  # Keep F within bounds\n            self.success_history[i] = self.success_history[i][-10:] #Keep only the last 10 successes.\n\n    def adjust_population_size(self):\n        \"\"\"Dynamically adjust population size based on optimization progress.\"\"\"\n        improvement_threshold = 1e-5 #Threshold for considering as improvement\n        n_improvements = np.sum(np.diff(np.sort(self.fitness)) < improvement_threshold)\n        if n_improvements > self.pop_size // 4: # If number of improvements exceeds 1/4th of pop_size\n            self.pop_size = min(int(self.pop_size * 1.1), int(self.budget/2)) #Increase population size\n            print(f\"Increasing pop_size to {self.pop_size}\")\n        elif n_improvements < self.pop_size // 10 and self.pop_size > self.pop_multiplier: #If number of improvements is less than 1/10th of pop_size and is larger than a threshold\n            self.pop_size = max(int(self.pop_size * 0.9), self.pop_multiplier)  #Decrease population size\n            print(f\"Decreasing pop_size to {self.pop_size}\")\n        \n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n\n        while self.fevals < self.budget:\n            for i in range(self.pop_size):\n                mutant = self.mutate(self.population, i)\n                mutant = np.clip(mutant, self.bounds_lb, self.bounds_ub)\n                trial = self.crossover(mutant, self.population[i])\n\n                f = func(trial)\n                self.fevals += 1\n\n                if f < self.fitness[i]:\n                    self.success_history[i].append(1)\n                    self.fitness[i] = f\n                    self.population[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                else:\n                    self.success_history[i].append(0)\n\n                if self.fevals >= self.budget:\n                    break\n\n            self.adapt_mutation_factor()\n            self.adjust_population_size()\n           \n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "An exception occurred: index 31 is out of bounds for axis 0 with size 31.", "error": "", "parent_ids": ["bbc35178-a913-4580-b812-202bf7339e9f"], "operator": null, "metadata": {}}
{"id": "2b5c9234-3f08-4430-af9a-5b0aa11c0445", "fitness": 0.43200982224169654, "name": "DynamicExplorationExploitation", "description": "Enhanced Dynamic Exploration-Exploitation with adaptive exploration rate based on fitness variance and success rate, using a decaying step size for perturbations.", "code": "import numpy as np\n\nclass DynamicExplorationExploitation:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_exploration_rate=0.5, initial_exploitation_rate=0.2, diversity_threshold=0.1, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.exploration_rate = initial_exploration_rate\n        self.exploitation_rate = initial_exploitation_rate\n        self.diversity_threshold = diversity_threshold\n        self.archive = []\n        self.archive_size = archive_size\n        self.step_size = 0.5  # Initial step size for perturbations\n        self.step_decay = 0.995 # Decay factor for step size\n        self.success_history = []\n\n    def __call__(self, func):\n        # Initialize population within the bounds\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        while self.budget > 0:\n            new_population = np.zeros_like(population)\n            new_fitness = np.zeros_like(fitness)\n\n            for i in range(self.pop_size):\n                if np.random.rand() < self.exploration_rate:\n                    # Global Exploration: Random perturbation with decaying step size\n                    new_individual = population[i] + np.random.normal(0, self.step_size, size=self.dim)\n                    new_individual = np.clip(new_individual, func.bounds.lb, func.bounds.ub)\n\n                else:\n                    # Local Exploitation: Perturbation around the best solution and/or archive\n                    if len(self.archive) > 0 and np.random.rand() < self.exploitation_rate:\n                        # Exploitation using archive\n                        archive_idx = np.random.randint(len(self.archive))\n                        new_individual = population[i] + 0.5 * (self.archive[archive_idx] - population[i]) + np.random.normal(0, self.step_size * 0.2, size=self.dim)\n                        new_individual = np.clip(new_individual, func.bounds.lb, func.bounds.ub)\n                    else:\n                        # Exploitation around best\n                        new_individual = population[i] + np.random.normal(0, self.step_size * 0.2, size=self.dim) + 0.1 * (self.x_opt - population[i])\n                        new_individual = np.clip(new_individual, func.bounds.lb, func.bounds.ub)\n\n                new_fitness[i] = func(new_individual)\n                self.budget -= 1\n                if new_fitness[i] < self.f_opt:\n                    self.f_opt = new_fitness[i]\n                    self.x_opt = new_individual\n\n                new_population[i] = new_individual\n\n            # Update population and archive\n            num_improvements = 0\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    population[i] = new_population[i]\n                    fitness[i] = new_fitness[i]\n                    num_improvements += 1\n\n                    # Archive the solution\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(new_population[i].copy())\n                    else:\n                        # Replace a random archive member\n                        replace_idx = np.random.randint(self.archive_size)\n                        self.archive[replace_idx] = new_population[i].copy()\n            \n            # Adaptive parameter control based on improvement, diversity, and fitness variance\n            improvement_ratio = num_improvements / self.pop_size\n            self.success_history.append(improvement_ratio)\n            if len(self.success_history) > 10:\n                 self.success_history.pop(0)\n            avg_success = np.mean(self.success_history)\n\n            # Calculate average pairwise distance (diversity metric)\n            distances = []\n            for i in range(self.pop_size):\n                for j in range(i + 1, self.pop_size):\n                    distances.append(np.linalg.norm(population[i] - population[j]))\n            avg_distance = np.mean(distances) if distances else 0.0\n            \n            fitness_variance = np.var(fitness)\n\n            # Adjust exploration rate\n            if avg_success > 0.2 and avg_distance > self.diversity_threshold and fitness_variance < 0.1:\n                self.exploration_rate *= 0.95  # Reduce exploration if exploitation is working well, diversity is high, and fitness variance is low\n                self.exploitation_rate = min(1.0, self.exploitation_rate * 1.05)\n            elif avg_success <= 0.1 or avg_distance <= self.diversity_threshold:\n                self.exploration_rate = min(1.0, self.exploration_rate * 1.05)  # Increase exploration if not improving or low diversity\n                self.exploitation_rate *= 0.95\n            \n            # Step size decay\n            self.step_size *= self.step_decay\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm DynamicExplorationExploitation scored 0.432 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["fb85ae98-5026-4b8e-8159-54a063aa6489"], "operator": null, "metadata": {"aucs": [0.12661678497010598, 0.30741842120510143, 0.44278482157259724, 0.6605466519305322, 0.3242977831630497, 0.4848776355893898, 0.29420827271262573, 0.38020732311075056, 0.3747828997211946, 0.2044490892725468, 0.8346073916295821, 0.984320330827602, 0.2856131256122517, 0.30904288366181143, 0.7049983718530838, 0.34246038274794754, 0.3151430208516821, 0.5418574405638346, 0.22396072409983248, 0.4980030897384091]}}
{"id": "8e3cdd02-6ab1-453a-98f9-f6d98857a9d4", "fitness": 0.5718791129322294, "name": "AdaptiveDE_Restart", "description": "An adaptive differential evolution algorithm that incorporates a restart mechanism when stagnation is detected based on fitness variance and adapts the crossover rate based on individual success.", "code": "import numpy as np\n\nclass AdaptiveDE_Restart:\n    def __init__(self, budget=10000, dim=10, F_init=0.5, Cr_init=0.9, restart_patience=500):\n        self.budget = budget\n        self.dim = dim\n        self.F_init = F_init  # Initial mutation factor\n        self.Cr_init = Cr_init  # Initial crossover rate\n        self.pop_size = int(4 + 3 * np.log(budget))  # Population size that scales with budget\n        self.population = None\n        self.fitness = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.F = np.full(self.pop_size, self.F_init)\n        self.Cr = np.full(self.pop_size, self.Cr_init)\n        self.restart_patience = restart_patience\n        self.no_improvement_count = 0\n        self.best_fitness = np.inf\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n\n    def mutate(self, pop, i):\n        indices = [idx for idx in range(len(pop)) if idx != i]\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        # Cauchy mutation for enhanced exploration\n        mutation = pop[a] + self.F[i] * (pop[b] - pop[c]) + 0.01 * np.random.standard_cauchy(size=self.dim)\n        return mutation\n\n    def crossover(self, mutant, target, i):\n        cross_points = np.random.rand(self.dim) < self.Cr[i]\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def adapt_mutation_factor(self):\n        for i in range(self.pop_size):\n            if np.random.rand() < 0.1:  # Probabilistically adapt F\n                if np.random.rand() < 0.5:\n                    self.F[i] *= 1.1  # Increase F\n                else:\n                    self.F[i] *= 0.9  # Decrease F\n                self.F[i] = np.clip(self.F[i], 0.1, 1.0)  # Keep F within [0.1, 1.0]\n\n    def adapt_crossover_rate(self):\n        \"\"\"Adapt the crossover rate based on individual success.\"\"\"\n        for i in range(self.pop_size):\n            if np.random.rand() < 0.1:  # Probabilistically adapt Cr\n                if np.random.rand() < 0.5:\n                    self.Cr[i] = min(self.Cr[i] + 0.1, 1.0)  # Increase Cr\n                else:\n                    self.Cr[i] = max(self.Cr[i] - 0.1, 0.1)  # Decrease Cr\n\n    def check_stagnation(self):\n        \"\"\"Check for stagnation based on fitness variance.\"\"\"\n        fitness_variance = np.var(self.fitness)\n        if fitness_variance < 1e-6:  # Stagnation threshold\n            self.no_improvement_count += 1\n        else:\n            self.no_improvement_count = 0\n\n        if self.no_improvement_count > self.restart_patience:\n            return True\n        else:\n            return False\n\n    def restart_population(self, func):\n        \"\"\"Restart the population.\"\"\"\n        self.initialize_population(func)\n        self.no_improvement_count = 0\n        self.F = np.full(self.pop_size, self.F_init)\n        self.Cr = np.full(self.pop_size, self.Cr_init)\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n        \n        fevals = self.pop_size # Initial population evaluation\n\n        while fevals < self.budget:\n            for i in range(self.pop_size):\n                mutant = self.mutate(self.population, i)\n                mutant = np.clip(mutant, self.bounds_lb, self.bounds_ub)  # Clip to bounds\n                trial = self.crossover(mutant, self.population[i], i)\n                \n                f = func(trial)\n                fevals += 1\n\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.population[i] = trial\n                    \n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                        self.best_fitness = f\n                        self.no_improvement_count = 0 # Reset counter\n                \n                if fevals >= self.budget:\n                    break\n            \n            self.adapt_mutation_factor()\n            self.adapt_crossover_rate()\n            \n            if self.check_stagnation():\n                self.restart_population(func)\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDE_Restart scored 0.572 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["bbc35178-a913-4580-b812-202bf7339e9f"], "operator": null, "metadata": {"aucs": [0.24681979517717267, 0.5241898829855763, 0.5171861984993926, 0.9259358383292499, 0.5415723539120811, 0.5907462007278541, 0.41179462238899767, 0.44527510199509857, 0.5485075253904959, 0.44801474472238045, 0.9170866821576362, 0.9914724644865568, 0.34175858620972344, 0.5198588345690366, 0.9138804159382264, 0.6105121623705981, 0.45740146158261163, 0.7138654675216709, 0.2433007010522572, 0.5284032186279695]}}
{"id": "0cbd1d26-c840-4a67-a83c-9634ba3bbc77", "fitness": 0.7970693270508339, "name": "SHADE", "description": "An adaptive differential evolution that uses a success history of mutation factors and crossover rates to guide future parameter choices.", "code": "import numpy as np\n\nclass SHADE:\n    def __init__(self, budget=10000, dim=10, memory_size=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = int(4 + 3 * np.log(budget))\n        self.memory_size = memory_size\n        self.memory_F = np.full(memory_size, 0.5)\n        self.memory_CR = np.full(memory_size, 0.5)\n        self.p = 0.1  # Percentage of top individuals for p-best selection\n        self.population = None\n        self.fitness = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.archive = [] # For storing inferior solutions\n        self.archive_size = self.pop_size  # Archive size, can be adjusted\n\n        self.memory_index = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n\n    def mutate(self, pop, i, F, Cr):\n        p_best_size = max(int(self.p * len(pop)), 1)\n        \n        # Tournament selection for p-best\n        indices = np.argsort(self.fitness)[:p_best_size]\n        p_best_idx = np.random.choice(indices)\n        p_best = pop[p_best_idx]\n\n        indices = [idx for idx in range(len(pop)) if idx != i]\n        if len(indices) < 2:\n            return pop[i]  # If only one element, no mutation\n\n        r1, r2 = np.random.choice(indices, 2, replace=False)\n\n        mutant = p_best + F * (pop[r1] - pop[r2])\n        return mutant\n\n\n    def crossover(self, mutant, target, Cr):\n        cross_points = np.random.rand(self.dim) < Cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n    \n    def update_archive(self, individual):\n        if len(self.archive) < self.archive_size:\n            self.archive.append(individual)\n        else:\n            idx = np.random.randint(0, self.archive_size)\n            self.archive[idx] = individual\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n        fevals = self.pop_size\n        \n        successful_F = []\n        successful_CR = []\n        \n        while fevals < self.budget:\n            for i in range(self.pop_size):\n                # Sample F and CR from memory\n                mem_idx = np.random.randint(0, self.memory_size)\n                F = self.memory_F[mem_idx]\n                Cr = self.memory_CR[mem_idx]\n\n                mutant = self.mutate(self.population, i, F, Cr)\n                mutant = np.clip(mutant, self.bounds_lb, self.bounds_ub)\n                trial = self.crossover(mutant, self.population[i], Cr)\n                \n                f = func(trial)\n                fevals += 1\n\n                if f < self.fitness[i]:\n                    successful_F.append(F)\n                    successful_CR.append(Cr)\n                    \n                    if len(self.archive) < self.archive_size:\n                        self.update_archive(self.population[i])\n                    else:\n                        self.update_archive(self.population[i])\n\n\n                    self.fitness[i] = f\n                    self.population[i] = trial\n                    \n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                else:\n                     self.update_archive(trial)\n\n                if fevals >= self.budget:\n                    break\n            \n            # Update memory\n            if successful_F:\n                self.memory_F[self.memory_index] = np.mean(successful_F)\n                self.memory_CR[self.memory_index] = np.mean(successful_CR)\n                self.memory_index = (self.memory_index + 1) % self.memory_size\n            \n            successful_F = []\n            successful_CR = []\n            \n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm SHADE scored 0.797 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["362496cf-1ef2-41ac-950f-11944911fba6"], "operator": null, "metadata": {"aucs": [0.3358671525649425, 0.8059451112198965, 0.8536873475750335, 0.9228114590598238, 0.8915023839526408, 0.9159132330197748, 0.8600462300512554, 0.8465224953145012, 0.8801292344950892, 0.885328332325349, 0.9226382598209059, 0.9975141095704447, 0.34980353092192906, 0.8569596540570548, 0.7386108712892201, 0.9170211125007397, 0.7956847630131327, 0.9168559627004533, 0.7252988184681611, 0.5232464790963284]}}
{"id": "c2d4dfa7-9602-4280-bcd0-d50488d6bc0e", "fitness": 0.42576380406925524, "name": "AdaptiveDE_Diversity_Mirrored", "description": "Adaptive Differential Evolution with diversity-based F, adaptive Cr, and a mirrored boundary handling strategy.", "code": "import numpy as np\n\nclass AdaptiveDE_Diversity_Mirrored:\n    def __init__(self, budget=10000, dim=10, F_initial=0.5, Cr_initial=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.F = F_initial  # Initial mutation factor\n        self.Cr = Cr_initial  # Initial crossover rate\n        self.pop_size = int(4 + 3 * np.log(budget))  # Population size that scales with budget\n        self.population = None\n        self.fitness = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.F_min = 0.1\n        self.F_max = 1.0\n        self.Cr_min = 0.1\n        self.Cr_max = 0.9\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n\n    def calculate_diversity(self):\n        # Calculate the average distance of each individual from the population mean\n        mean_position = np.mean(self.population, axis=0)\n        distances = np.linalg.norm(self.population - mean_position, axis=1)\n        diversity = np.mean(distances)\n        return diversity\n\n    def adjust_mutation_factor(self):\n        # Adjust mutation factor based on population diversity\n        diversity = self.calculate_diversity()\n        # Normalize diversity to [0, 1]\n        max_possible_distance = np.linalg.norm(self.bounds_ub - self.bounds_lb) * np.sqrt(self.dim)\n        normalized_diversity = diversity / max_possible_distance if max_possible_distance > 0 else 0\n\n        # Adapt F: If diversity is low, increase F to explore more.  If diversity is high, decrease F to exploit.\n        self.F = self.F_min + (self.F_max - self.F_min) * (1 - normalized_diversity)\n        self.F = np.clip(self.F, self.F_min, self.F_max)\n\n        # Adapt Cr similarly\n        self.Cr = self.Cr_min + (self.Cr_max - self.Cr_min) * normalized_diversity\n        self.Cr = np.clip(self.Cr, self.Cr_min, self.Cr_max)\n\n\n    def mutate(self, pop, i):\n        indices = [idx for idx in range(len(pop)) if idx != i]\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutation = pop[a] + self.F * (pop[b] - pop[c])\n        return mutation\n\n    def mirrored_boundary_handling(self, mutant):\n        # Apply mirrored boundary handling\n        for i in range(self.dim):\n            if mutant[i] < self.bounds_lb[i]:\n                mutant[i] = self.bounds_lb[i] + (self.bounds_lb[i] - mutant[i])\n            elif mutant[i] > self.bounds_ub[i]:\n                mutant[i] = self.bounds_ub[i] - (mutant[i] - self.bounds_ub[i])\n        return mutant\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.Cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n        \n        fevals = self.pop_size # Initial population evaluation\n\n        while fevals < self.budget:\n            self.adjust_mutation_factor() # Adjust F based on diversity\n\n            for i in range(self.pop_size):\n                mutant = self.mutate(self.population, i)\n                mutant = self.mirrored_boundary_handling(mutant)  # Mirrored boundary handling instead of clipping\n                trial = self.crossover(mutant, self.population[i])\n                \n                f = func(trial)\n                fevals += 1\n\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.population[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                \n                if fevals >= self.budget:\n                    break\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDE_Diversity_Mirrored scored 0.426 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["362496cf-1ef2-41ac-950f-11944911fba6"], "operator": null, "metadata": {"aucs": [0.1599459143314501, 0.19783769809600782, 0.4140188141719148, 0.5298193600035481, 0.3481810671386636, 0.6194859395885685, 0.32024406427940155, 0.40176787446556605, 0.336274988329172, 0.20111884016778603, 0.37631858609317337, 0.9926258714494435, 0.2751525254963101, 0.3146217177429439, 0.7507308999553872, 0.6620481199681955, 0.3310206318136496, 0.5637974373516853, 0.20090740241922156, 0.519358328523015]}}
{"id": "640916d8-8f1c-4e12-8ced-68afcc861fd9", "fitness": 0.3997252848699899, "name": "AdaptiveClampingPSO", "description": "Particle swarm optimization with adaptive velocity clamping and a combined exploration/exploitation strategy based on population diversity and stagnation detection.", "code": "import numpy as np\n\nclass AdaptiveClampingPSO:\n    def __init__(self, budget=10000, dim=10, pop_size=20, inertia=0.7, cognitive_coeff=1.5, social_coeff=1.5, stagnation_threshold=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.inertia = inertia\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.stagnation_threshold = stagnation_threshold\n        self.velocities = None\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.inf\n\n    def __call__(self, func):\n        # Initialize population and velocities within the bounds\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.random.uniform(-0.1, 0.1, size=(self.pop_size, self.dim)) #Initialize velocities\n\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n        personal_best_positions = population.copy() # Initialize personal best positions\n        personal_best_fitness = fitness.copy()\n        \n        # Calculate initial diversity\n        initial_diversity = np.mean([np.linalg.norm(population[i] - population[j]) for i in range(self.pop_size) for j in range(i+1, self.pop_size)])\n\n        while self.budget > 0:\n            # Calculate diversity\n            diversity = np.mean([np.linalg.norm(population[i] - population[j]) for i in range(self.pop_size) for j in range(i+1, self.pop_size)])\n            \n            for i in range(self.pop_size):\n                # Update velocity\n                r1, r2 = np.random.rand(2)\n                new_velocity = (self.inertia * self.velocities[i] +\n                                      self.cognitive_coeff * r1 * (personal_best_positions[i] - population[i]) +\n                                      self.social_coeff * r2 * (self.x_opt - population[i]))\n                \n                # Adaptive Velocity Clamping\n                v_max = 0.1 + 0.1 * (diversity / initial_diversity)  # Adjust v_max based on diversity\n                new_velocity = np.clip(new_velocity, -v_max, v_max)  # Clamp velocity\n\n                self.velocities[i] = new_velocity\n\n                # Update position\n                new_position = population[i] + self.velocities[i]\n                new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n\n                new_fitness = func(new_position)\n                self.budget -= 1\n\n                if self.budget <= 0:\n                    break\n                \n\n                # Update personal best\n                if new_fitness < personal_best_fitness[i]:\n                    personal_best_fitness[i] = new_fitness\n                    personal_best_positions[i] = new_position.copy()\n\n                #Update global best\n                if new_fitness < self.f_opt:\n                    self.f_opt = new_fitness\n                    self.x_opt = new_position.copy()\n            \n                population[i] = new_position\n            if self.budget <= 0:\n                break\n\n            # Stagnation Detection and Exploration\n            if self.f_opt >= self.previous_best_fitness:\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n            \n            self.previous_best_fitness = self.f_opt\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                # Trigger exploration: reset velocities and perturb best particle\n                self.velocities = np.random.uniform(-0.1, 0.1, size=(self.pop_size, self.dim))\n                self.x_opt = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim) # Perturb global best\n                self.stagnation_counter = 0\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveClampingPSO scored 0.400 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["dbc28305-35a2-4058-b975-d7fabea855ee"], "operator": null, "metadata": {"aucs": [0.11036150337040196, 0.20391613559149713, 0.5960491014673742, 0.9213642530411216, 0.17536289423376694, 0.19646759324283536, 0.23601423012274136, 0.23386007332107805, 0.20065490355583404, 0.17789959113975762, 0.27542102514519373, 0.9890276616351585, 0.2829625190661995, 0.18240195246184043, 0.7247632928185823, 0.5752308271114699, 0.40953322918160706, 0.8842898389906065, 0.17148896173609685, 0.44743611016663587]}}
{"id": "c3d00893-968d-41ea-90a3-02ff392adfbb", "fitness": -Infinity, "name": "OrthogonalLocalSearch", "description": "A population-based algorithm that uses orthogonal learning to efficiently explore the search space and combines it with a local search strategy based on the best solution found so far.", "code": "import numpy as np\n\nclass OrthogonalLocalSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=20, orthogonal_components=5, local_search_radius=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.orthogonal_components = orthogonal_components\n        self.local_search_radius = local_search_radius\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n    def generate_orthogonal_array(self, n, k, levels):\n        \"\"\"Generates an orthogonal array using Plackett-Burman design.\"\"\"\n        H = np.ones((n, n))\n        H[1:, 0] = -1\n        for i in range(1, n):\n            for j in range(1, n):\n                H[i, j] = H[1, (j * i) % n]\n        \n        array = np.zeros((n - 1, k), dtype=int)\n        for col in range(k):\n            array[:, col] = (H[1:n, col] + 1) // 2 * (levels - 1)\n        return array\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n\n        # Evaluate initial population\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Find initial best solution\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        while self.budget > 0:\n            # Orthogonal learning phase\n            if self.dim > 1: #Avoid errors when dimension is smaller than orthogonal_components\n                num_components = min(self.orthogonal_components, self.dim)\n                # Generate orthogonal array\n                oa = self.generate_orthogonal_array(num_components + 1, num_components, 2) # Levels are 0 or 1\n\n                # Select individuals for orthogonal design\n                indices = np.random.choice(self.pop_size, num_components, replace=False)\n                selected_individuals = population[indices]\n\n                # Create new individuals based on orthogonal array\n                for row in range(oa.shape[0]):\n                    new_individual = self.x_opt.copy()\n                    for col in range(num_components):\n                        # Perturb dimensions based on orthogonal array values\n                        dimension_index = col # Ensure dimension_index is within the bounds\n                        if oa[row, col] == 0:\n                            new_individual[dimension_index] = selected_individuals[col, dimension_index] # Use selected individual dimension value\n                        else:\n                            new_individual[dimension_index] = self.x_opt[dimension_index] + np.random.uniform(-self.local_search_radius, self.local_search_radius) # Random perturbation\n                        \n                        new_individual[dimension_index] = np.clip(new_individual[dimension_index], func.bounds.lb, func.bounds.ub) # Clip to bounds\n                        \n\n                    # Evaluate the new individual\n                    f = func(new_individual)\n                    self.budget -= 1\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = new_individual.copy()\n\n                    # Replace the worst individual in the population with the new one (if better)\n                    worst_index = np.argmax(fitness)\n                    if f < fitness[worst_index]:\n                        population[worst_index] = new_individual.copy()\n                        fitness[worst_index] = f\n\n            # Local search around the best solution\n            for i in range(self.pop_size):\n                new_individual = self.x_opt + np.random.normal(0, self.local_search_radius, size=self.dim)\n                new_individual = np.clip(new_individual, func.bounds.lb, func.bounds.ub)\n\n                f = func(new_individual)\n                self.budget -= 1\n\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = new_individual.copy()\n                    \n                    #Replace worst in population\n                    worst_index = np.argmax(fitness)\n                    if f < fitness[worst_index]:\n                        population[worst_index] = new_individual.copy()\n                        fitness[worst_index] = f\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "An exception occurred: setting an array element with a sequence..", "error": "", "parent_ids": ["2b5c9234-3f08-4430-af9a-5b0aa11c0445"], "operator": null, "metadata": {}}
{"id": "2bb9bc53-9d93-4d72-be46-aeec7e6dd417", "fitness": -Infinity, "name": "OrthogonalExplorationExploitation", "description": "An enhanced exploration-exploitation algorithm employing orthogonal learning to generate promising candidate solutions based on the population's statistical properties.", "code": "import numpy as np\n\nclass OrthogonalExplorationExploitation:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_exploration_rate=0.5, initial_exploitation_rate=0.2, orthogonal_components=5, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.exploration_rate = initial_exploration_rate\n        self.exploitation_rate = initial_exploitation_rate\n        self.orthogonal_components = orthogonal_components\n        self.archive = []\n        self.archive_size = archive_size\n        self.success_history = []\n\n    def __call__(self, func):\n        # Initialize population within the bounds\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        while self.budget > 0:\n            new_population = np.zeros_like(population)\n            new_fitness = np.zeros_like(fitness)\n\n            for i in range(self.pop_size):\n                if np.random.rand() < self.exploration_rate:\n                    # Orthogonal Exploration: Generate candidates based on orthogonal design\n                    basis_vectors = np.random.randn(self.orthogonal_components, self.dim)\n                    basis_vectors = self.orthogonalize(basis_vectors)\n                    \n                    candidate = population[i].copy()\n                    for j in range(self.orthogonal_components):\n                        step = np.random.uniform(-0.5, 0.5)  # Random step along orthogonal direction\n                        candidate += step * basis_vectors[j]\n                        candidate = np.clip(candidate, func.bounds.lb, func.bounds.ub)\n\n                    new_individual = candidate\n                else:\n                    # Exploitation around the best solution and/or archive\n                    if len(self.archive) > 0 and np.random.rand() < self.exploitation_rate:\n                        # Exploitation using archive\n                        archive_idx = np.random.randint(len(self.archive))\n                        new_individual = population[i] + 0.5 * (self.archive[archive_idx] - population[i]) + 0.1 * np.random.randn(self.dim)\n                        new_individual = np.clip(new_individual, func.bounds.lb, func.bounds.ub)\n                    else:\n                        # Exploitation around best\n                        new_individual = population[i] + 0.2 * np.random.randn(self.dim) + 0.1 * (self.x_opt - population[i])\n                        new_individual = np.clip(new_individual, func.bounds.lb, func.bounds.ub)\n\n                new_fitness[i] = func(new_individual)\n                self.budget -= 1\n                if new_fitness[i] < self.f_opt:\n                    self.f_opt = new_fitness[i]\n                    self.x_opt = new_individual\n\n                new_population[i] = new_individual\n\n            # Update population and archive\n            num_improvements = 0\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    population[i] = new_population[i]\n                    fitness[i] = new_fitness[i]\n                    num_improvements += 1\n\n                    # Archive the solution\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(new_population[i].copy())\n                    else:\n                        # Replace a random archive member\n                        replace_idx = np.random.randint(self.archive_size)\n                        self.archive[replace_idx] = new_population[i].copy()\n            \n            # Adaptive parameter control based on improvement\n            improvement_ratio = num_improvements / self.pop_size\n            self.success_history.append(improvement_ratio)\n            if len(self.success_history) > 10:\n                 self.success_history.pop(0)\n            avg_success = np.mean(self.success_history)\n\n            if avg_success > 0.2:\n                self.exploration_rate *= 0.95  # Reduce exploration if exploitation is working well\n                self.exploitation_rate = min(1.0, self.exploitation_rate * 1.05)\n            else:\n                self.exploration_rate = min(1.0, self.exploration_rate * 1.05)  # Increase exploration if not improving\n                self.exploitation_rate *= 0.95\n\n        return self.f_opt, self.x_opt\n\n    def orthogonalize(self, vectors):\n        # Gram-Schmidt orthogonalization\n        basis = []\n        for v in vectors:\n            w = v - np.sum([np.dot(v,b)*b for b in basis], axis=0)\n            if (w**2).sum() > 1e-10:  # Ensure vector is non-zero\n                basis.append(w/np.linalg.norm(w))\n        return np.array(basis)", "configspace": "", "generation": 3, "feedback": "An exception occurred: index 2 is out of bounds for axis 0 with size 2.", "error": "", "parent_ids": ["2b5c9234-3f08-4430-af9a-5b0aa11c0445"], "operator": null, "metadata": {}}
{"id": "54c47f06-15cd-4cdb-9b68-194d21d67ad5", "fitness": -Infinity, "name": "AdaptiveDE_SelfAdaptivePopSize_Tournament", "description": "An adaptive differential evolution with a self-adaptive population size based on stagnation detection and a tournament selection scheme.", "code": "import numpy as np\n\nclass AdaptiveDE_SelfAdaptivePopSize_Tournament:\n    def __init__(self, budget=10000, dim=10, F_initial=0.5, Cr_initial=0.9, pop_size_initial=50):\n        self.budget = budget\n        self.dim = dim\n        self.F = F_initial\n        self.Cr = Cr_initial\n        self.pop_size = pop_size_initial\n        self.population = None\n        self.fitness = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.F_min = 0.1\n        self.F_max = 1.0\n        self.Cr_min = 0.1\n        self.Cr_max = 0.9\n        self.stagnation_threshold = 10  # Number of iterations without improvement to consider stagnation\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n\n    def adjust_population_size(self):\n        # Detect stagnation\n        if len(self.best_fitness_history) > self.stagnation_threshold:\n            if np.std(self.best_fitness_history[-self.stagnation_threshold:]) < 1e-6:  #Stagnation if little fitness change\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n\n        if self.stagnation_counter > 3:  # Stagnation detected, reduce population size\n            self.pop_size = max(10, int(self.pop_size * 0.8))\n            self.stagnation_counter = 0\n            # Re-initialize population\n            self.initialize_population(func) #Use func defined later. \n        elif self.stagnation_counter > 0:\n            self.pop_size = min(100, int(self.pop_size * 1.2))\n            self.stagnation_counter = 0\n            self.initialize_population(func)\n\n    def adjust_mutation_factor(self):\n          # Adapt F and Cr based on recent success\n        if len(self.best_fitness_history) > 1:\n            improvement = self.best_fitness_history[-2] - self.best_fitness_history[-1]\n            if improvement > 0:  # If there was improvement, reduce F\n                self.F = max(self.F_min, self.F * 0.9)\n                self.Cr = min(self.Cr_max, self.Cr * 1.1)\n            else:  # If no improvement, increase F\n                self.F = min(self.F_max, self.F * 1.1)\n                self.Cr = max(self.Cr_min, self.Cr * 0.9)\n\n\n    def mutate(self, pop, i):\n        indices = [idx for idx in range(len(pop)) if idx != i]\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutation = pop[a] + self.F * (pop[b] - pop[c])\n        return mutation\n\n    def boundary_handling(self, mutant):\n        # Clip to boundaries\n        return np.clip(mutant, self.bounds_lb, self.bounds_ub)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.Cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n    \n    def tournament_selection(self, trial_fitness, current_fitness):\n        # Tournament selection: randomly select two individuals and choose the better one\n        competitor_idx = np.random.randint(0, self.pop_size)\n        if trial_fitness < current_fitness or trial_fitness < self.fitness[competitor_idx]:\n            return True  # Trial wins the tournament\n        else:\n            return False\n\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n        \n        fevals = self.pop_size\n\n        self.best_fitness_history.append(np.min(self.fitness))\n\n        while fevals < self.budget:\n            self.adjust_population_size()\n            self.adjust_mutation_factor()\n\n            for i in range(self.pop_size):\n                mutant = self.mutate(self.population, i)\n                mutant = self.boundary_handling(mutant)\n                trial = self.crossover(mutant, self.population[i])\n                \n                f = func(trial)\n                fevals += 1\n                \n                if self.tournament_selection(f, self.fitness[i]):\n                    self.fitness[i] = f\n                    self.population[i] = trial\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n\n                if fevals >= self.budget:\n                    break\n            \n            self.best_fitness_history.append(np.min(self.fitness))\n        \n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "An exception occurred: name 'func' is not defined.", "error": "", "parent_ids": ["c2d4dfa7-9602-4280-bcd0-d50488d6bc0e"], "operator": null, "metadata": {}}
{"id": "5dd2032c-f1d7-4c87-ac82-cb8301bbc7e1", "fitness": -Infinity, "name": "AdaptiveDE_Restart_Improved", "description": "Implements a more robust stagnation check using exponentially weighted moving average and an archive to store successful solutions, improving adaptation and exploration.", "code": "import numpy as np\n\nclass AdaptiveDE_Restart_Improved:\n    def __init__(self, budget=10000, dim=10, F_init=0.5, Cr_init=0.9, restart_patience=500, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.F_init = F_init  # Initial mutation factor\n        self.Cr_init = Cr_init  # Initial crossover rate\n        self.pop_size = int(4 + 3 * np.log(budget))  # Population size that scales with budget\n        self.population = None\n        self.fitness = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.F = np.full(self.pop_size, self.F_init)\n        self.Cr = np.full(self.pop_size, self.Cr_init)\n        self.restart_patience = restart_patience\n        self.no_improvement_count = 0\n        self.best_fitness = np.inf\n        self.archive_size = archive_size\n        self.archive = [] # Archive to store successful solutions\n        self.ewma_fitness = None  # Exponentially weighted moving average of fitness\n        self.ewma_alpha = 0.05  # Weighting factor for EWMA\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        if self.ewma_fitness is None:\n            self.ewma_fitness = np.mean(self.fitness) # Initialize EWMA with mean fitness\n\n    def mutate(self, pop, i):\n        indices = [idx for idx in range(len(pop)) if idx != i]\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutation = pop[a] + self.F[i] * (pop[b] - pop[c])\n\n        # Incorporate information from the archive if available\n        if self.archive:\n            arc_idx = np.random.randint(len(self.archive))\n            mutation += 0.1 * (self.archive[arc_idx] - pop[i])\n\n        mutation += 0.01 * np.random.standard_cauchy(size=self.dim) # Cauchy noise\n        return mutation\n\n    def crossover(self, mutant, target, i):\n        cross_points = np.random.rand(self.dim) < self.Cr[i]\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def adapt_mutation_factor(self):\n        for i in range(self.pop_size):\n            if np.random.rand() < 0.1:  # Probabilistically adapt F\n                self.F[i] = np.random.normal(self.F[i], 0.1) # Sample from normal distribution\n                self.F[i] = np.clip(self.F[i], 0.1, 1.0)  # Keep F within [0.1, 1.0]\n\n    def adapt_crossover_rate(self):\n        \"\"\"Adapt the crossover rate based on individual success, using a simple reward/penalty.\"\"\"\n        for i in range(self.pop_size):\n            if np.random.rand() < 0.1:  # Probabilistically adapt Cr\n                if np.random.rand() < 0.5:\n                    self.Cr[i] = min(self.Cr[i] + 0.1, 1.0)  # Increase Cr\n                else:\n                    self.Cr[i] = max(self.Cr[i] - 0.1, 0.1)  # Decrease Cr\n\n    def update_archive(self, x, fitness_value):\n        \"\"\"Update the archive with successful solutions.\"\"\"\n        if len(self.archive) < self.archive_size:\n            self.archive.append(x)\n        else:\n            # Replace the worst solution in the archive, if the new one is better\n            worst_index = np.argmax([func(x) for x in self.archive]) #find the index of the worst element\n            if fitness_value < func(self.archive[worst_index]):\n                self.archive[worst_index] = x\n                \n    def check_stagnation(self):\n        \"\"\"Check for stagnation using EWMA of fitness.\"\"\"\n        current_mean_fitness = np.mean(self.fitness)\n        self.ewma_fitness = self.ewma_alpha * current_mean_fitness + (1 - self.ewma_alpha) * self.ewma_fitness\n\n        if abs(current_mean_fitness - self.ewma_fitness) < 1e-6:  # Stagnation threshold\n            self.no_improvement_count += 1\n        else:\n            self.no_improvement_count = 0\n\n        if self.no_improvement_count > self.restart_patience:\n            return True\n        else:\n            return False\n\n    def restart_population(self, func):\n        \"\"\"Restart the population and clear the archive.\"\"\"\n        self.initialize_population(func)\n        self.no_improvement_count = 0\n        self.F = np.full(self.pop_size, self.F_init)\n        self.Cr = np.full(self.pop_size, self.Cr_init)\n        self.archive = [] # Clear the archive\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n        \n        fevals = self.pop_size # Initial population evaluation\n\n        while fevals < self.budget:\n            for i in range(self.pop_size):\n                mutant = self.mutate(self.population, i)\n                mutant = np.clip(mutant, self.bounds_lb, self.bounds_ub)  # Clip to bounds\n                trial = self.crossover(mutant, self.population[i], i)\n                \n                f = func(trial)\n                fevals += 1\n\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.population[i] = trial\n                    self.update_archive(trial, f) # Update the archive\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                        self.best_fitness = f\n                        self.no_improvement_count = 0 # Reset counter\n                \n                if fevals >= self.budget:\n                    break\n            \n            self.adapt_mutation_factor()\n            self.adapt_crossover_rate()\n            \n            if self.check_stagnation():\n                self.restart_population(func)\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "An exception occurred: name 'func' is not defined.", "error": "", "parent_ids": ["8e3cdd02-6ab1-453a-98f9-f6d98857a9d4"], "operator": null, "metadata": {}}
{"id": "50dd8e33-3958-47e2-8044-00c3bb473e7a", "fitness": 0.0, "name": "DynamicExplorationExploitation", "description": "Improved Dynamic Exploration-Exploitation with adaptive step size control based on success rate and a diversity maintenance strategy using a worst-individual replacement scheme.", "code": "import numpy as np\n\nclass DynamicExplorationExploitation:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_exploration_rate=0.5, initial_exploitation_rate=0.2, diversity_threshold=0.1, archive_size=10, step_size_scale=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.exploration_rate = initial_exploration_rate\n        self.exploitation_rate = initial_exploitation_rate\n        self.diversity_threshold = diversity_threshold\n        self.archive = []\n        self.archive_size = archive_size\n        self.step_size = step_size_scale  # Scale factor for step size\n        self.step_decay = 0.995  # Decay factor for step size\n        self.success_history = []\n        self.min_step_size = 0.01 * self.step_size # Minimum allowed step size to avoid premature convergence.\n        self.worst_replace_freq = 5 #Frequency to replace the worst member\n\n    def __call__(self, func):\n        # Initialize population within the bounds\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        generation = 0\n        while self.budget > 0:\n            new_population = np.zeros_like(population)\n            new_fitness = np.zeros_like(fitness)\n\n            for i in range(self.pop_size):\n                if np.random.rand() < self.exploration_rate:\n                    # Global Exploration: Random perturbation with decaying step size\n                    step = self.step_size * (func.bounds.ub - func.bounds.lb) #Dynamic scaling based on the search space\n                    new_individual = population[i] + np.random.normal(0, step, size=self.dim)\n                    new_individual = np.clip(new_individual, func.bounds.lb, func.bounds.ub)\n\n                else:\n                    # Local Exploitation: Perturbation around the best solution and/or archive\n                    if len(self.archive) > 0 and np.random.rand() < self.exploitation_rate:\n                        # Exploitation using archive\n                        archive_idx = np.random.randint(len(self.archive))\n                        new_individual = population[i] + 0.5 * (self.archive[archive_idx] - population[i]) + np.random.normal(0, self.step_size * 0.2 * (func.bounds.ub - func.bounds.lb), size=self.dim)\n                        new_individual = np.clip(new_individual, func.bounds.lb, func.bounds.ub)\n                    else:\n                        # Exploitation around best\n                        new_individual = population[i] + np.random.normal(0, self.step_size * 0.2 * (func.bounds.ub - func.bounds.lb), size=self.dim) + 0.1 * (self.x_opt - population[i])\n                        new_individual = np.clip(new_individual, func.bounds.lb, func.bounds.ub)\n\n                new_fitness[i] = func(new_individual)\n                self.budget -= 1\n                if new_fitness[i] < self.f_opt:\n                    self.f_opt = new_fitness[i]\n                    self.x_opt = new_individual\n\n                new_population[i] = new_individual\n\n            # Update population and archive\n            num_improvements = 0\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    population[i] = new_population[i]\n                    fitness[i] = new_fitness[i]\n                    num_improvements += 1\n\n                    # Archive the solution\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(new_population[i].copy())\n                    else:\n                        # Replace a random archive member\n                        replace_idx = np.random.randint(self.archive_size)\n                        self.archive[replace_idx] = new_population[i].copy()\n\n            #Diversity maintenance\n            generation += 1\n            if generation % self.worst_replace_freq == 0:\n                worst_index = np.argmax(fitness)\n                population[worst_index] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                fitness[worst_index] = func(population[worst_index])\n                self.budget -= 1 #Account for the new evaluation\n\n            # Adaptive parameter control based on improvement, diversity, and fitness variance\n            improvement_ratio = num_improvements / self.pop_size\n            self.success_history.append(improvement_ratio)\n            if len(self.success_history) > 10:\n                self.success_history.pop(0)\n            avg_success = np.mean(self.success_history)\n\n            # Calculate average pairwise distance (diversity metric)\n            distances = []\n            for i in range(self.pop_size):\n                for j in range(i + 1, self.pop_size):\n                    distances.append(np.linalg.norm(population[i] - population[j]))\n            avg_distance = np.mean(distances) if distances else 0.0\n\n            fitness_variance = np.var(fitness)\n\n            # Adjust exploration rate\n            if avg_success > 0.2 and avg_distance > self.diversity_threshold and fitness_variance < 0.1:\n                self.exploration_rate *= 0.95  # Reduce exploration if exploitation is working well, diversity is high, and fitness variance is low\n                self.exploitation_rate = min(1.0, self.exploitation_rate * 1.05)\n            elif avg_success <= 0.1 or avg_distance <= self.diversity_threshold:\n                self.exploration_rate = min(1.0, self.exploration_rate * 1.05)  # Increase exploration if not improving or low diversity\n                self.exploitation_rate *= 0.95\n\n            # Step size decay, but ensure it doesn't go below the minimum\n            self.step_size = max(self.min_step_size, self.step_size * self.step_decay)\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm DynamicExplorationExploitation scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2b5c9234-3f08-4430-af9a-5b0aa11c0445"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "cfe60870-5636-449f-932c-130c05572b48", "fitness": 0.0, "name": "AdaptiveDE_Archive_Diversity_Rejuvenation", "description": "Adaptive Differential Evolution with archive, diversity-based parameter adaptation, mirrored boundary handling, and periodic population rejuvenation to escape local optima.", "code": "import numpy as np\n\nclass AdaptiveDE_Archive_Diversity_Rejuvenation:\n    def __init__(self, budget=10000, dim=10, F_initial=0.5, Cr_initial=0.9, archive_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.F = F_initial  # Initial mutation factor\n        self.Cr = Cr_initial  # Initial crossover rate\n        self.pop_size = int(4 + 3 * np.log(budget))  # Population size that scales with budget\n        self.population = None\n        self.fitness = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.F_min = 0.1\n        self.F_max = 1.0\n        self.Cr_min = 0.1\n        self.Cr_max = 0.9\n        self.archive = []\n        self.archive_size = archive_size\n        self.rejuvenation_interval = int(budget / 10) # Rejuvenate every 10% of budget\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n\n    def calculate_diversity(self):\n        # Calculate the average distance of each individual from the population mean\n        mean_position = np.mean(self.population, axis=0)\n        distances = np.linalg.norm(self.population - mean_position, axis=1)\n        diversity = np.mean(distances)\n        return diversity\n\n    def adjust_mutation_factor(self):\n        # Adjust mutation factor based on population diversity\n        diversity = self.calculate_diversity()\n        # Normalize diversity to [0, 1]\n        max_possible_distance = np.linalg.norm(self.bounds_ub - self.bounds_lb) * np.sqrt(self.dim)\n        normalized_diversity = diversity / max_possible_distance if max_possible_distance > 0 else 0\n\n        # Adapt F: If diversity is low, increase F to explore more.  If diversity is high, decrease F to exploit.\n        self.F = self.F_min + (self.F_max - self.F_min) * (1 - normalized_diversity)\n        self.F = np.clip(self.F, self.F_min, self.F_max)\n\n        # Adapt Cr similarly\n        self.Cr = self.Cr_min + (self.Cr_max - self.Cr_min) * normalized_diversity\n        self.Cr = np.clip(self.Cr, self.Cr_min, self.Cr_max)\n\n    def mutate(self, pop, i):\n        indices = [idx for idx in range(len(pop)) if idx != i]\n        if len(self.archive) > 0 and np.random.rand() < 0.1:  # Use archive occasionally\n            donor_pool = np.concatenate((pop[indices], self.archive))\n            a, b = np.random.choice(len(donor_pool), 2, replace=False)\n            if a < len(indices):\n                a = indices[a]\n                donor_a = pop[a]\n            else:\n                donor_a = donor_pool[a]\n\n            if b < len(indices):\n                b = indices[b]\n                donor_b = pop[b]\n            else:\n                donor_b = donor_pool[b]\n            mutation = pop[i] + self.F * (donor_a - donor_b)\n        else:\n             a, b, c = np.random.choice(indices, 3, replace=False)\n             mutation = pop[a] + self.F * (pop[b] - pop[c])\n        return mutation\n\n    def mirrored_boundary_handling(self, mutant):\n        # Apply mirrored boundary handling\n        for i in range(self.dim):\n            if mutant[i] < self.bounds_lb[i]:\n                mutant[i] = self.bounds_lb[i] + (self.bounds_lb[i] - mutant[i])\n            elif mutant[i] > self.bounds_ub[i]:\n                mutant[i] = self.bounds_ub[i] - (mutant[i] - self.bounds_ub[i])\n        return mutant\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.Cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n    \n    def update_archive(self, individual):\n        if len(self.archive) < self.archive_size:\n            self.archive.append(individual)\n        else:\n            # Replace a random element in the archive\n            index_to_replace = np.random.randint(0, self.archive_size)\n            self.archive[index_to_replace] = individual\n\n    def rejuvenate_population(self, func):\n        # Replace the worst 20% of the population with new random individuals\n        num_to_replace = int(0.2 * self.pop_size)\n        worst_indices = np.argsort(self.fitness)[-num_to_replace:]\n        for i in worst_indices:\n            self.population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub)\n            self.fitness[i] = func(self.population[i])\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n        \n        fevals = self.pop_size # Initial population evaluation\n        rejuvenation_counter = 0\n\n        while fevals < self.budget:\n            self.adjust_mutation_factor() # Adjust F based on diversity\n            \n            if rejuvenation_counter > self.rejuvenation_interval:\n                self.rejuvenate_population(func)\n                rejuvenation_counter = 0\n\n            for i in range(self.pop_size):\n                mutant = self.mutate(self.population, i)\n                mutant = self.mirrored_boundary_handling(mutant)  # Mirrored boundary handling instead of clipping\n                trial = self.crossover(mutant, self.population[i])\n                \n                f = func(trial)\n                fevals += 1\n\n                if f < self.fitness[i]:\n                    self.update_archive(self.population[i].copy())\n                    self.fitness[i] = f\n                    self.population[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                \n                if fevals >= self.budget:\n                    break\n            rejuvenation_counter += self.pop_size\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDE_Archive_Diversity_Rejuvenation scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c2d4dfa7-9602-4280-bcd0-d50488d6bc0e"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "6642cdb0-5a27-4724-bb3a-c2979ad8181d", "fitness": 0.0, "name": "AdaptiveDE_Orthogonal", "description": "Adaptive Differential Evolution with orthogonal learning to efficiently explore the search space using orthogonal array-based experimental design.", "code": "import numpy as np\n\nclass AdaptiveDE_Orthogonal:\n    def __init__(self, budget=10000, dim=10, F_initial=0.5, Cr_initial=0.9, pop_size=None):\n        self.budget = budget\n        self.dim = dim\n        self.F = F_initial\n        self.Cr = Cr_initial\n        self.pop_size = int(4 + 3 * np.log(budget)) if pop_size is None else pop_size\n        self.population = None\n        self.fitness = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.F_min = 0.1\n        self.F_max = 1.0\n        self.Cr_min = 0.1\n        self.Cr_max = 0.9\n        self.orthogonal_matrix = None  # Orthogonal array\n        self.oa_design_points = 10  # Number of design points to sample with OA\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n\n    def adjust_mutation_factor(self):\n        # Simplified F adaptation\n        self.F = np.clip(self.F + np.random.normal(0, 0.1), self.F_min, self.F_max)\n        self.Cr = np.clip(self.Cr + np.random.normal(0, 0.1), self.Cr_min, self.Cr_max)\n\n    def mutate(self, pop, i):\n        indices = [idx for idx in range(len(pop)) if idx != i]\n        a, b, c = np.random.choice(indices, 3, replace=False)\n        mutant = pop[a] + self.F * (pop[b] - pop[c])\n        return mutant\n\n    def boundary_handling(self, mutant):\n        # Clipping boundary handling\n        return np.clip(mutant, self.bounds_lb, self.bounds_ub)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.Cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n    \n    def generate_orthogonal_array(self, n_factors, n_levels):\n        \"\"\"Generates an orthogonal array.  Simplification: L18.\"\"\"\n        if n_factors <= 2:\n            return np.array([[0,0],[0,1],[0,2],[1,0],[1,1],[1,2]]) #Minimal example\n        \n        oa = np.array([\n            [0, 0, 0, 0, 0, 0, 0],\n            [0, 1, 1, 1, 1, 1, 1],\n            [0, 2, 2, 2, 2, 2, 2],\n            [1, 0, 1, 2, 0, 2, 1],\n            [1, 1, 2, 0, 2, 1, 0],\n            [1, 2, 0, 1, 1, 0, 2],\n            [2, 0, 2, 1, 1, 0, 2],\n            [2, 1, 0, 2, 0, 2, 1],\n            [2, 2, 1, 0, 2, 1, 0]\n        ])\n        return oa[:, :n_factors]  # Limit to necessary factors\n\n    def orthogonal_learning(self):\n        # Select a random individual as the center\n        center_index = np.random.randint(0, self.pop_size)\n        center = self.population[center_index]\n\n        # Generate orthogonal array\n        if self.orthogonal_matrix is None:\n             self.orthogonal_matrix = self.generate_orthogonal_array(self.dim, 3) #limited support, max dim 7.\n        \n        if self.orthogonal_matrix.shape[1] < self.dim: #Fallback if the orthogonal array becomes too large.\n            self.orthogonal_matrix = np.random.randint(0,3, size = (9,self.dim))\n            \n        oa_design_points = self.orthogonal_matrix.shape[0]\n        \n        # Sample points based on orthogonal array\n        sampled_points = np.zeros((oa_design_points, self.dim))\n        for i in range(oa_design_points):\n            for j in range(self.dim):\n                level = self.orthogonal_matrix[i, j % self.orthogonal_matrix.shape[1]]  # Use modulo for larger dims\n                sampled_points[i, j] = center[j] + (level - 1) * 0.5  # level-1 to get -0.5, 0, 0.5 perturbations\n                sampled_points[i,j] = np.clip(sampled_points[i,j], self.bounds_lb[j], self.bounds_ub[j])\n                \n\n        return sampled_points\n        \n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n        \n        fevals = self.pop_size\n\n        while fevals < self.budget:\n            self.adjust_mutation_factor()\n\n            for i in range(self.pop_size):\n                mutant = self.mutate(self.population, i)\n                mutant = self.boundary_handling(mutant)\n                trial = self.crossover(mutant, self.population[i])\n                \n                f = func(trial)\n                fevals += 1\n\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.population[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n\n                if fevals >= self.budget:\n                    break\n            \n            # Orthogonal learning phase\n            oa_points = self.orthogonal_learning()\n            oa_fitness = np.array([func(x) for x in oa_points])\n            fevals += len(oa_points)\n\n            # Update population with best OA point\n            best_oa_index = np.argmin(oa_fitness)\n            best_oa_fitness = oa_fitness[best_oa_index]\n\n            worst_pop_index = np.argmax(self.fitness)\n            if best_oa_fitness < self.fitness[worst_pop_index]:\n                self.fitness[worst_pop_index] = best_oa_fitness\n                self.population[worst_pop_index] = oa_points[best_oa_index]\n\n                if best_oa_fitness < self.f_opt:\n                    self.f_opt = best_oa_fitness\n                    self.x_opt = oa_points[best_oa_index]\n            \n            if fevals >= self.budget:\n                break\n        \n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDE_Orthogonal scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c2d4dfa7-9602-4280-bcd0-d50488d6bc0e"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "a7a98c22-f00c-40c5-896b-b47ae99f557c", "fitness": 0.42450763484140247, "name": "AdaptiveDE_Archive_Mirrored", "description": "Adaptive Differential Evolution with Archive for stagnation resistance, diversity-based F/Cr adaptation, and mirrored boundary handling.", "code": "import numpy as np\n\nclass AdaptiveDE_Archive_Mirrored:\n    def __init__(self, budget=10000, dim=10, F_initial=0.5, Cr_initial=0.9, archive_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.F = F_initial  # Initial mutation factor\n        self.Cr = Cr_initial  # Initial crossover rate\n        self.pop_size = int(4 + 3 * np.log(budget))  # Population size that scales with budget\n        self.population = None\n        self.fitness = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.F_min = 0.1\n        self.F_max = 1.0\n        self.Cr_min = 0.1\n        self.Cr_max = 0.9\n        self.archive = []  # Archive for storing removed individuals\n        self.archive_size = archive_size\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n\n    def calculate_diversity(self):\n        # Calculate the average distance of each individual from the population mean\n        mean_position = np.mean(self.population, axis=0)\n        distances = np.linalg.norm(self.population - mean_position, axis=1)\n        diversity = np.mean(distances)\n        return diversity\n\n    def adjust_mutation_factor(self):\n        # Adjust mutation factor based on population diversity\n        diversity = self.calculate_diversity()\n        # Normalize diversity to [0, 1]\n        max_possible_distance = np.linalg.norm(self.bounds_ub - self.bounds_lb) * np.sqrt(self.dim)\n        normalized_diversity = diversity / max_possible_distance if max_possible_distance > 0 else 0\n\n        # Adapt F: If diversity is low, increase F to explore more.  If diversity is high, decrease F to exploit.\n        self.F = self.F_min + (self.F_max - self.F_min) * (1 - normalized_diversity)\n        self.F = np.clip(self.F, self.F_min, self.F_max)\n\n        # Adapt Cr similarly\n        self.Cr = self.Cr_min + (self.Cr_max - self.Cr_min) * normalized_diversity\n        self.Cr = np.clip(self.Cr, self.Cr_min, self.Cr_max)\n\n\n    def mutate(self, pop, i):\n        indices = [idx for idx in range(len(pop)) if idx != i]\n        if len(self.archive) > 0:\n            # Use archive individuals with a small probability\n            if np.random.rand() < 0.1:\n                arch_ind = np.random.randint(0, len(self.archive))\n                pop_indices = np.random.choice(indices, 2, replace=False)\n                a = self.archive[arch_ind]\n                b, c = pop[pop_indices[0]], pop[pop_indices[1]]\n            else:\n                a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n        else:\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n        \n        mutation = a + self.F * (b - c)\n        return mutation\n\n    def mirrored_boundary_handling(self, mutant):\n        # Apply mirrored boundary handling\n        for i in range(self.dim):\n            if mutant[i] < self.bounds_lb[i]:\n                mutant[i] = self.bounds_lb[i] + (self.bounds_lb[i] - mutant[i])\n            elif mutant[i] > self.bounds_ub[i]:\n                mutant[i] = self.bounds_ub[i] - (mutant[i] - self.bounds_ub[i])\n        return mutant\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.Cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n        \n        fevals = self.pop_size # Initial population evaluation\n\n        while fevals < self.budget:\n            self.adjust_mutation_factor() # Adjust F based on diversity\n\n            for i in range(self.pop_size):\n                mutant = self.mutate(self.population, i)\n                mutant = self.mirrored_boundary_handling(mutant)  # Mirrored boundary handling instead of clipping\n                trial = self.crossover(mutant, self.population[i])\n                \n                f = func(trial)\n                fevals += 1\n\n                if f < self.fitness[i]:\n                    # Archive the replaced individual\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        # Replace a random element in the archive\n                        replace_index = np.random.randint(0, self.archive_size)\n                        self.archive[replace_index] = self.population[i].copy()\n\n                    self.fitness[i] = f\n                    self.population[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                \n                if fevals >= self.budget:\n                    break\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDE_Archive_Mirrored scored 0.425 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c2d4dfa7-9602-4280-bcd0-d50488d6bc0e"], "operator": null, "metadata": {"aucs": [0.15085575156642206, 0.22042613560189028, 0.3771518231832782, 0.4765782412424081, 0.3216529860008712, 0.6270444690411794, 0.32343655439025665, 0.39981216697519895, 0.3450422075952674, 0.20374042713850482, 0.3516039741990745, 0.9850468025683567, 0.26687173029350375, 0.34035806369371835, 0.7810174650807621, 0.6771545066693816, 0.36248433154041837, 0.5811586611488981, 0.1918975221241569, 0.5068188767745014]}}
{"id": "89d0b88e-b933-48a0-be37-77f0ad40f309", "fitness": 0.6009316987701148, "name": "ImprovedSHADE", "description": "Improved SHADE with weighted Lehmer mean for parameter adaptation, jitter in mutation, and adaptive archive management.", "code": "import numpy as np\n\nclass ImprovedSHADE:\n    def __init__(self, budget=10000, dim=10, memory_size=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = int(4 + 3 * np.log(budget))\n        self.memory_size = memory_size\n        self.memory_F = np.full(memory_size, 0.5)\n        self.memory_CR = np.full(memory_size, 0.5)\n        self.p = 0.1  # Percentage of top individuals for p-best selection\n        self.population = None\n        self.fitness = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.archive = [] # For storing inferior solutions\n        self.archive_size = self.pop_size  # Archive size, can be adjusted\n\n        self.memory_index = 0\n        self.sf_M = []\n        self.scr_M = []\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n\n    def mutate(self, pop, i, F, Cr):\n        p_best_size = max(int(self.p * len(pop)), 1)\n        \n        # Tournament selection for p-best\n        indices = np.argsort(self.fitness)[:p_best_size]\n        p_best_idx = np.random.choice(indices)\n        p_best = pop[p_best_idx]\n\n        indices = [idx for idx in range(len(pop)) if idx != i]\n        if len(indices) < 2:\n            return pop[i]  # If only one element, no mutation\n\n        r1, r2 = np.random.choice(indices, 2, replace=False)\n\n        # Jitter to prevent premature convergence\n        jitter = np.random.uniform(-0.05, 0.05, size=self.dim)\n        mutant = p_best + F * (pop[r1] - pop[r2] + jitter)\n\n        return mutant\n\n\n    def crossover(self, mutant, target, Cr):\n        cross_points = np.random.rand(self.dim) < Cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n    \n    def update_archive(self, individual, success):\n        if len(self.archive) < self.archive_size:\n            self.archive.append(individual)\n        else:\n            # Adaptive archive update: replace only if the new individual is worse than a random archive member\n            idx = np.random.randint(0, len(self.archive))\n            if success:  # Only replace archive elements if trial vector was successful\n                self.archive[idx] = individual\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n        fevals = self.pop_size\n        \n        self.sf_M = []\n        self.scr_M = []\n        weights = []\n        \n        while fevals < self.budget:\n            sf_k = []\n            scr_k = []\n            weights = []\n            for i in range(self.pop_size):\n                # Sample F and CR from memory\n                mem_idx = np.random.randint(0, self.memory_size)\n                F = self.memory_F[mem_idx]\n                Cr = self.memory_CR[mem_idx]\n\n                # Truncate and Reflect F\n                if F > 1:\n                    F = 1\n                if F <= 0:\n                    F = 0.000001\n                    \n\n                mutant = self.mutate(self.population, i, F, Cr)\n                mutant = np.clip(mutant, self.bounds_lb, self.bounds_ub)\n                trial = self.crossover(mutant, self.population[i], Cr)\n                \n                f = func(trial)\n                fevals += 1\n\n                if f < self.fitness[i]:\n                    delta = abs(f - self.fitness[i])\n                    sf_k.append(F)\n                    scr_k.append(Cr)\n                    weights.append(delta)\n                    \n                    self.update_archive(self.population[i], True)\n\n                    self.fitness[i] = f\n                    self.population[i] = trial\n                    \n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                else:\n                     self.update_archive(trial, False) # trial wasn't successful\n\n                if fevals >= self.budget:\n                    break\n            \n            # Update memory using Lehmer mean\n            if sf_k:\n                weights = np.array(weights) / np.sum(weights)\n                sum_weights_sfk = np.sum(weights * np.array(sf_k)**2)\n                sum_weights = np.sum(weights * np.array(sf_k))\n                \n                if sum_weights > 0:\n                    mean_F = sum_weights_sfk / sum_weights\n                else:\n                    mean_F = 0.5\n                \n                mean_CR = np.mean(scr_k)\n                \n                self.memory_F[self.memory_index] = mean_F\n                self.memory_CR[self.memory_index] = mean_CR\n                self.memory_index = (self.memory_index + 1) % self.memory_size\n            \n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm ImprovedSHADE scored 0.601 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0cbd1d26-c840-4a67-a83c-9634ba3bbc77"], "operator": null, "metadata": {"aucs": [0.23488908766553318, 0.5742550215305844, 0.5358217514875574, 0.9338930924600151, 0.5584578606864443, 0.6240474403472369, 0.46143231487268876, 0.4644170168282816, 0.5705621622595136, 0.5132234114947757, 0.9096668412362816, 0.99948651626628, 0.29148956336288057, 0.5573945718454305, 0.9437367540620478, 0.6561486955593975, 0.48275726048585454, 0.7657419272245056, 0.42926067076426466, 0.5119520149627212]}}
{"id": "01f10593-c403-4072-83b5-34d55de1a4dc", "fitness": 0.0, "name": "AdaptiveDE_Restart_Mirrored_Orthogonal", "description": "An adaptive differential evolution algorithm that incorporates a restart mechanism when stagnation is detected based on fitness variance and adapts the crossover rate based on individual success, additionally using a mirrored boundary handling strategy and orthogonal learning.", "code": "import numpy as np\n\nclass AdaptiveDE_Restart_Mirrored_Orthogonal:\n    def __init__(self, budget=10000, dim=10, pop_size=None, restart_patience=500, memory_size=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else int(4 + 3 * np.log(budget))\n        self.restart_patience = restart_patience\n        self.memory_size = memory_size\n        self.memory_CR = np.full(memory_size, 0.5)\n        self.memory_index = 0\n        self.population = None\n        self.fitness = None\n        self.best_fitness = np.inf\n        self.best_individual = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.evals = 0\n        self.last_improvement = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals += self.pop_size\n\n        best_idx = np.argmin(self.fitness)\n        if self.fitness[best_idx] < self.best_fitness:\n            self.best_fitness = self.fitness[best_idx]\n            self.best_individual = self.population[best_idx]\n            self.last_improvement = self.evals\n\n    def mutate(self, pop, i, F):\n        indices = [idx for idx in range(len(pop)) if idx != i]\n        if len(indices) < 2:\n            return pop[i]  # If only one element, no mutation\n        r1, r2 = np.random.choice(indices, 2, replace=False)\n        return pop[i] + F * (pop[r1] - pop[r2])\n\n    def crossover(self, mutant, target, Cr):\n        cross_points = np.random.rand(self.dim) < Cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def handle_bounds(self, x):\n        # Mirrored boundary handling\n        for i in range(self.dim):\n            if x[i] < self.bounds_lb[i]:\n                x[i] = self.bounds_lb[i] + (self.bounds_lb[i] - x[i])\n                if x[i] > self.bounds_ub[i]:\n                     x[i] = self.bounds_lb[i]\n            elif x[i] > self.bounds_ub[i]:\n                x[i] = self.bounds_ub[i] - (x[i] - self.bounds_ub[i])\n                if x[i] < self.bounds_lb[i]:\n                    x[i] = self.bounds_ub[i]\n\n        return x\n\n    def orthogonal_learning(self, func, individual, num_samples=5):\n        # Generate orthogonal array (simplified, for demonstration)\n        OA = np.array([[-1, -1], [-1, 1], [1, -1], [1, 1]])  # L4 orthogonal array\n        best_f = np.inf\n        best_x = individual\n\n        for i in range(num_samples):  # Reduce samples for faster runtime\n            # Randomly select two dimensions\n            d1, d2 = np.random.choice(self.dim, 2, replace=False)\n            \n            for row in OA:\n                x = individual.copy()\n                delta1 = 0.05 * (self.bounds_ub[d1] - self.bounds_lb[d1]) * row[0]\n                delta2 = 0.05 * (self.bounds_ub[d2] - self.bounds_lb[d2]) * row[1]\n                \n                x[d1] += delta1\n                x[d2] += delta2\n                \n                x = self.handle_bounds(x)  # Ensure within bounds\n\n                f = func(x)\n                self.evals += 1\n                \n                if f < best_f:\n                    best_f = f\n                    best_x = x\n\n        return best_x, best_f\n        \n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.best_fitness = np.inf\n        self.best_individual = None\n        self.evals = 0\n        self.last_improvement = 0\n        self.memory_CR = np.full(self.memory_size, 0.5)\n        self.memory_index = 0\n\n        self.initialize_population(func)\n\n        while self.evals < self.budget:\n            old_fitness = np.copy(self.fitness)\n            successful_CR = []\n            \n            for i in range(self.pop_size):\n                # Parameter adaptation\n                mem_idx = np.random.randint(0, self.memory_size)\n                Cr = self.memory_CR[mem_idx]\n                F = np.random.normal(0.5, 0.3)\n                F = np.clip(F, 0.0, 1.0)\n\n                # Mutation and Crossover\n                mutant = self.mutate(self.population, i, F)\n                mutant = self.handle_bounds(mutant)  # Mirrored boundary handling\n                trial = self.crossover(mutant, self.population[i], Cr)\n                trial = self.handle_bounds(trial)\n\n                # Evaluation\n                f = func(trial)\n                self.evals += 1\n\n                # Selection\n                if f < self.fitness[i]:\n                    successful_CR.append(Cr)\n                    self.fitness[i] = f\n                    self.population[i] = trial\n\n                    if f < self.best_fitness:\n                        self.best_fitness = f\n                        self.best_individual = trial\n                        self.last_improvement = self.evals\n\n                # Orthogonal learning on best individual\n                if np.random.rand() < 0.1 and self.evals - self.last_improvement < self.restart_patience / 2:  # Apply orthogonal learning only if not stagnant and with small probability\n                    best_x_ol, best_f_ol = self.orthogonal_learning(func, self.population[i])\n                    if best_f_ol < self.fitness[i]:\n                        self.fitness[i] = best_f_ol\n                        self.population[i] = best_x_ol\n                        if best_f_ol < self.best_fitness:\n                            self.best_fitness = best_f_ol\n                            self.best_individual = best_x_ol\n                            self.last_improvement = self.evals\n\n                if self.evals >= self.budget:\n                    break\n\n            # Memory update\n            if successful_CR:\n                self.memory_CR[self.memory_index] = np.mean(successful_CR)\n                self.memory_index = (self.memory_index + 1) % self.memory_size\n\n            # Restart mechanism\n            if self.evals - self.last_improvement > self.restart_patience:\n                self.initialize_population(func)\n                self.last_improvement = self.evals #Reset last improvement\n\n        return self.best_fitness, self.best_individual", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDE_Restart_Mirrored_Orthogonal scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0cbd1d26-c840-4a67-a83c-9634ba3bbc77"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "df153811-89fa-41a6-aacf-decb5834d88c", "fitness": 0.5683033567125381, "name": "SOM_DE", "description": "Population-based algorithm that combines differential evolution mutation with a self-organizing map to adaptively cluster individuals and apply different search strategies within each cluster, enhancing exploration and exploitation based on cluster fitness variance.", "code": "import numpy as np\n\nclass SOM_DE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, som_grid_size=5, de_mutation_factor=0.7, de_crossover_rate=0.9, learning_rate=0.1, sigma=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.som_grid_size = som_grid_size\n        self.de_mutation_factor = de_mutation_factor\n        self.de_crossover_rate = de_crossover_rate\n        self.learning_rate = learning_rate\n        self.sigma = sigma  # Initial neighborhood radius for SOM\n        self.som = np.random.rand(som_grid_size, som_grid_size, dim)  # Initialize SOM weights\n        self.cluster_assignments = np.zeros(pop_size, dtype=int)\n        self.min_sigma = 0.01\n        self.sigma_decay = 0.995\n        self.lr_decay = 0.995\n\n\n    def find_closest_node(self, individual):\n        distances = np.sum((self.som - individual)**2, axis=2)\n        return np.unravel_index(np.argmin(distances), distances.shape)\n\n    def update_som(self, individual, winner_coords):\n        for i in range(self.som_grid_size):\n            for j in range(self.som_grid_size):\n                distance = np.sqrt((i - winner_coords[0])**2 + (j - winner_coords[1])**2)\n                influence = np.exp(-distance**2 / (2 * self.sigma**2))\n                self.som[i, j] += self.learning_rate * influence * (individual - self.som[i, j])\n\n    def differential_evolution(self, population, fitness, func):\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            a, b, c = population[idxs]\n            mutant = a + self.de_mutation_factor * (b - c)\n            mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n            trial = np.copy(population[i])\n            for j in range(self.dim):\n                if np.random.rand() < self.de_crossover_rate or j == np.random.randint(self.dim):\n                    trial[j] = mutant[j]\n\n            f_trial = func(trial)\n            self.budget -= 1\n            if f_trial < fitness[i]:\n                population[i] = trial\n                fitness[i] = f_trial\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n        return population, fitness\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        while self.budget > 0:\n            # SOM Clustering and Update\n            for i in range(self.pop_size):\n                winner_coords = self.find_closest_node(population[i])\n                self.update_som(population[i], winner_coords)\n                self.cluster_assignments[i] = winner_coords[0] * self.som_grid_size + winner_coords[1]\n\n            # Apply Differential Evolution\n            population, fitness = self.differential_evolution(population, fitness, func)\n\n            # Adaptive Parameter Control (Decay)\n            self.sigma = max(self.min_sigma, self.sigma * self.sigma_decay)\n            self.learning_rate *= self.lr_decay\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm SOM_DE scored 0.568 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2b5c9234-3f08-4430-af9a-5b0aa11c0445"], "operator": null, "metadata": {"aucs": [0.20047697650745322, 0.4380345653650778, 0.49424272198853003, 0.7797358664440819, 0.5937397944317837, 0.667813342081345, 0.4197778725065656, 0.47354893483298033, 0.5897795704272377, 0.4758336885653651, 0.7953214132922188, 0.9999215251718793, 0.47649652193225034, 0.543866955235011, 0.8864450449088537, 0.6211546132090735, 0.445965175724256, 0.7535064505460426, 0.2124746189440806, 0.4979314821366756]}}
{"id": "377b024f-0528-4b3d-929d-7971321f3eb7", "fitness": 0.3100533514902325, "name": "EnhancedDynamicExplorationExploitation", "description": "Introducing a learning rate for exploration and exploitation rates based on exponentially weighted moving averages of success and diversity, and integrating orthogonal learning to enhance population diversity.", "code": "import numpy as np\n\nclass EnhancedDynamicExplorationExploitation:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_exploration_rate=0.5, initial_exploitation_rate=0.2, diversity_threshold=0.1, archive_size=10, exploration_lr=0.1, exploitation_lr=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.exploration_rate = initial_exploration_rate\n        self.exploitation_rate = initial_exploitation_rate\n        self.diversity_threshold = diversity_threshold\n        self.archive = []\n        self.archive_size = archive_size\n        self.step_size = 0.5  # Initial step size for perturbations\n        self.step_decay = 0.995 # Decay factor for step size\n        self.success_ewma = 0.0  # Exponentially weighted moving average of success\n        self.diversity_ewma = 0.0 # Exponentially weighted moving average of diversity\n        self.exploration_lr = exploration_lr\n        self.exploitation_lr = exploitation_lr\n\n    def __call__(self, func):\n        # Initialize population within the bounds\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        while self.budget > 0:\n            new_population = np.zeros_like(population)\n            new_fitness = np.zeros_like(fitness)\n\n            for i in range(self.pop_size):\n                if np.random.rand() < self.exploration_rate:\n                    # Global Exploration: Random perturbation with decaying step size\n                    new_individual = population[i] + np.random.normal(0, self.step_size, size=self.dim)\n                    new_individual = np.clip(new_individual, func.bounds.lb, func.bounds.ub)\n\n                else:\n                    # Local Exploitation: Perturbation around the best solution and/or archive\n                    if len(self.archive) > 0 and np.random.rand() < self.exploitation_rate:\n                        # Exploitation using archive\n                        archive_idx = np.random.randint(len(self.archive))\n                        new_individual = population[i] + 0.5 * (self.archive[archive_idx] - population[i]) + np.random.normal(0, self.step_size * 0.2, size=self.dim)\n                        new_individual = np.clip(new_individual, func.bounds.lb, func.bounds.ub)\n                    else:\n                        # Exploitation around best\n                        new_individual = population[i] + np.random.normal(0, self.step_size * 0.2, size=self.dim) + 0.1 * (self.x_opt - population[i])\n                        new_individual = np.clip(new_individual, func.bounds.lb, func.bounds.ub)\n\n                new_fitness[i] = func(new_individual)\n                self.budget -= 1\n                if new_fitness[i] < self.f_opt:\n                    self.f_opt = new_fitness[i]\n                    self.x_opt = new_individual\n\n                new_population[i] = new_individual\n\n            # Update population and archive\n            num_improvements = 0\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    population[i] = new_population[i]\n                    fitness[i] = new_fitness[i]\n                    num_improvements += 1\n\n                    # Archive the solution\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(new_population[i].copy())\n                    else:\n                        # Replace a random archive member\n                        replace_idx = np.random.randint(self.archive_size)\n                        self.archive[replace_idx] = new_population[i].copy()\n            \n            # Calculate average pairwise distance (diversity metric)\n            distances = []\n            for i in range(self.pop_size):\n                for j in range(i + 1, self.pop_size):\n                    distances.append(np.linalg.norm(population[i] - population[j]))\n            avg_distance = np.mean(distances) if distances else 0.0\n            \n            # Update EWMA of success and diversity\n            improvement_ratio = num_improvements / self.pop_size\n            self.success_ewma = (1 - self.exploration_lr) * self.success_ewma + self.exploration_lr * improvement_ratio\n            self.diversity_ewma = (1 - self.exploration_lr) * self.diversity_ewma + self.exploration_lr * avg_distance\n\n            # Adjust exploration and exploitation rates based on EWMA\n            if self.success_ewma > 0.2 and self.diversity_ewma > self.diversity_threshold:\n                self.exploration_rate *= (1 - self.exploitation_lr)  # Reduce exploration if exploitation is working well and diversity is high\n                self.exploitation_rate = min(1.0, self.exploitation_rate * (1 + self.exploitation_lr))\n            else:\n                self.exploration_rate = min(1.0, self.exploration_rate * (1 + self.exploitation_lr))  # Increase exploration if not improving or low diversity\n                self.exploitation_rate *= (1 - self.exploitation_lr)\n\n            # Step size decay\n            self.step_size *= self.step_decay\n\n            # Orthogonal Learning\n            if np.random.rand() < 0.1:\n                orthogonal_matrix = np.random.randn(self.dim, self.dim)\n                q, r = np.linalg.qr(orthogonal_matrix)\n                for i in range(self.pop_size):\n                    perturbation = np.dot(q, np.random.normal(0, self.step_size * 0.1, size=self.dim))\n                    new_individual = population[i] + perturbation\n                    new_individual = np.clip(new_individual, func.bounds.lb, func.bounds.ub)\n                    new_fitness_value = func(new_individual)\n                    self.budget -= 1\n                    if new_fitness_value < fitness[i]:\n                        population[i] = new_individual\n                        fitness[i] = new_fitness_value\n                        if new_fitness_value < self.f_opt:\n                            self.f_opt = new_fitness_value\n                            self.x_opt = new_individual\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm EnhancedDynamicExplorationExploitation scored 0.310 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2b5c9234-3f08-4430-af9a-5b0aa11c0445"], "operator": null, "metadata": {"aucs": [0.12477412776496644, 0.20275752706278694, 0.48223738954970985, 0.18729494362704813, 0.30206431700711545, 0.4078687472963819, 0.2634395759086945, 0.3751007863089548, 0.3387530073604388, 0.20150982440183562, 0.20626549097421198, 0.9865749411619305, 0.2621062424391809, 0]}}
{"id": "779600d9-6ef4-4548-86a6-01e8605aa7d9", "fitness": -Infinity, "name": "DynamicResourceAllocation", "description": "A population-based algorithm with dynamic resource allocation between global search (Nelder-Mead) and local refinement (BFGS), guided by success rate.", "code": "import numpy as np\nfrom scipy.optimize import minimize, Bounds, shgo\n\nclass DynamicResourceAllocation:\n    def __init__(self, budget=10000, dim=10, pop_size=10, initial_global_ratio=0.5, success_memory=10, nelder_mead_max_iter=50, bfgs_max_iter=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.global_ratio = initial_global_ratio  # Fraction of budget allocated to global search\n        self.success_memory = success_memory  # Number of iterations to remember success rate\n        self.success_history = []  # Store success/failure for adjusting global_ratio\n        self.nelder_mead_max_iter = nelder_mead_max_iter\n        self.bfgs_max_iter = bfgs_max_iter\n        self.bounds = Bounds(-5.0, 5.0)\n        self.archive = []\n        self.archive_size = 10\n\n\n    def __call__(self, func):\n        population = np.random.uniform(self.bounds.lb, self.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n        \n        global_budget = int(self.budget * self.global_ratio)\n        local_budget = self.budget - global_budget\n        \n        # Global search (Nelder-Mead on a few random starting points)\n        global_iters = min(self.pop_size, global_budget // self.nelder_mead_max_iter) # Limit iterations based on remaining budget.\n\n        for i in range(global_iters):\n            if self.budget <= 0:\n                break\n            x0 = np.random.uniform(self.bounds.lb, self.bounds.ub, size=self.dim)\n            res = minimize(func, x0, method='Nelder-Mead', bounds=self.bounds, options={'maxiter': self.nelder_mead_max_iter, 'maxfev': self.nelder_mead_max_iter})\n            \n            if res.fun < self.f_opt:\n                self.f_opt = res.fun\n                self.x_opt = res.x\n                self.success_history.append(1)\n            else:\n                self.success_history.append(0)\n            self.budget -= res.nfev\n\n        # Local search (BFGS on current population)\n        for i in range(self.pop_size):\n            if self.budget <= 0:\n                break\n            res = minimize(func, population[i], method='L-BFGS-B', bounds=self.bounds, options={'maxiter': self.bfgs_max_iter, 'maxfun': self.bfgs_max_iter})\n            if res.fun < self.f_opt:\n                self.f_opt = res.fun\n                self.x_opt = res.x\n                self.success_history.append(1)\n\n                if len(self.archive) < self.archive_size:\n                    self.archive.append(res.x.copy())\n                else:\n                    replace_idx = np.random.randint(self.archive_size)\n                    self.archive[replace_idx] = res.x.copy()\n\n            else:\n                self.success_history.append(0)\n\n            self.budget -= res.nfev\n\n        # Adaptive resource allocation\n        if len(self.success_history) > self.success_memory:\n            self.success_history = self.success_history[-self.success_memory:]\n            success_rate = np.mean(self.success_history)\n            if success_rate > 0.2:\n                self.global_ratio *= 0.9  # Reduce global search\n            else:\n                self.global_ratio = min(1.0, self.global_ratio * 1.1)  # Increase global search\n            self.global_ratio = np.clip(self.global_ratio, 0.1, 0.9)\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "An exception occurred: name 'Bounds' is not defined.", "error": "", "parent_ids": ["377b024f-0528-4b3d-929d-7971321f3eb7"], "operator": null, "metadata": {}}
{"id": "7f4be50d-8fcb-4735-9e59-910d298abd27", "fitness": 0.0, "name": "EnhancedDynamicExplorationExploitation", "description": "Enhanced dynamic exploration-exploitation with adaptive step size control using success rate, orthogonal learning with dynamic probability, and improved archive handling with fitness-based replacement.", "code": "import numpy as np\n\nclass EnhancedDynamicExplorationExploitation:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_exploration_rate=0.5, initial_exploitation_rate=0.2, diversity_threshold=0.1, archive_size=10, exploration_lr=0.1, exploitation_lr=0.1, initial_step_size=0.5, step_decay=0.995, orthogonal_learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.exploration_rate = initial_exploration_rate\n        self.exploitation_rate = initial_exploitation_rate\n        self.diversity_threshold = diversity_threshold\n        self.archive = []\n        self.archive_size = archive_size\n        self.step_size = initial_step_size  # Initial step size for perturbations\n        self.step_decay = step_decay # Decay factor for step size\n        self.success_ewma = 0.0  # Exponentially weighted moving average of success\n        self.diversity_ewma = 0.0 # Exponentially weighted moving average of diversity\n        self.exploration_lr = exploration_lr\n        self.exploitation_lr = exploitation_lr\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n\n    def __call__(self, func):\n        # Initialize population within the bounds\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        while self.budget > 0:\n            new_population = np.zeros_like(population)\n            new_fitness = np.zeros_like(fitness)\n\n            for i in range(self.pop_size):\n                if np.random.rand() < self.exploration_rate:\n                    # Global Exploration: Random perturbation with decaying step size\n                    new_individual = population[i] + np.random.normal(0, self.step_size, size=self.dim)\n                    new_individual = np.clip(new_individual, func.bounds.lb, func.bounds.ub)\n\n                else:\n                    # Local Exploitation: Perturbation around the best solution and/or archive\n                    if len(self.archive) > 0 and np.random.rand() < self.exploitation_rate:\n                        # Exploitation using archive\n                        archive_idx = np.random.randint(len(self.archive))\n                        new_individual = population[i] + 0.5 * (self.archive[archive_idx] - population[i]) + np.random.normal(0, self.step_size * 0.2, size=self.dim)\n                        new_individual = np.clip(new_individual, func.bounds.lb, func.bounds.ub)\n                    else:\n                        # Exploitation around best\n                        new_individual = population[i] + np.random.normal(0, self.step_size * 0.2, size=self.dim) + 0.1 * (self.x_opt - population[i])\n                        new_individual = np.clip(new_individual, func.bounds.lb, func.bounds.ub)\n\n                new_fitness[i] = func(new_individual)\n                self.budget -= 1\n                if new_fitness[i] < self.f_opt:\n                    self.f_opt = new_fitness[i]\n                    self.x_opt = new_individual\n\n                new_population[i] = new_individual\n\n            # Update population and archive\n            num_improvements = 0\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    population[i] = new_population[i]\n                    fitness[i] = new_fitness[i]\n                    num_improvements += 1\n\n                    # Archive the solution, replacing the worst if full\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(new_population[i].copy())\n                    else:\n                        archive_fitness = [func(x) for x in self.archive]\n                        worst_archive_index = np.argmax(archive_fitness)\n                        if new_fitness[i] < archive_fitness[worst_archive_index]:\n                            self.archive[worst_archive_index] = new_population[i].copy()\n            \n            # Calculate average pairwise distance (diversity metric)\n            distances = []\n            for i in range(self.pop_size):\n                for j in range(i + 1, self.pop_size):\n                    distances.append(np.linalg.norm(population[i] - population[j]))\n            avg_distance = np.mean(distances) if distances else 0.0\n            \n            # Update EWMA of success and diversity\n            improvement_ratio = num_improvements / self.pop_size\n            self.success_ewma = (1 - self.exploration_lr) * self.success_ewma + self.exploration_lr * improvement_ratio\n            self.diversity_ewma = (1 - self.exploration_lr) * self.diversity_ewma + self.exploration_lr * avg_distance\n\n            # Adjust exploration and exploitation rates based on EWMA\n            if self.success_ewma > 0.2 and self.diversity_ewma > self.diversity_threshold:\n                self.exploration_rate *= (1 - self.exploitation_lr)  # Reduce exploration if exploitation is working well and diversity is high\n                self.exploitation_rate = min(1.0, self.exploitation_rate * (1 + self.exploitation_lr))\n                self.step_size *= 0.95 # Reduce step size when doing well\n            else:\n                self.exploration_rate = min(1.0, self.exploration_rate * (1 + self.exploitation_lr))  # Increase exploration if not improving or low diversity\n                self.exploitation_rate *= (1 - self.exploitation_lr)\n                self.step_size *= 1.05 # Increase step size when not doing well\n                self.step_size = min(self.step_size, 0.5)\n\n\n            # Step size decay\n            self.step_size *= self.step_decay\n\n            # Orthogonal Learning, adaptive probability based on exploration rate\n            if np.random.rand() < self.orthogonal_learning_rate * self.exploration_rate:\n                orthogonal_matrix = np.random.randn(self.dim, self.dim)\n                q, r = np.linalg.qr(orthogonal_matrix)\n                for i in range(self.pop_size):\n                    perturbation = np.dot(q, np.random.normal(0, self.step_size * 0.1, size=self.dim))\n                    new_individual = population[i] + perturbation\n                    new_individual = np.clip(new_individual, func.bounds.lb, func.bounds.ub)\n                    new_fitness_value = func(new_individual)\n                    self.budget -= 1\n                    if new_fitness_value < fitness[i]:\n                        population[i] = new_individual\n                        fitness[i] = new_fitness_value\n                        if new_fitness_value < self.f_opt:\n                            self.f_opt = new_fitness_value\n                            self.x_opt = new_individual\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm EnhancedDynamicExplorationExploitation scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["377b024f-0528-4b3d-929d-7971321f3eb7"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "2a42f89b-fcd4-4afd-88ce-5607510f65aa", "fitness": 0.0, "name": "EnhancedDynamicExplorationExploitation", "description": "Enhanced exploration-exploitation balance with adaptive learning rates for exploration/exploitation, improved diversity maintenance using a restart mechanism, and orthogonal learning with adaptive step size.", "code": "import numpy as np\n\nclass EnhancedDynamicExplorationExploitation:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_exploration_rate=0.5, initial_exploitation_rate=0.2, diversity_threshold=0.1, archive_size=10, exploration_lr=0.1, exploitation_lr=0.1, restart_prob=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.exploration_rate = initial_exploration_rate\n        self.exploitation_rate = initial_exploitation_rate\n        self.diversity_threshold = diversity_threshold\n        self.archive = []\n        self.archive_size = archive_size\n        self.step_size = 0.5  # Initial step size for perturbations\n        self.step_decay = 0.995 # Decay factor for step size\n        self.success_ewma = 0.0  # Exponentially weighted moving average of success\n        self.diversity_ewma = 0.0 # Exponentially weighted moving average of diversity\n        self.exploration_lr = exploration_lr\n        self.exploitation_lr = exploitation_lr\n        self.restart_prob = restart_prob  # Probability of restarting the population\n        self.orthogonal_learning_prob = 0.1\n\n    def __call__(self, func):\n        # Initialize population within the bounds\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        while self.budget > 0:\n            new_population = np.zeros_like(population)\n            new_fitness = np.zeros_like(fitness)\n\n            for i in range(self.pop_size):\n                if np.random.rand() < self.exploration_rate:\n                    # Global Exploration: Random perturbation with decaying step size\n                    new_individual = population[i] + np.random.normal(0, self.step_size, size=self.dim)\n                    new_individual = np.clip(new_individual, func.bounds.lb, func.bounds.ub)\n\n                else:\n                    # Local Exploitation: Perturbation around the best solution and/or archive\n                    if len(self.archive) > 0 and np.random.rand() < self.exploitation_rate:\n                        # Exploitation using archive\n                        archive_idx = np.random.randint(len(self.archive))\n                        new_individual = population[i] + 0.5 * (self.archive[archive_idx] - population[i]) + np.random.normal(0, self.step_size * 0.2, size=self.dim)\n                        new_individual = np.clip(new_individual, func.bounds.lb, func.bounds.ub)\n                    else:\n                        # Exploitation around best\n                        new_individual = population[i] + np.random.normal(0, self.step_size * 0.2, size=self.dim) + 0.1 * (self.x_opt - population[i])\n                        new_individual = np.clip(new_individual, func.bounds.lb, func.bounds.ub)\n\n                new_fitness[i] = func(new_individual)\n                self.budget -= 1\n                if new_fitness[i] < self.f_opt:\n                    self.f_opt = new_fitness[i]\n                    self.x_opt = new_individual\n\n                new_population[i] = new_individual\n\n            # Update population and archive\n            num_improvements = 0\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    population[i] = new_population[i]\n                    fitness[i] = new_fitness[i]\n                    num_improvements += 1\n\n                    # Archive the solution\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(new_population[i].copy())\n                    else:\n                        # Replace the worst archive member\n                        archive_fitness = [func(x) for x in self.archive]\n                        worst_archive_index = np.argmax(archive_fitness)\n                        self.archive[worst_archive_index] = new_population[i].copy()\n            \n            # Calculate average pairwise distance (diversity metric)\n            distances = []\n            for i in range(self.pop_size):\n                for j in range(i + 1, self.pop_size):\n                    distances.append(np.linalg.norm(population[i] - population[j]))\n            avg_distance = np.mean(distances) if distances else 0.0\n            \n            # Update EWMA of success and diversity\n            improvement_ratio = num_improvements / self.pop_size\n            self.success_ewma = (1 - self.exploration_lr) * self.success_ewma + self.exploration_lr * improvement_ratio\n            self.diversity_ewma = (1 - self.exploitation_lr) * self.diversity_ewma + self.exploitation_lr * avg_distance\n\n            # Adjust exploration and exploitation rates based on EWMA\n            if self.success_ewma > 0.2 and self.diversity_ewma > self.diversity_threshold:\n                self.exploration_rate *= (1 - self.exploitation_lr)  # Reduce exploration if exploitation is working well and diversity is high\n                self.exploitation_rate = min(1.0, self.exploitation_rate * (1 + self.exploitation_lr))\n            else:\n                self.exploration_rate = min(1.0, self.exploration_rate * (1 + self.exploitation_lr))  # Increase exploration if not improving or low diversity\n                self.exploitation_rate *= (1 - self.exploitation_lr)\n\n            # Step size decay\n            self.step_size *= self.step_decay\n\n            # Orthogonal Learning with adaptive step size\n            if np.random.rand() < self.orthogonal_learning_prob:\n                orthogonal_matrix = np.random.randn(self.dim, self.dim)\n                q, r = np.linalg.qr(orthogonal_matrix)\n                orthogonal_step_size = self.step_size * 0.1\n                for i in range(self.pop_size):\n                    perturbation = np.dot(q, np.random.normal(0, orthogonal_step_size, size=self.dim))\n                    new_individual = population[i] + perturbation\n                    new_individual = np.clip(new_individual, func.bounds.lb, func.bounds.ub)\n                    new_fitness_value = func(new_individual)\n                    self.budget -= 1\n                    if new_fitness_value < fitness[i]:\n                        population[i] = new_individual\n                        fitness[i] = new_fitness_value\n                        if new_fitness_value < self.f_opt:\n                            self.f_opt = new_fitness_value\n                            self.x_opt = new_individual\n                            \n            # Population restart mechanism\n            if np.random.rand() < self.restart_prob:\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm EnhancedDynamicExplorationExploitation scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["377b024f-0528-4b3d-929d-7971321f3eb7"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "774b9f0b-58f9-4b21-9a53-78b0b80dc4f1", "fitness": 0.0, "name": "EnhancedSOM_DE", "description": "Enhanced SOM-DE with adaptive mutation factor, cluster-aware parameter adaptation, and a local search refinement step for improved exploitation.", "code": "import numpy as np\n\nclass EnhancedSOM_DE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, som_grid_size=5, \n                 de_mutation_factor=0.7, de_crossover_rate=0.9, learning_rate=0.1, sigma=1.0,\n                 mutation_adaptation_rate=0.1, local_search_probability=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.som_grid_size = som_grid_size\n        self.de_mutation_factor = de_mutation_factor\n        self.de_crossover_rate = de_crossover_rate\n        self.learning_rate = learning_rate\n        self.sigma = sigma  # Initial neighborhood radius for SOM\n        self.som = np.random.rand(som_grid_size, som_grid_size, dim)  # Initialize SOM weights\n        self.cluster_assignments = np.zeros(pop_size, dtype=int)\n        self.min_sigma = 0.01\n        self.sigma_decay = 0.995\n        self.lr_decay = 0.995\n        self.mutation_adaptation_rate = mutation_adaptation_rate\n        self.local_search_probability = local_search_probability\n\n        self.archive = []\n        self.archive_size = pop_size\n        self.success_mutation_factors = []\n        self.success_crossover_rates = []\n        self.memory_size = 10\n        self.memory_mutation_factors = np.full(self.memory_size, self.de_mutation_factor)\n        self.memory_crossover_rates = np.full(self.memory_size, self.de_crossover_rate)\n        self.memory_index = 0\n\n\n    def find_closest_node(self, individual):\n        distances = np.sum((self.som - individual)**2, axis=2)\n        return np.unravel_index(np.argmin(distances), distances.shape)\n\n    def update_som(self, individual, winner_coords):\n        for i in range(self.som_grid_size):\n            for j in range(self.som_grid_size):\n                distance = np.sqrt((i - winner_coords[0])**2 + (j - winner_coords[1])**2)\n                influence = np.exp(-distance**2 / (2 * self.sigma**2))\n                self.som[i, j] += self.learning_rate * influence * (individual - self.som[i, j])\n\n    def differential_evolution(self, population, fitness, func):\n        new_population = np.copy(population)\n        new_fitness = np.copy(fitness)\n\n        for i in range(self.pop_size):\n            # Adaptive mutation factor and crossover rate\n            mutation_factor = self.memory_mutation_factors[np.random.randint(self.memory_size)]\n            crossover_rate = self.memory_crossover_rates[np.random.randint(self.memory_size)]\n\n            idxs = np.random.choice(self.pop_size + len(self.archive), 3, replace=False)\n            a = population[idxs[0]] if idxs[0] < self.pop_size else self.archive[idxs[0] - self.pop_size]\n            b = population[idxs[1]] if idxs[1] < self.pop_size else self.archive[idxs[1] - self.pop_size]\n            c = population[idxs[2]] if idxs[2] < self.pop_size else self.archive[idxs[2] - self.pop_size]\n            \n            mutant = a + mutation_factor * (b - c)\n            mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n            trial = np.copy(population[i])\n            for j in range(self.dim):\n                if np.random.rand() < crossover_rate or j == np.random.randint(self.dim):\n                    trial[j] = mutant[j]\n\n            f_trial = func(trial)\n            self.budget -= 1\n            if f_trial < fitness[i]:\n                new_population[i] = trial\n                new_fitness[i] = f_trial\n                self.success_mutation_factors.append(mutation_factor)\n                self.success_crossover_rates.append(crossover_rate)\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                # Update archive\n                if len(self.archive) < self.archive_size:\n                    self.archive.append(population[i])\n                else:\n                    j = np.random.randint(self.archive_size)\n                    self.archive[j] = population[i]\n            else:\n                # Add to archive if not successful but better than a random archive member\n                if len(self.archive) > 0:\n                    j = np.random.randint(len(self.archive))\n                    if fitness[i] < func(self.archive[j]): #Ensure func evaluation is accounted for budget\n                        self.budget -=1\n                        self.archive[j] = population[i]\n\n        return new_population, new_fitness\n\n    def local_search(self, individual, func, step_size=0.1):\n        \"\"\"Performs a local search around the individual.\"\"\"\n        best_individual = individual\n        best_fitness = func(individual)\n        self.budget -= 1\n        \n        for _ in range(5):  # Limited evaluations\n            neighbor = individual + np.random.uniform(-step_size, step_size, size=self.dim)\n            neighbor = np.clip(neighbor, func.bounds.lb, func.bounds.ub)\n            \n            fitness = func(neighbor)\n            self.budget -= 1\n            \n            if fitness < best_fitness:\n                best_fitness = fitness\n                best_individual = neighbor\n                if fitness < self.f_opt:\n                    self.f_opt = fitness\n                    self.x_opt = neighbor\n        \n        return best_individual, best_fitness\n    \n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        while self.budget > 0:\n            # SOM Clustering and Update\n            for i in range(self.pop_size):\n                winner_coords = self.find_closest_node(population[i])\n                self.update_som(population[i], winner_coords)\n                self.cluster_assignments[i] = winner_coords[0] * self.som_grid_size + winner_coords[1]\n\n            # Apply Differential Evolution\n            population, fitness = self.differential_evolution(population, fitness, func)\n\n            # Local Search Refinement\n            for i in range(self.pop_size):\n                if np.random.rand() < self.local_search_probability:\n                    population[i], fitness[i] = self.local_search(population[i], func)\n\n            # Update memory of successful mutation factors and crossover rates\n            if self.success_mutation_factors:\n                self.memory_mutation_factors[self.memory_index] = np.mean(self.success_mutation_factors)\n                self.memory_crossover_rates[self.memory_index] = np.mean(self.success_crossover_rates)\n                self.memory_index = (self.memory_index + 1) % self.memory_size\n                self.success_mutation_factors = []\n                self.success_crossover_rates = []\n\n            # Adaptive Parameter Control (Decay)\n            self.sigma = max(self.min_sigma, self.sigma * self.sigma_decay)\n            self.learning_rate *= self.lr_decay\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm EnhancedSOM_DE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["df153811-89fa-41a6-aacf-decb5834d88c"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "d38f557f-22b3-47f6-acaf-4dffec705158", "fitness": -Infinity, "name": "FuzzyAdaptiveDE", "description": "An adaptive differential evolution algorithm with a fuzzy logic controller for parameter adaptation based on success rate and diversity.", "code": "import numpy as np\nimport skfuzzy as fuzz\nfrom skfuzzy import control as ctrl\n\nclass FuzzyAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=None):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else int(4 + 3 * np.log(budget))\n        self.population = None\n        self.fitness = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.F = 0.5\n        self.Cr = 0.9\n        self.archive = []\n        self.archive_size = 5\n        self.fevals = 0\n        \n        # Fuzzy Logic Controller Setup\n        self.setup_fuzzy_controller()\n\n    def setup_fuzzy_controller(self):\n        # Antecedent (Input) variables\n        success_rate = ctrl.Antecedent(np.arange(0, 1.01, 0.01), 'success_rate')\n        diversity = ctrl.Antecedent(np.arange(0, 1.01, 0.01), 'diversity')\n\n        # Consequent (Output) variables\n        adjust_f = ctrl.Consequent(np.arange(-0.2, 0.21, 0.01), 'adjust_f')\n        adjust_cr = ctrl.Consequent(np.arange(-0.2, 0.21, 0.01), 'adjust_cr')\n\n        # Membership functions (Gaussian)\n        success_rate['low'] = fuzz.gaussmf(success_rate.universe, 0, 0.3)\n        success_rate['medium'] = fuzz.gaussmf(success_rate.universe, 0.5, 0.3)\n        success_rate['high'] = fuzz.gaussmf(success_rate.universe, 1, 0.3)\n\n        diversity['low'] = fuzz.gaussmf(diversity.universe, 0, 0.3)\n        diversity['medium'] = fuzz.gaussmf(diversity.universe, 0.5, 0.3)\n        diversity['high'] = fuzz.gaussmf(diversity.universe, 1, 0.3)\n\n        adjust_f['decrease'] = fuzz.gaussmf(adjust_f.universe, -0.2, 0.1)\n        adjust_f['none'] = fuzz.gaussmf(adjust_f.universe, 0, 0.1)\n        adjust_f['increase'] = fuzz.gaussmf(adjust_f.universe, 0.2, 0.1)\n\n        adjust_cr['decrease'] = fuzz.gaussmf(adjust_cr.universe, -0.2, 0.1)\n        adjust_cr['none'] = fuzz.gaussmf(adjust_cr.universe, 0, 0.1)\n        adjust_cr['increase'] = fuzz.gaussmf(adjust_cr.universe, 0.2, 0.1)\n\n        # Rules\n        rule1 = ctrl.Rule(success_rate['low'] & diversity['low'], (adjust_f['increase'], adjust_cr['increase']))\n        rule2 = ctrl.Rule(success_rate['low'] & diversity['medium'], (adjust_f['increase'], adjust_cr['none']))\n        rule3 = ctrl.Rule(success_rate['low'] & diversity['high'], (adjust_f['increase'], adjust_cr['decrease']))\n\n        rule4 = ctrl.Rule(success_rate['medium'] & diversity['low'], (adjust_f['none'], adjust_cr['increase']))\n        rule5 = ctrl.Rule(success_rate['medium'] & diversity['medium'], (adjust_f['none'], adjust_cr['none']))\n        rule6 = ctrl.Rule(success_rate['medium'] & diversity['high'], (adjust_f['none'], adjust_cr['decrease']))\n\n        rule7 = ctrl.Rule(success_rate['high'] & diversity['low'], (adjust_f['decrease'], adjust_cr['increase']))\n        rule8 = ctrl.Rule(success_rate['high'] & diversity['medium'], (adjust_f['decrease'], adjust_cr['none']))\n        rule9 = ctrl.Rule(success_rate['high'] & diversity['high'], (adjust_f['decrease'], adjust_cr['decrease']))\n\n        # Control System\n        self.parameter_control = ctrl.ControlSystem([rule1, rule2, rule3, rule4, rule5, rule6, rule7, rule8, rule9])\n        self.parameter_simulation = ctrl.ControlSystemSimulation(self.parameter_control)\n\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.fevals = self.pop_size\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.population, axis=0)\n        distances = np.linalg.norm(self.population - mean_position, axis=1)\n        diversity = np.mean(distances)\n        max_possible_distance = np.linalg.norm(self.bounds_ub - self.bounds_lb) * np.sqrt(self.dim)\n        normalized_diversity = diversity / max_possible_distance if max_possible_distance > 0 else 0\n        return normalized_diversity\n\n    def mutate(self, pop, i):\n        indices = [idx for idx in range(len(pop)) if idx != i]\n        if len(self.archive) > 0 and np.random.rand() < 0.1:\n            arch_ind = np.random.randint(0, len(self.archive))\n            pop_indices = np.random.choice(indices, 2, replace=False)\n            a = self.archive[arch_ind]\n            b, c = pop[pop_indices[0]], pop[pop_indices[1]]\n        else:\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n        \n        mutation = a + self.F * (b - c)\n        return mutation\n\n    def mirrored_boundary_handling(self, mutant):\n        for i in range(self.dim):\n            if mutant[i] < self.bounds_lb[i]:\n                mutant[i] = self.bounds_lb[i] + (self.bounds_lb[i] - mutant[i])\n            elif mutant[i] > self.bounds_ub[i]:\n                mutant[i] = self.bounds_ub[i] - (mutant[i] - self.bounds_ub[i])\n        return mutant\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.Cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n        \n        successful_individuals = 0\n\n        while self.fevals < self.budget:\n            diversity = self.calculate_diversity()\n            \n            for i in range(self.pop_size):\n                mutant = self.mutate(self.population, i)\n                mutant = self.mirrored_boundary_handling(mutant)\n                trial = self.crossover(mutant, self.population[i])\n                \n                f = func(trial)\n                self.fevals += 1\n\n                if f < self.fitness[i]:\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        replace_index = np.random.randint(0, self.archive_size)\n                        self.archive[replace_index] = self.population[i].copy()\n\n                    self.fitness[i] = f\n                    self.population[i] = trial\n                    successful_individuals += 1\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                \n                if self.fevals >= self.budget:\n                    break\n\n            # Fuzzy Logic Control for F and Cr adaptation\n            success_rate = successful_individuals / self.pop_size\n            self.parameter_simulation.input['success_rate'] = success_rate\n            self.parameter_simulation.input['diversity'] = diversity\n            self.parameter_simulation.compute()\n\n            f_adjustment = self.parameter_simulation.output['adjust_f']\n            cr_adjustment = self.parameter_simulation.output['adjust_cr']\n\n            self.F = np.clip(self.F + f_adjustment, 0.1, 1.0)\n            self.Cr = np.clip(self.Cr + cr_adjustment, 0.1, 0.9)\n            \n            successful_individuals = 0 # Reset for next generation\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "An exception occurred: No module named 'skfuzzy'.", "error": "", "parent_ids": ["a7a98c22-f00c-40c5-896b-b47ae99f557c"], "operator": null, "metadata": {}}
{"id": "05f2004d-0e19-4ff8-b397-48fb3c1733c0", "fitness": 0.12995835487900423, "name": "EnhancedDynamicExplorationExploitation", "description": "Dynamically adjusts exploration and exploitation with success and diversity EWMA, employing orthogonal learning and a velocity-based perturbation inspired by PSO for enhanced search.", "code": "import numpy as np\n\nclass EnhancedDynamicExplorationExploitation:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_exploration_rate=0.5, initial_exploitation_rate=0.2, diversity_threshold=0.1, archive_size=10, exploration_lr=0.1, exploitation_lr=0.1, inertia_weight=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.exploration_rate = initial_exploration_rate\n        self.exploitation_rate = initial_exploitation_rate\n        self.diversity_threshold = diversity_threshold\n        self.archive = []\n        self.archive_size = archive_size\n        self.step_size = 0.5  # Initial step size for perturbations\n        self.step_decay = 0.995 # Decay factor for step size\n        self.success_ewma = 0.0  # Exponentially weighted moving average of success\n        self.diversity_ewma = 0.0 # Exponentially weighted moving average of diversity\n        self.exploration_lr = exploration_lr\n        self.exploitation_lr = exploitation_lr\n        self.inertia_weight = inertia_weight\n        self.velocity = np.zeros((pop_size, dim)) # Initialize velocity for PSO-like update\n\n    def __call__(self, func):\n        # Initialize population within the bounds\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        while self.budget > 0:\n            new_population = np.zeros_like(population)\n            new_fitness = np.zeros_like(fitness)\n\n            for i in range(self.pop_size):\n                if np.random.rand() < self.exploration_rate:\n                    # Global Exploration: Random perturbation with decaying step size\n                    new_individual = population[i] + np.random.normal(0, self.step_size, size=self.dim)\n                    new_individual = np.clip(new_individual, func.bounds.lb, func.bounds.ub)\n\n                else:\n                    # Local Exploitation: Perturbation around the best solution and/or archive\n                    if len(self.archive) > 0 and np.random.rand() < self.exploitation_rate:\n                        # Exploitation using archive, incorporate velocity\n                        archive_idx = np.random.randint(len(self.archive))\n                        self.velocity[i] = self.inertia_weight * self.velocity[i] + \\\n                                            0.5 * (self.archive[archive_idx] - population[i]) + \\\n                                            np.random.normal(0, self.step_size * 0.2, size=self.dim)\n                        new_individual = population[i] + self.velocity[i]\n                        new_individual = np.clip(new_individual, func.bounds.lb, func.bounds.ub)\n                    else:\n                        # Exploitation around best, incorporate velocity\n                        self.velocity[i] = self.inertia_weight * self.velocity[i] + \\\n                                            np.random.normal(0, self.step_size * 0.2, size=self.dim) + \\\n                                            0.1 * (self.x_opt - population[i])\n                        new_individual = population[i] + self.velocity[i]\n                        new_individual = np.clip(new_individual, func.bounds.lb, func.bounds.ub)\n\n                new_fitness[i] = func(new_individual)\n                self.budget -= 1\n                if new_fitness[i] < self.f_opt:\n                    self.f_opt = new_fitness[i]\n                    self.x_opt = new_individual\n\n                new_population[i] = new_individual\n\n            # Update population and archive\n            num_improvements = 0\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    population[i] = new_population[i]\n                    fitness[i] = new_fitness[i]\n                    num_improvements += 1\n\n                    # Archive the solution\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(new_population[i].copy())\n                    else:\n                        # Replace a random archive member\n                        replace_idx = np.random.randint(self.archive_size)\n                        self.archive[replace_idx] = new_population[i].copy()\n            \n            # Calculate average pairwise distance (diversity metric)\n            distances = []\n            for i in range(self.pop_size):\n                for j in range(i + 1, self.pop_size):\n                    distances.append(np.linalg.norm(population[i] - population[j]))\n            avg_distance = np.mean(distances) if distances else 0.0\n            \n            # Update EWMA of success and diversity\n            improvement_ratio = num_improvements / self.pop_size\n            self.success_ewma = (1 - self.exploration_lr) * self.success_ewma + self.exploration_lr * improvement_ratio\n            self.diversity_ewma = (1 - self.exploration_lr) * self.diversity_ewma + self.exploration_lr * avg_distance\n\n            # Adjust exploration and exploitation rates based on EWMA\n            if self.success_ewma > 0.2 and self.diversity_ewma > self.diversity_threshold:\n                self.exploration_rate *= (1 - self.exploitation_lr)  # Reduce exploration if exploitation is working well and diversity is high\n                self.exploitation_rate = min(1.0, self.exploitation_rate * (1 + self.exploitation_lr))\n            else:\n                self.exploration_rate = min(1.0, self.exploration_rate * (1 + self.exploitation_lr))  # Increase exploration if not improving or low diversity\n                self.exploitation_rate *= (1 - self.exploitation_lr)\n\n            # Step size decay\n            self.step_size *= self.step_decay\n\n            # Orthogonal Learning: Apply more frequently when diversity is low\n            if np.random.rand() < (0.5 if self.diversity_ewma < self.diversity_threshold else 0.1):\n                orthogonal_matrix = np.random.randn(self.dim, self.dim)\n                q, r = np.linalg.qr(orthogonal_matrix)\n                for i in range(self.pop_size):\n                    perturbation = np.dot(q, np.random.normal(0, self.step_size * 0.1, size=self.dim))\n                    new_individual = population[i] + perturbation\n                    new_individual = np.clip(new_individual, func.bounds.lb, func.bounds.ub)\n                    new_fitness_value = func(new_individual)\n                    self.budget -= 1\n                    if new_fitness_value < fitness[i]:\n                        population[i] = new_individual\n                        fitness[i] = new_fitness_value\n                        if new_fitness_value < self.f_opt:\n                            self.f_opt = new_fitness_value\n                            self.x_opt = new_individual\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm EnhancedDynamicExplorationExploitation scored 0.130 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["377b024f-0528-4b3d-929d-7971321f3eb7"], "operator": null, "metadata": {"aucs": [0.13942242768463498, 0.25045263695237774, 0]}}
{"id": "6dae5775-13cc-4504-b718-f314f41222ed", "fitness": 0.0, "name": "AdaptiveSOM_DE", "description": "Hybrid SOM-DE with adaptive strategy selection based on cluster fitness variance and individual success rates, enhanced parameter adaptation, and local search for exploration.", "code": "import numpy as np\n\nclass AdaptiveSOM_DE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, som_grid_size=5,\n                 de_mutation_factor=0.7, de_crossover_rate=0.9, learning_rate=0.1, sigma=1.0,\n                 local_search_prob=0.1, success_rate_memory=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.som_grid_size = som_grid_size\n        self.de_mutation_factor = de_mutation_factor\n        self.de_crossover_rate = de_crossover_rate\n        self.learning_rate = learning_rate\n        self.sigma = sigma\n        self.som = np.random.rand(som_grid_size, som_grid_size, dim)\n        self.cluster_assignments = np.zeros(pop_size, dtype=int)\n        self.min_sigma = 0.01\n        self.sigma_decay = 0.995\n        self.lr_decay = 0.995\n        self.local_search_prob = local_search_prob\n        self.success_rate_memory = success_rate_memory\n        self.success_rates = np.zeros(pop_size)  # Track individual success rates\n        self.success_history = np.zeros((pop_size, success_rate_memory))\n        self.success_index = np.zeros(pop_size, dtype=int)\n\n    def find_closest_node(self, individual):\n        distances = np.sum((self.som - individual)**2, axis=2)\n        return np.unravel_index(np.argmin(distances), distances.shape)\n\n    def update_som(self, individual, winner_coords):\n        for i in range(self.som_grid_size):\n            for j in range(self.som_grid_size):\n                distance = np.sqrt((i - winner_coords[0])**2 + (j - winner_coords[1])**2)\n                influence = np.exp(-distance**2 / (2 * self.sigma**2))\n                self.som[i, j] += self.learning_rate * influence * (individual - self.som[i, j])\n\n    def differential_evolution(self, population, fitness, func, cluster_id):\n        # Adaptive mutation factor and crossover rate based on cluster variance\n        cluster_population = population[self.cluster_assignments == cluster_id]\n        if len(cluster_population) > 1:\n            cluster_fitness = fitness[self.cluster_assignments == cluster_id]\n            fitness_variance = np.var(cluster_fitness)\n            adaptive_mutation_factor = self.de_mutation_factor * (1 + fitness_variance)  # Higher variance, higher mutation\n            adaptive_crossover_rate = self.de_crossover_rate * (1 - fitness_variance) # higher variance, lower crossover\n        else:\n            adaptive_mutation_factor = self.de_mutation_factor\n            adaptive_crossover_rate = self.de_crossover_rate\n\n        for i in range(self.pop_size):\n            if self.cluster_assignments[i] != cluster_id:\n                continue\n\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            a, b, c = population[idxs]\n            mutant = a + adaptive_mutation_factor * (b - c)\n            mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n            trial = np.copy(population[i])\n            for j in range(self.dim):\n                if np.random.rand() < adaptive_crossover_rate or j == np.random.randint(self.dim):\n                    trial[j] = mutant[j]\n\n            f_trial = func(trial)\n            self.budget -= 1\n            if f_trial < fitness[i]:\n                # Update success rate\n                self.success_history[i, self.success_index[i]] = 1\n                self.success_index[i] = (self.success_index[i] + 1) % self.success_rate_memory\n                fitness[i] = f_trial\n                population[i] = trial\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n            else:\n                self.success_history[i, self.success_index[i]] = 0\n                self.success_index[i] = (self.success_index[i] + 1) % self.success_rate_memory\n\n        return population, fitness\n\n    def local_search(self, individual, func):\n        # Small random perturbation\n        perturbation = np.random.normal(0, 0.1, size=self.dim)\n        trial = individual + perturbation\n        trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n        f_trial = func(trial)\n        self.budget -= 1\n        return trial, f_trial\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        while self.budget > 0:\n            # SOM Clustering and Update\n            for i in range(self.pop_size):\n                winner_coords = self.find_closest_node(population[i])\n                self.update_som(population[i], winner_coords)\n                self.cluster_assignments[i] = winner_coords[0] * self.som_grid_size + winner_coords[1]\n\n            # Iterate through clusters\n            for cluster_id in range(self.som_grid_size * self.som_grid_size):\n                # Apply Differential Evolution to each cluster\n                population, fitness = self.differential_evolution(population, fitness, func, cluster_id)\n\n            # Local Search with probability proportional to individual success rate\n            for i in range(self.pop_size):\n                self.success_rates[i] = np.mean(self.success_history[i])\n                if np.random.rand() < self.local_search_prob * (0.1 + self.success_rates[i]): # Ensure at least 0.1 probability\n                    trial, f_trial = self.local_search(population[i], func)\n                    if f_trial < fitness[i]:\n                        fitness[i] = f_trial\n                        population[i] = trial\n                        if f_trial < self.f_opt:\n                            self.f_opt = f_trial\n                            self.x_opt = trial\n\n\n            # Adaptive Parameter Control (Decay)\n            self.sigma = max(self.min_sigma, self.sigma * self.sigma_decay)\n            self.learning_rate *= self.lr_decay\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveSOM_DE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["df153811-89fa-41a6-aacf-decb5834d88c"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "387aaea1-3cae-49da-8787-cc4e70f036a2", "fitness": 0.26134730677441753, "name": "EnhancedAdaptiveDE", "description": "Enhanced Adaptive DE with probabilistic archive usage, orthogonal crossover, and self-adaptive population size for improved exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, F_initial=0.5, Cr_initial=0.9, archive_size=5, pop_scaling=4):\n        self.budget = budget\n        self.dim = dim\n        self.F = F_initial\n        self.Cr = Cr_initial\n        self.pop_scaling = pop_scaling # Scales population size\n        self.pop_size = int(self.pop_scaling + 3 * np.log(dim))\n        self.population = None\n        self.fitness = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.F_min = 0.1\n        self.F_max = 1.0\n        self.Cr_min = 0.1\n        self.Cr_max = 0.9\n        self.archive = []\n        self.archive_size = archive_size\n        self.success_F = []\n        self.success_Cr = []\n        self.memory_size = 10\n        self.memory_F = np.full(self.memory_size, F_initial)\n        self.memory_Cr = np.full(self.memory_size, Cr_initial)\n        self.memory_idx = 0\n        self.p_archive = 0.1 # Probability of using archive individual\n\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.population, axis=0)\n        distances = np.linalg.norm(self.population - mean_position, axis=1)\n        diversity = np.mean(distances)\n        return diversity\n\n    def adjust_parameters(self):\n        # Lehmer mean for F and Cr adaptation\n        if self.success_F:\n            self.F = np.sum(np.array(self.success_F)**2) / np.sum(self.success_F) if np.sum(self.success_F) > 0 else 0.5\n            self.Cr = np.mean(self.success_Cr)\n        else:\n            self.F = self.memory_F[self.memory_idx]\n            self.Cr = self.memory_Cr[self.memory_idx]\n\n        self.F = np.clip(self.F, self.F_min, self.F_max)\n        self.Cr = np.clip(self.Cr, self.Cr_min, self.Cr_max)\n\n    def mutate(self, pop, i):\n        indices = [idx for idx in range(len(pop)) if idx != i]\n        if len(self.archive) > 0 and np.random.rand() < self.p_archive:\n            arch_ind = np.random.randint(0, len(self.archive))\n            pop_indices = np.random.choice(indices, 2, replace=False)\n            a = self.archive[arch_ind]\n            b, c = pop[pop_indices[0]], pop[pop_indices[1]]\n        else:\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n        \n        mutation = a + self.F * (b - c)\n        return mutation\n\n    def mirrored_boundary_handling(self, mutant):\n        for i in range(self.dim):\n            if mutant[i] < self.bounds_lb[i]:\n                mutant[i] = self.bounds_lb[i] + (self.bounds_lb[i] - mutant[i])\n            elif mutant[i] > self.bounds_ub[i]:\n                mutant[i] = self.bounds_ub[i] - (mutant[i] - self.bounds_ub[i])\n        return mutant\n    \n    def orthogonal_crossover(self, mutant, target):\n        # Orthogonal Crossover\n        num_groups = 2\n        group_size = self.dim // num_groups\n        trial = target.copy()\n\n        for g in range(num_groups):\n            start_index = g * group_size\n            end_index = (g + 1) * group_size if g < num_groups - 1 else self.dim\n            if np.random.rand() < self.Cr: # Apply with Cr probability\n                trial[start_index:end_index] = mutant[start_index:end_index]\n        return trial\n\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n        fevals = self.pop_size\n        self.success_F = []\n        self.success_Cr = []\n\n\n        while fevals < self.budget:\n            self.adjust_parameters()\n            self.success_F = []\n            self.success_Cr = []\n\n            for i in range(self.pop_size):\n                mutant = self.mutate(self.population, i)\n                mutant = self.mirrored_boundary_handling(mutant)\n                trial = self.orthogonal_crossover(mutant, self.population[i])\n                \n                f = func(trial)\n                fevals += 1\n\n                if f < self.fitness[i]:\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        replace_index = np.random.randint(0, self.archive_size)\n                        self.archive[replace_index] = self.population[i].copy()\n\n                    self.fitness[i] = f\n                    self.population[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                \n                if fevals >= self.budget:\n                    break\n            \n            # Update Memory\n            if self.success_F:\n                self.memory_F[self.memory_idx] = np.mean(self.success_F)\n                self.memory_Cr[self.memory_idx] = np.mean(self.success_Cr)\n                self.memory_idx = (self.memory_idx + 1) % self.memory_size\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm EnhancedAdaptiveDE scored 0.261 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a7a98c22-f00c-40c5-896b-b47ae99f557c"], "operator": null, "metadata": {"aucs": [0.14596000177578172, 0.21635443074215366, 0.23833239899735925, 0.22000190498663685, 0.3433743757695815, 0.15995197228144842, 0.28312418598979705, 0.18745661107598643, 0.2615905039266546, 0.15863125306096082, 0.3010903822843054, 0.29989499600856284, 0.25253542801667206, 0.23742089352181506, 0.5710846452975262, 0.3001643029588469, 0.23252069064976844, 0.16907108198111276, 0.20680433669520848, 0.44158173946817203]}}
{"id": "f10b3942-d168-44c6-861d-cb3da11fea95", "fitness": 0.5989666432335431, "name": "ImprovedSHADE_Diversity", "description": "Introducing a diversity metric based on the population's Euclidean distance from its centroid to dynamically adjust mutation rate and archive acceptance probability.", "code": "import numpy as np\n\nclass ImprovedSHADE_Diversity:\n    def __init__(self, budget=10000, dim=10, memory_size=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = int(4 + 3 * np.log(budget))\n        self.memory_size = memory_size\n        self.memory_F = np.full(memory_size, 0.5)\n        self.memory_CR = np.full(memory_size, 0.5)\n        self.p = 0.1  # Percentage of top individuals for p-best selection\n        self.population = None\n        self.fitness = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.archive = [] # For storing inferior solutions\n        self.archive_size = self.pop_size  # Archive size, can be adjusted\n\n        self.memory_index = 0\n        self.sf_M = []\n        self.scr_M = []\n        self.diversity_threshold = 0.1 # Parameter to control diversity adaptation\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n\n    def calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        distances = np.linalg.norm(self.population - centroid, axis=1)\n        diversity = np.mean(distances)\n        return diversity\n\n    def mutate(self, pop, i, F, Cr, diversity):\n        p_best_size = max(int(self.p * len(pop)), 1)\n        \n        # Tournament selection for p-best\n        indices = np.argsort(self.fitness)[:p_best_size]\n        p_best_idx = np.random.choice(indices)\n        p_best = pop[p_best_idx]\n\n        indices = [idx for idx in range(len(pop)) if idx != i]\n        if len(indices) < 2:\n            return pop[i]  # If only one element, no mutation\n\n        r1, r2 = np.random.choice(indices, 2, replace=False)\n\n        # Dynamic Jitter based on diversity\n        jitter_scale = 0.05 * (1 + diversity) # Increased jitter when diversity is high\n        jitter = np.random.uniform(-jitter_scale, jitter_scale, size=self.dim)\n        mutant = p_best + F * (pop[r1] - pop[r2] + jitter)\n\n        return mutant\n\n\n    def crossover(self, mutant, target, Cr):\n        cross_points = np.random.rand(self.dim) < Cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n    \n    def update_archive(self, individual, success, diversity):\n        if len(self.archive) < self.archive_size:\n            self.archive.append(individual)\n        else:\n            # Adaptive archive update based on diversity: replace less frequently when diversity is low\n            idx = np.random.randint(0, len(self.archive))\n            \n            # Increased probability of replacing with successful trials when diversity is high\n            archive_acceptance_prob = 0.1 + 0.9 * diversity # Ranges between 0.1 and 1\n            \n            if success and np.random.rand() < archive_acceptance_prob:\n                self.archive[idx] = individual\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n        fevals = self.pop_size\n        \n        self.sf_M = []\n        self.scr_M = []\n        weights = []\n        \n        while fevals < self.budget:\n            sf_k = []\n            scr_k = []\n            weights = []\n\n            diversity = self.calculate_diversity()\n            \n            for i in range(self.pop_size):\n                # Sample F and CR from memory\n                mem_idx = np.random.randint(0, self.memory_size)\n                F = self.memory_F[mem_idx]\n                Cr = self.memory_CR[mem_idx]\n\n                # Truncate and Reflect F\n                if F > 1:\n                    F = 1\n                if F <= 0:\n                    F = 0.000001\n                    \n\n                mutant = self.mutate(self.population, i, F, Cr, diversity)\n                mutant = np.clip(mutant, self.bounds_lb, self.bounds_ub)\n                trial = self.crossover(mutant, self.population[i], Cr)\n                \n                f = func(trial)\n                fevals += 1\n\n                if f < self.fitness[i]:\n                    delta = abs(f - self.fitness[i])\n                    sf_k.append(F)\n                    scr_k.append(Cr)\n                    weights.append(delta)\n                    \n                    self.update_archive(self.population[i], True, diversity)\n\n                    self.fitness[i] = f\n                    self.population[i] = trial\n                    \n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                else:\n                     self.update_archive(trial, False, diversity) # trial wasn't successful\n\n                if fevals >= self.budget:\n                    break\n            \n            # Update memory using Lehmer mean\n            if sf_k:\n                weights = np.array(weights) / np.sum(weights)\n                sum_weights_sfk = np.sum(weights * np.array(sf_k)**2)\n                sum_weights = np.sum(weights * np.array(sf_k))\n                \n                if sum_weights > 0:\n                    mean_F = sum_weights_sfk / sum_weights\n                else:\n                    mean_F = 0.5\n                \n                mean_CR = np.mean(scr_k)\n                \n                self.memory_F[self.memory_index] = mean_F\n                self.memory_CR[self.memory_index] = mean_CR\n                self.memory_index = (self.memory_index + 1) % self.memory_size\n            \n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm ImprovedSHADE_Diversity scored 0.599 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["89d0b88e-b933-48a0-be37-77f0ad40f309"], "operator": null, "metadata": {"aucs": [0.31008265005646796, 0.5306333340219493, 0.5426504367069012, 0.9478181041203124, 0.5910476137227065, 0.6445146751908102, 0.4392026406870122, 0.4856193508604151, 0.551569100709494, 0.47929048842602784, 0.9269406910850825, 0.9969420275063526, 0.3454843763899891, 0.5787229496421658, 0.9272924320365271, 0.6620466422133163, 0.4962087701301001, 0.7460043495201361, 0.24077055340160936, 0.53649167824349]}}
{"id": "823ee2e9-0fd2-4a3b-976d-d6df61041486", "fitness": 0.7275192973668169, "name": "ImprovedSHADE_Ortho_AdaptPop", "description": "Improved SHADE with orthogonal crossover, adaptive population sizing, and a combined archive strategy for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass ImprovedSHADE_Ortho_AdaptPop:\n    def __init__(self, budget=10000, dim=10, memory_size=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = int(4 + 3 * np.log(budget))  # Initial population size\n        self.memory_size = memory_size\n        self.memory_F = np.full(memory_size, 0.5)\n        self.memory_CR = np.full(memory_size, 0.5)\n        self.p = 0.1  # Percentage of top individuals for p-best selection\n        self.population = None\n        self.fitness = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.archive = []  # For storing inferior solutions\n        self.archive_size = self.pop_size  # Archive size, can be adjusted\n        self.memory_index = 0\n        self.sf_M = []\n        self.scr_M = []\n        self.fevals = 0  # Track function evaluations\n        self.min_pop_size = 4  # Minimum population size\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.fevals += self.pop_size\n\n    def mutate(self, pop, i, F, Cr, archive):\n        p_best_size = max(int(self.p * len(pop)), 1)\n\n        # Tournament selection for p-best\n        indices = np.argsort(self.fitness)[:p_best_size]\n        p_best_idx = np.random.choice(indices)\n        p_best = pop[p_best_idx]\n\n        indices = [idx for idx in range(len(pop)) if idx != i]\n        if len(indices) == 0:\n            return pop[i]\n\n        r1 = np.random.choice(indices)\n\n        # Choose r2 from current pop or archive\n        if len(archive) > 0 and np.random.rand() < 0.5:  # 50% chance to use archive\n            r2_idx = np.random.randint(0, len(archive))\n            r2 = archive[r2_idx]\n        else:\n            remaining_indices = [idx for idx in indices if idx != r1]\n            if len(remaining_indices) == 0:\n                r2 = pop[r1]\n            else:\n                r2_idx = np.random.choice(remaining_indices)\n                r2 = pop[r2_idx]\n\n        mutant = p_best + F * (pop[r1] - r2)\n        return mutant\n\n    def orthogonal_crossover(self, mutant, target):\n        # Generate an orthogonal array (example using a simple Hadamard matrix)\n        H = np.array([[1, 1], [1, -1]])\n\n        # Ensure dimensions are compatible by repeating or truncating\n        if self.dim > H.shape[1]:\n            H_repeated = np.tile(H, (1, int(np.ceil(self.dim / H.shape[1]))))\n            H_used = H_repeated[:, :self.dim]\n        else:\n            H_used = H[:, :self.dim]\n\n        trial = np.copy(target)\n\n        for j in range(self.dim):\n            if H_used[0, j] == H_used[1, j]:\n                continue  # No change\n            else:\n                trial[j] = mutant[j]\n\n        return trial\n\n\n    def crossover(self, mutant, target, Cr):\n        cross_points = np.random.rand(self.dim) < Cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def update_archive(self, individual, archive):\n        archive.append(individual)\n        return archive\n\n    def reduce_population(self):\n        if self.pop_size > self.min_pop_size:\n            sorted_indices = np.argsort(self.fitness)[::-1]  # Worst to best\n            reduction_amount = max(1, self.pop_size // 10)  # Reduce by 10%, but at least 1\n            removed_indices = sorted_indices[:reduction_amount]\n\n            self.population = np.delete(self.population, removed_indices, axis=0)\n            self.fitness = np.delete(self.fitness, removed_indices)\n            self.pop_size = len(self.population)\n            print(f\"Population reduced to {self.pop_size}\")\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n        self.archive = []\n        \n        self.sf_M = []\n        self.scr_M = []\n        weights = []\n\n        while self.fevals < self.budget:\n            sf_k = []\n            scr_k = []\n            weights = []\n\n            for i in range(self.pop_size):\n                # Sample F and CR from memory\n                mem_idx = np.random.randint(0, self.memory_size)\n                F = self.memory_F[mem_idx]\n                Cr = self.memory_CR[mem_idx]\n\n                # Truncate and Reflect F\n                if F > 1:\n                    F = 1\n                if F <= 0:\n                    F = 0.000001\n\n                mutant = self.mutate(self.population, i, F, Cr, self.archive)\n                mutant = np.clip(mutant, self.bounds_lb, self.bounds_ub)\n\n                # Use orthogonal crossover with a probability or with the regular crossover\n                if np.random.rand() < 0.5:  # 50% chance to use orthogonal crossover\n                    trial = self.orthogonal_crossover(mutant, self.population[i])\n                else:\n                    trial = self.crossover(mutant, self.population[i], Cr)\n\n                f = func(trial)\n                self.fevals += 1\n\n                if f < self.fitness[i]:\n                    delta = abs(f - self.fitness[i])\n                    sf_k.append(F)\n                    scr_k.append(Cr)\n                    weights.append(delta)\n\n                    self.archive = self.update_archive(self.population[i], self.archive)\n                    if len(self.archive) > self.archive_size:\n                        self.archive.pop(np.random.randint(0, len(self.archive)))  # Keep archive size constant\n\n                    self.fitness[i] = f\n                    self.population[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                else:\n                    self.archive = self.update_archive(trial, self.archive)  # Add even unsuccessful trials\n                    if len(self.archive) > self.archive_size:\n                        self.archive.pop(np.random.randint(0, len(self.archive)))\n\n                if self.fevals >= self.budget:\n                    break\n\n            # Update memory using Lehmer mean\n            if sf_k:\n                weights = np.array(weights) / np.sum(weights)\n                sum_weights_sfk = np.sum(weights * np.array(sf_k) ** 2)\n                sum_weights = np.sum(weights * np.array(sf_k))\n\n                if sum_weights > 0:\n                    mean_F = sum_weights_sfk / sum_weights\n                else:\n                    mean_F = 0.5\n\n                mean_CR = np.mean(scr_k)\n\n                self.memory_F[self.memory_index] = mean_F\n                self.memory_CR[self.memory_index] = mean_CR\n                self.memory_index = (self.memory_index + 1) % self.memory_size\n\n            # Adaptive population size reduction\n            if self.fevals > self.budget // 2:  # Start reducing after half of the budget is spent\n                self.reduce_population()\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm ImprovedSHADE_Ortho_AdaptPop scored 0.728 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["89d0b88e-b933-48a0-be37-77f0ad40f309"], "operator": null, "metadata": {"aucs": [0.30917210428285946, 0.5421373646854195, 0.8284489364681591, 0.9123055982032002, 0.8207500868214315, 0.873041335504429, 0.6828941659316902, 0.7776796265500443, 0.8471597435616762, 0.7761802740202944, 0.8614395927840668, 0.9983660974054537, 0.2855371488683016, 0.8038953143683275, 0.9433112202310888, 0.8626462101818844, 0.6267889317179469, 0.8903426363586048, 0.3814151761599138, 0.5268743832315439]}}
{"id": "53b07a3b-ef02-47cf-ad91-f65511f57f19", "fitness": -Infinity, "name": "CMA_ES_DE", "description": "A population-based algorithm that uses a combination of differential evolution, covariance matrix adaptation, and a restart mechanism to balance exploration and exploitation.", "code": "import numpy as np\n\nclass CMA_ES_DE:\n    def __init__(self, budget=10000, dim=10, pop_size=None, initial_sigma=0.1, restart_trigger=1e-12):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.initial_sigma = initial_sigma\n        self.mean = None\n        self.sigma = initial_sigma\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.eigenspace_rotation = None\n        self.eigenvalues = None\n        self.restart_trigger = restart_trigger\n        self.mu = self.pop_size // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cc = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.restart_count = 0\n\n    def initialize(self):\n        self.mean = np.random.uniform(-2.5, 2.5, size=self.dim)  # Initialize within bounds\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.eigenspace_rotation = None\n        self.eigenvalues = None\n\n    def sample_population(self):\n        z = np.random.randn(self.dim, self.pop_size)\n        if self.eigenspace_rotation is None:\n            y = self.sigma * z\n        else:\n            y = self.sigma * np.dot(self.eigenspace_rotation, np.diag(np.sqrt(self.eigenvalues)) @ z)\n        return self.mean[:, np.newaxis] + y\n\n    def update_distribution(self, population, fitness_values):\n        indices = np.argsort(fitness_values)\n        best_indices = indices[:self.mu]\n        \n        y = population[:, best_indices] - self.mean[:, np.newaxis]\n        \n        delta_mean = np.sum(self.weights * y, axis=1)\n        self.mean += delta_mean\n\n        self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * delta_mean / self.sigma\n        hsig = np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**2) < (1.4 + 2 / (self.dim + 1))\n\n        self.pc = (1 - self.cc) * self.pc + hsig * np.sqrt(self.cc * (2 - self.cc) * self.mueff) * delta_mean\n        \n        C_temp = np.dot(y, np.diag(self.weights) @ y.T)\n        self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :]) + self.cmu * C_temp\n\n        self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / np.sqrt(self.dim) - 1))\n\n        self.eigenvalues, self.eigenspace_rotation = np.linalg.eigh(self.C)\n        self.eigenvalues = np.maximum(self.eigenvalues, 1e-15)\n        \n    def differential_mutation(self, population, fitness_values, F=0.8, Cr=0.7):\n        new_population = np.copy(population)\n        for i in range(population.shape[1]):\n            idxs = [idx for idx in range(population.shape[1]) if idx != i]\n            a, b, c = np.random.choice(idxs, 3, replace=False)\n            \n            mutant = population[:, a] + F * (population[:, b] - population[:, c])\n\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < Cr or j == j_rand:\n                    new_population[j, i] = mutant[j]\n                else:\n                    new_population[j, i] = population[j, i]\n\n            new_population[:, i] = np.clip(new_population[:, i], -5, 5)\n        return new_population\n\n    def __call__(self, func):\n        self.initialize()\n        \n        while self.budget > 0:\n            population = self.sample_population()\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            fitness_values = np.array([func(x) for x in population.T])\n            self.budget -= self.pop_size\n\n            best_index = np.argmin(fitness_values)\n            if fitness_values[best_index] < self.f_opt:\n                self.f_opt = fitness_values[best_index]\n                self.x_opt = population[:, best_index]\n                \n            self.update_distribution(population, fitness_values)\n            \n            #Restart Mechanism\n            if np.min(self.eigenvalues) > self.restart_trigger or self.sigma < 1e-10:\n                self.restart_count += 1\n                self.initialize()\n\n            # Apply Differential Evolution Mutation\n            if np.random.rand() < 0.3:\n                mutated_population = self.differential_mutation(population, fitness_values)\n                mutated_fitness_values = np.array([func(x) for x in mutated_population.T])\n                self.budget -= self.pop_size\n\n                for i in range(self.pop_size):\n                    if mutated_fitness_values[i] < fitness_values[i]:\n                        population[:, i] = mutated_population[:, i]\n                        fitness_values[i] = mutated_fitness_values[i]\n\n                    if fitness_values[i] < self.f_opt:\n                        self.f_opt = fitness_values[i]\n                        self.x_opt = population[:, i]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "An exception occurred: operands could not be broadcast together with shapes (2,6) (2,) (2,) .", "error": "", "parent_ids": ["377b024f-0528-4b3d-929d-7971321f3eb7"], "operator": null, "metadata": {}}
{"id": "8e066679-5596-46e3-9e53-c396866f5d43", "fitness": 0.6481391764390836, "name": "AdaptiveDE_Archive_Mirrored_LSHADE", "description": "Adaptively adjusts F and Cr based on the Lehmer mean of successful values, and incorporates a simplified L-SHADE memory for improved exploration.", "code": "import numpy as np\n\nclass AdaptiveDE_Archive_Mirrored_LSHADE:\n    def __init__(self, budget=10000, dim=10, archive_size=5, memory_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = int(4 + 3 * np.log(budget))  # Population size that scales with budget\n        self.population = None\n        self.fitness = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.archive = []  # Archive for storing removed individuals\n        self.archive_size = archive_size\n        self.memory_size = memory_size\n        self.memory_F = np.full(self.memory_size, 0.5)\n        self.memory_Cr = np.full(self.memory_size, 0.5)\n        self.sf = []\n        self.scr = []\n        self.p = 0.1  # Probability of using archive individual\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n\n    def mutate(self, pop, i, current_generation):\n        memory_index = current_generation % self.memory_size\n        Cr = self.memory_Cr[memory_index]\n        F = self.memory_F[memory_index]\n\n        indices = [idx for idx in range(len(pop)) if idx != i]\n        if len(self.archive) > 0 and np.random.rand() < self.p:\n            arch_ind = np.random.randint(0, len(self.archive))\n            pop_indices = np.random.choice(indices, 2, replace=False)\n            a = self.archive[arch_ind]\n            b, c = pop[pop_indices[0]], pop[pop_indices[1]]\n        else:\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n        \n        mutation = a + F * (b - c)\n        return mutation, Cr, F\n\n    def mirrored_boundary_handling(self, mutant):\n        # Apply mirrored boundary handling\n        for i in range(self.dim):\n            if mutant[i] < self.bounds_lb[i]:\n                mutant[i] = self.bounds_lb[i] + (self.bounds_lb[i] - mutant[i])\n            elif mutant[i] > self.bounds_ub[i]:\n                mutant[i] = self.bounds_ub[i] - (mutant[i] - self.bounds_ub[i])\n        return mutant\n\n    def crossover(self, mutant, target, Cr):\n        cross_points = np.random.rand(self.dim) < Cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def update_memory(self):\n        if self.sf:\n            self.sf = np.array(self.sf)\n            self.scr = np.array(self.scr)\n\n            # Lehmer mean for F\n            sum_sf_squared = np.sum(self.sf ** 2)\n            sum_sf = np.sum(self.sf)\n            if sum_sf > 0:\n                lehmer_mean_F = sum_sf_squared / sum_sf\n            else:\n                lehmer_mean_F = 0.5  # Default value if no successful F values\n\n            # Lehmer mean for Cr\n            lehmer_mean_Cr = np.mean(self.scr)\n\n            memory_index = self.generation % self.memory_size\n            self.memory_F[memory_index] = lehmer_mean_F\n            self.memory_Cr[memory_index] = lehmer_mean_Cr\n\n        self.sf = []\n        self.scr = []\n\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n        fevals = self.pop_size\n        self.generation = 0\n        self.sf = []\n        self.scr = []\n\n\n        while fevals < self.budget:\n            self.generation += 1\n            for i in range(self.pop_size):\n                mutant, Cr, F = self.mutate(self.population, i, self.generation)\n                mutant = self.mirrored_boundary_handling(mutant)  # Mirrored boundary handling instead of clipping\n                trial = self.crossover(mutant, self.population[i], Cr)\n                \n                f = func(trial)\n                fevals += 1\n\n                if f < self.fitness[i]:\n                    # Archive the replaced individual\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        # Replace a random element in the archive\n                        replace_index = np.random.randint(0, self.archive_size)\n                        self.archive[replace_index] = self.population[i].copy()\n\n                    self.sf.append(F)\n                    self.scr.append(Cr)\n                    self.fitness[i] = f\n                    self.population[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                \n                if fevals >= self.budget:\n                    break\n            self.update_memory()\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDE_Archive_Mirrored_LSHADE scored 0.648 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a7a98c22-f00c-40c5-896b-b47ae99f557c"], "operator": null, "metadata": {"aucs": [0.2195452446780013, 0.36083445259448244, 0.6486690879084986, 0.8628652775788217, 0.776578120494418, 0.815062467664374, 0.6555585676870086, 0.6301928736685942, 0.7675494820877834, 0.6474574797865872, 0.8095617125964296, 0.9904180588801528, 0.2815835041207232, 0.7119175500507, 0.8758059699140136, 0.8178337851572458, 0.5059187296586324, 0.8585439739300547, 0.21564083121999578, 0.5112463591051547]}}
{"id": "66b89da4-7fba-4a01-ab52-0bcf50901fdf", "fitness": -Infinity, "name": "AdaptiveSHADE_DualArchive_EntropyCR", "description": "An enhanced SHADE variant utilizing a dual archive system and a self-adapting crossover rate based on population entropy.", "code": "import numpy as np\n\nclass AdaptiveSHADE_DualArchive_EntropyCR:\n    def __init__(self, budget=10000, dim=10, memory_size=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = int(4 + 3 * np.log(budget))\n        self.memory_size = memory_size\n        self.memory_F = np.full(memory_size, 0.5)\n        self.memory_CR = np.full(memory_size, 0.5)\n        self.p = 0.1  # Percentage of top individuals for p-best selection\n        self.population = None\n        self.fitness = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.archive1 = []  # For storing inferior solutions (exploration)\n        self.archive2 = []  # For storing promising solutions (exploitation)\n        self.archive_size = self.pop_size  # Archive size\n        self.memory_index = 0\n        self.sf_M = []\n        self.scr_M = []\n        self.entropy_threshold = 0.5\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n\n    def calculate_entropy(self):\n        # Calculate entropy based on the distribution of individuals in the search space\n        hist, _ = np.histogramdd(self.population, bins=10, range=[(self.bounds_lb, self.bounds_ub)] * self.dim)\n        probabilities = hist / np.sum(hist)\n        probabilities = probabilities[probabilities > 0]  # Avoid log(0)\n        entropy = -np.sum(probabilities * np.log2(probabilities))\n        return entropy\n\n    def mutate(self, pop, i, F):\n        p_best_size = max(int(self.p * len(pop)), 1)\n        indices = np.argsort(self.fitness)[:p_best_size]\n        p_best_idx = np.random.choice(indices)\n        p_best = pop[p_best_idx]\n\n        indices = [idx for idx in range(len(pop)) if idx != i]\n        if len(indices) < 2:\n            return pop[i]\n        r1, r2 = np.random.choice(indices, 2, replace=False)\n\n        mutant = p_best + F * (pop[r1] - pop[r2])\n        return mutant\n\n    def crossover(self, mutant, target, Cr):\n        cross_points = np.random.rand(self.dim) < Cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def update_archive(self, individual, fitness, archive):\n        if len(archive) < self.archive_size:\n            archive.append((individual, fitness))\n        else:\n            worst_index = np.argmax([item[1] for item in archive])  # Find the worst fitness\n            if fitness < archive[worst_index][1]:  # Replace if better\n                archive[worst_index] = (individual, fitness)\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n        fevals = self.pop_size\n\n        self.sf_M = []\n        self.scr_M = []\n\n        while fevals < self.budget:\n            sf_k = []\n            scr_k = []\n            weights = []\n            entropy = self.calculate_entropy()\n            adaptive_Cr = self.memory_CR[self.memory_index]\n\n            # Adapt CR based on entropy: higher entropy -> higher Cr, lower entropy -> lower Cr\n            adaptive_Cr = np.clip(adaptive_Cr + 0.1 * (entropy - self.entropy_threshold), 0.1, 0.9)\n\n            for i in range(self.pop_size):\n                mem_idx = np.random.randint(0, self.memory_size)\n                F = self.memory_F[mem_idx]\n                Cr = adaptive_Cr\n\n                if F > 1:\n                    F = 1\n                if F <= 0:\n                    F = 0.000001\n\n                mutant = self.mutate(self.population, i, F)\n                mutant = np.clip(mutant, self.bounds_lb, self.bounds_ub)\n                trial = self.crossover(mutant, self.population[i], Cr)\n\n                f = func(trial)\n                fevals += 1\n\n                if f < self.fitness[i]:\n                    delta = abs(f - self.fitness[i])\n                    sf_k.append(F)\n                    scr_k.append(Cr)\n                    weights.append(delta)\n\n                    self.update_archive(self.population[i], self.fitness[i], self.archive1)  # inferior goes to archive 1\n                    self.update_archive(trial, f, self.archive2) # Promising trial goes to archive 2\n\n                    self.fitness[i] = f\n                    self.population[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                else:\n                    self.update_archive(trial, f, self.archive1)  # unsuccessful goes to archive 1\n\n                if fevals >= self.budget:\n                    break\n\n            if sf_k:\n                weights = np.array(weights) / np.sum(weights)\n                sum_weights_sfk = np.sum(weights * np.array(sf_k)**2)\n                sum_weights = np.sum(weights * np.array(sf_k))\n                if sum_weights > 0:\n                    mean_F = sum_weights_sfk / sum_weights\n                else:\n                    mean_F = 0.5\n                mean_CR = np.mean(scr_k)\n\n                self.memory_F[self.memory_index] = mean_F\n                self.memory_CR[self.memory_index] = mean_CR\n                self.memory_index = (self.memory_index + 1) % self.memory_size\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "An exception occurred: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all().", "error": "", "parent_ids": ["f10b3942-d168-44c6-861d-cb3da11fea95"], "operator": null, "metadata": {}}
{"id": "a31483a3-b107-4c7d-a085-a85a2ed75bf1", "fitness": 0.0, "name": "EnhancedAdaptiveDE", "description": "Introduce a tournament selection to enhance the exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, F_initial=0.5, Cr_initial=0.9, archive_size=5, pop_scaling=4, tournament_size=3):\n        self.budget = budget\n        self.dim = dim\n        self.F = F_initial\n        self.Cr = Cr_initial\n        self.pop_scaling = pop_scaling # Scales population size\n        self.pop_size = int(self.pop_scaling + 3 * np.log(dim))\n        self.population = None\n        self.fitness = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.F_min = 0.1\n        self.F_max = 1.0\n        self.Cr_min = 0.1\n        self.Cr_max = 0.9\n        self.archive = []\n        self.archive_size = archive_size\n        self.success_F = []\n        self.success_Cr = []\n        self.memory_size = 10\n        self.memory_F = np.full(self.memory_size, F_initial)\n        self.memory_Cr = np.full(self.memory_size, Cr_initial)\n        self.memory_idx = 0\n        self.p_archive = 0.1 # Probability of using archive individual\n        self.tournament_size = tournament_size\n\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.population, axis=0)\n        distances = np.linalg.norm(self.population - mean_position, axis=1)\n        diversity = np.mean(distances)\n        return diversity\n\n    def adjust_parameters(self):\n        # Lehmer mean for F and Cr adaptation\n        if self.success_F:\n            self.F = np.sum(np.array(self.success_F)**2) / np.sum(self.success_F) if np.sum(self.success_F) > 0 else 0.5\n            self.Cr = np.mean(self.success_Cr)\n        else:\n            self.F = self.memory_F[self.memory_idx]\n            self.Cr = self.memory_Cr[self.memory_idx]\n\n        self.F = np.clip(self.F, self.F_min, self.F_max)\n        self.Cr = np.clip(self.Cr, self.Cr_min, self.Cr_max)\n\n    def mutate(self, pop, i):\n        indices = [idx for idx in range(len(pop)) if idx != i]\n        if len(self.archive) > 0 and np.random.rand() < self.p_archive:\n            arch_ind = np.random.randint(0, len(self.archive))\n            pop_indices = np.random.choice(indices, 2, replace=False)\n            a = self.archive[arch_ind]\n            b, c = pop[pop_indices[0]], pop[pop_indices[1]]\n        else:\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n        \n        mutation = a + self.F * (b - c)\n        return mutation\n\n    def mirrored_boundary_handling(self, mutant):\n        for i in range(self.dim):\n            if mutant[i] < self.bounds_lb[i]:\n                mutant[i] = self.bounds_lb[i] + (self.bounds_lb[i] - mutant[i])\n            elif mutant[i] > self.bounds_ub[i]:\n                mutant[i] = self.bounds_ub[i] - (mutant[i] - self.bounds_ub[i])\n        return mutant\n    \n    def orthogonal_crossover(self, mutant, target):\n        # Orthogonal Crossover\n        num_groups = 2\n        group_size = self.dim // num_groups\n        trial = target.copy()\n\n        for g in range(num_groups):\n            start_index = g * group_size\n            end_index = (g + 1) * group_size if g < num_groups - 1 else self.dim\n            if np.random.rand() < self.Cr: # Apply with Cr probability\n                trial[start_index:end_index] = mutant[start_index:end_index]\n        return trial\n\n    def tournament_selection(self):\n        indices = np.random.choice(self.pop_size, self.tournament_size, replace=False)\n        winners = self.population[indices]\n        fitnesses = self.fitness[indices]\n        return winners[np.argmin(fitnesses)]\n\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n        fevals = self.pop_size\n        self.success_F = []\n        self.success_Cr = []\n\n\n        while fevals < self.budget:\n            self.adjust_parameters()\n            self.success_F = []\n            self.success_Cr = []\n\n            for i in range(self.pop_size):\n                mutant = self.mutate(self.population, i)\n                mutant = self.mirrored_boundary_handling(mutant)\n                trial = self.orthogonal_crossover(mutant, self.population[i])\n                \n                f = func(trial)\n                fevals += 1\n\n                if f < self.fitness[i]:\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        replace_index = np.random.randint(0, self.archive_size)\n                        self.archive[replace_index] = self.population[i].copy()\n\n                    self.fitness[i] = f\n                    self.population[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                else:\n                     # Tournament Selection: replace the current individual with a tournament winner\n                     winner = self.tournament_selection()\n                     self.population[i] = winner\n                     self.fitness[i] = func(winner)  # Re-evaluate the winner's fitness\n                     fevals += 1\n                     if self.fitness[i] < self.f_opt:\n                         self.f_opt = self.fitness[i]\n                         self.x_opt = winner\n\n\n                \n                if fevals >= self.budget:\n                    break\n            \n            # Update Memory\n            if self.success_F:\n                self.memory_F[self.memory_idx] = np.mean(self.success_F)\n                self.memory_Cr[self.memory_idx] = np.mean(self.success_Cr)\n                self.memory_idx = (self.memory_idx + 1) % self.memory_size\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm EnhancedAdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["387aaea1-3cae-49da-8787-cc4e70f036a2"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "dd0e8f55-9d42-4945-8ddd-d8e39bf0715f", "fitness": -Infinity, "name": "HybridSHADE_CMAES", "description": "A hybrid DE algorithm combining SHADE's parameter adaptation with CMA-ES's covariance matrix adaptation for enhanced exploration and exploitation in high-dimensional spaces.", "code": "import numpy as np\n\nclass HybridSHADE_CMAES:\n    def __init__(self, budget=10000, dim=10, memory_size=100, pop_size=None):\n        self.budget = budget\n        self.dim = dim\n        self.memory_size = memory_size\n        self.memory_F = np.full(memory_size, 0.5)\n        self.memory_CR = np.full(memory_size, 0.5)\n        self.p = 0.1\n        self.population = None\n        self.fitness = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.archive = []\n        self.archive_size = None  # Initialize archive size dynamically\n        self.memory_index = 0\n        self.sf_M = []\n        self.scr_M = []\n        self.fevals = 0\n\n        if pop_size is None:\n           self.pop_size = int(4 + 3 * np.log(budget))  # Adaptive population size based on budget\n        else:\n            self.pop_size = pop_size\n\n        self.archive_size = self.pop_size  # Adaptive archive size equal to population size\n        self.min_pop_size = 4 # Minimum population size\n\n        # CMA-ES parameters\n        self.mu = self.pop_size // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.m = None # Mean\n        self.C = None # Covariance matrix\n        self.sigma = 0.3 # Step-size\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + (1 / (10 * self.dim**2)))\n        self.c_sigma = (self.pop_size + 2) / (self.dim + self.pop_size + 5) # learning rate for step-size control\n        self.c_c = 4 / (self.dim + 4)\n        self.c_1 = 2 / ((self.dim + 1.3)**2 + self.mu)\n        self.c_mu = 2 * (self.mu - 2 + 1/(self.mu)) / ((self.dim + 2)**2 + self.mu)\n        self.d_sigma = 1 + 2 * max(0, np.sqrt((self.mu - 1)/(self.dim + 1)) - 1) + self.c_sigma\n\n        self.B = None\n        self.D = None\n        self.eigen_update_freq = self.pop_size * 5 # Update frequency for eigendecomposition\n\n        self.eigen_update_counter = 0 # Counter for eigenvalue update\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.fevals += self.pop_size\n        self.m = np.mean(self.population, axis=0)\n        self.C = np.eye(self.dim) # Initialize covariance matrix\n        self.B = np.eye(self.dim) # Initialize rotation matrix (eigenvectors)\n        self.D = np.ones(self.dim) # Initialize scaling (eigenvalues)\n\n    def mutate(self, pop, i, F, Cr, archive):\n        p_best_size = max(int(self.p * len(pop)), 1)\n        indices = np.argsort(self.fitness)[:p_best_size]\n        p_best_idx = np.random.choice(indices)\n        p_best = pop[p_best_idx]\n\n        indices = [idx for idx in range(len(pop)) if idx != i]\n        if len(indices) == 0:\n            return pop[i]\n\n        r1 = np.random.choice(indices)\n        if len(archive) > 0 and np.random.rand() < 0.5:\n            r2_idx = np.random.randint(0, len(archive))\n            r2 = archive[r2_idx]\n        else:\n            remaining_indices = [idx for idx in indices if idx != r1]\n            if len(remaining_indices) == 0:\n                r2 = pop[r1]\n            else:\n                r2_idx = np.random.choice(remaining_indices)\n                r2 = pop[r2_idx]\n\n        mutant = p_best + F * (pop[r1] - r2)\n        return mutant\n\n    def crossover(self, mutant, target, Cr):\n        cross_points = np.random.rand(self.dim) < Cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def update_archive(self, individual, archive):\n        archive.append(individual)\n        return archive\n\n    def reduce_population(self):\n        if self.pop_size > self.min_pop_size:\n            sorted_indices = np.argsort(self.fitness)[::-1]\n            reduction_amount = max(1, self.pop_size // 10)\n            removed_indices = sorted_indices[:reduction_amount]\n\n            self.population = np.delete(self.population, removed_indices, axis=0)\n            self.fitness = np.delete(self.fitness, removed_indices)\n            self.pop_size = len(self.population)\n            print(f\"Population reduced to {self.pop_size}\")\n            self.mu = self.pop_size // 2\n\n    def sample_with_cmaes(self):\n         z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n         y = self.B @ (self.D[:, None] * z.T)\n         x = self.m + self.sigma * y.T\n         return x\n\n    def update_cmaes(self, x):\n        x_k = np.zeros((self.dim, self.mu))\n        z_k = np.zeros((self.dim, self.mu))\n        fitness_sorted_indices = np.argsort(self.fitness)[:self.mu]\n\n        for i in range(self.mu):\n            x_k[:, i] = x[fitness_sorted_indices[i]] - self.m\n            z_k[:, i] = np.linalg.solve(self.B @ np.diag(self.D), x_k[:, i])\n\n        # Update mean\n        self.m = self.m + self.c_c * self.pc\n\n        # Update evolution path\n        self.ps = (1 - self.c_sigma) * self.ps + np.sqrt(self.c_sigma * (2 - self.c_sigma)) * (self.B @ z_k).mean(axis=1) # Mean of z_k\n\n        # Step size adaptation\n        self.sigma = self.sigma * np.exp((self.c_sigma/self.d_sigma) * (np.linalg.norm(self.ps)/self.chiN - 1))\n\n        # Covariance matrix adaptation\n        self.pc = (1 - self.c_c) * self.pc + np.sqrt(self.c_c * (2 - self.c_c)) * x_k.mean(axis=1)\n\n        C_temp = self.c_1 * (self.pc[:, None] @ self.pc[None, :])\n\n        for i in range(self.mu):\n            C_temp += self.c_mu * self.weights[i] * (x_k[:, i, None] @ x_k[:, i, None].T)\n\n        self.C = (1 - self.c_1 - self.c_mu) * self.C + C_temp\n\n        self.eigen_update_counter += 1\n        if self.eigen_update_counter >= self.eigen_update_freq:\n            self.eigen_update_counter = 0\n            try:\n                self.D, self.B = np.linalg.eigh(self.C)\n                self.D = np.sqrt(np.maximum(self.D, 1e-16))\n            except np.linalg.LinAlgError as e:\n                print(f\"Eigenvalue decomposition failed: {e}. Resetting C to identity.\")\n                self.C = np.eye(self.dim)  # Reset C to identity matrix\n                self.D = np.ones(self.dim)\n                self.B = np.eye(self.dim)\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n        self.archive = []\n        self.sf_M = []\n        self.scr_M = []\n\n        while self.fevals < self.budget:\n            sf_k = []\n            scr_k = []\n            weights = []\n\n            # DE part for exploration\n            for i in range(self.pop_size):\n                mem_idx = np.random.randint(0, self.memory_size)\n                F = self.memory_F[mem_idx]\n                Cr = self.memory_CR[mem_idx]\n\n                if F > 1:\n                    F = 1\n                if F <= 0:\n                    F = 0.000001\n\n                mutant = self.mutate(self.population, i, F, Cr, self.archive)\n                mutant = np.clip(mutant, self.bounds_lb, self.bounds_ub)\n                trial = self.crossover(mutant, self.population[i], Cr)\n\n                f = func(trial)\n                self.fevals += 1\n\n                if f < self.fitness[i]:\n                    delta = abs(f - self.fitness[i])\n                    sf_k.append(F)\n                    scr_k.append(Cr)\n                    weights.append(delta)\n\n                    self.archive = self.update_archive(self.population[i], self.archive)\n                    if len(self.archive) > self.archive_size:\n                        self.archive.pop(np.random.randint(0, len(self.archive)))\n\n                    self.fitness[i] = f\n                    self.population[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                else:\n                     self.archive = self.update_archive(trial, self.archive)\n                     if len(self.archive) > self.archive_size:\n                         self.archive.pop(np.random.randint(0, len(self.archive)))\n\n                if self.fevals >= self.budget:\n                    break\n\n            if sf_k:\n                weights = np.array(weights) / np.sum(weights)\n                sum_weights_sfk = np.sum(weights * np.array(sf_k) ** 2)\n                sum_weights = np.sum(weights * np.array(sf_k))\n\n                if sum_weights > 0:\n                    mean_F = sum_weights_sfk / sum_weights\n                else:\n                    mean_F = 0.5\n\n                mean_CR = np.mean(scr_k)\n\n                self.memory_F[self.memory_index] = mean_F\n                self.memory_CR[self.memory_index] = mean_CR\n                self.memory_index = (self.memory_index + 1) % self.memory_size\n\n            # CMA-ES Part: Exploit the best individuals to guide the search\n            x_cmaes = self.sample_with_cmaes()\n            x_cmaes = np.clip(x_cmaes, self.bounds_lb, self.bounds_ub)\n\n            fitness_cmaes = np.array([func(xi) for xi in x_cmaes])\n            self.fevals += self.pop_size\n\n            # Replace worst individuals in the population with CMA-ES sampled points\n            worst_indices = np.argsort(self.fitness)[-self.pop_size//2:]\n            self.population[worst_indices] = x_cmaes[:self.pop_size//2]\n            self.fitness[worst_indices] = fitness_cmaes[:self.pop_size//2]\n\n            if np.min(fitness_cmaes) < self.f_opt:\n                 self.f_opt = np.min(fitness_cmaes)\n                 self.x_opt = x_cmaes[np.argmin(fitness_cmaes)]\n\n            # Update CMA-ES parameters\n            self.update_cmaes(x_cmaes)\n            \n            # Adaptive population size reduction\n            if self.fevals > self.budget // 2:  # Start reducing after half of the budget is spent\n                self.reduce_population()\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "An exception occurred: shape mismatch: value array of shape (8,2) could not be broadcast to indexing result of shape (9,2).", "error": "", "parent_ids": ["823ee2e9-0fd2-4a3b-976d-d6df61041486"], "operator": null, "metadata": {}}
{"id": "eca9c1a1-6e35-4e42-b225-c1188aedbc79", "fitness": 0.4242785434207887, "name": "NoveltyRestartDE", "description": "DE with a novelty-based restart mechanism triggered by low population diversity and a toroidal boundary handling strategy.", "code": "import numpy as np\n\nclass NoveltyRestartDE:\n    def __init__(self, budget=10000, dim=10, F_initial=0.5, Cr_initial=0.9, pop_scaling=4, restart_threshold=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.F = F_initial\n        self.Cr = Cr_initial\n        self.pop_scaling = pop_scaling\n        self.pop_size = int(self.pop_scaling * dim)\n        self.population = None\n        self.fitness = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.restart_threshold = restart_threshold\n        self.best_fitness_history = []\n        self.toroidal = True\n        self.F_min = 0.1\n        self.F_max = 1.0\n        self.Cr_min = 0.1\n        self.Cr_max = 0.9\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.population, axis=0)\n        distances = np.linalg.norm(self.population - mean_position, axis=1)\n        diversity = np.std(distances)  # Using standard deviation as diversity measure\n        return diversity\n\n    def should_restart(self):\n        diversity = self.calculate_diversity()\n        return diversity < self.restart_threshold\n\n    def restart_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        print(\"Restarting population due to low diversity.\") #debug\n\n    def mutate(self, pop, i):\n        indices = [idx for idx in range(len(pop)) if idx != i]\n        a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n        mutation = a + self.F * (b - c)\n        return mutation\n\n    def toroidal_boundary_handling(self, mutant):\n        if self.toroidal:\n            for i in range(self.dim):\n                width = self.bounds_ub[i] - self.bounds_lb[i]\n                if mutant[i] < self.bounds_lb[i]:\n                    mutant[i] = self.bounds_ub[i] - (self.bounds_lb[i] - mutant[i]) % width\n                elif mutant[i] > self.bounds_ub[i]:\n                    mutant[i] = self.bounds_lb[i] + (mutant[i] - self.bounds_ub[i]) % width\n        else:\n            mutant = np.clip(mutant, self.bounds_lb, self.bounds_ub)\n        return mutant\n\n    def crossover(self, mutant, target):\n        trial = target.copy()\n        for i in range(self.dim):\n            if np.random.rand() < self.Cr:\n                trial[i] = mutant[i]\n        return trial\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n        fevals = self.pop_size\n\n        while fevals < self.budget:\n            if self.should_restart():\n                self.restart_population(func)\n                fevals += self.pop_size  # Account for re-evaluation\n                if fevals >= self.budget:\n                   break\n\n            for i in range(self.pop_size):\n                mutant = self.mutate(self.population, i)\n                mutant = self.toroidal_boundary_handling(mutant)\n                trial = self.crossover(mutant, self.population[i])\n                \n                f = func(trial)\n                fevals += 1\n\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.population[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n\n                if fevals >= self.budget:\n                    break\n        \n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm NoveltyRestartDE scored 0.424 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["387aaea1-3cae-49da-8787-cc4e70f036a2"], "operator": null, "metadata": {"aucs": [0.20810333570181527, 0.3992241069494529, 0.3908738418969193, 0.51609431600845, 0.32108062209038646, 0.3739957123626778, 0.38410710484370125, 0.3344211019312805, 0.3177663125863529, 0.26646610952628147, 0.4851931562772954, 0.9981003921824196, 0.36984490761380007, 0.3196417986417247, 0.7607843925697066, 0.43703457641918597, 0.3581965210578998, 0.4707532227065113, 0.2525011178070612, 0.5213882192428523]}}
{"id": "b5f53273-1fd3-4232-b8aa-7572ba9cd3a0", "fitness": 0.6347348549138878, "name": "AdaptiveDE_Archive_Mirrored_LSHADE_Toroidal", "description": "Adaptive Differential Evolution with a weighted Lehmer mean for parameter adaptation and a toroidal boundary handling strategy to enhance exploration.", "code": "import numpy as np\n\nclass AdaptiveDE_Archive_Mirrored_LSHADE_Toroidal:\n    def __init__(self, budget=10000, dim=10, archive_size=5, memory_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = int(4 + 3 * np.log(budget))  # Population size that scales with budget\n        self.population = None\n        self.fitness = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.archive = []  # Archive for storing removed individuals\n        self.archive_size = archive_size\n        self.memory_size = memory_size\n        self.memory_F = np.full(self.memory_size, 0.5)\n        self.memory_Cr = np.full(self.memory_size, 0.5)\n        self.sf = []\n        self.scr = []\n        self.p = 0.1  # Probability of using archive individual\n        self.w = 0.5 # Weight for Lehmer mean update\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n\n    def mutate(self, pop, i, current_generation):\n        memory_index = current_generation % self.memory_size\n        Cr = self.memory_Cr[memory_index]\n        F = self.memory_F[memory_index]\n\n        indices = [idx for idx in range(len(pop)) if idx != i]\n        if len(self.archive) > 0 and np.random.rand() < self.p:\n            arch_ind = np.random.randint(0, len(self.archive))\n            pop_indices = np.random.choice(indices, 2, replace=False)\n            a = self.archive[arch_ind]\n            b, c = pop[pop_indices[0]], pop[pop_indices[1]]\n        else:\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n        \n        mutation = a + F * (b - c)\n        return mutation, Cr, F\n\n    def toroidal_boundary_handling(self, mutant):\n        # Apply toroidal boundary handling\n        for i in range(self.dim):\n            width = self.bounds_ub[i] - self.bounds_lb[i]\n            if mutant[i] < self.bounds_lb[i]:\n                mutant[i] = self.bounds_ub[i] - (self.bounds_lb[i] - mutant[i]) % width\n            elif mutant[i] > self.bounds_ub[i]:\n                mutant[i] = self.bounds_lb[i] + (mutant[i] - self.bounds_ub[i]) % width\n        return mutant\n\n    def mirrored_boundary_handling(self, mutant):\n        # Apply mirrored boundary handling\n        for i in range(self.dim):\n            if mutant[i] < self.bounds_lb[i]:\n                mutant[i] = self.bounds_lb[i] + (self.bounds_lb[i] - mutant[i])\n            elif mutant[i] > self.bounds_ub[i]:\n                mutant[i] = self.bounds_ub[i] - (mutant[i] - self.bounds_ub[i])\n        return mutant\n\n\n    def crossover(self, mutant, target, Cr):\n        cross_points = np.random.rand(self.dim) < Cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def update_memory(self):\n        if self.sf:\n            self.sf = np.array(self.sf)\n            self.scr = np.array(self.scr)\n\n            # Lehmer mean for F\n            sum_sf_squared = np.sum(self.sf ** 2)\n            sum_sf = np.sum(self.sf)\n            if sum_sf > 0:\n                lehmer_mean_F = sum_sf_squared / sum_sf\n            else:\n                lehmer_mean_F = 0.5  # Default value if no successful F values\n\n            # Lehmer mean for Cr\n            lehmer_mean_Cr = np.mean(self.scr)\n\n            memory_index = self.generation % self.memory_size\n            self.memory_F[memory_index] = self.w * self.memory_F[memory_index] + (1 - self.w) * lehmer_mean_F\n            self.memory_Cr[memory_index] = self.w *  self.memory_Cr[memory_index] + (1 - self.w) * lehmer_mean_Cr\n\n        self.sf = []\n        self.scr = []\n\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n        fevals = self.pop_size\n        self.generation = 0\n        self.sf = []\n        self.scr = []\n\n\n        while fevals < self.budget:\n            self.generation += 1\n            for i in range(self.pop_size):\n                mutant, Cr, F = self.mutate(self.population, i, self.generation)\n                mutant = self.toroidal_boundary_handling(mutant)  # Toroidal boundary handling\n                trial = self.crossover(mutant, self.population[i], Cr)\n                \n                f = func(trial)\n                fevals += 1\n\n                if f < self.fitness[i]:\n                    # Archive the replaced individual\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        # Replace a random element in the archive\n                        replace_index = np.random.randint(0, self.archive_size)\n                        self.archive[replace_index] = self.population[i].copy()\n\n                    self.sf.append(F)\n                    self.scr.append(Cr)\n                    self.fitness[i] = f\n                    self.population[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                \n                if fevals >= self.budget:\n                    break\n            self.update_memory()\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDE_Archive_Mirrored_LSHADE_Toroidal scored 0.635 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["8e066679-5596-46e3-9e53-c396866f5d43"], "operator": null, "metadata": {"aucs": [0.21507349064426806, 0.39306645372021576, 0.6457207796135307, 0.8524971584902598, 0.7534561219321528, 0.8264500793376341, 0.6479801614096843, 0.6680988179692385, 0.7689191064167804, 0.24073391224405227, 0.8379392036085029, 0.9991011092961875, 0.2877944998065812, 0.6956376152146158, 0.880803240476199, 0.8214226553505526, 0.5299888156569799, 0.8268597402785314, 0.2817005717688126, 0.5214535650429792]}}
{"id": "348607c7-8345-42c2-a8ed-f2267ea0c7b9", "fitness": 0.6136051204518823, "name": "ImprovedSHADE_Ortho_AdaptPop_v2", "description": "Improved SHADE with dynamic F/CR adaptation based on success history, orthogonal crossover for enhanced diversity, and a more aggressive population reduction strategy.", "code": "import numpy as np\n\nclass ImprovedSHADE_Ortho_AdaptPop_v2:\n    def __init__(self, budget=10000, dim=10, memory_size=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = int(4 + 3 * np.log(budget))  # Initial population size\n        self.memory_size = memory_size\n        self.memory_F = np.full(memory_size, 0.5)\n        self.memory_CR = np.full(memory_size, 0.5)\n        self.p = 0.1  # Percentage of top individuals for p-best selection\n        self.population = None\n        self.fitness = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.archive = []  # For storing inferior solutions\n        self.archive_size = self.pop_size  # Archive size, can be adjusted\n        self.memory_index = 0\n        self.sf_M = []\n        self.scr_M = []\n        self.fevals = 0  # Track function evaluations\n        self.min_pop_size = 4  # Minimum population size\n        self.reduction_trigger = 0.25 #Fraction of budget before reduction starts\n        self.reduction_rate = 0.2 #Fraction of population reduced each time\n        self.use_orthogonal = True #Use orthogonal crossover\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.fevals += self.pop_size\n\n    def mutate(self, pop, i, F, Cr, archive):\n        p_best_size = max(int(self.p * len(pop)), 1)\n\n        # Tournament selection for p-best\n        indices = np.argsort(self.fitness)[:p_best_size]\n        p_best_idx = np.random.choice(indices)\n        p_best = pop[p_best_idx]\n\n        indices = [idx for idx in range(len(pop)) if idx != i]\n        if len(indices) == 0:\n            return pop[i]\n\n        r1 = np.random.choice(indices)\n\n        # Choose r2 from current pop or archive\n        if len(archive) > 0 and np.random.rand() < 0.5:  # 50% chance to use archive\n            r2_idx = np.random.randint(0, len(archive))\n            r2 = archive[r2_idx]\n        else:\n            remaining_indices = [idx for idx in indices if idx != r1]\n            if len(remaining_indices) == 0:\n                r2 = pop[r1]\n            else:\n                r2_idx = np.random.choice(remaining_indices)\n                r2 = pop[r2_idx]\n\n        mutant = p_best + F * (pop[r1] - r2)\n        return mutant\n\n    def orthogonal_crossover(self, mutant, target):\n        # Generate an orthogonal array (example using a simple Hadamard matrix)\n        H = np.array([[1, 1], [1, -1]])\n\n        # Ensure dimensions are compatible by repeating or truncating\n        if self.dim > H.shape[1]:\n            H_repeated = np.tile(H, (1, int(np.ceil(self.dim / H.shape[1]))))\n            H_used = H_repeated[:, :self.dim]\n        else:\n            H_used = H[:, :self.dim]\n\n        trial = np.copy(target)\n\n        for j in range(self.dim):\n            if H_used[0, j] == H_used[1, j]:\n                continue  # No change\n            else:\n                trial[j] = mutant[j]\n\n        return trial\n\n    def crossover(self, mutant, target, Cr):\n        cross_points = np.random.rand(self.dim) < Cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def update_archive(self, individual, archive):\n        archive.append(individual)\n        return archive\n\n    def reduce_population(self):\n        if self.pop_size > self.min_pop_size:\n            sorted_indices = np.argsort(self.fitness)[::-1]  # Worst to best\n            reduction_amount = max(1, int(self.pop_size * self.reduction_rate))  # Reduce by reduction_rate, but at least 1\n            removed_indices = sorted_indices[:reduction_amount]\n\n            self.population = np.delete(self.population, removed_indices, axis=0)\n            self.fitness = np.delete(self.fitness, removed_indices)\n            self.pop_size = len(self.population)\n            print(f\"Population reduced to {self.pop_size}\")\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n        self.archive = []\n        \n        self.sf_M = []\n        self.scr_M = []\n        weights = []\n\n        while self.fevals < self.budget:\n            sf_k = []\n            scr_k = []\n            weights = []\n\n            for i in range(self.pop_size):\n                # Sample F and CR from memory\n                mem_idx = np.random.randint(0, self.memory_size)\n                F = self.memory_F[mem_idx]\n                Cr = self.memory_CR[mem_idx]\n\n                # Truncate and Reflect F\n                if F > 1:\n                    F = 1\n                if F <= 0:\n                    F = 0.000001\n\n                mutant = self.mutate(self.population, i, F, Cr, self.archive)\n                mutant = np.clip(mutant, self.bounds_lb, self.bounds_ub)\n\n                # Use orthogonal crossover with a probability or with the regular crossover\n                if self.use_orthogonal and np.random.rand() < 0.5:  # 50% chance to use orthogonal crossover\n                    trial = self.orthogonal_crossover(mutant, self.population[i])\n                else:\n                    trial = self.crossover(mutant, self.population[i], Cr)\n\n                f = func(trial)\n                self.fevals += 1\n\n                if f < self.fitness[i]:\n                    delta = abs(f - self.fitness[i])\n                    sf_k.append(F)\n                    scr_k.append(Cr)\n                    weights.append(delta)\n\n                    self.archive = self.update_archive(self.population[i], self.archive)\n                    if len(self.archive) > self.archive_size:\n                        self.archive.pop(np.random.randint(0, len(self.archive)))  # Keep archive size constant\n\n                    self.fitness[i] = f\n                    self.population[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                else:\n                    self.archive = self.update_archive(trial, self.archive)  # Add even unsuccessful trials\n                    if len(self.archive) > self.archive_size:\n                        self.archive.pop(np.random.randint(0, len(self.archive)))\n\n                if self.fevals >= self.budget:\n                    break\n\n            # Update memory using Lehmer mean\n            if sf_k:\n                weights = np.array(weights) / np.sum(weights)\n                sum_weights_sfk = np.sum(weights * np.array(sf_k) ** 2)\n                sum_weights = np.sum(weights * np.array(sf_k))\n\n                if sum_weights > 0:\n                    mean_F = sum_weights_sfk / sum_weights\n                else:\n                    mean_F = 0.5\n\n                mean_CR = np.average(scr_k, weights=weights)\n\n                self.memory_F[self.memory_index] = mean_F\n                self.memory_CR[self.memory_index] = mean_CR\n                self.memory_index = (self.memory_index + 1) % self.memory_size\n\n            # Adaptive population size reduction\n            if self.fevals > self.budget * self.reduction_trigger:  # Start reducing after reduction_trigger of the budget is spent\n                self.reduce_population()\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm ImprovedSHADE_Ortho_AdaptPop_v2 scored 0.614 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["823ee2e9-0fd2-4a3b-976d-d6df61041486"], "operator": null, "metadata": {"aucs": [0.19555297325045462, 0.31684684488489145, 0.5734250351446735, 0.9020758791921526, 0.7259376018186213, 0.8664533143952756, 0.3691403132310844, 0.6037866105425347, 0.6714699180285402, 0.23780534124682073, 0.8418443478687823, 0.9993393801692049, 0.296387713772468, 0.6587565291655202, 0.8084200081681931, 0.8192914948217348, 0.5626160370266847, 0.9105150620249616, 0.3823534286145134, 0.5300845756705381]}}
{"id": "730cb02b-d718-4694-a376-520da6605807", "fitness": 0.7064999895881703, "name": "ImprovedSHADE_Ortho_AdaptPop_Aging", "description": "Improved SHADE with orthogonal crossover, adaptive population sizing, a combined archive strategy, and introduces aging and re-initialization of individuals to enhance exploration and escape local optima.", "code": "import numpy as np\n\nclass ImprovedSHADE_Ortho_AdaptPop_Aging:\n    def __init__(self, budget=10000, dim=10, memory_size=100, aging_rate=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = int(4 + 3 * np.log(budget))  # Initial population size\n        self.memory_size = memory_size\n        self.memory_F = np.full(memory_size, 0.5)\n        self.memory_CR = np.full(memory_size, 0.5)\n        self.p = 0.1  # Percentage of top individuals for p-best selection\n        self.population = None\n        self.fitness = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.archive = []  # For storing inferior solutions\n        self.archive_size = self.pop_size  # Archive size, can be adjusted\n        self.memory_index = 0\n        self.sf_M = []\n        self.scr_M = []\n        self.fevals = 0  # Track function evaluations\n        self.min_pop_size = 4  # Minimum population size\n        self.aging_rate = aging_rate  # Rate at which fitness degrades\n        self.age = None  # Individual ages\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.fevals += self.pop_size\n        self.age = np.zeros(self.pop_size)  # Initialize age for each individual\n\n    def mutate(self, pop, i, F, Cr, archive):\n        p_best_size = max(int(self.p * len(pop)), 1)\n\n        # Tournament selection for p-best\n        indices = np.argsort(self.fitness)[:p_best_size]\n        p_best_idx = np.random.choice(indices)\n        p_best = pop[p_best_idx]\n\n        indices = [idx for idx in range(len(pop)) if idx != i]\n        if len(indices) == 0:\n            return pop[i]\n\n        r1 = np.random.choice(indices)\n\n        # Choose r2 from current pop or archive\n        if len(archive) > 0 and np.random.rand() < 0.5:  # 50% chance to use archive\n            r2_idx = np.random.randint(0, len(archive))\n            r2 = archive[r2_idx]\n        else:\n            remaining_indices = [idx for idx in indices if idx != r1]\n            if len(remaining_indices) == 0:\n                r2 = pop[r1]\n            else:\n                r2_idx = np.random.choice(remaining_indices)\n                r2 = pop[r2_idx]\n\n        mutant = p_best + F * (pop[r1] - r2)\n        return mutant\n\n    def orthogonal_crossover(self, mutant, target):\n        # Generate an orthogonal array (example using a simple Hadamard matrix)\n        H = np.array([[1, 1], [1, -1]])\n\n        # Ensure dimensions are compatible by repeating or truncating\n        if self.dim > H.shape[1]:\n            H_repeated = np.tile(H, (1, int(np.ceil(self.dim / H.shape[1]))))\n            H_used = H_repeated[:, :self.dim]\n        else:\n            H_used = H[:, :self.dim]\n\n        trial = np.copy(target)\n\n        for j in range(self.dim):\n            if H_used[0, j] == H_used[1, j]:\n                continue  # No change\n            else:\n                trial[j] = mutant[j]\n\n        return trial\n\n\n    def crossover(self, mutant, target, Cr):\n        cross_points = np.random.rand(self.dim) < Cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def update_archive(self, individual, archive):\n        archive.append(individual)\n        return archive\n\n    def reduce_population(self):\n        if self.pop_size > self.min_pop_size:\n            sorted_indices = np.argsort(self.fitness)[::-1]  # Worst to best\n            reduction_amount = max(1, self.pop_size // 10)  # Reduce by 10%, but at least 1\n            removed_indices = sorted_indices[:reduction_amount]\n\n            self.population = np.delete(self.population, removed_indices, axis=0)\n            self.fitness = np.delete(self.fitness, removed_indices, axis=0)\n            self.age = np.delete(self.age, removed_indices)\n            self.pop_size = len(self.population)\n\n    def reinitialize_individual(self, func, i):\n        self.population[i] = np.random.uniform(self.bounds_lb, self.bounds_ub)\n        self.fitness[i] = func(self.population[i])\n        self.fevals += 1\n        self.age[i] = 0  # Reset age\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n        self.archive = []\n        \n        self.sf_M = []\n        self.scr_M = []\n        weights = []\n\n        while self.fevals < self.budget:\n            sf_k = []\n            scr_k = []\n            weights = []\n\n            for i in range(self.pop_size):\n                # Aging mechanism: Degrade fitness over time\n                self.fitness[i] *= (1 + self.aging_rate * self.age[i])\n\n                # Sample F and CR from memory\n                mem_idx = np.random.randint(0, self.memory_size)\n                F = self.memory_F[mem_idx]\n                Cr = self.memory_CR[mem_idx]\n\n                # Truncate and Reflect F\n                if F > 1:\n                    F = 1\n                if F <= 0:\n                    F = 0.000001\n\n                mutant = self.mutate(self.population, i, F, Cr, self.archive)\n                mutant = np.clip(mutant, self.bounds_lb, self.bounds_ub)\n\n                # Use orthogonal crossover with a probability or with the regular crossover\n                if np.random.rand() < 0.5:  # 50% chance to use orthogonal crossover\n                    trial = self.orthogonal_crossover(mutant, self.population[i])\n                else:\n                    trial = self.crossover(mutant, self.population[i], Cr)\n\n                f = func(trial)\n                self.fevals += 1\n\n                if f < self.fitness[i]:\n                    delta = abs(f - self.fitness[i])\n                    sf_k.append(F)\n                    scr_k.append(Cr)\n                    weights.append(delta)\n\n                    self.archive = self.update_archive(self.population[i], self.archive)\n                    if len(self.archive) > self.archive_size:\n                        self.archive.pop(np.random.randint(0, len(self.archive)))  # Keep archive size constant\n\n                    self.fitness[i] = f\n                    self.population[i] = trial\n                    self.age[i] = 0  # Reset age if improved\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                else:\n                    self.archive = self.update_archive(trial, self.archive)  # Add even unsuccessful trials\n                    if len(self.archive) > self.archive_size:\n                        self.archive.pop(np.random.randint(0, len(self.archive)))\n\n                    self.age[i] += 1  # Increment age if not improved\n                    # Re-initialize if individual is too old\n                    if self.age[i] > 10:\n                        self.reinitialize_individual(func, i)\n\n                if self.fevals >= self.budget:\n                    break\n\n            # Update memory using Lehmer mean\n            if sf_k:\n                weights = np.array(weights) / np.sum(weights)\n                sum_weights_sfk = np.sum(weights * np.array(sf_k) ** 2)\n                sum_weights = np.sum(weights * np.array(sf_k))\n\n                if sum_weights > 0:\n                    mean_F = sum_weights_sfk / sum_weights\n                else:\n                    mean_F = 0.5\n\n                mean_CR = np.mean(scr_k)\n\n                self.memory_F[self.memory_index] = mean_F\n                self.memory_CR[self.memory_index] = mean_CR\n                self.memory_index = (self.memory_index + 1) % self.memory_size\n\n            # Adaptive population size reduction\n            if self.fevals > self.budget // 2:  # Start reducing after half of the budget is spent\n                self.reduce_population()\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm ImprovedSHADE_Ortho_AdaptPop_Aging scored 0.706 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["823ee2e9-0fd2-4a3b-976d-d6df61041486"], "operator": null, "metadata": {"aucs": [0.2737071295722522, 0.4705042124392894, 0.8130392373418684, 0.9236085364890487, 0.8473613525580094, 0.8795692078527525, 0.7793236043270442, 0.7957048645652929, 0.8534725037808597, 0.8109763483296719, 0.8806636448725831, 0.9936403858081031, 0.29047541496385276, 0.8406238368337557, 0.7413306937688446, 0.8755565115802654, 0.4406557453166028, 0.9007151505479328, 0.2297095553691494, 0.4893618554462257]}}
{"id": "8e0eb69c-0712-44d3-99a5-e7c844d1b3ec", "fitness": 0.4961629809220346, "name": "CauchySHADE_Crowding_AdaptPop", "description": "Introducing a Cauchy mutation operator and a selection mechanism favoring individuals with high crowding distance to enhance exploration in crowded regions of the search space.", "code": "import numpy as np\n\nclass CauchySHADE_Crowding_AdaptPop:\n    def __init__(self, budget=10000, dim=10, memory_size=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = int(4 + 3 * np.log(budget))  # Initial population size\n        self.memory_size = memory_size\n        self.memory_F = np.full(memory_size, 0.5)\n        self.memory_CR = np.full(memory_size, 0.5)\n        self.p = 0.1  # Percentage of top individuals for p-best selection\n        self.population = None\n        self.fitness = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.archive = []  # For storing inferior solutions\n        self.archive_size = self.pop_size  # Archive size, can be adjusted\n        self.memory_index = 0\n        self.sf_M = []\n        self.scr_M = []\n        self.fevals = 0  # Track function evaluations\n        self.min_pop_size = 4  # Minimum population size\n        self.crowding_distances = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.fevals += self.pop_size\n        self.crowding_distances = np.zeros(self.pop_size) # Initialize crowding distances\n\n    def mutate(self, pop, i, F, Cr, archive):\n        p_best_size = max(int(self.p * len(pop)), 1)\n\n        # Tournament selection for p-best\n        indices = np.argsort(self.fitness)[:p_best_size]\n        p_best_idx = np.random.choice(indices)\n        p_best = pop[p_best_idx]\n\n        indices = [idx for idx in range(len(pop)) if idx != i]\n        if len(indices) == 0:\n            return pop[i]\n\n        r1 = np.random.choice(indices)\n\n        # Choose r2 from current pop or archive\n        if len(archive) > 0 and np.random.rand() < 0.5:  # 50% chance to use archive\n            r2_idx = np.random.randint(0, len(archive))\n            r2 = archive[r2_idx]\n        else:\n            remaining_indices = [idx for idx in indices if idx != r1]\n            if len(remaining_indices) == 0:\n                r2 = pop[r1]\n            else:\n                r2_idx = np.random.choice(remaining_indices)\n                r2 = pop[r2_idx]\n\n        # Cauchy mutation\n        cauchy_scale = F  # Use F as the scale parameter\n        cauchy_values = np.random.standard_cauchy(size=self.dim) * cauchy_scale\n        mutant = p_best + cauchy_values * (pop[r1] - r2) # Apply Cauchy distribution\n\n        return mutant\n\n    def crossover(self, mutant, target, Cr):\n        cross_points = np.random.rand(self.dim) < Cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def update_archive(self, individual, archive):\n        archive.append(individual)\n        return archive\n\n    def reduce_population(self):\n        if self.pop_size > self.min_pop_size:\n            sorted_indices = np.argsort(self.fitness)[::-1]  # Worst to best\n            reduction_amount = max(1, self.pop_size // 10)  # Reduce by 10%, but at least 1\n            removed_indices = sorted_indices[:reduction_amount]\n\n            self.population = np.delete(self.population, removed_indices, axis=0)\n            self.fitness = np.delete(self.fitness, removed_indices)\n            self.pop_size = len(self.population)\n            self.crowding_distances = np.delete(self.crowding_distances, removed_indices)\n            print(f\"Population reduced to {self.pop_size}\")\n\n    def calculate_crowding_distance(self):\n        \"\"\"Calculates crowding distance for each individual in the population.\"\"\"\n        if self.pop_size <= 2:\n            self.crowding_distances = np.ones(self.pop_size)  # Assign max distance if pop size is too small\n            return\n\n        distances = np.zeros(self.pop_size)\n        \n        # Normalize fitness values to range [0, 1]\n        fitness_min = np.min(self.fitness)\n        fitness_max = np.max(self.fitness)\n        if fitness_max == fitness_min:\n            normalized_fitness = np.zeros(self.pop_size)  # Avoid division by zero\n        else:\n            normalized_fitness = (self.fitness - fitness_min) / (fitness_max - fitness_min)\n        \n        # Sort by fitness\n        sorted_indices = np.argsort(normalized_fitness)\n        \n        # Boundary individuals have maximum distance\n        distances[sorted_indices[0]] = np.inf\n        distances[sorted_indices[-1]] = np.inf\n\n        # Calculate distances for intermediate individuals\n        for i in range(1, self.pop_size - 1):\n            distances[sorted_indices[i]] = normalized_fitness[sorted_indices[i+1]] - normalized_fitness[sorted_indices[i-1]]\n\n        self.crowding_distances = distances\n\n    def selection(self):\n        \"\"\"Binary tournament selection with crowding distance.\"\"\"\n        indices = np.arange(self.pop_size)\n        selected_indices = []\n        for _ in range(self.pop_size):\n            # Select two random individuals\n            idx1, idx2 = np.random.choice(indices, 2, replace=False)\n\n            # Compare based on fitness and crowding distance\n            if self.fitness[idx1] < self.fitness[idx2] or \\\n               (self.fitness[idx1] == self.fitness[idx2] and self.crowding_distances[idx1] > self.crowding_distances[idx2]):\n                selected_indices.append(idx1)\n            else:\n                selected_indices.append(idx2)\n\n        return self.population[selected_indices], self.fitness[selected_indices]\n\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n        self.archive = []\n        \n        self.sf_M = []\n        self.scr_M = []\n        weights = []\n\n        while self.fevals < self.budget:\n            sf_k = []\n            scr_k = []\n            weights = []\n\n            # Calculate crowding distances\n            self.calculate_crowding_distance()\n\n            for i in range(self.pop_size):\n                # Sample F and CR from memory\n                mem_idx = np.random.randint(0, self.memory_size)\n                F = self.memory_F[mem_idx]\n                Cr = self.memory_CR[mem_idx]\n\n                # Truncate and Reflect F\n                if F > 1:\n                    F = 1\n                if F <= 0:\n                    F = 0.000001\n\n                mutant = self.mutate(self.population, i, F, Cr, self.archive)\n                mutant = np.clip(mutant, self.bounds_lb, self.bounds_ub)\n\n                trial = self.crossover(mutant, self.population[i], Cr)\n\n                f = func(trial)\n                self.fevals += 1\n\n                if f < self.fitness[i]:\n                    delta = abs(f - self.fitness[i])\n                    sf_k.append(F)\n                    scr_k.append(Cr)\n                    weights.append(delta)\n\n                    self.archive = self.update_archive(self.population[i], self.archive)\n                    if len(self.archive) > self.archive_size:\n                        self.archive.pop(np.random.randint(0, len(self.archive)))  # Keep archive size constant\n\n                    self.fitness[i] = f\n                    self.population[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                else:\n                    self.archive = self.update_archive(trial, self.archive)  # Add even unsuccessful trials\n                    if len(self.archive) > self.archive_size:\n                        self.archive.pop(np.random.randint(0, len(self.archive)))\n\n                if self.fevals >= self.budget:\n                    break\n\n            # Update memory using Lehmer mean\n            if sf_k:\n                weights = np.array(weights) / np.sum(weights)\n                sum_weights_sfk = np.sum(weights * np.array(sf_k) ** 2)\n                sum_weights = np.sum(weights * np.array(sf_k))\n\n                if sum_weights > 0:\n                    mean_F = sum_weights_sfk / sum_weights\n                else:\n                    mean_F = 0.5\n\n                mean_CR = np.mean(scr_k)\n\n                self.memory_F[self.memory_index] = mean_F\n                self.memory_CR[self.memory_index] = mean_CR\n                self.memory_index = (self.memory_index + 1) % self.memory_size\n\n            # Selection\n            self.population, self.fitness = self.selection()\n\n            # Adaptive population size reduction\n            if self.fevals > self.budget // 2:  # Start reducing after half of the budget is spent\n                self.reduce_population()\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm CauchySHADE_Crowding_AdaptPop scored 0.496 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["823ee2e9-0fd2-4a3b-976d-d6df61041486"], "operator": null, "metadata": {"aucs": [0.19166425335152437, 0.16844623937996328, 0.8421030115594599, 0.9205307685006414, 0.29362984833274874, 0.9226713869768673, 0.2981490476374663, 0.35325401967213477, 0.33870579995181693, 0.1795961502142256, 0.3913565599084562, 0.9941801642538763, 0.23168253033328223, 0.2878224039445406, 0.7227933583569608, 0.8882442637715717, 0.30215510508706045, 0.9429049426764826, 0.17452385826599237, 0.47884590626562096]}}
{"id": "14a242e9-0566-4668-954c-e1ad4d8e88b4", "fitness": 0.5910356157180008, "name": "OrthogonalSHADE_Diversity", "description": "Introduces orthogonal learning to SHADE with diversity-guided adaptation of crossover and mutation parameters and an enhanced archive handling.", "code": "import numpy as np\n\nclass OrthogonalSHADE_Diversity:\n    def __init__(self, budget=10000, dim=10, memory_size=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = int(4 + 3 * np.log(budget))\n        self.memory_size = memory_size\n        self.memory_F = np.full(memory_size, 0.5)\n        self.memory_CR = np.full(memory_size, 0.5)\n        self.p = 0.1  # Percentage of top individuals for p-best selection\n        self.population = None\n        self.fitness = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.archive = [] # For storing inferior solutions\n        self.archive_size = self.pop_size  # Archive size, can be adjusted\n\n        self.memory_index = 0\n        self.sf_M = []\n        self.scr_M = []\n        self.diversity_threshold = 0.1 # Parameter to control diversity adaptation\n        self.ol_samples = 5 # Number of samples for orthogonal learning\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n\n    def calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        distances = np.linalg.norm(self.population - centroid, axis=1)\n        diversity = np.mean(distances)\n        return diversity\n\n    def mutate(self, pop, i, F, Cr, diversity):\n        p_best_size = max(int(self.p * len(pop)), 1)\n        \n        # Tournament selection for p-best\n        indices = np.argsort(self.fitness)[:p_best_size]\n        p_best_idx = np.random.choice(indices)\n        p_best = pop[p_best_idx]\n\n        indices = [idx for idx in range(len(pop)) if idx != i]\n        if len(indices) < 2:\n            return pop[i]  # If only one element, no mutation\n\n        r1, r2 = np.random.choice(indices, 2, replace=False)\n\n        # Dynamic Jitter based on diversity\n        jitter_scale = 0.05 * (1 + diversity) # Increased jitter when diversity is high\n        jitter = np.random.uniform(-jitter_scale, jitter_scale, size=self.dim)\n        mutant = p_best + F * (pop[r1] - pop[r2] + jitter)\n\n        return mutant\n\n    def orthogonal_learning(self, func, current_individual, bounds_lb, bounds_ub):\n        dim = len(current_individual)\n        levels = self.ol_samples\n        design_matrix = self.create_orthogonal_array(dim, levels)\n        \n        candidates = np.zeros((levels, dim))\n        for i in range(levels):\n            candidates[i] = current_individual.copy()\n            for j in range(dim):\n                candidates[i][j] = bounds_lb[j] + (bounds_ub[j] - bounds_lb[j]) * (design_matrix[i, j] - 1) / (levels - 1)\n\n        fitness_values = np.array([func(x) for x in candidates])\n        best_index = np.argmin(fitness_values)\n        return candidates[best_index], fitness_values[best_index]\n    \n    def create_orthogonal_array(self, dim, levels):\n          # Generate a simple orthogonal array (L_25(5^6)\n          # For demonstration, we assume levels = 5 and use a predefined OA.\n          # In practice, use specialized OA libraries or methods for different levels and dimensions\n          if levels == 5 and dim <= 6:\n              oa = np.array([[1, 1, 1, 1, 1, 1],\n                             [1, 2, 2, 2, 2, 2],\n                             [1, 3, 3, 3, 3, 3],\n                             [1, 4, 4, 4, 4, 4],\n                             [1, 5, 5, 5, 5, 5],\n                             [2, 1, 2, 3, 4, 5],\n                             [2, 2, 1, 4, 5, 3],\n                             [2, 3, 4, 5, 1, 2],\n                             [2, 4, 5, 1, 3, 4],\n                             [2, 5, 3, 2, 1, 5],\n                             [3, 1, 3, 5, 2, 4],\n                             [3, 2, 4, 1, 3, 5],\n                             [3, 3, 5, 2, 4, 1],\n                             [3, 4, 1, 3, 5, 2],\n                             [3, 5, 2, 4, 1, 3],\n                             [4, 1, 4, 2, 5, 3],\n                             [4, 2, 3, 5, 1, 4],\n                             [4, 3, 1, 4, 2, 5],\n                             [4, 4, 2, 5, 3, 1],\n                             [4, 5, 5, 3, 4, 2],\n                             [5, 1, 5, 4, 3, 2],\n                             [5, 2, 5, 3, 4, 1],\n                             [5, 3, 2, 1, 5, 4],\n                             [5, 4, 3, 2, 1, 5],\n                             [5, 5, 1, 5, 2, 3]])\n              return oa[:, :dim]\n          else:\n              raise ValueError(\"Orthogonal array is only defined for levels=5 and dim<=6 in this example.\")\n\n    def crossover(self, mutant, target, Cr, diversity):\n        cross_points = np.random.rand(self.dim) < Cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n    \n    def update_archive(self, individual, success, diversity):\n        if len(self.archive) < self.archive_size:\n            self.archive.append(individual)\n        else:\n            # Adaptive archive update based on diversity: replace less frequently when diversity is low\n            idx = np.random.randint(0, len(self.archive))\n            \n            # Increased probability of replacing with successful trials when diversity is high\n            archive_acceptance_prob = 0.1 + 0.9 * diversity # Ranges between 0.1 and 1\n            \n            if success and np.random.rand() < archive_acceptance_prob:\n                self.archive[idx] = individual\n            elif not success and np.random.rand() < 0.1:  #Small chance to replace regardless.\n                self.archive[idx] = individual\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n        fevals = self.pop_size\n        \n        self.sf_M = []\n        self.scr_M = []\n        weights = []\n        \n        while fevals < self.budget:\n            sf_k = []\n            scr_k = []\n            weights = []\n\n            diversity = self.calculate_diversity()\n            \n            for i in range(self.pop_size):\n                # Sample F and CR from memory\n                mem_idx = np.random.randint(0, self.memory_size)\n                F = self.memory_F[mem_idx]\n                Cr = self.memory_CR[mem_idx]\n\n                # Truncate and Reflect F\n                if F > 1:\n                    F = 1\n                if F <= 0:\n                    F = 0.000001\n                    \n\n                mutant = self.mutate(self.population, i, F, Cr, diversity)\n                mutant = np.clip(mutant, self.bounds_lb, self.bounds_ub)\n                trial = self.crossover(mutant, self.population[i], Cr, diversity)\n                \n                f = func(trial)\n                fevals += 1\n\n                if f < self.fitness[i]:\n                    delta = abs(f - self.fitness[i])\n                    sf_k.append(F)\n                    scr_k.append(Cr)\n                    weights.append(delta)\n                    \n                    self.update_archive(self.population[i], True, diversity)\n\n                    self.fitness[i] = f\n                    self.population[i] = trial\n                    \n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                else:\n                     self.update_archive(trial, False, diversity) # trial wasn't successful\n\n                if fevals >= self.budget:\n                    break\n            \n            # Update memory using Lehmer mean\n            if sf_k:\n                weights = np.array(weights) / np.sum(weights)\n                sum_weights_sfk = np.sum(weights * np.array(sf_k)**2)\n                sum_weights = np.sum(weights * np.array(sf_k))\n                \n                if sum_weights > 0:\n                    mean_F = sum_weights_sfk / sum_weights\n                else:\n                    mean_F = 0.5\n                \n                mean_CR = np.mean(scr_k)\n                \n                self.memory_F[self.memory_index] = mean_F\n                self.memory_CR[self.memory_index] = mean_CR\n                self.memory_index = (self.memory_index + 1) % self.memory_size\n            \n            # Orthogonal Learning: Apply OL to the best individual of the generation. Limit FE\n            if fevals + self.ol_samples * self.dim < self.budget: #Do not exceed budget.\n                 best_index = np.argmin(self.fitness)\n                 best_individual = self.population[best_index]\n                 \n                 ol_individual, ol_fitness = self.orthogonal_learning(func, best_individual, self.bounds_lb, self.bounds_ub)\n                 fevals += self.ol_samples\n                 \n                 if ol_fitness < self.fitness[best_index]:\n                     self.fitness[best_index] = ol_fitness\n                     self.population[best_index] = ol_individual\n                     \n                     if ol_fitness < self.f_opt:\n                         self.f_opt = ol_fitness\n                         self.x_opt = ol_individual\n                \n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm OrthogonalSHADE_Diversity scored 0.591 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f10b3942-d168-44c6-861d-cb3da11fea95"], "operator": null, "metadata": {"aucs": [0.2704130079294289, 0.5838567605889418, 0.5410209538451591, 0.9233261814216943, 0.5812584936917158, 0.6203739730185143, 0.4278807635331766, 0.4827686726638152, 0.5593512288464376, 0.4593862940400104, 0.8999672693348578, 0.9958346244903984, 0.2474629997786747, 0.5396021140507393, 0.9347294083796386, 0.6372884760306445, 0.46142495062988387, 0.7582902396329314, 0.24122621053530924, 0.6552496919180442]}}
{"id": "f1717dd4-e8c6-46d2-8dd0-2327ab35b204", "fitness": -Infinity, "name": "AttractRepelDE", "description": "Population-based algorithm with a novel \"attraction-repulsion\" mechanism where individuals are attracted to better solutions and repelled from crowded regions, combined with adaptive parameter control based on success rates.", "code": "import numpy as np\n\nclass AttractRepelDE:\n    def __init__(self, budget=10000, dim=10, pop_size=None, archive_size_factor=2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else int(4 + 3 * np.log(budget))\n        self.archive_size = int(archive_size_factor * self.pop_size)\n        self.F = 0.5\n        self.Cr = 0.5\n        self.population = None\n        self.fitness = None\n        self.archive = []\n        self.success_F = []\n        self.success_Cr = []\n        self.sf_M = []\n        self.scr_M = []\n        self.memory_size = 100\n        self.memory_F = np.full(self.memory_size, 0.5)\n        self.memory_CR = np.full(self.memory_size, 0.5)\n        self.memory_index = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n\n    def calculate_crowding(self):\n        crowding = np.zeros(self.pop_size)\n        for i in range(self.pop_size):\n            for j in range(self.pop_size):\n                if i != j:\n                    dist = np.linalg.norm(self.population[i] - self.population[j])\n                    crowding[i] += 1 / (dist + 1e-8) # Avoid division by zero\n        return crowding\n\n    def mutate(self, i):\n        indices = [idx for idx in range(self.pop_size) if idx != i]\n        a, b, c = np.random.choice(indices, 3, replace=False)\n\n        # Attraction-Repulsion mechanism\n        crowding = self.calculate_crowding()\n        best_idx = np.argmin(self.fitness)\n        attraction = self.population[best_idx] - self.population[i]\n        repulsion = np.zeros(self.dim)\n\n        if crowding[i] > 0:\n            repulsion = np.sum([\n                (self.population[i] - self.population[j]) / (np.linalg.norm(self.population[i] - self.population[j]) + 1e-8)\n                for j in range(self.pop_size) if i != j\n            ], axis=0) / crowding[i]\n\n        mutant = self.population[i] + self.F * (self.population[a] - self.population[b]) + 0.1 * attraction - 0.1 * repulsion\n\n        return mutant\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.Cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def update_archive(self, individual):\n        if len(self.archive) < self.archive_size:\n            self.archive.append(individual)\n        else:\n            idx = np.random.randint(0, len(self.archive))\n            self.archive[idx] = individual\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n        fevals = self.pop_size\n\n        while fevals < self.budget:\n            self.success_F = []\n            self.success_Cr = []\n            \n            for i in range(self.pop_size):\n                # Sample F and Cr from memory\n                mem_idx = np.random.randint(0, self.memory_size)\n                self.F = self.memory_F[mem_idx]\n                self.Cr = self.memory_CR[mem_idx]\n                \n                mutant = self.mutate(i)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                trial = self.crossover(mutant, self.population[i])\n                f = func(trial)\n                fevals += 1\n\n                if f < self.fitness[i]:\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n\n                    self.update_archive(self.population[i])\n                    self.fitness[i] = f\n                    self.population[i] = trial\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                else:\n                    self.update_archive(trial)\n\n                if fevals >= self.budget:\n                    break\n            if self.success_F:\n                mean_F = np.mean(self.success_F)\n                mean_Cr = np.mean(self.success_Cr)\n            else:\n                mean_F = 0.5\n                mean_Cr = 0.5\n            \n            self.memory_F[self.memory_index] = mean_F\n            self.memory_CR[self.memory_index] = mean_Cr\n            self.memory_index = (self.memory_index + 1) % self.memory_size\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "An exception occurred: Evaluation timed out after 60 seconds..", "error": "", "parent_ids": ["f10b3942-d168-44c6-861d-cb3da11fea95"], "operator": null, "metadata": {}}
{"id": "11c8560b-6bd3-4aa4-9596-7c6126660c60", "fitness": 0.28311044338630315, "name": "EnhancedAdaptiveDE", "description": "Introduces a dynamically adjusted archive size based on population diversity and utilizes a weighted Lehmer mean for parameter adaptation, enhancing both exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, F_initial=0.5, Cr_initial=0.9, archive_size_max=10, pop_scaling=4):\n        self.budget = budget\n        self.dim = dim\n        self.F = F_initial\n        self.Cr = Cr_initial\n        self.pop_scaling = pop_scaling # Scales population size\n        self.pop_size = int(self.pop_scaling + 3 * np.log(dim))\n        self.population = None\n        self.fitness = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.F_min = 0.1\n        self.F_max = 1.0\n        self.Cr_min = 0.1\n        self.Cr_max = 0.9\n        self.archive = []\n        self.archive_size_max = archive_size_max\n        self.archive_size = archive_size_max // 2  # Initial archive size\n        self.success_F = []\n        self.success_Cr = []\n        self.memory_size = 10\n        self.memory_F = np.full(self.memory_size, F_initial)\n        self.memory_Cr = np.full(self.memory_size, Cr_initial)\n        self.memory_idx = 0\n        self.p_archive = 0.1 # Probability of using archive individual\n        self.diversity_threshold = 0.1 # Threshold for diversity adaptation\n\n\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.population, axis=0)\n        distances = np.linalg.norm(self.population - mean_position, axis=1)\n        diversity = np.mean(distances)\n        return diversity\n\n    def adjust_parameters(self):\n        # Weighted Lehmer mean for F and Cr adaptation\n        if self.success_F:\n            weights = np.arange(1, len(self.success_F) + 1)\n            weighted_F_sq = np.sum(weights * np.array(self.success_F)**2)\n            weighted_F = np.sum(weights * np.array(self.success_F))\n            self.F = weighted_F_sq / weighted_F if weighted_F > 0 else 0.5\n            self.Cr = np.average(self.success_Cr, weights=weights)\n        else:\n            self.F = self.memory_F[self.memory_idx]\n            self.Cr = self.memory_Cr[self.memory_idx]\n\n        self.F = np.clip(self.F, self.F_min, self.F_max)\n        self.Cr = np.clip(self.Cr, self.Cr_min, self.Cr_max)\n\n    def adjust_archive_size(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold:\n            self.archive_size = min(self.archive_size + 1, self.archive_size_max)\n        else:\n            self.archive_size = max(self.archive_size - 1, 0)  # Ensure archive size is non-negative\n\n    def mutate(self, pop, i):\n        indices = [idx for idx in range(len(pop)) if idx != i]\n        if len(self.archive) > 0 and np.random.rand() < self.p_archive:\n            arch_ind = np.random.randint(0, len(self.archive))\n            pop_indices = np.random.choice(indices, 2, replace=False)\n            a = self.archive[arch_ind]\n            b, c = pop[pop_indices[0]], pop[pop_indices[1]]\n        else:\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n        \n        mutation = a + self.F * (b - c)\n        return mutation\n\n    def mirrored_boundary_handling(self, mutant):\n        for i in range(self.dim):\n            if mutant[i] < self.bounds_lb[i]:\n                mutant[i] = self.bounds_lb[i] + (self.bounds_lb[i] - mutant[i])\n            elif mutant[i] > self.bounds_ub[i]:\n                mutant[i] = self.bounds_ub[i] - (mutant[i] - self.bounds_ub[i])\n        return mutant\n    \n    def orthogonal_crossover(self, mutant, target):\n        # Orthogonal Crossover\n        num_groups = 2\n        group_size = self.dim // num_groups\n        trial = target.copy()\n\n        for g in range(num_groups):\n            start_index = g * group_size\n            end_index = (g + 1) * group_size if g < num_groups - 1 else self.dim\n            if np.random.rand() < self.Cr: # Apply with Cr probability\n                trial[start_index:end_index] = mutant[start_index:end_index]\n        return trial\n\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n        fevals = self.pop_size\n        self.success_F = []\n        self.success_Cr = []\n\n\n        while fevals < self.budget:\n            self.adjust_parameters()\n            self.adjust_archive_size()\n            self.success_F = []\n            self.success_Cr = []\n\n            for i in range(self.pop_size):\n                mutant = self.mutate(self.population, i)\n                mutant = self.mirrored_boundary_handling(mutant)\n                trial = self.orthogonal_crossover(mutant, self.population[i])\n                \n                f = func(trial)\n                fevals += 1\n\n                if f < self.fitness[i]:\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        if self.archive_size > 0:\n                            replace_index = np.random.randint(0, self.archive_size)\n                            self.archive[replace_index] = self.population[i].copy()\n\n                    self.fitness[i] = f\n                    self.population[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                \n                if fevals >= self.budget:\n                    break\n            \n            # Update Memory\n            if self.success_F:\n                self.memory_F[self.memory_idx] = np.mean(self.success_F)\n                self.memory_Cr[self.memory_idx] = np.mean(self.success_Cr)\n                self.memory_idx = (self.memory_idx + 1) % self.memory_size\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm EnhancedAdaptiveDE scored 0.283 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["387aaea1-3cae-49da-8787-cc4e70f036a2"], "operator": null, "metadata": {"aucs": [0.08564876039688119, 0.20302499938994412, 0.2704346936520504, 0.16244527412001974, 0.1922064453639254, 0.2074811988155546, 0.2719981169762581, 0.20457975136987827, 0.19844583158771778, 0.17683460362707304, 0.20698532342034337, 0.9975951862274687, 0.24489215942180376, 0.27367467575807514, 0.5486859098082333, 0.29188940031038746, 0.20642492430380455, 0.2614952298906935, 0.18872707316874215, 0.4687393101172088]}}
{"id": "ce9efe8e-cb8d-4211-b86d-3f7ace28dc13", "fitness": 0.7001010957476962, "name": "NeighborhoodAdaptiveDE", "description": "Adaptive Differential Evolution with a Neighborhood-Based Mutation and a Cauchy distributed step size, combined with an archive and mirrored boundary handling.", "code": "import numpy as np\n\nclass NeighborhoodAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, F_initial=0.5, Cr_initial=0.9, archive_size=5, pop_scaling=4, neighborhood_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.F = F_initial\n        self.Cr = Cr_initial\n        self.pop_scaling = pop_scaling\n        self.pop_size = int(self.pop_scaling + 3 * np.log(dim))\n        self.population = None\n        self.fitness = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.F_min = 0.1\n        self.F_max = 1.0\n        self.Cr_min = 0.1\n        self.Cr_max = 0.9\n        self.archive = []\n        self.archive_size = archive_size\n        self.success_F = []\n        self.success_Cr = []\n        self.memory_size = 10\n        self.memory_F = np.full(self.memory_size, F_initial)\n        self.memory_Cr = np.full(self.memory_size, Cr_initial)\n        self.memory_idx = 0\n        self.p_archive = 0.1\n        self.neighborhood_size = neighborhood_size\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.population, axis=0)\n        distances = np.linalg.norm(self.population - mean_position, axis=1)\n        diversity = np.mean(distances)\n        return diversity\n\n    def adjust_parameters(self):\n        if self.success_F:\n            self.F = np.sum(np.array(self.success_F)**2) / np.sum(self.success_F) if np.sum(self.success_F) > 0 else 0.5\n            self.Cr = np.mean(self.success_Cr)\n        else:\n            self.F = self.memory_F[self.memory_idx]\n            self.Cr = self.memory_Cr[self.memory_idx]\n\n        self.F = np.clip(self.F, self.F_min, self.F_max)\n        self.Cr = np.clip(self.Cr, self.Cr_min, self.Cr_max)\n\n    def mutate(self, pop, i):\n        # Neighborhood-based mutation\n        indices = [idx for idx in range(len(pop)) if idx != i]\n        neighborhood_indices = np.random.choice(indices, min(self.neighborhood_size, len(indices)), replace=False)\n        \n        if len(self.archive) > 0 and np.random.rand() < self.p_archive:\n            arch_ind = np.random.randint(0, len(self.archive))\n            a = self.archive[arch_ind]\n            \n            remaining_indices = [idx for idx in indices if idx not in neighborhood_indices]\n            if remaining_indices:\n                b_index = np.random.choice(remaining_indices)\n            else:\n                b_index = np.random.choice(neighborhood_indices)\n            b = pop[b_index]\n        else:\n             a = pop[np.random.choice(neighborhood_indices)]\n             remaining_indices = [idx for idx in indices if idx not in neighborhood_indices]\n             if remaining_indices:\n                b_index = np.random.choice(remaining_indices)\n             else:\n                b_index = np.random.choice(neighborhood_indices)\n             b = pop[b_index]\n\n        cauchy_step = self.F * np.random.standard_cauchy(size=self.dim)\n        mutation = a + cauchy_step * (b - pop[i]) # Using Cauchy distribution for step size\n        return mutation\n\n    def mirrored_boundary_handling(self, mutant):\n        for i in range(self.dim):\n            if mutant[i] < self.bounds_lb[i]:\n                mutant[i] = self.bounds_lb[i] + (self.bounds_lb[i] - mutant[i])\n            elif mutant[i] > self.bounds_ub[i]:\n                mutant[i] = self.bounds_ub[i] - (mutant[i] - self.bounds_ub[i])\n        return mutant\n\n    def crossover(self, mutant, target):\n        trial = target.copy()\n        for i in range(self.dim):\n            if np.random.rand() < self.Cr:\n                trial[i] = mutant[i]\n        return trial\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n        fevals = self.pop_size\n        self.success_F = []\n        self.success_Cr = []\n\n        while fevals < self.budget:\n            self.adjust_parameters()\n            self.success_F = []\n            self.success_Cr = []\n\n            for i in range(self.pop_size):\n                mutant = self.mutate(self.population, i)\n                mutant = self.mirrored_boundary_handling(mutant)\n                trial = self.crossover(mutant, self.population[i])\n                \n                f = func(trial)\n                fevals += 1\n\n                if f < self.fitness[i]:\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        replace_index = np.random.randint(0, self.archive_size)\n                        self.archive[replace_index] = self.population[i].copy()\n\n                    self.fitness[i] = f\n                    self.population[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n\n                if fevals >= self.budget:\n                    break\n\n            # Update Memory\n            if self.success_F:\n                self.memory_F[self.memory_idx] = np.mean(self.success_F)\n                self.memory_Cr[self.memory_idx] = np.mean(self.success_Cr)\n                self.memory_idx = (self.memory_idx + 1) % self.memory_size\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm NeighborhoodAdaptiveDE scored 0.700 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["387aaea1-3cae-49da-8787-cc4e70f036a2"], "operator": null, "metadata": {"aucs": [0.24648244840637412, 0.3801992654005927, 0.8453668056275591, 0.952496887047213, 0.8070255626035117, 0.9194444219128666, 0.7833028015342632, 0.47700854740592247, 0.8759149166833586, 0.8470821233192416, 0.5752257966128848, 0.9987776963539453, 0.2622396273322648, 0.8431544255357943, 0.79744168990462, 0.9305507817084478, 0.7845574387075926, 0.9417129345249272, 0.21954947824998405, 0.5144882660825612]}}
{"id": "db750980-9995-4f6f-97b0-94825807e720", "fitness": -Infinity, "name": "SelfAdaptiveMutationDE", "description": "Introducing a self-adaptive mutation strategy based on the success rate of different mutation operators, combined with a local search refinement step.", "code": "import numpy as np\n\nclass SelfAdaptiveMutationDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.population = None\n        self.fitness = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.archive = []\n        self.mutation_ops = ['DE/rand/1', 'DE/best/1', 'DE/current-to-best/1']\n        self.success_rates = {op: 1.0 / len(self.mutation_ops) for op in self.mutation_ops}\n        self.local_search_prob = 0.1\n        self.fevals = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.fevals += self.pop_size\n\n    def mutate(self, op, pop, i, F, Cr):\n        if op == 'DE/rand/1':\n            indices = np.random.choice(len(pop), 3, replace=False)\n            x_r1, x_r2, x_r3 = pop[indices]\n            mutant = pop[i] + F * (x_r2 - x_r3)\n        elif op == 'DE/best/1':\n            best_idx = np.argmin(self.fitness)\n            x_best = pop[best_idx]\n            indices = np.random.choice(len(pop), 2, replace=False)\n            x_r1, x_r2 = pop[indices]\n            mutant = x_best + F * (x_r1 - x_r2)\n        elif op == 'DE/current-to-best/1':\n            best_idx = np.argmin(self.fitness)\n            x_best = pop[best_idx]\n            indices = np.random.choice(len(pop), 2, replace=False)\n            x_r1, x_r2 = pop[indices]\n            mutant = pop[i] + F * (x_best - pop[i]) + F * (x_r1 - x_r2)\n        else:\n            raise ValueError(\"Invalid mutation operator\")\n\n        return mutant\n\n    def crossover(self, mutant, target, Cr):\n        cross_points = np.random.rand(self.dim) < Cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def local_search(self, x, func):\n        # Simple local search: perturb each dimension slightly\n        x_new = x.copy()\n        for i in range(self.dim):\n            x_new[i] += np.random.normal(0, 0.01)  # Small perturbation\n            x_new[i] = np.clip(x_new[i], self.bounds_lb, self.bounds_ub)\n        f_new = func(x_new)\n        self.fevals += 1\n        return x_new, f_new\n\n    def update_archive(self, individual):\n        self.archive.append(individual)\n        if len(self.archive) > self.archive_size:\n            self.archive.pop(0)\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n\n        while self.fevals < self.budget:\n            op_probs = np.array(list(self.success_rates.values()))\n            op_probs /= np.sum(op_probs)\n\n            for i in range(self.pop_size):\n                # Select mutation operator based on success rates\n                op = np.random.choice(list(self.mutation_ops), p=op_probs)\n\n                # Parameters for DE\n                F = np.random.uniform(0.1, 0.9)\n                Cr = np.random.uniform(0.1, 0.9)\n\n                mutant = self.mutate(op, self.population, i, F, Cr)\n                mutant = np.clip(mutant, self.bounds_lb, self.bounds_ub)\n                trial = self.crossover(mutant, self.population[i], Cr)\n\n                f = func(trial)\n                self.fevals += 1\n\n                # Local search refinement\n                if np.random.rand() < self.local_search_prob:\n                    trial, f = self.local_search(trial, func)\n\n                if f < self.fitness[i]:\n                    # Update success rate for chosen mutation operator\n                    self.success_rates[op] *= 0.9  # Decay old success\n                    self.success_rates[op] += 0.1  # Increment new success\n\n                    self.fitness[i] = f\n                    self.population[i] = trial\n                    self.update_archive(self.population[i])\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                else:\n                    self.success_rates[op] *= 0.9  # Decay success if not improved\n                    self.update_archive(trial) # Add even unsuccessful trials\n\n                # Normalize success rates to maintain probabilities\n                sum_success = np.sum(list(self.success_rates.values()))\n                for key in self.success_rates:\n                    self.success_rates[key] /= sum_success\n\n\n                if self.fevals >= self.budget:\n                    break\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "An exception occurred: setting an array element with a sequence..", "error": "", "parent_ids": ["730cb02b-d718-4694-a376-520da6605807"], "operator": null, "metadata": {}}
{"id": "0ee0ee0b-1303-415b-ab7c-b2b6104f7639", "fitness": 0.0, "name": "AdaptiveDE_SA", "description": "DE with a self-adaptive strategy for population size and a combined local search (Simulated Annealing) for exploitation.", "code": "import numpy as np\n\nclass AdaptiveDE_SA:\n    def __init__(self, budget=10000, dim=10, archive_size=5, memory_size=5, initial_pop_size=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size  # Initial population size\n        self.min_pop_size = 10\n        self.max_pop_size = 100\n        self.population = None\n        self.fitness = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.archive = []  # Archive for storing removed individuals\n        self.archive_size = archive_size\n        self.memory_size = memory_size\n        self.memory_F = np.full(self.memory_size, 0.5)\n        self.memory_Cr = np.full(self.memory_size, 0.5)\n        self.sf = []\n        self.scr = []\n        self.p = 0.1  # Probability of using archive individual\n        self.w = 0.5  # Weight for Lehmer mean update\n        self.sa_iterations = 5  # Number of SA iterations per individual\n        self.initial_temp = 1.0\n        self.cooling_rate = 0.95\n\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n\n    def mutate(self, pop, i, current_generation):\n        memory_index = current_generation % self.memory_size\n        Cr = self.memory_Cr[memory_index]\n        F = self.memory_F[memory_index]\n\n        indices = [idx for idx in range(len(pop)) if idx != i]\n        if len(self.archive) > 0 and np.random.rand() < self.p:\n            arch_ind = np.random.randint(0, len(self.archive))\n            pop_indices = np.random.choice(indices, 2, replace=False)\n            a = self.archive[arch_ind]\n            b, c = pop[pop_indices[0]], pop[pop_indices[1]]\n        else:\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n\n        mutation = a + F * (b - c)\n        return mutation, Cr, F\n\n    def boundary_handling(self, mutant):\n        # Clip the mutant to stay within the bounds\n        mutant = np.clip(mutant, self.bounds_lb, self.bounds_ub)\n        return mutant\n\n    def crossover(self, mutant, target, Cr):\n        cross_points = np.random.rand(self.dim) < Cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def update_memory(self):\n        if self.sf:\n            self.sf = np.array(self.sf)\n            self.scr = np.array(self.scr)\n\n            # Lehmer mean for F\n            sum_sf_squared = np.sum(self.sf ** 2)\n            sum_sf = np.sum(self.sf)\n            if sum_sf > 0:\n                lehmer_mean_F = sum_sf_squared / sum_sf\n            else:\n                lehmer_mean_F = 0.5  # Default value if no successful F values\n\n            # Lehmer mean for Cr\n            lehmer_mean_Cr = np.mean(self.scr)\n\n            memory_index = self.generation % self.memory_size\n            self.memory_F[memory_index] = self.w * self.memory_F[memory_index] + (1 - self.w) * lehmer_mean_F\n            self.memory_Cr[memory_index] = self.w * self.memory_Cr[memory_index] + (1 - self.w) * lehmer_mean_Cr\n\n        self.sf = []\n        self.scr = []\n\n    def adjust_population_size(self):\n        success_rate = len(self.sf) / self.pop_size if self.pop_size > 0 else 0\n        \n        if success_rate > 0.2:\n            self.pop_size = min(self.pop_size + 1, self.max_pop_size)\n        elif success_rate < 0.1:\n            self.pop_size = max(self.pop_size - 1, self.min_pop_size)\n\n        self.population = self.population[:self.pop_size]\n        self.fitness = self.fitness[:self.pop_size]\n\n\n    def simulated_annealing(self, func, individual):\n        current_state = individual.copy()\n        current_energy = func(current_state)\n        temperature = self.initial_temp\n\n        for _ in range(self.sa_iterations):\n            neighbor = current_state + np.random.normal(0, 0.1, size=self.dim)  # Small perturbation\n            neighbor = self.boundary_handling(neighbor)\n            neighbor_energy = func(neighbor)\n\n            delta_energy = neighbor_energy - current_energy\n            if delta_energy < 0 or np.random.rand() < np.exp(-delta_energy / temperature):\n                current_state = neighbor.copy()\n                current_energy = neighbor_energy\n\n            temperature *= self.cooling_rate\n\n        return current_state, current_energy\n\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n        fevals = self.pop_size\n        self.generation = 0\n        self.sf = []\n        self.scr = []\n\n        while fevals < self.budget:\n            self.generation += 1\n            \n            for i in range(self.pop_size):\n                mutant, Cr, F = self.mutate(self.population, i, self.generation)\n                mutant = self.boundary_handling(mutant)\n                trial = self.crossover(mutant, self.population[i], Cr)\n\n                # Apply Simulated Annealing as local search\n                trial, f = self.simulated_annealing(func, trial)\n                fevals += self.sa_iterations  # Account for SA evaluations\n\n                if f < self.fitness[i]:\n                    # Archive the replaced individual\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        # Replace a random element in the archive\n                        replace_index = np.random.randint(0, self.archive_size)\n                        self.archive[replace_index] = self.population[i].copy()\n\n                    self.sf.append(F)\n                    self.scr.append(Cr)\n                    self.fitness[i] = f\n                    self.population[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n\n                if fevals >= self.budget:\n                    break\n\n            self.update_memory()\n            self.adjust_population_size()\n            \n            # Repopulate if the population size changed\n            if len(self.population) < self.pop_size:\n                num_new_individuals = self.pop_size - len(self.population)\n                new_individuals = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(num_new_individuals, self.dim))\n                self.population = np.vstack((self.population, new_individuals))\n                self.fitness = np.concatenate((self.fitness, [func(x) for x in new_individuals]))\n                fevals += num_new_individuals\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveDE_SA scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b5f53273-1fd3-4232-b8aa-7572ba9cd3a0"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "d1b08a7e-85a4-42fb-a8e8-34c70bf8e86e", "fitness": 0.0, "name": "NeighborhoodAdaptiveDE", "description": "Adaptive Differential Evolution with Neighborhood Mutation, Cauchy step size, Archive, Mirrored Boundaries, and a Population Restart mechanism based on Diversity and Stagnation to enhance exploration and escape local optima.", "code": "import numpy as np\n\nclass NeighborhoodAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, F_initial=0.5, Cr_initial=0.9, archive_size=5, pop_scaling=4, neighborhood_size=5, restart_trigger=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.F = F_initial\n        self.Cr = Cr_initial\n        self.pop_scaling = pop_scaling\n        self.pop_size = int(self.pop_scaling + 3 * np.log(dim))\n        self.population = None\n        self.fitness = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.F_min = 0.1\n        self.F_max = 1.0\n        self.Cr_min = 0.1\n        self.Cr_max = 0.9\n        self.archive = []\n        self.archive_size = archive_size\n        self.success_F = []\n        self.success_Cr = []\n        self.memory_size = 10\n        self.memory_F = np.full(self.memory_size, F_initial)\n        self.memory_Cr = np.full(self.memory_size, Cr_initial)\n        self.memory_idx = 0\n        self.p_archive = 0.1\n        self.neighborhood_size = neighborhood_size\n        self.restart_trigger = restart_trigger  # Fraction of budget to consider stagnation\n        self.initial_diversity = None\n        self.stagnation_counter = 0\n        self.max_stagnation = 20 # Number of iterations to allow stagnation before restart\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.initial_diversity = self.calculate_diversity()\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.population, axis=0)\n        distances = np.linalg.norm(self.population - mean_position, axis=1)\n        diversity = np.mean(distances)\n        return diversity\n\n    def adjust_parameters(self):\n        if self.success_F:\n            self.F = np.sum(np.array(self.success_F)**2) / np.sum(self.success_F) if np.sum(self.success_F) > 0 else 0.5\n            self.Cr = np.mean(self.success_Cr)\n        else:\n            self.F = self.memory_F[self.memory_idx]\n            self.Cr = self.memory_Cr[self.memory_idx]\n\n        self.F = np.clip(self.F, self.F_min, self.F_max)\n        self.Cr = np.clip(self.Cr, self.Cr_min, self.Cr_max)\n\n    def mutate(self, pop, i):\n        # Neighborhood-based mutation\n        indices = [idx for idx in range(len(pop)) if idx != i]\n        neighborhood_indices = np.random.choice(indices, min(self.neighborhood_size, len(indices)), replace=False)\n        \n        if len(self.archive) > 0 and np.random.rand() < self.p_archive:\n            arch_ind = np.random.randint(0, len(self.archive))\n            a = self.archive[arch_ind]\n            \n            remaining_indices = [idx for idx in indices if idx not in neighborhood_indices]\n            if remaining_indices:\n                b_index = np.random.choice(remaining_indices)\n            else:\n                b_index = np.random.choice(neighborhood_indices)\n            b = pop[b_index]\n        else:\n             a = pop[np.random.choice(neighborhood_indices)]\n             remaining_indices = [idx for idx in indices if idx not in neighborhood_indices]\n             if remaining_indices:\n                b_index = np.random.choice(remaining_indices)\n             else:\n                b_index = np.random.choice(neighborhood_indices)\n             b = pop[b_index]\n\n        cauchy_step = self.F * np.random.standard_cauchy(size=self.dim)\n        mutation = a + cauchy_step * (b - pop[i]) # Using Cauchy distribution for step size\n        return mutation\n\n    def mirrored_boundary_handling(self, mutant):\n        for i in range(self.dim):\n            if mutant[i] < self.bounds_lb[i]:\n                mutant[i] = self.bounds_lb[i] + (self.bounds_lb[i] - mutant[i])\n            elif mutant[i] > self.bounds_ub[i]:\n                mutant[i] = self.bounds_ub[i] - (mutant[i] - self.bounds_ub[i])\n        return mutant\n\n    def crossover(self, mutant, target):\n        trial = target.copy()\n        for i in range(self.dim):\n            if np.random.rand() < self.Cr:\n                trial[i] = mutant[i]\n        return trial\n    \n    def restart_population(self, func):\n        \"\"\"Re-initializes the population to enhance exploration.\"\"\"\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.initial_diversity = self.calculate_diversity()\n        self.stagnation_counter = 0\n        self.archive = [] # Clear archive\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n        fevals = self.pop_size\n        self.success_F = []\n        self.success_Cr = []\n\n        while fevals < self.budget:\n            self.adjust_parameters()\n            self.success_F = []\n            self.success_Cr = []\n\n            for i in range(self.pop_size):\n                mutant = self.mutate(self.population, i)\n                mutant = self.mirrored_boundary_handling(mutant)\n                trial = self.crossover(mutant, self.population[i])\n                \n                f = func(trial)\n                fevals += 1\n\n                if f < self.fitness[i]:\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        replace_index = np.random.randint(0, self.archive_size)\n                        self.archive[replace_index] = self.population[i].copy()\n\n                    self.fitness[i] = f\n                    self.population[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                        self.stagnation_counter = 0 # Reset counter upon improvement\n                else:\n                    self.stagnation_counter +=1\n\n                if fevals >= self.budget:\n                    break\n\n            # Diversity Check and Population Restart\n            current_diversity = self.calculate_diversity()\n            if self.initial_diversity is not None and current_diversity < self.restart_trigger * self.initial_diversity or self.stagnation_counter > self.max_stagnation:\n                self.restart_population(func)\n\n            # Update Memory\n            if self.success_F:\n                self.memory_F[self.memory_idx] = np.mean(self.success_F)\n                self.memory_Cr[self.memory_idx] = np.mean(self.success_Cr)\n                self.memory_idx = (self.memory_idx + 1) % self.memory_size\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm NeighborhoodAdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ce9efe8e-cb8d-4211-b86d-3f7ace28dc13"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "467bc73f-b71d-4ec9-803a-83ec4d70e140", "fitness": 0.0, "name": "OrthogonalAdaptiveDE", "description": "An Adaptive Differential Evolution with a cauchy-distributed step size and orthogonal crossover, which enhances exploration by dynamically adjusting the crossover rate and mutation factor and exploits promising regions by integrating orthogonal design for efficient search.", "code": "import numpy as np\n\nclass OrthogonalAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, F_initial=0.5, Cr_initial=0.9, archive_size=5, pop_scaling=4, orthogonal_samples=5):\n        self.budget = budget\n        self.dim = dim\n        self.F = F_initial\n        self.Cr = Cr_initial\n        self.pop_scaling = pop_scaling\n        self.pop_size = int(self.pop_scaling * dim)  # Scale population size by dimension\n        self.population = None\n        self.fitness = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.F_min = 0.1\n        self.F_max = 1.0\n        self.Cr_min = 0.1\n        self.Cr_max = 0.9\n        self.archive = []\n        self.archive_size = archive_size\n        self.success_F = []\n        self.success_Cr = []\n        self.memory_size = 10\n        self.memory_F = np.full(self.memory_size, F_initial)\n        self.memory_Cr = np.full(self.memory_size, Cr_initial)\n        self.memory_idx = 0\n        self.p_archive = 0.1\n        self.orthogonal_samples = orthogonal_samples\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n\n    def adjust_parameters(self):\n        if self.success_F:\n            self.F = np.sum(np.array(self.success_F)**2) / np.sum(self.success_F) if np.sum(self.success_F) > 0 else 0.5\n            self.Cr = np.mean(self.success_Cr)\n        else:\n            self.F = self.memory_F[self.memory_idx]\n            self.Cr = self.memory_Cr[self.memory_idx]\n\n        self.F = np.clip(self.F, self.F_min, self.F_max)\n        self.Cr = np.clip(self.Cr, self.Cr_min, self.Cr_max)\n\n    def mutate(self, pop, i):\n        indices = [idx for idx in range(len(pop)) if idx != i]\n        if len(self.archive) > 0 and np.random.rand() < self.p_archive:\n            arch_ind = np.random.randint(0, len(self.archive))\n            a = self.archive[arch_ind]\n            b_index = np.random.choice(indices)\n            b = pop[b_index]\n        else:\n            choices = np.random.choice(indices, 2, replace=False)\n            a = pop[choices[0]]\n            b = pop[choices[1]]\n\n        cauchy_step = self.F * np.random.standard_cauchy(size=self.dim)\n        mutation = a + cauchy_step * (b - pop[i])\n        return mutation\n\n    def mirrored_boundary_handling(self, mutant):\n        for i in range(self.dim):\n            if mutant[i] < self.bounds_lb[i]:\n                mutant[i] = self.bounds_lb[i] + (self.bounds_lb[i] - mutant[i])\n            elif mutant[i] > self.bounds_ub[i]:\n                mutant[i] = self.bounds_ub[i] - (mutant[i] - self.bounds_ub[i])\n        return mutant\n\n    def crossover(self, mutant, target):\n        trial = target.copy()\n        for i in range(self.dim):\n            if np.random.rand() < self.Cr:\n                trial[i] = mutant[i]\n        return trial\n\n    def orthogonal_crossover(self, func, target):\n        levels = self.orthogonal_samples\n        doe = self.generate_orthogonal_array(self.dim, levels)\n        trials = np.zeros((levels, self.dim))\n\n        for i in range(levels):\n            trial = target.copy()\n            for j in range(self.dim):\n                value_range = self.bounds_ub[j] - self.bounds_lb[j]\n                trial[j] = self.bounds_lb[j] + value_range * doe[i, j] / (levels - 1)\n            trials[i] = trial\n\n        fitness_values = [func(trial) for trial in trials]\n        best_index = np.argmin(fitness_values)\n        return trials[best_index], fitness_values[best_index]\n\n    def generate_orthogonal_array(self, dim, levels):\n        # Simple orthogonal array generation (L_25(5^6) for up to 6 factors at 5 levels)\n        if levels == 2:\n            design = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 0]])\n        elif levels == 3:\n            design = np.array([[0, 0, 0], [0, 1, 1], [0, 2, 2], [1, 0, 1], [1, 1, 2], [1, 2, 0], [2, 0, 2], [2, 1, 0], [2, 2, 1]])\n        elif levels == 4:\n            design = np.array([[0, 0, 0], [0, 1, 1], [0, 2, 2], [0, 3, 3], [1, 0, 1], [1, 1, 0], [1, 2, 3], [1, 3, 2], [2, 0, 2], [2, 1, 3], [2, 2, 0], [2, 3, 1], [3, 0, 3], [3, 1, 2], [3, 2, 1], [3, 3, 0]])\n        elif levels == 5:\n            design = np.array([[0, 0, 0, 0, 0, 0], [0, 1, 1, 1, 1, 1], [0, 2, 2, 2, 2, 2], [0, 3, 3, 3, 3, 3], [0, 4, 4, 4, 4, 4],\n                               [1, 0, 1, 2, 3, 4], [1, 1, 0, 3, 4, 2], [1, 2, 3, 4, 2, 0], [1, 3, 4, 2, 0, 3], [1, 4, 2, 0, 3, 4],\n                               [2, 0, 2, 4, 1, 3], [2, 1, 3, 2, 0, 4], [2, 2, 4, 0, 3, 1], [2, 3, 1, 3, 4, 0], [2, 4, 0, 1, 4, 2],\n                               [3, 0, 3, 1, 4, 2], [3, 1, 4, 0, 2, 3], [3, 2, 1, 4, 0, 4], [3, 3, 0, 2, 3, 1], [3, 4, 4, 3, 1, 0],\n                               [4, 0, 4, 3, 2, 1], [4, 1, 2, 4, 3, 0], [4, 2, 0, 1, 4, 3], [4, 3, 3, 0, 1, 4], [4, 4, 1, 2, 0, 2]])\n        else:\n            raise ValueError(\"Levels must be 2, 3, 4, or 5\")\n        \n        # If dimension is larger than what the array can handle, truncate\n        if dim > design.shape[1]:\n            design = design[:, :dim]\n        return design[:levels, :dim]\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n        fevals = self.pop_size\n        self.success_F = []\n        self.success_Cr = []\n\n        while fevals < self.budget:\n            self.adjust_parameters()\n            self.success_F = []\n            self.success_Cr = []\n\n            for i in range(self.pop_size):\n                mutant = self.mutate(self.population, i)\n                mutant = self.mirrored_boundary_handling(mutant)\n                trial = self.crossover(mutant, self.population[i])\n\n                # Apply orthogonal crossover with a probability\n                if np.random.rand() < 0.5:\n                    trial, f = self.orthogonal_crossover(func, self.population[i])\n                    fevals += self.orthogonal_samples\n                else:\n                    f = func(trial)\n                    fevals += 1\n\n                if f < self.fitness[i]:\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        replace_index = np.random.randint(0, self.archive_size)\n                        self.archive[replace_index] = self.population[i].copy()\n\n                    self.fitness[i] = f\n                    self.population[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n\n                if fevals >= self.budget:\n                    break\n\n            # Update Memory\n            if self.success_F:\n                self.memory_F[self.memory_idx] = np.mean(self.success_F)\n                self.memory_Cr[self.memory_idx] = np.mean(self.success_Cr)\n                self.memory_idx = (self.memory_idx + 1) % self.memory_size\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm OrthogonalAdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ce9efe8e-cb8d-4211-b86d-3f7ace28dc13"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "421c6573-910e-4246-9e6f-0be9adee1fce", "fitness": 0.5984724176195166, "name": "CMAES_Aging_OrthoInit", "description": "Covariance Matrix Adaptation Evolution Strategy with an aging mechanism and population diversity enhancement through orthogonal initialization.", "code": "import numpy as np\n\nclass CMAES_Aging_OrthoInit:\n    def __init__(self, budget=10000, dim=10, pop_size=None, aging_rate=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else int(4 + 3 * np.log(dim))\n        self.mu = self.pop_size // 2  # Number of parents\n        self.c_m = 1  # Learning rate for mean\n        self.c_sigma = None  # Learning rate for step size\n        self.c_c = None # Learning rate for covariance matrix\n        self.damps = None\n\n        self.mean = None\n        self.sigma = 0.1\n        self.C = None\n\n        self.eigen_decomposition_counter = 0\n        self.eigen_decomposition_frequency = 10\n        self.P_sigma = None\n        self.P_c = None\n        self.c_1 = None\n        self.c_mu = None\n\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.fevals = 0\n\n        self.aging_rate = aging_rate\n        self.age = None\n\n    def initialize_population(self, func):\n        # Orthogonal initialization for better diversity\n        H = np.array([[1, 1], [1, -1]])\n        if self.dim > H.shape[1]:\n            H_repeated = np.tile(H, (1, int(np.ceil(self.dim / H.shape[1]))))\n            H_used = H_repeated[:, :self.dim]\n        else:\n            H_used = H[:, :self.dim]\n            \n        self.mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n\n        self.population = np.zeros((self.pop_size, self.dim))\n        for i in range(self.pop_size):\n            z = np.random.normal(0, 1, size=self.dim)\n            x = self.mean + self.sigma * z\n            self.population[i,:] = np.clip(x, func.bounds.lb, func.bounds.ub) # Clip to bounds\n           \n        self.fitness = np.array([func(x) for x in self.population])\n        self.fevals += self.pop_size\n        self.age = np.zeros(self.pop_size)\n\n    def initialize_CMA_parameters(self):\n        self.C = np.eye(self.dim)\n        self.P_sigma = np.zeros(self.dim)\n        self.P_c = np.zeros(self.dim)\n\n        self.c_sigma = (self.mu + 2) / (self.dim + self.mu + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1) + self.c_sigma\n\n        self.c_1 = 2 / ((self.dim + 1.3)**2 + self.mu)\n        self.c_mu = min(1 - self.c_1, 2 * (self.mu - 1 + 1) / ((self.dim + 2)**2 + self.mu))\n\n    def sample_population(self):\n        z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n        C_sqrt = np.linalg.cholesky(self.C)  # Cholesky decomposition\n        self.population = self.mean + self.sigma * z @ C_sqrt.T\n\n        # Clip to bounds\n        self.population = np.clip(self.population, self.bounds_lb, self.bounds_ub)\n\n    def update_parameters(self):\n        # Sort the population and fitness based on fitness\n        sorted_indices = np.argsort(self.fitness)\n        self.population = self.population[sorted_indices]\n        self.fitness = self.fitness[sorted_indices]\n\n        # Select parents\n        parents = self.population[:self.mu]\n\n        # Update the mean\n        old_mean = self.mean.copy()\n        self.mean = np.mean(parents, axis=0)\n        \n        # Update evolution paths\n        z = (self.mean - old_mean) / self.sigma\n        C_sqrt = np.linalg.cholesky(self.C)\n\n        z_in_C_space = np.linalg.solve(C_sqrt.T, z)\n        \n        self.P_sigma = (1 - self.c_sigma) * self.P_sigma + np.sqrt(self.c_sigma * (2 - self.c_sigma)) * z_in_C_space\n        hsig = np.linalg.norm(self.P_sigma) / np.sqrt(1 - (1 - self.c_sigma)**(2 * (self.fevals / self.pop_size))) < (1.4 + 2 / (self.dim + 1))\n\n        self.P_c = (1 - self.c_c) * self.P_c + hsig * np.sqrt(self.c_c * (2 - self.c_c)) * z_in_C_space\n\n        # Update covariance matrix\n        y = parents - old_mean\n        \n        delta_C = np.zeros_like(self.C)\n        for i in range(self.mu):\n            weight = self.c_mu\n            dy = (parents[i] - old_mean) / self.sigma\n            delta_C += weight * np.outer(dy, dy)\n        \n        self.C = (1 - self.c_1 - self.c_mu) * self.C + self.c_1 * np.outer(self.P_c, self.P_c) + delta_C\n\n        # Update step size\n        self.sigma *= np.exp((self.c_sigma / self.damps) * (np.linalg.norm(self.P_sigma) / np.sqrt(self.dim) - 1))\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n        self.initialize_population(func)\n        self.initialize_CMA_parameters()\n        self.c_c = (4 + self.mu / self.dim) / (self.dim + 4 + 2 * self.mu / self.dim)\n        \n        while self.fevals < self.budget:\n            # Aging mechanism: Degrade fitness over time\n            for i in range(self.pop_size):\n                self.fitness[i] *= (1 + self.aging_rate * self.age[i])\n\n            # Generate new samples\n            self.sample_population()\n\n            # Evaluate the fitness of the new samples\n            new_fitness = np.array([func(x) for x in self.population])\n            self.fevals += self.pop_size\n            \n            # Check budget\n            if self.fevals > self.budget:\n                new_fitness = new_fitness[:self.budget - (self.fevals - self.pop_size)]\n                self.population = self.population[:self.budget - (self.fevals - self.pop_size)]\n                break\n\n            self.fitness = new_fitness\n            \n            # Update parameters\n            self.update_parameters()\n            \n            # Update age\n            self.age += 1\n            \n            # Reset age of best individual\n            best_index = np.argmin(self.fitness)\n            self.age[best_index] = 0\n\n            # Update optimal solution\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index]\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm CMAES_Aging_OrthoInit scored 0.598 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["730cb02b-d718-4694-a376-520da6605807"], "operator": null, "metadata": {"aucs": [0.19226818668400236, 0.3502401491710113, 0.6899791331746084, 0.9337798247616235, 0.5477432134539368, 0.8679983778215707, 0.3500514588003768, 0.6438390080587957, 0.8653205139920668, 0.6758464160576314, 0.6046548793573849, 0.9615518920030909, 0.28943336191126134, 0.40717945010956624, 0.7676133459929396, 0.8170564694140806, 0.35261334218387186, 0.8920861552323276, 0.27382920622197215, 0.48636396798821346]}}
{"id": "d3288946-96ba-41e7-aa09-9ad91ec45747", "fitness": 0.40657991126129317, "name": "AdaptiveDE_MirrorSampling_Jitter", "description": "Improved Adaptive DE with a mirrored sampling strategy around the best solution, enhanced parameter adaptation, and jittering to boost exploration capabilities.", "code": "import numpy as np\n\nclass AdaptiveDE_MirrorSampling_Jitter:\n    def __init__(self, budget=10000, dim=10, archive_size=5, memory_size=5, pop_multiplier=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = int(4 + pop_multiplier * np.log(budget))  # Population size scaling\n        self.population = None\n        self.fitness = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.archive = []  # Archive for storing removed individuals\n        self.archive_size = archive_size\n        self.memory_size = memory_size\n        self.memory_F = np.full(self.memory_size, 0.5)\n        self.memory_Cr = np.full(self.memory_size, 0.5)\n        self.sf = []\n        self.scr = []\n        self.p = 0.1  # Probability of using archive individual\n        self.w = 0.5  # Weight for Lehmer mean update\n        self.best_idx = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.best_idx = np.argmin(self.fitness)\n\n    def mutate(self, pop, i, current_generation):\n        memory_index = current_generation % self.memory_size\n        Cr = self.memory_Cr[memory_index]\n        F = self.memory_F[memory_index]\n\n        indices = [idx for idx in range(len(pop)) if idx != i]\n        if len(self.archive) > 0 and np.random.rand() < self.p:\n            arch_ind = np.random.randint(0, len(self.archive))\n            pop_indices = np.random.choice(indices, 2, replace=False)\n            a = self.archive[arch_ind]\n            b, c = pop[pop_indices[0]], pop[pop_indices[1]]\n        else:\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n\n        # Mirror sampling around the best solution\n        if np.random.rand() < 0.2:\n            a = self.population[self.best_idx] + np.random.normal(0, 0.1, self.dim) * (self.bounds_ub - self.bounds_lb) # Add noise\n\n        mutation = a + F * (b - c)\n        return mutation, Cr, F\n\n    def toroidal_boundary_handling(self, mutant):\n        # Apply toroidal boundary handling\n        for i in range(self.dim):\n            width = self.bounds_ub[i] - self.bounds_lb[i]\n            if mutant[i] < self.bounds_lb[i]:\n                mutant[i] = self.bounds_ub[i] - (self.bounds_lb[i] - mutant[i]) % width\n            elif mutant[i] > self.bounds_ub[i]:\n                mutant[i] = self.bounds_lb[i] + (mutant[i] - self.bounds_ub[i]) % width\n        return mutant\n\n    def mirrored_boundary_handling(self, mutant):\n        # Apply mirrored boundary handling\n        for i in range(self.dim):\n            if mutant[i] < self.bounds_lb[i]:\n                mutant[i] = self.bounds_lb[i] + (self.bounds_lb[i] - mutant[i])\n            elif mutant[i] > self.bounds_ub[i]:\n                mutant[i] = self.bounds_ub[i] - (mutant[i] - self.bounds_ub[i])\n        return mutant\n\n    def crossover(self, mutant, target, Cr):\n        cross_points = np.random.rand(self.dim) < Cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n\n        # Jittering: Add small random perturbation\n        if np.random.rand() < 0.1:\n            trial += np.random.normal(0, 0.01, self.dim) * (self.bounds_ub - self.bounds_lb)\n            trial = np.clip(trial, self.bounds_lb, self.bounds_ub)\n\n        return trial\n\n    def update_memory(self):\n        if self.sf:\n            self.sf = np.array(self.sf)\n            self.scr = np.array(self.scr)\n\n            # Lehmer mean for F\n            sum_sf_squared = np.sum(self.sf ** 2)\n            sum_sf = np.sum(self.sf)\n            if sum_sf > 0:\n                lehmer_mean_F = sum_sf_squared / sum_sf\n            else:\n                lehmer_mean_F = 0.5  # Default value if no successful F values\n\n            # Exponential smoothing for Cr\n            lehmer_mean_Cr = np.mean(self.scr)\n\n            memory_index = self.generation % self.memory_size\n            self.memory_F[memory_index] = self.w * self.memory_F[memory_index] + (1 - self.w) * lehmer_mean_F\n            self.memory_Cr[memory_index] = self.w * self.memory_Cr[memory_index] + (1 - self.w) * lehmer_mean_Cr\n\n        self.sf = []\n        self.scr = []\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n        fevals = self.pop_size\n        self.generation = 0\n        self.sf = []\n        self.scr = []\n\n        while fevals < self.budget:\n            self.generation += 1\n            for i in range(self.pop_size):\n                mutant, Cr, F = self.mutate(self.population, i, self.generation)\n                mutant = self.toroidal_boundary_handling(mutant)  # Toroidal boundary handling\n                trial = self.crossover(mutant, self.population[i], Cr)\n\n                f = func(trial)\n                fevals += 1\n\n                if f < self.fitness[i]:\n                    # Archive the replaced individual\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        # Replace a random element in the archive\n                        replace_index = np.random.randint(0, self.archive_size)\n                        self.archive[replace_index] = self.population[i].copy()\n\n                    self.sf.append(F)\n                    self.scr.append(Cr)\n                    self.fitness[i] = f\n                    self.population[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                        self.best_idx = i\n\n                if fevals >= self.budget:\n                    break\n            self.update_memory()\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveDE_MirrorSampling_Jitter scored 0.407 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b5f53273-1fd3-4232-b8aa-7572ba9cd3a0"], "operator": null, "metadata": {"aucs": [0.1637031436080788, 0.245354698091222, 0.3912594558032053, 0.6044740452465256, 0.33431703668982116, 0.45092673643603776, 0.3099252482792504, 0.35700609020970153, 0.3541634527428882, 0.19597104285668643, 0.3792090522480511, 0.9971151714860497, 0.2791512966835731, 0.33272917454058937, 0.7412638268973077, 0.45926873344595054, 0.34192025859171604, 0.5055095274531145, 0.19120860636482606, 0.49712162755126843]}}
{"id": "504cc04a-9022-43fd-80df-d2ddae74a75b", "fitness": 0.40974777361749837, "name": "EnhancedAdaptiveDE", "description": "An enhanced Adaptive Differential Evolution incorporates a diversity-guided parameter adaptation, elitist JADE mutation, and a combined archive and population-based exploration strategy.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, F_initial=0.5, Cr_initial=0.9, archive_size=5, pop_scaling=4, p_best=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.F = F_initial\n        self.Cr = Cr_initial\n        self.pop_scaling = pop_scaling\n        self.pop_size = int(self.pop_scaling + 3 * np.log(dim))\n        self.population = None\n        self.fitness = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.F_min = 0.1\n        self.F_max = 1.0\n        self.Cr_min = 0.1\n        self.Cr_max = 0.9\n        self.archive = []\n        self.archive_size = archive_size\n        self.success_F = []\n        self.success_Cr = []\n        self.memory_size = 10\n        self.memory_F = np.full(self.memory_size, F_initial)\n        self.memory_Cr = np.full(self.memory_size, Cr_initial)\n        self.memory_idx = 0\n        self.p_archive = 0.1\n        self.p_best = p_best\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.population, axis=0)\n        distances = np.linalg.norm(self.population - mean_position, axis=1)\n        diversity = np.mean(distances)\n        return diversity\n\n    def adjust_parameters(self, diversity):\n        # Diversity-guided parameter adaptation\n        if diversity > 0.5: # Higher diversity indicates more exploration needed\n            self.F = np.random.uniform(0.5, 1.0)\n            self.Cr = np.random.uniform(0.7, 0.9)\n        else: # Lower diversity indicates exploitation\n            if self.success_F:\n                self.F = np.sum(np.array(self.success_F)**2) / np.sum(self.success_F) if np.sum(self.success_F) > 0 else 0.5\n                self.Cr = np.mean(self.success_Cr)\n            else:\n                self.F = self.memory_F[self.memory_idx]\n                self.Cr = self.memory_Cr[self.memory_idx]\n\n        self.F = np.clip(self.F, self.F_min, self.F_max)\n        self.Cr = np.clip(self.Cr, self.Cr_min, self.Cr_max)\n\n    def mutate(self, pop, i):\n        # Elitist JADE-style mutation\n        p_best_indices = np.argsort(self.fitness)[:max(1, int(self.p_best * self.pop_size))]\n        x_pbest = pop[np.random.choice(p_best_indices)]\n        \n        indices = [idx for idx in range(len(pop)) if idx != i]\n\n        if len(self.archive) > 0 and np.random.rand() < self.p_archive:\n            arch_ind = np.random.randint(0, len(self.archive))\n            a = self.archive[arch_ind]\n            \n            remaining_indices = indices\n            if remaining_indices:\n                b_index = np.random.choice(remaining_indices)\n            else:\n                 b_index = np.random.choice(indices) #Should not happen\n            b = pop[b_index]\n        else:\n            a_index = np.random.choice(indices)\n            b_index = np.random.choice([idx for idx in indices if idx != a_index])\n\n            a = pop[a_index]\n            b = pop[b_index]\n\n        mutation = pop[i] + self.F * (x_pbest - pop[i]) + self.F * (a - b)\n        return mutation\n\n    def mirrored_boundary_handling(self, mutant):\n        for i in range(self.dim):\n            if mutant[i] < self.bounds_lb[i]:\n                mutant[i] = self.bounds_lb[i] + (self.bounds_lb[i] - mutant[i])\n            elif mutant[i] > self.bounds_ub[i]:\n                mutant[i] = self.bounds_ub[i] - (mutant[i] - self.bounds_ub[i])\n        return mutant\n\n    def crossover(self, mutant, target):\n        trial = target.copy()\n        j_rand = np.random.randint(self.dim)\n        for i in range(self.dim):\n            if np.random.rand() < self.Cr or i == j_rand:\n                trial[i] = mutant[i]\n        return trial\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n        fevals = self.pop_size\n        self.success_F = []\n        self.success_Cr = []\n\n        while fevals < self.budget:\n            diversity = self.calculate_diversity()\n            self.adjust_parameters(diversity)\n            self.success_F = []\n            self.success_Cr = []\n\n            for i in range(self.pop_size):\n                mutant = self.mutate(self.population, i)\n                mutant = self.mirrored_boundary_handling(mutant)\n                trial = self.crossover(mutant, self.population[i])\n                \n                f = func(trial)\n                fevals += 1\n\n                if f < self.fitness[i]:\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n\n                    # Combined archive and population update\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        if np.random.rand() < 0.5: #50% probability to use the archive for population replacement, enhancing exploration\n                            replace_index = np.random.randint(0, self.archive_size)\n                            self.population[i] = self.archive[replace_index].copy()\n                            self.fitness[i] = func(self.population[i]) #re-evaluate the fitness value\n                            fevals += 1\n\n                        replace_index = np.random.randint(0, self.archive_size)\n                        self.archive[replace_index] = self.population[i].copy()\n\n\n                    self.fitness[i] = f\n                    self.population[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n\n                if fevals >= self.budget:\n                    break\n\n            # Update Memory\n            if self.success_F:\n                self.memory_F[self.memory_idx] = np.mean(self.success_F)\n                self.memory_Cr[self.memory_idx] = np.mean(self.success_Cr)\n                self.memory_idx = (self.memory_idx + 1) % self.memory_size\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm EnhancedAdaptiveDE scored 0.410 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ce9efe8e-cb8d-4211-b86d-3f7ace28dc13"], "operator": null, "metadata": {"aucs": [0.2254901829159911, 0.26305340967317115, 0.37857326950691195, 0.9017738451322413, 0.24675148527345425, 0.3091343855473808, 0.27572764577537556, 0.2815800342213354, 0.23475020972433214, 0.2514410044194134, 0.4052996658242197, 0.99940810020174, 0.23264171929785316, 0.21774465310613078, 0.672086213430827, 0.312274578267944, 0.33321905365223436, 0.9374921549655078, 0.23172336060931498, 0.4847905008045885]}}
{"id": "1ca9a63d-ddc0-46cc-8792-cbb02e7c6b94", "fitness": 0.690191693815313, "name": "AdaptiveDE_ReducePop_Reflecting", "description": "Adaptive Differential Evolution with a self-adaptive strategy for population size reduction, combined with a weighted Lehmer mean for F/CR adaptation and a reflecting boundary handling technique.", "code": "import numpy as np\n\nclass AdaptiveDE_ReducePop_Reflecting:\n    def __init__(self, budget=10000, dim=10, archive_size=5, memory_size=5, initial_pop_size=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.initial_pop_size = initial_pop_size\n        self.population = None\n        self.fitness = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.archive = []  # Archive for storing removed individuals\n        self.archive_size = archive_size\n        self.memory_size = memory_size\n        self.memory_F = np.full(self.memory_size, 0.5)\n        self.memory_Cr = np.full(self.memory_size, 0.5)\n        self.sf = []\n        self.scr = []\n        self.p = 0.1  # Probability of using archive individual\n        self.w = 0.5  # Weight for Lehmer mean update\n        self.reduction_factor = 0.95 # Factor for population size reduction\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n\n    def mutate(self, pop, i, current_generation):\n        memory_index = current_generation % self.memory_size\n        Cr = self.memory_Cr[memory_index]\n        F = self.memory_F[memory_index]\n\n        indices = [idx for idx in range(len(pop)) if idx != i]\n        if len(self.archive) > 0 and np.random.rand() < self.p:\n            arch_ind = np.random.randint(0, len(self.archive))\n            pop_indices = np.random.choice(indices, 2, replace=False)\n            a = self.archive[arch_ind]\n            b, c = pop[pop_indices[0]], pop[pop_indices[1]]\n        else:\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n        \n        mutation = a + F * (b - c)\n        return mutation, Cr, F\n\n    def reflecting_boundary_handling(self, mutant):\n        # Apply reflecting boundary handling\n        for i in range(self.dim):\n            if mutant[i] < self.bounds_lb[i]:\n                mutant[i] = self.bounds_lb[i] + np.abs(mutant[i] - self.bounds_lb[i])\n            elif mutant[i] > self.bounds_ub[i]:\n                mutant[i] = self.bounds_ub[i] - np.abs(mutant[i] - self.bounds_ub[i])\n\n            # Ensure the value remains within the bounds after reflection\n            mutant[i] = np.clip(mutant[i], self.bounds_lb[i], self.bounds_ub[i])\n        return mutant\n\n    def crossover(self, mutant, target, Cr):\n        cross_points = np.random.rand(self.dim) < Cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def update_memory(self):\n        if self.sf:\n            self.sf = np.array(self.sf)\n            self.scr = np.array(self.scr)\n\n            # Lehmer mean for F\n            sum_sf_squared = np.sum(self.sf ** 2)\n            sum_sf = np.sum(self.sf)\n            if sum_sf > 0:\n                lehmer_mean_F = sum_sf_squared / sum_sf\n            else:\n                lehmer_mean_F = 0.5  # Default value if no successful F values\n\n            # Lehmer mean for Cr\n            lehmer_mean_Cr = np.mean(self.scr)\n\n            memory_index = self.generation % self.memory_size\n            self.memory_F[memory_index] = self.w * self.memory_F[memory_index] + (1 - self.w) * lehmer_mean_F\n            self.memory_Cr[memory_index] = self.w *  self.memory_Cr[memory_index] + (1 - self.w) * lehmer_mean_Cr\n\n        self.sf = []\n        self.scr = []\n\n    def reduce_population(self):\n        if self.pop_size > 10: # Minimum population size\n             # Sort population based on fitness\n            sorted_indices = np.argsort(self.fitness)[::-1]\n            num_to_remove = max(1, int(self.pop_size * (1 - self.reduction_factor)))  # Reduce at least by 1\n            \n            # Archive the individuals to be removed\n            for i in range(num_to_remove):\n                if len(self.archive) < self.archive_size:\n                    self.archive.append(self.population[sorted_indices[i]].copy())\n                else:\n                    replace_index = np.random.randint(0, self.archive_size)\n                    self.archive[replace_index] = self.population[sorted_indices[i]].copy()\n\n            # Keep the best individuals\n            keep_indices = sorted_indices[num_to_remove:]\n            self.population = self.population[keep_indices]\n            self.fitness = self.fitness[keep_indices]\n            self.pop_size = len(self.population)\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n        fevals = self.pop_size\n        self.generation = 0\n        self.sf = []\n        self.scr = []\n\n        while fevals < self.budget:\n            self.generation += 1\n            for i in range(self.pop_size):\n                mutant, Cr, F = self.mutate(self.population, i, self.generation)\n                mutant = self.reflecting_boundary_handling(mutant)  # Reflecting boundary handling\n                trial = self.crossover(mutant, self.population[i], Cr)\n                \n                f = func(trial)\n                fevals += 1\n\n                if f < self.fitness[i]:\n                    # Archive the replaced individual\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        # Replace a random element in the archive\n                        replace_index = np.random.randint(0, self.archive_size)\n                        self.archive[replace_index] = self.population[i].copy()\n\n                    self.sf.append(F)\n                    self.scr.append(Cr)\n                    self.fitness[i] = f\n                    self.population[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                \n                if fevals >= self.budget:\n                    break\n\n            self.update_memory()\n\n            # Reduce population size every few generations (e.g., every 5 generations)\n            if self.generation % 5 == 0:\n                self.reduce_population()\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveDE_ReducePop_Reflecting scored 0.690 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b5f53273-1fd3-4232-b8aa-7572ba9cd3a0"], "operator": null, "metadata": {"aucs": [0.2854615338527575, 0.6220410248565311, 0.7116586788794712, 0.8369185664032455, 0.7363744068970799, 0.8083094680880714, 0.6492648737277175, 0.6986982861620527, 0.7518892764294799, 0.6623118949211979, 0.7900526750678063, 0.9981170493920734, 0.3237420955525968, 0.7287624790019012, 0.8716824335336505, 0.796708041037285, 0.5715478583353067, 0.8246100598818035, 0.6110882697305846, 0.5245949045556448]}}
{"id": "c6e5bc64-3827-4b00-976f-bb47b493ee62", "fitness": 0.6412887405041322, "name": "ImprovedSHADE_Ortho_AdaptPop_DBE", "description": "Introduces a Distance-Based Exploration (DBE) strategy that encourages diversity by biasing mutation towards individuals that are distant from the current population, potentially improving exploration and escaping local optima, combined with success-history based parameter adaptation.", "code": "import numpy as np\n\nclass ImprovedSHADE_Ortho_AdaptPop_DBE:\n    def __init__(self, budget=10000, dim=10, memory_size=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = int(4 + 3 * np.log(budget))  # Initial population size\n        self.memory_size = memory_size\n        self.memory_F = np.full(memory_size, 0.5)\n        self.memory_CR = np.full(memory_size, 0.5)\n        self.p = 0.1  # Percentage of top individuals for p-best selection\n        self.population = None\n        self.fitness = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.archive = []  # For storing inferior solutions\n        self.archive_size = self.pop_size  # Archive size, can be adjusted\n        self.memory_index = 0\n        self.sf_M = []\n        self.scr_M = []\n        self.fevals = 0  # Track function evaluations\n        self.min_pop_size = 4  # Minimum population size\n        self.reduction_trigger = 0.25 #Fraction of budget before reduction starts\n        self.reduction_rate = 0.2 #Fraction of population reduced each time\n        self.use_orthogonal = True #Use orthogonal crossover\n        self.dbe_probability = 0.1 # Probability of using Distance-Based Exploration\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.fevals += self.pop_size\n\n    def calculate_distances(self, pop, i):\n        \"\"\"Calculates Euclidean distances from individual i to all other individuals.\"\"\"\n        distances = np.linalg.norm(pop - pop[i], axis=1)\n        distances[i] = np.inf  # Exclude distance to itself\n        return distances\n\n    def mutate(self, pop, i, F, Cr, archive):\n        if np.random.rand() < self.dbe_probability:\n             # Distance-Based Exploration\n            distances = self.calculate_distances(pop, i)\n            r1 = np.argmin(distances) #Farthest Individual\n\n        else:\n\n            p_best_size = max(int(self.p * len(pop)), 1)\n\n            # Tournament selection for p-best\n            indices = np.argsort(self.fitness)[:p_best_size]\n            p_best_idx = np.random.choice(indices)\n            p_best = pop[p_best_idx]\n            indices = [idx for idx in range(len(pop)) if idx != i]\n            if len(indices) == 0:\n                return pop[i]\n            r1 = np.random.choice(indices)\n        \n        # Choose r2 from current pop or archive\n        if len(archive) > 0 and np.random.rand() < 0.5:  # 50% chance to use archive\n            r2_idx = np.random.randint(0, len(archive))\n            r2 = archive[r2_idx]\n        else:\n            indices = [idx for idx in range(len(pop)) if idx != i and idx != r1] #Ensure distinct indices\n            if len(indices) == 0:\n                r2 = pop[r1] #If only one individual, take that\n            else:\n                r2_idx = np.random.choice(indices)\n                r2 = pop[r2_idx]\n        \n        if np.random.rand() < self.dbe_probability:\n            p_best = pop[np.random.choice(range(len(pop)))] #Choose random individual when using DBE for p_best\n        else:\n            p_best_size = max(int(self.p * len(pop)), 1)\n            # Tournament selection for p-best\n            indices = np.argsort(self.fitness)[:p_best_size]\n            p_best_idx = np.random.choice(indices)\n            p_best = pop[p_best_idx]\n\n        mutant = p_best + F * (pop[r1] - r2)\n        return mutant\n\n    def orthogonal_crossover(self, mutant, target):\n        # Generate an orthogonal array (example using a simple Hadamard matrix)\n        H = np.array([[1, 1], [1, -1]])\n\n        # Ensure dimensions are compatible by repeating or truncating\n        if self.dim > H.shape[1]:\n            H_repeated = np.tile(H, (1, int(np.ceil(self.dim / H.shape[1]))))\n            H_used = H_repeated[:, :self.dim]\n        else:\n            H_used = H[:, :self.dim]\n\n        trial = np.copy(target)\n\n        for j in range(self.dim):\n            if H_used[0, j] == H_used[1, j]:\n                continue  # No change\n            else:\n                trial[j] = mutant[j]\n\n        return trial\n\n    def crossover(self, mutant, target, Cr):\n        cross_points = np.random.rand(self.dim) < Cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def update_archive(self, individual, archive):\n        archive.append(individual)\n        return archive\n\n    def reduce_population(self):\n        if self.pop_size > self.min_pop_size:\n            sorted_indices = np.argsort(self.fitness)[::-1]  # Worst to best\n            reduction_amount = max(1, int(self.pop_size * self.reduction_rate))  # Reduce by reduction_rate, but at least 1\n            removed_indices = sorted_indices[:reduction_amount]\n\n            self.population = np.delete(self.population, removed_indices, axis=0)\n            self.fitness = np.delete(self.fitness, removed_indices)\n            self.pop_size = len(self.population)\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n        self.archive = []\n        \n        self.sf_M = []\n        self.scr_M = []\n        weights = []\n\n        while self.fevals < self.budget:\n            sf_k = []\n            scr_k = []\n            weights = []\n\n            for i in range(self.pop_size):\n                # Sample F and CR from memory\n                mem_idx = np.random.randint(0, self.memory_size)\n                F = self.memory_F[mem_idx]\n                Cr = self.memory_CR[mem_idx]\n\n                # Truncate and Reflect F\n                if F > 1:\n                    F = 1\n                if F <= 0:\n                    F = 0.000001\n\n                mutant = self.mutate(self.population, i, F, Cr, self.archive)\n                mutant = np.clip(mutant, self.bounds_lb, self.bounds_ub)\n\n                # Use orthogonal crossover with a probability or with the regular crossover\n                if self.use_orthogonal and np.random.rand() < 0.5:  # 50% chance to use orthogonal crossover\n                    trial = self.orthogonal_crossover(mutant, self.population[i])\n                else:\n                    trial = self.crossover(mutant, self.population[i], Cr)\n\n                f = func(trial)\n                self.fevals += 1\n\n                if f < self.fitness[i]:\n                    delta = abs(f - self.fitness[i])\n                    sf_k.append(F)\n                    scr_k.append(Cr)\n                    weights.append(delta)\n\n                    self.archive = self.update_archive(self.population[i], self.archive)\n                    if len(self.archive) > self.archive_size:\n                        self.archive.pop(np.random.randint(0, len(self.archive)))  # Keep archive size constant\n\n                    self.fitness[i] = f\n                    self.population[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                else:\n                    self.archive = self.update_archive(trial, self.archive)  # Add even unsuccessful trials\n                    if len(self.archive) > self.archive_size:\n                        self.archive.pop(np.random.randint(0, len(self.archive)))\n\n                if self.fevals >= self.budget:\n                    break\n\n            # Update memory using Lehmer mean\n            if sf_k:\n                weights = np.array(weights) / np.sum(weights)\n                sum_weights_sfk = np.sum(weights * np.array(sf_k) ** 2)\n                sum_weights = np.sum(weights * np.array(sf_k))\n\n                if sum_weights > 0:\n                    mean_F = sum_weights_sfk / sum_weights\n                else:\n                    mean_F = 0.5\n\n                mean_CR = np.average(scr_k, weights=weights)\n\n                self.memory_F[self.memory_index] = mean_F\n                self.memory_CR[self.memory_index] = mean_CR\n                self.memory_index = (self.memory_index + 1) % self.memory_size\n\n            # Adaptive population size reduction\n            if self.fevals > self.budget * self.reduction_trigger:  # Start reducing after reduction_trigger of the budget is spent\n                self.reduce_population()\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm ImprovedSHADE_Ortho_AdaptPop_DBE scored 0.641 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["348607c7-8345-42c2-a8ed-f2267ea0c7b9"], "operator": null, "metadata": {"aucs": [0.2279920371576768, 0.2549879936116213, 0.5741255542990764, 0.9409411350050774, 0.6713810221033343, 0.8443109272327647, 0.5156340146819771, 0.5783802196018464, 0.7033412093174259, 0.6944412331860434, 0.8797044992153089, 0.9947774424073901, 0.28155121331181443, 0.7203691204142666, 0.9026601465704429, 0.8482302159956936, 0.5537939494559532, 0.8954746964555722, 0.23106384080879105, 0.5126143392505682]}}
{"id": "e1392b59-68b7-4834-87b3-6373847ec856", "fitness": -Infinity, "name": "EnhancedAdaptiveDE", "description": "Enhanced Adaptive Differential Evolution with orthogonal learning, a diversity-guided population scaling, and aging with local search.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, F_initial=0.5, Cr_initial=0.9, archive_size=5, pop_scaling=4, neighborhood_size=5, aging_frequency=100, local_search_radius=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.F = F_initial\n        self.Cr = Cr_initial\n        self.pop_scaling = pop_scaling\n        self.pop_size = int(self.pop_scaling + 3 * np.log(dim))\n        self.population = None\n        self.fitness = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.F_min = 0.1\n        self.F_max = 1.0\n        self.Cr_min = 0.1\n        self.Cr_max = 0.9\n        self.archive = []\n        self.archive_size = archive_size\n        self.success_F = []\n        self.success_Cr = []\n        self.memory_size = 10\n        self.memory_F = np.full(self.memory_size, F_initial)\n        self.memory_Cr = np.full(self.memory_size, Cr_initial)\n        self.memory_idx = 0\n        self.p_archive = 0.1\n        self.neighborhood_size = neighborhood_size\n        self.aging_frequency = aging_frequency\n        self.local_search_radius = local_search_radius\n        self.age = np.zeros(self.pop_size)\n        self.max_age = 50  # Maximum age before local search\n        self.diversity_threshold = 0.01\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n\n    def calculate_diversity(self):\n        mean_position = np.mean(self.population, axis=0)\n        distances = np.linalg.norm(self.population - mean_position, axis=1)\n        diversity = np.mean(distances)\n        return diversity\n\n    def adjust_parameters(self):\n        if self.success_F:\n            self.F = np.sum(np.array(self.success_F)**2) / np.sum(self.success_F) if np.sum(self.success_F) > 0 else 0.5\n            self.Cr = np.mean(self.success_Cr)\n        else:\n            self.F = self.memory_F[self.memory_idx]\n            self.Cr = self.memory_Cr[self.memory_idx]\n\n        self.F = np.clip(self.F, self.F_min, self.F_max)\n        self.Cr = np.clip(self.Cr, self.Cr_min, self.Cr_max)\n\n    def orthogonal_learning(self, i):\n        # Generate an orthogonal matrix\n        H = np.random.randn(self.dim, self.dim)\n        Q, _ = np.linalg.qr(H)\n\n        # Create new individuals based on orthogonal directions\n        new_individuals = []\n        for j in range(self.dim):\n            direction = Q[:, j]\n            step_size = np.random.uniform(-self.local_search_radius, self.local_search_radius)\n            new_x = self.population[i] + step_size * direction\n            new_x = np.clip(new_x, self.bounds_lb, self.bounds_ub)  # Ensure bounds are respected\n            new_individuals.append(new_x)\n\n        # Evaluate new individuals\n        new_fitnesses = [func(x) for x in new_individuals]\n        best_index = np.argmin(new_fitnesses)\n        return new_individuals[best_index], new_fitnesses[best_index]\n\n    def mutate(self, pop, i):\n        # Neighborhood-based mutation\n        indices = [idx for idx in range(len(pop)) if idx != i]\n        neighborhood_indices = np.random.choice(indices, min(self.neighborhood_size, len(indices)), replace=False)\n        \n        if len(self.archive) > 0 and np.random.rand() < self.p_archive:\n            arch_ind = np.random.randint(0, len(self.archive))\n            a = self.archive[arch_ind]\n            \n            remaining_indices = [idx for idx in indices if idx not in neighborhood_indices]\n            if remaining_indices:\n                b_index = np.random.choice(remaining_indices)\n            else:\n                b_index = np.random.choice(neighborhood_indices)\n            b = pop[b_index]\n        else:\n             a = pop[np.random.choice(neighborhood_indices)]\n             remaining_indices = [idx for idx in indices if idx not in neighborhood_indices]\n             if remaining_indices:\n                b_index = np.random.choice(remaining_indices)\n             else:\n                b_index = np.random.choice(neighborhood_indices)\n             b = pop[b_index]\n\n        cauchy_step = self.F * np.random.standard_cauchy(size=self.dim)\n        mutation = a + cauchy_step * (b - pop[i]) # Using Cauchy distribution for step size\n        return mutation\n\n    def mirrored_boundary_handling(self, mutant):\n        for i in range(self.dim):\n            if mutant[i] < self.bounds_lb[i]:\n                mutant[i] = self.bounds_lb[i] + (self.bounds_lb[i] - mutant[i])\n            elif mutant[i] > self.bounds_ub[i]:\n                mutant[i] = self.bounds_ub[i] - (mutant[i] - self.bounds_ub[i])\n        return mutant\n\n    def crossover(self, mutant, target):\n        trial = target.copy()\n        for i in range(self.dim):\n            if np.random.rand() < self.Cr:\n                trial[i] = mutant[i]\n        return trial\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n        fevals = self.pop_size\n        self.success_F = []\n        self.success_Cr = []\n\n        while fevals < self.budget:\n            self.adjust_parameters()\n            self.success_F = []\n            self.success_Cr = []\n\n            # Adjust population size based on diversity\n            diversity = self.calculate_diversity()\n            if diversity < self.diversity_threshold:\n                self.pop_size = int(self.pop_scaling * 2 + 3 * np.log(self.dim))  # Increase population size\n                new_population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size - len(self.population), self.dim))\n                self.population = np.vstack((self.population, new_population))\n                self.fitness = np.concatenate((self.fitness, np.array([func(x) for x in new_population])))\n                self.age = np.concatenate((self.age, np.zeros(len(new_population))))\n            else:\n                 self.pop_size = int(self.pop_scaling + 3 * np.log(self.dim)) #Reset to nominal size if diversity is good enough\n                 if len(self.population) > self.pop_size:\n                     indices_to_remove = np.argsort(self.age)[-len(self.population) + self.pop_size:]\n                     self.population = np.delete(self.population, indices_to_remove, axis=0)\n                     self.fitness = np.delete(self.fitness, indices_to_remove)\n                     self.age = np.delete(self.age, indices_to_remove)\n            \n            for i in range(self.pop_size):\n                mutant = self.mutate(self.population, i)\n                mutant = self.mirrored_boundary_handling(mutant)\n                trial = self.crossover(mutant, self.population[i])\n                \n                f = func(trial)\n                fevals += 1\n\n                if f < self.fitness[i]:\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        replace_index = np.random.randint(0, self.archive_size)\n                        self.archive[replace_index] = self.population[i].copy()\n\n                    self.fitness[i] = f\n                    self.population[i] = trial\n                    self.age[i] = 0\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                else:\n                    self.age[i] +=1\n                \n                # Aging and Local Search\n                if self.age[i] > self.max_age and fevals < self.budget:\n                    new_x, new_f = self.orthogonal_learning(i) # Perform orthogonal learning for local search\n                    fevals += self.dim # each call to orthogonal_learning evaluates dim new individuals, approximately\n                    if new_f < self.fitness[i]:\n                        self.fitness[i] = new_f\n                        self.population[i] = new_x\n                        self.age[i] = 0\n                        if new_f < self.f_opt:\n                            self.f_opt = new_f\n                            self.x_opt = new_x\n                    else:\n                        self.age[i] = 0  # Reset age even if local search fails, to avoid stagnation\n\n                if fevals >= self.budget:\n                    break\n\n            # Update Memory\n            if self.success_F:\n                self.memory_F[self.memory_idx] = np.mean(self.success_F)\n                self.memory_Cr[self.memory_idx] = np.mean(self.success_Cr)\n                self.memory_idx = (self.memory_idx + 1) % self.memory_size\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "An exception occurred: name 'func' is not defined.", "error": "", "parent_ids": ["ce9efe8e-cb8d-4211-b86d-3f7ace28dc13"], "operator": null, "metadata": {}}
{"id": "b66caa3b-b431-4b88-ac47-8bfdb89d0252", "fitness": 0.0, "name": "EnhancedAdaptiveDE", "description": "Enhanced Adaptive DE with improved parameter adaptation using a success-history based adaptation and a larger archive, combined with toroidal and mirrored boundary handling and selective pressure to enhance both exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, archive_size=10, memory_size=10, initial_pop_factor=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = int(4 + initial_pop_factor * np.log(dim))  # Dynamically adjust population size based on dimension\n        self.population = None\n        self.fitness = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.archive = []  # Archive for storing potentially useful individuals\n        self.archive_size = archive_size\n        self.memory_size = memory_size\n        self.memory_F = np.full(self.memory_size, 0.5)\n        self.memory_Cr = np.full(self.memory_size, 0.5)\n        self.sf = []\n        self.scr = []\n        self.p = 0.1  # Probability of using archive individual\n        self.w = 0.5  # Weight for Lehmer mean update\n        self.success_count = 0 # count successful Cr and F values to adjust p\n        self.min_pop_size = 4\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n\n    def mutate(self, pop, i, current_generation):\n        memory_index = current_generation % self.memory_size\n        Cr = self.memory_Cr[memory_index]\n        F = self.memory_F[memory_index]\n\n        indices = [idx for idx in range(len(pop)) if idx != i]\n        if len(self.archive) > 0 and np.random.rand() < self.p:\n            arch_ind = np.random.randint(0, len(self.archive))\n            pop_indices = np.random.choice(indices, 2, replace=False)\n            a = self.archive[arch_ind]\n            b, c = pop[pop_indices[0]], pop[pop_indices[1]]\n        else:\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n\n        mutation = a + F * (b - c)\n        return mutation, Cr, F\n\n    def toroidal_boundary_handling(self, mutant):\n        for i in range(self.dim):\n            width = self.bounds_ub[i] - self.bounds_lb[i]\n            if mutant[i] < self.bounds_lb[i]:\n                mutant[i] = self.bounds_ub[i] - (self.bounds_lb[i] - mutant[i]) % width\n            elif mutant[i] > self.bounds_ub[i]:\n                mutant[i] = self.bounds_lb[i] + (mutant[i] - self.bounds_ub[i]) % width\n        return mutant\n\n    def mirrored_boundary_handling(self, mutant):\n        for i in range(self.dim):\n            if mutant[i] < self.bounds_lb[i]:\n                mutant[i] = self.bounds_lb[i] + (self.bounds_lb[i] - mutant[i])\n            elif mutant[i] > self.bounds_ub[i]:\n                mutant[i] = self.bounds_ub[i] - (mutant[i] - self.bounds_ub[i])\n        return mutant\n\n    def crossover(self, mutant, target, Cr):\n        cross_points = np.random.rand(self.dim) < Cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def update_memory(self):\n        if self.sf:\n            self.sf = np.array(self.sf)\n            self.scr = np.array(self.scr)\n\n            # Lehmer mean for F\n            sum_sf_squared = np.sum(self.sf ** 2)\n            sum_sf = np.sum(self.sf)\n            if sum_sf > 0:\n                lehmer_mean_F = sum_sf_squared / sum_sf\n            else:\n                lehmer_mean_F = 0.5  # Default value if no successful F values\n\n            # Weighted average for Cr (more robust)\n            lehmer_mean_Cr = np.mean(self.scr)\n\n            memory_index = self.generation % self.memory_size\n            self.memory_F[memory_index] = self.w * self.memory_F[memory_index] + (1 - self.w) * lehmer_mean_F\n            self.memory_Cr[memory_index] = self.w * self.memory_Cr[memory_index] + (1 - self.w) * lehmer_mean_Cr\n\n            # Adjust archive probability p based on success\n            if len(self.sf) > 0:\n                self.success_count += 1\n            else:\n                self.success_count = max(0, self.success_count - 1)  # Reduce if no success\n\n            self.p = min(0.2, self.success_count / self.memory_size) # adaptive p\n\n        self.sf = []\n        self.scr = []\n\n    def reduce_population(self):\n        if self.pop_size > self.min_pop_size:\n            reduction_amount = max(1, int(0.1 * self.pop_size)) # Reduce by at least 1\n            indices_to_remove = np.argsort(self.fitness)[-reduction_amount:]  # Remove worst individuals\n            self.population = np.delete(self.population, indices_to_remove, axis=0)\n            self.fitness = np.delete(self.fitness, indices_to_remove)\n            self.pop_size = len(self.population)\n\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n        fevals = self.pop_size\n        self.generation = 0\n        self.sf = []\n        self.scr = []\n\n        while fevals < self.budget:\n            self.generation += 1\n            for i in range(self.pop_size):\n                mutant, Cr, F = self.mutate(self.population, i, self.generation)\n                mutant = self.toroidal_boundary_handling(mutant)  # Toroidal boundary handling\n                mutant = self.mirrored_boundary_handling(mutant)\n                trial = self.crossover(mutant, self.population[i], Cr)\n\n                f = func(trial)\n                fevals += 1\n\n                if f < self.fitness[i]:\n                    # Archive the replaced individual\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        # Replace the worst element in the archive\n                        if self.archive:  # Ensure archive is not empty before proceeding\n                            archive_fitness = [func(x) for x in self.archive]  # Evaluate fitness of archive members\n                            replace_index = np.argmax(archive_fitness) # Replace worst in archive\n                            self.archive[replace_index] = self.population[i].copy()\n\n                    self.sf.append(F)\n                    self.scr.append(Cr)\n                    self.fitness[i] = f\n                    self.population[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n\n                if fevals >= self.budget:\n                    break\n            self.update_memory()\n            self.reduce_population() # Reduce population size over time\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm EnhancedAdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b5f53273-1fd3-4232-b8aa-7572ba9cd3a0"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "962df2c3-3a64-41ec-848c-815dccd0bb37", "fitness": -Infinity, "name": "NoveltyArchiveSHADE", "description": "SHADE with a dynamic external archive using a novelty metric to maintain diversity and an adaptive Cauchy mutation operator for enhanced exploration.", "code": "import numpy as np\n\nclass NoveltyArchiveSHADE:\n    def __init__(self, budget=10000, dim=10, memory_size=100, archive_size_factor=2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = int(4 + 3 * np.log(budget))  # Initial population size\n        self.memory_size = memory_size\n        self.memory_F = np.full(memory_size, 0.5)\n        self.memory_CR = np.full(memory_size, 0.5)\n        self.p = 0.1  # Percentage of top individuals for p-best selection\n        self.population = None\n        self.fitness = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.archive = []  # For storing diverse solutions\n        self.archive_size = int(self.pop_size * archive_size_factor)  # Archive size is a factor of pop_size\n        self.memory_index = 0\n        self.sf_M = []\n        self.scr_M = []\n        self.fevals = 0  # Track function evaluations\n        self.min_pop_size = 4  # Minimum population size\n        self.reduction_trigger = 0.25 #Fraction of budget before reduction starts\n        self.reduction_rate = 0.2 #Fraction of population reduced each time\n        self.novelty_threshold = 0.01 # Threshold for novelty, can be adapted\n        self.use_cauchy = True  #Use Cauchy mutation\n    \n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.fevals += self.pop_size\n\n    def mutate(self, pop, i, F, Cr, archive):\n        p_best_size = max(int(self.p * len(pop)), 1)\n\n        # Tournament selection for p-best\n        indices = np.argsort(self.fitness)[:p_best_size]\n        p_best_idx = np.random.choice(indices)\n        p_best = pop[p_best_idx]\n\n        indices = [idx for idx in range(len(pop)) if idx != i]\n        if len(indices) == 0:\n            return pop[i]\n\n        r1 = np.random.choice(indices)\n\n        # Choose r2 from current pop or archive\n        if len(archive) > 0 and np.random.rand() < 0.5:  # 50% chance to use archive\n            r2_idx = np.random.randint(0, len(archive))\n            r2 = archive[r2_idx]\n        else:\n            remaining_indices = [idx for idx in indices if idx != r1]\n            if len(remaining_indices) == 0:\n                r2 = pop[r1]\n            else:\n                r2_idx = np.random.choice(remaining_indices)\n                r2 = pop[r2_idx]\n        \n        if self.use_cauchy and np.random.rand() < 0.5: # 50% chance to use cauchy mutation\n            mutant = p_best + F * (pop[r1] - r2) + 0.01 * np.random.standard_cauchy(size=self.dim)  # Added cauchy noise\n        else:\n            mutant = p_best + F * (pop[r1] - r2)\n        return mutant\n\n    def crossover(self, mutant, target, Cr):\n        cross_points = np.random.rand(self.dim) < Cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def calculate_novelty(self, individual, archive, k=5):\n        if not archive:\n            return np.inf  # Maximum novelty if archive is empty\n\n        distances = [np.linalg.norm(individual - member) for member in archive]\n        nearest_neighbors_distances = sorted(distances)[:k]  # k-nearest neighbors\n        novelty = np.mean(nearest_neighbors_distances)\n        return novelty\n\n    def update_archive(self, individual):\n        if not self.archive:\n            self.archive.append(individual)\n            return\n\n        novelty = self.calculate_novelty(individual, self.archive)\n        \n        if novelty > self.novelty_threshold: #Only add individual to the archive if it is novel enough\n            if len(self.archive) < self.archive_size:\n                self.archive.append(individual)\n            else:\n                #Replace the most similar member in the archive\n                novelties = [self.calculate_novelty(member, [individual]) for member in self.archive]\n                worst_index = np.argmin(novelties)\n                self.archive[worst_index] = individual #Replace least novel member\n\n\n    def reduce_population(self):\n        if self.pop_size > self.min_pop_size:\n            sorted_indices = np.argsort(self.fitness)[::-1]  # Worst to best\n            reduction_amount = max(1, int(self.pop_size * self.reduction_rate))  # Reduce by reduction_rate, but at least 1\n            removed_indices = sorted_indices[:reduction_amount]\n\n            self.population = np.delete(self.population, removed_indices, axis=0)\n            self.fitness = np.delete(self.fitness, removed_indices)\n            self.pop_size = len(self.population)\n            print(f\"Population reduced to {self.pop_size}\")\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n        self.archive = []\n        \n        self.sf_M = []\n        self.scr_M = []\n        weights = []\n\n        while self.fevals < self.budget:\n            sf_k = []\n            scr_k = []\n            weights = []\n\n            for i in range(self.pop_size):\n                # Sample F and CR from memory\n                mem_idx = np.random.randint(0, self.memory_size)\n                F = self.memory_F[mem_idx]\n                Cr = self.memory_CR[mem_idx]\n\n                # Truncate and Reflect F\n                if F > 1:\n                    F = 1\n                if F <= 0:\n                    F = 0.000001\n\n                mutant = self.mutate(self.population, i, F, Cr, self.archive)\n                mutant = np.clip(mutant, self.bounds_lb, self.bounds_ub)\n\n                trial = self.crossover(mutant, self.population[i], Cr)\n\n                f = func(trial)\n                self.fevals += 1\n\n                if f < self.fitness[i]:\n                    delta = abs(f - self.fitness[i])\n                    sf_k.append(F)\n                    scr_k.append(Cr)\n                    weights.append(delta)\n\n                    self.update_archive(self.population[i])\n\n                    self.fitness[i] = f\n                    self.population[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                else:\n                    self.update_archive(trial)  # Add even unsuccessful trials\n\n                if self.fevals >= self.budget:\n                    break\n\n            # Update memory using Lehmer mean\n            if sf_k:\n                weights = np.array(weights) / np.sum(weights)\n                sum_weights_sfk = np.sum(weights * np.array(sf_k) ** 2)\n                sum_weights = np.sum(weights * np.array(sf_k))\n\n                if sum_weights > 0:\n                    mean_F = sum_weights_sfk / sum_weights\n                else:\n                    mean_F = 0.5\n\n                mean_CR = np.average(scr_k, weights=weights)\n\n                self.memory_F[self.memory_index] = mean_F\n                self.memory_CR[self.memory_index] = mean_CR\n                self.memory_index = (self.memory_index + 1) % self.memory_size\n\n            # Adaptive population size reduction\n            if self.fevals > self.budget * self.reduction_trigger:  # Start reducing after reduction_trigger of the budget is spent\n                self.reduce_population()\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "An exception occurred: Evaluation timed out after 60 seconds..", "error": "", "parent_ids": ["348607c7-8345-42c2-a8ed-f2267ea0c7b9"], "operator": null, "metadata": {}}
{"id": "248cd5c0-ee81-4f76-aa90-84c6b50fc7ca", "fitness": 0.0, "name": "CMAES_RankOne_SpectralInit_DynPop", "description": "CMA-ES with a simplified rank-one update, spectral initialization of covariance matrix, and dynamic population size based on fitness improvement.", "code": "import numpy as np\n\nclass CMAES_RankOne_SpectralInit_DynPop:\n    def __init__(self, budget=10000, dim=10, pop_size=None, target_success_rate=0.25):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else int(4 + 3 * np.log(dim))\n        self.mu = self.pop_size // 2\n        self.mean = None\n        self.sigma = 0.1\n        self.C = None\n        self.p_sigma = None\n        self.c_sigma = None\n        self.damps = 1\n        self.c_1 = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.fevals = 0\n        self.target_success_rate = target_success_rate\n        self.success_history = []\n        self.min_pop_size = 4\n        self.max_pop_size = 100\n\n    def initialize(self, func):\n        self.mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.p_sigma = np.zeros(self.dim)\n\n        self.c_sigma = (self.mu + 2) / (self.dim + self.mu + 5)\n\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1) + self.c_sigma\n        self.c_1 = 2 / ((self.dim + 1.3)**2 + self.mu)\n\n    def spectral_initialization(self, func):\n        # Sample initial population around the initial mean\n        initial_samples = np.random.normal(self.mean, self.sigma, size=(self.pop_size, self.dim))\n        initial_samples = np.clip(initial_samples, func.bounds.lb, func.bounds.ub)\n        initial_fitness = np.array([func(x) for x in initial_samples])\n        self.fevals += self.pop_size\n\n        # Estimate covariance from the initial samples\n        centered_samples = initial_samples - self.mean\n        self.C = np.cov(centered_samples.T)\n        \n        # Add a small identity matrix to ensure positive definiteness\n        self.C += 1e-8 * np.eye(self.dim)\n        \n    def sample_population(self):\n        z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n        C_sqrt = np.linalg.cholesky(self.C)\n        self.population = self.mean + self.sigma * z @ C_sqrt.T\n        self.population = np.clip(self.population, self.bounds_lb, self.bounds_ub)\n\n    def update_parameters(self):\n        sorted_indices = np.argsort(self.fitness)\n        self.population = self.population[sorted_indices]\n        self.fitness = self.fitness[sorted_indices]\n\n        old_mean = self.mean.copy()\n        self.mean = np.mean(self.population[:self.mu], axis=0)\n        \n        z = (self.mean - old_mean) / self.sigma\n        C_sqrt = np.linalg.cholesky(self.C)\n        z_in_C_space = np.linalg.solve(C_sqrt.T, z)\n        \n        self.p_sigma = (1 - self.c_sigma) * self.p_sigma + np.sqrt(self.c_sigma * (2 - self.c_sigma)) * z_in_C_space\n\n        self.C = (1 - self.c_1) * self.C + self.c_1 * np.outer(self.p_sigma, self.p_sigma)\n        self.sigma *= np.exp((self.c_sigma / self.damps) * (np.linalg.norm(self.p_sigma) / np.sqrt(self.dim) - 1))\n\n    def adjust_population_size(self):\n        # Track the recent success rate\n        if len(self.success_history) > 10:\n            self.success_history.pop(0)\n        \n        # Update the population size based on success\n        current_success_rate = np.mean(self.success_history) if self.success_history else 0.5\n        \n        if current_success_rate < self.target_success_rate:\n            self.pop_size = max(self.min_pop_size, int(self.pop_size * 0.9))\n        elif current_success_rate > self.target_success_rate:\n            self.pop_size = min(self.max_pop_size, int(self.pop_size * 1.1))\n        \n        self.mu = self.pop_size // 2\n        \n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize(func)\n        self.spectral_initialization(func)\n\n        while self.fevals < self.budget:\n            self.sample_population()\n            self.fitness = np.array([func(x) for x in self.population])\n            self.fevals += self.pop_size\n            \n            if self.fevals > self.budget:\n                self.fitness = self.fitness[:self.budget - (self.fevals - self.pop_size)]\n                self.population = self.population[:self.budget - (self.fevals - self.pop_size)]\n                break\n\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index]\n                self.success_history.append(1)  # Record a success\n            else:\n                self.success_history.append(0)  # Record a failure\n            \n            self.update_parameters()\n            self.adjust_population_size()\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm CMAES_RankOne_SpectralInit_DynPop scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["421c6573-910e-4246-9e6f-0be9adee1fce"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "c8073ab9-9893-44f7-9c4b-58f0d9a856a9", "fitness": 0.0, "name": "CMAES_Aging_OrthoInit_AdaptPop_SVD", "description": "CMA-ES with adaptive population size, dynamic covariance update based on singular value decomposition, and a rejuvenation strategy to combat premature convergence.", "code": "import numpy as np\n\nclass CMAES_Aging_OrthoInit_AdaptPop_SVD:\n    def __init__(self, budget=10000, dim=10, pop_size=None, aging_rate=0.05, min_pop_size=4):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else int(4 + 3 * np.log(dim))\n        self.min_pop_size = min_pop_size\n        self.mu = self.pop_size // 2  # Number of parents\n        self.c_m = 1  # Learning rate for mean\n        self.c_sigma = None  # Learning rate for step size\n        self.c_c = None # Learning rate for covariance matrix\n        self.damps = None\n\n        self.mean = None\n        self.sigma = 0.1\n        self.C = None\n\n        self.eigen_decomposition_counter = 0\n        self.eigen_decomposition_frequency = 10\n        self.P_sigma = None\n        self.P_c = None\n        self.c_1 = None\n        self.c_mu = None\n\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.fevals = 0\n\n        self.aging_rate = aging_rate\n        self.age = None\n        self.rejuvenation_probability = 0.05\n        self.min_sigma = 1e-10\n\n    def initialize_population(self, func):\n        # Orthogonal initialization for better diversity\n        H = np.array([[1, 1], [1, -1]])\n        if self.dim > H.shape[1]:\n            H_repeated = np.tile(H, (1, int(np.ceil(self.dim / H.shape[1]))))\n            H_used = H_repeated[:, :self.dim]\n        else:\n            H_used = H[:, :self.dim]\n            \n        self.mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n\n        self.population = np.zeros((self.pop_size, self.dim))\n        for i in range(self.pop_size):\n            z = np.random.normal(0, 1, size=self.dim)\n            x = self.mean + self.sigma * z\n            self.population[i,:] = np.clip(x, func.bounds.lb, func.bounds.ub) # Clip to bounds\n           \n        self.fitness = np.array([func(x) for x in self.population])\n        self.fevals += self.pop_size\n        self.age = np.zeros(self.pop_size)\n\n    def initialize_CMA_parameters(self):\n        self.C = np.eye(self.dim)\n        self.P_sigma = np.zeros(self.dim)\n        self.P_c = np.zeros(self.dim)\n\n        self.c_sigma = (self.mu + 2) / (self.dim + self.mu + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1) + self.c_sigma\n\n        self.c_1 = 2 / ((self.dim + 1.3)**2 + self.mu)\n        self.c_mu = min(1 - self.c_1, 2 * (self.mu - 1 + 1) / ((self.dim + 2)**2 + self.mu))\n\n    def sample_population(self):\n        z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n        C_sqrt = np.linalg.cholesky(self.C)  # Cholesky decomposition\n        self.population = self.mean + self.sigma * z @ C_sqrt.T\n\n        # Clip to bounds\n        self.population = np.clip(self.population, self.bounds_lb, self.bounds_ub)\n\n    def update_parameters(self):\n        # Sort the population and fitness based on fitness\n        sorted_indices = np.argsort(self.fitness)\n        self.population = self.population[sorted_indices]\n        self.fitness = self.fitness[sorted_indices]\n\n        # Select parents\n        parents = self.population[:self.mu]\n\n        # Update the mean\n        old_mean = self.mean.copy()\n        self.mean = np.mean(parents, axis=0)\n        \n        # Update evolution paths\n        z = (self.mean - old_mean) / self.sigma\n        C_sqrt = np.linalg.cholesky(self.C)\n\n        z_in_C_space = np.linalg.solve(C_sqrt.T, z)\n        \n        self.P_sigma = (1 - self.c_sigma) * self.P_sigma + np.sqrt(self.c_sigma * (2 - self.c_sigma)) * z_in_C_space\n        hsig = np.linalg.norm(self.P_sigma) / np.sqrt(1 - (1 - self.c_sigma)**(2 * (self.fevals / self.pop_size))) < (1.4 + 2 / (self.dim + 1))\n\n        self.P_c = (1 - self.c_c) * self.P_c + hsig * np.sqrt(self.c_c * (2 - self.c_c)) * z_in_C_space\n\n        # Update covariance matrix\n        y = parents - old_mean\n        \n        delta_C = np.zeros_like(self.C)\n        for i in range(self.mu):\n            weight = self.c_mu\n            dy = (parents[i] - old_mean) / self.sigma\n            delta_C += weight * np.outer(dy, dy)\n        \n        self.C = (1 - self.c_1 - self.c_mu) * self.C + self.c_1 * np.outer(self.P_c, self.P_c) + delta_C\n        \n        # Dynamic Covariance Update with SVD\n        try:\n            U, s, V = np.linalg.svd(self.C)\n            self.C = U @ np.diag(s) @ V\n        except np.linalg.LinAlgError:\n            self.C = np.eye(self.dim)  # Reset if SVD fails\n        \n        # Update step size\n        self.sigma *= np.exp((self.c_sigma / self.damps) * (np.linalg.norm(self.P_sigma) / np.sqrt(self.dim) - 1))\n        self.sigma = max(self.sigma, self.min_sigma)\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n        self.initialize_population(func)\n        self.initialize_CMA_parameters()\n        self.c_c = (4 + self.mu / self.dim) / (self.dim + 4 + 2 * self.mu / self.dim)\n        \n        while self.fevals < self.budget:\n            # Aging mechanism: Degrade fitness over time\n            for i in range(self.pop_size):\n                self.fitness[i] *= (1 + self.aging_rate * self.age[i])\n\n            # Generate new samples\n            self.sample_population()\n\n            # Evaluate the fitness of the new samples\n            new_fitness = np.array([func(x) for x in self.population])\n            self.fevals += self.pop_size\n            \n            # Check budget\n            if self.fevals > self.budget:\n                new_fitness = new_fitness[:self.budget - (self.fevals - self.pop_size)]\n                self.population = self.population[:self.budget - (self.fevals - self.pop_size)]\n                break\n\n            self.fitness = new_fitness\n            \n            # Update parameters\n            self.update_parameters()\n            \n            # Update age\n            self.age += 1\n            \n            # Rejuvenation\n            for i in range(self.pop_size):\n                if np.random.rand() < self.rejuvenation_probability:\n                    self.population[i] = np.random.uniform(self.bounds_lb, self.bounds_ub, size=self.dim)\n                    self.fitness[i] = func(self.population[i])\n                    self.fevals += 1\n                    self.age[i] = 0\n                    if self.fevals >= self.budget:\n                       break\n\n            # Reset age of best individual\n            best_index = np.argmin(self.fitness)\n            self.age[best_index] = 0\n\n            # Update optimal solution\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index]\n\n            # Adaptive Population Size Adjustment (simplified)\n            if self.fevals > self.budget // 2:\n                if self.pop_size > self.min_pop_size and np.std(self.fitness) < 1e-3:\n                    self.pop_size = max(self.pop_size // 2, self.min_pop_size)\n                    self.mu = self.pop_size // 2\n                    self.initialize_CMA_parameters() # Reinitialize CMA parameters\n                    self.population = self.population[:self.pop_size]\n                    self.fitness = self.fitness[:self.pop_size]\n                    self.age = self.age[:self.pop_size]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm CMAES_Aging_OrthoInit_AdaptPop_SVD scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["421c6573-910e-4246-9e6f-0be9adee1fce"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "9e712d06-1fe7-45bb-a1d5-8fe676ce3d01", "fitness": -Infinity, "name": "ImprovedSHADE_Ortho_AdaptPop_DBE_LocalSearch", "description": "Introducing a local search component using a Nelder-Mead simplex method to refine promising solutions identified by the Improved SHADE algorithm.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass ImprovedSHADE_Ortho_AdaptPop_DBE_LocalSearch:\n    def __init__(self, budget=10000, dim=10, memory_size=100, local_search_frequency=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = int(4 + 3 * np.log(budget))  # Initial population size\n        self.memory_size = memory_size\n        self.memory_F = np.full(memory_size, 0.5)\n        self.memory_CR = np.full(memory_size, 0.5)\n        self.p = 0.1  # Percentage of top individuals for p-best selection\n        self.population = None\n        self.fitness = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.archive = []  # For storing inferior solutions\n        self.archive_size = self.pop_size  # Archive size, can be adjusted\n        self.memory_index = 0\n        self.sf_M = []\n        self.scr_M = []\n        self.fevals = 0  # Track function evaluations\n        self.min_pop_size = 4  # Minimum population size\n        self.reduction_trigger = 0.25 #Fraction of budget before reduction starts\n        self.reduction_rate = 0.2 #Fraction of population reduced each time\n        self.use_orthogonal = True #Use orthogonal crossover\n        self.dbe_probability = 0.1 # Probability of using Distance-Based Exploration\n        self.local_search_frequency = local_search_frequency #Probability to perform local search\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.fevals += self.pop_size\n\n    def calculate_distances(self, pop, i):\n        \"\"\"Calculates Euclidean distances from individual i to all other individuals.\"\"\"\n        distances = np.linalg.norm(pop - pop[i], axis=1)\n        distances[i] = np.inf  # Exclude distance to itself\n        return distances\n\n    def mutate(self, pop, i, F, Cr, archive):\n        if np.random.rand() < self.dbe_probability:\n             # Distance-Based Exploration\n            distances = self.calculate_distances(pop, i)\n            r1 = np.argmin(distances) #Farthest Individual\n\n        else:\n\n            p_best_size = max(int(self.p * len(pop)), 1)\n\n            # Tournament selection for p-best\n            indices = np.argsort(self.fitness)[:p_best_size]\n            p_best_idx = np.random.choice(indices)\n            p_best = pop[p_best_idx]\n            indices = [idx for idx in range(len(pop)) if idx != i]\n            if len(indices) == 0:\n                return pop[i]\n            r1 = np.random.choice(indices)\n        \n        # Choose r2 from current pop or archive\n        if len(archive) > 0 and np.random.rand() < 0.5:  # 50% chance to use archive\n            r2_idx = np.random.randint(0, len(archive))\n            r2 = archive[r2_idx]\n        else:\n            indices = [idx for idx in range(len(pop)) if idx != i and idx != r1] #Ensure distinct indices\n            if len(indices) == 0:\n                r2 = pop[r1] #If only one individual, take that\n            else:\n                r2_idx = np.random.choice(indices)\n                r2 = pop[r2_idx]\n        \n        if np.random.rand() < self.dbe_probability:\n            p_best = pop[np.random.choice(range(len(pop)))] #Choose random individual when using DBE for p_best\n        else:\n            p_best_size = max(int(self.p * len(pop)), 1)\n            # Tournament selection for p-best\n            indices = np.argsort(self.fitness)[:p_best_size]\n            p_best_idx = np.random.choice(indices)\n            p_best = pop[p_best_idx]\n\n        mutant = p_best + F * (pop[r1] - r2)\n        return mutant\n\n    def orthogonal_crossover(self, mutant, target):\n        # Generate an orthogonal array (example using a simple Hadamard matrix)\n        H = np.array([[1, 1], [1, -1]])\n\n        # Ensure dimensions are compatible by repeating or truncating\n        if self.dim > H.shape[1]:\n            H_repeated = np.tile(H, (1, int(np.ceil(self.dim / H.shape[1]))))\n            H_used = H_repeated[:, :self.dim]\n        else:\n            H_used = H[:, :self.dim]\n\n        trial = np.copy(target)\n\n        for j in range(self.dim):\n            if H_used[0, j] == H_used[1, j]:\n                continue  # No change\n            else:\n                trial[j] = mutant[j]\n\n        return trial\n\n    def crossover(self, mutant, target, Cr):\n        cross_points = np.random.rand(self.dim) < Cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def update_archive(self, individual, archive):\n        archive.append(individual)\n        return archive\n\n    def reduce_population(self):\n        if self.pop_size > self.min_pop_size:\n            sorted_indices = np.argsort(self.fitness)[::-1]  # Worst to best\n            reduction_amount = max(1, int(self.pop_size * self.reduction_rate))  # Reduce by reduction_rate, but at least 1\n            removed_indices = sorted_indices[:reduction_amount]\n\n            self.population = np.delete(self.population, removed_indices, axis=0)\n            self.fitness = np.delete(self.fitness, removed_indices)\n            self.pop_size = len(self.population)\n\n    def local_search(self, func, x0):\n        \"\"\"Performs local search using Nelder-Mead.\"\"\"\n        bounds = [(self.bounds_lb, self.bounds_ub) for _ in range(self.dim)]\n        result = minimize(func, x0, method='Nelder-Mead', bounds=bounds, options={'maxfev': max(1, int(0.01 * self.budget))}) # Limit FE\n        self.fevals += result.nfev #Update fevals\n        return result.fun, result.x\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n        self.archive = []\n        \n        self.sf_M = []\n        self.scr_M = []\n        weights = []\n\n        while self.fevals < self.budget:\n            sf_k = []\n            scr_k = []\n            weights = []\n\n            for i in range(self.pop_size):\n                # Sample F and CR from memory\n                mem_idx = np.random.randint(0, self.memory_size)\n                F = self.memory_F[mem_idx]\n                Cr = self.memory_CR[mem_idx]\n\n                # Truncate and Reflect F\n                if F > 1:\n                    F = 1\n                if F <= 0:\n                    F = 0.000001\n\n                mutant = self.mutate(self.population, i, F, Cr, self.archive)\n                mutant = np.clip(mutant, self.bounds_lb, self.bounds_ub)\n\n                # Use orthogonal crossover with a probability or with the regular crossover\n                if self.use_orthogonal and np.random.rand() < 0.5:  # 50% chance to use orthogonal crossover\n                    trial = self.orthogonal_crossover(mutant, self.population[i])\n                else:\n                    trial = self.crossover(mutant, self.population[i], Cr)\n\n                f = func(trial)\n                self.fevals += 1\n\n                if f < self.fitness[i]:\n                    delta = abs(f - self.fitness[i])\n                    sf_k.append(F)\n                    scr_k.append(Cr)\n                    weights.append(delta)\n\n                    self.archive = self.update_archive(self.population[i], self.archive)\n                    if len(self.archive) > self.archive_size:\n                        self.archive.pop(np.random.randint(0, len(self.archive)))  # Keep archive size constant\n\n                    self.fitness[i] = f\n                    self.population[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                else:\n                    self.archive = self.update_archive(trial, self.archive)  # Add even unsuccessful trials\n                    if len(self.archive) > self.archive_size:\n                        self.archive.pop(np.random.randint(0, len(self.archive)))\n\n                # Perform local search with a given probability\n                if np.random.rand() < self.local_search_frequency:\n                    f_local, x_local = self.local_search(func, self.population[i])\n                    if f_local < self.fitness[i]:\n                        self.fitness[i] = f_local\n                        self.population[i] = x_local\n                        if f_local < self.f_opt:\n                            self.f_opt = f_local\n                            self.x_opt = x_local\n\n                if self.fevals >= self.budget:\n                    break\n\n            # Update memory using Lehmer mean\n            if sf_k:\n                weights = np.array(weights) / np.sum(weights)\n                sum_weights_sfk = np.sum(weights * np.array(sf_k) ** 2)\n                sum_weights = np.sum(weights * np.array(sf_k))\n\n                if sum_weights > 0:\n                    mean_F = sum_weights_sfk / sum_weights\n                else:\n                    mean_F = 0.5\n\n                mean_CR = np.average(scr_k, weights=weights)\n\n                self.memory_F[self.memory_index] = mean_F\n                self.memory_CR[self.memory_index] = mean_CR\n                self.memory_index = (self.memory_index + 1) % self.memory_size\n\n            # Adaptive population size reduction\n            if self.fevals > self.budget * self.reduction_trigger:  # Start reducing after reduction_trigger of the budget is spent\n                self.reduce_population()\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "An exception occurred: name 'minimize' is not defined.", "error": "", "parent_ids": ["c6e5bc64-3827-4b00-976f-bb47b493ee62"], "operator": null, "metadata": {}}
{"id": "3b5eb39f-b6bb-470a-b8b0-7884304d5efa", "fitness": -Infinity, "name": "ImprovedSHADE_Ortho_AdaptPop_DBE_LocalSearch", "description": "Introduces a local search phase using Nelder-Mead simplex method to refine promising solutions found by Improved SHADE, enhancing local exploitation.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass ImprovedSHADE_Ortho_AdaptPop_DBE_LocalSearch:\n    def __init__(self, budget=10000, dim=10, memory_size=100, local_search_probability = 0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = int(4 + 3 * np.log(budget))  # Initial population size\n        self.memory_size = memory_size\n        self.memory_F = np.full(memory_size, 0.5)\n        self.memory_CR = np.full(memory_size, 0.5)\n        self.p = 0.1  # Percentage of top individuals for p-best selection\n        self.population = None\n        self.fitness = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.archive = []  # For storing inferior solutions\n        self.archive_size = self.pop_size  # Archive size, can be adjusted\n        self.memory_index = 0\n        self.sf_M = []\n        self.scr_M = []\n        self.fevals = 0  # Track function evaluations\n        self.min_pop_size = 4  # Minimum population size\n        self.reduction_trigger = 0.25 #Fraction of budget before reduction starts\n        self.reduction_rate = 0.2 #Fraction of population reduced each time\n        self.use_orthogonal = True #Use orthogonal crossover\n        self.dbe_probability = 0.1 # Probability of using Distance-Based Exploration\n        self.local_search_probability = local_search_probability\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.fevals += self.pop_size\n\n    def calculate_distances(self, pop, i):\n        \"\"\"Calculates Euclidean distances from individual i to all other individuals.\"\"\"\n        distances = np.linalg.norm(pop - pop[i], axis=1)\n        distances[i] = np.inf  # Exclude distance to itself\n        return distances\n\n    def mutate(self, pop, i, F, Cr, archive):\n        if np.random.rand() < self.dbe_probability:\n             # Distance-Based Exploration\n            distances = self.calculate_distances(pop, i)\n            r1 = np.argmin(distances) #Farthest Individual\n\n        else:\n\n            p_best_size = max(int(self.p * len(pop)), 1)\n\n            # Tournament selection for p-best\n            indices = np.argsort(self.fitness)[:p_best_size]\n            p_best_idx = np.random.choice(indices)\n            p_best = pop[p_best_idx]\n            indices = [idx for idx in range(len(pop)) if idx != i]\n            if len(indices) == 0:\n                return pop[i]\n            r1 = np.random.choice(indices)\n        \n        # Choose r2 from current pop or archive\n        if len(archive) > 0 and np.random.rand() < 0.5:  # 50% chance to use archive\n            r2_idx = np.random.randint(0, len(archive))\n            r2 = archive[r2_idx]\n        else:\n            indices = [idx for idx in range(len(pop)) if idx != i and idx != r1] #Ensure distinct indices\n            if len(indices) == 0:\n                r2 = pop[r1] #If only one individual, take that\n            else:\n                r2_idx = np.random.choice(indices)\n                r2 = pop[r2_idx]\n        \n        if np.random.rand() < self.dbe_probability:\n            p_best = pop[np.random.choice(range(len(pop)))] #Choose random individual when using DBE for p_best\n        else:\n            p_best_size = max(int(self.p * len(pop)), 1)\n            # Tournament selection for p-best\n            indices = np.argsort(self.fitness)[:p_best_size]\n            p_best_idx = np.random.choice(indices)\n            p_best = pop[p_best_idx]\n\n        mutant = p_best + F * (pop[r1] - r2)\n        return mutant\n\n    def orthogonal_crossover(self, mutant, target):\n        # Generate an orthogonal array (example using a simple Hadamard matrix)\n        H = np.array([[1, 1], [1, -1]])\n\n        # Ensure dimensions are compatible by repeating or truncating\n        if self.dim > H.shape[1]:\n            H_repeated = np.tile(H, (1, int(np.ceil(self.dim / H.shape[1]))))\n            H_used = H_repeated[:, :self.dim]\n        else:\n            H_used = H[:, :self.dim]\n\n        trial = np.copy(target)\n\n        for j in range(self.dim):\n            if H_used[0, j] == H_used[1, j]:\n                continue  # No change\n            else:\n                trial[j] = mutant[j]\n\n        return trial\n\n    def crossover(self, mutant, target, Cr):\n        cross_points = np.random.rand(self.dim) < Cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def update_archive(self, individual, archive):\n        archive.append(individual)\n        return archive\n\n    def reduce_population(self):\n        if self.pop_size > self.min_pop_size:\n            sorted_indices = np.argsort(self.fitness)[::-1]  # Worst to best\n            reduction_amount = max(1, int(self.pop_size * self.reduction_rate))  # Reduce by reduction_rate, but at least 1\n            removed_indices = sorted_indices[:reduction_amount]\n\n            self.population = np.delete(self.population, removed_indices, axis=0)\n            self.fitness = np.delete(self.fitness, removed_indices)\n            self.pop_size = len(self.population)\n\n    def local_search(self, func, x0):\n        \"\"\"\n        Performs local search using the Nelder-Mead simplex algorithm.\n        \"\"\"\n        bounds = [(self.bounds_lb, self.bounds_ub) for _ in range(self.dim)]\n        result = minimize(func, x0, method='Nelder-Mead', bounds=bounds, options={'maxfev': max(1, int(self.budget/ (self.pop_size * 10)))})\n        self.fevals += result.nfev\n        return result.fun, result.x\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n        self.archive = []\n        \n        self.sf_M = []\n        self.scr_M = []\n        weights = []\n\n        while self.fevals < self.budget:\n            sf_k = []\n            scr_k = []\n            weights = []\n\n            for i in range(self.pop_size):\n                # Sample F and CR from memory\n                mem_idx = np.random.randint(0, self.memory_size)\n                F = self.memory_F[mem_idx]\n                Cr = self.memory_CR[mem_idx]\n\n                # Truncate and Reflect F\n                if F > 1:\n                    F = 1\n                if F <= 0:\n                    F = 0.000001\n\n                mutant = self.mutate(self.population, i, F, Cr, self.archive)\n                mutant = np.clip(mutant, self.bounds_lb, self.bounds_ub)\n\n                # Use orthogonal crossover with a probability or with the regular crossover\n                if self.use_orthogonal and np.random.rand() < 0.5:  # 50% chance to use orthogonal crossover\n                    trial = self.orthogonal_crossover(mutant, self.population[i])\n                else:\n                    trial = self.crossover(mutant, self.population[i], Cr)\n\n                f = func(trial)\n                self.fevals += 1\n\n                if f < self.fitness[i]:\n                    delta = abs(f - self.fitness[i])\n                    sf_k.append(F)\n                    scr_k.append(Cr)\n                    weights.append(delta)\n\n                    self.archive = self.update_archive(self.population[i], self.archive)\n                    if len(self.archive) > self.archive_size:\n                        self.archive.pop(np.random.randint(0, len(self.archive)))  # Keep archive size constant\n\n                    # Local search application\n                    if np.random.rand() < self.local_search_probability:\n                        f_local, trial_local = self.local_search(func, trial)\n                        if f_local < f:\n                            f = f_local\n                            trial = trial_local\n\n\n                    self.fitness[i] = f\n                    self.population[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                else:\n                    self.archive = self.update_archive(trial, self.archive)  # Add even unsuccessful trials\n                    if len(self.archive) > self.archive_size:\n                        self.archive.pop(np.random.randint(0, len(self.archive)))\n\n                if self.fevals >= self.budget:\n                    break\n\n            # Update memory using Lehmer mean\n            if sf_k:\n                weights = np.array(weights) / np.sum(weights)\n                sum_weights_sfk = np.sum(weights * np.array(sf_k) ** 2)\n                sum_weights = np.sum(weights * np.array(sf_k))\n\n                if sum_weights > 0:\n                    mean_F = sum_weights_sfk / sum_weights\n                else:\n                    mean_F = 0.5\n\n                mean_CR = np.average(scr_k, weights=weights)\n\n                self.memory_F[self.memory_index] = mean_F\n                self.memory_CR[self.memory_index] = mean_CR\n                self.memory_index = (self.memory_index + 1) % self.memory_size\n\n            # Adaptive population size reduction\n            if self.fevals > self.budget * self.reduction_trigger:  # Start reducing after reduction_trigger of the budget is spent\n                self.reduce_population()\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "An exception occurred: name 'minimize' is not defined.", "error": "", "parent_ids": ["c6e5bc64-3827-4b00-976f-bb47b493ee62"], "operator": null, "metadata": {}}
{"id": "7bbeb35a-56c7-4fb2-8627-eb373a4d0a27", "fitness": -Infinity, "name": "ImprovedSHADE_Ortho_AdaptPop_DBE_EnhancedArchive", "description": "Improved SHADE with archive enhancement using worst solutions and a refined distance-based exploration incorporating adaptive probability.", "code": "import numpy as np\n\nclass ImprovedSHADE_Ortho_AdaptPop_DBE_EnhancedArchive:\n    def __init__(self, budget=10000, dim=10, memory_size=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = int(4 + 3 * np.log(budget))  # Initial population size\n        self.memory_size = memory_size\n        self.memory_F = np.full(memory_size, 0.5)\n        self.memory_CR = np.full(memory_size, 0.5)\n        self.p = 0.1  # Percentage of top individuals for p-best selection\n        self.population = None\n        self.fitness = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.archive = []  # For storing inferior solutions\n        self.archive_size = self.pop_size  # Archive size, can be adjusted\n        self.memory_index = 0\n        self.sf_M = []\n        self.scr_M = []\n        self.fevals = 0  # Track function evaluations\n        self.min_pop_size = 4  # Minimum population size\n        self.reduction_trigger = 0.25 #Fraction of budget before reduction starts\n        self.reduction_rate = 0.2 #Fraction of population reduced each time\n        self.use_orthogonal = True #Use orthogonal crossover\n        self.dbe_probability_initial = 0.1 # Initial Probability of using Distance-Based Exploration\n        self.dbe_probability = self.dbe_probability_initial\n        self.dbe_adaptive = True # adapt dbe probability\n        self.dbe_success = 0\n        self.dbe_failure = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.fevals += self.pop_size\n\n    def calculate_distances(self, pop, i):\n        \"\"\"Calculates Euclidean distances from individual i to all other individuals.\"\"\"\n        distances = np.linalg.norm(pop - pop[i], axis=1)\n        distances[i] = np.inf  # Exclude distance to itself\n        return distances\n\n    def mutate(self, pop, i, F, Cr, archive):\n        if np.random.rand() < self.dbe_probability:\n             # Distance-Based Exploration\n            distances = self.calculate_distances(pop, i)\n            r1 = np.argmin(distances) #Farthest Individual\n\n        else:\n\n            p_best_size = max(int(self.p * len(pop)), 1)\n\n            # Tournament selection for p-best\n            indices = np.argsort(self.fitness)[:p_best_size]\n            p_best_idx = np.random.choice(indices)\n            p_best = pop[p_best_idx]\n            indices = [idx for idx in range(len(pop)) if idx != i]\n            if len(indices) == 0:\n                return pop[i]\n            r1 = np.random.choice(indices)\n        \n        # Choose r2 from current pop or archive\n        if len(archive) > 0 and np.random.rand() < 0.5:  # 50% chance to use archive\n            r2_idx = np.random.randint(0, len(archive))\n            r2 = archive[r2_idx]\n        else:\n            indices = [idx for idx in range(len(pop)) if idx != i and idx != r1] #Ensure distinct indices\n            if len(indices) == 0:\n                r2 = pop[r1] #If only one individual, take that\n            else:\n                r2_idx = np.random.choice(indices)\n                r2 = pop[r2_idx]\n        \n        if np.random.rand() < self.dbe_probability:\n            p_best = pop[np.random.choice(range(len(pop)))] #Choose random individual when using DBE for p_best\n        else:\n            p_best_size = max(int(self.p * len(pop)), 1)\n            # Tournament selection for p-best\n            indices = np.argsort(self.fitness)[:p_best_size]\n            p_best = pop[p_best_idx]\n\n        mutant = p_best + F * (pop[r1] - r2)\n        return mutant\n\n    def orthogonal_crossover(self, mutant, target):\n        # Generate an orthogonal array (example using a simple Hadamard matrix)\n        H = np.array([[1, 1], [1, -1]])\n\n        # Ensure dimensions are compatible by repeating or truncating\n        if self.dim > H.shape[1]:\n            H_repeated = np.tile(H, (1, int(np.ceil(self.dim / H.shape[1]))))\n            H_used = H_repeated[:, :self.dim]\n        else:\n            H_used = H[:, :self.dim]\n\n        trial = np.copy(target)\n\n        for j in range(self.dim):\n            if H_used[0, j] == H_used[1, j]:\n                continue  # No change\n            else:\n                trial[j] = mutant[j]\n\n        return trial\n\n    def crossover(self, mutant, target, Cr):\n        cross_points = np.random.rand(self.dim) < Cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def update_archive(self, individual, archive, fitness_val):\n        \"\"\"Updates the archive, prioritizing diversity and including worst solutions.\"\"\"\n        archive.append(individual)\n        # Remove the *best* element from the archive to prevent premature convergence\n        if len(archive) > self.archive_size:\n             fitnesses = np.array([self.__archive_fitness(a) for a in archive])\n             archive.pop(np.argmin(fitnesses))\n\n        return archive\n\n    def __archive_fitness(self, individual):\n         \"\"\"Dummy function to make the archive work with the same fitness\"\"\"\n         return np.random.rand()\n\n\n    def reduce_population(self):\n        if self.pop_size > self.min_pop_size:\n            sorted_indices = np.argsort(self.fitness)[::-1]  # Worst to best\n            reduction_amount = max(1, int(self.pop_size * self.reduction_rate))  # Reduce by reduction_rate, but at least 1\n            removed_indices = sorted_indices[:reduction_amount]\n\n            self.population = np.delete(self.population, removed_indices, axis=0)\n            self.fitness = np.delete(self.fitness, removed_indices)\n            self.pop_size = len(self.population)\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n        self.archive = []\n        \n        self.sf_M = []\n        self.scr_M = []\n        weights = []\n\n        while self.fevals < self.budget:\n            sf_k = []\n            scr_k = []\n            weights = []\n\n            for i in range(self.pop_size):\n                # Sample F and CR from memory\n                mem_idx = np.random.randint(0, self.memory_size)\n                F = self.memory_F[mem_idx]\n                Cr = self.memory_CR[mem_idx]\n\n                # Truncate and Reflect F\n                if F > 1:\n                    F = 1\n                if F <= 0:\n                    F = 0.000001\n\n                mutant = self.mutate(self.population, i, F, Cr, self.archive)\n                mutant = np.clip(mutant, self.bounds_lb, self.bounds_ub)\n\n                # Use orthogonal crossover with a probability or with the regular crossover\n                if self.use_orthogonal and np.random.rand() < 0.5:  # 50% chance to use orthogonal crossover\n                    trial = self.orthogonal_crossover(mutant, self.population[i])\n                else:\n                    trial = self.crossover(mutant, self.population[i], Cr)\n\n                f = func(trial)\n                self.fevals += 1\n\n                if f < self.fitness[i]:\n                    delta = abs(f - self.fitness[i])\n                    sf_k.append(F)\n                    scr_k.append(Cr)\n                    weights.append(delta)\n\n                    self.archive = self.update_archive(self.population[i], self.archive, self.fitness[i])\n                    if len(self.archive) > self.archive_size:\n                        self.archive.pop(np.random.randint(0, len(self.archive)))  # Keep archive size constant\n\n                    self.fitness[i] = f\n                    self.population[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                        if self.dbe_adaptive:\n                            self.dbe_success += 1\n\n                else:\n                    self.archive = self.update_archive(trial, self.archive, f)  # Add even unsuccessful trials\n                    if len(self.archive) > self.archive_size:\n                        self.archive.pop(np.random.randint(0, len(self.archive)))\n\n                    if self.dbe_adaptive:\n                        self.dbe_failure += 1\n\n\n                if self.fevals >= self.budget:\n                    break\n\n            # Update memory using Lehmer mean\n            if sf_k:\n                weights = np.array(weights) / np.sum(weights)\n                sum_weights_sfk = np.sum(weights * np.array(sf_k) ** 2)\n                sum_weights = np.sum(weights * np.array(sf_k))\n\n                if sum_weights > 0:\n                    mean_F = sum_weights_sfk / sum_weights\n                else:\n                    mean_F = 0.5\n\n                mean_CR = np.average(scr_k, weights=weights)\n\n                self.memory_F[self.memory_index] = mean_F\n                self.memory_CR[self.memory_index] = mean_CR\n                self.memory_index = (self.memory_index + 1) % self.memory_size\n            \n            if self.dbe_adaptive:\n                if (self.dbe_success + self.dbe_failure) > 10:  # update the prob every 10 iter\n                    success_rate = self.dbe_success / (self.dbe_success + self.dbe_failure)\n                    # Adjust probability, ensuring it stays within bounds\n                    self.dbe_probability = max(0.01, min(0.5, success_rate)) #Adaptive DBE\n                    self.dbe_success = 0\n                    self.dbe_failure = 0\n\n            # Adaptive population size reduction\n            if self.fevals > self.budget * self.reduction_trigger:  # Start reducing after reduction_trigger of the budget is spent\n                self.reduce_population()\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "An exception occurred: local variable 'p_best_idx' referenced before assignment.", "error": "", "parent_ids": ["c6e5bc64-3827-4b00-976f-bb47b493ee62"], "operator": null, "metadata": {}}
{"id": "eddb72e9-b0ac-4b79-8e6e-722a74875903", "fitness": 0.0, "name": "CMAES_Aging_OrthoInit_AdaptPop_DynBounds", "description": "CMA-ES with adaptive population size based on the progress rate, orthogonal initialization, and dynamic bounds adjustment to prevent premature convergence.", "code": "import numpy as np\n\nclass CMAES_Aging_OrthoInit_AdaptPop_DynBounds:\n    def __init__(self, budget=10000, dim=10, pop_size=None, aging_rate=0.05, target_success_rate=0.25, min_pop_size=4):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else int(4 + 3 * np.log(dim))\n        self.mu = self.pop_size // 2  # Number of parents\n        self.c_m = 1  # Learning rate for mean\n        self.c_sigma = None  # Learning rate for step size\n        self.c_c = None # Learning rate for covariance matrix\n        self.damps = None\n\n        self.mean = None\n        self.sigma = 0.1\n        self.C = None\n\n        self.eigen_decomposition_counter = 0\n        self.eigen_decomposition_frequency = 10\n        self.P_sigma = None\n        self.P_c = None\n        self.c_1 = None\n        self.c_mu = None\n\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.fevals = 0\n\n        self.aging_rate = aging_rate\n        self.age = None\n        self.target_success_rate = target_success_rate\n        self.success_rate = target_success_rate  # Initialize with target to avoid early population increase\n        self.min_pop_size = min_pop_size\n        self.last_improvement = 0\n        self.improvement_threshold = 0.01  # Threshold for considering an improvement significant\n        self.expansion_factor = 1.1\n        self.contraction_factor = 0.9\n        self.bounds_expansion = 0.1 # Expand by 10%\n        self.bounds_contraction = 0.9 # Contract by 10%\n        self.dynamic_bounds_active = True\n\n    def initialize_population(self, func):\n        # Orthogonal initialization for better diversity\n        H = np.array([[1, 1], [1, -1]])\n        if self.dim > H.shape[1]:\n            H_repeated = np.tile(H, (1, int(np.ceil(self.dim / H.shape[1]))))\n            H_used = H_repeated[:, :self.dim]\n        else:\n            H_used = H[:, :self.dim]\n            \n        self.mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n\n        self.population = np.zeros((self.pop_size, self.dim))\n        for i in range(self.pop_size):\n            z = np.random.normal(0, 1, size=self.dim)\n            x = self.mean + self.sigma * z\n            self.population[i,:] = np.clip(x, func.bounds.lb, func.bounds.ub) # Clip to bounds\n           \n        self.fitness = np.array([func(x) for x in self.population])\n        self.fevals += self.pop_size\n        self.age = np.zeros(self.pop_size)\n\n    def initialize_CMA_parameters(self):\n        self.C = np.eye(self.dim)\n        self.P_sigma = np.zeros(self.dim)\n        self.P_c = np.zeros(self.dim)\n\n        self.c_sigma = (self.mu + 2) / (self.dim + self.mu + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1) + self.c_sigma\n\n        self.c_1 = 2 / ((self.dim + 1.3)**2 + self.mu)\n        self.c_mu = min(1 - self.c_1, 2 * (self.mu - 1 + 1) / ((self.dim + 2)**2 + self.mu))\n\n    def sample_population(self):\n        z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n        C_sqrt = np.linalg.cholesky(self.C)  # Cholesky decomposition\n        self.population = self.mean + self.sigma * z @ C_sqrt.T\n\n        # Clip to bounds\n        self.population = np.clip(self.population, self.bounds_lb, self.bounds_ub)\n\n    def update_parameters(self, func):\n        # Sort the population and fitness based on fitness\n        sorted_indices = np.argsort(self.fitness)\n        self.population = self.population[sorted_indices]\n        self.fitness = self.fitness[sorted_indices]\n\n        # Select parents\n        parents = self.population[:self.mu]\n\n        # Update the mean\n        old_mean = self.mean.copy()\n        self.mean = np.mean(parents, axis=0)\n        \n        # Update evolution paths\n        z = (self.mean - old_mean) / self.sigma\n        C_sqrt = np.linalg.cholesky(self.C)\n\n        z_in_C_space = np.linalg.solve(C_sqrt.T, z)\n        \n        self.P_sigma = (1 - self.c_sigma) * self.P_sigma + np.sqrt(self.c_sigma * (2 - self.c_sigma)) * z_in_C_space\n        hsig = np.linalg.norm(self.P_sigma) / np.sqrt(1 - (1 - self.c_sigma)**(2 * (self.fevals / self.pop_size))) < (1.4 + 2 / (self.dim + 1))\n\n        self.P_c = (1 - self.c_c) * self.P_c + hsig * np.sqrt(self.c_c * (2 - self.c_c)) * z_in_C_space\n\n        # Update covariance matrix\n        y = parents - old_mean\n        \n        delta_C = np.zeros_like(self.C)\n        for i in range(self.mu):\n            weight = self.c_mu\n            dy = (parents[i] - old_mean) / self.sigma\n            delta_C += weight * np.outer(dy, dy)\n        \n        self.C = (1 - self.c_1 - self.c_mu) * self.C + self.c_1 * np.outer(self.P_c, self.P_c) + delta_C\n\n        # Update step size\n        self.sigma *= np.exp((self.c_sigma / self.damps) * (np.linalg.norm(self.P_sigma) / np.sqrt(self.dim) - 1))\n\n        # Adaptive Population Size\n        n_success = np.sum(self.fitness < self.f_opt)\n        self.success_rate = 0.9 * self.success_rate + 0.1 * (n_success / self.pop_size)\n\n        if self.success_rate < self.target_success_rate / 2 and self.pop_size > self.min_pop_size:\n            self.pop_size = max(self.min_pop_size, int(self.pop_size * self.contraction_factor))\n            self.mu = self.pop_size // 2\n            print(f\"Contracting population to {self.pop_size}\")\n        elif self.success_rate > 2 * self.target_success_rate and self.fevals - self.last_improvement > self.budget / 10:\n            self.pop_size = int(self.pop_size * self.expansion_factor)\n            self.mu = self.pop_size // 2\n            print(f\"Expanding population to {self.pop_size}\")\n\n        # Dynamic Bounds Adjustment\n        if self.dynamic_bounds_active:\n            best_individual = self.population[0]\n            range_x = np.max(self.population, axis=0) - np.min(self.population, axis=0)\n            \n            # Check if the population is converging too close to the boundary\n            close_to_lb = np.any(best_individual - self.bounds_lb < 0.05 * (self.bounds_ub - self.bounds_lb))\n            close_to_ub = np.any(self.bounds_ub - best_individual < 0.05 * (self.bounds_ub - self.bounds_lb))\n            \n            if close_to_lb or close_to_ub:\n                print(\"Adjusting bounds\")\n                self.bounds_lb = func.bounds.lb * self.bounds_contraction\n                self.bounds_ub = func.bounds.ub * self.bounds_expansion\n                self.population = np.clip(self.population, self.bounds_lb, self.bounds_ub)\n                self.mean = np.clip(self.mean, self.bounds_lb, self.bounds_ub)\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n        self.initialize_population(func)\n        self.initialize_CMA_parameters()\n        self.c_c = (4 + self.mu / self.dim) / (self.dim + 4 + 2 * self.mu / self.dim)\n        \n        while self.fevals < self.budget:\n            # Aging mechanism: Degrade fitness over time\n            for i in range(self.pop_size):\n                self.fitness[i] *= (1 + self.aging_rate * self.age[i])\n\n            # Generate new samples\n            self.sample_population()\n\n            # Evaluate the fitness of the new samples\n            new_fitness = np.array([func(x) for x in self.population])\n            self.fevals += self.pop_size\n            \n            # Check budget\n            if self.fevals > self.budget:\n                new_fitness = new_fitness[:self.budget - (self.fevals - self.pop_size)]\n                self.population = self.population[:self.budget - (self.fevals - self.pop_size)]\n                break\n\n            self.fitness = new_fitness\n            \n            # Update parameters\n            self.update_parameters(func)\n            \n            # Update age\n            self.age += 1\n            \n            # Reset age of best individual\n            best_index = np.argmin(self.fitness)\n            self.age[best_index] = 0\n\n            # Update optimal solution\n            if self.fitness[best_index] < self.f_opt - self.improvement_threshold:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index]\n                self.last_improvement = self.fevals\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm CMAES_Aging_OrthoInit_AdaptPop_DynBounds scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["421c6573-910e-4246-9e6f-0be9adee1fce"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "a2c544b1-1e00-4f04-b664-fce5bb1b8fb2", "fitness": 0.5921550973270494, "name": "CMAES_ArchiveRecombination", "description": "CMA-ES with Archive-based Recombination, where the archive stores previously evaluated solutions, and recombination incorporates both population and archive members, promoting exploration and exploitation.", "code": "import numpy as np\n\nclass CMAES_ArchiveRecombination:\n    def __init__(self, budget=10000, dim=10, pop_size=None, archive_size=100, archive_probability=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else int(4 + 3 * np.log(dim))\n        self.mu = self.pop_size // 2\n        self.archive_size = archive_size\n        self.archive_probability = archive_probability\n\n        self.mean = None\n        self.sigma = 0.1\n        self.C = None\n\n        self.P_sigma = None\n        self.P_c = None\n\n        self.c_sigma = None\n        self.c_c = None\n        self.c_1 = None\n        self.c_mu = None\n        self.damps = None\n\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.fevals = 0\n\n        self.archive_x = []\n        self.archive_f = []\n\n    def initialize_parameters(self):\n        self.C = np.eye(self.dim)\n        self.P_sigma = np.zeros(self.dim)\n        self.P_c = np.zeros(self.dim)\n\n        self.c_sigma = (self.mu + 2) / (self.dim + self.mu + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1) + self.c_sigma\n\n        self.c_1 = 2 / ((self.dim + 1.3)**2 + self.mu)\n        self.c_mu = min(1 - self.c_1, 2 * (self.mu - 1 + 1) / ((self.dim + 2)**2 + self.mu))\n\n    def sample_population(self):\n        z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n        C_sqrt = np.linalg.cholesky(self.C)\n        self.population = self.mean + self.sigma * z @ C_sqrt.T\n        self.population = np.clip(self.population, self.bounds_lb, self.bounds_ub)\n\n    def update_archive(self, population, fitness):\n        for i in range(len(population)):\n            x = population[i]\n            f = fitness[i]\n\n            if len(self.archive_x) < self.archive_size:\n                self.archive_x.append(x)\n                self.archive_f.append(f)\n            else:\n                # Replace worst element in archive\n                max_index = np.argmax(self.archive_f) # Assuming minimization\n                if f < self.archive_f[max_index]:\n                    self.archive_x[max_index] = x\n                    self.archive_f[max_index] = f\n\n    def recombine_population(self):\n        # Recombine individuals from the population with individuals from the archive\n        for i in range(self.pop_size):\n            if np.random.rand() < self.archive_probability and len(self.archive_x) > 0:\n                # Choose a random individual from the archive\n                archive_index = np.random.randint(0, len(self.archive_x))\n                archive_individual = self.archive_x[archive_index]\n\n                # Perform recombination (e.g., averaging)\n                alpha = np.random.rand()\n                self.population[i] = alpha * self.population[i] + (1 - alpha) * archive_individual\n                self.population[i] = np.clip(self.population[i], self.bounds_lb, self.bounds_ub)\n\n    def update_parameters(self):\n        sorted_indices = np.argsort(self.fitness)\n        self.population = self.population[sorted_indices]\n        self.fitness = self.fitness[sorted_indices]\n\n        parents = self.population[:self.mu]\n        old_mean = self.mean.copy()\n        self.mean = np.mean(parents, axis=0)\n\n        z = (self.mean - old_mean) / self.sigma\n        C_sqrt = np.linalg.cholesky(self.C)\n        z_in_C_space = np.linalg.solve(C_sqrt.T, z)\n\n        self.P_sigma = (1 - self.c_sigma) * self.P_sigma + np.sqrt(self.c_sigma * (2 - self.c_sigma)) * z_in_C_space\n        hsig = np.linalg.norm(self.P_sigma) / np.sqrt(1 - (1 - self.c_sigma)**(2 * (self.fevals / self.pop_size))) < (1.4 + 2 / (self.dim + 1))\n        self.P_c = (1 - self.c_c) * self.P_c + hsig * np.sqrt(self.c_c * (2 - self.c_c)) * z_in_C_space\n\n        y = parents - old_mean\n        delta_C = np.zeros_like(self.C)\n        for i in range(self.mu):\n            weight = self.c_mu\n            dy = (parents[i] - old_mean) / self.sigma\n            delta_C += weight * np.outer(dy, dy)\n\n        self.C = (1 - self.c_1 - self.c_mu) * self.C + self.c_1 * np.outer(self.P_c, self.P_c) + delta_C\n        self.sigma *= np.exp((self.c_sigma / self.damps) * (np.linalg.norm(self.P_sigma) / np.sqrt(self.dim) - 1))\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n        self.mean = np.random.uniform(self.bounds_lb, self.bounds_ub, size=self.dim)\n        self.population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n\n        self.initialize_parameters()\n        self.c_c = (4 + self.mu / self.dim) / (self.dim + 4 + 2 * self.mu / self.dim)\n\n        while self.fevals < self.budget:\n            self.sample_population()\n            self.recombine_population()  # Recombine with archive\n\n            fitness = np.array([func(x) for x in self.population])\n            self.fevals += self.pop_size\n\n            if self.fevals > self.budget:\n                 fitness = fitness[:self.budget - (self.fevals - self.pop_size)]\n                 self.population = self.population[:self.budget - (self.fevals - self.pop_size)]\n                 break\n           \n            self.fitness = fitness\n            self.update_archive(self.population, self.fitness)\n            self.update_parameters()\n\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm CMAES_ArchiveRecombination scored 0.592 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["421c6573-910e-4246-9e6f-0be9adee1fce"], "operator": null, "metadata": {"aucs": [0.2561871482842426, 0.26797804910373857, 0.6410756842266437, 0.8819251773416223, 0.7405998457081443, 0.8377615545907329, 0.6032596923668254, 0.675410782456316, 0.3126348699785436, 0.6582113291876253, 0.8353133116978217, 0.9712811784110065, 0.292465294949687, 0.571772875734502, 0.6711192830934016, 0.48777152622283504, 0.5016451239918234, 0.912506550721399, 0.2142047546908874, 0.5099779137831912]}}
{"id": "ba881cec-dcf7-437a-99de-cc2a76ef0431", "fitness": 0.4833768346484411, "name": "StochasticRankingDE", "description": "Differential Evolution with stochastic ranking for constraint handling and a simplified parameter adaptation based on the population's success rate.", "code": "import numpy as np\n\nclass StochasticRankingDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, p_cross=0.9, p_rank=0.45):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.p_cross = p_cross\n        self.p_rank = p_rank\n        self.population = None\n        self.fitness = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.F = 0.5  # Mutation factor\n        self.Cr = 0.7 # Crossover rate\n        self.best_fitness = np.inf\n        self.best_solution = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.best_fitness = np.min(self.fitness)\n        self.best_solution = self.population[np.argmin(self.fitness)].copy()\n\n    def evaluate(self, func, individual):\n        return func(individual)\n\n    def mutate(self, pop, i):\n        indices = [idx for idx in range(len(pop)) if idx != i]\n        a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n        mutant = a + self.F * (b - c)\n        mutant = np.clip(mutant, self.bounds_lb, self.bounds_ub)\n        return mutant\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.Cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def stochastic_ranking(self, fitness_current, fitness_trial):\n        \"\"\"\n        Compares two solutions using stochastic ranking.\n\n        Args:\n            fitness_current: Fitness of the current solution.\n            fitness_trial: Fitness of the trial solution.\n\n        Returns:\n            True if the trial solution should be accepted, False otherwise.\n        \"\"\"\n        if fitness_current < 0 or fitness_trial < 0:\n            if np.random.rand() < self.p_rank or fitness_trial < fitness_current:\n                return True\n            else:\n                return False\n        else:\n            if fitness_trial < fitness_current:\n                return True\n            else:\n                return False\n\n    def adjust_parameters(self, success_rate):\n        \"\"\"\n        Adjust F and Cr based on the success rate of the population.\n        \"\"\"\n        if success_rate > 0.2:\n            self.F = min(self.F * 1.1, 0.9)  # Increase F if doing well\n            self.Cr = min(self.Cr * 1.1, 1.0) # Increase Cr if doing well\n        elif success_rate < 0.1:\n            self.F = max(self.F * 0.9, 0.1)  # Decrease F if not doing well\n            self.Cr = max(self.Cr * 0.9, 0.1) # Decrease Cr if not doing well\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.initialize_population(func)\n        fevals = self.pop_size\n        generation = 0\n\n        while fevals < self.budget:\n            generation += 1\n            successful_trials = 0\n            for i in range(self.pop_size):\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, self.population[i])\n\n                fitness_trial = self.evaluate(func, trial)\n                fevals += 1\n\n                if self.stochastic_ranking(self.fitness[i], fitness_trial):\n                    self.fitness[i] = fitness_trial\n                    self.population[i] = trial\n                    successful_trials += 1\n\n                    if fitness_trial < self.best_fitness:\n                        self.best_fitness = fitness_trial\n                        self.best_solution = trial.copy()\n                \n                if fevals >= self.budget:\n                    break\n            \n            # Adjust parameters based on success rate\n            success_rate = successful_trials / self.pop_size\n            self.adjust_parameters(success_rate)\n\n        return self.best_fitness, self.best_solution", "configspace": "", "generation": 7, "feedback": "The algorithm StochasticRankingDE scored 0.483 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["1ca9a63d-ddc0-46cc-8792-cbb02e7c6b94"], "operator": null, "metadata": {"aucs": [0.18770183021700593, 0.23680212080647212, 0.4359035255543361, 0.6996928671997535, 0.4573003580804309, 0.5095355846233379, 0.3793846081550142, 0.39518769407432186, 0.4313300386553166, 0.40004305813876484, 0.733449869903879, 0.9887797067131581, 0.25601623336336854, 0.4642935539733207, 0.7984372900901473, 0.5497889968445977, 0.3918799092832934, 0.6088414219151965, 0.24237106442575174, 0.500796960951356]}}
{"id": "0b747827-f94c-41cb-a157-01708ce0db5d", "fitness": 0.6020434637176966, "name": "ImprovedSHADE_Ortho_AdaptPop_DBE_DynArchive", "description": "Introduces a dynamic archive resizing strategy based on the population diversity and exploration progress, enhancing the balance between exploration and exploitation, alongside a diversity maintenance mechanism.", "code": "import numpy as np\n\nclass ImprovedSHADE_Ortho_AdaptPop_DBE_DynArchive:\n    def __init__(self, budget=10000, dim=10, memory_size=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = int(4 + 3 * np.log(budget))  # Initial population size\n        self.memory_size = memory_size\n        self.memory_F = np.full(memory_size, 0.5)\n        self.memory_CR = np.full(memory_size, 0.5)\n        self.p = 0.1  # Percentage of top individuals for p-best selection\n        self.population = None\n        self.fitness = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.archive = []  # For storing inferior solutions\n        self.archive_size = self.pop_size  # Initial archive size\n        self.memory_index = 0\n        self.sf_M = []\n        self.scr_M = []\n        self.fevals = 0  # Track function evaluations\n        self.min_pop_size = 4  # Minimum population size\n        self.reduction_trigger = 0.25  # Fraction of budget before reduction starts\n        self.reduction_rate = 0.2  # Fraction of population reduced each time\n        self.use_orthogonal = True  # Use orthogonal crossover\n        self.dbe_probability = 0.1  # Probability of using Distance-Based Exploration\n        self.diversity_threshold = 0.1 #Threshold for population diversity.\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.fevals += self.pop_size\n\n    def calculate_distances(self, pop, i):\n        \"\"\"Calculates Euclidean distances from individual i to all other individuals.\"\"\"\n        distances = np.linalg.norm(pop - pop[i], axis=1)\n        distances[i] = np.inf  # Exclude distance to itself\n        return distances\n\n    def mutate(self, pop, i, F, Cr, archive):\n        if np.random.rand() < self.dbe_probability:\n            # Distance-Based Exploration\n            distances = self.calculate_distances(pop, i)\n            r1 = np.argmin(distances)  # Farthest Individual\n\n        else:\n\n            p_best_size = max(int(self.p * len(pop)), 1)\n\n            # Tournament selection for p-best\n            indices = np.argsort(self.fitness)[:p_best_size]\n            p_best_idx = np.random.choice(indices)\n            p_best = pop[p_best_idx]\n            indices = [idx for idx in range(len(pop)) if idx != i]\n            if len(indices) == 0:\n                return pop[i]\n            r1 = np.random.choice(indices)\n\n        # Choose r2 from current pop or archive\n        if len(archive) > 0 and np.random.rand() < 0.5:  # 50% chance to use archive\n            r2_idx = np.random.randint(0, len(archive))\n            r2 = archive[r2_idx]\n        else:\n            indices = [idx for idx in range(len(pop)) if idx != i and idx != r1]  # Ensure distinct indices\n            if len(indices) == 0:\n                r2 = pop[r1]  # If only one individual, take that\n            else:\n                r2_idx = np.random.choice(indices)\n                r2 = pop[r2_idx]\n\n        if np.random.rand() < self.dbe_probability:\n            p_best = pop[np.random.choice(range(len(pop)))]  # Choose random individual when using DBE for p_best\n        else:\n            p_best_size = max(int(self.p * len(pop)), 1)\n            # Tournament selection for p-best\n            indices = np.argsort(self.fitness)[:p_best_size]\n            p_best_idx = np.random.choice(indices)\n            p_best = pop[p_best_idx]\n\n        mutant = p_best + F * (pop[r1] - r2)\n        return mutant\n\n    def orthogonal_crossover(self, mutant, target):\n        # Generate an orthogonal array (example using a simple Hadamard matrix)\n        H = np.array([[1, 1], [1, -1]])\n\n        # Ensure dimensions are compatible by repeating or truncating\n        if self.dim > H.shape[1]:\n            H_repeated = np.tile(H, (1, int(np.ceil(self.dim / H.shape[1]))))\n            H_used = H_repeated[:, :self.dim]\n        else:\n            H_used = H[:, :self.dim]\n\n        trial = np.copy(target)\n\n        for j in range(self.dim):\n            if H_used[0, j] == H_used[1, j]:\n                continue  # No change\n            else:\n                trial[j] = mutant[j]\n\n        return trial\n\n    def crossover(self, mutant, target, Cr):\n        cross_points = np.random.rand(self.dim) < Cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def update_archive(self, individual, archive):\n        archive.append(individual)\n        return archive\n\n    def reduce_population(self):\n        if self.pop_size > self.min_pop_size:\n            sorted_indices = np.argsort(self.fitness)[::-1]  # Worst to best\n            reduction_amount = max(1, int(self.pop_size * self.reduction_rate))  # Reduce by reduction_rate, but at least 1\n            removed_indices = sorted_indices[:reduction_amount]\n\n            self.population = np.delete(self.population, removed_indices, axis=0)\n            self.fitness = np.delete(self.fitness, removed_indices)\n            self.pop_size = len(self.population)\n\n    def calculate_population_diversity(self):\n        \"\"\"Calculates the average distance of each individual to the population mean.\"\"\"\n        mean_position = np.mean(self.population, axis=0)\n        distances = np.linalg.norm(self.population - mean_position, axis=1)\n        avg_distance = np.mean(distances)\n        return avg_distance\n\n    def adjust_archive_size(self):\n        \"\"\"Dynamically adjusts the archive size based on population diversity.\"\"\"\n        diversity = self.calculate_population_diversity()\n\n        if diversity < self.diversity_threshold:\n            # If diversity is low, increase archive size to encourage exploration\n            self.archive_size = min(2 * self.pop_size, self.budget // 10)  # Limit archive size\n        else:\n            # If diversity is high, reduce archive size to focus on exploitation\n            self.archive_size = max(self.pop_size // 2, 10)  # Ensure a minimum archive size\n\n    def maintain_diversity(self):\n         \"\"\"Introduce a diversity maintenance mechanism to prevent premature convergence.\"\"\"\n         diversity = self.calculate_population_diversity()\n         if diversity < self.diversity_threshold:\n             # Select a subset of the population (e.g., the worst 20%)\n             num_to_diversify = max(1, int(0.2 * self.pop_size))  # At least 1\n             worst_indices = np.argsort(self.fitness)[-num_to_diversify:] # Indices of worst individuals\n\n             # Replace them with random individuals from the search space\n             for idx in worst_indices:\n                 self.population[idx] = np.random.uniform(self.bounds_lb, self.bounds_ub, size=self.dim)\n                 self.fitness[idx] = np.inf  # Reset fitness to be re-evaluated in next iteration\n\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n        self.archive = []\n\n        self.sf_M = []\n        self.scr_M = []\n        weights = []\n\n        while self.fevals < self.budget:\n            sf_k = []\n            scr_k = []\n            weights = []\n\n            #Adjust Archive size\n            self.adjust_archive_size()\n\n            for i in range(self.pop_size):\n                # Sample F and CR from memory\n                mem_idx = np.random.randint(0, self.memory_size)\n                F = self.memory_F[mem_idx]\n                Cr = self.memory_CR[mem_idx]\n\n                # Truncate and Reflect F\n                if F > 1:\n                    F = 1\n                if F <= 0:\n                    F = 0.000001\n\n                mutant = self.mutate(self.population, i, F, Cr, self.archive)\n                mutant = np.clip(mutant, self.bounds_lb, self.bounds_ub)\n\n                # Use orthogonal crossover with a probability or with the regular crossover\n                if self.use_orthogonal and np.random.rand() < 0.5:  # 50% chance to use orthogonal crossover\n                    trial = self.orthogonal_crossover(mutant, self.population[i])\n                else:\n                    trial = self.crossover(mutant, self.population[i], Cr)\n\n                f = func(trial)\n                self.fevals += 1\n\n                if f < self.fitness[i]:\n                    delta = abs(f - self.fitness[i])\n                    sf_k.append(F)\n                    scr_k.append(Cr)\n                    weights.append(delta)\n\n                    self.archive = self.update_archive(self.population[i], self.archive)\n                    if len(self.archive) > self.archive_size:\n                        self.archive.pop(np.random.randint(0, len(self.archive)))  # Keep archive size constant\n\n                    self.fitness[i] = f\n                    self.population[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                else:\n                    self.archive = self.update_archive(trial, self.archive)  # Add even unsuccessful trials\n                    if len(self.archive) > self.archive_size:\n                        self.archive.pop(np.random.randint(0, len(self.archive)))\n\n                if self.fevals >= self.budget:\n                    break\n\n            # Update memory using Lehmer mean\n            if sf_k:\n                weights = np.array(weights) / np.sum(weights)\n                sum_weights_sfk = np.sum(weights * np.array(sf_k) ** 2)\n                sum_weights = np.sum(weights * np.array(sf_k))\n\n                if sum_weights > 0:\n                    mean_F = sum_weights_sfk / sum_weights\n                else:\n                    mean_F = 0.5\n\n                mean_CR = np.average(scr_k, weights=weights)\n\n                self.memory_F[self.memory_index] = mean_F\n                self.memory_CR[self.memory_index] = mean_CR\n                self.memory_index = (self.memory_index + 1) % self.memory_size\n\n            # Adaptive population size reduction\n            if self.fevals > self.budget * self.reduction_trigger:  # Start reducing after reduction_trigger of the budget is spent\n                self.reduce_population()\n\n            self.maintain_diversity() #Maintain diversity\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm ImprovedSHADE_Ortho_AdaptPop_DBE_DynArchive scored 0.602 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c6e5bc64-3827-4b00-976f-bb47b493ee62"], "operator": null, "metadata": {"aucs": [0.22876622592310925, 0.33495192071162005, 0.5510332114054233, 0.9035120308975227, 0.7901651190626277, 0.7131951891435815, 0.35593767256072006, 0.5512901626278854, 0.6450801244881543, 0.6118632564918615, 0.8367996579008814, 0.9990759181425791, 0.2263613935493224, 0.5283655507471474, 0.7895382307188547, 0.8146109404763776, 0.5118559648689951, 0.8303745507221243, 0.3163576187717456, 0.5017345351433966]}}
{"id": "818d0090-e624-457f-af15-5ba91399ed24", "fitness": 0.44577544449390993, "name": "AdaptiveDE_ReducePop_Velocity_OrthoCrossover", "description": "Implements a modified Adaptive Differential Evolution with dynamic population size adjustment, velocity-based mutation, orthogonal crossover, and a ring topology-based parameter adaptation strategy.", "code": "import numpy as np\n\nclass AdaptiveDE_ReducePop_Velocity_OrthoCrossover:\n    def __init__(self, budget=10000, dim=10, archive_size=5, memory_size=5, initial_pop_size=50, ring_neighbors=3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.initial_pop_size = initial_pop_size\n        self.population = None\n        self.fitness = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.archive = []  # Archive for storing removed individuals\n        self.archive_size = archive_size\n        self.memory_size = memory_size\n        self.memory_F = np.full(self.memory_size, 0.5)\n        self.memory_Cr = np.full(self.memory_size, 0.5)\n        self.sf = []\n        self.scr = []\n        self.p = 0.1  # Probability of using archive individual\n        self.w = 0.5  # Weight for Lehmer mean update\n        self.reduction_factor = 0.95 # Factor for population size reduction\n        self.velocity = None\n        self.ring_neighbors = ring_neighbors\n        self.min_pop_size = 10\n\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.velocity = np.zeros_like(self.population) # Initialize velocity\n\n    def mutate(self, pop, i, current_generation):\n        memory_index = current_generation % self.memory_size\n        Cr = self.memory_Cr[memory_index]\n        F = self.memory_F[memory_index]\n\n        indices = [idx for idx in range(len(pop)) if idx != i]\n        if len(self.archive) > 0 and np.random.rand() < self.p:\n            arch_ind = np.random.randint(0, len(self.archive))\n            pop_indices = np.random.choice(indices, 2, replace=False)\n            a = self.archive[arch_ind]\n            b, c = pop[pop_indices[0]], pop[pop_indices[1]]\n        else:\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n\n        # Velocity-based mutation\n        self.velocity[i] = F * (a - pop[i]) + F * (b - c)\n        mutation = pop[i] + self.velocity[i]\n\n        return mutation, Cr, F\n\n    def reflecting_boundary_handling(self, mutant):\n        # Apply reflecting boundary handling\n        for i in range(self.dim):\n            if mutant[i] < self.bounds_lb[i]:\n                mutant[i] = self.bounds_lb[i] + np.abs(mutant[i] - self.bounds_lb[i])\n            elif mutant[i] > self.bounds_ub[i]:\n                mutant[i] = self.bounds_ub[i] - np.abs(mutant[i] - self.bounds_ub[i])\n\n            # Ensure the value remains within the bounds after reflection\n            mutant[i] = np.clip(mutant[i], self.bounds_lb[i], self.bounds_ub[i])\n        return mutant\n\n    def orthogonal_crossover(self, mutant, target):\n        # Create orthogonal matrix (Hadamard matrix)\n        H = self.create_hadamard_matrix(self.dim)\n\n        # Orthogonal Crossover\n        trial = np.zeros_like(mutant)\n        for i in range(self.dim):\n            if H[i, 0] > 0:  # Select based on Hadamard matrix values\n                trial[i] = mutant[i]\n            else:\n                trial[i] = target[i]\n        return trial\n\n    def create_hadamard_matrix(self, n):\n        # Ensure n is a power of 2 for Hadamard matrix creation\n        if n & (n - 1) != 0:\n            n = 2**int(np.ceil(np.log2(n)))  # Pad to the next power of 2\n        if n == 1:\n            return np.array([[1]])\n        H = np.array([[1, 1], [1, -1]])\n        while H.shape[0] < n:\n            H = np.kron(H, np.array([[1, 1], [1, -1]]))\n        return H[:self.dim, :self.dim]  # Truncate to original dimension\n    \n\n    def update_memory(self, i):\n        if self.sf and self.scr:\n            self.sf = np.array(self.sf)\n            self.scr = np.array(self.scr)\n\n            # Lehmer mean for F\n            sum_sf_squared = np.sum(self.sf ** 2)\n            sum_sf = np.sum(self.sf)\n            if sum_sf > 0:\n                lehmer_mean_F = sum_sf_squared / sum_sf\n            else:\n                lehmer_mean_F = 0.5  # Default value if no successful F values\n\n            # Lehmer mean for Cr\n            lehmer_mean_Cr = np.mean(self.scr)\n\n            memory_index = self.generation % self.memory_size\n            self.memory_F[memory_index] = self.w * self.memory_F[memory_index] + (1 - self.w) * lehmer_mean_F\n            self.memory_Cr[memory_index] = self.w *  self.memory_Cr[memory_index] + (1 - self.w) * lehmer_mean_Cr\n\n            self.sf = []\n            self.scr = []\n\n    def reduce_population(self):\n        if self.pop_size > self.min_pop_size:  # Minimum population size\n            # Sort population based on fitness\n            sorted_indices = np.argsort(self.fitness)[::-1]\n            num_to_remove = max(1, int(self.pop_size * (1 - self.reduction_factor)))  # Reduce at least by 1\n            \n            # Archive the individuals to be removed\n            for i in range(num_to_remove):\n                if len(self.archive) < self.archive_size:\n                    self.archive.append(self.population[sorted_indices[i]].copy())\n                else:\n                    replace_index = np.random.randint(0, self.archive_size)\n                    self.archive[replace_index] = self.population[sorted_indices[i]].copy()\n\n            # Keep the best individuals\n            keep_indices = sorted_indices[num_to_remove:]\n            self.population = self.population[keep_indices]\n            self.fitness = self.fitness[keep_indices]\n            self.velocity = self.velocity[keep_indices]\n            self.pop_size = len(self.population)\n\n    def ring_topology_adaptation(self):\n        # Implement a ring topology-based adaptation for F and Cr\n        for i in range(self.pop_size):\n            neighbors_indices = [(i + j) % self.pop_size for j in range(-self.ring_neighbors, self.ring_neighbors + 1) if j != 0]\n            \n            # Gather F and Cr values from neighbors\n            neighbor_F = [self.memory_F[self.generation % self.memory_size] for _ in neighbors_indices]\n            neighbor_Cr = [self.memory_Cr[self.generation % self.memory_size] for _ in neighbors_indices]\n\n            # Adapt F and Cr based on neighbors' values (e.g., average)\n            adapted_F = np.mean(neighbor_F) if neighbor_F else self.memory_F[self.generation % self.memory_size]\n            adapted_Cr = np.mean(neighbor_Cr) if neighbor_Cr else self.memory_Cr[self.generation % self.memory_size]\n            \n            # Update memory with adapted values\n            memory_index = self.generation % self.memory_size\n            self.memory_F[memory_index] = adapted_F\n            self.memory_Cr[memory_index] = adapted_Cr\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n        fevals = self.pop_size\n        self.generation = 0\n        self.sf = []\n        self.scr = []\n        \n        while fevals < self.budget:\n            self.generation += 1\n            self.ring_topology_adaptation()  # Adapt F and Cr based on ring topology\n\n            for i in range(self.pop_size):\n                mutant, Cr, F = self.mutate(self.population, i, self.generation)\n                mutant = self.reflecting_boundary_handling(mutant)  # Reflecting boundary handling\n                trial = self.orthogonal_crossover(mutant, self.population[i])\n                \n                f = func(trial)\n                fevals += 1\n\n                if f < self.fitness[i]:\n                    # Archive the replaced individual\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        # Replace a random element in the archive\n                        replace_index = np.random.randint(0, self.archive_size)\n                        self.archive[replace_index] = self.population[i].copy()\n\n                    self.sf.append(F)\n                    self.scr.append(Cr)\n                    self.fitness[i] = f\n                    self.population[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                \n                if fevals >= self.budget:\n                    break\n\n            self.update_memory(i)\n\n            # Reduce population size every few generations (e.g., every 5 generations)\n            if self.generation % 5 == 0:\n                self.reduce_population()\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveDE_ReducePop_Velocity_OrthoCrossover scored 0.446 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["1ca9a63d-ddc0-46cc-8792-cbb02e7c6b94"], "operator": null, "metadata": {"aucs": [0.1472650601186093, 0.6175578744772476, 0.3829804294304997, 0.5974613506629147, 0.6286322021302148, 0.26962541003684637, 0.31048111764931896, 0.3821250962073405, 0.5173715413098633, 0.20257448873203876, 0.5402390317689874, 0.9812860052079169, 0.3429346494919707, 0.6551644079757357, 0.6254210421993082, 0.3074695754614222, 0.32947827755274106, 0.4022857601943892, 0.19821749294167534, 0.47693807632915886]}}
{"id": "c70f7968-4165-4315-acd2-218ae47b96ce", "fitness": 0.32743122850271483, "name": "AdaptiveDE_OrthoLearn_Restart", "description": "Adaptive Differential Evolution with orthogonal learning and a restart mechanism based on stagnation detection.", "code": "import numpy as np\n\nclass AdaptiveDE_OrthoLearn_Restart:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=5, memory_size=5, restart_threshold=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.memory_size = memory_size\n        self.restart_threshold = restart_threshold  # Number of iterations without improvement before restart\n        self.memory_F = np.full(self.memory_size, 0.5)\n        self.memory_Cr = np.full(self.memory_size, 0.5)\n        self.archive = []\n        self.sf = []\n        self.scr = []\n        self.p = 0.1\n        self.w = 0.5\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n        self.best_fitness_history.append(self.f_opt)\n\n    def mutate(self, pop, i, current_generation):\n        memory_index = current_generation % self.memory_size\n        Cr = self.memory_Cr[memory_index]\n        F = self.memory_F[memory_index]\n\n        indices = [idx for idx in range(len(pop)) if idx != i]\n        if len(self.archive) > 0 and np.random.rand() < self.p:\n            arch_ind = np.random.randint(0, len(self.archive))\n            pop_indices = np.random.choice(indices, 2, replace=False)\n            a = self.archive[arch_ind]\n            b, c = pop[pop_indices[0]], pop[pop_indices[1]]\n        else:\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n        \n        mutation = a + F * (b - c)\n        return mutation, Cr, F\n\n    def orthogonal_learning(self, trial):\n        # Generate an orthogonal matrix (e.g., using Hadamard matrix if dim is a power of 2)\n        if (self.dim & (self.dim - 1) == 0): # Check if dim is a power of 2\n            H = self.hadamard(self.dim)\n        else:\n            # If not a power of 2, use random orthogonal matrix\n            H = np.random.randn(self.dim, self.dim)\n            Q, R = np.linalg.qr(H)\n            H = Q\n        \n        # Generate new solutions based on the orthogonal matrix\n        new_solutions = []\n        for i in range(self.dim):\n            direction = H[i, :]\n            step_size = np.random.uniform(-0.1, 0.1) # Small step size\n            new_point = trial + step_size * direction\n            new_point = np.clip(new_point, self.bounds_lb, self.bounds_ub)\n            new_solutions.append(new_point)\n\n        return new_solutions\n\n    def hadamard(self, n):\n        if n == 1:\n            return np.array([[1]])\n        H = self.hadamard(n // 2)\n        return np.vstack([np.hstack([H, H]), np.hstack([H, -H])])\n\n    def crossover(self, mutant, target, Cr):\n        cross_points = np.random.rand(self.dim) < Cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def update_memory(self):\n        if self.sf:\n            self.sf = np.array(self.sf)\n            self.scr = np.array(self.scr)\n\n            # Lehmer mean for F\n            sum_sf_squared = np.sum(self.sf ** 2)\n            sum_sf = np.sum(self.sf)\n            if sum_sf > 0:\n                lehmer_mean_F = sum_sf_squared / sum_sf\n            else:\n                lehmer_mean_F = 0.5  # Default value if no successful F values\n\n            # Mean for Cr\n            lehmer_mean_Cr = np.mean(self.scr)\n\n            memory_index = self.generation % self.memory_size\n            self.memory_F[memory_index] = self.w * self.memory_F[memory_index] + (1 - self.w) * lehmer_mean_F\n            self.memory_Cr[memory_index] = self.w *  self.memory_Cr[memory_index] + (1 - self.w) * lehmer_mean_Cr\n\n        self.sf = []\n        self.scr = []\n\n    def restart_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.initialize_population(func)\n        fevals = self.pop_size\n        self.generation = 0\n\n        while fevals < self.budget:\n            self.generation += 1\n            improved = False\n            for i in range(self.pop_size):\n                mutant, Cr, F = self.mutate(self.population, i, self.generation)\n                trial = self.crossover(mutant, self.population[i], Cr)\n\n                # Orthogonal learning\n                new_solutions = self.orthogonal_learning(trial)\n                new_fitnesses = [func(x) for x in new_solutions]\n                fevals += len(new_solutions)\n                best_index = np.argmin(new_fitnesses)\n                best_trial = new_solutions[best_index]\n                f = new_fitnesses[best_index]\n\n                if f < self.fitness[i]:\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        replace_index = np.random.randint(0, self.archive_size)\n                        self.archive[replace_index] = self.population[i].copy()\n\n                    self.sf.append(F)\n                    self.scr.append(Cr)\n                    self.fitness[i] = f\n                    self.population[i] = best_trial\n                    improved = True\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = best_trial\n                \n                if fevals >= self.budget:\n                    break\n\n            self.update_memory()\n\n            if improved:\n                self.stagnation_counter = 0\n                self.best_fitness_history.append(self.f_opt)\n            else:\n                self.stagnation_counter += 1\n\n            if self.stagnation_counter > self.restart_threshold:\n                self.restart_population(func)\n                self.stagnation_counter = 0\n                self.best_fitness_history.append(self.f_opt)\n                fevals += self.pop_size  # Account for evaluations during restart\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveDE_OrthoLearn_Restart scored 0.327 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["1ca9a63d-ddc0-46cc-8792-cbb02e7c6b94"], "operator": null, "metadata": {"aucs": [0.13644731533831522, 0.19495076096621555, 0.2974928634622349, 0.2835074592509157, 0.247172134071591, 0.31666564800843244, 0.2785995408608002, 0.27361070865340553, 0.24518304981857175, 0.1925366021676146, 0.273203334847279, 0.9710955079953192, 0.26620682272359575, 0.27146084651952374, 0.6708587205655515, 0.37000116029202246, 0.26697553236428173, 0.34664834678916534, 0.17379775293280397, 0.47221046242665743]}}
{"id": "6a00d247-7e32-470c-a43b-4f601768a3ca", "fitness": 0.21820135871755136, "name": "DynamicVoronoiDE", "description": "A self-tuning Differential Evolution with a dynamic population size based on the function evaluations remaining, using Voronoi-based diversity maintenance and a combined mutation strategy of DE/rand/1 and DE/current-to-best/1.", "code": "import numpy as np\nfrom scipy.spatial import Voronoi\n\nclass DynamicVoronoiDE:\n    def __init__(self, budget=10000, dim=10, pop_scaling=4):\n        self.budget = budget\n        self.dim = dim\n        self.pop_scaling = pop_scaling\n        self.population = None\n        self.fitness = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.F = 0.5\n        self.Cr = 0.9\n        self.min_pop = 4\n\n    def initialize_population(self, func):\n        self.pop_size = max(self.min_pop, int(self.pop_scaling * np.log(self.dim)))\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n\n    def dynamic_population_size(self, remaining_evals):\n          # Linearly decrease population size based on remaining evaluations\n          new_pop_size = max(self.min_pop, int(self.pop_scaling * np.log(self.dim) * (remaining_evals / self.budget)))\n          if new_pop_size != self.pop_size:\n              self.resize_population(new_pop_size)\n              \n    def resize_population(self, new_size):\n        if new_size < self.pop_size:\n            # Remove worst individuals\n            indices_to_keep = np.argsort(self.fitness)[:new_size]\n            self.population = self.population[indices_to_keep]\n            self.fitness = self.fitness[indices_to_keep]\n        elif new_size > self.pop_size:\n            # Add new random individuals\n            num_to_add = new_size - self.pop_size\n            new_individuals = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(num_to_add, self.dim))\n            new_fitness = np.array([self.func(x) for x in new_individuals])  # Evaluate outside the loop\n            self.population = np.vstack((self.population, new_individuals))\n            self.fitness = np.concatenate((self.fitness, new_fitness))\n            self.fevals += num_to_add\n            \n        self.pop_size = new_size\n\n    def voronoi_diversity(self):\n        try:\n            vor = Voronoi(self.population)\n            volumes = []\n            for region_index in vor.point_region:\n                if region_index == -1:\n                    volumes.append(0)\n                else:\n                    region = vor.regions[region_index]\n                    if not all(v >= 0 for v in region):\n                        volumes.append(0)\n                    else:\n                        vertices = vor.vertices[region]\n                        if vertices.shape[0] < 2:\n                             volumes.append(0)\n                             continue\n\n                        # Approximate volume calculation\n                        center = np.mean(vertices, axis=0)\n                        total_volume = 0\n                        for i in range(len(vertices)):\n                           j = (i + 1) % len(vertices)\n                           triangle = np.vstack((vertices[i], vertices[j], center))\n                           #For 3D and higher dimension use other method.\n                           if self.dim <= 3:\n                             total_volume += np.abs(np.linalg.det(triangle) / 6.0)  # Volume of tetrahedron\n                           else:\n                             total_volume += np.abs(np.linalg.norm(np.cross(vertices[j]-vertices[i], center-vertices[i]))) / 2.0  # Area of triangle in higher D.\n                        volumes.append(total_volume)\n            volumes = np.array(volumes)\n            normalized_volumes = volumes / np.sum(volumes)\n            return normalized_volumes\n\n        except Exception as e:\n            # Handle cases where Voronoi computation fails\n            print(f\"Voronoi error: {e}\")\n            return np.ones(self.pop_size) / self.pop_size\n\n\n    def mutate(self, i):\n        if np.random.rand() < 0.5:\n            # DE/rand/1\n            indices = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[indices]\n            mutant = x_r1 + self.F * (x_r2 - x_r3)\n        else:\n            # DE/current-to-best/1\n            best_index = np.argmin(self.fitness)\n            x_best = self.population[best_index]\n            indices = [idx for idx in range(self.pop_size) if idx != i]\n            if not indices:\n                indices = [i] #Ensure we do not have zero population\n            r1 = np.random.choice(indices)\n            mutant = self.population[i] + self.F * (x_best - self.population[i]) + self.F * (self.population[r1] - self.population[i])\n        return mutant\n\n    def boundary_handling(self, mutant):\n        return np.clip(mutant, self.bounds_lb, self.bounds_ub)\n\n    def crossover(self, mutant, target):\n        trial = target.copy()\n        j_rand = np.random.randint(self.dim)\n        for i in range(self.dim):\n            if np.random.rand() < self.Cr or i == j_rand:\n                trial[i] = mutant[i]\n        return trial\n\n    def __call__(self, func):\n        self.func = func\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n        self.fevals = self.pop_size\n\n        while self.fevals < self.budget:\n            self.dynamic_population_size(self.budget - self.fevals) # Adjust population size\n            normalized_volumes = self.voronoi_diversity() #Calculate diversity\n\n            for i in range(self.pop_size):\n                mutant = self.mutate(i)\n                mutant = self.boundary_handling(mutant)\n                trial = self.crossover(mutant, self.population[i])\n                f = func(trial)\n                self.fevals += 1\n\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.population[i] = trial\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n\n                if self.fevals >= self.budget:\n                    break\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm DynamicVoronoiDE scored 0.218 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["504cc04a-9022-43fd-80df-d2ddae74a75b"], "operator": null, "metadata": {"aucs": [0.07977758897627063, 0.17498581466619179, 0.2366708065598816, 0.15715563730138804, 0.11773281520943202, 0.1641074922753678, 0.193333710344179, 0.1743295232473261, 0.17439038611085766, 0.1422239184530414, 0.1515554451911425, 0.9993643366446503, 0.21897304958518626, 0.13424808996211868, 0.12736741079217795, 0.2274053062058491, 0.18002302797724368, 0.13675763585095468, 0.10130682728961915, 0.47231835170814873]}}
{"id": "0be72194-0e98-4c2d-9c97-6baded979bff", "fitness": -Infinity, "name": "SelfAdaptiveSRDE_BFGS", "description": "Implements Differential Evolution with a self-adaptive mutation factor, stochastic ranking, and a local search component using a simplified BFGS update to refine promising solutions.", "code": "import numpy as np\nfrom scipy.linalg import solve\nfrom scipy.optimize import minimize\n\nclass SelfAdaptiveSRDE_BFGS:\n    def __init__(self, budget=10000, dim=10, pop_size=50, p_cross=0.9, p_rank=0.45, local_search_freq=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.p_cross = p_cross\n        self.p_rank = p_rank\n        self.population = None\n        self.fitness = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.F = 0.5  # Initial mutation factor\n        self.Cr = 0.7 # Initial Crossover rate\n        self.best_fitness = np.inf\n        self.best_solution = None\n        self.memory_F = np.ones(self.pop_size) * self.F  # Memory for F values\n        self.memory_Cr = np.ones(self.pop_size) * self.Cr # Memory for Cr values\n        self.local_search_freq = local_search_freq\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.best_fitness = np.min(self.fitness)\n        self.best_solution = self.population[np.argmin(self.fitness)].copy()\n\n    def evaluate(self, func, individual):\n        return func(individual)\n\n    def mutate(self, pop, i, gen):\n        indices = [idx for idx in range(len(pop)) if idx != i]\n        a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n\n        # Self-adaptive mutation factor\n        self.F = self.memory_F[i]\n        mutant = a + self.F * (b - c)\n        mutant = np.clip(mutant, self.bounds_lb, self.bounds_ub)\n        return mutant\n\n    def crossover(self, mutant, target, i):\n        # Self-adaptive crossover rate\n        self.Cr = self.memory_Cr[i]\n        cross_points = np.random.rand(self.dim) < self.Cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def stochastic_ranking(self, fitness_current, fitness_trial):\n        if fitness_current < 0 or fitness_trial < 0:\n            if np.random.rand() < self.p_rank or fitness_trial < fitness_current:\n                return True\n            else:\n                return False\n        else:\n            if fitness_trial < fitness_current:\n                return True\n            else:\n                return False\n\n    def update_memory(self, i, F_val, Cr_val):\n         self.memory_F[i] = F_val\n         self.memory_Cr[i] = Cr_val\n\n    def local_search(self, func, x):\n        # Simplified BFGS update\n        result = minimize(func, x, method='L-BFGS-B', bounds=func.bounds, options={'maxiter': 5}) # Reduced iterations to save budget\n        return result.fun, result.x\n    \n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.initialize_population(func)\n        fevals = self.pop_size\n        generation = 0\n\n        while fevals < self.budget:\n            generation += 1\n            successful_trials = 0\n            for i in range(self.pop_size):\n                mutant = self.mutate(self.population, i, generation)\n                trial = self.crossover(mutant, self.population[i], i)\n\n                fitness_trial = self.evaluate(func, trial)\n                fevals += 1\n\n                if self.stochastic_ranking(self.fitness[i], fitness_trial):\n                    self.fitness[i] = fitness_trial\n                    self.population[i] = trial\n                    successful_trials += 1\n                    self.update_memory(i, np.random.uniform(0.1, 0.9), np.random.uniform(0.1, 0.9)) # Update memory with random values\n\n                    if fitness_trial < self.best_fitness:\n                        self.best_fitness = fitness_trial\n                        self.best_solution = trial.copy()\n                else:\n                     self.update_memory(i, self.F, self.Cr) # Keep old values\n                \n                if fevals >= self.budget:\n                    break\n            \n            # Local search every few generations\n            if generation % self.local_search_freq == 0:\n                for i in range(self.pop_size):\n                    f_ls, x_ls = self.local_search(func, self.population[i].copy())\n                    fevals += 5 # Account for the local search function evaluations\n                    if f_ls < self.fitness[i]:\n                        self.fitness[i] = f_ls\n                        self.population[i] = x_ls\n                        if f_ls < self.best_fitness:\n                            self.best_fitness = f_ls\n                            self.best_solution = x_ls.copy()\n\n            if fevals >= self.budget:\n                break\n\n        return self.best_fitness, self.best_solution", "configspace": "", "generation": 8, "feedback": "An exception occurred: name 'minimize' is not defined.", "error": "", "parent_ids": ["ba881cec-dcf7-437a-99de-cc2a76ef0431"], "operator": null, "metadata": {}}
{"id": "31412513-16d0-4859-a646-cd94dc19025c", "fitness": 0.0, "name": "SelfAdaptiveStochasticRankingDE", "description": "Implements a self-adaptive differential evolution with a modified stochastic ranking that dynamically adjusts based on success and introduces a local search component for exploitation.", "code": "import numpy as np\n\nclass SelfAdaptiveStochasticRankingDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, p_rank=0.45, F_init=0.5, Cr_init=0.7, local_search_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.p_rank = p_rank\n        self.population = None\n        self.fitness = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.F = np.full(pop_size, F_init)  # Mutation factor for each individual\n        self.Cr = np.full(pop_size, Cr_init) # Crossover rate for each individual\n        self.best_fitness = np.inf\n        self.best_solution = None\n        self.fevals = 0\n        self.local_search_prob = local_search_prob  # Probability of performing local search\n        self.archive = [] # Archive of past solutions\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.fevals += self.pop_size\n        self.best_fitness = np.min(self.fitness)\n        self.best_solution = self.population[np.argmin(self.fitness)].copy()\n        self.archive = list(self.population.copy()) # Initialize the archive with initial pop\n\n    def evaluate(self, func, individual):\n        if self.fevals < self.budget:\n            self.fevals += 1\n            return func(individual)\n        else:\n            return np.inf\n\n    def mutate(self, pop, i):\n        indices = [idx for idx in range(len(pop)) if idx != i]\n        a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n        mutant = a + self.F[i] * (b - c)\n        mutant = np.clip(mutant, self.bounds_lb, self.bounds_ub)\n        return mutant\n\n    def crossover(self, mutant, target, cr):\n        cross_points = np.random.rand(self.dim) < cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def stochastic_ranking(self, fitness_current, fitness_trial):\n        if fitness_current < 0 or fitness_trial < 0:\n            if np.random.rand() < self.p_rank or fitness_trial < fitness_current:\n                return True\n            else:\n                return False\n        else:\n            if fitness_trial < fitness_current:\n                return True\n            else:\n                return False\n\n    def update_parameters(self, i, success):\n        \"\"\"\n        Update F and Cr for individual i based on success.\n        \"\"\"\n        if success:\n            self.F[i] = np.clip(np.random.normal(0.5, 0.3), 0.1, 0.9)\n            self.Cr[i] = np.clip(np.random.normal(0.7, 0.3), 0.1, 1.0)\n        else:\n            self.F[i] = np.clip(np.random.normal(self.F[i], 0.3), 0.1, 0.9)\n            self.Cr[i] = np.clip(np.random.normal(self.Cr[i], 0.3), 0.1, 1.0)\n\n    def local_search(self, individual, func, step_size=0.1):\n        \"\"\"\n        Performs a simple local search around the given individual.\n        \"\"\"\n        for _ in range(5):  # Perform a few local search steps\n            if self.fevals >= self.budget:\n              break\n            direction = np.random.uniform(-1, 1, size=self.dim)\n            new_individual = individual + step_size * direction\n            new_individual = np.clip(new_individual, self.bounds_lb, self.bounds_ub)\n            new_fitness = self.evaluate(func, new_individual)\n            if new_fitness < self.best_fitness:\n                self.best_fitness = new_fitness\n                self.best_solution = new_individual.copy()\n            if new_fitness < func(individual):\n              individual = new_individual.copy()\n        return individual\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.initialize_population(func)\n\n        while self.fevals < self.budget:\n            for i in range(self.pop_size):\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, self.population[i], self.Cr[i])\n\n                fitness_trial = self.evaluate(func, trial)\n\n                if fitness_trial < self.best_fitness:\n                    self.best_fitness = fitness_trial\n                    self.best_solution = trial.copy()\n                \n                if self.stochastic_ranking(self.fitness[i], fitness_trial):\n                    success = True\n                    self.fitness[i] = fitness_trial\n                    self.population[i] = trial\n                \n                    if np.array_equal(self.population[i], self.archive[-1]) == False:\n                      self.archive.append(self.population[i].copy()) # Add to the archive\n\n                else:\n                    success = False\n\n                self.update_parameters(i, success)\n\n                # Local search with probability\n                if np.random.rand() < self.local_search_prob:\n                    self.population[i] = self.local_search(self.population[i].copy(), func)\n                    self.fitness[i] = func(self.population[i])\n                    self.fevals += 1\n\n                    if self.fitness[i] < self.best_fitness:\n                        self.best_fitness = self.fitness[i]\n                        self.best_solution = self.population[i].copy()\n                if self.fevals >= self.budget:\n                    break\n\n        return self.best_fitness, self.best_solution", "configspace": "", "generation": 8, "feedback": "The algorithm SelfAdaptiveStochasticRankingDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ba881cec-dcf7-437a-99de-cc2a76ef0431"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "df8d66a5-4ac5-42ea-bda7-63665f6a06d9", "fitness": 0.0, "name": "OrthogonalLearningDE", "description": "Differential Evolution with a self-adaptive strategy for F and CR parameters, using a history-based adaptation combined with orthogonal learning to enhance exploration.", "code": "import numpy as np\n\nclass OrthogonalLearningDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.population = None\n        self.fitness = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.memory_F = np.full(archive_size, 0.5)\n        self.memory_Cr = np.full(archive_size, 0.7)\n        self.archive = []\n        self.best_fitness = np.inf\n        self.best_solution = None\n        self.sf = []\n        self.scr = []\n        self.p_rank = 0.45 # Stochastic ranking probability\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.best_fitness = np.min(self.fitness)\n        self.best_solution = self.population[np.argmin(self.fitness)].copy()\n\n    def evaluate(self, func, individual):\n        return func(individual)\n\n    def mutate(self, pop, i, F):\n        indices = [idx for idx in range(len(pop)) if idx != i]\n        a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n        mutant = a + F * (b - c)\n        mutant = np.clip(mutant, self.bounds_lb, self.bounds_ub)\n        return mutant\n\n    def crossover(self, mutant, target, Cr):\n        cross_points = np.random.rand(self.dim) < Cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def orthogonal_learning(self, trial):\n        \"\"\"\n        Performs orthogonal learning to generate a potentially better solution.\n        \"\"\"\n        num_samples = 2\n        ol_solutions = []\n        for _ in range(num_samples):\n            # Generate a random vector in the search space\n            rand_vec = np.random.uniform(self.bounds_lb, self.bounds_ub, size=self.dim)\n            # Combine the trial vector with the random vector\n            new_solution = 0.5 * (trial + rand_vec)\n            new_solution = np.clip(new_solution, self.bounds_lb, self.bounds_ub)\n            ol_solutions.append(new_solution)\n\n        return ol_solutions\n\n    def update_memory(self, F, Cr):\n        self.sf.append(F)\n        self.scr.append(Cr)\n\n    def select_memory(self):\n        idx = np.random.randint(0, self.archive_size)\n        return self.memory_F[idx], self.memory_Cr[idx]\n\n    def update_archive(self, x):\n        self.archive.append(x)\n        if len(self.archive) > self.archive_size:\n            self.archive.pop(0)\n\n    def stochastic_ranking(self, fitness_current, fitness_trial):\n        \"\"\"\n        Compares two solutions using stochastic ranking.\n\n        Args:\n            fitness_current: Fitness of the current solution.\n            fitness_trial: Fitness of the trial solution.\n\n        Returns:\n            True if the trial solution should be accepted, False otherwise.\n        \"\"\"\n        if fitness_current < 0 or fitness_trial < 0:\n            if np.random.rand() < self.p_rank or fitness_trial < fitness_current:\n                return True\n            else:\n                return False\n        else:\n            if fitness_trial < fitness_current:\n                return True\n            else:\n                return False\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.initialize_population(func)\n        fevals = self.pop_size\n        generation = 0\n\n        while fevals < self.budget:\n            generation += 1\n            self.sf = []\n            self.scr = []\n\n            for i in range(self.pop_size):\n                F, Cr = self.select_memory()\n                F = np.clip(np.random.normal(F, 0.1), 0.1, 0.9)\n                Cr = np.clip(np.random.normal(Cr, 0.1), 0.1, 0.9)\n\n                mutant = self.mutate(self.population, i, F)\n                trial = self.crossover(mutant, self.population[i], Cr)\n\n                fitness_trial = self.evaluate(func, trial)\n                fevals += 1\n\n                # Orthogonal Learning\n                ol_solutions = self.orthogonal_learning(trial)\n                ol_fitnesses = [self.evaluate(func, sol) for sol in ol_solutions]\n                fevals += len(ol_solutions)\n                best_ol_index = np.argmin(ol_fitnesses)\n                best_ol_solution = ol_solutions[best_ol_index]\n                fitness_ol = ol_fitnesses[best_ol_index]\n\n                if self.stochastic_ranking(self.fitness[i], fitness_trial):\n                    if self.stochastic_ranking(fitness_trial, fitness_ol):\n                         self.fitness[i] = fitness_ol\n                         self.population[i] = best_ol_solution\n                         if fitness_ol < self.best_fitness:\n                             self.best_fitness = fitness_ol\n                             self.best_solution = best_ol_solution.copy()\n                             self.update_memory(F,Cr)\n                    else:\n                        self.fitness[i] = fitness_trial\n                        self.population[i] = trial\n                        if fitness_trial < self.best_fitness:\n                             self.best_fitness = fitness_trial\n                             self.best_solution = trial.copy()\n                             self.update_memory(F,Cr)\n                else:\n                    pass\n\n                if fevals >= self.budget:\n                    break\n\n            if self.sf and self.scr:\n                self.memory_F[generation % self.archive_size] = np.mean(self.sf) if self.sf else 0.5\n                self.memory_Cr[generation % self.archive_size] = np.mean(self.scr) if self.scr else 0.7\n\n        return self.best_fitness, self.best_solution", "configspace": "", "generation": 8, "feedback": "The algorithm OrthogonalLearningDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ba881cec-dcf7-437a-99de-cc2a76ef0431"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "7f9d9d72-43cc-4ec7-936a-64a0f789dcfd", "fitness": -Infinity, "name": "ImprovedSHADE_Ortho_AdaptPop_DBE_DynArchive_Cluster", "description": "Combines improved SHADE with dynamic diversity control via clustering and orthogonal learning to enhance exploration and exploitation.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass ImprovedSHADE_Ortho_AdaptPop_DBE_DynArchive_Cluster:\n    def __init__(self, budget=10000, dim=10, memory_size=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = int(4 + 3 * np.log(budget))  # Initial population size\n        self.memory_size = memory_size\n        self.memory_F = np.full(memory_size, 0.5)\n        self.memory_CR = np.full(memory_size, 0.5)\n        self.p = 0.1  # Percentage of top individuals for p-best selection\n        self.population = None\n        self.fitness = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.archive = []  # For storing inferior solutions\n        self.archive_size = self.pop_size  # Initial archive size\n        self.memory_index = 0\n        self.sf_M = []\n        self.scr_M = []\n        self.fevals = 0  # Track function evaluations\n        self.min_pop_size = 4  # Minimum population size\n        self.reduction_trigger = 0.25  # Fraction of budget before reduction starts\n        self.reduction_rate = 0.2  # Fraction of population reduced each time\n        self.use_orthogonal = True  # Use orthogonal crossover\n        self.dbe_probability = 0.1  # Probability of using Distance-Based Exploration\n        self.diversity_threshold = 0.1  # Threshold for population diversity.\n        self.cluster_threshold = 0.05  # Threshold for cluster diversity\n        self.num_clusters = 5  # Number of clusters for diversity analysis\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.fevals += self.pop_size\n\n    def calculate_distances(self, pop, i):\n        \"\"\"Calculates Euclidean distances from individual i to all other individuals.\"\"\"\n        distances = np.linalg.norm(pop - pop[i], axis=1)\n        distances[i] = np.inf  # Exclude distance to itself\n        return distances\n\n    def mutate(self, pop, i, F, Cr, archive):\n        if np.random.rand() < self.dbe_probability:\n            # Distance-Based Exploration\n            distances = self.calculate_distances(pop, i)\n            r1 = np.argmin(distances)  # Farthest Individual\n\n        else:\n            p_best_size = max(int(self.p * len(pop)), 1)\n\n            # Tournament selection for p-best\n            indices = np.argsort(self.fitness)[:p_best_size]\n            p_best_idx = np.random.choice(indices)\n            p_best = pop[p_best_idx]\n            indices = [idx for idx in range(len(pop)) if idx != i]\n            if len(indices) == 0:\n                return pop[i]\n            r1 = np.random.choice(indices)\n\n        # Choose r2 from current pop or archive\n        if len(archive) > 0 and np.random.rand() < 0.5:  # 50% chance to use archive\n            r2_idx = np.random.randint(0, len(archive))\n            r2 = archive[r2_idx]\n        else:\n            indices = [idx for idx in range(len(pop)) if idx != i and idx != r1]  # Ensure distinct indices\n            if len(indices) == 0:\n                r2 = pop[r1]  # If only one individual, take that\n            else:\n                r2_idx = np.random.choice(indices)\n                r2 = pop[r2_idx]\n\n        if np.random.rand() < self.dbe_probability:\n            p_best = pop[np.random.choice(range(len(pop)))]  # Choose random individual when using DBE for p_best\n        else:\n            p_best_size = max(int(self.p * len(pop)), 1)\n            # Tournament selection for p-best\n            indices = np.argsort(self.fitness)[:p_best_size]\n            p_best_idx = np.random.choice(indices)\n            p_best = pop[p_best_idx]\n\n        mutant = p_best + F * (pop[r1] - r2)\n        return mutant\n\n    def orthogonal_crossover(self, mutant, target):\n        # Generate an orthogonal array (example using a simple Hadamard matrix)\n        H = np.array([[1, 1], [1, -1]])\n\n        # Ensure dimensions are compatible by repeating or truncating\n        if self.dim > H.shape[1]:\n            H_repeated = np.tile(H, (1, int(np.ceil(self.dim / H.shape[1]))))\n            H_used = H_repeated[:, :self.dim]\n        else:\n            H_used = H[:, :self.dim]\n\n        trial = np.copy(target)\n\n        for j in range(self.dim):\n            if H_used[0, j] == H_used[1, j]:\n                continue  # No change\n            else:\n                trial[j] = mutant[j]\n\n        return trial\n\n    def crossover(self, mutant, target, Cr):\n        cross_points = np.random.rand(self.dim) < Cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def update_archive(self, individual, archive):\n        archive.append(individual)\n        return archive\n\n    def reduce_population(self):\n        if self.pop_size > self.min_pop_size:\n            sorted_indices = np.argsort(self.fitness)[::-1]  # Worst to best\n            reduction_amount = max(1, int(self.pop_size * self.reduction_rate))  # Reduce by reduction_rate, but at least 1\n            removed_indices = sorted_indices[:reduction_amount]\n\n            self.population = np.delete(self.population, removed_indices, axis=0)\n            self.fitness = np.delete(self.fitness, removed_indices)\n            self.pop_size = len(self.population)\n\n    def calculate_population_diversity(self):\n        \"\"\"Calculates the average distance of each individual to the population mean.\"\"\"\n        mean_position = np.mean(self.population, axis=0)\n        distances = np.linalg.norm(self.population - mean_position, axis=1)\n        avg_distance = np.mean(distances)\n        return avg_distance\n\n    def adjust_archive_size(self):\n        \"\"\"Dynamically adjusts the archive size based on population diversity.\"\"\"\n        diversity = self.calculate_population_diversity()\n\n        if diversity < self.diversity_threshold:\n            # If diversity is low, increase archive size to encourage exploration\n            self.archive_size = min(2 * self.pop_size, self.budget // 10)  # Limit archive size\n        else:\n            # If diversity is high, reduce archive size to focus on exploitation\n            self.archive_size = max(self.pop_size // 2, 10)  # Ensure a minimum archive size\n\n    def maintain_diversity(self):\n        \"\"\"Introduce a diversity maintenance mechanism using k-means clustering.\"\"\"\n        kmeans = KMeans(n_clusters=self.num_clusters, random_state=0, n_init=10)\n        labels = kmeans.fit_predict(self.population)\n\n        for cluster_id in range(self.num_clusters):\n            cluster_indices = np.where(labels == cluster_id)[0]\n            if len(cluster_indices) > 0:\n                cluster_positions = self.population[cluster_indices]\n                cluster_diversity = np.mean(np.linalg.norm(cluster_positions - np.mean(cluster_positions, axis=0), axis=1))\n\n                if cluster_diversity < self.cluster_threshold:\n                    # If cluster diversity is low, re-initialize a random member\n                    worst_index = cluster_indices[np.argmax(self.fitness[cluster_indices])]\n                    self.population[worst_index] = np.random.uniform(self.bounds_lb, self.bounds_ub, size=self.dim)\n                    self.fitness[worst_index] = np.inf  # Reset fitness\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n        self.archive = []\n\n        self.sf_M = []\n        self.scr_M = []\n        weights = []\n\n        while self.fevals < self.budget:\n            sf_k = []\n            scr_k = []\n            weights = []\n\n            # Adjust Archive size\n            self.adjust_archive_size()\n\n            for i in range(self.pop_size):\n                # Sample F and CR from memory\n                mem_idx = np.random.randint(0, self.memory_size)\n                F = self.memory_F[mem_idx]\n                Cr = self.memory_CR[mem_idx]\n\n                # Truncate and Reflect F\n                if F > 1:\n                    F = 1\n                if F <= 0:\n                    F = 0.000001\n\n                mutant = self.mutate(self.population, i, F, Cr, self.archive)\n                mutant = np.clip(mutant, self.bounds_lb, self.bounds_ub)\n\n                # Use orthogonal crossover with a probability or with the regular crossover\n                if self.use_orthogonal and np.random.rand() < 0.5:  # 50% chance to use orthogonal crossover\n                    trial = self.orthogonal_crossover(mutant, self.population[i])\n                else:\n                    trial = self.crossover(mutant, self.population[i], Cr)\n\n                f = func(trial)\n                self.fevals += 1\n\n                if f < self.fitness[i]:\n                    delta = abs(f - self.fitness[i])\n                    sf_k.append(F)\n                    scr_k.append(Cr)\n                    weights.append(delta)\n\n                    self.archive = self.update_archive(self.population[i], self.archive)\n                    if len(self.archive) > self.archive_size:\n                        self.archive.pop(np.random.randint(0, len(self.archive)))  # Keep archive size constant\n\n                    self.fitness[i] = f\n                    self.population[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                else:\n                    self.archive = self.update_archive(trial, self.archive)  # Add even unsuccessful trials\n                    if len(self.archive) > self.archive_size:\n                        self.archive.pop(np.random.randint(0, len(self.archive)))\n\n                if self.fevals >= self.budget:\n                    break\n\n            # Update memory using Lehmer mean\n            if sf_k:\n                weights = np.array(weights) / np.sum(weights)\n                sum_weights_sfk = np.sum(weights * np.array(sf_k) ** 2)\n                sum_weights = np.sum(weights * np.array(sf_k))\n\n                if sum_weights > 0:\n                    mean_F = sum_weights_sfk / sum_weights\n                else:\n                    mean_F = 0.5\n\n                mean_CR = np.average(scr_k, weights=weights)\n\n                self.memory_F[self.memory_index] = mean_F\n                self.memory_CR[self.memory_index] = mean_CR\n                self.memory_index = (self.memory_index + 1) % self.memory_size\n\n            # Adaptive population size reduction\n            if self.fevals > self.budget * self.reduction_trigger:  # Start reducing after reduction_trigger of the budget is spent\n                self.reduce_population()\n\n            self.maintain_diversity()  # Maintain diversity\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "An exception occurred: name 'KMeans' is not defined.", "error": "", "parent_ids": ["0b747827-f94c-41cb-a157-01708ce0db5d"], "operator": null, "metadata": {}}
{"id": "8f9e6c02-9a5e-4361-903f-8ced52724649", "fitness": 0.0, "name": "ImprovedSHADE_Ortho_AdaptPop_DBE_DynArchive_Mirroring", "description": "Introduces a mirrored sampling technique around the p-best individual to enhance exploitation and exploration, coupled with dynamic diversity control using both threshold and worst individual replacement.", "code": "import numpy as np\n\nclass ImprovedSHADE_Ortho_AdaptPop_DBE_DynArchive_Mirroring:\n    def __init__(self, budget=10000, dim=10, memory_size=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = int(4 + 3 * np.log(budget))  # Initial population size\n        self.memory_size = memory_size\n        self.memory_F = np.full(memory_size, 0.5)\n        self.memory_CR = np.full(memory_size, 0.5)\n        self.p = 0.1  # Percentage of top individuals for p-best selection\n        self.population = None\n        self.fitness = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.archive = []  # For storing inferior solutions\n        self.archive_size = self.pop_size  # Initial archive size\n        self.memory_index = 0\n        self.sf_M = []\n        self.scr_M = []\n        self.fevals = 0  # Track function evaluations\n        self.min_pop_size = 4  # Minimum population size\n        self.reduction_trigger = 0.25  # Fraction of budget before reduction starts\n        self.reduction_rate = 0.2  # Fraction of population reduced each time\n        self.use_orthogonal = True  # Use orthogonal crossover\n        self.dbe_probability = 0.1  # Probability of using Distance-Based Exploration\n        self.diversity_threshold = 0.1 #Threshold for population diversity.\n        self.mirroring_probability = 0.3  # Probability of using mirroring\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.fevals += self.pop_size\n\n    def calculate_distances(self, pop, i):\n        \"\"\"Calculates Euclidean distances from individual i to all other individuals.\"\"\"\n        distances = np.linalg.norm(pop - pop[i], axis=1)\n        distances[i] = np.inf  # Exclude distance to itself\n        return distances\n\n    def mutate(self, pop, i, F, Cr, archive):\n        if np.random.rand() < self.dbe_probability:\n            # Distance-Based Exploration\n            distances = self.calculate_distances(pop, i)\n            r1 = np.argmin(distances)  # Farthest Individual\n\n        else:\n\n            p_best_size = max(int(self.p * len(pop)), 1)\n\n            # Tournament selection for p-best\n            indices = np.argsort(self.fitness)[:p_best_size]\n            p_best_idx = np.random.choice(indices)\n            p_best = pop[p_best_idx]\n            indices = [idx for idx in range(len(pop)) if idx != i]\n            if len(indices) == 0:\n                return pop[i]\n            r1 = np.random.choice(indices)\n\n        # Choose r2 from current pop or archive\n        if len(archive) > 0 and np.random.rand() < 0.5:  # 50% chance to use archive\n            r2_idx = np.random.randint(0, len(archive))\n            r2 = archive[r2_idx]\n        else:\n            indices = [idx for idx in range(len(pop)) if idx != i and idx != r1]  # Ensure distinct indices\n            if len(indices) == 0:\n                r2 = pop[r1]  # If only one individual, take that\n            else:\n                r2_idx = np.random.choice(indices)\n                r2 = pop[r2_idx]\n\n        if np.random.rand() < self.dbe_probability:\n            p_best = pop[np.random.choice(range(len(pop)))]  # Choose random individual when using DBE for p_best\n        else:\n            p_best_size = max(int(self.p * len(pop)), 1)\n            # Tournament selection for p-best\n            indices = np.argsort(self.fitness)[:p_best_size]\n            p_best_idx = np.random.choice(indices)\n            p_best = pop[p_best_idx]\n\n        mutant = p_best + F * (pop[r1] - r2)\n        return mutant\n\n    def orthogonal_crossover(self, mutant, target):\n        # Generate an orthogonal array (example using a simple Hadamard matrix)\n        H = np.array([[1, 1], [1, -1]])\n\n        # Ensure dimensions are compatible by repeating or truncating\n        if self.dim > H.shape[1]:\n            H_repeated = np.tile(H, (1, int(np.ceil(self.dim / H.shape[1]))))\n            H_used = H_repeated[:, :self.dim]\n        else:\n            H_used = H[:, :self.dim]\n\n        trial = np.copy(target)\n\n        for j in range(self.dim):\n            if H_used[0, j] == H_used[1, j]:\n                continue  # No change\n            else:\n                trial[j] = mutant[j]\n\n        return trial\n\n    def crossover(self, mutant, target, Cr):\n        cross_points = np.random.rand(self.dim) < Cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def mirroring(self, p_best, mutant):\n        \"\"\"Mirror the mutant around the p_best individual.\"\"\"\n        mirrored_mutant = 2 * p_best - mutant\n        return mirrored_mutant\n\n    def update_archive(self, individual, archive):\n        archive.append(individual)\n        return archive\n\n    def reduce_population(self):\n        if self.pop_size > self.min_pop_size:\n            sorted_indices = np.argsort(self.fitness)[::-1]  # Worst to best\n            reduction_amount = max(1, int(self.pop_size * self.reduction_rate))  # Reduce by reduction_rate, but at least 1\n            removed_indices = sorted_indices[:reduction_amount]\n\n            self.population = np.delete(self.population, removed_indices, axis=0)\n            self.fitness = np.delete(self.fitness, removed_indices)\n            self.pop_size = len(self.population)\n\n    def calculate_population_diversity(self):\n        \"\"\"Calculates the average distance of each individual to the population mean.\"\"\"\n        mean_position = np.mean(self.population, axis=0)\n        distances = np.linalg.norm(self.population - mean_position, axis=1)\n        avg_distance = np.mean(distances)\n        return avg_distance\n\n    def adjust_archive_size(self):\n        \"\"\"Dynamically adjusts the archive size based on population diversity.\"\"\"\n        diversity = self.calculate_population_diversity()\n\n        if diversity < self.diversity_threshold:\n            # If diversity is low, increase archive size to encourage exploration\n            self.archive_size = min(2 * self.pop_size, self.budget // 10)  # Limit archive size\n        else:\n            # If diversity is high, reduce archive size to focus on exploitation\n            self.archive_size = max(self.pop_size // 2, 10)  # Ensure a minimum archive size\n\n    def maintain_diversity(self):\n         \"\"\"Introduce a diversity maintenance mechanism to prevent premature convergence.\"\"\"\n         diversity = self.calculate_population_diversity()\n         if diversity < self.diversity_threshold:\n             # Select a subset of the population (e.g., the worst 20%)\n             num_to_diversify = max(1, int(0.2 * self.pop_size))  # At least 1\n             worst_indices = np.argsort(self.fitness)[-num_to_diversify:] # Indices of worst individuals\n\n             # Replace them with random individuals from the search space\n             for idx in worst_indices:\n                 self.population[idx] = np.random.uniform(self.bounds_lb, self.bounds_ub, size=self.dim)\n                 self.fitness[idx] = np.inf  # Reset fitness to be re-evaluated in next iteration\n\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n        self.archive = []\n\n        self.sf_M = []\n        self.scr_M = []\n        weights = []\n\n        while self.fevals < self.budget:\n            sf_k = []\n            scr_k = []\n            weights = []\n\n            #Adjust Archive size\n            self.adjust_archive_size()\n\n            for i in range(self.pop_size):\n                # Sample F and CR from memory\n                mem_idx = np.random.randint(0, self.memory_size)\n                F = self.memory_F[mem_idx]\n                Cr = self.memory_CR[mem_idx]\n\n                # Truncate and Reflect F\n                if F > 1:\n                    F = 1\n                if F <= 0:\n                    F = 0.000001\n\n                mutant = self.mutate(self.population, i, F, Cr, self.archive)\n                mutant = np.clip(mutant, self.bounds_lb, self.bounds_ub)\n                \n                #Mirroring\n                p_best_size = max(int(self.p * len(self.population)), 1)\n                indices = np.argsort(self.fitness)[:p_best_size]\n                p_best_idx = np.random.choice(indices)\n                p_best = self.population[p_best_idx]\n\n                if np.random.rand() < self.mirroring_probability:\n                    mirrored_mutant = self.mirroring(p_best, mutant)\n                    mirrored_mutant = np.clip(mirrored_mutant, self.bounds_lb, self.bounds_ub)\n\n                    #Evaluate both and select best\n                    f_mutant = func(mutant)\n                    f_mirrored = func(mirrored_mutant)\n                    self.fevals += 2\n\n                    if f_mirrored < f_mutant:\n                        mutant = mirrored_mutant\n                        f = f_mirrored\n                    else:\n                        f = f_mutant\n\n                else:\n                    f = func(mutant)\n                    self.fevals += 1\n\n                # Use orthogonal crossover with a probability or with the regular crossover\n                if self.use_orthogonal and np.random.rand() < 0.5:  # 50% chance to use orthogonal crossover\n                    trial = self.orthogonal_crossover(mutant, self.population[i])\n                else:\n                    trial = self.crossover(mutant, self.population[i], Cr)\n\n                #Evaluate trial\n                #f = func(trial) #already calculated when mirroring\n                #self.fevals += 1   #already calculated when mirroring\n\n                if f < self.fitness[i]:\n                    delta = abs(f - self.fitness[i])\n                    sf_k.append(F)\n                    scr_k.append(Cr)\n                    weights.append(delta)\n\n                    self.archive = self.update_archive(self.population[i], self.archive)\n                    if len(self.archive) > self.archive_size:\n                        self.archive.pop(np.random.randint(0, len(self.archive)))  # Keep archive size constant\n\n                    self.fitness[i] = f\n                    self.population[i] = mutant #trial #mutant\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = self.population[i] #trial #self.population[i]\n                else:\n                    self.archive = self.update_archive(trial, self.archive)  # Add even unsuccessful trials\n                    if len(self.archive) > self.archive_size:\n                        self.archive.pop(np.random.randint(0, len(self.archive)))\n\n                if self.fevals >= self.budget:\n                    break\n\n            # Update memory using Lehmer mean\n            if sf_k:\n                weights = np.array(weights) / np.sum(weights)\n                sum_weights_sfk = np.sum(weights * np.array(sf_k) ** 2)\n                sum_weights = np.sum(weights * np.array(sf_k))\n\n                if sum_weights > 0:\n                    mean_F = sum_weights_sfk / sum_weights\n                else:\n                    mean_F = 0.5\n\n                mean_CR = np.average(scr_k, weights=weights)\n\n                self.memory_F[self.memory_index] = mean_F\n                self.memory_CR[self.memory_index] = mean_CR\n                self.memory_index = (self.memory_index + 1) % self.memory_size\n\n            # Adaptive population size reduction\n            if self.fevals > self.budget * self.reduction_trigger:  # Start reducing after reduction_trigger of the budget is spent\n                self.reduce_population()\n\n            self.maintain_diversity() #Maintain diversity\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm ImprovedSHADE_Ortho_AdaptPop_DBE_DynArchive_Mirroring scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0b747827-f94c-41cb-a157-01708ce0db5d"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "4960304d-59a3-45fd-a1f1-3247524e5a8d", "fitness": 0.27898177648413347, "name": "AdaptiveStochasticRankingDE", "description": "Adaptive Differential Evolution with velocity-based mutation, stochastic ranking, and self-adaptive parameters F and Cr based on individual success.", "code": "import numpy as np\n\nclass AdaptiveStochasticRankingDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, p_rank=0.45, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.p_rank = p_rank\n        self.population = None\n        self.fitness = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.F = 0.5  # Initial mutation factor\n        self.Cr = 0.7 # Initial Crossover rate\n        self.best_fitness = np.inf\n        self.best_solution = None\n        self.archive_size = archive_size\n        self.archive = []\n        self.archive_fitness = []\n        self.velocities = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.best_fitness = np.min(self.fitness)\n        self.best_solution = self.population[np.argmin(self.fitness)].copy()\n        self.velocities = np.zeros((self.pop_size, self.dim))\n\n    def evaluate(self, func, individual):\n        return func(individual)\n\n    def mutate(self, pop, i):\n        indices = [idx for idx in range(len(pop)) if idx != i]\n        a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n        # Velocity-based mutation\n        self.velocities[i] = 0.5 * self.velocities[i] + self.F * (b - c)\n        mutant = a + self.velocities[i]\n        mutant = np.clip(mutant, self.bounds_lb, self.bounds_ub)\n        return mutant\n\n    def crossover(self, mutant, target, cr):\n        cross_points = np.random.rand(self.dim) < cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def stochastic_ranking(self, fitness_current, fitness_trial):\n        \"\"\"\n        Compares two solutions using stochastic ranking.\n        \"\"\"\n        if fitness_current < 0 or fitness_trial < 0:\n            if np.random.rand() < self.p_rank or fitness_trial < fitness_current:\n                return True\n            else:\n                return False\n        else:\n            if fitness_trial < fitness_current:\n                return True\n            else:\n                return True\n\n    def update_archive(self, individual, fitness):\n        if len(self.archive) < self.archive_size:\n            self.archive.append(individual)\n            self.archive_fitness.append(fitness)\n        else:\n            # Replace the worst element in the archive\n            max_index = np.argmax(self.archive_fitness)\n            if fitness < self.archive_fitness[max_index]:\n                self.archive[max_index] = individual\n                self.archive_fitness[max_index] = fitness\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.initialize_population(func)\n        fevals = self.pop_size\n        generation = 0\n\n        F_values = np.full(self.pop_size, self.F)\n        Cr_values = np.full(self.pop_size, self.Cr)\n\n        while fevals < self.budget:\n            generation += 1\n            for i in range(self.pop_size):\n                # Self-adaptive parameters\n                F_i = F_values[i]\n                Cr_i = Cr_values[i]\n                \n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, self.population[i], Cr_i)\n\n                fitness_trial = self.evaluate(func, trial)\n                fevals += 1\n\n                if self.stochastic_ranking(self.fitness[i], fitness_trial):\n                    \n                    if fitness_trial < self.fitness[i]:\n                        # Successful trial: adapt parameters\n                        F_values[i] = np.clip(np.random.normal(0.5, 0.1), 0.1, 0.9)\n                        Cr_values[i] = np.clip(np.random.normal(0.7, 0.1), 0.1, 0.9)\n\n                    self.fitness[i] = fitness_trial\n                    self.population[i] = trial\n                    \n\n                    if fitness_trial < self.best_fitness:\n                        self.best_fitness = fitness_trial\n                        self.best_solution = trial.copy()\n                        \n                    self.update_archive(trial, fitness_trial)\n                else:\n                    # Unsuccessful trial: adapt parameters\n                    F_values[i] = np.clip(np.random.normal(F_i, 0.1), 0.1, 0.9)\n                    Cr_values[i] = np.clip(np.random.normal(Cr_i, 0.1), 0.1, 0.9)\n\n\n                if fevals >= self.budget:\n                    break\n        return self.best_fitness, self.best_solution", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveStochasticRankingDE scored 0.279 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ba881cec-dcf7-437a-99de-cc2a76ef0431"], "operator": null, "metadata": {"aucs": [0.10422285798299291, 0.16894514828220275, 0.26014908731453645, 0.2002899051525533, 0.20067010100782523, 0.2186871483205719, 0.23964529466144302, 0.2049068323181743, 0.20131264970770613, 0.15034196738603933, 0.21594813310973415, 0.998919907297739, 0.24096103649489908, 0.21420537092684633, 0.5900982558098088, 0.25413263930923713, 0.2352145528273335, 0.2689804456611472, 0.15519660170290395, 0.4568075944089751]}}
{"id": "def4f117-f8e2-432d-9fbf-c84904cae154", "fitness": 0.6160207233587836, "name": "AdaptiveStochasticRankingDE", "description": "Implements Differential Evolution with a self-adaptive strategy for F and Cr, utilizing a success history, and enhances exploration by re-initializing individuals stuck in local optima based on fitness stagnation.", "code": "import numpy as np\n\nclass AdaptiveStochasticRankingDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, p_rank=0.45, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.p_rank = p_rank\n        self.archive_size = archive_size\n        self.population = None\n        self.fitness = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.F = 0.5  # Initial mutation factor\n        self.Cr = 0.7 # Initial Crossover rate\n        self.best_fitness = np.inf\n        self.best_solution = None\n        self.F_history = []\n        self.Cr_history = []\n        self.archive = []\n        self.archive_fitness = []\n        self.stagnation_counter = np.zeros(pop_size)\n        self.stagnation_threshold = 50  # Generations before re-initialization\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.best_fitness = np.min(self.fitness)\n        self.best_solution = self.population[np.argmin(self.fitness)].copy()\n\n    def evaluate(self, func, individual):\n        return func(individual)\n\n    def mutate(self, pop, i):\n        indices = [idx for idx in range(len(pop)) if idx != i]\n        a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n        \n        # Self-adaptive F\n        Fi = np.random.choice(self.F_history + [0.5]) if self.F_history else 0.5\n        mutant = a + Fi * (b - c)\n        mutant = np.clip(mutant, self.bounds_lb, self.bounds_ub)\n        return mutant\n\n    def crossover(self, mutant, target):\n        # Self-adaptive Cr\n        Cri = np.random.choice(self.Cr_history + [0.7]) if self.Cr_history else 0.7\n        cross_points = np.random.rand(self.dim) < Cri\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def stochastic_ranking(self, fitness_current, fitness_trial):\n        \"\"\"\n        Compares two solutions using stochastic ranking.\n        \"\"\"\n        if fitness_current < 0 or fitness_trial < 0:\n            if np.random.rand() < self.p_rank or fitness_trial < fitness_current:\n                return True\n            else:\n                return False\n        else:\n            if fitness_trial < fitness_current:\n                return True\n            else:\n                return False\n\n    def update_archive(self, individual, fitness):\n        if len(self.archive) < self.archive_size:\n            self.archive.append(individual)\n            self.archive_fitness.append(fitness)\n        else:\n            # Replace a random element in the archive\n            idx = np.random.randint(0, self.archive_size)\n            self.archive[idx] = individual\n            self.archive_fitness[idx] = fitness\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.initialize_population(func)\n        fevals = self.pop_size\n        generation = 0\n\n        while fevals < self.budget:\n            generation += 1\n            successful_F = []\n            successful_Cr = []\n            for i in range(self.pop_size):\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, self.population[i])\n\n                fitness_trial = self.evaluate(func, trial)\n                fevals += 1\n\n                if self.stochastic_ranking(self.fitness[i], fitness_trial):\n                    if fitness_trial < self.fitness[i]:\n                        # Successful trial\n                        Fi = np.random.choice(self.F_history + [0.5]) if self.F_history else 0.5\n                        Cri = np.random.choice(self.Cr_history + [0.7]) if self.Cr_history else 0.7\n                        successful_F.append(Fi)\n                        successful_Cr.append(Cri)\n                        \n                    self.fitness[i] = fitness_trial\n                    self.population[i] = trial\n                    self.stagnation_counter[i] = 0  # Reset stagnation counter\n\n                    if fitness_trial < self.best_fitness:\n                        self.best_fitness = fitness_trial\n                        self.best_solution = trial.copy()\n                        self.update_archive(trial.copy(), fitness_trial) # Update archive with better solution\n                else:\n                    self.stagnation_counter[i] += 1 # Increment stagnation counter\n\n                if fevals >= self.budget:\n                    break\n\n                # Re-initialize individuals stuck in local optima\n                if self.stagnation_counter[i] > self.stagnation_threshold:\n                    self.population[i] = np.random.uniform(self.bounds_lb, self.bounds_ub)\n                    self.fitness[i] = self.evaluate(func, self.population[i])\n                    fevals += 1\n                    self.stagnation_counter[i] = 0\n                    if self.fitness[i] < self.best_fitness:\n                      self.best_fitness = self.fitness[i]\n                      self.best_solution = self.population[i].copy()\n            \n            # Update F and Cr history\n            if successful_F:\n                self.F_history = successful_F\n            if successful_Cr:\n                self.Cr_history = successful_Cr\n\n        return self.best_fitness, self.best_solution", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveStochasticRankingDE scored 0.616 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ba881cec-dcf7-437a-99de-cc2a76ef0431"], "operator": null, "metadata": {"aucs": [0.2043538591248829, 0.36747519041527765, 0.5933721392004105, 0.8378620012013702, 0.6932564743943075, 0.7588634709561746, 0.5551951233682377, 0.6099232070395133, 0.6531011073253549, 0.5271132457352912, 0.7868607929743388, 0.9897263330748333, 0.33343538853913035, 0.6653281885413382, 0.8838223158837302, 0.7641894664306241, 0.5127056676717732, 0.8239372450521956, 0.24552130355226132, 0.514371946694627]}}
{"id": "91b18c1e-a646-4bb1-9bed-ac8f13770311", "fitness": 0.4809365748714046, "name": "AdaptiveDE_ReducePop_Velocity_OrthoCrossover_Restart", "description": "Adaptive Differential Evolution with orthogonal crossover, velocity-based mutation, dynamic population size, and a self-adaptive strategy for crossover and mutation parameters using a success-history based adaptation, coupled with a restart mechanism to escape local optima.", "code": "import numpy as np\n\nclass AdaptiveDE_ReducePop_Velocity_OrthoCrossover_Restart:\n    def __init__(self, budget=10000, dim=10, archive_size=5, memory_size=5, initial_pop_size=50, ring_neighbors=3, restart_trigger=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.initial_pop_size = initial_pop_size\n        self.population = None\n        self.fitness = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.archive = []  # Archive for storing removed individuals\n        self.archive_size = archive_size\n        self.memory_size = memory_size\n        self.memory_F = np.full(self.memory_size, 0.5)\n        self.memory_Cr = np.full(self.memory_size, 0.5)\n        self.sf = []\n        self.scr = []\n        self.p = 0.1  # Probability of using archive individual\n        self.w = 0.5  # Weight for Lehmer mean update\n        self.reduction_factor = 0.95 # Factor for population size reduction\n        self.velocity = None\n        self.ring_neighbors = ring_neighbors\n        self.min_pop_size = 10\n        self.restart_trigger = restart_trigger  # Number of generations before a potential restart\n        self.restart_count = 0\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.inf\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.velocity = np.zeros_like(self.population) # Initialize velocity\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.previous_best_fitness = self.f_opt\n\n    def mutate(self, pop, i, current_generation):\n        memory_index = current_generation % self.memory_size\n        Cr = self.memory_Cr[memory_index]\n        F = self.memory_F[memory_index]\n\n        indices = [idx for idx in range(len(pop)) if idx != i]\n        if len(self.archive) > 0 and np.random.rand() < self.p:\n            arch_ind = np.random.randint(0, len(self.archive))\n            pop_indices = np.random.choice(indices, 2, replace=False)\n            a = self.archive[arch_ind]\n            b, c = pop[pop_indices[0]], pop[pop_indices[1]]\n        else:\n            a, b, c = pop[np.random.choice(indices, 3, replace=False)]\n\n        # Velocity-based mutation\n        self.velocity[i] = F * (a - pop[i]) + F * (b - c)\n        mutation = pop[i] + self.velocity[i]\n\n        return mutation, Cr, F\n\n    def reflecting_boundary_handling(self, mutant):\n        # Apply reflecting boundary handling\n        for i in range(self.dim):\n            if mutant[i] < self.bounds_lb[i]:\n                mutant[i] = self.bounds_lb[i] + np.abs(mutant[i] - self.bounds_lb[i])\n            elif mutant[i] > self.bounds_ub[i]:\n                mutant[i] = self.bounds_ub[i] - np.abs(mutant[i] - self.bounds_ub[i])\n\n            # Ensure the value remains within the bounds after reflection\n            mutant[i] = np.clip(mutant[i], self.bounds_lb[i], self.bounds_ub[i])\n        return mutant\n\n    def orthogonal_crossover(self, mutant, target):\n        # Create orthogonal matrix (Hadamard matrix)\n        H = self.create_hadamard_matrix(self.dim)\n\n        # Orthogonal Crossover\n        trial = np.zeros_like(mutant)\n        for i in range(self.dim):\n            if H[i, 0] > 0:  # Select based on Hadamard matrix values\n                trial[i] = mutant[i]\n            else:\n                trial[i] = target[i]\n        return trial\n\n    def create_hadamard_matrix(self, n):\n        # Ensure n is a power of 2 for Hadamard matrix creation\n        if n & (n - 1) != 0:\n            n = 2**int(np.ceil(np.log2(n)))  # Pad to the next power of 2\n        if n == 1:\n            return np.array([[1]])\n        H = np.array([[1, 1], [1, -1]])\n        while H.shape[0] < n:\n            H = np.kron(H, np.array([[1, 1], [1, -1]]))\n        return H[:self.dim, :self.dim]  # Truncate to original dimension\n    \n\n    def update_memory(self):\n        if self.sf and self.scr:\n            self.sf = np.array(self.sf)\n            self.scr = np.array(self.scr)\n\n            # Lehmer mean for F\n            sum_sf_squared = np.sum(self.sf ** 2)\n            sum_sf = np.sum(self.sf)\n            if sum_sf > 0:\n                lehmer_mean_F = sum_sf_squared / sum_sf\n            else:\n                lehmer_mean_F = 0.5  # Default value if no successful F values\n\n            # Mean for Cr\n            mean_Cr = np.mean(self.scr)\n\n            memory_index = self.generation % self.memory_size\n            self.memory_F[memory_index] = self.w * self.memory_F[memory_index] + (1 - self.w) * lehmer_mean_F\n            self.memory_Cr[memory_index] = self.w *  self.memory_Cr[memory_index] + (1 - self.w) * mean_Cr\n\n            self.sf = []\n            self.scr = []\n\n    def reduce_population(self):\n        if self.pop_size > self.min_pop_size:  # Minimum population size\n            # Sort population based on fitness\n            sorted_indices = np.argsort(self.fitness)[::-1]\n            num_to_remove = max(1, int(self.pop_size * (1 - self.reduction_factor)))  # Reduce at least by 1\n            \n            # Archive the individuals to be removed\n            for i in range(num_to_remove):\n                if len(self.archive) < self.archive_size:\n                    self.archive.append(self.population[sorted_indices[i]].copy())\n                else:\n                    replace_index = np.random.randint(0, self.archive_size)\n                    self.archive[replace_index] = self.population[sorted_indices[i]].copy()\n\n            # Keep the best individuals\n            keep_indices = sorted_indices[num_to_remove:]\n            self.population = self.population[keep_indices]\n            self.fitness = self.fitness[keep_indices]\n            self.velocity = self.velocity[keep_indices]\n            self.pop_size = len(self.population)\n\n    def ring_topology_adaptation(self):\n        # Implement a ring topology-based adaptation for F and Cr\n        for i in range(self.pop_size):\n            neighbors_indices = [(i + j) % self.pop_size for j in range(-self.ring_neighbors, self.ring_neighbors + 1) if j != 0]\n            \n            # Gather F and Cr values from neighbors\n            neighbor_F = [self.memory_F[self.generation % self.memory_size] for _ in neighbors_indices]\n            neighbor_Cr = [self.memory_Cr[self.generation % self.memory_size] for _ in neighbors_indices]\n\n            # Adapt F and Cr based on neighbors' values (e.g., average)\n            adapted_F = np.mean(neighbor_F) if neighbor_F else self.memory_F[self.generation % self.memory_size]\n            adapted_Cr = np.mean(neighbor_Cr) if neighbor_Cr else self.memory_Cr[self.generation % self.memory_size]\n            \n            # Update memory with adapted values\n            memory_index = self.generation % self.memory_size\n            self.memory_F[memory_index] = adapted_F\n            self.memory_Cr[memory_index] = adapted_Cr\n\n    def should_restart(self):\n        if self.f_opt < self.previous_best_fitness:\n            self.previous_best_fitness = self.f_opt\n            self.stagnation_counter = 0\n            return False\n        else:\n            self.stagnation_counter += 1\n            return self.stagnation_counter >= self.restart_trigger\n\n    def restart_population(self, func):\n        # Re-initialize the population\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.initial_pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.velocity = np.zeros_like(self.population)\n        self.pop_size = self.initial_pop_size\n        self.stagnation_counter = 0\n        self.restart_count += 1\n        \n        # Update optimal solution\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n        self.previous_best_fitness = self.f_opt\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n        fevals = self.pop_size\n        self.generation = 0\n        self.sf = []\n        self.scr = []\n        \n        while fevals < self.budget:\n            self.generation += 1\n            self.ring_topology_adaptation()  # Adapt F and Cr based on ring topology\n\n            for i in range(self.pop_size):\n                mutant, Cr, F = self.mutate(self.population, i, self.generation)\n                mutant = self.reflecting_boundary_handling(mutant)  # Reflecting boundary handling\n                trial = self.orthogonal_crossover(mutant, self.population[i])\n                \n                f = func(trial)\n                fevals += 1\n\n                if f < self.fitness[i]:\n                    # Archive the replaced individual\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        # Replace a random element in the archive\n                        replace_index = np.random.randint(0, self.archive_size)\n                        self.archive[replace_index] = self.population[i].copy()\n\n                    self.sf.append(F)\n                    self.scr.append(Cr)\n                    self.fitness[i] = f\n                    self.population[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                \n                if fevals >= self.budget:\n                    break\n\n            self.update_memory()\n\n            # Reduce population size every few generations (e.g., every 5 generations)\n            if self.generation % 5 == 0:\n                self.reduce_population()\n\n            if self.should_restart() and self.restart_count < 3:\n                self.restart_population(func)\n                fevals += self.pop_size # Account for new evaluations\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveDE_ReducePop_Velocity_OrthoCrossover_Restart scored 0.481 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["818d0090-e624-457f-af15-5ba91399ed24"], "operator": null, "metadata": {"aucs": [0.35978361942581394, 0.8154312354114317, 0.3166282097165033, 0.603550807948372, 0.288436959727286, 0.48730686063576745, 0.4116285328854652, 0.46319706886808476, 0.5052274560252166, 0.2647235759301263, 0.8825031532066884, 0.9973860891465347, 0.3373084792723746, 0]}}
{"id": "f19b04b5-b446-4056-8bb9-9e1d514ec47f", "fitness": 0.5963296107914118, "name": "ImprovedSHADE_Ortho_AdaptPop_DBE_DynArchive", "description": "Integrates a self-adaptive neighborhood search radius for Distance-Based Exploration (DBE) and adjusts the archive update frequency based on population diversity.", "code": "import numpy as np\n\nclass ImprovedSHADE_Ortho_AdaptPop_DBE_DynArchive:\n    def __init__(self, budget=10000, dim=10, memory_size=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = int(4 + 3 * np.log(budget))  # Initial population size\n        self.memory_size = memory_size\n        self.memory_F = np.full(memory_size, 0.5)\n        self.memory_CR = np.full(memory_size, 0.5)\n        self.p = 0.1  # Percentage of top individuals for p-best selection\n        self.population = None\n        self.fitness = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.archive = []  # For storing inferior solutions\n        self.archive_size = self.pop_size  # Initial archive size\n        self.memory_index = 0\n        self.sf_M = []\n        self.scr_M = []\n        self.fevals = 0  # Track function evaluations\n        self.min_pop_size = 4  # Minimum population size\n        self.reduction_trigger = 0.25  # Fraction of budget before reduction starts\n        self.reduction_rate = 0.2  # Fraction of population reduced each time\n        self.use_orthogonal = True  # Use orthogonal crossover\n        self.dbe_probability = 0.1  # Probability of using Distance-Based Exploration\n        self.diversity_threshold = 0.1 #Threshold for population diversity.\n        self.dbe_neighborhood_size = 0.2 # Initial fraction of population for DBE neighborhood\n        self.archive_update_prob = 0.5 # Initial probability of adding to the archive\n        self.archive_update_alpha = 0.1 #Learning rate for archive update prob.\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.fevals += self.pop_size\n\n    def calculate_distances(self, pop, i):\n        \"\"\"Calculates Euclidean distances from individual i to all other individuals.\"\"\"\n        distances = np.linalg.norm(pop - pop[i], axis=1)\n        distances[i] = np.inf  # Exclude distance to itself\n        return distances\n\n    def mutate(self, pop, i, F, Cr, archive):\n        if np.random.rand() < self.dbe_probability:\n            # Distance-Based Exploration\n            distances = self.calculate_distances(pop, i)\n            # Self-adaptive neighborhood for DBE\n            num_neighbors = max(1, int(self.dbe_neighborhood_size * len(pop)))\n            nearest_neighbors_indices = np.argsort(distances)[:num_neighbors]\n            r1 = np.random.choice(nearest_neighbors_indices) # Select random neighbor\n        else:\n\n            p_best_size = max(int(self.p * len(pop)), 1)\n\n            # Tournament selection for p-best\n            indices = np.argsort(self.fitness)[:p_best_size]\n            p_best_idx = np.random.choice(indices)\n            p_best = pop[p_best_idx]\n            indices = [idx for idx in range(len(pop)) if idx != i]\n            if len(indices) == 0:\n                return pop[i]\n            r1 = np.random.choice(indices)\n\n        # Choose r2 from current pop or archive\n        if len(archive) > 0 and np.random.rand() < 0.5:  # 50% chance to use archive\n            r2_idx = np.random.randint(0, len(archive))\n            r2 = archive[r2_idx]\n        else:\n            indices = [idx for idx in range(len(pop)) if idx != i and idx != r1]  # Ensure distinct indices\n            if len(indices) == 0:\n                r2 = pop[r1]  # If only one individual, take that\n            else:\n                r2_idx = np.random.choice(indices)\n                r2 = pop[r2_idx]\n\n        if np.random.rand() < self.dbe_probability:\n            p_best = pop[np.random.choice(range(len(pop)))]  # Choose random individual when using DBE for p_best\n        else:\n            p_best_size = max(int(self.p * len(pop)), 1)\n            # Tournament selection for p-best\n            indices = np.argsort(self.fitness)[:p_best_size]\n            p_best_idx = np.random.choice(indices)\n            p_best = pop[p_best_idx]\n\n        mutant = p_best + F * (pop[r1] - r2)\n        return mutant\n\n    def orthogonal_crossover(self, mutant, target):\n        # Generate an orthogonal array (example using a simple Hadamard matrix)\n        H = np.array([[1, 1], [1, -1]])\n\n        # Ensure dimensions are compatible by repeating or truncating\n        if self.dim > H.shape[1]:\n            H_repeated = np.tile(H, (1, int(np.ceil(self.dim / H.shape[1]))))\n            H_used = H_repeated[:, :self.dim]\n        else:\n            H_used = H[:, :self.dim]\n\n        trial = np.copy(target)\n\n        for j in range(self.dim):\n            if H_used[0, j] == H_used[1, j]:\n                continue  # No change\n            else:\n                trial[j] = mutant[j]\n\n        return trial\n\n    def crossover(self, mutant, target, Cr):\n        cross_points = np.random.rand(self.dim) < Cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def update_archive(self, individual, archive):\n        archive.append(individual)\n        return archive\n\n    def reduce_population(self):\n        if self.pop_size > self.min_pop_size:\n            sorted_indices = np.argsort(self.fitness)[::-1]  # Worst to best\n            reduction_amount = max(1, int(self.pop_size * self.reduction_rate))  # Reduce by reduction_rate, but at least 1\n            removed_indices = sorted_indices[:reduction_amount]\n\n            self.population = np.delete(self.population, removed_indices, axis=0)\n            self.fitness = np.delete(self.fitness, removed_indices)\n            self.pop_size = len(self.population)\n\n    def calculate_population_diversity(self):\n        \"\"\"Calculates the average distance of each individual to the population mean.\"\"\"\n        mean_position = np.mean(self.population, axis=0)\n        distances = np.linalg.norm(self.population - mean_position, axis=1)\n        avg_distance = np.mean(distances)\n        return avg_distance\n\n    def adjust_archive_size(self):\n        \"\"\"Dynamically adjusts the archive size based on population diversity.\"\"\"\n        diversity = self.calculate_population_diversity()\n\n        if diversity < self.diversity_threshold:\n            # If diversity is low, increase archive size to encourage exploration\n            self.archive_size = min(2 * self.pop_size, self.budget // 10)  # Limit archive size\n        else:\n            # If diversity is high, reduce archive size to focus on exploitation\n            self.archive_size = max(self.pop_size // 2, 10)  # Ensure a minimum archive size\n\n    def maintain_diversity(self):\n         \"\"\"Introduce a diversity maintenance mechanism to prevent premature convergence.\"\"\"\n         diversity = self.calculate_population_diversity()\n         if diversity < self.diversity_threshold:\n             # Select a subset of the population (e.g., the worst 20%)\n             num_to_diversify = max(1, int(0.2 * self.pop_size))  # At least 1\n             worst_indices = np.argsort(self.fitness)[-num_to_diversify:] # Indices of worst individuals\n\n             # Replace them with random individuals from the search space\n             for idx in worst_indices:\n                 self.population[idx] = np.random.uniform(self.bounds_lb, self.bounds_ub, size=self.dim)\n                 self.fitness[idx] = np.inf  # Reset fitness to be re-evaluated in next iteration\n\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n        self.archive = []\n\n        self.sf_M = []\n        self.scr_M = []\n        weights = []\n\n        while self.fevals < self.budget:\n            sf_k = []\n            scr_k = []\n            weights = []\n\n            #Adjust Archive size\n            self.adjust_archive_size()\n\n            # Calculate population diversity\n            diversity = self.calculate_population_diversity()\n\n            for i in range(self.pop_size):\n                # Sample F and CR from memory\n                mem_idx = np.random.randint(0, self.memory_size)\n                F = self.memory_F[mem_idx]\n                Cr = self.memory_CR[mem_idx]\n\n                # Truncate and Reflect F\n                if F > 1:\n                    F = 1\n                if F <= 0:\n                    F = 0.000001\n\n                mutant = self.mutate(self.population, i, F, Cr, self.archive)\n                mutant = np.clip(mutant, self.bounds_lb, self.bounds_ub)\n\n                # Use orthogonal crossover with a probability or with the regular crossover\n                if self.use_orthogonal and np.random.rand() < 0.5:  # 50% chance to use orthogonal crossover\n                    trial = self.orthogonal_crossover(mutant, self.population[i])\n                else:\n                    trial = self.crossover(mutant, self.population[i], Cr)\n\n                f = func(trial)\n                self.fevals += 1\n\n                if f < self.fitness[i]:\n                    delta = abs(f - self.fitness[i])\n                    sf_k.append(F)\n                    scr_k.append(Cr)\n                    weights.append(delta)\n\n                    #Probabilistic Archive update\n                    if np.random.rand() < self.archive_update_prob:\n                         self.archive = self.update_archive(self.population[i], self.archive)\n\n                    if len(self.archive) > self.archive_size:\n                        self.archive.pop(np.random.randint(0, len(self.archive)))  # Keep archive size constant\n\n                    self.fitness[i] = f\n                    self.population[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n\n                    # Update archive update probability\n                    self.archive_update_prob += self.archive_update_alpha * (1 - self.archive_update_prob)\n\n                else:\n                    #Probabilistic Archive update (add even unsuccessful trials)\n                    if np.random.rand() < self.archive_update_prob:\n                        self.archive = self.update_archive(trial, self.archive)  # Add even unsuccessful trials\n\n                    if len(self.archive) > self.archive_size:\n                        self.archive.pop(np.random.randint(0, len(self.archive)))\n\n                     # Decrease archive update probability\n                    self.archive_update_prob -= self.archive_update_alpha * self.archive_update_prob\n\n                if self.fevals >= self.budget:\n                    break\n\n            # Update memory using Lehmer mean\n            if sf_k:\n                weights = np.array(weights) / np.sum(weights)\n                sum_weights_sfk = np.sum(weights * np.array(sf_k) ** 2)\n                sum_weights = np.sum(weights * np.array(sf_k))\n\n                if sum_weights > 0:\n                    mean_F = sum_weights_sfk / sum_weights\n                else:\n                    mean_F = 0.5\n\n                mean_CR = np.average(scr_k, weights=weights)\n\n                self.memory_F[self.memory_index] = mean_F\n                self.memory_CR[self.memory_index] = mean_CR\n                self.memory_index = (self.memory_index + 1) % self.memory_size\n\n            # Adaptive population size reduction\n            if self.fevals > self.budget * self.reduction_trigger:  # Start reducing after reduction_trigger of the budget is spent\n                self.reduce_population()\n\n            self.maintain_diversity() #Maintain diversity\n\n            #Adapt DBE neighborhood size\n            if diversity > self.diversity_threshold:\n                self.dbe_neighborhood_size = min(self.dbe_neighborhood_size + 0.01, 0.5)\n            else:\n                self.dbe_neighborhood_size = max(self.dbe_neighborhood_size - 0.01, 0.05)\n\n\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm ImprovedSHADE_Ortho_AdaptPop_DBE_DynArchive scored 0.596 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0b747827-f94c-41cb-a157-01708ce0db5d"], "operator": null, "metadata": {"aucs": [0.20065814058055131, 0.39452882436719094, 0.5858809708198716, 0.8605989659135541, 0.7201489354990411, 0.7382947840300282, 0.5743845099442494, 0.5922220008924088, 0.6949878873381576, 0.41384092311500964, 0.833670646968608, 0.9989915464277425, 0.2668089252896805, 0.6548335903006066, 0.935773719611595, 0.3445483699305538, 0.5340291419495937, 0.8420983754595286, 0.2150960804717139, 0.5251958769185505]}}
{"id": "82054ea9-cb6b-412c-a517-0729c4c34d80", "fitness": 0.5986387849529177, "name": "ImprovedSHADE_Ortho_AdaptPop_DBE_DynArchive_EnhancedExploration", "description": "Enhance exploration in Improved SHADE by adaptively adjusting the mutation strategy based on population stagnation and dynamically controlling the archive update frequency using a success rate criterion.", "code": "import numpy as np\n\nclass ImprovedSHADE_Ortho_AdaptPop_DBE_DynArchive_EnhancedExploration:\n    def __init__(self, budget=10000, dim=10, memory_size=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = int(4 + 3 * np.log(budget))  # Initial population size\n        self.memory_size = memory_size\n        self.memory_F = np.full(memory_size, 0.5)\n        self.memory_CR = np.full(memory_size, 0.5)\n        self.p = 0.1  # Percentage of top individuals for p-best selection\n        self.population = None\n        self.fitness = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.archive = []  # For storing inferior solutions\n        self.archive_size = self.pop_size  # Initial archive size\n        self.memory_index = 0\n        self.sf_M = []\n        self.scr_M = []\n        self.fevals = 0  # Track function evaluations\n        self.min_pop_size = 4  # Minimum population size\n        self.reduction_trigger = 0.25  # Fraction of budget before reduction starts\n        self.reduction_rate = 0.2  # Fraction of population reduced each time\n        self.use_orthogonal = True  # Use orthogonal crossover\n        self.dbe_probability = 0.1  # Probability of using Distance-Based Exploration\n        self.diversity_threshold = 0.1 #Threshold for population diversity.\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 10\n        self.success_rate = 0.0\n        self.success_history = []\n        self.success_window = 20 #Window size to calculate the success rate dynamically\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.fevals += self.pop_size\n\n    def calculate_distances(self, pop, i):\n        \"\"\"Calculates Euclidean distances from individual i to all other individuals.\"\"\"\n        distances = np.linalg.norm(pop - pop[i], axis=1)\n        distances[i] = np.inf  # Exclude distance to itself\n        return distances\n\n    def mutate(self, pop, i, F, Cr, archive):\n        if self.stagnation_counter > self.stagnation_threshold:\n            # Increase exploration if stagnant\n            dbe_prob = 0.5  # Increase DBE probability during stagnation\n            F = 1.0 #Increase F to promote exploration\n        else:\n            dbe_prob = self.dbe_probability\n        \n        if np.random.rand() < dbe_prob:\n            # Distance-Based Exploration\n            distances = self.calculate_distances(pop, i)\n            r1 = np.argmin(distances)  # Farthest Individual\n\n        else:\n\n            p_best_size = max(int(self.p * len(pop)), 1)\n\n            # Tournament selection for p-best\n            indices = np.argsort(self.fitness)[:p_best_size]\n            p_best_idx = np.random.choice(indices)\n            p_best = pop[p_best_idx]\n            indices = [idx for idx in range(len(pop)) if idx != i]\n            if len(indices) == 0:\n                return pop[i]\n            r1 = np.random.choice(indices)\n\n        # Choose r2 from current pop or archive\n        if len(archive) > 0 and np.random.rand() < 0.5:  # 50% chance to use archive\n            r2_idx = np.random.randint(0, len(archive))\n            r2 = archive[r2_idx]\n        else:\n            indices = [idx for idx in range(len(pop)) if idx != i and idx != r1]  # Ensure distinct indices\n            if len(indices) == 0:\n                r2 = pop[r1]  # If only one individual, take that\n            else:\n                r2_idx = np.random.choice(indices)\n                r2 = pop[r2_idx]\n\n        if np.random.rand() < dbe_prob:\n            p_best = pop[np.random.choice(range(len(pop)))]  # Choose random individual when using DBE for p_best\n        else:\n            p_best_size = max(int(self.p * len(pop)), 1)\n            # Tournament selection for p-best\n            indices = np.argsort(self.fitness)[:p_best_size]\n            p_best_idx = np.random.choice(indices)\n            p_best = pop[p_best_idx]\n\n        mutant = p_best + F * (pop[r1] - r2)\n        return mutant\n\n    def orthogonal_crossover(self, mutant, target):\n        # Generate an orthogonal array (example using a simple Hadamard matrix)\n        H = np.array([[1, 1], [1, -1]])\n\n        # Ensure dimensions are compatible by repeating or truncating\n        if self.dim > H.shape[1]:\n            H_repeated = np.tile(H, (1, int(np.ceil(self.dim / H.shape[1]))))\n            H_used = H_repeated[:, :self.dim]\n        else:\n            H_used = H[:, :self.dim]\n\n        trial = np.copy(target)\n\n        for j in range(self.dim):\n            if H_used[0, j] == H_used[1, j]:\n                continue  # No change\n            else:\n                trial[j] = mutant[j]\n\n        return trial\n\n    def crossover(self, mutant, target, Cr):\n        cross_points = np.random.rand(self.dim) < Cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def update_archive(self, individual, archive, success):\n        # Add to archive only if the update was successful\n        if success:\n            archive.append(individual)\n        return archive\n\n    def reduce_population(self):\n        if self.pop_size > self.min_pop_size:\n            sorted_indices = np.argsort(self.fitness)[::-1]  # Worst to best\n            reduction_amount = max(1, int(self.pop_size * self.reduction_rate))  # Reduce by reduction_rate, but at least 1\n            removed_indices = sorted_indices[:reduction_amount]\n\n            self.population = np.delete(self.population, removed_indices, axis=0)\n            self.fitness = np.delete(self.fitness, removed_indices)\n            self.pop_size = len(self.population)\n\n    def calculate_population_diversity(self):\n        \"\"\"Calculates the average distance of each individual to the population mean.\"\"\"\n        mean_position = np.mean(self.population, axis=0)\n        distances = np.linalg.norm(self.population - mean_position, axis=1)\n        avg_distance = np.mean(distances)\n        return avg_distance\n\n    def adjust_archive_size(self):\n        \"\"\"Dynamically adjusts the archive size based on population diversity.\"\"\"\n        diversity = self.calculate_population_diversity()\n\n        if diversity < self.diversity_threshold:\n            # If diversity is low, increase archive size to encourage exploration\n            self.archive_size = min(2 * self.pop_size, self.budget // 10)  # Limit archive size\n        else:\n            # If diversity is high, reduce archive size to focus on exploitation\n            self.archive_size = max(self.pop_size // 2, 10)  # Ensure a minimum archive size\n\n    def maintain_diversity(self):\n         \"\"\"Introduce a diversity maintenance mechanism to prevent premature convergence.\"\"\"\n         diversity = self.calculate_population_diversity()\n         if diversity < self.diversity_threshold:\n             # Select a subset of the population (e.g., the worst 20%)\n             num_to_diversify = max(1, int(0.2 * self.pop_size))  # At least 1\n             worst_indices = np.argsort(self.fitness)[-num_to_diversify:] # Indices of worst individuals\n\n             # Replace them with random individuals from the search space\n             for idx in worst_indices:\n                 self.population[idx] = np.random.uniform(self.bounds_lb, self.bounds_ub, size=self.dim)\n                 self.fitness[idx] = np.inf  # Reset fitness to be re-evaluated in next iteration\n\n    def calculate_success_rate(self):\n        \"\"\"Calculates the recent success rate of updates.\"\"\"\n        if len(self.success_history) > self.success_window:\n            self.success_history.pop(0)  # Remove oldest entry if window is full\n        if self.success_history:\n            self.success_rate = np.mean(self.success_history)\n        else:\n            self.success_rate = 0.0\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n        self.archive = []\n\n        self.sf_M = []\n        self.scr_M = []\n        weights = []\n        self.stagnation_counter = 0\n\n        while self.fevals < self.budget:\n            sf_k = []\n            scr_k = []\n            weights = []\n            old_f_opt = self.f_opt\n\n            #Adjust Archive size\n            self.adjust_archive_size()\n\n            for i in range(self.pop_size):\n                # Sample F and CR from memory\n                mem_idx = np.random.randint(0, self.memory_size)\n                F = self.memory_F[mem_idx]\n                Cr = self.memory_CR[mem_idx]\n\n                # Truncate and Reflect F\n                if F > 1:\n                    F = 1\n                if F <= 0:\n                    F = 0.000001\n\n                mutant = self.mutate(self.population, i, F, Cr, self.archive)\n                mutant = np.clip(mutant, self.bounds_lb, self.bounds_ub)\n\n                # Use orthogonal crossover with a probability or with the regular crossover\n                if self.use_orthogonal and np.random.rand() < 0.5:  # 50% chance to use orthogonal crossover\n                    trial = self.orthogonal_crossover(mutant, self.population[i])\n                else:\n                    trial = self.crossover(mutant, self.population[i], Cr)\n\n                f = func(trial)\n                self.fevals += 1\n\n                if f < self.fitness[i]:\n                    delta = abs(f - self.fitness[i])\n                    sf_k.append(F)\n                    scr_k.append(Cr)\n                    weights.append(delta)\n\n                    # Dynamically update archive based on success rate\n                    if self.success_rate < 0.3 or np.random.rand() < 0.5:  #More exploration\n                        self.archive = self.update_archive(self.population[i], self.archive, True) #Success = True is dummy\n                    else:\n                        self.archive = self.update_archive(self.population[i], self.archive, False) #Success = False is dummy\n\n                    if len(self.archive) > self.archive_size:\n                        self.archive.pop(np.random.randint(0, len(self.archive)))  # Keep archive size constant\n\n                    self.fitness[i] = f\n                    self.population[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                        self.success_history.append(1)\n                    else:\n                        self.success_history.append(0)\n\n                else:\n                    #Dynamically update archive based on success rate - even unsuccessful trials\n                    if self.success_rate < 0.3 or np.random.rand() < 0.5: #More exploration\n                        self.archive = self.update_archive(trial, self.archive, True) #Success = True is dummy\n                    else:\n                        self.archive = self.update_archive(trial, self.archive, False) #Success = False is dummy\n\n                    if len(self.archive) > self.archive_size:\n                        self.archive.pop(np.random.randint(0, len(self.archive)))\n                    self.success_history.append(0)\n\n                if self.fevals >= self.budget:\n                    break\n            \n            self.calculate_success_rate()\n\n            # Update memory using Lehmer mean\n            if sf_k:\n                weights = np.array(weights) / np.sum(weights)\n                sum_weights_sfk = np.sum(weights * np.array(sf_k) ** 2)\n                sum_weights = np.sum(weights * np.array(sf_k))\n\n                if sum_weights > 0:\n                    mean_F = sum_weights_sfk / sum_weights\n                else:\n                    mean_F = 0.5\n\n                mean_CR = np.average(scr_k, weights=weights)\n\n                self.memory_F[self.memory_index] = mean_F\n                self.memory_CR[self.memory_index] = mean_CR\n                self.memory_index = (self.memory_index + 1) % self.memory_size\n\n            # Adaptive population size reduction\n            if self.fevals > self.budget * self.reduction_trigger:  # Start reducing after reduction_trigger of the budget is spent\n                self.reduce_population()\n\n            self.maintain_diversity() #Maintain diversity\n\n            if self.f_opt == old_f_opt:\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm ImprovedSHADE_Ortho_AdaptPop_DBE_DynArchive_EnhancedExploration scored 0.599 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0b747827-f94c-41cb-a157-01708ce0db5d"], "operator": null, "metadata": {"aucs": [0.1743429547555937, 0.3428044077275969, 0.5548511692511289, 0.9046153586683946, 0.6582115731820184, 0.758204775433154, 0.36910783953059945, 0.5022968842168334, 0.683595915432727, 0.6239522686637773, 0.8514403953728376, 0.9963009440893168, 0.2534917918853593, 0.6372960045127023, 0.8008802396078173, 0.7159451568066504, 0.44077118372033963, 0.8970129633559938, 0.29164232597980133, 0.5160115468657098]}}
{"id": "fa041619-2c3a-44ed-b9d6-45e9891077d7", "fitness": 0.48189844024200923, "name": "EnhancedSHADE", "description": "Enhanced SHADE with adaptive archive, orthogonal crossover, distance-based exploration, dynamic population size, and a novel diversity restoration mechanism using a combination of opposition-based learning and random perturbation.", "code": "import numpy as np\n\nclass EnhancedSHADE:\n    def __init__(self, budget=10000, dim=10, memory_size=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = int(4 + 3 * np.log(budget))  # Initial population size\n        self.memory_size = memory_size\n        self.memory_F = np.full(memory_size, 0.5)\n        self.memory_CR = np.full(memory_size, 0.5)\n        self.p = 0.1  # Percentage of top individuals for p-best selection\n        self.population = None\n        self.fitness = None\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.archive = []  # For storing inferior solutions\n        self.archive_size = self.pop_size  # Initial archive size\n        self.memory_index = 0\n        self.sf_M = []\n        self.scr_M = []\n        self.fevals = 0  # Track function evaluations\n        self.min_pop_size = 4  # Minimum population size\n        self.reduction_trigger = 0.25  # Fraction of budget before reduction starts\n        self.reduction_rate = 0.2  # Fraction of population reduced each time\n        self.use_orthogonal = True  # Use orthogonal crossover\n        self.dbe_probability = 0.1  # Probability of using Distance-Based Exploration\n        self.diversity_threshold = 0.1 #Threshold for population diversity.\n        self.obl_rate = 0.2  # Rate of applying opposition-based learning\n        self.random_perturbation_rate = 0.3 # Rate of applying random perturbation\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.fevals += self.pop_size\n\n    def calculate_distances(self, pop, i):\n        \"\"\"Calculates Euclidean distances from individual i to all other individuals.\"\"\"\n        distances = np.linalg.norm(pop - pop[i], axis=1)\n        distances[i] = np.inf  # Exclude distance to itself\n        return distances\n\n    def mutate(self, pop, i, F, Cr, archive):\n        if np.random.rand() < self.dbe_probability:\n            # Distance-Based Exploration\n            distances = self.calculate_distances(pop, i)\n            r1 = np.argmin(distances)  # Farthest Individual\n\n        else:\n\n            p_best_size = max(int(self.p * len(pop)), 1)\n\n            # Tournament selection for p-best\n            indices = np.argsort(self.fitness)[:p_best_size]\n            p_best_idx = np.random.choice(indices)\n            p_best = pop[p_best_idx]\n            indices = [idx for idx in range(len(pop)) if idx != i]\n            if len(indices) == 0:\n                return pop[i]\n            r1 = np.random.choice(indices)\n\n        # Choose r2 from current pop or archive\n        if len(archive) > 0 and np.random.rand() < 0.5:  # 50% chance to use archive\n            r2_idx = np.random.randint(0, len(archive))\n            r2 = archive[r2_idx]\n        else:\n            indices = [idx for idx in range(len(pop)) if idx != i and idx != r1]  # Ensure distinct indices\n            if len(indices) == 0:\n                r2 = pop[r1]  # If only one individual, take that\n            else:\n                r2_idx = np.random.choice(indices)\n                r2 = pop[r2_idx]\n\n        if np.random.rand() < self.dbe_probability:\n            p_best = pop[np.random.choice(range(len(pop)))]  # Choose random individual when using DBE for p_best\n        else:\n            p_best_size = max(int(self.p * len(pop)), 1)\n            # Tournament selection for p-best\n            indices = np.argsort(self.fitness)[:p_best_size]\n            p_best_idx = np.random.choice(indices)\n            p_best = pop[p_best_idx]\n\n        mutant = p_best + F * (pop[r1] - r2)\n        return mutant\n\n    def orthogonal_crossover(self, mutant, target):\n        # Generate an orthogonal array (example using a simple Hadamard matrix)\n        H = np.array([[1, 1], [1, -1]])\n\n        # Ensure dimensions are compatible by repeating or truncating\n        if self.dim > H.shape[1]:\n            H_repeated = np.tile(H, (1, int(np.ceil(self.dim / H.shape[1]))))\n            H_used = H_repeated[:, :self.dim]\n        else:\n            H_used = H[:, :self.dim]\n\n        trial = np.copy(target)\n\n        for j in range(self.dim):\n            if H_used[0, j] == H_used[1, j]:\n                continue  # No change\n            else:\n                trial[j] = mutant[j]\n\n        return trial\n\n    def crossover(self, mutant, target, Cr):\n        cross_points = np.random.rand(self.dim) < Cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, target)\n        return trial\n\n    def update_archive(self, individual, archive):\n        archive.append(individual)\n        return archive\n\n    def reduce_population(self):\n        if self.pop_size > self.min_pop_size:\n            sorted_indices = np.argsort(self.fitness)[::-1]  # Worst to best\n            reduction_amount = max(1, int(self.pop_size * self.reduction_rate))  # Reduce by reduction_rate, but at least 1\n            removed_indices = sorted_indices[:reduction_amount]\n\n            self.population = np.delete(self.population, removed_indices, axis=0)\n            self.fitness = np.delete(self.fitness, removed_indices)\n            self.pop_size = len(self.population)\n\n    def calculate_population_diversity(self):\n        \"\"\"Calculates the average distance of each individual to the population mean.\"\"\"\n        mean_position = np.mean(self.population, axis=0)\n        distances = np.linalg.norm(self.population - mean_position, axis=1)\n        avg_distance = np.mean(distances)\n        return avg_distance\n\n    def adjust_archive_size(self):\n        \"\"\"Dynamically adjusts the archive size based on population diversity.\"\"\"\n        diversity = self.calculate_population_diversity()\n\n        if diversity < self.diversity_threshold:\n            # If diversity is low, increase archive size to encourage exploration\n            self.archive_size = min(2 * self.pop_size, self.budget // 10)  # Limit archive size\n        else:\n            # If diversity is high, reduce archive size to focus on exploitation\n            self.archive_size = max(self.pop_size // 2, 10)  # Ensure a minimum archive size\n\n    def opposition_based_learning(self, x):\n        \"\"\"Applies opposition-based learning to generate an opposite point.\"\"\"\n        return self.bounds_lb + self.bounds_ub - x\n\n    def random_perturbation(self, x):\n        \"\"\"Applies random perturbation to an individual.\"\"\"\n        return x + np.random.uniform(-0.1, 0.1, size=self.dim) #Adjust perturbation range as needed\n\n    def maintain_diversity(self, func):\n         \"\"\"Introduce a diversity maintenance mechanism using OBL and random perturbation.\"\"\"\n         diversity = self.calculate_population_diversity()\n         if diversity < self.diversity_threshold:\n             # Select a subset of the population (e.g., the worst 20%)\n             num_to_diversify = max(1, int(0.2 * self.pop_size))  # At least 1\n             worst_indices = np.argsort(self.fitness)[-num_to_diversify:] # Indices of worst individuals\n\n             # Replace them with either OBL or random perturbation\n             for idx in worst_indices:\n                 if np.random.rand() < self.obl_rate:\n                     # Apply opposition-based learning\n                     opposite_x = self.opposition_based_learning(self.population[idx])\n                     opposite_x = np.clip(opposite_x, self.bounds_lb, self.bounds_ub)  # Clip to bounds\n\n                     # Evaluate the opposite point\n                     f_opposite = func(opposite_x)\n                     self.fevals += 1\n\n                     # Replace if the opposite point is better\n                     if f_opposite < self.fitness[idx]:\n                         self.population[idx] = opposite_x\n                         self.fitness[idx] = f_opposite\n                     else:\n                         #If OBL is not better, apply random perturbation to original individual\n                         perturbed_x = self.random_perturbation(self.population[idx])\n                         perturbed_x = np.clip(perturbed_x, self.bounds_lb, self.bounds_ub)\n                         self.population[idx] = perturbed_x\n                         self.fitness[idx] = func(perturbed_x)\n                         self.fevals +=1\n                 elif np.random.rand() < self.random_perturbation_rate:\n                     # Apply random perturbation\n                     perturbed_x = self.random_perturbation(self.population[idx])\n                     perturbed_x = np.clip(perturbed_x, self.bounds_lb, self.bounds_ub)\n                     self.population[idx] = perturbed_x\n                     self.fitness[idx] = func(perturbed_x)\n                     self.fevals += 1\n                 else:\n                     # Replace with random individuals from the search space as a last resort\n                     self.population[idx] = np.random.uniform(self.bounds_lb, self.bounds_ub, size=self.dim)\n                     self.fitness[idx] = func(self.population[idx]) # Re-evaluate\n                     self.fevals +=1\n\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n        self.archive = []\n\n        self.sf_M = []\n        self.scr_M = []\n        weights = []\n\n        while self.fevals < self.budget:\n            sf_k = []\n            scr_k = []\n            weights = []\n\n            #Adjust Archive size\n            self.adjust_archive_size()\n\n            for i in range(self.pop_size):\n                # Sample F and CR from memory\n                mem_idx = np.random.randint(0, self.memory_size)\n                F = self.memory_F[mem_idx]\n                Cr = self.memory_CR[mem_idx]\n\n                # Truncate and Reflect F\n                if F > 1:\n                    F = 1\n                if F <= 0:\n                    F = 0.000001\n\n                mutant = self.mutate(self.population, i, F, Cr, self.archive)\n                mutant = np.clip(mutant, self.bounds_lb, self.bounds_ub)\n\n                # Use orthogonal crossover with a probability or with the regular crossover\n                if self.use_orthogonal and np.random.rand() < 0.5:  # 50% chance to use orthogonal crossover\n                    trial = self.orthogonal_crossover(mutant, self.population[i])\n                else:\n                    trial = self.crossover(mutant, self.population[i], Cr)\n\n                f = func(trial)\n                self.fevals += 1\n\n                if f < self.fitness[i]:\n                    delta = abs(f - self.fitness[i])\n                    sf_k.append(F)\n                    scr_k.append(Cr)\n                    weights.append(delta)\n\n                    self.archive = self.update_archive(self.population[i], self.archive)\n                    if len(self.archive) > self.archive_size:\n                        self.archive.pop(np.random.randint(0, len(self.archive)))  # Keep archive size constant\n\n                    self.fitness[i] = f\n                    self.population[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                else:\n                    self.archive = self.update_archive(trial, self.archive)  # Add even unsuccessful trials\n                    if len(self.archive) > self.archive_size:\n                        self.archive.pop(np.random.randint(0, len(self.archive)))\n\n                if self.fevals >= self.budget:\n                    break\n\n            # Update memory using Lehmer mean\n            if sf_k:\n                weights = np.array(weights) / np.sum(weights)\n                sum_weights_sfk = np.sum(weights * np.array(sf_k) ** 2)\n                sum_weights = np.sum(weights * np.array(sf_k))\n\n                if sum_weights > 0:\n                    mean_F = sum_weights_sfk / sum_weights\n                else:\n                    mean_F = 0.5\n\n                mean_CR = np.average(scr_k, weights=weights)\n\n                self.memory_F[self.memory_index] = mean_F\n                self.memory_CR[self.memory_index] = mean_CR\n                self.memory_index = (self.memory_index + 1) % self.memory_size\n\n            # Adaptive population size reduction\n            if self.fevals > self.budget * self.reduction_trigger:  # Start reducing after reduction_trigger of the budget is spent\n                self.reduce_population()\n\n            self.maintain_diversity(func) #Maintain diversity\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm EnhancedSHADE scored 0.482 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0b747827-f94c-41cb-a157-01708ce0db5d"], "operator": null, "metadata": {"aucs": [0.20882748987681377, 0.36265915521836545, 0.5633448053513122, 0.8728049719507287, 0.7158935162023408, 0.7835754754772423, 0.34808210785927063, 0]}}
{"id": "f82d2448-a6ad-4ab0-886f-b466f32dc68e", "fitness": 0.592734107330135, "name": "CMAES_AdaptiveArchive", "description": "CMA-ES with a self-adaptive archive size and a distance-based archive update strategy to improve diversity and convergence.", "code": "import numpy as np\n\nclass CMAES_AdaptiveArchive:\n    def __init__(self, budget=10000, dim=10, pop_size=None, archive_size_initial=10, archive_size_max=100, archive_update_frequency=10, distance_threshold=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else int(4 + 3 * np.log(dim))\n        self.mu = self.pop_size // 2\n        self.archive_size = archive_size_initial\n        self.archive_size_max = archive_size_max\n        self.archive_update_frequency = archive_update_frequency\n        self.distance_threshold = distance_threshold\n\n        self.mean = None\n        self.sigma = 0.1\n        self.C = None\n\n        self.P_sigma = None\n        self.P_c = None\n\n        self.c_sigma = None\n        self.c_c = None\n        self.c_1 = None\n        self.c_mu = None\n        self.damps = None\n\n        self.bounds_lb = None\n        self.bounds_ub = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.fevals = 0\n\n        self.archive_x = []\n        self.archive_f = []\n        self.archive_age = [] # Track the age of each archive member\n\n    def initialize_parameters(self):\n        self.C = np.eye(self.dim)\n        self.P_sigma = np.zeros(self.dim)\n        self.P_c = np.zeros(self.dim)\n\n        self.c_sigma = (self.mu + 2) / (self.dim + self.mu + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1) + self.c_sigma\n\n        self.c_1 = 2 / ((self.dim + 1.3)**2 + self.mu)\n        self.c_mu = min(1 - self.c_1, 2 * (self.mu - 1 + 1) / ((self.dim + 2)**2 + self.mu))\n\n    def sample_population(self):\n        z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n        C_sqrt = np.linalg.cholesky(self.C)\n        self.population = self.mean + self.sigma * z @ C_sqrt.T\n        self.population = np.clip(self.population, self.bounds_lb, self.bounds_ub)\n\n    def update_archive(self, population, fitness):\n        for i in range(len(population)):\n            x = population[i]\n            f = fitness[i]\n\n            if len(self.archive_x) < self.archive_size:\n                self.archive_x.append(x)\n                self.archive_f.append(f)\n                self.archive_age.append(0)\n            else:\n                # Replace the oldest element\n                max_age_index = np.argmax(self.archive_age)\n                if f < self.archive_f[max_age_index]:\n                    self.archive_x[max_age_index] = x\n                    self.archive_f[max_age_index] = f\n                    self.archive_age[max_age_index] = 0\n\n        # Increment age of all archive members\n        self.archive_age = [age + 1 for age in self.archive_age]\n\n    def distance_based_archive_maintenance(self):\n        if len(self.archive_x) > 1:\n            distances = np.zeros((len(self.archive_x), len(self.archive_x)))\n            for i in range(len(self.archive_x)):\n                for j in range(i + 1, len(self.archive_x)):\n                    distances[i, j] = np.linalg.norm(self.archive_x[i] - self.archive_x[j])\n                    distances[j, i] = distances[i, j]\n\n            # Identify similar archive members\n            for i in range(len(self.archive_x)):\n                for j in range(i + 1, len(self.archive_x)):\n                    if distances[i, j] < self.distance_threshold:\n                        # Replace the older/worse element based on fitness\n                        if self.archive_age[i] > self.archive_age[j] or self.archive_f[i] > self.archive_f[j]:\n                            self.archive_x[i] = np.random.uniform(self.bounds_lb, self.bounds_ub, size=self.dim)\n                            self.archive_f[i] = np.inf\n                            self.archive_age[i] = 0\n                        else:\n                            self.archive_x[j] = np.random.uniform(self.bounds_lb, self.bounds_ub, size=self.dim)\n                            self.archive_f[j] = np.inf\n                            self.archive_age[j] = 0\n\n    def adjust_archive_size(self):\n        # Dynamically adjust archive size based on performance. Increase if diversity is low, decrease if archive is too full.\n        if self.fevals % (self.budget // 10) == 0: # Adjust every 10% of budget\n            if len(self.archive_x) == self.archive_size and self.archive_size < self.archive_size_max:\n                 self.archive_size = min(self.archive_size + 1, self.archive_size_max)\n            elif len(self.archive_x) > self.archive_size:\n                 self.archive_x = self.archive_x[:self.archive_size]\n                 self.archive_f = self.archive_f[:self.archive_size]\n                 self.archive_age = self.archive_age[:self.archive_size]\n\n    def update_parameters(self):\n        sorted_indices = np.argsort(self.fitness)\n        self.population = self.population[sorted_indices]\n        self.fitness = self.fitness[sorted_indices]\n\n        parents = self.population[:self.mu]\n        old_mean = self.mean.copy()\n        self.mean = np.mean(parents, axis=0)\n\n        z = (self.mean - old_mean) / self.sigma\n        C_sqrt = np.linalg.cholesky(self.C)\n        z_in_C_space = np.linalg.solve(C_sqrt.T, z)\n\n        self.P_sigma = (1 - self.c_sigma) * self.P_sigma + np.sqrt(self.c_sigma * (2 - self.c_sigma)) * z_in_C_space\n        hsig = np.linalg.norm(self.P_sigma) / np.sqrt(1 - (1 - self.c_sigma)**(2 * (self.fevals / self.pop_size))) < (1.4 + 2 / (self.dim + 1))\n        self.P_c = (1 - self.c_c) * self.P_c + hsig * np.sqrt(self.c_c * (2 - self.c_c)) * z_in_C_space\n\n        y = parents - old_mean\n        delta_C = np.zeros_like(self.C)\n        for i in range(self.mu):\n            weight = self.c_mu\n            dy = (parents[i] - old_mean) / self.sigma\n            delta_C += weight * np.outer(dy, dy)\n\n        self.C = (1 - self.c_1 - self.c_mu) * self.C + self.c_1 * np.outer(self.P_c, self.P_c) + delta_C\n        self.sigma *= np.exp((self.c_sigma / self.damps) * (np.linalg.norm(self.P_sigma) / np.sqrt(self.dim) - 1))\n\n    def __call__(self, func):\n        self.bounds_lb = func.bounds.lb\n        self.bounds_ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n        self.mean = np.random.uniform(self.bounds_lb, self.bounds_ub, size=self.dim)\n        self.population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n\n        self.initialize_parameters()\n        self.c_c = (4 + self.mu / self.dim) / (self.dim + 4 + 2 * self.mu / self.dim)\n\n        while self.fevals < self.budget:\n            self.sample_population()\n\n            fitness = np.array([func(x) for x in self.population])\n            self.fevals += self.pop_size\n\n            if self.fevals > self.budget:\n                 fitness = fitness[:self.budget - (self.fevals - self.pop_size)]\n                 self.population = self.population[:self.budget - (self.fevals - self.pop_size)]\n                 break\n           \n            self.fitness = fitness\n            self.update_archive(self.population, self.fitness)\n\n            if self.fevals % self.archive_update_frequency == 0:\n                self.distance_based_archive_maintenance()\n\n            self.adjust_archive_size()\n            self.update_parameters()\n\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm CMAES_AdaptiveArchive scored 0.593 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a2c544b1-1e00-4f04-b664-fce5bb1b8fb2"], "operator": null, "metadata": {"aucs": [0.21688376612689797, 0.29818100304514805, 0.7232862575161056, 0.9128304179936648, 0.74211236016574, 0.8433829517517626, 0.33162325680179827, 0.6421825660744753, 0.5376902213099444, 0.4505553341339912, 0.7038741775457598, 0.9783312227873603, 0.2861537802660099, 0.6739292370950768, 0.7105857069039089, 0.7123457344032897, 0.46257384370298593, 0.9130911600182018, 0.21791197841395005, 0.49715717054663]}}
