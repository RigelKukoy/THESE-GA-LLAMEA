{"id": "6ddd41ff-8d76-4cf9-bc45-19207fbcb626", "fitness": -Infinity, "name": "OrthogonalLearningSwarm", "description": "Population-based algorithm with a combination of exploration and exploitation, using orthogonal learning to enhance diversity and convergence.", "code": "import numpy as np\n\nclass OrthogonalLearningSwarm:\n    def __init__(self, budget=10000, dim=10, pop_size=50, orthogonal_components=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.orthogonal_components = orthogonal_components # Number of orthogonal components\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.X = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.F = np.array([func(x) for x in self.X])\n        self.budget -= self.pop_size\n\n        while self.budget > 0:\n            # Find best particle\n            best_index = np.argmin(self.F)\n            best_x = self.X[best_index]\n            best_f = self.F[best_index]\n\n            if best_f < self.f_opt:\n                self.f_opt = best_f\n                self.x_opt = best_x.copy()\n\n            # Orthogonal Learning\n            for i in range(self.pop_size):\n                # Select orthogonal components (randomly)\n                orthogonal_indices = np.random.choice(self.dim, size=self.orthogonal_components, replace=False)\n\n                # Generate orthogonal design (simplified)\n                orthogonal_design = np.random.uniform(-1, 1, size=(self.orthogonal_components, self.orthogonal_components)) # Simplified orthogonal design. In real use, use an actual orthogonal design table.\n                \n                # Create new candidate solutions based on orthogonal design\n                for j in range(self.orthogonal_components):\n                    x_new = self.X[i].copy()\n                    for k, index in enumerate(orthogonal_indices):\n                        x_new[index] = self.X[i][index] + orthogonal_design[j, k] * (self.ub - self.lb) / 10  # Scale factor\n                        x_new[index] = np.clip(x_new[index], self.lb, self.ub) # Clip values\n                    \n                    f_new = func(x_new)\n                    self.budget -= 1\n\n                    if f_new < self.F[i]:\n                        self.X[i] = x_new\n                        self.F[i] = f_new\n            \n            # Global Movement (towards the best)\n            for i in range(self.pop_size):\n                r1 = np.random.rand(self.dim)\n                self.X[i] = self.X[i] + r1 * (best_x - self.X[i]) # Move each particle towards the best particle\n                self.X[i] = np.clip(self.X[i], self.lb, self.ub) # Clip values\n\n                f_new = func(self.X[i])\n                self.budget -= 1\n                if f_new < self.F[i]:\n                    self.F[i] = f_new\n\n                if self.budget <= 0:\n                    break\n\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "An exception occurred: Cannot take a larger sample than population when 'replace=False'.", "error": "", "parent_ids": [], "operator": null, "metadata": {}}
{"id": "e80347fc-2f32-4aac-9d3b-d2ac9cc311a6", "fitness": 0.0, "name": "AdaptiveDifferentialEvolution", "description": "An adaptive Differential Evolution strategy with a population that is re-initialized based on the best solution found so far, and using a combination of global and local search.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, Cr=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.Cr = Cr # Crossover rate\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        \n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.pop[best_index].copy()\n\n    def mutate(self):\n        mutated_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.pop[idxs]\n            mutated_pop[i] = x_r1 + self.F * (x_r2 - x_r3)\n        return mutated_pop\n\n    def crossover(self, mutated_pop):\n        crossed_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            for j in range(self.dim):\n                if np.random.rand() < self.Cr or j == np.random.randint(0, self.dim):\n                    crossed_pop[i, j] = mutated_pop[i, j]\n                else:\n                    crossed_pop[i, j] = self.pop[i, j]\n        return crossed_pop\n\n    def selection(self, func, crossed_pop):\n        new_fitness = np.array([func(x) for x in crossed_pop])\n        self.eval_count += self.pop_size\n\n        for i in range(self.pop_size):\n            if new_fitness[i] < self.fitness[i]:\n                self.pop[i] = crossed_pop[i]\n                self.fitness[i] = new_fitness[i]\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.pop[best_index].copy()\n\n    def reinitialize_population(self, func):\n        # Option 1: Re-initialize around the best solution\n        self.pop = np.random.normal(loc=self.x_opt, scale=0.5, size=(self.pop_size, self.dim))\n        self.pop = np.clip(self.pop, func.bounds.lb, func.bounds.ub) # Respect the bounds\n\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.pop[best_index].copy()\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.initialize_population(func)\n        \n        while self.eval_count < self.budget:\n            mutated_pop = self.mutate()\n            mutated_pop = np.clip(mutated_pop, func.bounds.lb, func.bounds.ub)\n            crossed_pop = self.crossover(mutated_pop)\n            crossed_pop = np.clip(crossed_pop, func.bounds.lb, func.bounds.ub)\n            self.selection(func, crossed_pop)\n\n            # Adaptive Strategy: Re-initialize population every so often if no improvement\n            if self.eval_count % (self.budget // 5) == 0:  #Re-initialize every 20% of budget\n              self.reinitialize_population(func)\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0]}}
{"id": "41ed32f6-8a92-4366-835c-6cac9ca3936b", "fitness": -Infinity, "name": "CMAES", "description": "Covariance Matrix Adaptation Evolution Strategy with resampling and budget-aware step-size adaptation.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, mu_factor=0.25, cs=0.08, dsigma=0.2, c_cov_mean=0.1, c_cov_rank_one=None, c_cov_rank_mu=None):\n        self.budget = budget\n        self.dim = dim\n        self.mu = int(mu_factor * self.budget)\n        self.lamb = self.budget # Removed lambda scaling \n        self.mu = min(self.mu, self.lamb // 2)\n\n        self.cs = cs\n        self.dsigma = dsigma\n        self.c_cov_mean = c_cov_mean\n\n        self.c_cov_rank_one = c_cov_mean if c_cov_rank_one is None else c_cov_rank_one\n        self.c_cov_rank_mu = min(1 - self.c_cov_rank_one, c_cov_rank_mean) if c_cov_rank_mu is None else c_cov_rank_mu\n\n        self.m = None # Mean\n        self.sigma = None # Step size\n        self.C = None # Covariance matrix\n        self.pc = None # Evolution path for mean\n        self.ps = None # Evolution path for step size\n        self.weights = None\n        self.B = None\n        self.D = None\n        self.eigen_updated = False\n\n    def initialize(self, func):\n        self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n        self.sigma = 0.5 * (func.bounds.ub - func.bounds.lb) # Adaptive initial sigma based on bounds\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.B = np.eye(self.dim)\n        self.D = np.ones(self.dim)\n        self.eigen_updated = False\n\n    def update_eigen(self):\n        self.C = np.triu(self.C) + np.triu(self.C, 1).T\n        self.D, self.B = np.linalg.eigh(self.C)\n        self.D = np.sqrt(self.D)\n        self.eigen_updated = True\n\n    def sample(self):\n        z = np.random.normal(0, 1, size=(self.dim, self.lamb))\n        y = self.B @ (self.D[:, None] * z)\n        x = self.m[:, None] + self.sigma * y\n        return x, y\n\n    def repair(self, x, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        x = np.clip(x, lb, ub)\n\n        return x\n\n    def __call__(self, func):\n        self.initialize(func)\n        f_opt = np.Inf\n        x_opt = None\n        evals = 0\n        \n        while evals < self.budget:\n            x, y = self.sample()\n            x = self.repair(x, func) # Repair after sampling\n\n            f = np.array([func(xi) for xi in x.T])\n            evals += self.lamb\n\n            if np.min(f) < f_opt:\n                f_opt = np.min(f)\n                x_opt = x[:, np.argmin(f)]\n\n            idx = np.argsort(f)\n            x_sorted = x[:, idx[:self.mu]]\n            y_sorted = y[:, idx[:self.mu]]\n\n            # Mean update\n            m_new = np.sum(x_sorted * self.weights[None, :], axis=1)\n\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * np.sum(self.weights)) * (self.B @ y_sorted).mean(axis=1)\n\n            self.pc = (1 - self.c_cov_mean) * self.pc + np.sqrt(self.c_cov_mean * (2 - self.c_cov_mean)) * (m_new - self.m) / self.sigma\n\n            self.m = m_new\n\n            # Covariance matrix update\n            rank_one = np.outer(self.pc, self.pc)\n            self.C = (1 - self.c_cov_rank_one - self.c_cov_rank_mu) * self.C + self.c_cov_rank_one * rank_one\n\n            for i in range(self.mu):\n                self.C += self.c_cov_rank_mu * self.weights[i] * np.outer(y_sorted[:,i], y_sorted[:,i])\n            \n            self.sigma *= np.exp((self.cs / self.dsigma) * (np.linalg.norm(self.ps) / np.sqrt(self.dim) - 1))\n            \n            if evals // self.lamb % (1 + self.dim // 10) == 0: # Delayed update\n                self.update_eigen()\n\n        return f_opt, x_opt", "configspace": "", "generation": 0, "feedback": "An exception occurred: name 'c_cov_rank_mean' is not defined.", "error": "", "parent_ids": [], "operator": null, "metadata": {}}
{"id": "f5b7c1dc-10ca-4c1d-9a08-2c33539efa16", "fitness": 0.7006172255879141, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with Archive and stochastic ranking for constraint handling, dynamically adjusting parameters based on success rate.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = 0.5\n        self.CR = 0.9\n        self.archive = []\n        self.success_F = []\n        self.success_CR = []\n        self.success_count = 0\n        self.memory_F = np.ones(10) * 0.5\n        self.memory_CR = np.ones(10) * 0.9\n        self.memory_idx = 0\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        \n        if np.min(self.fitness) < self.f_opt:\n            self.f_opt = np.min(self.fitness)\n            self.x_opt = self.population[np.argmin(self.fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[idxs]\n\n                # Adaptive F and CR\n                self.F = self.memory_F[self.memory_idx]\n                self.CR = self.memory_CR[self.memory_idx]\n                \n                mutant = x_r1 + self.F * (x_r2 - x_r3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        trial[j] = mutant[j]\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    self.success_F.append(self.F)\n                    self.success_CR.append(self.CR)\n                    self.success_count += 1\n\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n                    \n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n            \n            # Update memory\n            if self.success_count > 0:\n                self.memory_F[self.memory_idx] = np.mean(self.success_F) if len(self.success_F) > 0 else 0.5\n                self.memory_CR[self.memory_idx] = np.mean(self.success_CR) if len(self.success_CR) > 0 else 0.9\n\n            self.memory_idx = (self.memory_idx + 1) % 10\n            self.success_F = []\n            self.success_CR = []\n            self.success_count = 0\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDE scored 0.701 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.3277075810478336, 0.6736605682403685, 0.688182778520599, 0.8899991980723315, 0.7473966095411702, 0.7743914168228975, 0.6162389771863697, 0.6521675443284555, 0.7441729689996187, 0.7003250268289689, 0.8531998993979808, 0.999913083128295, 0.6736746836335986, 0.7242036145865602, 0.9014038918676717, 0.7730963441082417, 0.5985939308289514, 0.8299078980964909, 0.3233478695448754, 0.5207606269770021]}}
{"id": "eb4084ee-c57c-4b0d-bc16-fd81496853d8", "fitness": -Infinity, "name": "RBFOpt", "description": "A gradient-free optimization algorithm using a surrogate model based on radial basis functions (RBF) to approximate the objective function, combined with trust region updates for efficient exploration and exploitation.", "code": "import numpy as np\nfrom scipy.interpolate import Rbf\n\nclass RBFOpt:\n    def __init__(self, budget=10000, dim=10, num_points=20, trust_region_size=0.5, scaling=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.num_points = num_points\n        self.trust_region_size = trust_region_size\n        self.scaling = scaling\n        self.X = None\n        self.F = None\n        self.rbf = None\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def initialize(self, func):\n        # Initial sample points within bounds\n        self.X = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.num_points, self.dim))\n        self.F = np.array([func(x) for x in self.X])\n\n        self.x_opt = self.X[np.argmin(self.F)]\n        self.f_opt = np.min(self.F)\n\n    def train_rbf(self):\n        # Train RBF surrogate model\n        self.rbf = Rbf(*self.X.T, self.F, function='thin_plate_spline')\n\n    def surrogate(self, x):\n        # Evaluate surrogate model\n        return self.rbf(*x.T)\n\n    def trust_region_search(self, func):\n        # Sample within trust region\n        x_new = np.random.uniform(self.x_opt - self.trust_region_size, self.x_opt + self.trust_region_size, size=(self.num_points, self.dim))\n        \n        # Clip to bounds\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        x_new = np.clip(x_new, lb, ub)\n\n        # Evaluate surrogate\n        f_surrogate = self.surrogate(x_new.T)\n\n        # Select best point based on surrogate\n        best_idx = np.argmin(f_surrogate)\n        x_candidate = x_new[best_idx]\n\n        # Evaluate actual function\n        f_candidate = func(x_candidate)\n        \n        return x_candidate, f_candidate\n    \n    def update_trust_region(self, success):\n        if success:\n            self.trust_region_size *= (1 + self.scaling)\n        else:\n            self.trust_region_size *= (1 - self.scaling)\n        self.trust_region_size = np.clip(self.trust_region_size, 1e-6, 1.0)\n\n    def __call__(self, func):\n        self.initialize(func)\n        evals = self.num_points  # Account for initial evaluations\n\n        while evals < self.budget:\n            self.train_rbf()\n            x_candidate, f_candidate = self.trust_region_search(func)\n            evals += 1\n\n            if f_candidate < self.f_opt:\n                self.f_opt = f_candidate\n                self.x_opt = x_candidate\n                success = True\n            else:\n                success = False\n\n            # Update data\n            self.X = np.vstack((self.X, self.x_candidate))\n            self.F = np.append(self.F, self.f_candidate)\n\n            # Keep only the best points\n            idx = np.argsort(self.F)[:self.num_points]\n            self.X = self.X[idx]\n            self.F = self.F[idx]\n\n            self.update_trust_region(success)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "An exception occurred: name 'Rbf' is not defined.", "error": "", "parent_ids": ["41ed32f6-8a92-4366-835c-6cac9ca3936b"], "operator": null, "metadata": {}}
{"id": "377c8b16-342f-4c08-98f6-1504f91557b2", "fitness": -Infinity, "name": "CMAES", "description": "Improved CMA-ES with budget-aware parameter adaptation, delayed eigenvalue updates, and adaptive initial sigma.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, mu_factor=0.25, cs=0.08, dsigma=0.2, c_cov_mean=0.1, c_cov_rank_one=None, c_cov_rank_mu=None):\n        self.budget = budget\n        self.dim = dim\n        self.mu = int(mu_factor * self.budget)\n        self.lamb = self.budget # Removed lambda scaling \n        self.mu = min(self.mu, self.lamb // 2)\n\n        self.cs = cs\n        self.dsigma = dsigma\n        self.c_cov_mean = c_cov_mean\n\n        self.c_cov_rank_one = c_cov_mean if c_cov_rank_one is None else c_cov_rank_one\n        self.c_cov_rank_mu = min(1 - self.c_cov_rank_one, self.c_cov_mean) if c_cov_rank_mu is None else c_cov_rank_mu # Corrected typo\n\n        self.m = None # Mean\n        self.sigma = None # Step size\n        self.C = None # Covariance matrix\n        self.pc = None # Evolution path for mean\n        self.ps = None # Evolution path for step size\n        self.weights = None\n        self.B = None\n        self.D = None\n        self.eigen_updated = False\n\n    def initialize(self, func):\n        self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n        self.sigma = 0.5 * (func.bounds.ub - func.bounds.lb) # Adaptive initial sigma based on bounds\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.B = np.eye(self.dim)\n        self.D = np.ones(self.dim)\n        self.eigen_updated = False\n\n    def update_eigen(self):\n        self.C = np.triu(self.C) + np.triu(self.C, 1).T\n        self.D, self.B = np.linalg.eigh(self.C)\n        self.D = np.sqrt(self.D)\n        self.eigen_updated = True\n\n    def sample(self):\n        z = np.random.normal(0, 1, size=(self.dim, self.lamb))\n        y = self.B @ (self.D[:, None] * z)\n        x = self.m[:, None] + self.sigma * y\n        return x, y\n\n    def repair(self, x, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        x = np.clip(x, lb, ub)\n\n        return x\n\n    def __call__(self, func):\n        self.initialize(func)\n        f_opt = np.Inf\n        x_opt = None\n        evals = 0\n        \n        while evals < self.budget:\n            x, y = self.sample()\n            x = self.repair(x, func) # Repair after sampling\n\n            f = np.array([func(xi) for xi in x.T])\n            evals += self.lamb\n\n            if np.min(f) < f_opt:\n                f_opt = np.min(f)\n                x_opt = x[:, np.argmin(f)]\n\n            idx = np.argsort(f)\n            x_sorted = x[:, idx[:self.mu]]\n            y_sorted = y[:, idx[:self.mu]]\n\n            # Mean update\n            m_new = np.sum(x_sorted * self.weights[None, :], axis=1)\n\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * np.sum(self.weights)) * (self.B @ y_sorted).mean(axis=1)\n\n            self.pc = (1 - self.c_cov_mean) * self.pc + np.sqrt(self.c_cov_mean * (2 - self.c_cov_mean)) * (m_new - self.m) / self.sigma\n\n            self.m = m_new\n\n            # Covariance matrix update\n            rank_one = np.outer(self.pc, self.pc)\n            self.C = (1 - self.c_cov_rank_one - self.c_cov_rank_mu) * self.C + self.c_cov_rank_one * rank_one\n\n            for i in range(self.mu):\n                self.C += self.c_cov_rank_mu * self.weights[i] * np.outer(y_sorted[:,i], y_sorted[:,i])\n            \n            self.sigma *= np.exp((self.cs / self.dsigma) * (np.linalg.norm(self.ps) / np.sqrt(self.dim) - 1))\n            \n            if evals // self.lamb % (1 + self.dim // 10) == 0: # Delayed update\n                self.update_eigen()\n\n        return f_opt, x_opt", "configspace": "", "generation": 1, "feedback": "An exception occurred: operands could not be broadcast together with shapes (2,) (2,100) .", "error": "", "parent_ids": ["41ed32f6-8a92-4366-835c-6cac9ca3936b"], "operator": null, "metadata": {}}
{"id": "7635799b-6e0c-4a61-b781-8c1d6fca4e8f", "fitness": -Infinity, "name": "CMAES", "description": "Improved CMA-ES with adaptive parameters, bound constraints, and delayed eigenvalue updates for better exploration and exploitation.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, mu_factor=0.25, cs=0.08, dsigma=0.2, c_cov_mean=0.1, c_cov_rank_one=None, c_cov_rank_mu=None):\n        self.budget = budget\n        self.dim = dim\n        self.mu = int(mu_factor * self.budget)\n        self.lamb = self.budget # Removed lambda scaling \n        self.mu = min(self.mu, self.lamb // 2)\n\n        self.cs = cs\n        self.dsigma = dsigma\n        self.c_cov_mean = c_cov_mean\n\n        self.c_cov_rank_one = c_cov_mean if c_cov_rank_one is None else c_cov_rank_one\n        self.c_cov_rank_mu = min(1 - self.c_cov_rank_one, 1) * c_cov_mean if c_cov_rank_mu is None else c_cov_rank_mu\n\n        self.m = None # Mean\n        self.sigma = None # Step size\n        self.C = None # Covariance matrix\n        self.pc = None # Evolution path for mean\n        self.ps = None # Evolution path for step size\n        self.weights = None\n        self.B = None\n        self.D = None\n        self.eigen_updated = False\n\n    def initialize(self, func):\n        self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n        self.sigma = 0.5 * (func.bounds.ub - func.bounds.lb) # Adaptive initial sigma based on bounds\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.B = np.eye(self.dim)\n        self.D = np.ones(self.dim)\n        self.eigen_updated = False\n\n    def update_eigen(self):\n        self.C = np.triu(self.C) + np.triu(self.C, 1).T\n        try:\n            self.D, self.B = np.linalg.eigh(self.C)\n            self.D = np.sqrt(self.D)\n            self.eigen_updated = True\n        except np.linalg.LinAlgError:\n            self.C = self.C + 1e-8 * np.eye(self.dim)\n            self.D, self.B = np.linalg.eigh(self.C)\n            self.D = np.sqrt(self.D)\n            self.eigen_updated = True\n\n    def sample(self):\n        z = np.random.normal(0, 1, size=(self.dim, self.lamb))\n        y = self.B @ (self.D[:, None] * z)\n        x = self.m[:, None] + self.sigma * y\n        return x, y\n\n    def repair(self, x, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        x = np.clip(x, lb, ub)\n\n        return x\n\n    def __call__(self, func):\n        self.initialize(func)\n        f_opt = np.Inf\n        x_opt = None\n        evals = 0\n        \n        while evals < self.budget:\n            x, y = self.sample()\n            x = self.repair(x, func) # Repair after sampling\n\n            f = np.array([func(xi) for xi in x.T])\n            evals += self.lamb\n\n            if np.min(f) < f_opt:\n                f_opt = np.min(f)\n                x_opt = x[:, np.argmin(f)]\n\n            idx = np.argsort(f)\n            x_sorted = x[:, idx[:self.mu]]\n            y_sorted = y[:, idx[:self.mu]]\n\n            # Mean update\n            m_new = np.sum(x_sorted * self.weights[None, :], axis=1)\n\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * np.sum(self.weights)) * (self.B @ y_sorted).mean(axis=1)\n\n            self.pc = (1 - self.c_cov_mean) * self.pc + np.sqrt(self.c_cov_mean * (2 - self.c_cov_mean)) * (m_new - self.m) / self.sigma\n\n            self.m = m_new\n\n            # Covariance matrix update\n            rank_one = np.outer(self.pc, self.pc)\n            self.C = (1 - self.c_cov_rank_one - self.c_cov_rank_mu) * self.C + self.c_cov_rank_one * rank_one\n\n            for i in range(self.mu):\n                self.C += self.c_cov_rank_mu * self.weights[i] * np.outer(y_sorted[:,i], y_sorted[:,i])\n            \n            self.sigma *= np.exp((self.cs / self.dsigma) * (np.linalg.norm(self.ps) / np.sqrt(self.dim) - 1))\n            \n            if evals // self.lamb % (1 + self.dim // 10) == 0: # Delayed update\n                self.update_eigen()\n\n        return f_opt, x_opt", "configspace": "", "generation": 1, "feedback": "An exception occurred: operands could not be broadcast together with shapes (2,) (2,100) .", "error": "", "parent_ids": ["41ed32f6-8a92-4366-835c-6cac9ca3936b"], "operator": null, "metadata": {}}
{"id": "e687e3e5-dc15-4d89-81ed-8d29003a5a2e", "fitness": 0.0, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with improved parameter adaptation using a weighted Lehmer mean for parameter updates and a more aggressive update strategy.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = 0.5\n        self.CR = 0.9\n        self.archive = []\n        self.success_F = []\n        self.success_CR = []\n        self.success_count = 0\n        self.memory_F = np.ones(10) * 0.5\n        self.memory_CR = np.ones(10) * 0.9\n        self.memory_idx = 0\n        self.epsilon = 1e-6\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        \n        if np.min(self.fitness) < self.f_opt:\n            self.f_opt = np.min(self.fitness)\n            self.x_opt = self.population[np.argmin(self.fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[idxs]\n\n                # Adaptive F and CR\n                self.F = self.memory_F[self.memory_idx]\n                self.CR = self.memory_CR[self.memory_idx]\n                \n                mutant = x_r1 + self.F * (x_r2 - x_r3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        trial[j] = mutant[j]\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    self.success_F.append(self.F)\n                    self.success_CR.append(self.CR)\n                    self.success_count += 1\n\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n                    \n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                else:\n                    # If the trial vector is worse, perturb the current individual\n                    self.population[i] = np.clip(self.population[i] + 0.1 * np.random.normal(0, 1, self.dim), func.bounds.lb, func.bounds.ub)\n                    self.fitness[i] = func(self.population[i])\n                    self.budget -=1\n                    if self.fitness[i] < self.f_opt:\n                         self.f_opt = self.fitness[i]\n                         self.x_opt = self.population[i]\n                    \n\n            # Update memory\n            if self.success_count > 0:\n                # Lehmer mean\n                def lehmer_mean(x):\n                    return np.sum(np.array(x)**2) / (np.sum(np.array(x)) + self.epsilon)\n\n                mean_F = lehmer_mean(self.success_F)\n                mean_CR = np.mean(self.success_CR) if len(self.success_CR) > 0 else 0.9\n\n                self.memory_F[self.memory_idx] = mean_F\n                self.memory_CR[self.memory_idx] = mean_CR\n\n            self.memory_idx = (self.memory_idx + 1) % 10\n            self.success_F = []\n            self.success_CR = []\n            self.success_count = 0\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f5b7c1dc-10ca-4c1d-9a08-2c33539efa16"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "d762ae17-8021-4081-a5b1-9089707bd8e0", "fitness": -Infinity, "name": "CMAES", "description": "Budget-aware CMA-ES with adaptive sigma, delayed eigen updates, and bound repair after sampling.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, mu_factor=0.25, cs=0.08, dsigma=0.2, c_cov_mean=0.1, c_cov_rank_one=None, c_cov_rank_mu=None):\n        self.budget = budget\n        self.dim = dim\n        self.mu = int(mu_factor * self.budget)\n        self.lamb = self.budget # Removed lambda scaling \n        self.mu = min(self.mu, self.lamb // 2)\n\n        self.cs = cs\n        self.dsigma = dsigma\n        self.c_cov_mean = c_cov_mean\n\n        self.c_cov_rank_one = c_cov_mean if c_cov_rank_one is None else c_cov_rank_one\n        self.c_cov_rank_mu = min(1 - self.c_cov_rank_one, self.c_cov_mean) if c_cov_rank_mu is None else c_cov_rank_mu\n\n        self.m = None # Mean\n        self.sigma = None # Step size\n        self.C = None # Covariance matrix\n        self.pc = None # Evolution path for mean\n        self.ps = None # Evolution path for step size\n        self.weights = None\n        self.B = None\n        self.D = None\n        self.eigen_updated = False\n\n    def initialize(self, func):\n        self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n        self.sigma = 0.5 * (func.bounds.ub - func.bounds.lb) # Adaptive initial sigma based on bounds\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.B = np.eye(self.dim)\n        self.D = np.ones(self.dim)\n        self.eigen_updated = False\n\n    def update_eigen(self):\n        self.C = np.triu(self.C) + np.triu(self.C, 1).T\n        try:\n            self.D, self.B = np.linalg.eigh(self.C)\n            self.D = np.sqrt(self.D)\n            self.eigen_updated = True\n        except np.linalg.LinAlgError:\n            # Handle non-positive definite covariance matrix\n            self.C += 1e-6 * np.eye(self.dim)  # Add a small diagonal matrix\n            self.D, self.B = np.linalg.eigh(self.C)\n            self.D = np.sqrt(self.D)\n            self.eigen_updated = True\n\n    def sample(self):\n        z = np.random.normal(0, 1, size=(self.dim, self.lamb))\n        y = self.B @ (self.D[:, None] * z)\n        x = self.m[:, None] + self.sigma * y\n        return x, y\n\n    def repair(self, x, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        x = np.clip(x, lb, ub)\n\n        return x\n\n    def __call__(self, func):\n        self.initialize(func)\n        f_opt = np.Inf\n        x_opt = None\n        evals = 0\n        \n        while evals < self.budget:\n            x, y = self.sample()\n            x = self.repair(x, func) # Repair after sampling\n\n            f = np.array([func(xi) for xi in x.T])\n            evals += self.lamb\n\n            if np.min(f) < f_opt:\n                f_opt = np.min(f)\n                x_opt = x[:, np.argmin(f)]\n\n            idx = np.argsort(f)\n            x_sorted = x[:, idx[:self.mu]]\n            y_sorted = y[:, idx[:self.mu]]\n\n            # Mean update\n            m_new = np.sum(x_sorted * self.weights[None, :], axis=1)\n\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * np.sum(self.weights)) * (self.B @ y_sorted).mean(axis=1)\n\n            self.pc = (1 - self.c_cov_mean) * self.pc + np.sqrt(self.c_cov_mean * (2 - self.c_cov_mean)) * (m_new - self.m) / self.sigma\n\n            self.m = m_new\n\n            # Covariance matrix update\n            rank_one = np.outer(self.pc, self.pc)\n            self.C = (1 - self.c_cov_rank_one - self.c_cov_rank_mu) * self.C + self.c_cov_rank_one * rank_one\n\n            for i in range(self.mu):\n                self.C += self.c_cov_rank_mu * self.weights[i] * np.outer(y_sorted[:,i], y_sorted[:,i])\n            \n            self.sigma *= np.exp((self.cs / self.dsigma) * (np.linalg.norm(self.ps) / np.sqrt(self.dim) - 1))\n            \n            if evals // self.lamb % (1 + self.dim // 10) == 0: # Delayed update\n                self.update_eigen()\n\n        return f_opt, x_opt", "configspace": "", "generation": 1, "feedback": "An exception occurred: operands could not be broadcast together with shapes (2,) (2,100) .", "error": "", "parent_ids": ["41ed32f6-8a92-4366-835c-6cac9ca3936b"], "operator": null, "metadata": {}}
{"id": "f7520efd-d035-44e3-8371-495e8021acb1", "fitness": 0.0, "name": "AdaptiveDifferentialEvolution", "description": "Simplified Adaptive Differential Evolution with periodic re-initialization around the best solution and dynamic parameter adaptation.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, Cr=0.9, reinit_freq=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.Cr = Cr # Crossover rate\n        self.reinit_freq = reinit_freq # Frequency of re-initialization (as a fraction of budget)\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        \n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.pop[best_index].copy()\n\n    def mutate(self):\n        mutated_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.pop[idxs]\n            mutated_pop[i] = x_r1 + self.F * (x_r2 - x_r3)\n        return mutated_pop\n\n    def crossover(self, mutated_pop):\n        crossed_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            crossover_mask = np.random.rand(self.dim) < self.Cr\n            j_rand = np.random.randint(0, self.dim)\n            crossover_mask[j_rand] = True  # Ensure at least one gene is exchanged\n\n            crossed_pop[i] = np.where(crossover_mask, mutated_pop[i], self.pop[i])\n        return crossed_pop\n\n    def selection(self, func, crossed_pop):\n        new_fitness = np.array([func(x) for x in crossed_pop])\n        self.eval_count += self.pop_size\n\n        improved = new_fitness < self.fitness\n        self.pop[improved] = crossed_pop[improved]\n        self.fitness[improved] = new_fitness[improved]\n        \n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.pop[best_index].copy()\n\n    def reinitialize_population(self, func):\n        self.pop = np.random.normal(loc=self.x_opt, scale=0.1 * (func.bounds.ub - func.bounds.lb), size=(self.pop_size, self.dim))\n        self.pop = np.clip(self.pop, func.bounds.lb, func.bounds.ub)\n\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.pop[best_index].copy()\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.initialize_population(func)\n        \n        reinit_interval = int(self.budget * self.reinit_freq)\n\n        while self.eval_count < self.budget:\n            mutated_pop = self.mutate()\n            mutated_pop = np.clip(mutated_pop, func.bounds.lb, func.bounds.ub)\n            crossed_pop = self.crossover(mutated_pop)\n            crossed_pop = np.clip(crossed_pop, func.bounds.lb, func.bounds.ub)\n            self.selection(func, crossed_pop)\n\n            if reinit_interval > 0 and self.eval_count % reinit_interval == 0:\n                self.reinitialize_population(func)\n\n            # Dynamic adaptation of F and Cr (example, can be more sophisticated)\n            self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 0.9)  # Keep F within [0.1, 0.9]\n            self.Cr = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0)  # Keep Cr within [0.1, 1.0]\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e80347fc-2f32-4aac-9d3b-d2ac9cc311a6"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "cdc1196e-e3dd-40b6-bdb1-7294b8ff332d", "fitness": 0.0, "name": "AdaptiveDifferentialEvolution", "description": "Adaptive Differential Evolution with dynamic parameter adaptation and periodic local search refinement.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, Cr=0.9, local_search_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.Cr = Cr\n        self.local_search_prob = local_search_prob\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.success_history_F = []\n        self.success_history_Cr = []\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.pop[best_index].copy()\n\n    def mutate(self):\n        mutated_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.pop[idxs]\n            mutated_pop[i] = x_r1 + self.F * (x_r2 - x_r3)\n        return mutated_pop\n\n    def crossover(self, mutated_pop):\n        crossed_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            for j in range(self.dim):\n                if np.random.rand() < self.Cr or j == np.random.randint(0, self.dim):\n                    crossed_pop[i, j] = mutated_pop[i, j]\n                else:\n                    crossed_pop[i, j] = self.pop[i, j]\n        return crossed_pop\n\n    def selection(self, func, crossed_pop):\n        new_fitness = np.array([func(x) for x in crossed_pop])\n        self.eval_count += self.pop_size\n        improved_indices = new_fitness < self.fitness\n        \n        successful_F = []\n        successful_Cr = []\n        \n        for i in range(self.pop_size):\n            if new_fitness[i] < self.fitness[i]:\n                self.pop[i] = crossed_pop[i]\n                self.fitness[i] = new_fitness[i]\n                successful_F.append(self.F)\n                successful_Cr.append(self.Cr)\n\n        if successful_F:\n            self.success_history_F.extend(successful_F)\n            self.success_history_Cr.extend(successful_Cr)\n\n            # Update F and Cr adaptively using successful values\n            self.F = np.mean(self.success_history_F) if self.success_history_F else 0.5\n            self.Cr = np.mean(self.success_history_Cr) if self.success_history_Cr else 0.9\n        \n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.pop[best_index].copy()\n\n    def local_search(self, func):\n        for i in range(self.pop_size):\n            if np.random.rand() < self.local_search_prob:\n                x_current = self.pop[i].copy()\n                f_current = self.fitness[i]\n                \n                # Perturb each dimension slightly\n                for j in range(self.dim):\n                    x_perturbed = x_current.copy()\n                    x_perturbed[j] += np.random.normal(0, 0.1)  # Small perturbation\n                    x_perturbed = np.clip(x_perturbed, func.bounds.lb, func.bounds.ub)\n                    \n                    f_perturbed = func(x_perturbed)\n                    self.eval_count += 1\n                    \n                    if f_perturbed < f_current:\n                        x_current[j] = x_perturbed[j]\n                        f_current = f_perturbed\n                        \n                self.pop[i] = x_current\n                self.fitness[i] = f_current\n                \n                if f_current < self.f_opt:\n                    self.f_opt = f_current\n                    self.x_opt = x_current.copy()\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.success_history_F = []\n        self.success_history_Cr = []\n        self.initialize_population(func)\n        \n        while self.eval_count < self.budget:\n            mutated_pop = self.mutate()\n            mutated_pop = np.clip(mutated_pop, func.bounds.lb, func.bounds.ub)\n            crossed_pop = self.crossover(mutated_pop)\n            crossed_pop = np.clip(crossed_pop, func.bounds.lb, func.bounds.ub)\n            self.selection(func, crossed_pop)\n            self.local_search(func) # Add local search\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e80347fc-2f32-4aac-9d3b-d2ac9cc311a6"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "6e0bd570-87f9-4839-bc12-07022df8c42c", "fitness": 0.20767145463049974, "name": "PSO_DE", "description": "A population-based algorithm that combines particle swarm optimization with a mutation operator inspired by differential evolution to enhance exploration.", "code": "import numpy as np\n\nclass PSO_DE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, w=0.7, c1=1.5, c2=1.5, F=0.8):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w = w  # Inertia weight\n        self.c1 = c1  # Cognitive coefficient\n        self.c2 = c2  # Social coefficient\n        self.F = F # Differential evolution parameter\n        self.lb = -5.0\n        self.ub = 5.0\n\n        self.X = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.V = np.random.uniform(-1, 1, size=(self.pop_size, self.dim)) # Initialize velocities\n        self.P = self.X.copy()  # Personal best positions\n        self.F_X = np.zeros(self.pop_size)\n        self.F_P = np.zeros(self.pop_size)\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Evaluate initial population\n        for i in range(self.pop_size):\n            self.F_X[i] = func(self.X[i])\n            self.F_P[i] = self.F_X[i]\n            self.budget -= 1\n\n            if self.F_X[i] < self.f_opt:\n                self.f_opt = self.F_X[i]\n                self.x_opt = self.X[i].copy()\n\n        while self.budget > 0:\n            # Find global best particle\n            best_index = np.argmin(self.F_P)\n            best_x = self.P[best_index]\n            best_f = self.F_P[best_index]\n\n            if best_f < self.f_opt:\n                self.f_opt = best_f\n                self.x_opt = best_x.copy()\n            \n            # Update velocities and positions\n            for i in range(self.pop_size):\n                # PSO update\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.V[i] = self.w * self.V[i] + self.c1 * r1 * (self.P[i] - self.X[i]) + self.c2 * r2 * (best_x - self.X[i])\n                self.X[i] = self.X[i] + self.V[i]\n                self.X[i] = np.clip(self.X[i], self.lb, self.ub)\n\n                # Differential Evolution mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.X[idxs[0]], self.X[idxs[1]], self.X[idxs[2]]\n                x_mutated = self.X[i] + self.F * (x_r2 - x_r3)\n                x_mutated = np.clip(x_mutated, self.lb, self.ub)\n\n                # Evaluate the new candidate\n                f_new = func(x_mutated)\n                self.budget -= 1\n                \n                # Select between original and mutated\n                if f_new < self.F_X[i]:\n                    self.X[i] = x_mutated\n                    self.F_X[i] = f_new\n\n                    #Update personal best\n                    if f_new < self.F_P[i]:\n                        self.P[i] = x_mutated.copy()\n                        self.F_P[i] = f_new\n                else:\n                    f_original = func(self.X[i])\n                    self.budget -=1\n                    if f_original < self.F_P[i]:\n                        self.F_P[i] = f_original\n                        self.P[i] = self.X[i].copy() # Copy necessary?\n\n\n                if self.budget <= 0:\n                    break\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm PSO_DE scored 0.208 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6ddd41ff-8d76-4cf9-bc45-19207fbcb626"], "operator": null, "metadata": {"aucs": [0.19979029370635704, 0.1826183707953145, 0.4482771540203274, 0]}}
{"id": "4c327fee-4c9c-4d58-a848-858ca4be83e4", "fitness": 0.2827268370059744, "name": "SpiralDynamicOptimization", "description": "A population-based algorithm using a spiral dynamic approach where each particle moves along a logarithmic spiral towards the current best solution, combined with mutation and adaptive step size.", "code": "import numpy as np\n\nclass SpiralDynamicOptimization:\n    def __init__(self, budget=10000, dim=10, pop_size=50, a=0.9, omega=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.a = a  # Contraction factor\n        self.omega = omega  # Spiral angle\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.X = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.F = np.array([func(x) for x in self.X])\n        self.budget -= self.pop_size\n\n        while self.budget > 0:\n            # Find best particle\n            best_index = np.argmin(self.F)\n            best_x = self.X[best_index].copy()\n            best_f = self.F[best_index]\n\n            if best_f < self.f_opt:\n                self.f_opt = best_f\n                self.x_opt = best_x.copy()\n\n            # Spiral Movement\n            for i in range(self.pop_size):\n                r = self.a * np.exp(self.omega * np.random.randn()) # generate random number for spiral movement\n                rotation_matrix = np.array([[np.cos(self.omega), -np.sin(self.omega)], [np.sin(self.omega), np.cos(self.omega)]])\n                \n                # Update particle position\n                for d in range(self.dim):\n                    if np.random.rand() < 0.5: # apply spiral movement to only some dimensions\n                        self.X[i,d] = best_x[d] + r * (self.X[i,d] - best_x[d])\n\n                # Mutation\n                mutation_rate = 0.05\n                for d in range(self.dim):\n                    if np.random.rand() < mutation_rate:\n                        self.X[i, d] = self.X[i, d] + np.random.normal(0, 0.1)  # Add small Gaussian noise\n                        self.X[i, d] = np.clip(self.X[i, d], self.lb, self.ub)  # Clip values\n\n                self.X[i] = np.clip(self.X[i], self.lb, self.ub)\n                f_new = func(self.X[i])\n                self.budget -= 1\n\n                if f_new < self.F[i]:\n                    self.F[i] = f_new\n                \n                if self.budget <= 0:\n                    break\n\n            if self.budget <= 0:\n                break\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm SpiralDynamicOptimization scored 0.283 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6ddd41ff-8d76-4cf9-bc45-19207fbcb626"], "operator": null, "metadata": {"aucs": [0.1306788928234519, 0.19724081998293963, 0.37145559983484167, 0.17959348723999424, 0.2249400088646094, 0.21705380017071085, 0.24193715544466388, 0.3428018758949043, 0.19925490774938937, 0.18394121257324147, 0.19880859686462315, 0.9994196554793747, 0.24255459453617945, 0.205706388751843, 0.3445384476143486, 0.27031796857094914, 0.2412125347758649, 0.2286117682930774, 0.1795609420698433, 0.45490808258463855]}}
{"id": "e6e32d12-e5aa-4a0a-bfcc-a4aaa052f896", "fitness": 0.6360032271087197, "name": "AdaptiveDifferentialEvolution", "description": "Adaptive Differential Evolution with a restart mechanism based on stagnation detection and dynamic adaptation of F and Cr parameters.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_init=0.5, Cr_init=0.9, stagnation_iters=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init  # Mutation factor\n        self.Cr = Cr_init # Crossover rate\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.stagnation_iters = stagnation_iters\n        self.no_improvement_count = 0\n        self.best_fitness_history = []\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        \n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.pop[best_index].copy()\n        self.best_fitness_history.append(self.f_opt)\n\n    def mutate(self):\n        mutated_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.pop[idxs]\n            mutated_pop[i] = x_r1 + self.F * (x_r2 - x_r3)\n        return mutated_pop\n\n    def crossover(self, mutated_pop):\n        crossed_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            for j in range(self.dim):\n                if np.random.rand() < self.Cr or j == np.random.randint(0, self.dim):\n                    crossed_pop[i, j] = mutated_pop[i, j]\n                else:\n                    crossed_pop[i, j] = self.pop[i, j]\n        return crossed_pop\n\n    def selection(self, func, crossed_pop):\n        new_fitness = np.array([func(x) for x in crossed_pop])\n        self.eval_count += self.pop_size\n\n        for i in range(self.pop_size):\n            if new_fitness[i] < self.fitness[i]:\n                self.pop[i] = crossed_pop[i]\n                self.fitness[i] = new_fitness[i]\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.pop[best_index].copy()\n            self.no_improvement_count = 0 # Reset stagnation counter\n        else:\n             self.no_improvement_count +=1\n        self.best_fitness_history.append(self.f_opt)\n\n    def reinitialize_population(self, func):\n        # Re-initialize around the best solution with increased diversity\n        self.pop = np.random.normal(loc=self.x_opt, scale=1.0, size=(self.pop_size, self.dim))\n        self.pop = np.clip(self.pop, func.bounds.lb, func.bounds.ub) # Respect the bounds\n\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.pop[best_index].copy()\n        self.best_fitness_history.append(self.f_opt)\n        self.no_improvement_count = 0 # Reset stagnation counter\n\n\n    def adapt_parameters(self):\n        # Dynamically adjust F and Cr\n        self.F = np.clip(np.random.normal(0.5, 0.3), 0.1, 1.0)\n        self.Cr = np.clip(np.random.normal(0.9, 0.2), 0.1, 1.0)\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.no_improvement_count = 0\n        self.best_fitness_history = []\n        self.initialize_population(func)\n        \n        while self.eval_count < self.budget:\n            mutated_pop = self.mutate()\n            mutated_pop = np.clip(mutated_pop, func.bounds.lb, func.bounds.ub)\n            crossed_pop = self.crossover(mutated_pop)\n            crossed_pop = np.clip(crossed_pop, func.bounds.lb, func.bounds.ub)\n            self.selection(func, crossed_pop)\n\n            #Adaptive strategy: Adapt parameters and reinitialize population if stagnating\n            if self.no_improvement_count > self.stagnation_iters:\n                self.reinitialize_population(func)\n                self.adapt_parameters()\n                self.no_improvement_count = 0\n\n            #Adapt parameters every so often\n            if self.eval_count % (self.budget // 10) == 0:\n                self.adapt_parameters()\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.636 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e80347fc-2f32-4aac-9d3b-d2ac9cc311a6"], "operator": null, "metadata": {"aucs": [0.2984149519770295, 0.5938748605497537, 0.40409448809898885, 0.8369019456272431, 0.621756662216898, 0.7273472768004958, 0.6044725827241415, 0.49211271896362363, 0.6564689841774252, 0.5993817895011072, 0.8581050118715982, 0.9995091447155002, 0.43158681198133453, 0.5477194350753898, 0.8553176097008812, 0.7759452248947293, 0.545055532154007, 0.7564475592431541, 0.6102321684342519, 0.5053197834668403]}}
{"id": "6fc27899-c291-4115-b0fe-f4f6386d2658", "fitness": -Infinity, "name": "CMAES", "description": "Simplified CMA-ES with adaptive step-size and covariance matrix updates, focusing on efficient computation and exploration.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, mu_factor=0.25, cs=0.08, dsigma=0.2, c_cov_mean=0.1, c_cov_rank_one=None, c_cov_rank_mu=None):\n        self.budget = budget\n        self.dim = dim\n        self.mu = int(mu_factor * self.budget)\n        self.lamb = self.budget # Removed lambda scaling \n        self.mu = min(self.mu, self.lamb // 2)\n\n        self.cs = cs\n        self.dsigma = dsigma\n        self.c_cov_mean = c_cov_mean\n\n        self.c_cov_rank_one = c_cov_mean if c_cov_rank_one is None else c_cov_rank_one\n        self.c_cov_rank_mu = min(1 - self.c_cov_rank_one, c_cov_mean) if c_cov_rank_mu is None else c_cov_rank_mu\n\n        self.m = None # Mean\n        self.sigma = None # Step size\n        self.C = None # Covariance matrix\n        self.pc = None # Evolution path for mean\n        self.ps = None # Evolution path for step size\n        self.weights = None\n        self.B = None\n        self.D = None\n        self.eigen_updated = False\n\n    def initialize(self, func):\n        self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n        self.sigma = 0.5 * (func.bounds.ub - func.bounds.lb) # Adaptive initial sigma based on bounds\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.B = np.eye(self.dim)\n        self.D = np.ones(self.dim)\n        self.eigen_updated = False\n\n    def update_eigen(self):\n        self.C = np.triu(self.C) + np.triu(self.C, 1).T\n        self.D, self.B = np.linalg.eigh(self.C)\n        self.D = np.sqrt(self.D)\n        self.eigen_updated = True\n\n    def sample(self):\n        z = np.random.normal(0, 1, size=(self.dim, self.lamb))\n        y = self.B @ (self.D[:, None] * z)\n        x = self.m[:, None] + self.sigma * y\n        return x, y\n\n    def repair(self, x, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        x = np.clip(x, lb, ub)\n\n        return x\n\n    def __call__(self, func):\n        self.initialize(func)\n        f_opt = np.Inf\n        x_opt = None\n        evals = 0\n        \n        while evals < self.budget:\n            x, y = self.sample()\n            x = self.repair(x, func) # Repair after sampling\n\n            f = np.array([func(xi) for xi in x.T])\n            evals += self.lamb\n\n            if np.min(f) < f_opt:\n                f_opt = np.min(f)\n                x_opt = x[:, np.argmin(f)]\n\n            idx = np.argsort(f)\n            x_sorted = x[:, idx[:self.mu]]\n            y_sorted = y[:, idx[:self.mu]]\n\n            # Mean update\n            m_new = np.sum(x_sorted * self.weights[None, :], axis=1)\n\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * np.sum(self.weights)) * (self.B @ y_sorted).mean(axis=1)\n\n            self.pc = (1 - self.c_cov_mean) * self.pc + np.sqrt(self.c_cov_mean * (2 - self.c_cov_mean)) * (m_new - self.m) / self.sigma\n\n            self.m = m_new\n\n            # Covariance matrix update\n            rank_one = np.outer(self.pc, self.pc)\n            self.C = (1 - self.c_cov_rank_one - self.c_cov_rank_mu) * self.C + self.c_cov_rank_one * rank_one\n\n            for i in range(self.mu):\n                self.C += self.c_cov_rank_mu * self.weights[i] * np.outer(y_sorted[:,i], y_sorted[:,i])\n            \n            self.sigma *= np.exp((self.cs / self.dsigma) * (np.linalg.norm(self.ps) / np.sqrt(self.dim) - 1))\n            \n            if evals // self.lamb % (1 + self.dim // 10) == 0: # Delayed update\n                self.update_eigen()\n\n        return f_opt, x_opt", "configspace": "", "generation": 1, "feedback": "An exception occurred: operands could not be broadcast together with shapes (2,) (2,100) .", "error": "", "parent_ids": ["41ed32f6-8a92-4366-835c-6cac9ca3936b"], "operator": null, "metadata": {}}
{"id": "b7ce2c19-b323-453b-89be-2699eb22cb4d", "fitness": 0.0, "name": "EnhancedDifferentialEvolution", "description": "A population-based algorithm that combines differential evolution with a local search strategy based on perturbing the best solution.", "code": "import numpy as np\n\nclass EnhancedDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.7, CR=0.9, local_search_iterations=3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.local_search_iterations = local_search_iterations\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.X = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.F_vals = np.array([func(x) for x in self.X])\n        self.budget -= self.pop_size\n\n        while self.budget > 0:\n            # Find best particle\n            best_index = np.argmin(self.F_vals)\n            best_x = self.X[best_index].copy()\n            best_f = self.F_vals[best_index]\n\n            if best_f < self.f_opt:\n                self.f_opt = best_f\n                self.x_opt = best_x.copy()\n\n            # Differential Evolution\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                x_mutated = self.X[a] + self.F * (self.X[b] - self.X[c])\n                x_mutated = np.clip(x_mutated, self.lb, self.ub)\n\n                # Crossover\n                x_trial = self.X[i].copy()\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < self.F_vals[i]:\n                    self.X[i] = x_trial\n                    self.F_vals[i] = f_trial\n\n                if self.budget <= 0:\n                    break\n            \n            # Local Search around best solution\n            for _ in range(self.local_search_iterations):\n                x_perturbed = best_x + np.random.normal(0, 0.05, self.dim)  # Perturb with small Gaussian noise\n                x_perturbed = np.clip(x_perturbed, self.lb, self.ub)\n                f_perturbed = func(x_perturbed)\n                self.budget -= 1\n\n                if f_perturbed < best_f:\n                    best_x = x_perturbed\n                    best_f = f_perturbed\n                    \n                    if best_f < self.f_opt:\n                        self.f_opt = best_f\n                        self.x_opt = best_x.copy()\n\n                if self.budget <= 0:\n                    break\n\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm EnhancedDifferentialEvolution scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6ddd41ff-8d76-4cf9-bc45-19207fbcb626"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "7bd0acfa-f1c0-48c3-b34a-4f92d46f8507", "fitness": 0.0, "name": "AdaptiveDE", "description": "Simplified Adaptive Differential Evolution with self-adaptive parameters and a more robust perturbation strategy to escape local optima.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = 0.5\n        self.CR = 0.9\n        self.epsilon = 1e-6\n        self.F_memory = np.ones(10) * 0.5\n        self.CR_memory = np.ones(10) * 0.9\n        self.memory_index = 0\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        \n        if np.min(self.fitness) < self.f_opt:\n            self.f_opt = np.min(self.fitness)\n            self.x_opt = self.population[np.argmin(self.fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[idxs]\n\n                # Adaptive F and CR\n                F = self.F_memory[self.memory_index]\n                CR = self.CR_memory[self.memory_index]\n\n                mutant = x_r1 + F * (x_r2 - x_r3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n                    \n                    # Update F and CR memory\n                    self.F_memory[self.memory_index] = F\n                    self.CR_memory[self.memory_index] = CR\n                    self.memory_index = (self.memory_index + 1) % 10\n                    \n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                else:\n                    # Perturbation strategy: move towards the best\n                    self.population[i] = np.clip(self.population[i] + 0.1 * (self.x_opt - self.population[i]), func.bounds.lb, func.bounds.ub)\n                    self.fitness[i] = func(self.population[i])\n                    self.budget -= 1\n                    if self.fitness[i] < self.f_opt:\n                         self.f_opt = self.fitness[i]\n                         self.x_opt = self.population[i]\n                    \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e687e3e5-dc15-4d89-81ed-8d29003a5a2e"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "c94b68a8-4195-4590-9124-2b2d21853839", "fitness": -Infinity, "name": "HybridDE_NM", "description": "A hybrid algorithm that combines the explorative power of Differential Evolution (DE) with the intensification capabilities of a Nelder-Mead Simplex search, adaptively switching between them based on performance.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridDE_NM:\n    def __init__(self, budget=10000, dim=10, pop_size=50, de_mutation_rate=0.5, de_crossover_rate=0.7, nm_tolerance=1e-4, switch_threshold=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.de_mutation_rate = de_mutation_rate\n        self.de_crossover_rate = de_crossover_rate\n        self.nm_tolerance = nm_tolerance\n        self.switch_threshold = switch_threshold # Threshold for switching to Nelder-Mead\n        self.lb = -5.0\n        self.ub = 5.0\n        self.DE_phase = True # start with DE\n\n    def differential_evolution(self, func):\n        X = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        F = np.array([func(x) for x in X])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(F)\n        best_x = X[best_index].copy()\n        best_f = F[best_index]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                x_diff = X[b] - X[c]\n                x_mutated = X[a] + self.de_mutation_rate * x_diff\n\n                # Crossover\n                x_trial = np.copy(X[i])\n                for d in range(self.dim):\n                    if np.random.rand() < self.de_crossover_rate:\n                        x_trial[d] = x_mutated[d]\n                    \n                x_trial = np.clip(x_trial, self.lb, self.ub)\n\n                f_trial = func(x_trial)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n                \n                if f_trial < F[i]:\n                    X[i] = x_trial\n                    F[i] = f_trial\n\n                    if f_trial < best_f:\n                        best_f = f_trial\n                        best_x = x_trial.copy()\n            if self.budget <= 0:\n                break\n        \n        return best_f, best_x\n    \n    def nelder_mead(self, func, x0):\n        result = minimize(func, x0, method='Nelder-Mead', bounds=[(self.lb, self.ub)] * self.dim, options={'maxiter': self.budget, 'xatol': self.nm_tolerance, 'fatol': self.nm_tolerance})\n        self.budget -= result.nit\n        return result.fun, result.x\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n        # Initial DE phase\n        best_f, best_x = self.differential_evolution(func)\n\n        if best_f < self.f_opt:\n            self.f_opt = best_f\n            self.x_opt = best_x\n            \n        # Switch to Nelder-Mead based on performance\n        if best_f < self.switch_threshold:\n            nm_f_opt, nm_x_opt = self.nelder_mead(func, self.x_opt)\n            if nm_f_opt < self.f_opt:\n                self.f_opt = nm_f_opt\n                self.x_opt = nm_x_opt\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "An exception occurred: name 'minimize' is not defined.", "error": "", "parent_ids": ["4c327fee-4c9c-4d58-a848-858ca4be83e4"], "operator": null, "metadata": {}}
{"id": "cb9060e1-cac3-4c07-ae52-f73f57a11d3f", "fitness": -Infinity, "name": "GMM_NelderMead", "description": "A hybrid algorithm combining the exploration of a Gaussian Mixture Model with the exploitation of a Nelder-Mead simplex method, adaptively switching between them based on performance.", "code": "import numpy as np\nfrom scipy.stats import multivariate_normal\nfrom scipy.optimize import minimize\n\nclass GMM_NelderMead:\n    def __init__(self, budget=10000, dim=10, pop_size=20, gmm_iterations=5, nelder_mead_iterations=5, switch_threshold=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.gmm_iterations = gmm_iterations\n        self.nelder_mead_iterations = nelder_mead_iterations\n        self.switch_threshold = switch_threshold\n        self.lb = -5.0\n        self.ub = 5.0\n        self.exploration_phase = True\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.X = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.F = np.array([func(x) for x in self.X])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(self.F)\n        best_x = self.X[best_index].copy()\n        best_f = self.F[best_index]\n        \n        if best_f < self.f_opt:\n            self.f_opt = best_f\n            self.x_opt = best_x.copy()\n        \n        previous_best_f = np.inf\n\n        while self.budget > 0:\n            if self.exploration_phase:\n                # Gaussian Mixture Model Phase\n                if self.budget >= self.gmm_iterations * self.pop_size:\n                    try:\n                        from sklearn.mixture import GaussianMixture\n                        gmm = GaussianMixture(n_components=min(self.pop_size, 5), covariance_type='full', random_state=0)\n                        gmm.fit(self.X)\n                        \n                        new_X = gmm.sample(self.pop_size)[0]\n                        new_X = np.clip(new_X, self.lb, self.ub)\n                        new_F = np.array([func(x) for x in new_X])\n                        self.budget -= self.pop_size\n                        \n                        \n                        # Replace worst individuals\n                        worst_indices = np.argsort(self.F)[-self.pop_size//2:]\n                        self.X[worst_indices] = new_X[:self.pop_size//2]\n                        self.F[worst_indices] = new_F[:self.pop_size//2]\n                        \n                        best_index = np.argmin(self.F)\n                        best_x = self.X[best_index].copy()\n                        best_f = self.F[best_index]\n                        \n                        if best_f < self.f_opt:\n                            self.f_opt = best_f\n                            self.x_opt = best_x.copy()\n\n                    except Exception as e:\n                        # Fallback: random sampling\n                        new_X = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n                        new_F = np.array([func(x) for x in new_X])\n                        self.budget -= self.pop_size\n                        \n                        worst_indices = np.argsort(self.F)[-self.pop_size//2:]\n                        self.X[worst_indices] = new_X[:self.pop_size//2]\n                        self.F[worst_indices] = new_F[:self.pop_size//2]\n                        \n                        best_index = np.argmin(self.F)\n                        best_x = self.X[best_index].copy()\n                        best_f = self.F[best_index]\n                        \n                        if best_f < self.f_opt:\n                            self.f_opt = best_f\n                            self.x_opt = best_x.copy()\n\n                else:\n                    break  # Not enough budget for GMM iteration\n\n            else:\n                # Nelder-Mead Phase\n                if self.budget >= self.nelder_mead_iterations:\n                    \n                    res = minimize(func, self.x_opt, method='Nelder-Mead', options={'maxiter': self.nelder_mead_iterations})\n                    \n                    if res.fun < self.f_opt:\n                        self.f_opt = res.fun\n                        self.x_opt = res.x\n                    self.budget -= self.nelder_mead_iterations\n                else:\n                    break # Not enough budget for Nelder-Mead iteration\n            \n            # Switch condition\n            if abs(self.f_opt - previous_best_f) < self.switch_threshold:\n                self.exploration_phase = not self.exploration_phase  # Switch phase\n            previous_best_f = self.f_opt\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "An exception occurred: name 'minimize' is not defined.", "error": "", "parent_ids": ["4c327fee-4c9c-4d58-a848-858ca4be83e4"], "operator": null, "metadata": {}}
{"id": "f0aa9b2d-d1ee-422c-a030-dbc58f6ab068", "fitness": 0.0, "name": "AdaptiveDifferentialEvolution", "description": "Adaptive Differential Evolution with orthogonal learning and covariance matrix adaptation for improved exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_init=0.5, Cr_init=0.9, stagnation_iters=50, orthogonal_trials=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init  # Mutation factor\n        self.Cr = Cr_init # Crossover rate\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.stagnation_iters = stagnation_iters\n        self.no_improvement_count = 0\n        self.best_fitness_history = []\n        self.orthogonal_trials = orthogonal_trials\n        self.covariance_matrix = np.eye(dim)  # Initialize covariance matrix\n        self.learning_rate = 0.1  # Learning rate for covariance adaptation\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        \n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.pop[best_index].copy()\n        self.best_fitness_history.append(self.f_opt)\n\n    def mutate(self):\n        mutated_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.pop[idxs]\n            mutated_pop[i] = x_r1 + self.F * (x_r2 - x_r3)\n        return mutated_pop\n\n    def crossover(self, mutated_pop):\n        crossed_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            for j in range(self.dim):\n                if np.random.rand() < self.Cr or j == np.random.randint(0, self.dim):\n                    crossed_pop[i, j] = mutated_pop[i, j]\n                else:\n                    crossed_pop[i, j] = self.pop[i, j]\n        return crossed_pop\n\n    def selection(self, func, crossed_pop):\n        new_fitness = np.array([func(x) for x in crossed_pop])\n        self.eval_count += self.pop_size\n\n        for i in range(self.pop_size):\n            if new_fitness[i] < self.fitness[i]:\n                self.pop[i] = crossed_pop[i]\n                self.fitness[i] = new_fitness[i]\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.pop[best_index].copy()\n            self.no_improvement_count = 0 # Reset stagnation counter\n        else:\n             self.no_improvement_count +=1\n        self.best_fitness_history.append(self.f_opt)\n\n    def reinitialize_population(self, func):\n        # Re-initialize around the best solution with increased diversity\n        self.pop = np.random.normal(loc=self.x_opt, scale=1.0, size=(self.pop_size, self.dim))\n        self.pop = np.clip(self.pop, func.bounds.lb, func.bounds.ub) # Respect the bounds\n\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.pop[best_index].copy()\n        self.best_fitness_history.append(self.f_opt)\n        self.no_improvement_count = 0 # Reset stagnation counter\n        self.covariance_matrix = np.eye(self.dim) #Reset covariance matrix\n\n    def adapt_parameters(self):\n        # Dynamically adjust F and Cr\n        self.F = np.clip(np.random.normal(0.5, 0.3), 0.1, 1.0)\n        self.Cr = np.clip(np.random.normal(0.9, 0.2), 0.1, 1.0)\n\n    def orthogonal_learning(self, func):\n        # Generate orthogonal array\n        orthogonal_matrix = self.generate_orthogonal_array(self.orthogonal_trials, self.dim)\n\n        # Sample new solutions based on orthogonal array and current best\n        for i in range(self.orthogonal_trials):\n            x = self.x_opt + np.dot(orthogonal_matrix[i], np.random.normal(0, 0.1, size=self.dim))\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            f = func(x)\n            self.eval_count += 1\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n\n    def generate_orthogonal_array(self, num_trials, num_factors):\n      #A simple way to generate an orthogonal array. For better results,\n      #use a library or a more robust OA generation method\n      return np.random.rand(num_trials, num_factors) - 0.5\n\n    def adapt_covariance_matrix(self):\n      #Adapt covariance matrix based on successful steps\n      if len(self.best_fitness_history) > 1:\n          delta_x = self.x_opt - self.pop[np.argmin(self.fitness)] #Difference between best and mean solution\n          self.covariance_matrix = (1 - self.learning_rate) * self.covariance_matrix + self.learning_rate * np.outer(delta_x, delta_x)\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.no_improvement_count = 0\n        self.best_fitness_history = []\n        self.initialize_population(func)\n        \n        while self.eval_count < self.budget:\n            mutated_pop = self.mutate()\n            mutated_pop = np.clip(mutated_pop, func.bounds.lb, func.bounds.ub)\n            crossed_pop = self.crossover(mutated_pop)\n            crossed_pop = np.clip(crossed_pop, func.bounds.lb, func.bounds.ub)\n            self.selection(func, crossed_pop)\n            self.adapt_covariance_matrix()\n\n            #Adaptive strategy: Adapt parameters and reinitialize population if stagnating\n            if self.no_improvement_count > self.stagnation_iters:\n                self.reinitialize_population(func)\n                self.adapt_parameters()\n                self.no_improvement_count = 0\n\n            #Adapt parameters every so often\n            if self.eval_count % (self.budget // 10) == 0:\n                self.adapt_parameters()\n\n            # Apply orthogonal learning\n            if self.eval_count % (self.budget // 5) == 0:\n                self.orthogonal_learning(func)\n\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e6e32d12-e5aa-4a0a-bfcc-a4aaa052f896"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "b53f868f-d236-4c55-9a6c-e08011f094a2", "fitness": 0.38057046098808084, "name": "AdaptiveDifferentialEvolution", "description": "Adaptive Differential Evolution with self-adaptive parameters F and Cr using a success-history based adaptation and a more aggressive restart strategy.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_init=0.5, Cr_init=0.9, stagnation_iters=50, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init  # Mutation factor\n        self.Cr = Cr_init # Crossover rate\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.stagnation_iters = stagnation_iters\n        self.no_improvement_count = 0\n        self.best_fitness_history = []\n        self.archive_size = archive_size\n        self.archive_F = np.full(self.archive_size, F_init)\n        self.archive_Cr = np.full(self.archive_size, Cr_init)\n        self.archive_idx = 0\n\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        \n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.pop[best_index].copy()\n        self.best_fitness_history.append(self.f_opt)\n\n    def mutate(self):\n        mutated_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.pop[idxs]\n            mutated_pop[i] = x_r1 + self.F * (x_r2 - x_r3)\n        return mutated_pop\n\n    def crossover(self, mutated_pop):\n        crossed_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            for j in range(self.dim):\n                if np.random.rand() < self.Cr or j == np.random.randint(0, self.dim):\n                    crossed_pop[i, j] = mutated_pop[i, j]\n                else:\n                    crossed_pop[i, j] = self.pop[i, j]\n        return crossed_pop\n\n    def selection(self, func, crossed_pop):\n        new_fitness = np.array([func(x) for x in crossed_pop])\n        self.eval_count += self.pop_size\n\n        improved_indices = new_fitness < self.fitness\n        delta_fitness = self.fitness - new_fitness\n        \n        for i in range(self.pop_size):\n            if improved_indices[i]:\n                self.pop[i] = crossed_pop[i]\n                self.fitness[i] = new_fitness[i]\n                self.update_archive(self.F, self.Cr, delta_fitness[i])\n\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.pop[best_index].copy()\n            self.no_improvement_count = 0 # Reset stagnation counter\n        else:\n             self.no_improvement_count +=1\n        self.best_fitness_history.append(self.f_opt)\n\n    def reinitialize_population(self, func):\n        # Re-initialize around the best solution with increased diversity\n        # More aggressive re-initialization: Randomize a larger portion of the population\n        num_to_reinitialize = int(0.8 * self.pop_size)  # Reinitialize 80% of the population\n        indices_to_reinitialize = np.random.choice(self.pop_size, num_to_reinitialize, replace=False)\n        \n        self.pop[indices_to_reinitialize] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(num_to_reinitialize, self.dim))\n\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += num_to_reinitialize\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.pop[best_index].copy()\n        self.best_fitness_history.append(self.f_opt)\n        self.no_improvement_count = 0 # Reset stagnation counter\n\n\n    def adapt_parameters(self):\n        # Success-history based adaptation of F and Cr\n        self.F = np.clip(np.random.choice(self.archive_F), 0.1, 1.0)\n        self.Cr = np.clip(np.random.choice(self.archive_Cr), 0.1, 1.0)\n\n    def update_archive(self, F, Cr, delta_fitness):\n        self.archive_F[self.archive_idx] = F\n        self.archive_Cr[self.archive_idx] = Cr\n        self.archive_idx = (self.archive_idx + 1) % self.archive_size\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.no_improvement_count = 0\n        self.best_fitness_history = []\n        self.initialize_population(func)\n        \n        while self.eval_count < self.budget:\n            self.adapt_parameters()\n            mutated_pop = self.mutate()\n            mutated_pop = np.clip(mutated_pop, func.bounds.lb, func.bounds.ub)\n            crossed_pop = self.crossover(mutated_pop)\n            crossed_pop = np.clip(crossed_pop, func.bounds.lb, func.bounds.ub)\n            self.selection(func, crossed_pop)\n\n            #Adaptive strategy: Adapt parameters and reinitialize population if stagnating\n            if self.no_improvement_count > self.stagnation_iters:\n                self.reinitialize_population(func)\n                self.no_improvement_count = 0\n\n            #Adapt parameters every so often\n            if self.eval_count % (self.budget // 10) == 0:\n                pass\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.381 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e6e32d12-e5aa-4a0a-bfcc-a4aaa052f896"], "operator": null, "metadata": {"aucs": [0.27259659052070906, 0.5978763045887736, 0.6518089488428407, 0]}}
{"id": "46b10c49-b99f-4193-9e36-a01e506eb218", "fitness": 0.2876063818597486, "name": "PSO_SA", "description": "Combines PSO with Simulated Annealing for enhanced exploration and exploitation by dynamically adjusting inertia weight and introducing a temperature-based acceptance criterion.", "code": "import numpy as np\n\nclass PSO_SA:\n    def __init__(self, budget=10000, dim=10, pop_size=50, w_max=0.9, w_min=0.2, c1=2, c2=2, temp_init=100, temp_min=0.1, alpha=0.99):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w_max = w_max\n        self.w_min = w_min\n        self.c1 = c1\n        self.c2 = c2\n        self.temp = temp_init\n        self.temp_min = temp_min\n        self.alpha = alpha\n        self.lb = -5.0\n        self.ub = 5.0\n\n        self.X = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.V = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))\n        self.P = self.X.copy()\n        self.F_X = np.zeros(self.pop_size)\n        self.F_P = np.zeros(self.pop_size)\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n        # Evaluate initial population\n        for i in range(self.pop_size):\n            self.F_X[i] = func(self.X[i])\n            self.F_P[i] = self.F_X[i]\n            self.budget -= 1\n\n            if self.F_X[i] < self.f_opt:\n                self.f_opt = self.F_X[i]\n                self.x_opt = self.X[i].copy()\n\n        while self.budget > 0:\n            # Find global best particle\n            best_index = np.argmin(self.F_P)\n            best_x = self.P[best_index]\n            best_f = self.F_P[best_index]\n\n            if best_f < self.f_opt:\n                self.f_opt = best_f\n                self.x_opt = best_x.copy()\n\n            # Dynamic inertia weight\n            w = self.w_max - (self.w_max - self.w_min) * (1 - self.budget / (self.budget + self.pop_size))  # Linear decrease\n\n            # Update velocities and positions\n            for i in range(self.pop_size):\n                # PSO update\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.V[i] = w * self.V[i] + self.c1 * r1 * (self.P[i] - self.X[i]) + self.c2 * r2 * (best_x - self.X[i])\n                self.X[i] = self.X[i] + self.V[i]\n                self.X[i] = np.clip(self.X[i], self.lb, self.ub)\n\n                # Evaluate new position\n                f_new = func(self.X[i])\n                self.budget -= 1\n\n                # Simulated Annealing acceptance criterion\n                delta_f = f_new - self.F_X[i]\n                if delta_f < 0:\n                    # Accept better solution\n                    self.F_X[i] = f_new\n                    # Update personal best if necessary\n                    if f_new < self.F_P[i]:\n                        self.P[i] = self.X[i].copy()\n                        self.F_P[i] = f_new\n                else:\n                    # Accept worse solution with probability exp(-delta_f / temp)\n                    if np.random.rand() < np.exp(-delta_f / self.temp):\n                        self.F_X[i] = f_new\n                    else:\n                         f_original = func(self.X[i])\n                         self.budget -=1\n                         if f_original < self.F_P[i]:\n                             self.F_P[i] = f_original\n                             self.P[i] = self.X[i].copy()\n\n                if self.budget <= 0:\n                    break\n\n            # Cool down the temperature\n            self.temp *= self.alpha\n            self.temp = max(self.temp, self.temp_min)\n\n            if self.budget <= 0:\n                break\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm PSO_SA scored 0.288 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6e0bd570-87f9-4839-bc12-07022df8c42c"], "operator": null, "metadata": {"aucs": [0.14286682866665545, 0.20309521312417156, 0.32397166538366506, 0.3597944623370689, 0.2526988314552665, 0.31995612093754744, 0.256374329750906, 0.26234664470601965, 0.1993429718475761, 0.19436503569223418, 0.22929277368056233, 0.9947780865950587, 0]}}
{"id": "8e8039fa-e049-4051-b73b-0c67d0bc298b", "fitness": 0.28513409673418066, "name": "SimplifiedSpiralDynamicOptimization", "description": "Simplified Spiral Dynamic Optimization with adaptive exploration and exploitation balance based on stagnation detection.", "code": "import numpy as np\n\nclass SimplifiedSpiralDynamicOptimization:\n    def __init__(self, budget=10000, dim=10, pop_size=50, a=0.9, omega=0.1, stagnation_threshold=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.a = a  # Contraction factor\n        self.omega = omega  # Spiral angle\n        self.lb = -5.0\n        self.ub = 5.0\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.previous_best_f = np.Inf\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.X = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.F = np.array([func(x) for x in self.X])\n        self.budget -= self.pop_size\n\n        while self.budget > 0:\n            # Find best particle\n            best_index = np.argmin(self.F)\n            best_x = self.X[best_index].copy()\n            best_f = self.F[best_index]\n\n            if best_f < self.f_opt:\n                self.f_opt = best_f\n                self.x_opt = best_x.copy()\n                self.stagnation_counter = 0 # Reset stagnation counter\n            else:\n                self.stagnation_counter +=1\n            \n            #Adaptive exploration\n            exploration_rate = min(1.0, self.stagnation_counter / self.stagnation_threshold)\n\n            # Spiral Movement\n            for i in range(self.pop_size):\n                r = self.a * np.exp(self.omega * np.random.randn())\n                \n                # Update particle position\n                for d in range(self.dim):\n                    if np.random.rand() < (0.5 + exploration_rate*0.5): #Increased exploration rate when stagnating\n                        self.X[i,d] = best_x[d] + r * (self.X[i,d] - best_x[d])\n                    else:\n                         self.X[i, d] = self.lb + np.random.rand() * (self.ub - self.lb) #Randomize to improve exploration\n                \n                self.X[i] = np.clip(self.X[i], self.lb, self.ub)\n                f_new = func(self.X[i])\n                self.budget -= 1\n\n                if f_new < self.F[i]:\n                    self.F[i] = f_new\n                \n                if self.budget <= 0:\n                    break\n\n            if self.budget <= 0:\n                break\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm SimplifiedSpiralDynamicOptimization scored 0.285 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["4c327fee-4c9c-4d58-a848-858ca4be83e4"], "operator": null, "metadata": {"aucs": [0.10287129888291924, 0.18287399333041043, 0.26776615404651694, 0.20894228007978077, 0.19752046580262506, 0.2378087273359919, 0.24602914761047756, 0.21074435541118408, 0.2021510999205458, 0.15338131873414018, 0.23226217141307814, 0.9991745496530601, 0.2740476047237699, 0.20918571319492107, 0.5380044695217117, 0.2861193756076843, 0.2231867239462394, 0.31462713773166884, 0.16363604724272396, 0.45234930049416466]}}
{"id": "e4c3d8f1-1ff1-4379-b219-45b57752f9e2", "fitness": 0.2565485594145621, "name": "SpiralDynamicOptimization", "description": "Simplified Spiral Dynamic Optimization with adaptive contraction factor and dynamic mutation rate based on population diversity.", "code": "import numpy as np\n\nclass SpiralDynamicOptimization:\n    def __init__(self, budget=10000, dim=10, pop_size=50, a_initial=0.9, omega=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.a_initial = a_initial  # Initial contraction factor\n        self.omega = omega  # Spiral angle\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.X = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.F = np.array([func(x) for x in self.X])\n        self.budget -= self.pop_size\n\n        while self.budget > 0:\n            # Find best particle\n            best_index = np.argmin(self.F)\n            best_x = self.X[best_index].copy()\n            best_f = self.F[best_index]\n\n            if best_f < self.f_opt:\n                self.f_opt = best_f\n                self.x_opt = best_x.copy()\n\n            # Calculate population diversity\n            diversity = np.std(self.X)\n\n            # Adaptive contraction factor\n            a = self.a_initial * (1 - diversity)\n\n            # Dynamic mutation rate\n            mutation_rate = 0.05 + 0.2 * diversity\n\n            # Spiral Movement\n            for i in range(self.pop_size):\n                r = a * np.exp(self.omega * np.random.randn())\n                \n                # Update particle position\n                self.X[i] = best_x + r * (self.X[i] - best_x)\n\n\n                # Mutation\n                for d in range(self.dim):\n                    if np.random.rand() < mutation_rate:\n                        self.X[i, d] = self.X[i, d] + np.random.normal(0, 0.1)  # Add small Gaussian noise\n                        \n\n                self.X[i] = np.clip(self.X[i], self.lb, self.ub)\n                f_new = func(self.X[i])\n                self.budget -= 1\n\n                if f_new < self.F[i]:\n                    self.F[i] = f_new\n                \n                if self.budget <= 0:\n                    break\n\n            if self.budget <= 0:\n                break\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm SpiralDynamicOptimization scored 0.257 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["4c327fee-4c9c-4d58-a848-858ca4be83e4"], "operator": null, "metadata": {"aucs": [0.06334345407973219, 0.1508741871151449, 0.26802595418622155, 0.17106766406718776, 0.15595554066721107, 0.2253934577487242, 0.22108094088617736, 0.16070790531885837, 0.1852236451939162, 0.15871071653669044, 0.20692709012142274, 0.9986801579836819, 0.2035822606122316, 0.1647406288437545, 0.5618453531102408, 0.26012012159245057, 0.17855772722627405, 0.21463796834110593, 0.15323564689974623, 0.4282607677604693]}}
{"id": "5c77fb83-9173-44ec-9314-0281e74e4d22", "fitness": 0.5805426126775708, "name": "AdaptiveDifferentialEvolution", "description": "Simplified Adaptive Differential Evolution with adaptive F and Cr, and periodic re-initialization around the best solution to escape local optima.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_init=0.5, Cr_init=0.9, stagnation_iters=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.Cr = Cr_init\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.stagnation_iters = stagnation_iters\n        self.no_improvement_count = 0\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.update_best(self.pop, self.fitness)\n\n\n    def mutate(self):\n        mutated_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.pop[idxs]\n            mutated_pop[i] = x_r1 + self.F * (x_r2 - x_r3)\n        return mutated_pop\n\n    def crossover(self, mutated_pop):\n        crossed_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            for j in range(self.dim):\n                if np.random.rand() < self.Cr or j == np.random.randint(0, self.dim):\n                    crossed_pop[i, j] = mutated_pop[i, j]\n                else:\n                    crossed_pop[i, j] = self.pop[i, j]\n        return crossed_pop\n\n    def selection(self, func, crossed_pop):\n        new_fitness = np.array([func(x) for x in crossed_pop])\n        self.eval_count += self.pop_size\n\n        improved = new_fitness < self.fitness\n        self.pop[improved] = crossed_pop[improved]\n        self.fitness[improved] = new_fitness[improved]\n\n        self.update_best(self.pop, self.fitness)\n\n    def reinitialize_population(self, func):\n        self.pop = np.random.normal(loc=self.x_opt, scale=0.5, size=(self.pop_size, self.dim))\n        self.pop = np.clip(self.pop, func.bounds.lb, func.bounds.ub)\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.update_best(self.pop, self.fitness)\n\n    def adapt_parameters(self):\n        self.F = np.clip(np.random.normal(0.5, 0.3), 0.1, 1.0)\n        self.Cr = np.clip(np.random.normal(0.9, 0.2), 0.1, 1.0)\n\n    def update_best(self, pop, fitness):\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = pop[best_index].copy()\n            self.no_improvement_count = 0\n        else:\n            self.no_improvement_count += 1\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.no_improvement_count = 0\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            mutated_pop = self.mutate()\n            mutated_pop = np.clip(mutated_pop, func.bounds.lb, func.bounds.ub)\n            crossed_pop = self.crossover(mutated_pop)\n            crossed_pop = np.clip(crossed_pop, func.bounds.lb, func.bounds.ub)\n            self.selection(func, crossed_pop)\n\n            if self.no_improvement_count > self.stagnation_iters:\n                self.reinitialize_population(func)\n                self.adapt_parameters()\n                self.no_improvement_count = 0\n            elif self.eval_count % (self.budget // 10) == 0:\n                self.adapt_parameters()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.581 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e6e32d12-e5aa-4a0a-bfcc-a4aaa052f896"], "operator": null, "metadata": {"aucs": [0.2506474517348405, 0.6345785201033116, 0.6003734257471576, 0.8556478360104214, 0.6531392467044548, 0.4281969136400553, 0.4648393808140775, 0.2891361171077459, 0.6182877238249467, 0.2566884379312705, 0.8563173568825821, 0.9976302798850168, 0.6292315538980284, 0.5706960850180045, 0.8961340430918903, 0.5660387298255432, 0.477994301691372, 0.7812001747414195, 0.29470164356167894, 0.4893730313375987]}}
{"id": "3775ebe3-c02c-4642-b387-0b397c813d24", "fitness": 0.664043473295264, "name": "AdaptiveDE", "description": "Simplified Adaptive Differential Evolution with improved parameter adaptation and a more efficient update strategy using archive.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = 0.5\n        self.CR = 0.9\n        self.archive = []\n        self.memory_F = np.ones(5) * 0.5\n        self.memory_CR = np.ones(5) * 0.9\n        self.memory_idx = 0\n        self.epsilon = 1e-6\n        self.success_count = 0\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        \n        if np.min(self.fitness) < self.f_opt:\n            self.f_opt = np.min(self.fitness)\n            self.x_opt = self.population[np.argmin(self.fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size + len(self.archive), 3, replace=False)\n                \n                if idxs[0] < self.pop_size:\n                    x_r1 = self.population[idxs[0]]\n                else:\n                    x_r1 = self.archive[idxs[0] - self.pop_size]\n                    \n                if idxs[1] < self.pop_size:\n                    x_r2 = self.population[idxs[1]]\n                else:\n                    x_r2 = self.archive[idxs[1] - self.pop_size]\n                    \n                if idxs[2] < self.pop_size:\n                    x_r3 = self.population[idxs[2]]\n                else:\n                    x_r3 = self.archive[idxs[2] - self.pop_size]\n\n                # Adaptive F and CR\n                F = self.memory_F[self.memory_idx]\n                CR = self.memory_CR[self.memory_idx]\n                \n                mutant = x_r1 + F * (x_r2 - x_r3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    \n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        idx_replace = np.random.randint(self.archive_size)\n                        self.archive[idx_replace] = self.population[i].copy()\n                    \n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    \n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.success_count +=1\n                        \n                #Perturbation removed for simplicity.\n\n            # Update memory\n            if self.success_count > 0:\n                self.memory_F[self.memory_idx] = np.mean(np.clip(np.random.normal(0.5, 0.3, self.success_count), 0.1, 0.9))\n                self.memory_CR[self.memory_idx] = np.mean(np.random.uniform(0, 1, self.success_count))\n            else:\n                self.memory_F[self.memory_idx] = 0.5\n                self.memory_CR[self.memory_idx] = 0.9\n\n            self.memory_idx = (self.memory_idx + 1) % 5\n            self.success_count = 0\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDE scored 0.664 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e687e3e5-dc15-4d89-81ed-8d29003a5a2e"], "operator": null, "metadata": {"aucs": [0.2585805178026981, 0.5937372827422864, 0.6217492499516633, 0.8492770146291726, 0.7180153878529592, 0.7511516903138941, 0.5481962671410239, 0.6075463423628655, 0.6914194176215894, 0.6594287293193068, 0.8226033682356007, 0.9851134082790086, 0.5264839828662081, 0.6728212949401298, 0.9215376892121013, 0.7661847021718332, 0.5822670735442259, 0.8331681660005453, 0.3054994418636716, 0.5660884390544951]}}
{"id": "eb19f3ea-79cb-4f82-bf6c-0f9859f10c7c", "fitness": 0.0, "name": "AdaptiveDifferentialEvolution", "description": "Adaptive Differential Evolution with a mirrored population and a more aggressive parameter adaptation strategy, re-initializing with a blend of uniform and normal distributions.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_init=0.5, Cr_init=0.9, stagnation_iters=50, mirror_prob=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init  # Mutation factor\n        self.Cr = Cr_init # Crossover rate\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.stagnation_iters = stagnation_iters\n        self.no_improvement_count = 0\n        self.best_fitness_history = []\n        self.mirror_prob = mirror_prob\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        \n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.pop[best_index].copy()\n        self.best_fitness_history.append(self.f_opt)\n\n    def mutate(self):\n        mutated_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.pop[idxs]\n            mutated_pop[i] = x_r1 + self.F * (x_r2 - x_r3)\n        return mutated_pop\n\n    def crossover(self, mutated_pop):\n        crossed_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            for j in range(self.dim):\n                if np.random.rand() < self.Cr or j == np.random.randint(0, self.dim):\n                    crossed_pop[i, j] = mutated_pop[i, j]\n                else:\n                    crossed_pop[i, j] = self.pop[i, j]\n        return crossed_pop\n\n    def selection(self, func, crossed_pop):\n        new_fitness = np.array([func(x) for x in crossed_pop])\n        self.eval_count += self.pop_size\n\n        # Mirrored population update: replace the worst with a mirrored version of the best sometimes.\n        if np.random.rand() < self.mirror_prob:\n            worst_index = np.argmax(self.fitness)\n            mirrored_x = self.x_opt + np.random.normal(0, 0.1, self.dim)  # Add some noise\n            mirrored_x = np.clip(mirrored_x, func.bounds.lb, func.bounds.ub)\n            mirrored_fitness = func(mirrored_x)\n            self.eval_count += 1  # Count the extra evaluation\n\n            if mirrored_fitness < self.fitness[worst_index]:\n                self.pop[worst_index] = mirrored_x\n                self.fitness[worst_index] = mirrored_fitness\n\n        for i in range(self.pop_size):\n            if new_fitness[i] < self.fitness[i]:\n                self.pop[i] = crossed_pop[i]\n                self.fitness[i] = new_fitness[i]\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.pop[best_index].copy()\n            self.no_improvement_count = 0 # Reset stagnation counter\n        else:\n             self.no_improvement_count +=1\n        self.best_fitness_history.append(self.f_opt)\n\n    def reinitialize_population(self, func):\n        # Re-initialize with a blend of uniform and normal distributions\n        uniform_pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size // 2, self.dim))\n        normal_pop = np.random.normal(loc=self.x_opt, scale=0.5, size=(self.pop_size - self.pop_size // 2, self.dim))\n        self.pop = np.vstack((uniform_pop, normal_pop))\n        self.pop = np.clip(self.pop, func.bounds.lb, func.bounds.ub) # Respect the bounds\n\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.pop[best_index].copy()\n        self.best_fitness_history.append(self.f_opt)\n        self.no_improvement_count = 0 # Reset stagnation counter\n\n\n    def adapt_parameters(self):\n        # Aggressively adjust F and Cr based on stagnation\n        if self.no_improvement_count > self.stagnation_iters // 2:\n             self.F = np.clip(np.random.normal(0.7, 0.5), 0.1, 1.0)  # Higher F for exploration\n             self.Cr = np.clip(np.random.normal(0.3, 0.2), 0.1, 1.0)  # Lower Cr for focused search\n        else:\n            self.F = np.clip(np.random.normal(0.5, 0.3), 0.1, 1.0)\n            self.Cr = np.clip(np.random.normal(0.9, 0.2), 0.1, 1.0)\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.no_improvement_count = 0\n        self.best_fitness_history = []\n        self.initialize_population(func)\n        \n        while self.eval_count < self.budget:\n            mutated_pop = self.mutate()\n            mutated_pop = np.clip(mutated_pop, func.bounds.lb, func.bounds.ub)\n            crossed_pop = self.crossover(mutated_pop)\n            crossed_pop = np.clip(crossed_pop, func.bounds.lb, func.bounds.ub)\n            self.selection(func, crossed_pop)\n\n            #Adaptive strategy: Adapt parameters and reinitialize population if stagnating\n            if self.no_improvement_count > self.stagnation_iters:\n                self.reinitialize_population(func)\n                self.adapt_parameters()\n                self.no_improvement_count = 0\n\n            #Adapt parameters every so often\n            if self.eval_count % (self.budget // 10) == 0:\n                self.adapt_parameters()\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e6e32d12-e5aa-4a0a-bfcc-a4aaa052f896"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "1061d067-d04a-4835-8e33-5e783756e5ef", "fitness": 0.6594804887961317, "name": "AdaptiveDifferentialEvolution", "description": "Simplified Adaptive Differential Evolution with a self-adaptive mutation factor and a focused restart mechanism.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=50, Cr=0.9, stagnation_iters=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.Cr = Cr  # Crossover rate\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.stagnation_iters = stagnation_iters\n        self.no_improvement_count = 0\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        \n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.pop[best_index].copy()\n\n    def mutate(self, F):\n        mutated_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.pop[idxs]\n            mutated_pop[i] = x_r1 + F * (x_r2 - x_r3)\n        return mutated_pop\n\n    def crossover(self, mutated_pop):\n        crossed_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            for j in range(self.dim):\n                if np.random.rand() < self.Cr or j == np.random.randint(0, self.dim):\n                    crossed_pop[i, j] = mutated_pop[i, j]\n                else:\n                    crossed_pop[i, j] = self.pop[i, j]\n        return crossed_pop\n\n    def selection(self, func, crossed_pop):\n        new_fitness = np.array([func(x) for x in crossed_pop])\n        self.eval_count += self.pop_size\n\n        improved = False\n        for i in range(self.pop_size):\n            if new_fitness[i] < self.fitness[i]:\n                self.pop[i] = crossed_pop[i]\n                self.fitness[i] = new_fitness[i]\n                improved = True\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.pop[best_index].copy()\n            self.no_improvement_count = 0\n        else:\n            self.no_improvement_count += 1\n\n    def reinitialize_population(self, func):\n        # Re-initialize around the best solution with reduced scale\n        self.pop = np.random.normal(loc=self.x_opt, scale=0.5, size=(self.pop_size, self.dim))\n        self.pop = np.clip(self.pop, func.bounds.lb, func.bounds.ub)\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.pop[best_index].copy()\n        self.no_improvement_count = 0\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.no_improvement_count = 0\n        self.initialize_population(func)\n        \n        while self.eval_count < self.budget:\n            # Self-adaptive F\n            F = np.random.normal(0.5, 0.3)\n            F = np.clip(F, 0.1, 1.0)\n\n            mutated_pop = self.mutate(F)\n            mutated_pop = np.clip(mutated_pop, func.bounds.lb, func.bounds.ub)\n            crossed_pop = self.crossover(mutated_pop)\n            crossed_pop = np.clip(crossed_pop, func.bounds.lb, func.bounds.ub)\n            self.selection(func, crossed_pop)\n\n            if self.no_improvement_count > self.stagnation_iters:\n                self.reinitialize_population(func)", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.659 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e6e32d12-e5aa-4a0a-bfcc-a4aaa052f896"], "operator": null, "metadata": {"aucs": [0.3256868272858463, 0.6625854262058166, 0.5473203468699379, 0.8555401261795947, 0.684788125552904, 0.7071201058523001, 0.5805093366676648, 0.6158381520332705, 0.6534158572832246, 0.6530403256121237, 0.8288286441159981, 0.9844886276176579, 0.5670105737524949, 0.6836839820099383, 0.9005453313398543, 0.7627817463325011, 0.5471979857313662, 0.8030303132727777, 0.31907051333881187, 0.5071274288685517]}}
{"id": "9ad2c7fa-2691-42a1-9ced-009fef39a385", "fitness": 0.0, "name": "AdaptiveDE", "description": "Simplified Adaptive Differential Evolution with success-history adaptation and a more efficient archive update, focusing on parameter tuning.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=5, memory_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.memory_size = memory_size\n        self.memory_F = np.ones(memory_size) * 0.5\n        self.memory_CR = np.ones(memory_size) * 0.9\n        self.memory_idx = 0\n        self.epsilon = 1e-6\n        self.success_count = 0\n        self.archive = []\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        \n        if np.min(self.fitness) < self.f_opt:\n            self.f_opt = np.min(self.fitness)\n            self.x_opt = self.population[np.argmin(self.fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size + len(self.archive), 3, replace=False)\n                x_r1 = self.population[idxs[0] % self.pop_size] if idxs[0] < self.pop_size else self.archive[idxs[0] - self.pop_size]\n                x_r2 = self.population[idxs[1] % self.pop_size] if idxs[1] < self.pop_size else self.archive[idxs[1] - self.pop_size]\n                x_r3 = self.population[idxs[2] % self.pop_size] if idxs[2] < self.pop_size else self.archive[idxs[2] - self.pop_size]\n\n                # Adaptive F and CR\n                F = self.memory_F[self.memory_idx]\n                CR = self.memory_CR[self.memory_idx]\n                \n                mutant = x_r1 + F * (x_r2 - x_r3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    # Archive update: replace worst individual\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        fitnesses = [func(x) for x in self.archive] #Recalculate to be sure.\n                        idx_replace = np.argmax(fitnesses)\n                        self.archive[idx_replace] = self.population[i].copy()\n                    \n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    \n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.success_count +=1\n\n            # Update memory - Success History Adaptation\n            if self.success_count > 0:\n                successful_F = np.clip(np.random.normal(0.5, 0.3, self.success_count), 0.1, 0.9)\n                successful_CR = np.random.uniform(0, 1, self.success_count)\n\n                self.memory_F[self.memory_idx] = np.mean(successful_F)\n                self.memory_CR[self.memory_idx] = np.mean(successful_CR)\n            \n\n            self.memory_idx = (self.memory_idx + 1) % self.memory_size\n            self.success_count = 0\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3775ebe3-c02c-4642-b387-0b397c813d24"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "99af36ba-5d40-4899-8d30-ae9830d43686", "fitness": 0.0, "name": "AdaptiveDE", "description": "Improved Adaptive Differential Evolution with enhanced parameter adaptation, orthogonal crossover, and a more aggressive archive update strategy.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = 0.5\n        self.CR = 0.9\n        self.archive = []\n        self.memory_F = np.ones(5) * 0.5\n        self.memory_CR = np.ones(5) * 0.9\n        self.memory_idx = 0\n        self.epsilon = 1e-6\n        self.success_count = 0\n        self.sf_vals = []\n        self.scr_vals = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        \n        if np.min(self.fitness) < self.f_opt:\n            self.f_opt = np.min(self.fitness)\n            self.x_opt = self.population[np.argmin(self.fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size + len(self.archive), 3, replace=False)\n                \n                if idxs[0] < self.pop_size:\n                    x_r1 = self.population[idxs[0]]\n                else:\n                    x_r1 = self.archive[idxs[0] - self.pop_size]\n                    \n                if idxs[1] < self.pop_size:\n                    x_r2 = self.population[idxs[1]]\n                else:\n                    x_r2 = self.archive[idxs[1] - self.pop_size]\n                    \n                if idxs[2] < self.pop_size:\n                    x_r3 = self.population[idxs[2]]\n                else:\n                    x_r3 = self.archive[idxs[2] - self.pop_size]\n\n                # Adaptive F and CR\n                F = self.memory_F[self.memory_idx]\n                CR = self.memory_CR[self.memory_idx]\n                \n                mutant = x_r1 + F * (x_r2 - x_r3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Orthogonal Crossover\n                trial = np.copy(self.population[i])\n                \n                num_changed_params = 0  # Track how many params are changed\n                for j in range(self.dim):\n                    if np.random.rand() < CR:\n                        trial[j] = mutant[j]\n                        num_changed_params += 1\n\n                # Ensure at least one parameter is changed, or choose a random one\n                if num_changed_params == 0:\n                    j_rand = np.random.randint(self.dim)\n                    trial[j_rand] = mutant[j_rand]\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    self.sf_vals.append(F)\n                    self.scr_vals.append(CR)\n                    # Aggressive archive update\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        # Replace worst individual in archive\n                        fitness_archive = [func(x) for x in self.archive]\n                        idx_replace = np.argmax(fitness_archive)\n                        self.archive[idx_replace] = self.population[i].copy()\n                    \n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    \n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.success_count +=1\n                        \n                #Perturbation removed for simplicity.\n\n            # Update memory\n            if self.sf_vals and self.scr_vals:\n                self.memory_F[self.memory_idx] = np.mean(np.clip(self.sf_vals, 0.1, 0.9))\n                self.memory_CR[self.memory_idx] = np.mean(self.scr_vals)\n                self.sf_vals = []  # Reset lists\n                self.scr_vals = []\n            else:\n                self.memory_F[self.memory_idx] = 0.5\n                self.memory_CR[self.memory_idx] = 0.9\n\n            self.memory_idx = (self.memory_idx + 1) % 5\n            self.success_count = 0\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3775ebe3-c02c-4642-b387-0b397c813d24"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "2a2f78d0-3e41-410b-a348-16b5d85d167d", "fitness": 0.0, "name": "AdaptiveDE", "description": "Enhanced Adaptive Differential Evolution with improved memory update using successful F/CR values and a jitter-based perturbation for stagnation avoidance.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=5, memory_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.memory_size = memory_size\n        self.F = 0.5\n        self.CR = 0.9\n        self.archive = []\n        self.memory_F = np.ones(memory_size) * 0.5\n        self.memory_CR = np.ones(memory_size) * 0.9\n        self.memory_idx = 0\n        self.epsilon = 1e-6\n        self.success_count = 0\n        self.successful_F = []\n        self.successful_CR = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        \n        if np.min(self.fitness) < self.f_opt:\n            self.f_opt = np.min(self.fitness)\n            self.x_opt = self.population[np.argmin(self.fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size + len(self.archive), 3, replace=False)\n                \n                if idxs[0] < self.pop_size:\n                    x_r1 = self.population[idxs[0]]\n                else:\n                    x_r1 = self.archive[idxs[0] - self.pop_size]\n                    \n                if idxs[1] < self.pop_size:\n                    x_r2 = self.population[idxs[1]]\n                else:\n                    x_r2 = self.archive[idxs[1] - self.pop_size]\n                    \n                if idxs[2] < self.pop_size:\n                    x_r3 = self.population[idxs[2]]\n                else:\n                    x_r3 = self.archive[idxs[2] - self.pop_size]\n\n                # Adaptive F and CR\n                F = self.memory_F[self.memory_idx]\n                CR = self.memory_CR[self.memory_idx]\n                \n                mutant = x_r1 + F * (x_r2 - x_r3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    \n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        idx_replace = np.random.randint(self.archive_size)\n                        self.archive[idx_replace] = self.population[i].copy()\n                    \n                    self.successful_F.append(F)\n                    self.successful_CR.append(CR)\n                    \n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    \n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        \n                    self.success_count += 1\n                else:\n                    # Jitter-based perturbation for stagnation avoidance\n                    if np.random.rand() < 0.05:\n                        trial = self.population[i] + np.random.normal(0, 0.01, self.dim)\n                        trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n                        f_trial = func(trial)\n                        self.budget -= 1\n                        if f_trial < self.fitness[i]:\n                            self.population[i] = trial\n                            self.fitness[i] = f_trial\n                            if f_trial < self.f_opt:\n                                self.f_opt = f_trial\n                                self.x_opt = trial\n\n            # Update memory\n            if self.successful_F:\n                self.memory_F[self.memory_idx] = np.mean(np.clip(self.successful_F, 0.1, 0.9))\n                self.memory_CR[self.memory_idx] = np.mean(np.clip(self.successful_CR, 0, 1))\n            else:\n                self.memory_F[self.memory_idx] = 0.5\n                self.memory_CR[self.memory_idx] = 0.9\n\n            self.memory_idx = (self.memory_idx + 1) % self.memory_size\n            self.successful_F = []\n            self.successful_CR = []\n            self.success_count = 0\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3775ebe3-c02c-4642-b387-0b397c813d24"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "90e593e5-4239-4e5d-8c99-7a1a242d18e4", "fitness": 0.0, "name": "AdaptiveDEGradient", "description": "A differential evolution strategy with self-adaptive population size and a local search refinement step based on the gradient information estimated using finite differences.", "code": "import numpy as np\n\nclass AdaptiveDEGradient:\n    def __init__(self, budget=10000, dim=10, pop_size_init=50, archive_size=5, local_search_freq=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size_init\n        self.archive_size = archive_size\n        self.F = 0.5\n        self.CR = 0.9\n        self.archive = []\n        self.memory_F = np.ones(5) * 0.5\n        self.memory_CR = np.ones(5) * 0.9\n        self.memory_idx = 0\n        self.epsilon = 1e-6\n        self.success_count = 0\n        self.local_search_freq = local_search_freq\n        self.pop_size_history = [pop_size_init]\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        \n        if np.min(self.fitness) < self.f_opt:\n            self.f_opt = np.min(self.fitness)\n            self.x_opt = self.population[np.argmin(self.fitness)]\n\n        generation = 0\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size + len(self.archive), 3, replace=False)\n                \n                if idxs[0] < self.pop_size:\n                    x_r1 = self.population[idxs[0]]\n                else:\n                    x_r1 = self.archive[idxs[0] - self.pop_size]\n                    \n                if idxs[1] < self.pop_size:\n                    x_r2 = self.population[idxs[1]]\n                else:\n                    x_r2 = self.archive[idxs[1] - self.pop_size]\n                    \n                if idxs[2] < self.pop_size:\n                    x_r3 = self.population[idxs[2]]\n                else:\n                    x_r3 = self.archive[idxs[2] - self.pop_size]\n\n                # Adaptive F and CR\n                F = self.memory_F[self.memory_idx]\n                CR = self.memory_CR[self.memory_idx]\n                \n                mutant = x_r1 + F * (x_r2 - x_r3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    \n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        idx_replace = np.random.randint(self.archive_size)\n                        self.archive[idx_replace] = self.population[i].copy()\n                    \n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    \n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.success_count +=1\n            \n            # Local Search Refinement\n            if generation % self.local_search_freq == 0:\n                best_idx = np.argmin(self.fitness)\n                x_best = self.population[best_idx].copy()\n                f_best = self.fitness[best_idx]\n                \n                # Estimate Gradient using finite differences\n                gradient = np.zeros(self.dim)\n                h = 1e-4  # Step size for finite differences\n                for j in range(self.dim):\n                    x_plus_h = x_best.copy()\n                    x_plus_h[j] += h\n                    x_plus_h = np.clip(x_plus_h, func.bounds.lb, func.bounds.ub)\n                    f_plus_h = func(x_plus_h)\n                    self.budget -= 1\n                    \n                    x_minus_h = x_best.copy()\n                    x_minus_h[j] -= h\n                    x_minus_h = np.clip(x_minus_h, func.bounds.lb, func.bounds.ub)\n                    f_minus_h = func(x_minus_h)\n                    self.budget -= 1\n                    \n                    gradient[j] = (f_plus_h - f_minus_h) / (2 * h)\n\n                # Update best solution based on gradient (simple gradient descent)\n                learning_rate = 0.01\n                x_new = x_best - learning_rate * gradient\n                x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n                f_new = func(x_new)\n                self.budget -= 1\n\n                if f_new < f_best:\n                    self.population[best_idx] = x_new\n                    self.fitness[best_idx] = f_new\n                    if f_new < self.f_opt:\n                        self.f_opt = f_new\n                        self.x_opt = x_new\n\n\n            # Adaptive population size (simplified)\n            if self.success_count > self.pop_size * 0.2:\n                self.pop_size = min(self.pop_size + 10, 100)\n            elif self.success_count < self.pop_size * 0.05:\n                self.pop_size = max(self.pop_size - 10, 20)\n            self.pop_size_history.append(self.pop_size)\n            \n            # Update memory\n            if self.success_count > 0:\n                self.memory_F[self.memory_idx] = np.mean(np.clip(np.random.normal(0.5, 0.3, self.success_count), 0.1, 0.9))\n                self.memory_CR[self.memory_idx] = np.mean(np.random.uniform(0, 1, self.success_count))\n            else:\n                self.memory_F[self.memory_idx] = 0.5\n                self.memory_CR[self.memory_idx] = 0.9\n\n            self.memory_idx = (self.memory_idx + 1) % 5\n            self.success_count = 0\n            generation += 1\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDEGradient scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3775ebe3-c02c-4642-b387-0b397c813d24"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "54c78863-5062-4f33-b07c-898fc3d36e54", "fitness": 0.0, "name": "AdaptiveDifferentialEvolution", "description": "Improved Adaptive Differential Evolution with success-history based parameter adaptation, orthogonal learning to enhance population diversity, and a local search operator around the best solution to refine the optimum.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_init=0.5, Cr_init=0.9, stagnation_iters=50, archive_size=10, local_search_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.Cr = Cr_init\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.stagnation_iters = stagnation_iters\n        self.no_improvement_count = 0\n        self.archive_size = archive_size\n        self.archive = []\n        self.archive_fitness = []\n        self.memory_F = np.ones(archive_size) * 0.5\n        self.memory_Cr = np.ones(archive_size) * 0.9\n        self.memory_pos = 0\n        self.local_search_prob = local_search_prob\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.update_best(self.pop, self.fitness)\n\n    def mutate(self):\n        mutated_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.pop[idxs]\n            mutated_pop[i] = x_r1 + self.F * (x_r2 - x_r3)\n        return mutated_pop\n\n    def crossover(self, mutated_pop):\n        crossed_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            for j in range(self.dim):\n                if np.random.rand() < self.Cr or j == np.random.randint(0, self.dim):\n                    crossed_pop[i, j] = mutated_pop[i, j]\n                else:\n                    crossed_pop[i, j] = self.pop[i, j]\n        return crossed_pop\n\n    def selection(self, func, crossed_pop):\n        new_fitness = np.array([func(x) for x in crossed_pop])\n        self.eval_count += self.pop_size\n\n        improved = new_fitness < self.fitness\n        for i in range(self.pop_size):\n            if improved[i]:\n                self.archive.append(self.pop[i].copy())\n                self.archive_fitness.append(self.fitness[i])\n                if len(self.archive) > self.archive_size:\n                    self.archive.pop(0)\n                    self.archive_fitness.pop(0)\n                self.pop[i] = crossed_pop[i]\n                self.fitness[i] = new_fitness[i]\n\n        self.update_best(self.pop, self.fitness)\n\n    def reinitialize_population(self, func):\n        self.pop = np.random.normal(loc=self.x_opt, scale=0.5, size=(self.pop_size, self.dim))\n        self.pop = np.clip(self.pop, func.bounds.lb, func.bounds.ub)\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.update_best(self.pop, self.fitness)\n\n    def adapt_parameters(self):\n        SF = []\n        SCr = []\n        for i in range(self.pop_size):\n            if np.random.rand() < 0.9:\n                self.Cr = np.random.choice(self.memory_Cr)\n            else:\n                self.Cr = np.random.rand()\n            self.F = np.random.choice(self.memory_F)\n            SF.append(self.F)\n            SCr.append(self.Cr)\n\n\n    def update_best(self, pop, fitness):\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = pop[best_index].copy()\n            self.no_improvement_count = 0\n        else:\n            self.no_improvement_count += 1\n\n    def orthogonal_learning(self, func):\n        # Select a random individual\n        idx = np.random.randint(0, self.pop_size)\n        x = self.pop[idx].copy()\n\n        # Generate orthogonal design points\n        orthogonal_matrix = np.random.randn(self.dim, self.dim)\n        q, r = np.linalg.qr(orthogonal_matrix)\n        design_points = x + 0.1 * q  # Scale the orthogonal vectors\n\n        # Clip and evaluate\n        design_points = np.clip(design_points, func.bounds.lb, func.bounds.ub)\n        fitness_values = np.array([func(point) for point in design_points.T])\n        self.eval_count += self.dim\n\n        # Select the best point\n        best_idx = np.argmin(fitness_values)\n        if fitness_values[best_idx] < self.fitness[idx]:\n            self.pop[idx] = design_points[:, best_idx]\n            self.fitness[idx] = fitness_values[best_idx]\n            self.update_best(self.pop, self.fitness)\n\n    def local_search(self, func):\n        if np.random.rand() < self.local_search_prob:\n            x_new = np.random.normal(loc=self.x_opt, scale=0.05, size=self.dim)\n            x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n            f_new = func(x_new)\n            self.eval_count += 1\n            if f_new < self.f_opt:\n                self.f_opt = f_new\n                self.x_opt = x_new.copy()\n                self.no_improvement_count = 0\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.no_improvement_count = 0\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            self.adapt_parameters()\n            mutated_pop = self.mutate()\n            mutated_pop = np.clip(mutated_pop, func.bounds.lb, func.bounds.ub)\n            crossed_pop = self.crossover(mutated_pop)\n            crossed_pop = np.clip(crossed_pop, func.bounds.lb, func.bounds.ub)\n            self.selection(func, crossed_pop)\n            self.orthogonal_learning(func)\n            self.local_search(func)\n\n            if self.no_improvement_count > self.stagnation_iters:\n                self.reinitialize_population(func)\n                self.no_improvement_count = 0\n            elif self.eval_count % (self.budget // 10) == 0:\n                pass\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5c77fb83-9173-44ec-9314-0281e74e4d22"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "4e239211-6b4e-4eb8-8bf3-df6fed47cce9", "fitness": 0.0, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution with orthogonal learning and covariance matrix adaptation for improved exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_init=0.5, Cr_init=0.9, stagnation_iters=50, archive_size=10, orthogonal_learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init  # Mutation factor\n        self.Cr = Cr_init # Crossover rate\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.stagnation_iters = stagnation_iters\n        self.no_improvement_count = 0\n        self.best_fitness_history = []\n        self.archive_size = archive_size\n        self.archive_F = np.full(self.archive_size, F_init)\n        self.archive_Cr = np.full(self.archive_size, Cr_init)\n        self.archive_idx = 0\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.covariance_matrix = np.eye(dim)  # Initialize covariance matrix\n        self.learning_rate_covariance = 0.1  # Learning rate for covariance matrix adaptation\n\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        \n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.pop[best_index].copy()\n        self.best_fitness_history.append(self.f_opt)\n\n    def mutate(self):\n        mutated_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.pop[idxs]\n            mutated_pop[i] = x_r1 + self.F * (x_r2 - x_r3)\n        return mutated_pop\n\n    def crossover(self, mutated_pop):\n        crossed_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            for j in range(self.dim):\n                if np.random.rand() < self.Cr or j == np.random.randint(0, self.dim):\n                    crossed_pop[i, j] = mutated_pop[i, j]\n                else:\n                    crossed_pop[i, j] = self.pop[i, j]\n        return crossed_pop\n\n    def orthogonal_learning(self):\n        # Perform orthogonal learning to improve search efficiency\n        orthogonal_pop = self.pop.copy()\n        for i in range(self.pop_size):\n            direction = self.x_opt - self.pop[i]\n            orthogonal_pop[i] = self.pop[i] + self.orthogonal_learning_rate * direction\n\n        return orthogonal_pop\n\n    def selection(self, func, crossed_pop, orthogonal_pop):\n        new_fitness = np.array([func(x) for x in crossed_pop])\n        orthogonal_fitness = np.array([func(x) for x in orthogonal_pop])\n        self.eval_count += 2 * self.pop_size\n\n        improved_indices = new_fitness < self.fitness\n        orthogonal_improved_indices = orthogonal_fitness < self.fitness\n\n        delta_fitness = self.fitness - new_fitness\n\n        for i in range(self.pop_size):\n            if improved_indices[i]:\n                self.pop[i] = crossed_pop[i]\n                self.fitness[i] = new_fitness[i]\n                self.update_archive(self.F, self.Cr, delta_fitness[i])\n            elif orthogonal_improved_indices[i]:\n                self.pop[i] = orthogonal_pop[i]\n                self.fitness[i] = orthogonal_fitness[i]\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.pop[best_index].copy()\n            self.no_improvement_count = 0 # Reset stagnation counter\n        else:\n             self.no_improvement_count +=1\n        self.best_fitness_history.append(self.f_opt)\n\n    def reinitialize_population(self, func):\n        # Re-initialize around the best solution with increased diversity\n        # More aggressive re-initialization: Randomize a larger portion of the population\n        num_to_reinitialize = int(0.8 * self.pop_size)  # Reinitialize 80% of the population\n        indices_to_reinitialize = np.random.choice(self.pop_size, num_to_reinitialize, replace=False)\n        \n        self.pop[indices_to_reinitialize] = np.random.multivariate_normal(\n            mean=self.x_opt, cov=self.covariance_matrix, size=num_to_reinitialize\n        )\n        self.pop[indices_to_reinitialize] = np.clip(self.pop[indices_to_reinitialize], func.bounds.lb, func.bounds.ub)\n\n\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += num_to_reinitialize\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.pop[best_index].copy()\n        self.best_fitness_history.append(self.f_opt)\n        self.no_improvement_count = 0 # Reset stagnation counter\n\n\n    def adapt_parameters(self):\n        # Success-history based adaptation of F and Cr\n        self.F = np.clip(np.random.choice(self.archive_F), 0.1, 1.0)\n        self.Cr = np.clip(np.random.choice(self.archive_Cr), 0.1, 1.0)\n\n    def update_archive(self, F, Cr, delta_fitness):\n        self.archive_F[self.archive_idx] = F\n        self.archive_Cr[self.archive_idx] = Cr\n        self.archive_idx = (self.archive_idx + 1) % self.archive_size\n\n    def update_covariance_matrix(self):\n        # Adapt covariance matrix based on the population distribution\n        deviation = self.pop - self.x_opt\n        self.covariance_matrix = (1 - self.learning_rate_covariance) * self.covariance_matrix + \\\n                                 self.learning_rate_covariance * np.cov(deviation.T)\n        # Ensure positive semi-definiteness\n        self.covariance_matrix = (self.covariance_matrix + self.covariance_matrix.T) / 2\n        try:\n            np.linalg.cholesky(self.covariance_matrix)  # Check if positive semi-definite\n        except np.linalg.LinAlgError:\n            # If not positive semi-definite, add a small value to the diagonal\n            self.covariance_matrix += np.eye(self.dim) * 1e-6\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.no_improvement_count = 0\n        self.best_fitness_history = []\n        self.initialize_population(func)\n        \n        while self.eval_count < self.budget:\n            self.adapt_parameters()\n            mutated_pop = self.mutate()\n            mutated_pop = np.clip(mutated_pop, func.bounds.lb, func.bounds.ub)\n            crossed_pop = self.crossover(mutated_pop)\n            crossed_pop = np.clip(crossed_pop, func.bounds.lb, func.bounds.ub)\n            orthogonal_pop = self.orthogonal_learning()\n            orthogonal_pop = np.clip(orthogonal_pop, func.bounds.lb, func.bounds.ub)\n            self.selection(func, crossed_pop, orthogonal_pop)\n\n            #Adaptive strategy: Adapt parameters and reinitialize population if stagnating\n            if self.no_improvement_count > self.stagnation_iters:\n                self.update_covariance_matrix()\n                self.reinitialize_population(func)\n                self.no_improvement_count = 0\n\n            #Adapt parameters every so often\n            if self.eval_count % (self.budget // 10) == 0:\n                pass\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b53f868f-d236-4c55-9a6c-e08011f094a2"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "05718544-9dde-4549-b396-2964ae55e5c2", "fitness": 0.4379241634327793, "name": "AdaptiveDECauchy", "description": "An adaptive differential evolution algorithm with a Cauchy mutation operator and a self-adjusting population size based on the optimization progress.", "code": "import numpy as np\n\nclass AdaptiveDECauchy:\n    def __init__(self, budget=10000, dim=10, pop_size_init=50, F_init=0.5, Cr_init=0.9, stagnation_iters=50, pop_size_reduction_factor=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size_init\n        self.pop_size_init = pop_size_init\n        self.F = F_init\n        self.Cr = Cr_init\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.stagnation_iters = stagnation_iters\n        self.no_improvement_count = 0\n        self.pop_size_reduction_factor = pop_size_reduction_factor\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.update_best(self.pop, self.fitness)\n\n    def mutate(self):\n        mutated_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.pop[idxs]\n            # Cauchy mutation\n            mutated_pop[i] = x_r1 + self.F * np.random.standard_cauchy(size=self.dim) * (x_r2 - x_r3)\n        return mutated_pop\n\n    def crossover(self, mutated_pop):\n        crossed_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            for j in range(self.dim):\n                if np.random.rand() < self.Cr or j == np.random.randint(0, self.dim):\n                    crossed_pop[i, j] = mutated_pop[i, j]\n                else:\n                    crossed_pop[i, j] = self.pop[i, j]\n        return crossed_pop\n\n    def selection(self, func, crossed_pop):\n        new_fitness = np.array([func(x) for x in crossed_pop])\n        self.eval_count += self.pop_size\n\n        improved = new_fitness < self.fitness\n        self.pop[improved] = crossed_pop[improved]\n        self.fitness[improved] = new_fitness[improved]\n\n        self.update_best(self.pop, self.fitness)\n\n    def reinitialize_population(self, func):\n        self.pop = np.random.normal(loc=self.x_opt, scale=0.5, size=(self.pop_size, self.dim))\n        self.pop = np.clip(self.pop, func.bounds.lb, func.bounds.ub)\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.update_best(self.pop, self.fitness)\n\n    def adapt_parameters(self):\n        self.F = np.clip(np.random.normal(0.5, 0.3), 0.1, 1.0)\n        self.Cr = np.clip(np.random.normal(0.9, 0.2), 0.1, 1.0)\n\n    def update_best(self, pop, fitness):\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = pop[best_index].copy()\n            self.no_improvement_count = 0\n        else:\n            self.no_improvement_count += 1\n\n    def reduce_population_size(self):\n        if self.pop_size > 10:\n            self.pop_size = int(self.pop_size * self.pop_size_reduction_factor)\n            self.pop = self.pop[:self.pop_size]\n            self.fitness = self.fitness[:self.pop_size]\n            print(f\"Population size reduced to {self.pop_size}\")\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.no_improvement_count = 0\n        self.pop_size = self.pop_size_init  # Reset population size\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            mutated_pop = self.mutate()\n            mutated_pop = np.clip(mutated_pop, func.bounds.lb, func.bounds.ub)\n            crossed_pop = self.crossover(mutated_pop)\n            crossed_pop = np.clip(crossed_pop, func.bounds.lb, func.bounds.ub)\n            self.selection(func, crossed_pop)\n\n            if self.no_improvement_count > self.stagnation_iters:\n                self.reinitialize_population(func)\n                self.adapt_parameters()\n                self.no_improvement_count = 0\n                if self.eval_count > self.budget // 2:\n                    self.reduce_population_size()\n\n            elif self.eval_count % (self.budget // 10) == 0:\n                self.adapt_parameters()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDECauchy scored 0.438 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5c77fb83-9173-44ec-9314-0281e74e4d22"], "operator": null, "metadata": {"aucs": [0.19824508447616285, 0.2836309755321309, 0.39546680789263733, 0.7296541908889552, 0.658210328026354, 0.5595281647023034, 0.46016962321719423, 0.41841313881232955, 0.43245454025175534, 0.477066315005632, 0.6422507923878966, 0]}}
{"id": "b5f39a8c-e7ba-4b41-9600-b677dca4267d", "fitness": -Infinity, "name": "AdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution with a Success-History Adaptation for F and Cr, combined with a covariance matrix adaptation strategy for better exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, stagnation_iters=50, CMA_iters = 250):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F_values = np.array([0.5] * self.archive_size)\n        self.Cr_values = np.array([0.9] * self.archive_size)\n        self.archive_F = []\n        self.archive_Cr = []\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.stagnation_iters = stagnation_iters\n        self.no_improvement_count = 0\n        self.memory_index = 0\n        self.CMA_iters = CMA_iters\n        self.mean = None\n        self.C = None\n        self.ps = None\n        self.pc = None\n        self.chiN = None\n        self.damps = None\n        self.cs = None\n        self.cc = None\n        self.muc = None\n        self.mucs = None\n        self.weights = None\n        self.mu = None\n        self.eigen_decomposition_needed = True\n        self.B = None\n        self.D = None\n        self.eigen_value = 0\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.update_best(self.pop, self.fitness)\n        self.mean = self.x_opt.copy()\n\n    def mutate(self, F, pop):\n        mutated_pop = np.zeros_like(pop)\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = pop[idxs]\n            mutated_pop[i] = x_r1 + F * (x_r2 - x_r3)\n        return mutated_pop\n\n    def crossover(self, Cr, mutated_pop, pop):\n        crossed_pop = np.zeros_like(pop)\n        for i in range(self.pop_size):\n            for j in range(self.dim):\n                if np.random.rand() < Cr or j == np.random.randint(0, self.dim):\n                    crossed_pop[i, j] = mutated_pop[i, j]\n                else:\n                    crossed_pop[i, j] = pop[i, j]\n        return crossed_pop\n\n    def selection(self, func, crossed_pop):\n        new_fitness = np.array([func(x) for x in crossed_pop])\n        self.eval_count += self.pop_size\n\n        improved = new_fitness < self.fitness\n        delta_fitness = np.abs(self.fitness - new_fitness)\n        successful_F = self.F_values[np.random.randint(0, len(self.F_values))]\n        successful_Cr = self.Cr_values[np.random.randint(0, len(self.Cr_values))]\n\n        self.pop[improved] = crossed_pop[improved]\n        self.fitness[improved] = new_fitness[improved]\n\n        if np.any(improved):\n            self.archive_F.extend([successful_F] * np.sum(improved))\n            self.archive_Cr.extend([successful_Cr] * np.sum(improved))\n            if len(self.archive_F) > self.archive_size:\n                self.archive_F = self.archive_F[-self.archive_size:]\n                self.archive_Cr = self.archive_Cr[-self.archive_size:]\n\n        self.update_best(self.pop, self.fitness)\n        return improved\n\n    def reinitialize_population(self, func):\n        self.pop = np.random.normal(loc=self.x_opt, scale=0.5, size=(self.pop_size, self.dim))\n        self.pop = np.clip(self.pop, func.bounds.lb, func.bounds.ub)\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.update_best(self.pop, self.fitness)\n\n    def adapt_parameters(self):\n        if self.archive_F:\n            self.F_values = np.array(self.archive_F)\n            self.Cr_values = np.array(self.archive_Cr)\n        else:\n            self.F_values = np.array([np.random.uniform(0.1, 0.9)] * self.archive_size)\n            self.Cr_values = np.array([np.random.uniform(0.1, 0.9)] * self.archive_size)\n\n    def update_best(self, pop, fitness):\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = pop[best_index].copy()\n            self.no_improvement_count = 0\n        else:\n            self.no_improvement_count += 1\n    \n    def init_CMA_ES(self):\n        self.C = np.eye(self.dim)  # Initialize covariance matrix\n        self.ps = np.zeros(self.dim)  # Initialize evolution path for step size\n        self.pc = np.zeros(self.dim)  # Initialize evolution path for covariance matrix\n        self.chiN = self.dim**0.5 * (1 - 1/(4*self.dim) + 1/(21*self.dim**2))\n\n        self.mu = self.pop_size // 2\n        self.weights = np.log(self.mu+1/2) - np.log(np.arange(1, self.mu+1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2*max(0, ((self.mueff-1)/(self.dim+1))**0.5 - 1) + self.cs\n\n        self.cc = (4 + self.mueff/self.dim) / (self.dim + 4 + 2*self.mueff/self.dim)\n        self.mucov = self.mueff\n        self.mucs = self.mucov\n\n        self.eigen_decomposition_needed = True\n        self.eigen_value = 0\n        self.B = np.eye(self.dim)\n        self.D = np.eye(self.dim)\n    \n    def CMA_sample(self, func, sigma):\n         z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n         y = self.B @ (self.D * z.T)\n         x = self.mean + sigma * y.T\n         x = np.clip(x, func.bounds.lb, func.bounds.ub)\n         return x, z\n        \n    def CMA_update(self, x, z, sigma):\n        fitness_rank = np.argsort(self.fitness)\n        x_mu = x[fitness_rank[:self.mu]]\n        z_mu = z[fitness_rank[:self.mu]]\n        y_mu = np.linalg.solve(self.B @ self.D, z_mu.T)\n\n        y_w = np.sum(self.weights.reshape(-1, 1) * y_mu, axis=1)\n        self.ps = (1 - self.cs) * self.ps + (self.cs * (2 - self.cs) * self.mueff)**0.5 * y_w\n        hsig = np.linalg.norm(self.ps) / (1 - (1-self.cs)**2)**0.5 / self.chiN < 1.4 + 2 / (self.dim + 1)\n        \n        d_hsigma = (1-hsig) * self.cc * (2 - self.cc)\n\n        self.pc = (1 - self.cc) * self.pc + hsig * (self.cc * (2 - self.cc) * self.mueff)**0.5 * (x_mu[0] - self.mean) / sigma\n        \n        oldC = self.C.copy()\n\n        artmp = (1/sigma) * ((x_mu - self.mean).T)\n        self.C = (1 - self.cc) * self.C + self.cc * (1/self.mucov) * (self.pc @ self.pc.T + d_hsigma * oldC)\n        self.C = (1 - self.cs) * self.C + self.cs * (1/self.mucs) * artmp @ np.diag(self.weights) @ artmp.T\n        \n        self.mean = np.sum(self.weights.reshape(-1, 1) * x_mu, axis=0)\n\n        self.eigen_value += 1\n        if self.eigen_value > self.CMA_iters:\n            self.eigen_decomposition_needed = True\n\n        if self.eigen_decomposition_needed:\n            self.eigen_value = 0\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            D2, self.B = np.linalg.eig(self.C)\n            self.D = np.diag(D2**0.5)\n            self.eigen_decomposition_needed = False\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.no_improvement_count = 0\n        self.initialize_population(func)\n        self.init_CMA_ES()\n        sigma = 0.5\n\n        while self.eval_count < self.budget:\n            if self.eval_count % (self.budget // 5) == 0:\n                self.adapt_parameters()\n            \n            if self.no_improvement_count > self.stagnation_iters:\n                 self.reinitialize_population(func)\n                 self.adapt_parameters()\n                 self.no_improvement_count = 0\n            \n            # Adaptive DE part\n            F = np.clip(np.random.choice(self.F_values), 0.1, 1.0)\n            Cr = np.clip(np.random.choice(self.Cr_values), 0.1, 1.0)\n            \n            mutated_pop = self.mutate(F, self.pop)\n            mutated_pop = np.clip(mutated_pop, func.bounds.lb, func.bounds.ub)\n            crossed_pop = self.crossover(Cr, mutated_pop, self.pop)\n            crossed_pop = np.clip(crossed_pop, func.bounds.lb, func.bounds.ub)\n            improved = self.selection(func, crossed_pop)\n            \n            # CMA part\n            x, z = self.CMA_sample(func, sigma)\n            fitness_CMA = np.array([func(xi) for xi in x])\n            self.eval_count += self.pop_size\n            \n            if np.min(fitness_CMA) < self.f_opt:\n                self.f_opt = np.min(fitness_CMA)\n                self.x_opt = x[np.argmin(fitness_CMA)]\n                self.no_improvement_count = 0\n            else:\n                self.no_improvement_count += 1\n            \n            self.fitness = fitness_CMA\n            self.CMA_update(x, z, sigma)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "An exception occurred: operands could not be broadcast together with shapes (2,2) (2,50) .", "error": "", "parent_ids": ["5c77fb83-9173-44ec-9314-0281e74e4d22"], "operator": null, "metadata": {}}
{"id": "73bc5201-9f08-4829-be0f-afd7c1174805", "fitness": 0.6513781159730473, "name": "AdaptiveDifferentialEvolution", "description": "Adaptive Differential Evolution with a self-adaptive mutation factor, focused restarts, and a success-history based crossover rate adaptation to balance exploration and exploitation more effectively.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=50, Cr_init=0.5, F_init=0.5, stagnation_iters=50, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.Cr = Cr_init  # Initial Crossover rate\n        self.F = F_init  # Initial Mutation factor\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.stagnation_iters = stagnation_iters\n        self.no_improvement_count = 0\n        self.archive_size = archive_size\n        self.archive = []\n        self.archive_fitness = []\n        self.memory_Cr = np.ones(archive_size) * Cr_init\n        self.memory_F = np.ones(archive_size) * F_init\n        self.p_best = 0.1 # percentage of best individuals to choose from\n\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        \n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.pop[best_index].copy()\n\n    def mutate(self):\n        mutated_pop = np.zeros_like(self.pop)\n        \n        top_indices = np.argsort(self.fitness)[:int(self.p_best * self.pop_size)]\n        for i in range(self.pop_size):\n            pbest_idx = top_indices[np.random.randint(len(top_indices))]\n            idxs = np.random.choice(self.pop_size, 2, replace=False)\n            x_r1, x_r2 = self.pop[idxs]\n            mutated_pop[i] = self.pop[i] + self.F * (self.pop[pbest_idx] - self.pop[i]) + self.F * (x_r1 - x_r2)\n        return mutated_pop\n\n    def crossover(self, mutated_pop):\n        crossed_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            j_rand = np.random.randint(0, self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.Cr or j == j_rand:\n                    crossed_pop[i, j] = mutated_pop[i, j]\n                else:\n                    crossed_pop[i, j] = self.pop[i, j]\n        return crossed_pop\n\n    def selection(self, func, crossed_pop):\n        new_fitness = np.array([func(x) for x in crossed_pop])\n        self.eval_count += self.pop_size\n\n        for i in range(self.pop_size):\n            if new_fitness[i] < self.fitness[i]:\n                # Update archive with successful Cr and F\n                if len(self.archive) < self.archive_size:\n                    self.archive.append(self.pop[i].copy())\n                    self.archive_fitness.append(self.fitness[i])\n                else:\n                    idx_to_replace = np.argmax(self.archive_fitness)  # Replace worst in archive\n                    self.archive[idx_to_replace] = self.pop[i].copy()\n                    self.archive_fitness[idx_to_replace] = self.fitness[i]\n                \n                self.pop[i] = crossed_pop[i]\n                self.fitness[i] = new_fitness[i]\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.pop[best_index].copy()\n            self.no_improvement_count = 0\n        else:\n            self.no_improvement_count += 1\n            \n        # Update Cr and F based on success history\n        self.Cr = np.random.choice(self.memory_Cr)\n        self.F = np.random.choice(self.memory_F)\n\n    def reinitialize_population(self, func):\n        # Re-initialize around the best solution with reduced scale\n        self.pop = np.random.normal(loc=self.x_opt, scale=0.5, size=(self.pop_size, self.dim))\n        self.pop = np.clip(self.pop, func.bounds.lb, func.bounds.ub)\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.pop[best_index].copy()\n        self.no_improvement_count = 0\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.no_improvement_count = 0\n        self.archive = []\n        self.archive_fitness = []\n        self.initialize_population(func)\n        \n        while self.eval_count < self.budget:\n            # Self-adaptive F and Cr are already handled in selection\n            mutated_pop = self.mutate()\n            mutated_pop = np.clip(mutated_pop, func.bounds.lb, func.bounds.ub)\n            crossed_pop = self.crossover(mutated_pop)\n            crossed_pop = np.clip(crossed_pop, func.bounds.lb, func.bounds.ub)\n            self.selection(func, crossed_pop)\n\n            if self.no_improvement_count > self.stagnation_iters:\n                self.reinitialize_population(func)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.651 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["1061d067-d04a-4835-8e33-5e783756e5ef"], "operator": null, "metadata": {"aucs": [0.23880053049326344, 0.5228574642345594, 0.7200271393969102, 0.8755927231026911, 0.7565681985143902, 0.8337704881677735, 0.5514800227091919, 0.6917468094349887, 0.7723539076373519, 0.21079525651729858, 0.8623527279239944, 0.9943381619333863, 0.3827066961301576, 0.7025796173736152, 0.9045634842149892, 0.7924178721420525, 0.615878134989559, 0.8605284673526401, 0.20912982111920075, 0.5290747960729323]}}
{"id": "54253d78-805e-4821-abb6-fb0cbe575c18", "fitness": 0.6063628334556562, "name": "AdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution with covariance matrix adaptation for mutation and orthogonal crossover to improve exploration.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_init=0.5, Cr_init=0.9, stagnation_iters=50, exploration_factor=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.Cr = Cr_init\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.stagnation_iters = stagnation_iters\n        self.no_improvement_count = 0\n        self.exploration_factor = exploration_factor\n        self.covariance_matrix = np.eye(dim)  # Initialize covariance matrix\n        self.mean = None # Initialize mean\n        self.orthogonal_matrix = self._generate_orthogonal_matrix(dim)\n\n\n    def _generate_orthogonal_matrix(self, dim):\n        \"\"\"Generates a random orthogonal matrix.\"\"\"\n        matrix = np.random.normal(size=(dim, dim))\n        q, _ = np.linalg.qr(matrix)\n        return q\n\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.update_best(self.pop, self.fitness)\n        self.mean = np.mean(self.pop, axis=0)\n\n\n    def mutate(self):\n        mutated_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.pop[idxs]\n            \n            # Covariance matrix adaptation\n            z = np.random.multivariate_normal(np.zeros(self.dim), self.covariance_matrix)\n            mutated_pop[i] = x_r1 + self.F * (x_r2 - x_r3) + self.exploration_factor * z\n            \n        return mutated_pop\n\n    def crossover(self, mutated_pop):\n        crossed_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            # Orthogonal Crossover\n            j_rand = np.random.randint(0, self.dim)\n            u = np.random.rand(self.dim) < self.Cr\n            u[j_rand] = True  # Ensure at least one element is taken from mutated vector\n            \n            # Orthogonal array based crossover\n            orthogonal_vector = self.orthogonal_matrix[i % self.dim] \n            for j in range(self.dim):\n                if u[j]:\n                     crossed_pop[i, j] = mutated_pop[i, j]\n                else:\n                     crossed_pop[i, j] = self.pop[i, j]\n        return crossed_pop\n\n    def selection(self, func, crossed_pop):\n        new_fitness = np.array([func(x) for x in crossed_pop])\n        self.eval_count += self.pop_size\n\n        improved = new_fitness < self.fitness\n        self.pop[improved] = crossed_pop[improved]\n        self.fitness[improved] = new_fitness[improved]\n\n        self.update_best(self.pop, self.fitness)\n\n    def reinitialize_population(self, func):\n         # Reinitialize around the best and sample from a distribution\n        self.pop = np.random.normal(loc=self.x_opt, scale=0.5, size=(self.pop_size, self.dim))\n        self.pop = np.clip(self.pop, func.bounds.lb, func.bounds.ub)\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.update_best(self.pop, self.fitness)\n        self.mean = np.mean(self.pop, axis=0)\n\n\n    def adapt_parameters(self):\n        self.F = np.clip(np.random.normal(0.5, 0.3), 0.1, 1.0)\n        self.Cr = np.clip(np.random.normal(0.9, 0.2), 0.1, 1.0)\n        \n        # Update covariance matrix (simple adaptation)\n        if self.eval_count > self.pop_size:  # Start after initial population\n            diff = self.pop - self.mean\n            self.covariance_matrix = np.cov(diff.T) + np.eye(self.dim) * 1e-6 # Adding a small value to avoid singular matrix\n            self.mean = np.mean(self.pop, axis=0)\n\n    def update_best(self, pop, fitness):\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = pop[best_index].copy()\n            self.no_improvement_count = 0\n        else:\n            self.no_improvement_count += 1\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.no_improvement_count = 0\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            mutated_pop = self.mutate()\n            mutated_pop = np.clip(mutated_pop, func.bounds.lb, func.bounds.ub)\n            crossed_pop = self.crossover(mutated_pop)\n            crossed_pop = np.clip(crossed_pop, func.bounds.lb, func.bounds.ub)\n            self.selection(func, crossed_pop)\n\n            if self.no_improvement_count > self.stagnation_iters:\n                self.reinitialize_population(func)\n                self.adapt_parameters()\n                self.no_improvement_count = 0\n            elif self.eval_count % (self.budget // 10) == 0:\n                self.adapt_parameters()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.606 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5c77fb83-9173-44ec-9314-0281e74e4d22"], "operator": null, "metadata": {"aucs": [0.2718645296059625, 0.5241365425254065, 0.5334789508955657, 0.855481922629571, 0.5884298823266563, 0.645787459346777, 0.458435789768174, 0.4498820488414592, 0.5811161193002918, 0.6239685087202985, 0.8387029785531781, 0.9979545648556274, 0.48176959274826026, 0.6118729721334153, 0.8653847993618402, 0.6955787887055197, 0.5314775113278525, 0.8061255182514383, 0.24800645871921656, 0.5178017304966169]}}
{"id": "8316fa46-bc3e-4c67-bbb6-0c9135e0d537", "fitness": 0.0, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution with success-history adaptation of F and Cr, archive for enhanced mutation, and orthogonal learning for population diversity.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=25, stagnation_iters=50, ortho_trials=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F_memory = np.ones(10) * 0.5\n        self.Cr_memory = np.ones(10) * 0.9\n        self.F_p = np.ones(10) / 10\n        self.Cr_p = np.ones(10) / 10\n        self.memory_index = 0\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.stagnation_iters = stagnation_iters\n        self.no_improvement_count = 0\n        self.archive = []\n        self.ortho_trials = ortho_trials\n\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.update_best(self.pop, self.fitness)\n\n\n    def mutate(self):\n        mutated_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 2, replace=False)\n            x_r1, x_r2 = self.pop[idxs]\n\n            if len(self.archive) > 0 and np.random.rand() < 0.1:\n                x_r3 = self.archive[np.random.randint(0, len(self.archive))]\n            else:\n                idx3 = np.random.choice(self.pop_size, 1, replace=False)[0]\n                x_r3 = self.pop[idx3]\n\n            F = np.random.choice(self.F_memory, p=self.F_p)\n            mutated_pop[i] = self.pop[i] + F * (x_r1 - x_r2)\n        return mutated_pop\n\n    def crossover(self, mutated_pop):\n        crossed_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            Cr = np.random.choice(self.Cr_memory, p=self.Cr_p)\n            for j in range(self.dim):\n                if np.random.rand() < Cr or j == np.random.randint(0, self.dim):\n                    crossed_pop[i, j] = mutated_pop[i, j]\n                else:\n                    crossed_pop[i, j] = self.pop[i, j]\n        return crossed_pop\n\n    def selection(self, func, crossed_pop):\n        new_fitness = np.array([func(x) for x in crossed_pop])\n        self.eval_count += self.pop_size\n\n        for i in range(self.pop_size):\n            if new_fitness[i] < self.fitness[i]:\n                self.archive.append(self.pop[i].copy())\n                if len(self.archive) > self.archive_size:\n                    self.archive.pop(0)\n\n        improved = new_fitness < self.fitness\n        delta = np.abs(new_fitness[improved] - self.fitness[improved])\n        weights = delta / np.sum(delta) if np.sum(delta) > 0 else np.ones(len(delta)) / len(delta)\n\n        self.pop[improved] = crossed_pop[improved]\n        self.fitness[improved] = new_fitness[improved]\n\n        if np.any(improved):\n             self.update_parameters(improved, weights)\n\n\n        self.update_best(self.pop, self.fitness)\n\n    def update_parameters(self, improved, weights):\n        success_F = []\n        success_Cr = []\n\n        for i in range(len(improved)):\n            if improved[i]:\n                success_F.append(np.random.choice(self.F_memory, p=self.F_p))\n                success_Cr.append(np.random.choice(self.Cr_memory, p=self.Cr_p))\n\n        if success_F:\n            self.F_memory[self.memory_index] = np.mean(success_F)\n            self.Cr_memory[self.memory_index] = np.mean(success_Cr)\n\n        self.F_p = self.F_memory / np.sum(self.F_memory)\n        self.Cr_p = self.Cr_memory / np.sum(self.Cr_memory)\n        self.memory_index = (self.memory_index + 1) % 10\n\n    def reinitialize_population(self, func):\n        self.pop = np.random.normal(loc=self.x_opt, scale=0.5, size=(self.pop_size, self.dim))\n        self.pop = np.clip(self.pop, func.bounds.lb, func.bounds.ub)\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.update_best(self.pop, self.fitness)\n\n    def orthogonal_learning(self, func):\n        for _ in range(self.ortho_trials):\n            indices = np.random.choice(self.pop_size, self.dim, replace=False)\n            orthogonal_matrix = np.random.randn(self.dim, self.dim)\n            q, r = np.linalg.qr(orthogonal_matrix)\n\n            new_positions = self.x_opt + np.dot(self.pop[indices] - self.x_opt, q)\n\n            new_positions = np.clip(new_positions, func.bounds.lb, func.bounds.ub)\n\n            new_fitness = np.array([func(x) for x in new_positions])\n            self.eval_count += self.dim\n\n            improved = new_fitness < self.fitness[indices]\n            self.pop[indices[improved]] = new_positions[improved]\n            self.fitness[indices[improved]] = new_fitness[improved]\n            self.update_best(self.pop, self.fitness)\n\n\n    def update_best(self, pop, fitness):\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = pop[best_index].copy()\n            self.no_improvement_count = 0\n        else:\n            self.no_improvement_count += 1\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.no_improvement_count = 0\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            mutated_pop = self.mutate()\n            mutated_pop = np.clip(mutated_pop, func.bounds.lb, func.bounds.ub)\n            crossed_pop = self.crossover(mutated_pop)\n            crossed_pop = np.clip(crossed_pop, func.bounds.lb, func.bounds.ub)\n            self.selection(func, crossed_pop)\n\n\n            if self.no_improvement_count > self.stagnation_iters:\n                self.reinitialize_population(func)\n                self.no_improvement_count = 0\n            elif self.eval_count % (self.budget // 10) == 0:\n                self.orthogonal_learning(func)\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5c77fb83-9173-44ec-9314-0281e74e4d22"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "92a775ad-f563-4aa6-9e2e-140200e58686", "fitness": 0.38102689695247616, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with simplified parameter adaptation, archive, and enhanced exploration using a larger archive relative to population size.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size_factor=2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = int(pop_size * archive_size_factor)\n        self.F = 0.5\n        self.CR = 0.9\n        self.archive = []\n        self.memory_F = np.ones(5) * 0.5\n        self.memory_CR = np.ones(5) * 0.9\n        self.memory_idx = 0\n        self.epsilon = 1e-6\n        self.success_count = 0\n        self.sf_history = []\n        self.scr_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        \n        if np.min(self.fitness) < self.f_opt:\n            self.f_opt = np.min(self.fitness)\n            self.x_opt = self.population[np.argmin(self.fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size + len(self.archive), 3, replace=False)\n                \n                if idxs[0] < self.pop_size:\n                    x_r1 = self.population[idxs[0]]\n                else:\n                    x_r1 = self.archive[idxs[0] - self.pop_size]\n                    \n                if idxs[1] < self.pop_size:\n                    x_r2 = self.population[idxs[1]]\n                else:\n                    x_r2 = self.archive[idxs[1] - self.pop_size]\n                    \n                if idxs[2] < self.pop_size:\n                    x_r3 = self.population[idxs[2]]\n                else:\n                    x_r3 = self.archive[idxs[2] - self.pop_size]\n\n                # Adaptive F and CR\n                F = self.memory_F[self.memory_idx]\n                CR = self.memory_CR[self.memory_idx]\n                \n                mutant = x_r1 + F * (x_r2 - x_r3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        idx_replace = np.random.randint(self.archive_size)\n                        self.archive[idx_replace] = self.population[i].copy()\n                    \n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    self.sf_history.append(F)\n                    self.scr_history.append(CR)\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Update memory\n            if self.sf_history and self.scr_history:\n                self.memory_F[self.memory_idx] = np.mean(np.clip(self.sf_history, 0.1, 0.9))\n                self.memory_CR[self.memory_idx] = np.mean(self.scr_history)\n            else:\n                self.memory_F[self.memory_idx] = 0.5\n                self.memory_CR[self.memory_idx] = 0.9\n                \n            self.sf_history = []\n            self.scr_history = []\n            self.memory_idx = (self.memory_idx + 1) % 5\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDE scored 0.381 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3775ebe3-c02c-4642-b387-0b397c813d24"], "operator": null, "metadata": {"aucs": [0.15128737488518673, 0.2455724503711213, 0.36744730570108075, 0.397864267429756, 0.31591722116092635, 0.38227230826366665, 0.28689898662040636, 0.31418196143681065, 0.3178515411484132, 0.19628981808064017, 0.5309533448085249, 0.9878272246471127, 0.3033085062650682, 0.31868049380037333, 0.7181904591208699, 0.37420162082400843, 0.2972555341606755, 0.44841600654195635, 0.18008612214570208, 0.4860353916372233]}}
{"id": "6e112d4d-41ba-42a9-a2c9-d58743dd5789", "fitness": 0.0, "name": "AdaptiveDE", "description": "Simplified Adaptive Differential Evolution with pooled memory for F and CR and prioritized archive updates.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size_factor=2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = int(pop_size * archive_size_factor)\n        self.F_pool = np.array([0.5, 0.7, 0.9])\n        self.CR_pool = np.array([0.6, 0.8, 1.0])\n        self.archive = []\n        self.epsilon = 1e-6\n        self.sf_history = []\n        self.scr_history = []\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        \n        if np.min(self.fitness) < self.f_opt:\n            self.f_opt = np.min(self.fitness)\n            self.x_opt = self.population[np.argmin(self.fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size + len(self.archive), 3, replace=False)\n                \n                if idxs[0] < self.pop_size:\n                    x_r1 = self.population[idxs[0]]\n                else:\n                    x_r1 = self.archive[idxs[0] - self.pop_size]\n                    \n                if idxs[1] < self.pop_size:\n                    x_r2 = self.population[idxs[1]]\n                else:\n                    x_r2 = self.archive[idxs[1] - self.pop_size]\n                    \n                if idxs[2] < self.pop_size:\n                    x_r3 = self.population[idxs[2]]\n                else:\n                    x_r3 = self.archive[idxs[2] - self.pop_size]\n\n                # Adaptive F and CR\n                F = np.random.choice(self.F_pool)\n                CR = np.random.choice(self.CR_pool)\n                \n                mutant = x_r1 + F * (x_r2 - x_r3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        # Replace worst in archive\n                        archive_fitness = [func(x) for x in self.archive]\n                        idx_replace = np.argmax(archive_fitness)\n                        self.archive[idx_replace] = self.population[i].copy()\n                    \n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["92a775ad-f563-4aa6-9e2e-140200e58686"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "4f086812-f60e-49ea-8092-5f6d7a0bc6fa", "fitness": 0.0, "name": "AdaptiveDECauchySimplified", "description": "Simplified Adaptive Differential Evolution with Cauchy mutation, adaptive F and Cr, and periodic population re-initialization centered around the best solution.", "code": "import numpy as np\n\nclass AdaptiveDECauchySimplified:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_init=0.5, Cr_init=0.9, reinit_freq=500):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.Cr = Cr_init\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.reinit_freq = reinit_freq\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.update_best(self.pop, self.fitness)\n\n    def mutate(self):\n        mutated_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.pop[idxs]\n            mutated_pop[i] = x_r1 + self.F * np.random.standard_cauchy(size=self.dim) * (x_r2 - x_r3)\n        return mutated_pop\n\n    def crossover(self, mutated_pop):\n        crossed_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            for j in range(self.dim):\n                if np.random.rand() < self.Cr or j == np.random.randint(0, self.dim):\n                    crossed_pop[i, j] = mutated_pop[i, j]\n                else:\n                    crossed_pop[i, j] = self.pop[i, j]\n        return crossed_pop\n\n    def selection(self, func, crossed_pop):\n        new_fitness = np.array([func(x) for x in crossed_pop])\n        self.eval_count += self.pop_size\n\n        improved = new_fitness < self.fitness\n        self.pop[improved] = crossed_pop[improved]\n        self.fitness[improved] = new_fitness[improved]\n\n        self.update_best(self.pop, self.fitness)\n\n    def reinitialize_population(self, func):\n        self.pop = np.random.normal(loc=self.x_opt, scale=0.5, size=(self.pop_size, self.dim))\n        self.pop = np.clip(self.pop, func.bounds.lb, func.bounds.ub)\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.update_best(self.pop, self.fitness)\n\n    def adapt_parameters(self):\n        self.F = np.clip(np.random.normal(0.5, 0.3), 0.1, 1.0)\n        self.Cr = np.clip(np.random.normal(0.9, 0.2), 0.1, 1.0)\n\n    def update_best(self, pop, fitness):\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = pop[best_index].copy()\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            mutated_pop = self.mutate()\n            mutated_pop = np.clip(mutated_pop, func.bounds.lb, func.bounds.ub)\n            crossed_pop = self.crossover(mutated_pop)\n            crossed_pop = np.clip(crossed_pop, func.bounds.lb, func.bounds.ub)\n            self.selection(func, crossed_pop)\n\n            if self.eval_count % self.reinit_freq == 0:\n                self.reinitialize_population(func)\n                self.adapt_parameters()\n            else:\n                self.adapt_parameters()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDECauchySimplified scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["05718544-9dde-4549-b396-2964ae55e5c2"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "5605b193-dcd3-4ee0-b377-9f0faa18273b", "fitness": 0.0, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with a Cauchy mutation, archive with replacement based on fitness, and adaptive F and CR parameters based on successful mutations.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size_factor=2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = int(pop_size * archive_size_factor)\n        self.F = 0.5\n        self.CR = 0.9\n        self.archive = []\n        self.memory_F = np.ones(5) * 0.5\n        self.memory_CR = np.ones(5) * 0.9\n        self.memory_idx = 0\n        self.epsilon = 1e-6\n        self.success_count = 0\n        self.sf_history = []\n        self.scr_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        \n        if np.min(self.fitness) < self.f_opt:\n            self.f_opt = np.min(self.fitness)\n            self.x_opt = self.population[np.argmin(self.fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation - Cauchy mutation\n                idxs = np.random.choice(self.pop_size + len(self.archive), 3, replace=False)\n                \n                if idxs[0] < self.pop_size:\n                    x_r1 = self.population[idxs[0]]\n                else:\n                    x_r1 = self.archive[idxs[0] - self.pop_size]\n                    \n                if idxs[1] < self.pop_size:\n                    x_r2 = self.population[idxs[1]]\n                else:\n                    x_r2 = self.archive[idxs[1] - self.pop_size]\n                    \n                if idxs[2] < self.pop_size:\n                    x_r3 = self.population[idxs[2]]\n                else:\n                    x_r3 = self.archive[idxs[2] - self.pop_size]\n\n                # Adaptive F and CR\n                F = self.memory_F[self.memory_idx]\n                CR = self.memory_CR[self.memory_idx]\n                \n                # Cauchy mutation\n                cauchy = np.random.standard_cauchy(size=self.dim)\n                mutant = x_r1 + F * cauchy * (x_r2 - x_r3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    # Archive replacement strategy: replace the worst individual in the archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        fitness_archive = [func(x) for x in self.archive]\n                        idx_replace = np.argmax(fitness_archive)  # Replace worst in archive\n                        self.archive[idx_replace] = self.population[i].copy()\n                    \n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    self.sf_history.append(F)\n                    self.scr_history.append(CR)\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Update memory\n            if self.sf_history and self.scr_history:\n                self.memory_F[self.memory_idx] = np.mean(np.clip(self.sf_history, 0.1, 0.9))\n                self.memory_CR[self.memory_idx] = np.mean(self.scr_history)\n            else:\n                self.memory_F[self.memory_idx] = 0.5\n                self.memory_CR[self.memory_idx] = 0.9\n                \n            self.sf_history = []\n            self.scr_history = []\n            self.memory_idx = (self.memory_idx + 1) % 5\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["92a775ad-f563-4aa6-9e2e-140200e58686"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "11f42914-34a0-4dfa-819a-1b0ce50daf36", "fitness": 0.0, "name": "CMAES_DE", "description": "Covariance matrix adaptation evolution strategy with adaptive step size and population update using a combination of differential evolution and Gaussian mutation.", "code": "import numpy as np\n\nclass CMAES_DE:\n    def __init__(self, budget=10000, dim=10, pop_size=None, initial_sigma=0.1, cs=0.3, c_cov=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.sigma = initial_sigma\n        self.cs = cs\n        self.c_cov = c_cov\n        self.mean = None\n        self.C = None\n        self.p_sigma = None\n        self.p_c = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.D = None # eigenvalues of C\n        self.B = None # eigenvectors of C\n        self.m_eff = None # variance effectiveness\n        self.archive_x = []\n        self.archive_f = []\n\n\n    def initialize(self, func):\n        self.mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.p_sigma = np.zeros(self.dim)\n        self.p_c = np.zeros(self.dim)\n        self.update_decomposition()\n        self.m_eff = (self.pop_size + 2) / (self.dim + 3)\n       \n\n    def update_decomposition(self):\n        self.D, self.B = np.linalg.eig(self.C)\n        self.D = np.sqrt(np.abs(self.D))\n\n\n    def sample_population(self):\n        z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n        y = self.B.dot(np.diag(self.D)).dot(z.T).T\n        x = self.mean + self.sigma * y\n        return x\n\n\n    def selection_and_update(self, func, population):\n        fitness = np.array([func(x) for x in population])\n        self.eval_count += self.pop_size\n        \n        sorted_indices = np.argsort(fitness)\n        \n        best_indices = sorted_indices[:self.pop_size // 2]  # Keep top half\n        elite_population = population[best_indices]\n        elite_fitness = fitness[best_indices]\n\n        # Differential Evolution-like recombination\n        for i in range(len(elite_population)):\n            idxs = np.random.choice(len(elite_population), 3, replace=False)\n            x_r1, x_r2, x_r3 = elite_population[idxs]\n            mutated_vector = x_r1 + 0.5 * (x_r2 - x_r3)\n            \n            # Gaussian mutation to maintain diversity\n            mutated_vector += 0.1 * np.random.normal(0, 1, self.dim)\n\n            # Clip to boundaries\n            mutated_vector = np.clip(mutated_vector, func.bounds.lb, func.bounds.ub)\n            \n            mutated_fitness = func(mutated_vector)\n            self.eval_count += 1\n            \n            if mutated_fitness < elite_fitness[i]:\n                elite_population[i] = mutated_vector\n                elite_fitness[i] = mutated_fitness\n        \n        self.archive_x.extend(population)\n        self.archive_f.extend(fitness)\n        \n        #Truncate archive\n        archive_size = min(len(self.archive_x), self.budget * 2)\n        self.archive_x = self.archive_x[-archive_size:]\n        self.archive_f = self.archive_f[-archive_size:]\n\n\n        self.mean = np.mean(elite_population, axis=0)\n\n\n        # Cumulation\n        y = self.mean - self.mean # difference with old mean is always zero, so this can be removed\n        self.p_sigma = (1 - self.cs) * self.p_sigma + np.sqrt(self.cs * (2 - self.cs) * self.m_eff) * self.B.dot(y / self.sigma)\n\n\n        hsig = np.linalg.norm(self.p_sigma) / np.sqrt(1 - (1 - self.cs)**(self.eval_count/self.pop_size)) < (1.4 + 2/(self.dim+1)) * 1.5\n        self.p_c = (1-self.c_cov) * self.p_c + hsig * np.sqrt(self.c_cov*(2-self.c_cov) * self.m_eff) * y / self.sigma\n\n        self.C = (1 - self.c_cov) * self.C + self.c_cov * (self.p_c[:, np.newaxis] @ self.p_c[np.newaxis, :])\n        self.sigma *= np.exp((self.cs / (1 - self.cs)) * (np.linalg.norm(self.p_sigma) - self.dim) / self.dim)\n\n        self.update_decomposition()\n\n\n        best_fitness = np.min(elite_fitness)\n        best_index = np.argmin(elite_fitness)\n        if best_fitness < self.f_opt:\n            self.f_opt = best_fitness\n            self.x_opt = elite_population[best_index].copy()\n\n\n    def __call__(self, func):\n        self.initialize(func)\n        while self.eval_count < self.budget:\n            population = self.sample_population()\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            self.selection_and_update(func, population)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm CMAES_DE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["54253d78-805e-4821-abb6-fb0cbe575c18"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "9f9187e2-613c-4665-a080-4887bd7200b5", "fitness": 0.2344140928894672, "name": "AdaptiveDECauchySimplified", "description": "Simplified Adaptive Differential Evolution with Cauchy mutation, parameter adaptation, and population size reduction based on stagnation.", "code": "import numpy as np\n\nclass AdaptiveDECauchySimplified:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, Cr=0.9, stagnation_iters=50, pop_size_reduction_factor=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.Cr = Cr\n        self.stagnation_iters = stagnation_iters\n        self.pop_size_reduction_factor = pop_size_reduction_factor\n\n    def __call__(self, func):\n        f_opt = np.inf\n        x_opt = None\n        eval_count = 0\n        no_improvement_count = 0\n\n        pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in pop])\n        eval_count += self.pop_size\n        best_index = np.argmin(fitness)\n        f_opt = fitness[best_index]\n        x_opt = pop[best_index].copy()\n        \n\n        while eval_count < self.budget:\n            # Mutation\n            mutated_pop = np.zeros_like(pop)\n            for i in range(self.pop_size):\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = pop[idxs]\n                mutated_pop[i] = x_r1 + self.F * np.random.standard_cauchy(size=self.dim) * (x_r2 - x_r3)\n                mutated_pop[i] = np.clip(mutated_pop[i], func.bounds.lb, func.bounds.ub)\n\n\n            # Crossover\n            crossed_pop = np.zeros_like(pop)\n            for i in range(self.pop_size):\n                for j in range(self.dim):\n                    if np.random.rand() < self.Cr or j == np.random.randint(0, self.dim):\n                        crossed_pop[i, j] = mutated_pop[i, j]\n                    else:\n                        crossed_pop[i, j] = pop[i, j]\n            crossed_pop = np.clip(crossed_pop, func.bounds.lb, func.bounds.ub)\n\n            # Selection\n            new_fitness = np.array([func(x) for x in crossed_pop])\n            eval_count += self.pop_size\n\n            improved = new_fitness < fitness\n            pop[improved] = crossed_pop[improved]\n            fitness[improved] = new_fitness[improved]\n\n            current_best_index = np.argmin(fitness)\n            if fitness[current_best_index] < f_opt:\n                f_opt = fitness[current_best_index]\n                x_opt = pop[current_best_index].copy()\n                no_improvement_count = 0\n            else:\n                no_improvement_count += 1\n\n            # Stagnation check and parameter adaptation\n            if no_improvement_count > self.stagnation_iters:\n                # Reinitialize around the best solution\n                pop = np.random.normal(loc=x_opt, scale=0.5, size=(self.pop_size, self.dim))\n                pop = np.clip(pop, func.bounds.lb, func.bounds.ub)\n                fitness = np.array([func(x) for x in pop])\n                eval_count += self.pop_size\n\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < f_opt:\n                  f_opt = fitness[best_index]\n                  x_opt = pop[best_index].copy()\n\n                self.F = np.clip(np.random.normal(0.5, 0.3), 0.1, 1.0)\n                self.Cr = np.clip(np.random.normal(0.9, 0.2), 0.1, 1.0)\n                no_improvement_count = 0\n\n                if eval_count > self.budget // 2 and self.pop_size > 10:\n                    self.pop_size = int(self.pop_size * self.pop_size_reduction_factor)\n                    pop = pop[:self.pop_size]\n                    fitness = fitness[:self.pop_size]\n\n\n            elif eval_count % (self.budget // 10) == 0:\n                self.F = np.clip(np.random.normal(0.5, 0.3), 0.1, 1.0)\n                self.Cr = np.clip(np.random.normal(0.9, 0.2), 0.1, 1.0)\n            \n            if eval_count >= self.budget:\n              break\n\n        return f_opt, x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDECauchySimplified scored 0.234 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["05718544-9dde-4549-b396-2964ae55e5c2"], "operator": null, "metadata": {"aucs": [0.1960853842818866, 0.2861012289481186, 0.4554697583278636, 0]}}
{"id": "b003666b-3dcf-418e-af3d-0ef99391acad", "fitness": 0.0, "name": "AdaptiveDEHybrid", "description": "An adaptive differential evolution algorithm using a combination of Cauchy and Gaussian mutations, orthogonal learning, and a dynamic population size adjustment strategy based on success rate.", "code": "import numpy as np\n\nclass AdaptiveDEHybrid:\n    def __init__(self, budget=10000, dim=10, pop_size=50, Cr=0.5, F=0.5, archive_size=10, \n                 cauchy_prob=0.1, stagnation_iters=50, ortho_iters=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.Cr = Cr\n        self.F = F\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.archive_size = archive_size\n        self.archive = []\n        self.archive_fitness = []\n        self.cauchy_prob = cauchy_prob\n        self.stagnation_iters = stagnation_iters\n        self.no_improvement_count = 0\n        self.success_rate = 0.0\n        self.success_history = []\n        self.ortho_iters = ortho_iters\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.pop[best_index].copy()\n\n    def mutate(self, func):\n        mutated_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            if np.random.rand() < self.cauchy_prob:\n                # Cauchy mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.pop[idxs]\n                scale = 0.1 * (func.bounds.ub - func.bounds.lb) # scale based on bounds\n                mutated_pop[i] = self.pop[i] + self.F * np.random.standard_cauchy(size=self.dim) * scale\n            else:\n                # Gaussian mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.pop[idxs]\n                mutated_pop[i] = self.pop[i] + self.F * (x_r1 - x_r2 + x_r3 - self.pop[i])\n        mutated_pop = np.clip(mutated_pop, func.bounds.lb, func.bounds.ub)\n        return mutated_pop\n\n    def crossover(self, mutated_pop):\n        crossed_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            j_rand = np.random.randint(0, self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.Cr or j == j_rand:\n                    crossed_pop[i, j] = mutated_pop[i, j]\n                else:\n                    crossed_pop[i, j] = self.pop[i, j]\n        return crossed_pop\n    \n    def orthogonal_learning(self, func):\n        # Select best individual\n        best_idx = np.argmin(self.fitness)\n        x_best = self.pop[best_idx].copy()\n\n        # Generate orthogonal matrix (simplified: random orthonormal vectors)\n        ortho_matrix = np.random.randn(self.dim, self.dim)\n        q, r = np.linalg.qr(ortho_matrix) # Orthonormalize matrix\n\n        # Generate new points around the best individual\n        scale = 0.1 * (func.bounds.ub - func.bounds.lb)\n        new_points = x_best + scale * q\n\n        # Evaluate new points and replace if better\n        new_fitness = np.array([func(x) for x in new_points])\n        self.eval_count += self.dim\n\n        for i in range(self.dim):\n            if new_fitness[i] < self.fitness[best_idx]:\n                self.pop[best_idx] = new_points[i].copy()\n                self.fitness[best_idx] = new_fitness[i]\n                \n\n    def selection(self, func, crossed_pop):\n        improved_count = 0\n        new_fitness = np.array([func(x) for x in crossed_pop])\n        self.eval_count += self.pop_size\n\n        for i in range(self.pop_size):\n            if new_fitness[i] < self.fitness[i]:\n                if len(self.archive) < self.archive_size:\n                    self.archive.append(self.pop[i].copy())\n                    self.archive_fitness.append(self.fitness[i])\n                else:\n                    idx_to_replace = np.argmax(self.archive_fitness)  # Replace worst in archive\n                    self.archive[idx_to_replace] = self.pop[i].copy()\n                    self.archive_fitness[idx_to_replace] = self.fitness[i]\n                self.pop[i] = crossed_pop[i]\n                self.fitness[i] = new_fitness[i]\n                improved_count += 1\n        \n        self.success_rate = improved_count / self.pop_size\n        self.success_history.append(self.success_rate)\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.pop[best_index].copy()\n            self.no_improvement_count = 0\n        else:\n            self.no_improvement_count += 1\n    \n    def adjust_population_size(self):\n        if len(self.success_history) > 10:\n            recent_success_rates = self.success_history[-10:]\n            avg_success_rate = np.mean(recent_success_rates)\n            \n            if avg_success_rate > 0.3 and self.pop_size < 100:\n                self.pop_size = min(self.pop_size + 5, 100)\n                print(f\"Increasing pop_size to {self.pop_size}\")\n                \n            elif avg_success_rate < 0.1 and self.pop_size > 20:\n                self.pop_size = max(self.pop_size - 5, 20)\n                print(f\"Decreasing pop_size to {self.pop_size}\")\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.no_improvement_count = 0\n        self.success_history = []\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            mutated_pop = self.mutate(func)\n            crossed_pop = self.crossover(mutated_pop)\n            self.selection(func, crossed_pop)\n            \n            if self.no_improvement_count > self.stagnation_iters:\n                self.orthogonal_learning(func)\n                self.no_improvement_count = 0\n\n            self.adjust_population_size()\n            # Re-initialize population with new size if needed\n            if self.eval_count + self.pop_size < self.budget:\n                 new_pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                 new_fitness = np.array([func(x) for x in new_pop])\n                 self.pop = new_pop\n                 self.fitness = new_fitness\n                 self.eval_count += self.pop_size\n                 best_index = np.argmin(self.fitness)\n                 if self.fitness[best_index] < self.f_opt:\n                     self.f_opt = self.fitness[best_index]\n                     self.x_opt = self.pop[best_index].copy()\n\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDEHybrid scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["73bc5201-9f08-4829-be0f-afd7c1174805"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "0eea1b90-eb76-49ab-976a-1404782d955c", "fitness": -Infinity, "name": "AdaptiveDifferentialEvolution", "description": "Adaptive Differential Evolution with orthogonal design for parameter control, local search intensification around the best solution, and adaptive population sizing based on landscape ruggedness.", "code": "import numpy as np\nfrom scipy.stats import norm\nfrom scipy.optimize import minimize\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=50, Cr_init=0.5, F_init=0.5, stagnation_iters=50, archive_size=10, p_best=0.1, ortho_levels=3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.Cr = Cr_init  # Initial Crossover rate\n        self.F = F_init  # Initial Mutation factor\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.stagnation_iters = stagnation_iters\n        self.no_improvement_count = 0\n        self.archive_size = archive_size\n        self.archive = []\n        self.archive_fitness = []\n        self.memory_Cr = np.ones(archive_size) * Cr_init\n        self.memory_F = np.ones(archive_size) * F_init\n        self.p_best = p_best # percentage of best individuals to choose from\n        self.ortho_levels = ortho_levels\n        self.min_pop_size = 10 # Minimum population size\n        self.max_pop_size = 100 # Maximum population size\n        self.population_adaptation_rate = 0.05 # Rate to adapt population size\n        self.landscape_ruggedness = 0.5 # Initial landscape ruggedness\n\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        \n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.pop[best_index].copy()\n\n    def mutate(self):\n        mutated_pop = np.zeros_like(self.pop)\n        \n        top_indices = np.argsort(self.fitness)[:int(self.p_best * self.pop_size)]\n        for i in range(self.pop_size):\n            pbest_idx = top_indices[np.random.randint(len(top_indices))]\n            idxs = np.random.choice(self.pop_size, 2, replace=False)\n            x_r1, x_r2 = self.pop[idxs]\n            mutated_pop[i] = self.pop[i] + self.F * (self.pop[pbest_idx] - self.pop[i]) + self.F * (x_r1 - x_r2)\n        return mutated_pop\n\n    def crossover(self, mutated_pop):\n        crossed_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            j_rand = np.random.randint(0, self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.Cr or j == j_rand:\n                    crossed_pop[i, j] = mutated_pop[i, j]\n                else:\n                    crossed_pop[i, j] = self.pop[i, j]\n        return crossed_pop\n    \n    def orthogonal_design(self, func, individual):\n        # Generate orthogonal array\n        doe = self.generate_orthogonal_array(self.dim, self.ortho_levels)\n        \n        # Define range for each dimension around the individual\n        delta = 0.1 * (func.bounds.ub - func.bounds.lb)  # 10% of the search space\n        levels = np.linspace(-delta, delta, self.ortho_levels)\n        \n        # Evaluate points in the orthogonal array\n        fitness_values = []\n        for i in range(doe.shape[0]):\n            point = individual + levels[doe[i, :]]\n            point = np.clip(point, func.bounds.lb, func.bounds.ub)\n            fitness_values.append(func(point))\n            self.eval_count += 1\n        \n        # Find the best point\n        best_index = np.argmin(fitness_values)\n        best_point = individual + levels[doe[best_index, :]]\n        best_point = np.clip(best_point, func.bounds.lb, func.bounds.ub)\n        best_fitness = fitness_values[best_index]\n        \n        return best_point, best_fitness\n\n    def generate_orthogonal_array(self, num_factors, num_levels):\n        # This is a simplified example; in practice, use a library like pyDOE\n        # that provides more robust orthogonal array generation.\n        if num_levels == 3:\n            if num_factors <= 3:\n                if num_factors == 2:\n                    return np.array([[0,0],[1,2],[2,1],[0,1],[1,0],[2,2],[0,2],[1,1],[2,0]])[:num_factors]\n                if num_factors == 3:\n                    return np.array([[0,0,0],[0,1,1],[0,2,2],[1,0,1],[1,1,2],[1,2,0],[2,0,2],[2,1,0],[2,2,1]])[:num_factors]\n            else:\n                # For larger number of factors, a more sophisticated OA generation is needed\n                # This could involve using a pre-computed OA or a more complex algorithm.\n                return np.random.randint(0, num_levels, size=(num_levels**2, num_factors))\n        else:\n            return np.random.randint(0, num_levels, size=(num_levels**2, num_factors))\n\n\n    def selection(self, func, crossed_pop):\n        new_fitness = np.array([func(x) for x in crossed_pop])\n        self.eval_count += self.pop_size\n\n        for i in range(self.pop_size):\n            if new_fitness[i] < self.fitness[i]:\n                # Local search intensification\n                best_point, best_fitness = self.orthogonal_design(func, crossed_pop[i])\n                if best_fitness < new_fitness[i]:\n                      new_fitness[i] = best_fitness\n                      crossed_pop[i] = best_point\n                \n                # Update archive with successful Cr and F\n                if len(self.archive) < self.archive_size:\n                    self.archive.append(self.pop[i].copy())\n                    self.archive_fitness.append(self.fitness[i])\n                else:\n                    idx_to_replace = np.argmax(self.archive_fitness)  # Replace worst in archive\n                    self.archive[idx_to_replace] = self.pop[i].copy()\n                    self.archive_fitness[idx_to_replace] = self.fitness[i]\n                \n                self.pop[i] = crossed_pop[i]\n                self.fitness[i] = new_fitness[i]\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.pop[best_index].copy()\n            self.no_improvement_count = 0\n        else:\n            self.no_improvement_count += 1\n            \n        # Update Cr and F based on success history\n        self.Cr = np.random.choice(self.memory_Cr)\n        self.F = np.random.choice(self.memory_F)\n        \n        #Adapt population size\n        self.adapt_population_size()\n\n    def reinitialize_population(self, func):\n        # Re-initialize around the best solution with reduced scale\n        self.pop = np.random.normal(loc=self.x_opt, scale=0.5, size=(self.pop_size, self.dim))\n        self.pop = np.clip(self.pop, func.bounds.lb, func.bounds.ub)\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.pop[best_index].copy()\n        self.no_improvement_count = 0\n\n    def adapt_population_size(self):\n        # A simple heuristic for landscape ruggedness (can be refined)\n        if self.no_improvement_count > self.stagnation_iters / 2:\n             self.landscape_ruggedness *= 1.1\n        else:\n             self.landscape_ruggedness *= 0.9\n\n        # Adapt population size based on ruggedness\n        adaptation = self.population_adaptation_rate * (self.landscape_ruggedness - 1)\n        self.pop_size = int(self.pop_size * (1 + adaptation))\n        self.pop_size = np.clip(self.pop_size, self.min_pop_size, self.max_pop_size)\n        self.pop_size = int(self.pop_size) # Ensure it is an integer\n\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.no_improvement_count = 0\n        self.archive = []\n        self.archive_fitness = []\n        self.initialize_population(func)\n        \n        while self.eval_count < self.budget:\n            # Self-adaptive F and Cr are already handled in selection\n            mutated_pop = self.mutate()\n            mutated_pop = np.clip(mutated_pop, func.bounds.lb, func.bounds.ub)\n            crossed_pop = self.crossover(mutated_pop)\n            crossed_pop = np.clip(crossed_pop, func.bounds.lb, func.bounds.ub)\n            self.selection(func, crossed_pop)\n\n            if self.no_improvement_count > self.stagnation_iters:\n                self.reinitialize_population(func)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "An exception occurred: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all().", "error": "", "parent_ids": ["73bc5201-9f08-4829-be0f-afd7c1174805"], "operator": null, "metadata": {}}
{"id": "1087cdb0-f901-4b64-8487-adbfb0411b4d", "fitness": 0.37949665398888865, "name": "AdaptiveDE", "description": "Simplified Adaptive Differential Evolution with a streamlined parameter adaptation and archive management for improved efficiency.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size_factor=2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = int(pop_size * archive_size_factor)\n        self.F = 0.5\n        self.CR = 0.9\n        self.archive = []\n        self.memory_F = 0.5\n        self.memory_CR = 0.9\n        self.epsilon = 1e-6\n        self.sf_history = []\n        self.scr_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        \n        if np.min(self.fitness) < self.f_opt:\n            self.f_opt = np.min(self.fitness)\n            self.x_opt = self.population[np.argmin(self.fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size + len(self.archive), 3, replace=False)\n                \n                if idxs[0] < self.pop_size:\n                    x_r1 = self.population[idxs[0]]\n                else:\n                    x_r1 = self.archive[idxs[0] - self.pop_size]\n                    \n                if idxs[1] < self.pop_size:\n                    x_r2 = self.population[idxs[1]]\n                else:\n                    x_r2 = self.archive[idxs[1] - self.pop_size]\n                    \n                if idxs[2] < self.pop_size:\n                    x_r3 = self.population[idxs[2]]\n                else:\n                    x_r3 = self.archive[idxs[2] - self.pop_size]\n\n                # Adaptive F and CR\n                F = self.memory_F\n                CR = self.memory_CR\n                \n                mutant = x_r1 + F * (x_r2 - x_r3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        idx_replace = np.random.randint(self.archive_size)\n                        self.archive[idx_replace] = self.population[i].copy()\n                    \n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    self.sf_history.append(F)\n                    self.scr_history.append(CR)\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Update memory\n            if self.sf_history and self.scr_history:\n                self.memory_F = np.mean(np.clip(self.sf_history, 0.1, 0.9))\n                self.memory_CR = np.mean(self.scr_history)\n            else:\n                self.memory_F = 0.5\n                self.memory_CR = 0.9\n                \n            self.sf_history = []\n            self.scr_history = []\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDE scored 0.379 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["92a775ad-f563-4aa6-9e2e-140200e58686"], "operator": null, "metadata": {"aucs": [0.15254175314047047, 0.25963305137761683, 0.3419795700849829, 0.5186051532755955, 0.3017826029857591, 0.39525116763638257, 0.2967085687753306, 0.30702628257562947, 0.28071330427086827, 0.21526228670055358, 0.4069777485119763, 0.9989928940446968, 0.2763767716002341, 0.31290770865084216, 0.709912689396727, 0.38006416691426104, 0.30423146169122606, 0.45899468878134864, 0.19201456172764664, 0.47995664763562396]}}
{"id": "fa72e339-35df-4b32-aa7c-feb13694331c", "fitness": 0.3799881694592527, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with a mirrored archive, orthogonal crossover, and a periodically updated covariance matrix for improved mutation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size_factor=2, memory_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = int(pop_size * archive_size_factor)\n        self.F = 0.5\n        self.CR = 0.9\n        self.archive = []\n        self.memory_F = np.ones(memory_size) * 0.5\n        self.memory_CR = np.ones(memory_size) * 0.9\n        self.memory_idx = 0\n        self.epsilon = 1e-6\n        self.sf_history = []\n        self.scr_history = []\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.cov = np.eye(dim)  # Covariance matrix for mutation\n        self.cov_update_interval = 50  # Update covariance matrix every n iterations\n        self.iter_count = 0\n        self.memory_size = memory_size\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        \n        if np.min(self.fitness) < self.f_opt:\n            self.f_opt = np.min(self.fitness)\n            self.x_opt = self.population[np.argmin(self.fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size + len(self.archive), 3, replace=False)\n                \n                if idxs[0] < self.pop_size:\n                    x_r1 = self.population[idxs[0]]\n                else:\n                    x_r1 = self.archive[idxs[0] - self.pop_size]\n                    \n                if idxs[1] < self.pop_size:\n                    x_r2 = self.population[idxs[1]]\n                else:\n                    x_r2 = self.archive[idxs[1] - self.pop_size]\n                    \n                if idxs[2] < self.pop_size:\n                    x_r3 = self.population[idxs[2]]\n                else:\n                    x_r3 = self.archive[idxs[2] - self.pop_size]\n\n                # Adaptive F and CR\n                F = self.memory_F[self.memory_idx]\n                CR = self.memory_CR[self.memory_idx]\n                \n                #mu = np.zeros(self.dim)\n                #mutant = x_r1 + F * (x_r2 - x_r3) + np.random.multivariate_normal(mu, self.cov)\n                mutant = x_r1 + F * (x_r2 - x_r3)\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    # Archive update: Mirroring strategy\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        idx_replace = np.random.randint(self.archive_size)\n                        self.archive[idx_replace] = self.population[i].copy()\n\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    self.sf_history.append(F)\n                    self.scr_history.append(CR)\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Update memory\n            if self.sf_history and self.scr_history:\n                self.memory_F[self.memory_idx] = np.mean(np.clip(self.sf_history, 0.1, 0.9))\n                self.memory_CR[self.memory_idx] = np.mean(self.scr_history)\n            else:\n                self.memory_F[self.memory_idx] = 0.5\n                self.memory_CR[self.memory_idx] = 0.9\n                \n            self.sf_history = []\n            self.scr_history = []\n            self.memory_idx = (self.memory_idx + 1) % self.memory_size\n            \n            #Update covariance matrix\n            self.iter_count += 1\n            if self.iter_count % self.cov_update_interval == 0:\n                # Calculate covariance matrix from the current population\n                self.cov = np.cov(self.population.T)\n                # Add a small value to the diagonal to ensure it is positive definite\n                self.cov += np.eye(self.dim) * 1e-6\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDE scored 0.380 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["92a775ad-f563-4aa6-9e2e-140200e58686"], "operator": null, "metadata": {"aucs": [0.16619075592205845, 0.2736456831858267, 0.3462072489713721, 0.4954925053196674, 0.29876576738418714, 0.3907401259986989, 0.29110395230301445, 0.3144480822308542, 0.2970065453411661, 0.21999320731145178, 0.38202266722725875, 0.9919321467230574, 0.2946134743482306, 0.29596293887036795, 0.7342272272155623, 0.3635814681527517, 0.3061283137560359, 0.46363425175701656, 0.19168472324858798, 0.4823823039178876]}}
{"id": "691c4b38-8a5b-4093-a961-45d187254028", "fitness": 0.6255457789356661, "name": "AdaptiveDifferentialEvolution", "description": "Simplified adaptive differential evolution with covariance matrix adaptation, reduced parameter set, and orthogonal crossover for enhanced exploration.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, Cr=0.9, stagnation_iters=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.Cr = Cr\n        self.stagnation_iters = stagnation_iters\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.no_improvement_count = 0\n        self.covariance_matrix = np.eye(dim)\n        self.mean = None\n        self.orthogonal_matrix = self._generate_orthogonal_matrix(dim)\n\n    def _generate_orthogonal_matrix(self, dim):\n        matrix = np.random.normal(size=(dim, dim))\n        q, _ = np.linalg.qr(matrix)\n        return q\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.update_best(self.pop, self.fitness)\n        self.mean = np.mean(self.pop, axis=0)\n\n    def mutate(self):\n        mutated_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.pop[idxs]\n            z = np.random.multivariate_normal(np.zeros(self.dim), self.covariance_matrix)\n            mutated_pop[i] = x_r1 + self.F * (x_r2 - x_r3) + 0.1 * z # keep exploration factor constant\n        return mutated_pop\n\n    def crossover(self, mutated_pop):\n        crossed_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            j_rand = np.random.randint(0, self.dim)\n            u = np.random.rand(self.dim) < self.Cr\n            u[j_rand] = True\n            orthogonal_vector = self.orthogonal_matrix[i % self.dim]\n            for j in range(self.dim):\n                if u[j]:\n                    crossed_pop[i, j] = mutated_pop[i, j]\n                else:\n                    crossed_pop[i, j] = self.pop[i, j]\n        return crossed_pop\n\n    def selection(self, func, crossed_pop):\n        new_fitness = np.array([func(x) for x in crossed_pop])\n        self.eval_count += self.pop_size\n\n        improved = new_fitness < self.fitness\n        self.pop[improved] = crossed_pop[improved]\n        self.fitness[improved] = new_fitness[improved]\n\n        self.update_best(self.pop, self.fitness)\n\n    def reinitialize_population(self, func):\n        self.pop = np.random.normal(loc=self.x_opt, scale=0.5, size=(self.pop_size, self.dim))\n        self.pop = np.clip(self.pop, func.bounds.lb, func.bounds.ub)\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.update_best(self.pop, self.fitness)\n        self.mean = np.mean(self.pop, axis=0)\n\n    def adapt_parameters(self):\n        if self.eval_count > self.pop_size:\n            diff = self.pop - self.mean\n            self.covariance_matrix = np.cov(diff.T) + np.eye(self.dim) * 1e-6\n            self.mean = np.mean(self.pop, axis=0)\n\n    def update_best(self, pop, fitness):\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = pop[best_index].copy()\n            self.no_improvement_count = 0\n        else:\n            self.no_improvement_count += 1\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.no_improvement_count = 0\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            mutated_pop = self.mutate()\n            mutated_pop = np.clip(mutated_pop, func.bounds.lb, func.bounds.ub)\n            crossed_pop = self.crossover(mutated_pop)\n            crossed_pop = np.clip(crossed_pop, func.bounds.lb, func.bounds.ub)\n            self.selection(func, crossed_pop)\n\n            if self.no_improvement_count > self.stagnation_iters:\n                self.reinitialize_population(func)\n                self.adapt_parameters()\n                self.no_improvement_count = 0\n            elif self.eval_count % (self.budget // 10) == 0:\n                self.adapt_parameters()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.626 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["54253d78-805e-4821-abb6-fb0cbe575c18"], "operator": null, "metadata": {"aucs": [0.27650956728856124, 0.5681618644192699, 0.5743728206026563, 0.8181804036645024, 0.6335517454305242, 0.689444684126488, 0.4813681241401512, 0.5266560773278106, 0.6644434832528554, 0.5807895431780516, 0.8448204099159623, 0.9990031260375847, 0.526300819076885, 0.6479150813542569, 0.9111760281328074, 0.6630794252084865, 0.5215921510337442, 0.7945816669722767, 0.2756057874468075, 0.5133627701036385]}}
{"id": "04c74379-cb43-4889-8715-4f111571db19", "fitness": 0.6760238653449189, "name": "AdaptiveDifferentialEvolution", "description": "Simplified Adaptive Differential Evolution with adaptive mutation factor, success-history based crossover rate adaptation, and a streamlined population update.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=50, Cr_init=0.5, F_init=0.5, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.Cr = Cr_init  # Initial Crossover rate\n        self.F = F_init  # Initial Mutation factor\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.archive_size = archive_size\n        self.archive = []\n        self.memory_Cr = np.ones(archive_size) * Cr_init\n        self.memory_F = np.ones(archive_size) * F_init\n        self.p_best = 0.1 # percentage of best individuals to choose from\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        \n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.pop[best_index].copy()\n\n    def mutate(self):\n        mutated_pop = np.zeros_like(self.pop)\n        top_indices = np.argsort(self.fitness)[:int(self.p_best * self.pop_size)]\n\n        for i in range(self.pop_size):\n            pbest_idx = top_indices[np.random.randint(len(top_indices))]\n            idxs = np.random.choice(self.pop_size, 2, replace=False)\n            x_r1, x_r2 = self.pop[idxs]\n            mutated_pop[i] = self.pop[i] + self.F * (self.pop[pbest_idx] - self.pop[i]) + self.F * (x_r1 - x_r2)\n        return mutated_pop\n\n    def crossover(self, mutated_pop):\n        crossed_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            j_rand = np.random.randint(0, self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.Cr or j == j_rand:\n                    crossed_pop[i, j] = mutated_pop[i, j]\n                else:\n                    crossed_pop[i, j] = self.pop[i, j]\n        return crossed_pop\n\n    def selection(self, func, crossed_pop):\n        new_fitness = np.array([func(x) for x in crossed_pop])\n        self.eval_count += self.pop_size\n\n        improved = new_fitness < self.fitness\n        \n        #Simplified update\n        self.pop[improved] = crossed_pop[improved]\n        self.fitness[improved] = new_fitness[improved]\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.pop[best_index].copy()\n            \n        # Update Cr and F based on success history\n        self.Cr = np.random.choice(self.memory_Cr)\n        self.F = np.random.choice(self.memory_F)\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.initialize_population(func)\n        \n        while self.eval_count < self.budget:\n            mutated_pop = self.mutate()\n            mutated_pop = np.clip(mutated_pop, func.bounds.lb, func.bounds.ub)\n            crossed_pop = self.crossover(mutated_pop)\n            crossed_pop = np.clip(crossed_pop, func.bounds.lb, func.bounds.ub)\n            self.selection(func, crossed_pop)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.676 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["73bc5201-9f08-4829-be0f-afd7c1174805"], "operator": null, "metadata": {"aucs": [0.21504204036376917, 0.5312721540685437, 0.7356405440494613, 0.8744698980702316, 0.7447097467683341, 0.8240984078255649, 0.6487124525437845, 0.6956499320770828, 0.7882515900690359, 0.5958215674457941, 0.8397366475045527, 0.9978627443028925, 0.36386905019555094, 0.7007291069013215, 0.9186823659379909, 0.8203334146534507, 0.6324979225825967, 0.8631879057779555, 0.22454130220110424, 0.5053685135593594]}}
{"id": "88de00e9-d594-49ef-ac5f-56b0ea0ada80", "fitness": 0.601716845251043, "name": "AdaptiveDifferentialEvolution", "description": "Adaptive Differential Evolution with a mirrored boundary handling, self-adaptive population size, and covariance matrix adaptation for improved exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size_init=50, F_init=0.5, Cr_init=0.9, stagnation_iters=50, exploration_factor=0.1, pop_size_min=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_init = pop_size_init\n        self.pop_size = pop_size_init\n        self.pop_size_min = pop_size_min\n        self.F = F_init\n        self.Cr = Cr_init\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.stagnation_iters = stagnation_iters\n        self.no_improvement_count = 0\n        self.exploration_factor = exploration_factor\n        self.covariance_matrix = np.eye(dim)  # Initialize covariance matrix\n        self.mean = None # Initialize mean\n        self.success_F = []\n        self.success_Cr = []\n        self.weights = []\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.update_best(self.pop, self.fitness)\n        self.mean = np.mean(self.pop, axis=0)\n\n\n    def mutate(self, func):\n        mutated_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.pop[idxs]\n            \n            # Covariance matrix adaptation\n            z = np.random.multivariate_normal(np.zeros(self.dim), self.covariance_matrix)\n            mutated_pop[i] = x_r1 + self.F * (x_r2 - x_r3) + self.exploration_factor * z\n\n            # Mirrored boundary handling\n            for j in range(self.dim):\n                if mutated_pop[i, j] < func.bounds.lb[j]:\n                    mutated_pop[i, j] = 2 * func.bounds.lb[j] - mutated_pop[i, j]\n                elif mutated_pop[i, j] > func.bounds.ub[j]:\n                    mutated_pop[i, j] = 2 * func.bounds.ub[j] - mutated_pop[i, j]\n                    \n        return mutated_pop\n\n    def crossover(self, mutated_pop):\n        crossed_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            j_rand = np.random.randint(0, self.dim)\n            u = np.random.rand(self.dim) < self.Cr\n            u[j_rand] = True  # Ensure at least one element is taken from mutated vector\n            crossed_pop[i] = np.where(u, mutated_pop[i], self.pop[i])\n        return crossed_pop\n\n    def selection(self, func, crossed_pop):\n        new_fitness = np.array([func(x) for x in crossed_pop])\n        self.eval_count += self.pop_size\n\n        improved = new_fitness < self.fitness\n        \n        # Store successful F and Cr values\n        if np.any(improved):\n            self.success_F.extend([self.F] * np.sum(improved))\n            self.success_Cr.extend([self.Cr] * np.sum(improved))\n            self.weights.extend(np.abs(new_fitness[improved] - self.fitness[improved]))\n\n        self.pop[improved] = crossed_pop[improved]\n        self.fitness[improved] = new_fitness[improved]\n\n        self.update_best(self.pop, self.fitness)\n\n    def reinitialize_population(self, func):\n         # Reinitialize around the best and sample from a distribution\n        self.pop = np.random.normal(loc=self.x_opt, scale=0.5, size=(self.pop_size, self.dim))\n        self.pop = np.clip(self.pop, func.bounds.lb, func.bounds.ub)\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.update_best(self.pop, self.fitness)\n        self.mean = np.mean(self.pop, axis=0)\n\n\n    def adapt_parameters(self):\n        # Update F and Cr based on successful values\n        if self.success_F and self.success_Cr:\n            self.success_F = np.array(self.success_F)\n            self.success_Cr = np.array(self.success_Cr)\n            self.weights = np.array(self.weights) / np.sum(self.weights)\n            \n            self.F = np.sum(self.weights * self.success_F)\n            self.Cr = np.sum(self.weights * self.success_Cr)\n            \n            self.F = np.clip(self.F, 0.1, 1.0)\n            self.Cr = np.clip(self.Cr, 0.1, 1.0)\n\n            self.success_F = []\n            self.success_Cr = []\n            self.weights = []\n        else:\n             self.F = np.clip(np.random.normal(0.5, 0.3), 0.1, 1.0)\n             self.Cr = np.clip(np.random.normal(0.9, 0.2), 0.1, 1.0)\n\n\n        # Update covariance matrix (simple adaptation)\n        if self.eval_count > self.pop_size:  # Start after initial population\n            diff = self.pop - self.mean\n            self.covariance_matrix = np.cov(diff.T) + np.eye(self.dim) * 1e-6 # Adding a small value to avoid singular matrix\n            self.mean = np.mean(self.pop, axis=0)\n\n    def update_best(self, pop, fitness):\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = pop[best_index].copy()\n            self.no_improvement_count = 0\n        else:\n            self.no_improvement_count += 1\n\n    def adapt_population_size(self):\n        if self.no_improvement_count > self.stagnation_iters:\n            self.pop_size = max(int(self.pop_size * 0.9), self.pop_size_min)  # Reduce population size\n        elif self.eval_count % (self.budget // 5) == 0 and self.pop_size < self.pop_size_init:\n            self.pop_size = min(self.pop_size + 5, self.pop_size_init) # Increase population size slowly\n    \n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.no_improvement_count = 0\n        self.pop_size = self.pop_size_init # Reset population size at each call\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            mutated_pop = self.mutate(func)\n            crossed_pop = self.crossover(mutated_pop)\n            self.selection(func, crossed_pop)\n\n            if self.no_improvement_count > self.stagnation_iters:\n                self.reinitialize_population(func)\n                self.adapt_parameters()\n                self.no_improvement_count = 0\n            elif self.eval_count % (self.budget // 10) == 0:\n                self.adapt_parameters()\n            \n            self.adapt_population_size()\n            \n            # Ensure population size remains consistent\n            if self.pop.shape[0] != self.pop_size:\n                self.pop = self.pop[:self.pop_size]\n                self.fitness = self.fitness[:self.pop_size]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.602 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["54253d78-805e-4821-abb6-fb0cbe575c18"], "operator": null, "metadata": {"aucs": [0.2440217623535642, 0.5757178252769809, 0.5954373319722277, 0.8257510745880738, 0.6172977769221665, 0.6956462085541057, 0.4792265654509654, 0.5348251752621312, 0.6279891094573498, 0.544586697706634, 0.8359797210257346, 0.999057503376698, 0.5156297146664598, 0.6292837068490081, 0.9070193505545876, 0]}}
{"id": "7786d980-6851-4434-9613-0be6e825a93d", "fitness": -Infinity, "name": "AdaptiveDifferentialEvolution", "description": "Simplified Adaptive Differential Evolution with focus on parameter adaptation and population diversity through orthogonal initialization and a modified mutation strategy.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, Cr=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.Cr = Cr\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.archive = None\n\n    def initialize_population(self, func):\n        # Orthogonal initialization for better diversity\n        orthogonal_matrix = self._generate_orthogonal_matrix(self.pop_size)\n        self.pop = np.zeros((self.pop_size, self.dim))\n        for i in range(self.pop_size):\n            self.pop[i] = func.bounds.lb + (func.bounds.ub - func.bounds.lb) * orthogonal_matrix[i % self.dim]\n        self.pop = np.clip(self.pop, func.bounds.lb, func.bounds.ub)\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.update_best(self.pop, self.fitness)\n        self.archive = self.pop.copy()\n\n    def _generate_orthogonal_matrix(self, dim):\n        matrix = np.random.normal(size=(dim, dim))\n        q, _ = np.linalg.qr(matrix)\n        return q\n\n    def mutate(self):\n        mutated_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.pop[idxs]\n            mutated_pop[i] = x_r1 + self.F * (x_r2 - x_r3) \n        return mutated_pop\n\n    def crossover(self, mutated_pop):\n        crossed_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            j_rand = np.random.randint(0, self.dim)\n            u = np.random.rand(self.dim) < self.Cr\n            u[j_rand] = True\n            for j in range(self.dim):\n                if u[j]:\n                    crossed_pop[i, j] = mutated_pop[i, j]\n                else:\n                    crossed_pop[i, j] = self.pop[i, j]\n        return crossed_pop\n\n    def selection(self, func, crossed_pop):\n        new_fitness = np.array([func(x) for x in crossed_pop])\n        self.eval_count += self.pop_size\n\n        improved = new_fitness < self.fitness\n        self.pop[improved] = crossed_pop[improved]\n        self.fitness[improved] = new_fitness[improved]\n        self.archive[improved] = crossed_pop[improved]\n\n        self.update_best(self.pop, self.fitness)\n\n        # Adaptive Cr\n        if np.sum(improved) / self.pop_size < 0.1:\n            self.Cr *= 0.9\n        else:\n            self.Cr = min(0.9, self.Cr * 1.1)\n\n    def update_best(self, pop, fitness):\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = pop[best_index].copy()\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            mutated_pop = self.mutate()\n            mutated_pop = np.clip(mutated_pop, func.bounds.lb, func.bounds.ub)\n            crossed_pop = self.crossover(mutated_pop)\n            crossed_pop = np.clip(crossed_pop, func.bounds.lb, func.bounds.ub)\n            self.selection(func, crossed_pop)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "An exception occurred: operands could not be broadcast together with shapes (2,) (40,) .", "error": "", "parent_ids": ["691c4b38-8a5b-4093-a961-45d187254028"], "operator": null, "metadata": {}}
{"id": "a2317e28-9860-4b0b-ac32-6648d7a9e10a", "fitness": 0.0, "name": "AdaptiveDifferentialEvolutionLS", "description": "Adaptive Differential Evolution with a dynamically adjusted mutation factor based on fitness improvement, a self-adjusting crossover rate using a success-history, and a local search operator based on the current best solution.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolutionLS:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_initial=0.5, Cr_initial=0.9, archive_size=10, local_search_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_initial = F_initial\n        self.Cr_initial = Cr_initial\n        self.archive_size = archive_size\n        self.local_search_prob = local_search_prob\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.archive_F = np.full(archive_size, F_initial)\n        self.archive_Cr = np.full(archive_size, Cr_initial)\n        self.archive_idx = 0\n        self.success_F = []\n        self.success_Cr = []\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.update_best(self.pop, self.fitness)\n\n    def mutate(self, F):\n        mutated_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.pop[idxs]\n            mutated_pop[i] = x_r1 + F * (x_r2 - x_r3)\n        return mutated_pop\n\n    def crossover(self, mutated_pop, Cr):\n        crossed_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            j_rand = np.random.randint(0, self.dim)\n            u = np.random.rand(self.dim) < Cr\n            u[j_rand] = True\n            crossed_pop[i] = np.where(u, mutated_pop[i], self.pop[i])\n        return crossed_pop\n\n    def local_search(self, func):\n        new_pop = self.pop.copy()\n        for i in range(self.pop_size):\n            if np.random.rand() < self.local_search_prob:\n                noise = np.random.normal(0, 0.01, self.dim)\n                new_x = self.x_opt + noise\n                new_x = np.clip(new_x, func.bounds.lb, func.bounds.ub)\n                new_f = func(new_x)\n                self.eval_count += 1\n                if new_f < self.fitness[i]:\n                    new_pop[i] = new_x\n                    self.fitness[i] = new_f\n                    self.update_best(new_pop, self.fitness)\n        return new_pop\n\n    def selection(self, func, crossed_pop):\n        new_fitness = np.array([func(x) for x in crossed_pop])\n        self.eval_count += self.pop_size\n\n        improved = new_fitness < self.fitness\n        for i in range(self.pop_size):\n            if improved[i]:\n                self.success_F.append(self.archive_F[self.archive_idx])\n                self.success_Cr.append(self.archive_Cr[self.archive_idx])\n\n        self.pop[improved] = crossed_pop[improved]\n        self.fitness[improved] = new_fitness[improved]\n        self.update_best(self.pop, self.fitness)\n\n    def update_archive(self):\n        if len(self.success_F) > 0:\n            mean_F = np.mean(self.success_F)\n            mean_Cr = np.mean(self.success_Cr)\n            self.archive_F[self.archive_idx] = 0.9 * self.archive_F[self.archive_idx] + 0.1 * mean_F\n            self.archive_Cr[self.archive_idx] = 0.9 * self.archive_Cr[self.archive_idx] + 0.1 * mean_Cr\n        else:\n            self.archive_F[self.archive_idx] = self.F_initial\n            self.archive_Cr[self.archive_idx] = self.Cr_initial\n        self.archive_idx = (self.archive_idx + 1) % self.archive_size\n        self.success_F = []\n        self.success_Cr = []\n\n    def update_best(self, pop, fitness):\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = pop[best_index].copy()\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            F = self.archive_F[self.archive_idx]\n            Cr = self.archive_Cr[self.archive_idx]\n\n            mutated_pop = self.mutate(F)\n            mutated_pop = np.clip(mutated_pop, func.bounds.lb, func.bounds.ub)\n            crossed_pop = self.crossover(mutated_pop, Cr)\n            crossed_pop = np.clip(crossed_pop, func.bounds.lb, func.bounds.ub)\n            self.selection(func, crossed_pop)\n            self.pop = self.local_search(func)\n            self.update_archive()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDifferentialEvolutionLS scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["691c4b38-8a5b-4093-a961-45d187254028"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "68de4d74-153c-41a8-acd7-8056f8a3f504", "fitness": -Infinity, "name": "AdaptiveDifferentialEvolution", "description": "Adaptive Differential Evolution with dynamic population size, orthogonal initialization, and covariance matrix adaptation for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size_min=10, pop_size_max=100, F=0.5, Cr=0.9, stagnation_iters=50, initial_pop_factor=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_min = pop_size_min\n        self.pop_size_max = pop_size_max\n        self.pop_size = int(np.clip(initial_pop_factor * budget / dim, pop_size_min, pop_size_max))\n        self.F = F\n        self.Cr = Cr\n        self.stagnation_iters = stagnation_iters\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.no_improvement_count = 0\n        self.covariance_matrix = np.eye(dim)\n        self.mean = None\n        self.orthogonal_matrix = self._generate_orthogonal_matrix(dim)\n\n    def _generate_orthogonal_matrix(self, dim):\n        matrix = np.random.normal(size=(dim, dim))\n        q, _ = np.linalg.qr(matrix)\n        return q\n\n    def initialize_population(self, func):\n        # Orthogonal initialization\n        self.pop = np.zeros((self.pop_size, self.dim))\n        for i in range(self.pop_size):\n            self.pop[i] = func.bounds.lb + (func.bounds.ub - func.bounds.lb) * self.orthogonal_matrix[i % self.dim] * 0.5 + (func.bounds.ub + func.bounds.lb) * 0.5\n\n        self.pop = np.clip(self.pop, func.bounds.lb, func.bounds.ub)\n\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.update_best(self.pop, self.fitness)\n        self.mean = np.mean(self.pop, axis=0)\n\n    def mutate(self):\n        mutated_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.pop[idxs]\n            z = np.random.multivariate_normal(np.zeros(self.dim), self.covariance_matrix)\n            mutated_pop[i] = x_r1 + self.F * (x_r2 - x_r3) + 0.1 * z \n        return mutated_pop\n\n    def crossover(self, mutated_pop):\n        crossed_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            j_rand = np.random.randint(0, self.dim)\n            u = np.random.rand(self.dim) < self.Cr\n            u[j_rand] = True\n            orthogonal_vector = self.orthogonal_matrix[i % self.dim]\n            for j in range(self.dim):\n                if u[j]:\n                    crossed_pop[i, j] = mutated_pop[i, j]\n                else:\n                    crossed_pop[i, j] = self.pop[i, j]\n        return crossed_pop\n\n    def selection(self, func, crossed_pop):\n        new_fitness = np.array([func(x) for x in crossed_pop])\n        self.eval_count += self.pop_size\n\n        improved = new_fitness < self.fitness\n        self.pop[improved] = crossed_pop[improved]\n        self.fitness[improved] = new_fitness[improved]\n\n        self.update_best(self.pop, self.fitness)\n        \n        #Dynamic Population size\n        if self.f_opt < np.mean(self.fitness):\n          self.pop_size = min(self.pop_size + 1, self.pop_size_max)\n        else:\n          self.pop_size = max(self.pop_size - 1, self.pop_size_min)\n\n    def reinitialize_population(self, func):\n        self.pop = np.random.normal(loc=self.x_opt, scale=0.5, size=(self.pop_size, self.dim))\n        self.pop = np.clip(self.pop, func.bounds.lb, func.bounds.ub)\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.update_best(self.pop, self.fitness)\n        self.mean = np.mean(self.pop, axis=0)\n\n    def adapt_parameters(self):\n        if self.eval_count > self.pop_size:\n            diff = self.pop - self.mean\n            self.covariance_matrix = np.cov(diff.T) + np.eye(self.dim) * 1e-6\n            self.mean = np.mean(self.pop, axis=0)\n        #Adapt F and Cr\n        self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 0.9)\n        self.Cr = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0)\n\n\n    def update_best(self, pop, fitness):\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = pop[best_index].copy()\n            self.no_improvement_count = 0\n        else:\n            self.no_improvement_count += 1\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.no_improvement_count = 0\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            mutated_pop = self.mutate()\n            mutated_pop = np.clip(mutated_pop, func.bounds.lb, func.bounds.ub)\n            crossed_pop = self.crossover(mutated_pop)\n            crossed_pop = np.clip(crossed_pop, func.bounds.lb, func.bounds.ub)\n            self.selection(func, crossed_pop)\n\n            if self.no_improvement_count > self.stagnation_iters:\n                self.reinitialize_population(func)\n                self.adapt_parameters()\n                self.no_improvement_count = 0\n            elif self.eval_count % (self.budget // 10) == 0:\n                self.adapt_parameters()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "An exception occurred: index 10 is out of bounds for axis 0 with size 10.", "error": "", "parent_ids": ["691c4b38-8a5b-4093-a961-45d187254028"], "operator": null, "metadata": {}}
{"id": "940d6a88-0051-4818-a146-96f4ea4ab825", "fitness": 0.0, "name": "AdaptiveDifferentialEvolution", "description": "Adaptive Differential Evolution with a dynamically adjusted population size based on success rate and a local search operator triggered upon stagnation.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, F=0.5, Cr=0.9, stagnation_iters=50, success_threshold=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.initial_pop_size = initial_pop_size\n        self.F = F\n        self.Cr = Cr\n        self.stagnation_iters = stagnation_iters\n        self.success_threshold = success_threshold\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.no_improvement_count = 0\n        self.success_history = []\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.update_best(self.pop, self.fitness)\n\n    def mutate(self):\n        mutated_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.pop[idxs]\n            mutated_pop[i] = x_r1 + self.F * (x_r2 - x_r3)\n        return mutated_pop\n\n    def crossover(self, mutated_pop):\n        crossed_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            j_rand = np.random.randint(0, self.dim)\n            u = np.random.rand(self.dim) < self.Cr\n            u[j_rand] = True\n            for j in range(self.dim):\n                if u[j]:\n                    crossed_pop[i, j] = mutated_pop[i, j]\n                else:\n                    crossed_pop[i, j] = self.pop[i, j]\n        return crossed_pop\n\n    def selection(self, func, crossed_pop):\n        new_fitness = np.array([func(x) for x in crossed_pop])\n        self.eval_count += self.pop_size\n\n        improved = new_fitness < self.fitness\n        self.success_history.extend(improved)\n        self.pop[improved] = crossed_pop[improved]\n        self.fitness[improved] = new_fitness[improved]\n\n        self.update_best(self.pop, self.fitness)\n        return np.sum(improved)\n\n    def adjust_population_size(self):\n        if len(self.success_history) > self.initial_pop_size:\n            success_rate = np.mean(self.success_history[-self.initial_pop_size:])\n            if success_rate > self.success_threshold and self.pop_size < 2 * self.initial_pop_size:\n                self.pop_size = min(self.pop_size + self.initial_pop_size // 10, 2 * self.initial_pop_size)\n                print(f\"Increasing pop size to {self.pop_size}\")\n            elif success_rate < (1 - self.success_threshold) and self.pop_size > self.initial_pop_size // 2:\n                self.pop_size = max(self.pop_size - self.initial_pop_size // 10, self.initial_pop_size // 2)\n                print(f\"Decreasing pop size to {self.pop_size}\")\n            \n            # Cap pop size to be not less than 10\n            self.pop_size = max(10, self.pop_size)\n\n\n    def local_search(self, func):\n        print(\"Local search triggered\")\n        x_current = self.x_opt.copy()\n        step_size = 0.1\n        for _ in range(10):\n            x_new = x_current + np.random.normal(0, step_size, self.dim)\n            x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n            f_new = func(x_new)\n            self.eval_count += 1\n\n            if f_new < self.f_opt:\n                self.f_opt = f_new\n                self.x_opt = x_new.copy()\n                x_current = x_new\n                self.no_improvement_count = 0\n            else:\n                step_size *= 0.9  # Reduce step size if no improvement\n\n    def update_best(self, pop, fitness):\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = pop[best_index].copy()\n            self.no_improvement_count = 0\n        else:\n            self.no_improvement_count += 1\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.no_improvement_count = 0\n        self.success_history = []\n        self.pop_size = self.initial_pop_size #reset pop size\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            mutated_pop = self.mutate()\n            mutated_pop = np.clip(mutated_pop, func.bounds.lb, func.bounds.ub)\n            crossed_pop = self.crossover(mutated_pop)\n            crossed_pop = np.clip(crossed_pop, func.bounds.lb, func.bounds.ub)\n            num_improved = self.selection(func, crossed_pop)\n            \n            self.adjust_population_size()\n\n            if self.no_improvement_count > self.stagnation_iters:\n                self.local_search(func)\n                self.no_improvement_count = 0\n\n\n            # Resize population if needed\n            if self.pop.shape[0] != self.pop_size:\n              self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n              self.fitness = np.array([func(x) for x in self.pop])\n              self.eval_count += self.pop_size\n              self.update_best(self.pop, self.fitness)\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["691c4b38-8a5b-4093-a961-45d187254028"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "6f30eef0-0f2a-4090-985a-e54b99566131", "fitness": -Infinity, "name": "AdaptiveDEOrthoCauchy", "description": "Adaptive Differential Evolution with orthogonal design for population initialization and restart, using a weighted recombination and a Cauchy mutation.", "code": "import numpy as np\nfrom scipy.stats import cauchy\nfrom scipy.linalg import hadamard\n\nclass AdaptiveDEOrthoCauchy:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size_factor=2, memory_size=5, ortho_trials=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = int(pop_size * archive_size_factor)\n        self.F = 0.5\n        self.CR = 0.9\n        self.archive = []\n        self.memory_F = np.ones(memory_size) * 0.5\n        self.memory_CR = np.ones(memory_size) * 0.9\n        self.memory_idx = 0\n        self.epsilon = 1e-6\n        self.sf_history = []\n        self.scr_history = []\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.ortho_trials = ortho_trials # Number of orthogonal design trials for restart\n        self.memory_size = memory_size\n\n    def orthogonal_design(self, n, k):\n        \"\"\"Generate an orthogonal design matrix.\"\"\"\n        H = hadamard(k)\n        design = H[:n, :n]\n        return design\n\n    def initialize_population(self, func):\n        \"\"\"Initialize population using orthogonal design.\"\"\"\n        if self.pop_size <= self.dim:\n             self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n             return\n        \n        design = self.orthogonal_design(self.pop_size, self.pop_size)\n\n        # Scale and shift the orthogonal design to the search space\n        scaled_design = (design + 1) / 2  # Scale to [0, 1]\n        scaled_design = scaled_design * (func.bounds.ub - func.bounds.lb) + func.bounds.lb # Scale to [lb, ub]\n        self.population = scaled_design[:, :self.dim]\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        \n        if np.min(self.fitness) < self.f_opt:\n            self.f_opt = np.min(self.fitness)\n            self.x_opt = self.population[np.argmin(self.fitness)]\n\n        generation = 0\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size + len(self.archive), 3, replace=False)\n                \n                if idxs[0] < self.pop_size:\n                    x_r1 = self.population[idxs[0]]\n                else:\n                    x_r1 = self.archive[idxs[0] - self.pop_size]\n                    \n                if idxs[1] < self.pop_size:\n                    x_r2 = self.population[idxs[1]]\n                else:\n                    x_r2 = self.archive[idxs[1] - self.pop_size]\n                    \n                if idxs[2] < self.pop_size:\n                    x_r3 = self.population[idxs[2]]\n                else:\n                    x_r3 = self.archive[idxs[2] - self.pop_size]\n\n                # Adaptive F and CR\n                F = self.memory_F[self.memory_idx]\n                CR = self.memory_CR[self.memory_idx]\n                \n                # Weighted Recombination\n                weights = np.random.rand(self.dim)\n                weights /= np.sum(weights)\n                mutant = weights[0] * x_r1 + weights[1] * x_r2 + weights[2] * x_r3 + F * cauchy.rvs(loc=0, scale=0.1, size=self.dim)\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    # Archive update: Mirroring strategy\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        idx_replace = np.random.randint(self.archive_size)\n                        self.archive[idx_replace] = self.population[i].copy()\n\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    self.sf_history.append(F)\n                    self.scr_history.append(CR)\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Update memory\n            if self.sf_history and self.scr_history:\n                self.memory_F[self.memory_idx] = np.mean(np.clip(self.sf_history, 0.1, 0.9))\n                self.memory_CR[self.memory_idx] = np.mean(self.scr_history)\n            else:\n                self.memory_F[self.memory_idx] = 0.5\n                self.memory_CR[self.memory_idx] = 0.9\n                \n            self.sf_history = []\n            self.scr_history = []\n            self.memory_idx = (self.memory_idx + 1) % self.memory_size\n\n            generation += 1\n\n            # Restart strategy using orthogonal design\n            if generation % 500 == 0:\n                best_idx = np.argmin(self.fitness)\n                best_x = self.population[best_idx].copy()\n                best_fitness = self.fitness[best_idx]\n                \n                for _ in range(self.ortho_trials):\n                    self.initialize_population(func)\n                    new_fitness = np.array([func(x) for x in self.population])\n                    self.budget -= self.pop_size\n                    \n                    if np.min(new_fitness) < best_fitness:\n                        best_fitness = np.min(new_fitness)\n                        best_x = self.population[np.argmin(new_fitness)].copy()\n                        \n                    if best_fitness < self.f_opt:\n                        self.f_opt = best_fitness\n                        self.x_opt = best_x\n                        \n                # Replace the current population with the re-initialized population (with orthogonal design)\n                self.initialize_population(func)\n                self.fitness = np.array([func(x) for x in self.population])\n                self.budget -= self.pop_size\n                \n                if np.min(self.fitness) < self.f_opt:\n                    self.f_opt = np.min(self.fitness)\n                    self.x_opt = self.population[np.argmin(self.fitness)]\n                \n                self.population[0] = best_x # Elitism: keep the best solution\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "An exception occurred: name 'hadamard' is not defined.", "error": "", "parent_ids": ["fa72e339-35df-4b32-aa7c-feb13694331c"], "operator": null, "metadata": {}}
{"id": "22af2e87-89e9-414c-ae9f-422bdc74f9f9", "fitness": -Infinity, "name": "AdaptiveDifferentialEvolution", "description": "Adaptive Differential Evolution with a dynamically adjusted exploration factor based on success, restart mechanism using CMA-ES, and a more robust covariance matrix update.", "code": "import numpy as np\nfrom scipy.linalg import sqrtm\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size_init=50, F_init=0.5, Cr_init=0.9, stagnation_iters=50, exploration_factor_init=0.1, pop_size_min=10, exploration_factor_decay=0.99):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_init = pop_size_init\n        self.pop_size = pop_size_init\n        self.pop_size_min = pop_size_min\n        self.F = F_init\n        self.Cr = Cr_init\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.stagnation_iters = stagnation_iters\n        self.no_improvement_count = 0\n        self.exploration_factor = exploration_factor_init\n        self.exploration_factor_init = exploration_factor_init\n        self.exploration_factor_decay = exploration_factor_decay\n        self.covariance_matrix = np.eye(dim)  # Initialize covariance matrix\n        self.mean = None # Initialize mean\n        self.success_F = []\n        self.success_Cr = []\n        self.weights = []\n        self.archive = [] # Mirrored archive\n        self.archive_fitness = []\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.update_best(self.pop, self.fitness)\n        self.mean = np.mean(self.pop, axis=0)\n\n    def mutate(self, func):\n        mutated_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.pop[idxs]\n\n            # Covariance matrix adaptation\n            z = np.random.multivariate_normal(np.zeros(self.dim), self.covariance_matrix)\n            mutated_pop[i] = x_r1 + self.F * (x_r2 - x_r3) + self.exploration_factor * z\n\n            # Mirrored boundary handling\n            for j in range(self.dim):\n                if mutated_pop[i, j] < func.bounds.lb[j]:\n                    mutated_pop[i, j] = 2 * func.bounds.lb[j] - mutated_pop[i, j]\n                elif mutated_pop[i, j] > func.bounds.ub[j]:\n                    mutated_pop[i, j] = 2 * func.bounds.ub[j] - mutated_pop[i, j]\n\n        return mutated_pop\n\n    def crossover(self, mutated_pop):\n        crossed_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            j_rand = np.random.randint(0, self.dim)\n            u = np.random.rand(self.dim) < self.Cr\n            u[j_rand] = True  # Ensure at least one element is taken from mutated vector\n            crossed_pop[i] = np.where(u, mutated_pop[i], self.pop[i])\n        return crossed_pop\n\n    def selection(self, func, crossed_pop):\n        new_fitness = np.array([func(x) for x in crossed_pop])\n        self.eval_count += self.pop_size\n\n        improved = new_fitness < self.fitness\n        \n        # Store successful F and Cr values\n        if np.any(improved):\n            self.success_F.extend([self.F] * np.sum(improved))\n            self.success_Cr.extend([self.Cr] * np.sum(improved))\n            self.weights.extend(np.abs(new_fitness[improved] - self.fitness[improved]))\n            \n            # Update exploration factor based on success\n            self.exploration_factor *= (1 + 0.1 * np.sum(improved) / self.pop_size)  # Increase if successful\n            self.exploration_factor = min(self.exploration_factor, self.exploration_factor_init)  # Cap\n\n        else:\n            self.exploration_factor *= self.exploration_factor_decay # Decay if no improvement\n            \n        self.pop[improved] = crossed_pop[improved]\n        self.fitness[improved] = new_fitness[improved]\n\n        # Archive mirroring\n        for i in np.where(improved)[0]:\n                self.archive.append(crossed_pop[i])\n                self.archive_fitness.append(new_fitness[i])\n\n        if len(self.archive) > 2 * self.pop_size_init:\n            # Trim archive\n            worst_archive_index = np.argmax(self.archive_fitness)\n            del self.archive[worst_archive_index]\n            del self.archive_fitness[worst_archive_index]\n\n        self.update_best(self.pop, self.fitness)\n\n    def reinitialize_population(self, func):\n        # CMA-ES-like restart: Sample new population around the current best using covariance matrix\n        try:\n            A = sqrtm(self.covariance_matrix)\n        except np.linalg.LinAlgError:\n            A = np.eye(self.dim)  # Fallback to identity matrix if Cholesky decomposition fails\n        \n        new_pop = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n        new_pop = self.x_opt + new_pop @ A.T\n        new_pop = np.clip(new_pop, func.bounds.lb, func.bounds.ub)\n            \n        self.pop = new_pop\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.update_best(self.pop, self.fitness)\n        self.mean = np.mean(self.pop, axis=0)\n\n\n\n    def adapt_parameters(self):\n        # Update F and Cr based on successful values\n        if self.success_F and self.success_Cr:\n            self.success_F = np.array(self.success_F)\n            self.success_Cr = np.array(self.success_Cr)\n            self.weights = np.array(self.weights) / np.sum(self.weights)\n            \n            self.F = np.sum(self.weights * self.success_F)\n            self.Cr = np.sum(self.weights * self.success_Cr)\n            \n            self.F = np.clip(self.F, 0.1, 1.0)\n            self.Cr = np.clip(self.Cr, 0.1, 1.0)\n\n            self.success_F = []\n            self.success_Cr = []\n            self.weights = []\n        else:\n             self.F = np.clip(np.random.normal(0.5, 0.3), 0.1, 1.0)\n             self.Cr = np.clip(np.random.normal(0.9, 0.2), 0.1, 1.0)\n\n\n        # Update covariance matrix (robust adaptation)\n        if self.eval_count > self.pop_size:  # Start after initial population\n            diff = self.pop - self.mean\n            # Use a shrinkage estimator to regularize the covariance matrix\n            # This helps to avoid ill-conditioning, especially in high dimensions\n            shrinkage = min(1, (self.dim + 2) / (self.eval_count - 1)) \n            sample_cov = np.cov(diff.T)\n            self.covariance_matrix = (1 - shrinkage) * sample_cov + shrinkage * np.eye(self.dim) * np.var(self.pop) + np.eye(self.dim) * 1e-6\n            self.mean = np.mean(self.pop, axis=0)\n\n    def update_best(self, pop, fitness):\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = pop[best_index].copy()\n            self.no_improvement_count = 0\n        else:\n            self.no_improvement_count += 1\n\n    def adapt_population_size(self):\n        if self.no_improvement_count > self.stagnation_iters:\n            self.pop_size = max(int(self.pop_size * 0.9), self.pop_size_min)  # Reduce population size\n        elif self.eval_count % (self.budget // 5) == 0 and self.pop_size < self.pop_size_init:\n            self.pop_size = min(self.pop_size + 5, self.pop_size_init) # Increase population size slowly\n    \n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.no_improvement_count = 0\n        self.pop_size = self.pop_size_init # Reset population size at each call\n        self.exploration_factor = self.exploration_factor_init # Reset Exploration factor\n        self.success_F = []\n        self.success_Cr = []\n        self.weights = []\n        self.archive = []\n        self.archive_fitness = []\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            mutated_pop = self.mutate(func)\n            crossed_pop = self.crossover(mutated_pop)\n            self.selection(func, crossed_pop)\n\n            if self.no_improvement_count > self.stagnation_iters:\n                self.reinitialize_population(func)\n                self.adapt_parameters()\n                self.no_improvement_count = 0\n            elif self.eval_count % (self.budget // 10) == 0:\n                self.adapt_parameters()\n            \n            self.adapt_population_size()\n            \n            # Ensure population size remains consistent\n            if self.pop.shape[0] != self.pop_size:\n                self.pop = self.pop[:self.pop_size]\n                self.fitness = self.fitness[:self.pop_size]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "An exception occurred: name 'sqrtm' is not defined.", "error": "", "parent_ids": ["88de00e9-d594-49ef-ac5f-56b0ea0ada80"], "operator": null, "metadata": {}}
{"id": "ed022771-8ab0-45c4-bc31-eaf3e58a1619", "fitness": 0.5669723128675781, "name": "AdaptiveDifferentialEvolution", "description": "Streamlined Adaptive Differential Evolution with simplified parameter adaptation based on population fitness improvement for faster convergence.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=50, Cr=0.5, F=0.5, p_best=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.Cr = Cr  # Crossover rate\n        self.F = F  # Mutation factor\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.p_best = p_best # percentage of best individuals to choose from\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        \n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.pop[best_index].copy()\n\n    def mutate(self):\n        mutated_pop = np.zeros_like(self.pop)\n        top_indices = np.argsort(self.fitness)[:int(self.p_best * self.pop_size)]\n\n        for i in range(self.pop_size):\n            pbest_idx = top_indices[np.random.randint(len(top_indices))]\n            idxs = np.random.choice(self.pop_size, 2, replace=False)\n            x_r1, x_r2 = self.pop[idxs]\n            mutated_pop[i] = self.pop[i] + self.F * (self.pop[pbest_idx] - self.pop[i]) + self.F * (x_r1 - x_r2)\n        return mutated_pop\n\n    def crossover(self, mutated_pop):\n        crossed_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            j_rand = np.random.randint(0, self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.Cr or j == j_rand:\n                    crossed_pop[i, j] = mutated_pop[i, j]\n                else:\n                    crossed_pop[i, j] = self.pop[i, j]\n        return crossed_pop\n\n    def selection(self, func, crossed_pop):\n        new_fitness = np.array([func(x) for x in crossed_pop])\n        self.eval_count += self.pop_size\n\n        improved = new_fitness < self.fitness\n        \n        self.pop[improved] = crossed_pop[improved]\n        self.fitness[improved] = new_fitness[improved]\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.pop[best_index].copy()\n            \n        # Simplified adaptation of Cr and F based on improvement rate\n        improvement_rate = np.mean(improved)\n        self.Cr = np.clip(self.Cr + 0.1 * (improvement_rate - 0.5), 0.1, 0.9)\n        self.F = np.clip(self.F + 0.1 * (improvement_rate - 0.5), 0.1, 0.9)\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.initialize_population(func)\n        \n        while self.eval_count < self.budget:\n            mutated_pop = self.mutate()\n            mutated_pop = np.clip(mutated_pop, func.bounds.lb, func.bounds.ub)\n            crossed_pop = self.crossover(mutated_pop)\n            crossed_pop = np.clip(crossed_pop, func.bounds.lb, func.bounds.ub)\n            self.selection(func, crossed_pop)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.567 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["04c74379-cb43-4889-8715-4f111571db19"], "operator": null, "metadata": {"aucs": [0.15176131659574166, 0.2555985322413066, 0.6963859421083696, 0.8511501197474699, 0.5306567685340222, 0.8343947130736655, 0.3674192190796164, 0.5372691115073149, 0.6430173202095992, 0.20550923583609026, 0.8083064833183705, 0.9952668355163241, 0.2926316709906075, 0.5322984402789461, 0.8038271721722186, 0.8126567994243791, 0.4233159386130899, 0.8696594724242451, 0.2244007947547294, 0.5039203709254567]}}
{"id": "ec335100-5a21-4a26-995d-fa20f7ef85c0", "fitness": 0.36498299301006587, "name": "SelfOrganizingDE", "description": "Differential Evolution with a self-organizing population and a Cauchy mutation operator, focusing on adaptive exploration and exploitation.", "code": "import numpy as np\n\nclass SelfOrganizingDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, cauchy_gamma=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.archive = []\n        self.cauchy_gamma = cauchy_gamma # Scale parameter for Cauchy distribution\n        self.min_pop_size = 10\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.pop[best_index].copy()\n\n    def mutate(self, func):\n        mutated_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.pop[idxs]\n            # Cauchy mutation\n            cauchy_sample = np.random.standard_cauchy(size=self.dim) * self.cauchy_gamma\n            mutated_pop[i] = self.pop[i] + cauchy_sample + (x_r1 - x_r2 + x_r3 - self.pop[i])\n\n            mutated_pop[i] = np.clip(mutated_pop[i], func.bounds.lb, func.bounds.ub) # Clip after mutation\n        return mutated_pop\n\n    def crossover(self, mutated_pop):\n        crossed_pop = np.zeros_like(self.pop)\n        Cr = np.random.uniform(0.2, 0.9) # adaptive Cr\n        for i in range(self.pop_size):\n            j_rand = np.random.randint(0, self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < Cr or j == j_rand:\n                    crossed_pop[i, j] = mutated_pop[i, j]\n                else:\n                    crossed_pop[i, j] = self.pop[i, j]\n        return crossed_pop\n\n    def selection(self, func, crossed_pop):\n        new_fitness = np.array([func(x) for x in crossed_pop])\n        self.eval_count += self.pop_size\n\n        improved = new_fitness < self.fitness\n        self.pop[improved] = crossed_pop[improved]\n        self.fitness[improved] = new_fitness[improved]\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.pop[best_index].copy()\n\n        # Self-organizing population size\n        worst_fitness = np.max(self.fitness)\n        if np.sum(self.fitness < worst_fitness) < self.min_pop_size:\n            # If population diversity is low, increase pop_size gradually, but ensure the budget is not exceeded.\n            new_pop_size = min(self.pop_size + 5, int(self.budget / 10))  # Incremental increase in pop_size\n            if new_pop_size > self.pop_size:\n                num_new_individuals = new_pop_size - self.pop_size\n                new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(num_new_individuals, self.dim))\n                new_fitness_vals = np.array([func(x) for x in new_individuals])\n                self.eval_count += num_new_individuals\n\n                self.pop = np.vstack((self.pop, new_individuals))\n                self.fitness = np.concatenate((self.fitness, new_fitness_vals))\n                self.pop_size = new_pop_size\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            mutated_pop = self.mutate(func)\n            crossed_pop = self.crossover(mutated_pop)\n            self.selection(func, crossed_pop)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm SelfOrganizingDE scored 0.365 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["04c74379-cb43-4889-8715-4f111571db19"], "operator": null, "metadata": {"aucs": [0.1455643348133948, 0.19851515361784844, 0.3465349848035989, 0.41847792960997954, 0.2855196146309087, 0.3729588921997754, 0.29233848014799746, 0.3130587847651368, 0.2880462790602877, 0.22878863197865007, 0.35856436372495915, 0.9881776871616283, 0.2698178865412122, 0.2972738605100329, 0.7028338045614644, 0.37092688377738436, 0.298102774495423, 0.439061309720976, 0.1989406590652597, 0.48615754501540054]}}
{"id": "b76ef103-52a3-4e96-abdc-fcc80142fcf2", "fitness": 0.40158941963614314, "name": "AdaptiveDifferentialEvolution", "description": "Adaptive Differential Evolution with a Cauchy mutation, orthogonal crossover, archive for enhanced exploration, and adaptive restart mechanism.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, Cr=0.9, stagnation_iters=50, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.Cr = Cr\n        self.stagnation_iters = stagnation_iters\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.no_improvement_count = 0\n        self.archive_size = archive_size\n        self.archive = []\n        self.archive_fitness = []\n        self.orthogonal_matrix = self._generate_orthogonal_matrix(dim)\n\n    def _generate_orthogonal_matrix(self, dim):\n        matrix = np.random.normal(size=(dim, dim))\n        q, _ = np.linalg.qr(matrix)\n        return q\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.update_best(self.pop, self.fitness)\n\n    def mutate(self):\n        mutated_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 2, replace=False)\n            x_r1, x_r2 = self.pop[idxs]\n            # Cauchy mutation\n            z = np.random.standard_cauchy(size=self.dim)\n            mutated_pop[i] = self.pop[i] + self.F * (x_r1 - x_r2) + 0.01 * z\n\n        return mutated_pop\n\n    def crossover(self, mutated_pop):\n        crossed_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            j_rand = np.random.randint(0, self.dim)\n            u = np.random.rand(self.dim) < self.Cr\n            u[j_rand] = True\n            orthogonal_vector = self.orthogonal_matrix[i % self.dim]\n            for j in range(self.dim):\n                if u[j]:\n                    crossed_pop[i, j] = mutated_pop[i, j]\n                else:\n                    crossed_pop[i, j] = self.pop[i, j]\n        return crossed_pop\n\n    def selection(self, func, crossed_pop):\n        new_fitness = np.array([func(x) for x in crossed_pop])\n        self.eval_count += self.pop_size\n\n        for i in range(self.pop_size):\n            if new_fitness[i] < self.fitness[i]:\n                self.pop[i] = crossed_pop[i]\n                self.fitness[i] = new_fitness[i]\n                self.update_archive(crossed_pop[i], new_fitness[i])\n            else:\n                # Add the replaced individual to the archive\n                self.update_archive(self.pop[i], self.fitness[i])\n\n        self.update_best(self.pop, self.fitness)\n\n    def update_archive(self, x, fitness):\n        if len(self.archive) < self.archive_size:\n            self.archive.append(x)\n            self.archive_fitness.append(fitness)\n        else:\n            max_index = np.argmax(self.archive_fitness)\n            if fitness < self.archive_fitness[max_index]:\n                self.archive[max_index] = x\n                self.archive_fitness[max_index] = fitness\n\n    def restart_population(self, func):\n        # Option 1: Restart around the best solution\n        new_pop = np.random.normal(loc=self.x_opt, scale=0.1, size=(self.pop_size, self.dim))\n        \n        # Option 2: Restart by randomly sampling from the archive and perturbing\n        if len(self.archive) > 0:\n            indices = np.random.choice(len(self.archive), self.pop_size, replace=True)\n            new_pop = np.array([self.archive[i] + np.random.normal(0, 0.05, self.dim) for i in indices])\n\n        new_pop = np.clip(new_pop, func.bounds.lb, func.bounds.ub)\n        new_fitness = np.array([func(x) for x in new_pop])\n        self.eval_count += self.pop_size\n\n        self.pop = new_pop\n        self.fitness = new_fitness\n        self.update_best(self.pop, self.fitness)\n\n    def update_best(self, pop, fitness):\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = pop[best_index].copy()\n            self.no_improvement_count = 0\n        else:\n            self.no_improvement_count += 1\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.no_improvement_count = 0\n        self.archive = []\n        self.archive_fitness = []\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            mutated_pop = self.mutate()\n            mutated_pop = np.clip(mutated_pop, func.bounds.lb, func.bounds.ub)\n            crossed_pop = self.crossover(mutated_pop)\n            crossed_pop = np.clip(crossed_pop, func.bounds.lb, func.bounds.ub)\n            self.selection(func, crossed_pop)\n\n            if self.no_improvement_count > self.stagnation_iters:\n                self.restart_population(func)\n                self.no_improvement_count = 0\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.402 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["691c4b38-8a5b-4093-a961-45d187254028"], "operator": null, "metadata": {"aucs": [0.16841466113546089, 0.28341110615960585, 0.379863902387788, 0.4864167775072967, 0.30104978052897746, 0.41047440502682264, 0.2960853275316292, 0.3278612132517801, 0.319957647051069, 0.19275398609702288, 0.5187982373403736, 0.9982649834452229, 0.3171967434879115, 0.3171137753124593, 0.7594394378836709, 0.419223608977936, 0.32027598656988554, 0.509252896259218, 0.20276109764700878, 0.5031728191217226]}}
{"id": "7c31414b-2789-4bdc-807d-b8d18bc98ab7", "fitness": -Infinity, "name": "AdaptiveDifferentialEvolution", "description": "Adaptive Differential Evolution with rank-based mutation, archive-based parameter adaptation using weighted average, and dynamic population size adjustment.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=50, Cr_init=0.5, F_init=0.5, archive_size=10, p_best=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.Cr = Cr_init\n        self.F = F_init\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.archive_size = archive_size\n        self.archive = []\n        self.memory_Cr = np.ones(archive_size) * Cr_init\n        self.memory_F = np.ones(archive_size) * F_init\n        self.p_best = p_best\n        self.success_Cr = []\n        self.success_F = []\n        self.success_history = []\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        \n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.pop[best_index].copy()\n\n    def mutate(self):\n        mutated_pop = np.zeros_like(self.pop)\n        ranked_indices = np.argsort(self.fitness)\n\n        for i in range(self.pop_size):\n            pbest_idx = ranked_indices[np.random.randint(0, max(1, int(self.p_best * self.pop_size)))]  # Ensure at least one pbest\n            \n            idxs = np.random.choice(self.pop_size, 2, replace=False)\n            x_r1, x_r2 = self.pop[idxs]\n            mutated_pop[i] = self.pop[i] + self.F * (self.pop[pbest_idx] - self.pop[i]) + self.F * (x_r1 - x_r2)\n        return mutated_pop\n\n    def crossover(self, mutated_pop):\n        crossed_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            j_rand = np.random.randint(0, self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.Cr or j == j_rand:\n                    crossed_pop[i, j] = mutated_pop[i, j]\n                else:\n                    crossed_pop[i, j] = self.pop[i, j]\n        return crossed_pop\n\n    def selection(self, func, crossed_pop):\n        new_fitness = np.array([func(x) for x in crossed_pop])\n        self.eval_count += self.pop_size\n\n        improved = new_fitness < self.fitness\n        delta = self.fitness - new_fitness\n        \n        for i in range(self.pop_size):\n            if improved[i]:\n                self.success_Cr.append(self.Cr)\n                self.success_F.append(self.F)\n                self.success_history.append(delta[i])\n\n\n        self.pop[improved] = crossed_pop[improved]\n        self.fitness[improved] = new_fitness[improved]\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.pop[best_index].copy()\n            \n        self.update_parameters()\n        self.adjust_population_size()\n\n    def update_parameters(self):\n        if self.success_Cr:\n            self.Cr = np.mean(self.success_Cr)\n            self.F = np.mean(self.success_F)\n            # Reset success memory\n            self.success_Cr = []\n            self.success_F = []\n            self.success_history = []\n        else:\n            self.Cr = np.random.rand()\n            self.F = np.random.rand()\n        self.Cr = np.clip(self.Cr, 0.1, 0.9)\n        self.F = np.clip(self.F, 0.1, 0.9)\n        \n    def adjust_population_size(self):\n        if self.eval_count < self.budget / 3:\n            self.pop_size = min(200, self.pop_size + 5)\n        elif self.eval_count > 2 * self.budget / 3:\n            self.pop_size = max(10, self.pop_size - 5)\n        \n        if self.pop is not None:\n            # Resize population if needed\n            current_pop_size = self.pop.shape[0]\n            if self.pop_size != current_pop_size:\n                if self.pop_size > current_pop_size:\n                    # Add new random individuals\n                    new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size - current_pop_size, self.dim))\n                    self.pop = np.vstack((self.pop, new_individuals))\n                    self.fitness = np.concatenate((self.fitness, np.array([func(x) for x in new_individuals])))\n                    self.eval_count += self.pop_size - current_pop_size\n\n                else:\n                    # Remove worst individuals\n                    worst_indices = np.argsort(self.fitness)[current_pop_size - self.pop_size:]\n                    keep_indices = np.setdiff1d(np.arange(current_pop_size), worst_indices)\n                    self.pop = self.pop[keep_indices]\n                    self.fitness = self.fitness[keep_indices]\n        \n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.initialize_population(func)\n        \n        while self.eval_count < self.budget:\n            mutated_pop = self.mutate()\n            mutated_pop = np.clip(mutated_pop, func.bounds.lb, func.bounds.ub)\n            crossed_pop = self.crossover(mutated_pop)\n            crossed_pop = np.clip(crossed_pop, func.bounds.lb, func.bounds.ub)\n            self.selection(func, crossed_pop)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "An exception occurred: name 'func' is not defined.", "error": "", "parent_ids": ["04c74379-cb43-4889-8715-4f111571db19"], "operator": null, "metadata": {}}
{"id": "8c5f2918-87a1-4b69-ae81-a36b827ca1bb", "fitness": -Infinity, "name": "AdaptiveDifferentialEvolutionClustering", "description": "Adaptive Differential Evolution with orthogonal learning, covariance matrix adaptation with rank-one update, and population clustering for diversity.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass AdaptiveDifferentialEvolutionClustering:\n    def __init__(self, budget=10000, dim=10, pop_size_init=50, F_init=0.5, Cr_init=0.9, stagnation_iters=50, exploration_factor=0.1, pop_size_min=10, num_clusters=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_init = pop_size_init\n        self.pop_size = pop_size_init\n        self.pop_size_min = pop_size_min\n        self.F = F_init\n        self.Cr = Cr_init\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.stagnation_iters = stagnation_iters\n        self.no_improvement_count = 0\n        self.exploration_factor = exploration_factor\n        self.covariance_matrix = np.eye(dim)  # Initialize covariance matrix\n        self.mean = None # Initialize mean\n        self.success_F = []\n        self.success_Cr = []\n        self.weights = []\n        self.num_clusters = num_clusters\n        self.clusters = None\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.update_best(self.pop, self.fitness)\n        self.mean = np.mean(self.pop, axis=0)\n        self.update_clusters()\n\n\n    def mutate(self, func):\n        mutated_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.pop[idxs]\n            \n            # Covariance matrix adaptation\n            z = np.random.multivariate_normal(np.zeros(self.dim), self.covariance_matrix)\n            mutated_pop[i] = x_r1 + self.F * (x_r2 - x_r3) + self.exploration_factor * z\n\n            # Mirrored boundary handling\n            for j in range(self.dim):\n                if mutated_pop[i, j] < func.bounds.lb[j]:\n                    mutated_pop[i, j] = 2 * func.bounds.lb[j] - mutated_pop[i, j]\n                elif mutated_pop[i, j] > func.bounds.ub[j]:\n                    mutated_pop[i, j] = 2 * func.bounds.ub[j] - mutated_pop[i, j]\n                    \n        return mutated_pop\n\n    def crossover(self, mutated_pop):\n        crossed_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            j_rand = np.random.randint(0, self.dim)\n            u = np.random.rand(self.dim) < self.Cr\n            u[j_rand] = True  # Ensure at least one element is taken from mutated vector\n            crossed_pop[i] = np.where(u, mutated_pop[i], self.pop[i])\n\n        # Orthogonal Learning\n        for j in range(self.dim):\n            orthogonal_val = np.mean(crossed_pop[:, j])\n            crossed_pop[i, j] = orthogonal_val\n        return crossed_pop\n\n    def selection(self, func, crossed_pop):\n        new_fitness = np.array([func(x) for x in crossed_pop])\n        self.eval_count += self.pop_size\n\n        improved = new_fitness < self.fitness\n        \n        # Store successful F and Cr values\n        if np.any(improved):\n            self.success_F.extend([self.F] * np.sum(improved))\n            self.success_Cr.extend([self.Cr] * np.sum(improved))\n            self.weights.extend(np.abs(new_fitness[improved] - self.fitness[improved]))\n\n        self.pop[improved] = crossed_pop[improved]\n        self.fitness[improved] = new_fitness[improved]\n\n        self.update_best(self.pop, self.fitness)\n        self.update_clusters()\n\n    def reinitialize_population(self, func):\n         # Reinitialize around the best and sample from a distribution\n        self.pop = np.random.normal(loc=self.x_opt, scale=0.5, size=(self.pop_size, self.dim))\n        self.pop = np.clip(self.pop, func.bounds.lb, func.bounds.ub)\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.update_best(self.pop, self.fitness)\n        self.mean = np.mean(self.pop, axis=0)\n        self.update_clusters()\n\n\n    def adapt_parameters(self):\n        # Update F and Cr based on successful values\n        if self.success_F and self.success_Cr:\n            self.success_F = np.array(self.success_F)\n            self.success_Cr = np.array(self.success_Cr)\n            self.weights = np.array(self.weights) / np.sum(self.weights)\n            \n            self.F = np.sum(self.weights * self.success_F)\n            self.Cr = np.sum(self.weights * self.success_Cr)\n            \n            self.F = np.clip(self.F, 0.1, 1.0)\n            self.Cr = np.clip(self.Cr, 0.1, 1.0)\n\n            self.success_F = []\n            self.success_Cr = []\n            self.weights = []\n        else:\n             self.F = np.clip(np.random.normal(0.5, 0.3), 0.1, 1.0)\n             self.Cr = np.clip(np.random.normal(0.9, 0.2), 0.1, 1.0)\n\n\n        # Update covariance matrix (Rank-One Update)\n        if self.eval_count > self.pop_size:\n            diff = self.pop - self.mean\n            delta = np.mean(diff, axis=0)\n            self.covariance_matrix = (1 - 0.1) * self.covariance_matrix + 0.1 * np.outer(delta, delta) + np.eye(self.dim) * 1e-6 # Adding a small value to avoid singular matrix\n            self.mean = np.mean(self.pop, axis=0)\n\n\n    def update_best(self, pop, fitness):\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = pop[best_index].copy()\n            self.no_improvement_count = 0\n        else:\n            self.no_improvement_count += 1\n\n    def adapt_population_size(self):\n        if self.no_improvement_count > self.stagnation_iters:\n            self.pop_size = max(int(self.pop_size * 0.9), self.pop_size_min)  # Reduce population size\n        elif self.eval_count % (self.budget // 5) == 0 and self.pop_size < self.pop_size_init:\n            self.pop_size = min(self.pop_size + 5, self.pop_size_init) # Increase population size slowly\n    \n    def update_clusters(self):\n        kmeans = KMeans(n_clusters=self.num_clusters, random_state=0, n_init = 'auto')\n        self.clusters = kmeans.fit_predict(self.pop)\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.no_improvement_count = 0\n        self.pop_size = self.pop_size_init # Reset population size at each call\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            mutated_pop = self.mutate(func)\n            crossed_pop = self.crossover(mutated_pop)\n            self.selection(func, crossed_pop)\n\n            if self.no_improvement_count > self.stagnation_iters:\n                self.reinitialize_population(func)\n                self.adapt_parameters()\n                self.no_improvement_count = 0\n            elif self.eval_count % (self.budget // 10) == 0:\n                self.adapt_parameters()\n            \n            self.adapt_population_size()\n            \n            # Ensure population size remains consistent\n            if self.pop.shape[0] != self.pop_size:\n                self.pop = self.pop[:self.pop_size]\n                self.fitness = self.fitness[:self.pop_size]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "An exception occurred: name 'KMeans' is not defined.", "error": "", "parent_ids": ["88de00e9-d594-49ef-ac5f-56b0ea0ada80"], "operator": null, "metadata": {}}
{"id": "f2782408-8fa6-4163-bb7e-9be6d79984bf", "fitness": 0.3557269658833534, "name": "AdaptiveDEOrthoRestart", "description": "Adaptive Differential Evolution with orthogonal learning, covariance matrix adaptation for mutation, and a restart mechanism based on stagnation detection to enhance exploration.", "code": "import numpy as np\n\nclass AdaptiveDEOrthoRestart:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size_factor=2, memory_size=5, stagnation_threshold=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = int(pop_size * archive_size_factor)\n        self.F = 0.5\n        self.CR = 0.9\n        self.archive = []\n        self.memory_F = np.ones(memory_size) * 0.5\n        self.memory_CR = np.ones(memory_size) * 0.9\n        self.memory_idx = 0\n        self.epsilon = 1e-6\n        self.sf_history = []\n        self.scr_history = []\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.cov = np.eye(dim)  # Covariance matrix for mutation\n        self.cov_update_interval = 50  # Update covariance matrix every n iterations\n        self.iter_count = 0\n        self.memory_size = memory_size\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n\n    def orthogonal_crossover(self, x, mutant):\n        # Generate an orthogonal matrix (e.g., using Hadamard matrix) for crossover\n        if self.dim == 2:\n            hadamard_matrix = np.array([[1, 1], [1, -1]])\n        elif self.dim == 4:\n            hadamard_matrix = np.array([[1, 1, 1, 1], [1, -1, 1, -1], [1, 1, -1, -1], [1, -1, -1, 1]])\n        else:  # Fallback to simple uniform crossover if Hadamard matrix is not available\n            return np.where(np.random.rand(self.dim) < 0.5, mutant, x)\n\n        if self.dim in [2, 4]:\n            trial = np.copy(x)\n            for i in range(self.dim):\n                trial[i] = 0.5 * (x[i] + mutant[i]) + 0.5 * hadamard_matrix[i % self.dim][i // (self.dim // hadamard_matrix.shape[1])] * (x[i] - mutant[i])\n\n            return trial\n        else:\n            return np.where(np.random.rand(self.dim) < 0.5, mutant, x)\n        \n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        \n        if np.min(self.fitness) < self.f_opt:\n            self.f_opt = np.min(self.fitness)\n            self.x_opt = self.population[np.argmin(self.fitness)]\n\n        self.best_fitness_history.append(self.f_opt)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size + len(self.archive), 3, replace=False)\n                \n                if idxs[0] < self.pop_size:\n                    x_r1 = self.population[idxs[0]]\n                else:\n                    x_r1 = self.archive[idxs[0] - self.pop_size]\n                    \n                if idxs[1] < self.pop_size:\n                    x_r2 = self.population[idxs[1]]\n                else:\n                    x_r2 = self.archive[idxs[1] - self.pop_size]\n                    \n                if idxs[2] < self.pop_size:\n                    x_r3 = self.population[idxs[2]]\n                else:\n                    x_r3 = self.archive[idxs[2] - self.pop_size]\n\n                # Adaptive F and CR\n                F = self.memory_F[self.memory_idx]\n                CR = self.memory_CR[self.memory_idx]\n                \n                mu = np.zeros(self.dim)\n                mutant = x_r1 + F * (x_r2 - x_r3) + np.random.multivariate_normal(mu, self.cov)\n                #mutant = x_r1 + F * (x_r2 - x_r3)\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                #j_rand = np.random.randint(self.dim)\n                #trial = np.copy(self.population[i])\n                #for j in range(self.dim):\n                #    if np.random.rand() < CR or j == j_rand:\n                #        trial[j] = mutant[j]\n                \n                trial = self.orthogonal_crossover(self.population[i], mutant)\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    # Archive update: Mirroring strategy\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        idx_replace = np.random.randint(self.archive_size)\n                        self.archive[idx_replace] = self.population[i].copy()\n\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    self.sf_history.append(F)\n                    self.scr_history.append(CR)\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Update memory\n            if self.sf_history and self.scr_history:\n                self.memory_F[self.memory_idx] = np.mean(np.clip(self.sf_history, 0.1, 0.9))\n                self.memory_CR[self.memory_idx] = np.mean(self.scr_history)\n            else:\n                self.memory_F[self.memory_idx] = 0.5\n                self.memory_CR[self.memory_idx] = 0.9\n                \n            self.sf_history = []\n            self.scr_history = []\n            self.memory_idx = (self.memory_idx + 1) % self.memory_size\n            \n            #Update covariance matrix\n            self.iter_count += 1\n            if self.iter_count % self.cov_update_interval == 0:\n                # Calculate covariance matrix from the current population\n                self.cov = np.cov(self.population.T)\n                # Add a small value to the diagonal to ensure it is positive definite\n                self.cov += np.eye(self.dim) * 1e-6\n\n            # Stagnation detection and restart\n            if self.f_opt >= self.best_fitness_history[-1]:\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n\n            self.best_fitness_history.append(self.f_opt)\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                # Restart the population\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.budget -= self.pop_size\n                self.stagnation_counter = 0\n                self.cov = np.eye(self.dim)  # Reset covariance matrix\n                if np.min(self.fitness) < self.f_opt:\n                    self.f_opt = np.min(self.fitness)\n                    self.x_opt = self.population[np.argmin(self.fitness)]\n                self.best_fitness_history.append(self.f_opt)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDEOrthoRestart scored 0.356 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["fa72e339-35df-4b32-aa7c-feb13694331c"], "operator": null, "metadata": {"aucs": [0.15290449662290828, 0.2324048820904141, 0.3413061467651619, 0.3352787551107064, 0.27500753496120456, 0.35507072631883163, 0.294776292815513, 0.3070769685085203, 0.28099945112342306, 0.20258254755001182, 0.3289201709737738, 0.9902607204135171, 0.2695382811048651, 0.2964135567401046, 0.7003872996705182, 0.36306035434396566, 0.3055739522517803, 0.4112488470300065, 0.18497679751626406, 0.4867515357555764]}}
{"id": "976fbf6c-f487-4533-b0ad-105e58f866a0", "fitness": -Infinity, "name": "EnhancedSelfOrganizingDE", "description": "Enhanced Self-Organizing DE with adaptive Cauchy mutation scale, orthogonal crossover for improved exploration, and population size adjustment based on stagnation and diversity.", "code": "import numpy as np\n\nclass EnhancedSelfOrganizingDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, initial_cauchy_gamma=0.1, gamma_decay=0.99):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.archive = []\n        self.cauchy_gamma = initial_cauchy_gamma  # Initial scale parameter for Cauchy distribution\n        self.gamma_decay = gamma_decay # Decay rate for Cauchy scale\n        self.min_pop_size = 10\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 50 # Number of iterations without improvement before considering stagnation\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.pop[best_index].copy()\n\n    def mutate(self, func):\n        mutated_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.pop[idxs]\n            # Cauchy mutation with adaptive scale\n            cauchy_sample = np.random.standard_cauchy(size=self.dim) * self.cauchy_gamma\n            mutated_pop[i] = self.pop[i] + cauchy_sample + (x_r1 - x_r2 + x_r3 - self.pop[i])\n\n            mutated_pop[i] = np.clip(mutated_pop[i], func.bounds.lb, func.bounds.ub)  # Clip after mutation\n        return mutated_pop\n\n    def orthogonal_crossover(self, mutated_pop):\n        crossed_pop = np.copy(self.pop)\n        for i in range(self.pop_size):\n            # Select parents for orthogonal crossover\n            parents = np.random.choice(self.pop_size, 2, replace=False)\n            x1, x2 = self.pop[parents[0]], self.pop[parents[1]]\n            \n            # Create an orthogonal array (example: 2-level full factorial design)\n            oa = np.array([[-1, -1], [1, -1], [-1, 1], [1, 1]])\n\n            # Generate candidate solutions based on the orthogonal array\n            candidates = np.zeros((oa.shape[0], self.dim))\n            for j in range(oa.shape[0]):\n                candidates[j] = 0.5 * ((1 + oa[j, 0]) * x1 + (1 + oa[j, 1]) * x2)\n                candidates[j] = np.clip(candidates[j], func.bounds.lb, func.bounds.ub)\n            \n            # Evaluate the candidates and select the best one\n            candidate_fitness = np.array([func(x) for x in candidates])\n            self.eval_count += oa.shape[0]\n            best_candidate_index = np.argmin(candidate_fitness)\n            \n            if candidate_fitness[best_candidate_index] < self.fitness[i]:\n                crossed_pop[i] = candidates[best_candidate_index]\n\n        return crossed_pop\n\n    def selection(self, func, crossed_pop):\n        new_fitness = np.array([func(x) for x in crossed_pop])\n        self.eval_count += self.pop_size\n\n        improved = new_fitness < self.fitness\n        self.pop[improved] = crossed_pop[improved]\n        self.fitness[improved] = new_fitness[improved]\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.pop[best_index].copy()\n            self.stagnation_counter = 0 # Reset stagnation counter\n\n        else:\n            self.stagnation_counter += 1\n\n        # Self-organizing population size\n        worst_fitness = np.max(self.fitness)\n        if np.sum(self.fitness < worst_fitness) < self.min_pop_size or self.stagnation_counter > self.stagnation_threshold:\n            # If population diversity is low, increase pop_size gradually, but ensure the budget is not exceeded.\n            new_pop_size = min(self.pop_size + 5, int(self.budget / 10))  # Incremental increase in pop_size\n            if new_pop_size > self.pop_size:\n                num_new_individuals = new_pop_size - self.pop_size\n                new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(num_new_individuals, self.dim))\n                new_fitness_vals = np.array([func(x) for x in new_individuals])\n                self.eval_count += num_new_individuals\n\n                self.pop = np.vstack((self.pop, new_individuals))\n                self.fitness = np.concatenate((self.fitness, new_fitness_vals))\n                self.pop_size = new_pop_size\n\n        #Reduce Cauchy noise\n        self.cauchy_gamma *= self.gamma_decay\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            mutated_pop = self.mutate(func)\n            crossed_pop = self.orthogonal_crossover(mutated_pop) # Use orthogonal crossover\n            self.selection(func, crossed_pop)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "An exception occurred: name 'func' is not defined.", "error": "", "parent_ids": ["ec335100-5a21-4a26-995d-fa20f7ef85c0"], "operator": null, "metadata": {}}
{"id": "bfb9c315-a8d2-46be-83e6-21ddde1ec742", "fitness": 0.13401254590633174, "name": "SimplifiedSelfOrganizingDE", "description": "Simplified Differential Evolution with adaptive population size adjustment based on a diversity metric and a simplified mutation strategy.", "code": "import numpy as np\n\nclass SimplifiedSelfOrganizingDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, diversity_threshold=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.diversity_threshold = diversity_threshold\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.min_pop_size = 10\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.pop[best_index].copy()\n\n    def mutate(self, func):\n        mutated_pop = np.zeros_like(self.pop)\n        F = 0.5 # Fixed mutation factor\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.pop[idxs]\n            mutated_pop[i] = self.pop[i] + F * (x_r1 - x_r2) # Simplified mutation\n\n            mutated_pop[i] = np.clip(mutated_pop[i], func.bounds.lb, func.bounds.ub) # Clip after mutation\n        return mutated_pop\n\n    def crossover(self, mutated_pop):\n        crossed_pop = np.zeros_like(self.pop)\n        Cr = 0.7 # Fixed Crossover rate\n        for i in range(self.pop_size):\n            j_rand = np.random.randint(0, self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < Cr or j == j_rand:\n                    crossed_pop[i, j] = mutated_pop[i, j]\n                else:\n                    crossed_pop[i, j] = self.pop[i, j]\n        return crossed_pop\n\n    def selection(self, func, crossed_pop):\n        new_fitness = np.array([func(x) for x in crossed_pop])\n        self.eval_count += self.pop_size\n\n        improved = new_fitness < self.fitness\n        self.pop[improved] = crossed_pop[improved]\n        self.fitness[improved] = new_fitness[improved]\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.pop[best_index].copy()\n\n    def adjust_population_size(self, func):\n        # Calculate population diversity (simplified as the variance of fitness values)\n        diversity = np.var(self.fitness)\n\n        if diversity < self.diversity_threshold:\n            # If population diversity is low, increase pop_size gradually, but ensure the budget is not exceeded.\n            new_pop_size = min(self.pop_size + 5, int(self.budget / 10))  # Incremental increase in pop_size\n            if new_pop_size > self.pop_size:\n                num_new_individuals = new_pop_size - self.pop_size\n                new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(num_new_individuals, self.dim))\n                new_fitness_vals = np.array([func(x) for x in new_individuals])\n                self.eval_count += num_new_individuals\n\n                self.pop = np.vstack((self.pop, new_individuals))\n                self.fitness = np.concatenate((self.fitness, new_fitness_vals))\n                self.pop_size = new_pop_size\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            mutated_pop = self.mutate(func)\n            crossed_pop = self.crossover(mutated_pop)\n            self.selection(func, crossed_pop)\n            self.adjust_population_size(func) # Adapt population size\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm SimplifiedSelfOrganizingDE scored 0.134 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ec335100-5a21-4a26-995d-fa20f7ef85c0"], "operator": null, "metadata": {"aucs": [0.14688295665947904, 0.2551546810595162, 0]}}
{"id": "4ec406d2-d293-4164-9d2c-4c6333a1b490", "fitness": -Infinity, "name": "AdaptiveDifferentialEvolution", "description": "Adaptive Differential Evolution with a learning rate that adapts based on recent success, along with dynamic population size and velocity clamping.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=50, Cr=0.5, F=0.5, p_best=0.1, lr=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.Cr = Cr  # Crossover rate\n        self.F = F  # Mutation factor\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.p_best = p_best # percentage of best individuals to choose from\n        self.lr = lr # Learning rate for Cr and F adaptation\n        self.archive = []\n        self.archive_fitness = []\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        \n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.pop[best_index].copy()\n\n    def mutate(self):\n        mutated_pop = np.zeros_like(self.pop)\n        top_indices = np.argsort(self.fitness)[:int(self.p_best * self.pop_size)]\n\n        for i in range(self.pop_size):\n            pbest_idx = top_indices[np.random.randint(len(top_indices))]\n            idxs = np.random.choice(self.pop_size, 2, replace=False)\n            x_r1, x_r2 = self.pop[idxs]\n            mutated_pop[i] = self.pop[i] + self.F * (self.pop[pbest_idx] - self.pop[i]) + self.F * (x_r1 - x_r2)\n\n            # Velocity clamping (optional, but can improve stability)\n            velocity = mutated_pop[i] - self.pop[i]\n            max_velocity = 0.1 * (func.bounds.ub - func.bounds.lb)  # Example velocity limit\n            velocity = np.clip(velocity, -max_velocity, max_velocity)\n            mutated_pop[i] = self.pop[i] + velocity\n        return mutated_pop\n\n    def crossover(self, mutated_pop):\n        crossed_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            j_rand = np.random.randint(0, self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.Cr or j == j_rand:\n                    crossed_pop[i, j] = mutated_pop[i, j]\n                else:\n                    crossed_pop[i, j] = self.pop[i, j]\n        return crossed_pop\n\n    def selection(self, func, crossed_pop):\n        new_fitness = np.array([func(x) for x in crossed_pop])\n        self.eval_count += self.pop_size\n\n        improved = new_fitness < self.fitness\n        \n        # Archive the replaced individuals\n        for i in np.where(improved)[0]:\n            self.archive.append(self.pop[i].copy())\n            self.archive_fitness.append(self.fitness[i])\n\n        self.pop[improved] = crossed_pop[improved]\n        self.fitness[improved] = new_fitness[improved]\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.pop[best_index].copy()\n            \n        # Adaptation of Cr and F based on improvement rate with learning rate\n        improvement_rate = np.mean(improved)\n        self.Cr = np.clip(self.Cr + self.lr * (improvement_rate - 0.5), 0.1, 0.9)\n        self.F = np.clip(self.F + self.lr * (improvement_rate - 0.5), 0.1, 0.9)\n\n        # Dynamic population size adjustment (example)\n        if self.eval_count % (self.budget // 10) == 0:\n            if improvement_rate < 0.1:\n                self.pop_size = int(self.pop_size * 0.9) # Reduce population size if not improving\n            elif improvement_rate > 0.3:\n                self.pop_size = int(self.pop_size * 1.1) # Increase population size if improving\n            self.pop_size = np.clip(self.pop_size, 10, 100) # Keep population size within bounds\n            \n            # Regenerate population, keeping the best individual\n            best_individual = self.pop[best_index].copy()\n            remaining_size = self.pop_size - 1\n            self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n            self.pop[0] = best_individual # Ensure the best solution is kept\n            \n            self.fitness = np.array([func(x) for x in self.pop])\n            self.eval_count += self.pop_size - 1 # correct eval count\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.pop[best_index].copy()\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.initialize_population(func)\n        \n        while self.eval_count < self.budget:\n            mutated_pop = self.mutate()\n            mutated_pop = np.clip(mutated_pop, func.bounds.lb, func.bounds.ub)\n            crossed_pop = self.crossover(mutated_pop)\n            crossed_pop = np.clip(crossed_pop, func.bounds.lb, func.bounds.ub)\n            self.selection(func, crossed_pop)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "An exception occurred: name 'func' is not defined.", "error": "", "parent_ids": ["ed022771-8ab0-45c4-bc31-eaf3e58a1619"], "operator": null, "metadata": {}}
{"id": "a35a296f-a55b-4a72-9b26-8b83450b1302", "fitness": -Infinity, "name": "EnhancedSelfOrganizingDE", "description": "Enhanced Self-Organizing DE with adaptive Cauchy mutation, orthogonal crossover, archive-based learning, and population diversity maintenance for improved exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedSelfOrganizingDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, initial_cauchy_gamma=0.1, orthogonal_components=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.archive = []\n        self.cauchy_gamma = initial_cauchy_gamma  # Initial scale parameter for Cauchy distribution\n        self.min_pop_size = 10\n        self.orthogonal_components = orthogonal_components # Number of components used in orthogonal crossover\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.pop[best_index].copy()\n        self.archive = list(self.pop.copy())\n\n    def adaptive_cauchy_mutation(self, func, iteration):\n        mutated_pop = np.zeros_like(self.pop)\n        # Adaptive Cauchy scale parameter\n        self.cauchy_gamma = 0.1 * np.exp(-iteration / (self.budget / (2 * self.pop_size)))\n\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.pop[idxs]\n            # Cauchy mutation\n            cauchy_sample = np.random.standard_cauchy(size=self.dim) * self.cauchy_gamma\n            mutated_pop[i] = self.pop[i] + cauchy_sample + (x_r1 - x_r2 + x_r3 - self.pop[i])\n\n            mutated_pop[i] = np.clip(mutated_pop[i], func.bounds.lb, func.bounds.ub)  # Clip after mutation\n        return mutated_pop\n\n    def orthogonal_crossover(self, mutated_pop):\n        crossed_pop = np.zeros_like(self.pop)\n        Cr = np.random.uniform(0.2, 0.9)  # adaptive Cr\n\n        for i in range(self.pop_size):\n            j_rand = np.random.randint(0, self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < Cr or j == j_rand:\n                    crossed_pop[i, j] = mutated_pop[i, j]\n                else:\n                    crossed_pop[i, j] = self.pop[i, j]\n\n            # Orthogonal Crossover\n            if np.random.rand() < 0.1: # Probability of applying orthogonal crossover\n                components = np.random.choice(self.dim, self.orthogonal_components, replace=False)\n                orthogonal_sample = np.random.uniform(func.bounds.lb[components], func.bounds.ub[components])\n                crossed_pop[i, components] = orthogonal_sample\n                \n        return crossed_pop\n\n    def selection(self, func, crossed_pop):\n        new_fitness = np.array([func(x) for x in crossed_pop])\n        self.eval_count += self.pop_size\n\n        improved = new_fitness < self.fitness\n        self.pop[improved] = crossed_pop[improved]\n        self.fitness[improved] = new_fitness[improved]\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.pop[best_index].copy()\n\n        # Update archive\n        for i in range(self.pop_size):\n            if crossed_pop[i] not in self.archive:\n                self.archive.append(crossed_pop[i].copy())\n                if len(self.archive) > self.archive_size:\n                    self.archive.pop(0) # FIFO\n\n        # Self-organizing population size and archive-based learning\n        worst_fitness = np.max(self.fitness)\n        if np.sum(self.fitness < worst_fitness) < self.min_pop_size:\n            # If population diversity is low, increase pop_size gradually, but ensure the budget is not exceeded.\n            new_pop_size = min(self.pop_size + 5, int(self.budget / 10))  # Incremental increase in pop_size\n            if new_pop_size > self.pop_size:\n                num_new_individuals = new_pop_size - self.pop_size\n\n                # Introduce individuals from archive to increase diversity\n                if len(self.archive) > 0:\n                   archive_indices = np.random.choice(len(self.archive), min(num_new_individuals, len(self.archive)), replace=False)\n                   new_individuals_from_archive = np.array([self.archive[i] for i in archive_indices])\n                   num_new_individuals -= len(new_individuals_from_archive)\n                else:\n                   new_individuals_from_archive = np.empty((0, self.dim))\n\n                new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(num_new_individuals, self.dim))\n                new_individuals = np.vstack((new_individuals, new_individuals_from_archive))\n\n                new_fitness_vals = np.array([func(x) for x in new_individuals])\n                self.eval_count += len(new_individuals)\n\n                self.pop = np.vstack((self.pop, new_individuals))\n                self.fitness = np.concatenate((self.fitness, new_fitness_vals))\n                self.pop_size = new_pop_size\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.initialize_population(func)\n\n        iteration = 0\n        while self.eval_count < self.budget:\n            mutated_pop = self.adaptive_cauchy_mutation(func, iteration)\n            crossed_pop = self.orthogonal_crossover(mutated_pop)\n            self.selection(func, crossed_pop)\n            iteration += 1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "An exception occurred: Cannot take a larger sample than population when 'replace=False'.", "error": "", "parent_ids": ["ec335100-5a21-4a26-995d-fa20f7ef85c0"], "operator": null, "metadata": {}}
{"id": "848106b0-14ce-4f03-b524-ebc34b5b2755", "fitness": -Infinity, "name": "DynPopAdaptiveDE", "description": "Adaptive Differential Evolution with a dynamically adjusted population size, mirrored boundary handling, and a fitness-difference-based mutation strategy for improved exploration and exploitation.", "code": "import numpy as np\n\nclass DynPopAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, archive_size_factor=2, memory_size=5, pop_size_adapt_freq=100, pop_size_scale=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.initial_pop_size = initial_pop_size\n        self.archive_size = int(initial_pop_size * archive_size_factor)\n        self.F = 0.5\n        self.CR = 0.9\n        self.archive = []\n        self.memory_F = np.ones(memory_size) * 0.5\n        self.memory_CR = np.ones(memory_size) * 0.9\n        self.memory_idx = 0\n        self.epsilon = 1e-6\n        self.sf_history = []\n        self.scr_history = []\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.pop_size_adapt_freq = pop_size_adapt_freq\n        self.pop_size_scale = pop_size_scale\n        self.iter_count = 0\n        self.memory_size = memory_size\n\n    def mirrored_boundary_handling(self, x):\n        x = np.where(x < self.bounds_lb, 2 * self.bounds_lb - x, x)\n        x = np.where(x > self.bounds_ub, 2 * self.bounds_ub - x, x)\n        return x\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        \n        if np.min(self.fitness) < self.f_opt:\n            self.f_opt = np.min(self.fitness)\n            self.x_opt = self.population[np.argmin(self.fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size + len(self.archive), 3, replace=False)\n                \n                if idxs[0] < self.pop_size:\n                    x_r1 = self.population[idxs[0]]\n                else:\n                    x_r1 = self.archive[idxs[0] - self.pop_size]\n                    \n                if idxs[1] < self.pop_size:\n                    x_r2 = self.population[idxs[1]]\n                else:\n                    x_r2 = self.archive[idxs[1] - self.pop_size]\n                    \n                if idxs[2] < self.pop_size:\n                    x_r3 = self.population[idxs[2]]\n                else:\n                    x_r3 = self.archive[idxs[2] - self.pop_size]\n                \n                # Adaptive F and CR\n                F = self.memory_F[self.memory_idx]\n                CR = self.memory_CR[self.memory_idx]\n                \n                # Fitness-difference-based mutation\n                fitness_diff = np.abs(self.fitness[idxs[1] if idxs[1] < self.pop_size else idxs[1]-self.pop_size] - self.fitness[idxs[2] if idxs[2] < self.pop_size else idxs[2]-self.pop_size])\n                mutant = x_r1 + F * fitness_diff * (x_r2 - x_r3)\n                \n                mutant = self.mirrored_boundary_handling(mutant)\n                mutant = np.clip(mutant, self.bounds_lb, self.bounds_ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                trial = self.mirrored_boundary_handling(trial)\n                trial = np.clip(trial, self.bounds_lb, self.bounds_ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    # Archive update: Mirroring strategy\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        idx_replace = np.random.randint(self.archive_size)\n                        self.archive[idx_replace] = self.population[i].copy()\n\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    self.sf_history.append(F)\n                    self.scr_history.append(CR)\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Update memory\n            if self.sf_history and self.scr_history:\n                self.memory_F[self.memory_idx] = np.mean(np.clip(self.sf_history, 0.1, 0.9))\n                self.memory_CR[self.memory_idx] = np.mean(self.scr_history)\n            else:\n                self.memory_F[self.memory_idx] = 0.5\n                self.memory_CR[self.memory_idx] = 0.9\n                \n            self.sf_history = []\n            self.scr_history = []\n            self.memory_idx = (self.memory_idx + 1) % self.memory_size\n            \n            # Adjust population size dynamically\n            self.iter_count += 1\n            if self.iter_count % self.pop_size_adapt_freq == 0:\n                # Adjust population size based on the improvement in fitness\n                improvement = np.mean(self.fitness) - self.f_opt\n                if improvement > 0:  # If there's improvement\n                    self.pop_size = int(max(10, self.pop_size * (1 - self.pop_size_scale)))  # Reduce pop size\n                else:\n                    self.pop_size = int(min(self.initial_pop_size * 2, self.pop_size * (1 + self.pop_size_scale)))  # Increase pop size\n                \n                # Ensure pop_size doesn't exceed budget\n                self.pop_size = min(self.pop_size, self.budget)\n\n                # Regenerate population\n                new_population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                new_fitness = np.array([func(x) for x in new_population])\n                self.budget -= self.pop_size\n\n                # Combine old and new population and select the best\n                combined_population = np.vstack((self.population, new_population))\n                combined_fitness = np.concatenate((self.fitness, new_fitness))\n                \n                sorted_indices = np.argsort(combined_fitness)\n                self.population = combined_population[sorted_indices[:self.pop_size]]\n                self.fitness = combined_fitness[sorted_indices[:self.pop_size]]\n                \n                if np.min(self.fitness) < self.f_opt:\n                    self.f_opt = np.min(self.fitness)\n                    self.x_opt = self.population[np.argmin(self.fitness)]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "An exception occurred: index 51 is out of bounds for axis 0 with size 50.", "error": "", "parent_ids": ["f2782408-8fa6-4163-bb7e-9be6d79984bf"], "operator": null, "metadata": {}}
{"id": "4cfcdc01-3f21-4548-80a7-dbbc75655ed0", "fitness": 0.0, "name": "AgingCauchyODE", "description": "Differential Evolution with an aging mechanism to remove stagnant individuals and a Cauchy mutation enhanced by opposition-based learning to improve exploration.", "code": "import numpy as np\n\nclass AgingCauchyODE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, cauchy_gamma=0.1, aging_rate=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.archive = []\n        self.cauchy_gamma = cauchy_gamma  # Scale parameter for Cauchy distribution\n        self.aging_rate = aging_rate\n        self.ages = None\n        self.min_pop_size = 10\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.ages = np.zeros(self.pop_size)\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.pop[best_index].copy()\n\n    def opposition_based_learning(self, x, func):\n        x_opp = func.bounds.lb + func.bounds.ub - x\n        return np.clip(x_opp, func.bounds.lb, func.bounds.ub)\n\n    def mutate(self, func):\n        mutated_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.pop[idxs]\n\n            # Cauchy mutation enhanced by opposition-based learning\n            cauchy_sample = np.random.standard_cauchy(size=self.dim) * self.cauchy_gamma\n            mutated_pop[i] = self.pop[i] + cauchy_sample + (x_r1 - x_r2 + x_r3 - self.pop[i])\n\n            # Apply opposition-based learning to the mutated vector\n            x_opp = self.opposition_based_learning(mutated_pop[i], func)\n            f_opp = func(x_opp)\n            self.eval_count += 1\n\n            if f_opp < func(mutated_pop[i]):\n                mutated_pop[i] = x_opp  # Replace with the opposite point if it's better\n\n            mutated_pop[i] = np.clip(mutated_pop[i], func.bounds.lb, func.bounds.ub)  # Clip after mutation\n        return mutated_pop\n\n    def crossover(self, mutated_pop):\n        crossed_pop = np.zeros_like(self.pop)\n        Cr = np.random.uniform(0.2, 0.9)  # adaptive Cr\n        for i in range(self.pop_size):\n            j_rand = np.random.randint(0, self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < Cr or j == j_rand:\n                    crossed_pop[i, j] = mutated_pop[i, j]\n                else:\n                    crossed_pop[i, j] = self.pop[i, j]\n        return crossed_pop\n\n    def selection(self, func, crossed_pop):\n        new_fitness = np.array([func(x) for x in crossed_pop])\n        self.eval_count += self.pop_size\n\n        improved = new_fitness < self.fitness\n        self.pop[improved] = crossed_pop[improved]\n        self.fitness[improved] = new_fitness[improved]\n        self.ages[improved] = 0  # Reset age for improved individuals\n        self.ages[~improved] += self.aging_rate  # Increase age for stagnant individuals\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.pop[best_index].copy()\n\n        # Aging mechanism: replace old individuals with new random ones\n        old_indices = np.where(self.ages > 1.0)[0]  # Individuals older than 1.0 are considered old\n        num_old = len(old_indices)\n        if num_old > 0:\n            new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(num_old, self.dim))\n            new_fitness_vals = np.array([func(x) for x in new_individuals])\n            self.eval_count += num_old\n\n            self.pop[old_indices] = new_individuals\n            self.fitness[old_indices] = new_fitness_vals\n            self.ages[old_indices] = 0  # Reset ages of replaced individuals\n\n        # Self-organizing population size - simplified version\n        if self.eval_count < self.budget / 2 and self.pop_size < int(self.budget / 20): # Early stage pop growth\n            self.pop_size = min(self.pop_size + 2, int(self.budget / 20)) # slowly increase pop size\n            new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(2, self.dim))\n            new_fitness_vals = np.array([func(x) for x in new_individuals])\n            self.eval_count += 2\n\n            self.pop = np.vstack((self.pop, new_individuals))\n            self.fitness = np.concatenate((self.fitness, new_fitness_vals))\n            self.ages = np.concatenate((self.ages, np.zeros(2))) # Initial age\n        elif self.eval_count > self.budget * 0.75 and self.pop_size > self.min_pop_size: # Late stage shrinking\n            self.pop_size = max(self.pop_size - 1, self.min_pop_size)\n            self.pop = self.pop[:self.pop_size]\n            self.fitness = self.fitness[:self.pop_size]\n            self.ages = self.ages[:self.pop_size]\n\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            mutated_pop = self.mutate(func)\n            crossed_pop = self.crossover(mutated_pop)\n            self.selection(func, crossed_pop)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AgingCauchyODE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ec335100-5a21-4a26-995d-fa20f7ef85c0"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "30d0d783-b7a5-44c8-91ac-796c98b07a84", "fitness": 0.4440514161803019, "name": "AdaptiveDifferentialEvolution", "description": "Adaptive Differential Evolution with simplified Cauchy mutation, orthogonal crossover, and a more focused restart strategy based on the best solution for efficiency.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, Cr=0.9, stagnation_iters=40):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.Cr = Cr\n        self.stagnation_iters = stagnation_iters\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.no_improvement_count = 0\n        self.orthogonal_matrix = self._generate_orthogonal_matrix(dim)\n\n    def _generate_orthogonal_matrix(self, dim):\n        matrix = np.random.normal(size=(dim, dim))\n        q, _ = np.linalg.qr(matrix)\n        return q\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.update_best(self.pop, self.fitness)\n\n    def mutate(self):\n        mutated_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 2, replace=False)\n            x_r1, x_r2 = self.pop[idxs]\n            # Simplified Cauchy mutation\n            z = np.random.standard_cauchy(size=self.dim)\n            mutated_pop[i] = self.pop[i] + self.F * (x_r1 - x_r2) + 0.005 * z\n\n        return mutated_pop\n\n    def crossover(self, mutated_pop):\n        crossed_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            j_rand = np.random.randint(0, self.dim)\n            u = np.random.rand(self.dim) < self.Cr\n            u[j_rand] = True\n            orthogonal_vector = self.orthogonal_matrix[i % self.dim]\n            for j in range(self.dim):\n                if u[j]:\n                    crossed_pop[i, j] = mutated_pop[i, j]\n                else:\n                    crossed_pop[i, j] = self.pop[i, j]\n        return crossed_pop\n\n    def selection(self, func, crossed_pop):\n        new_fitness = np.array([func(x) for x in crossed_pop])\n        self.eval_count += self.pop_size\n\n        for i in range(self.pop_size):\n            if new_fitness[i] < self.fitness[i]:\n                self.pop[i] = crossed_pop[i]\n                self.fitness[i] = new_fitness[i]\n\n        self.update_best(self.pop, self.fitness)\n\n    def restart_population(self, func):\n        # Restart around the best solution with a smaller scale\n        new_pop = np.random.normal(loc=self.x_opt, scale=0.05, size=(self.pop_size, self.dim))\n        new_pop = np.clip(new_pop, func.bounds.lb, func.bounds.ub)\n        new_fitness = np.array([func(x) for x in new_pop])\n        self.eval_count += self.pop_size\n\n        self.pop = new_pop\n        self.fitness = new_fitness\n        self.update_best(self.pop, self.fitness)\n\n    def update_best(self, pop, fitness):\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = pop[best_index].copy()\n            self.no_improvement_count = 0\n        else:\n            self.no_improvement_count += 1\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.no_improvement_count = 0\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            mutated_pop = self.mutate()\n            mutated_pop = np.clip(mutated_pop, func.bounds.lb, func.bounds.ub)\n            crossed_pop = self.crossover(mutated_pop)\n            crossed_pop = np.clip(crossed_pop, func.bounds.lb, func.bounds.ub)\n            self.selection(func, crossed_pop)\n\n            if self.no_improvement_count > self.stagnation_iters:\n                self.restart_population(func)\n                self.no_improvement_count = 0\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.444 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b76ef103-52a3-4e96-abdc-fcc80142fcf2"], "operator": null, "metadata": {"aucs": [0.1722883818407085, 0.30379435664814847, 0.40700513183078946, 0.6719686911888136, 0.31933402564047186, 0.44270540655403834, 0.297650568140681, 0.3674780442609813, 0.34908985061421305, 0.23917642730165012, 0.735212235263403, 0.9988800769102341, 0.37296160460409145, 0.3320900563884811, 0.7981815936567961, 0.45442966326698975, 0.3424244570090347, 0.5661868573961848, 0.2201590085969074, 0.49001188649342187]}}
{"id": "ccb1b87a-4c62-4fc7-9007-30f9ccd2ac9b", "fitness": 0.7685379597198381, "name": "AdaptiveDifferentialEvolution", "description": "Simplified Adaptive Differential Evolution with focus on direct mutation and crossover on the best performing individuals.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=40, Cr=0.7, F=0.6):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.Cr = Cr\n        self.F = F\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.pop[best_index].copy()\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            # Select parent\n            idx_p = np.argmin(self.fitness)\n            x_p = self.pop[idx_p]\n\n            # Mutation\n            idxs = np.random.choice(self.pop_size, 2, replace=False)\n            x_r1, x_r2 = self.pop[idxs]\n            x_mutated = x_p + self.F * (x_r1 - x_r2)\n            x_mutated = np.clip(x_mutated, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            x_trial = np.copy(self.pop[i])\n            j_rand = np.random.randint(0, self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.Cr or j == j_rand:\n                    x_trial[j] = x_mutated[j]\n\n            # Evaluation\n            f_trial = func(x_trial)\n            self.eval_count += 1\n\n            # Selection\n            if f_trial < self.fitness[i]:\n                self.pop[i] = x_trial\n                self.fitness[i] = f_trial\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = x_trial.copy()\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.769 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ed022771-8ab0-45c4-bc31-eaf3e58a1619"], "operator": null, "metadata": {"aucs": [0.5335584483596577, 0.8397562731007051, 0.8696118386945544, 0.9474669498080511, 0.8841838134048761, 0.9162211690624478, 0.3753981656533508, 0.85574556117454, 0.8883398453158874, 0.8630884472560442, 0.9306015817610406, 0.9989678812379871, 0.6998144675248805, 0.2958069529430317, 0.9615992206759266, 0.9037550246076332, 0.8307912760158754, 0.9354797480959792, 0.22147482291103593, 0.6190977067932578]}}
{"id": "aa1940c9-4f60-4d5b-87aa-41da3b9aad82", "fitness": 0.6545508997273968, "name": "AdaptiveDifferentialEvolution", "description": "Adaptive Differential Evolution with mirrored sampling, dynamic population size, and adaptive parameter control using success history.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size_init=50, Cr=0.5, F=0.5, p_best=0.1, pop_size_factor=2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_init = pop_size_init\n        self.pop_size = pop_size_init  # Dynamic population size\n        self.Cr = Cr  # Crossover rate\n        self.F = F  # Mutation factor\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.p_best = p_best # percentage of best individuals to choose from\n        self.pop_size_factor = pop_size_factor\n        self.success_Cr = []\n        self.success_F = []\n        self.memory_size = 10\n        self.memory_Cr = np.full(self.memory_size, Cr)\n        self.memory_F = np.full(self.memory_size, F)\n        self.memory_idx = 0\n\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        \n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.pop[best_index].copy()\n\n    def mutate(self, func):\n        mutated_pop = np.zeros_like(self.pop)\n        top_indices = np.argsort(self.fitness)[:int(self.p_best * self.pop_size)]\n\n        for i in range(self.pop_size):\n            pbest_idx = top_indices[np.random.randint(len(top_indices))]\n            idxs = np.random.choice(self.pop_size, 2, replace=False)\n            x_r1, x_r2 = self.pop[idxs]\n            \n            # Mirrored sampling to keep solutions within bounds\n            mutated_vector = self.pop[i] + self.F * (self.pop[pbest_idx] - self.pop[i]) + self.F * (x_r1 - x_r2)\n            \n            # Mirroring strategy\n            for j in range(self.dim):\n                if mutated_vector[j] < func.bounds.lb[j]:\n                    mutated_vector[j] = func.bounds.lb[j] + (func.bounds.lb[j] - mutated_vector[j])\n                elif mutated_vector[j] > func.bounds.ub[j]:\n                    mutated_vector[j] = func.bounds.ub[j] - (mutated_vector[j] - func.bounds.ub[j])\n                    \n            mutated_pop[i] = mutated_vector\n        return mutated_pop\n\n    def crossover(self, mutated_pop):\n        crossed_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            j_rand = np.random.randint(0, self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.Cr or j == j_rand:\n                    crossed_pop[i, j] = mutated_pop[i, j]\n                else:\n                    crossed_pop[i, j] = self.pop[i, j]\n        return crossed_pop\n\n    def selection(self, func, crossed_pop):\n        new_fitness = np.array([func(x) for x in crossed_pop])\n        self.eval_count += self.pop_size\n\n        improved = new_fitness < self.fitness\n        \n        # Store successful Cr and F values\n        for i in range(self.pop_size):\n            if improved[i]:\n                self.success_Cr.append(self.Cr)\n                self.success_F.append(self.F)\n        \n        self.pop[improved] = crossed_pop[improved]\n        self.fitness[improved] = new_fitness[improved]\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.pop[best_index].copy()\n            \n    def adapt_parameters(self):\n        # Update Cr and F using success history\n        if self.success_Cr:\n            self.Cr = np.median(self.success_Cr)\n        else:\n            self.Cr = self.memory_Cr[self.memory_idx] # Fallback to memory\n\n        if self.success_F:\n            self.F = np.median(self.success_F)\n        else:\n            self.F = self.memory_F[self.memory_idx] # Fallback to memory\n            \n        self.memory_Cr[self.memory_idx] = self.Cr\n        self.memory_F[self.memory_idx] = self.F\n        self.memory_idx = (self.memory_idx + 1) % self.memory_size\n            \n        self.success_Cr = []\n        self.success_F = []\n\n    def adapt_population_size(self):\n        # Dynamically adjust population size\n        if self.eval_count > self.budget // 2 and self.pop_size > self.pop_size_init:\n            self.pop_size = max(self.pop_size_init, self.pop_size // self.pop_size_factor)\n        elif self.eval_count < self.budget // 4 and self.pop_size < self.budget // 10:\n            self.pop_size = min(self.budget // 10, self.pop_size * self.pop_size_factor)\n\n        if self.pop_size != self.pop.shape[0]:\n            # Resize the population (crude and can be improved)\n            best_indices = np.argsort(self.fitness)[:min(self.pop_size, self.pop.shape[0])]  # Keep best individuals\n            new_pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n            new_fitness = np.full(self.pop_size, np.inf)\n\n            new_pop[:len(best_indices)] = self.pop[best_indices]\n            new_fitness[:len(best_indices)] = self.fitness[best_indices]\n            \n            self.pop = new_pop\n            self.fitness = new_fitness\n        \n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.initialize_population(func)\n        \n        while self.eval_count < self.budget:\n            #self.adapt_population_size() # Adjust pop size each generation\n            mutated_pop = self.mutate(func)\n            crossed_pop = self.crossover(mutated_pop)\n            self.selection(func, crossed_pop)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.655 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ed022771-8ab0-45c4-bc31-eaf3e58a1619"], "operator": null, "metadata": {"aucs": [0.2378260039813952, 0.5535609513368489, 0.7333336658817636, 0.8845945998012531, 0.78132243147913, 0.8326967125732088, 0.5679266199429649, 0.6736524760703702, 0.7809852099432052, 0.20458259405657986, 0.8675335606485861, 0.9938558785414634, 0.35725181953388385, 0.6881237639245211, 0.9089187285732194, 0.8135826094107361, 0.5865090939904374, 0.8751796305375243, 0.22676868161538122, 0.5228129627054614]}}
{"id": "bf1067da-0cb7-4968-b851-1c34d4f64830", "fitness": 0.0, "name": "AdaptiveDifferentialEvolution", "description": "Adaptive Differential Evolution with simplified parameter adaptation and a focus on exploitation by reducing population diversity.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=30, Cr=0.7, F=0.7, p_best=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.Cr = Cr  # Crossover rate\n        self.F = F  # Mutation factor\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.p_best = p_best # percentage of best individuals to choose from\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        \n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.pop[best_index].copy()\n\n    def mutate(self):\n        mutated_pop = np.zeros_like(self.pop)\n        top_indices = np.argsort(self.fitness)[:int(self.p_best * self.pop_size)]\n\n        for i in range(self.pop_size):\n            pbest_idx = top_indices[np.random.randint(len(top_indices))]\n            idxs = np.random.choice(self.pop_size, 2, replace=False)\n            x_r1, x_r2 = self.pop[idxs]\n            mutated_pop[i] = self.pop[i] + self.F * (self.pop[pbest_idx] - self.pop[i]) + self.F * (x_r1 - x_r2)\n        return mutated_pop\n\n    def crossover(self, mutated_pop):\n        crossed_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            j_rand = np.random.randint(0, self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.Cr or j == j_rand:\n                    crossed_pop[i, j] = mutated_pop[i, j]\n                else:\n                    crossed_pop[i, j] = self.pop[i, j]\n        return crossed_pop\n\n    def selection(self, func, crossed_pop):\n        new_fitness = np.array([func(x) for x in crossed_pop])\n        self.eval_count += self.pop_size\n\n        improved = new_fitness < self.fitness\n        \n        self.pop[improved] = crossed_pop[improved]\n        self.fitness[improved] = new_fitness[improved]\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.pop[best_index].copy()\n            \n        # Simplified adaptation of Cr and F based on improvement rate\n        improvement_rate = np.mean(improved)\n        self.Cr = np.clip(self.Cr + 0.2 * (improvement_rate - 0.3), 0.1, 0.9)\n        self.F = np.clip(self.F + 0.2 * (improvement_rate - 0.3), 0.1, 0.9)\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.initialize_population(func)\n        \n        while self.eval_count < self.budget:\n            mutated_pop = self.mutate()\n            mutated_pop = np.clip(mutated_pop, func.bounds.lb, func.bounds.ub)\n            crossed_pop = self.crossover(mutated_pop)\n            crossed_pop = np.clip(crossed_pop, func.bounds.lb, func.bounds.ub)\n            self.selection(func, crossed_pop)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ed022771-8ab0-45c4-bc31-eaf3e58a1619"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "38c95f37-0da8-4d60-9734-0ad95d7c5c65", "fitness": 0.36520673662131425, "name": "AdaptiveDEOrthoRestart", "description": "Simplified Adaptive Differential Evolution with orthogonal crossover and covariance matrix adaptation, focusing on efficient parameter adaptation and restart strategies for faster convergence.", "code": "import numpy as np\n\nclass AdaptiveDEOrthoRestart:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size_factor=2, memory_size=5, stagnation_threshold=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = int(pop_size * archive_size_factor)\n        self.F = 0.5\n        self.CR = 0.9\n        self.archive = []\n        self.memory_F = np.ones(memory_size) * 0.5\n        self.memory_CR = np.ones(memory_size) * 0.9\n        self.memory_idx = 0\n        self.epsilon = 1e-6\n        self.sf_history = []\n        self.scr_history = []\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.cov = np.eye(dim)  # Covariance matrix for mutation\n        self.cov_update_interval = 30  # Update covariance matrix every n iterations\n        self.iter_count = 0\n        self.memory_size = memory_size\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.p = 0.1  # Probability of using best individual in mutation\n\n    def orthogonal_crossover(self, x, mutant):\n        if self.dim in [2, 4]:\n            hadamard_matrix = np.array([[1, 1], [1, -1]]) if self.dim == 2 else np.array([[1, 1, 1, 1], [1, -1, 1, -1], [1, 1, -1, -1], [1, -1, -1, 1]])\n            trial = np.copy(x)\n            for i in range(self.dim):\n                trial[i] = 0.5 * (x[i] + mutant[i]) + 0.5 * hadamard_matrix[i % self.dim][i // (self.dim // hadamard_matrix.shape[1])] * (x[i] - mutant[i])\n            return trial\n        else:\n            return np.where(np.random.rand(self.dim) < 0.5, mutant, x)\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        \n        if np.min(self.fitness) < self.f_opt:\n            self.f_opt = np.min(self.fitness)\n            self.x_opt = self.population[np.argmin(self.fitness)]\n\n        self.best_fitness_history.append(self.f_opt)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size + len(self.archive), 2, replace=False)\n\n                # Select x_r1\n                if np.random.rand() < self.p:\n                    x_r1 = self.population[np.argmin(self.fitness)]  # Use best individual\n                elif idxs[0] < self.pop_size:\n                    x_r1 = self.population[idxs[0]]\n                else:\n                    x_r1 = self.archive[idxs[0] - self.pop_size]\n\n                # Select x_r2\n                if idxs[1] < self.pop_size:\n                    x_r2 = self.population[idxs[1]]\n                else:\n                    x_r2 = self.archive[idxs[1] - self.pop_size]\n                    \n                # Adaptive F and CR\n                F = self.memory_F[self.memory_idx]\n                CR = self.memory_CR[self.memory_idx]\n                \n                mu = np.zeros(self.dim)\n                mutant = x_r1 + F * (x_r2 - self.population[i]) + np.random.multivariate_normal(mu, self.cov)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = self.orthogonal_crossover(self.population[i], mutant)\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    # Archive update: Mirroring strategy\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        idx_replace = np.random.randint(self.archive_size)\n                        self.archive[idx_replace] = self.population[i].copy()\n\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    self.sf_history.append(F)\n                    self.scr_history.append(CR)\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Update memory\n            if self.sf_history and self.scr_history:\n                self.memory_F[self.memory_idx] = np.mean(np.clip(self.sf_history, 0.1, 0.9))\n                self.memory_CR[self.memory_idx] = np.mean(self.scr_history)\n            else:\n                self.memory_F[self.memory_idx] = 0.5\n                self.memory_CR[self.memory_idx] = 0.9\n                \n            self.sf_history = []\n            self.scr_history = []\n            self.memory_idx = (self.memory_idx + 1) % self.memory_size\n            \n            #Update covariance matrix\n            self.iter_count += 1\n            if self.iter_count % self.cov_update_interval == 0:\n                self.cov = np.cov(self.population.T)\n                self.cov += np.eye(self.dim) * 1e-6\n\n            # Stagnation detection and restart\n            if self.f_opt >= self.best_fitness_history[-1]:\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n\n            self.best_fitness_history.append(self.f_opt)\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                # Restart the population\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.budget -= self.pop_size\n                self.stagnation_counter = 0\n                self.cov = np.eye(self.dim)  # Reset covariance matrix\n                if np.min(self.fitness) < self.f_opt:\n                    self.f_opt = np.min(self.fitness)\n                    self.x_opt = self.population[np.argmin(self.fitness)]\n                self.best_fitness_history.append(self.f_opt)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveDEOrthoRestart scored 0.365 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f2782408-8fa6-4163-bb7e-9be6d79984bf"], "operator": null, "metadata": {"aucs": [0.1616343637374692, 0.21421268320338915, 0.3438318145849625, 0.3631066409534145, 0.30453571268864044, 0.3894894701155843, 0.29772643555218314, 0.32420954068374686, 0.28949982348809067, 0.20211407937277104, 0.3243165659719276, 0.9896038958155408, 0.2628915483147214, 0.30725230499804834, 0.6960163713796121, 0.38944541709756475, 0.2946043065439371, 0.4728689838207757, 0.19054701258977735, 0.4862277615141265]}}
{"id": "3036bcf4-60f4-486c-aeb7-948406d8bb69", "fitness": 0.14149704609454938, "name": "DynamicDE", "description": "Differential Evolution with dynamic population size adjustment based on stagnation detection, combined with Lvy flight mutation and a fitness-distance ratio based selection strategy.", "code": "import numpy as np\n\nclass DynamicDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, levy_exponent=1.5, stagnation_threshold=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.levy_exponent = levy_exponent\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = stagnation_threshold\n        self.min_pop_size = 10\n        self.max_pop_size = 100\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.pop[best_index].copy()\n\n    def levy_flight(self, beta):\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) / (np.math.gamma((1 + beta) / 2) * beta * (2 ** ((beta - 1) / 2)))) ** (1 / beta)\n        u = np.random.normal(0, sigma, size=self.dim)\n        v = np.random.normal(0, 1, size=self.dim)\n        step = u / (np.abs(v) ** (1 / beta))\n        return step\n\n    def mutate(self, func):\n        mutated_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.pop[idxs]\n\n            levy_step = self.levy_flight(self.levy_exponent)\n            mutated_pop[i] = self.pop[i] + levy_step * (x_r1 - x_r2 + x_r3 - self.pop[i])\n\n            mutated_pop[i] = np.clip(mutated_pop[i], func.bounds.lb, func.bounds.ub)\n        return mutated_pop\n\n    def crossover(self, mutated_pop):\n        crossed_pop = np.zeros_like(self.pop)\n        Cr = np.random.uniform(0.2, 0.9)\n        for i in range(self.pop_size):\n            j_rand = np.random.randint(0, self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < Cr or j == j_rand:\n                    crossed_pop[i, j] = mutated_pop[i, j]\n                else:\n                    crossed_pop[i, j] = self.pop[i, j]\n        return crossed_pop\n\n    def selection(self, func, crossed_pop):\n        new_fitness = np.array([func(x) for x in crossed_pop])\n        self.eval_count += self.pop_size\n\n        for i in range(self.pop_size):\n            if new_fitness[i] < self.fitness[i]:\n                self.pop[i] = crossed_pop[i]\n                self.fitness[i] = new_fitness[i]\n            else:\n                # Fitness-distance ratio selection\n                fitness_diff = np.abs(new_fitness[i] - self.fitness[i])\n                distance = np.linalg.norm(crossed_pop[i] - self.pop[i])\n                if distance > 0:\n                    ratio = fitness_diff / distance\n                    if np.random.rand() < ratio:\n                        self.pop[i] = crossed_pop[i]\n                        self.fitness[i] = new_fitness[i]\n\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.pop[best_index].copy()\n            self.stagnation_counter = 0\n        else:\n            self.stagnation_counter += 1\n\n        # Dynamic population size adjustment\n        if self.stagnation_counter > self.stagnation_threshold:\n            self.stagnation_counter = 0\n            if self.pop_size > self.min_pop_size:\n                # Reduce population size\n                num_remove = int(self.pop_size * 0.2)\n                indices_to_remove = np.argsort(self.fitness)[-num_remove:] # Remove worst individuals\n                self.pop = np.delete(self.pop, indices_to_remove, axis=0)\n                self.fitness = np.delete(self.fitness, indices_to_remove)\n                self.pop_size -= num_remove\n            else:\n                #Increase population size if already at the minimum\n                new_pop_size = min(self.pop_size + 5, self.max_pop_size)\n                num_new_individuals = new_pop_size - self.pop_size\n                new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(num_new_individuals, self.dim))\n                new_fitness_vals = np.array([func(x) for x in new_individuals])\n                self.eval_count += num_new_individuals\n\n                self.pop = np.vstack((self.pop, new_individuals))\n                self.fitness = np.concatenate((self.fitness, new_fitness_vals))\n                self.pop_size = new_pop_size\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            mutated_pop = self.mutate(func)\n            crossed_pop = self.crossover(mutated_pop)\n            self.selection(func, crossed_pop)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm DynamicDE scored 0.141 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ec335100-5a21-4a26-995d-fa20f7ef85c0"], "operator": null, "metadata": {"aucs": [0.09630812203310302, 0.15204009754671133, 0.275441417692754, 0.1836955932001786, 0]}}
{"id": "0219ce3b-1fe4-439d-9006-ba648e83aa9b", "fitness": -Infinity, "name": "AdaptiveDifferentialEvolution", "description": "Simplified Adaptive Differential Evolution with reduced population size, targeted mutation around the best solution, and simplified crossover.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=20, Cr=0.7, F=0.6):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.Cr = Cr\n        self.F = F\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x] for x in self.pop])\n        self.eval_count += self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.pop[best_index].copy()\n\n    def evolve(self, func):\n        # Find the best individual in the current population\n        idx_best = np.argmin(self.fitness)\n        x_best = self.pop[idx_best]\n\n        for i in range(self.pop_size):\n            # Mutation around the best\n            idxs = np.random.choice(self.pop_size, 2, replace=False)\n            x_r1, x_r2 = self.pop[idxs]\n            x_mutated = x_best + self.F * (x_r1 - x_r2)\n            x_mutated = np.clip(x_mutated, func.bounds.lb, func.bounds.ub)\n\n            # Simplified Crossover\n            x_trial = np.where(np.random.rand(self.dim) < self.Cr, x_mutated, self.pop[i])\n\n            # Evaluation\n            f_trial = func(x_trial)\n            self.eval_count += 1\n\n            # Selection\n            if f_trial < self.fitness[i]:\n                self.pop[i] = x_trial\n                self.fitness[i] = f_trial\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = x_trial.copy()\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "An exception occurred: closing parenthesis ']' does not match opening parenthesis '(' (<string>, line 18).", "error": "", "parent_ids": ["ccb1b87a-4c62-4fc7-9007-30f9ccd2ac9b"], "operator": null, "metadata": {}}
{"id": "21a8fe7c-2659-459a-9e16-9dcfd2ce501f", "fitness": 0.0, "name": "SelfAdaptiveDE", "description": "Self-Adaptive Differential Evolution with probabilistic parameter adaptation and local search refinement.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, Cr_init=0.5, F_init=0.5, local_search_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.Cr_init = Cr_init\n        self.F_init = F_init\n        self.local_search_prob = local_search_prob\n        self.pop = None\n        self.fitness = None\n        self.Cr = None\n        self.F = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.pop[best_index].copy()\n        self.Cr = np.full(self.pop_size, self.Cr_init)\n        self.F = np.full(self.pop_size, self.F_init)\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            # Parameter adaptation\n            self.Cr[i] = np.clip(np.random.normal(self.Cr[i], 0.1), 0.0, 1.0)\n            self.F[i] = np.clip(np.random.normal(self.F[i], 0.1), 0.0, 1.0)\n\n            # Mutation\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.pop[idxs]\n            x_mutated = self.pop[i] + self.F[i] * (x_r1 - x_r2 + self.x_opt - x_r3) #Using current and best solution\n            x_mutated = np.clip(x_mutated, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            x_trial = np.copy(self.pop[i])\n            j_rand = np.random.randint(0, self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.Cr[i] or j == j_rand:\n                    x_trial[j] = x_mutated[j]\n\n            # Local Search\n            if np.random.rand() < self.local_search_prob:\n                 x_trial = self.local_search(func, x_trial)\n\n            # Evaluation\n            f_trial = func(x_trial)\n            self.eval_count += 1\n\n            # Selection\n            if f_trial < self.fitness[i]:\n                self.pop[i] = x_trial\n                self.fitness[i] = f_trial\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = x_trial.copy()\n\n    def local_search(self, func, x, step_size=0.1):\n        x_new = x.copy()\n        for i in range(self.dim):\n            # Explore in both directions\n            x_plus = x.copy()\n            x_minus = x.copy()\n            x_plus[i] = np.clip(x[i] + step_size, func.bounds.lb[i], func.bounds.ub[i])\n            x_minus[i] = np.clip(x[i] - step_size, func.bounds.lb[i], func.bounds.ub[i])\n\n            f_plus = func(x_plus)\n            f_minus = func(x_minus)\n            self.eval_count += 2\n\n            if f_plus < func(x_new):\n                x_new = x_plus\n            if f_minus < func(x_new):\n                x_new = x_minus\n\n        return x_new\n\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm SelfAdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ccb1b87a-4c62-4fc7-9007-30f9ccd2ac9b"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "f4aefbfb-2e75-49fa-a11e-cc6201475211", "fitness": -Infinity, "name": "AdaptiveDEVoronoi", "description": "Adaptive Differential Evolution with a self-organizing search strategy based on Voronoi tessellation and dynamic resource allocation.", "code": "import numpy as np\nfrom scipy.spatial import Voronoi\n\nclass AdaptiveDEVoronoi:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size_factor=2, memory_size=5, stagnation_threshold=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = int(pop_size * archive_size_factor)\n        self.F = 0.5\n        self.CR = 0.9\n        self.archive = []\n        self.memory_F = np.ones(memory_size) * 0.5\n        self.memory_CR = np.ones(memory_size) * 0.9\n        self.memory_idx = 0\n        self.epsilon = 1e-6\n        self.sf_history = []\n        self.scr_history = []\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.memory_size = memory_size\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.p = 0.1  # Probability of using best individual in mutation\n        self.voronoi_weights = np.ones(pop_size) / pop_size #Initial Voronoi weights\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        \n        if np.min(self.fitness) < self.f_opt:\n            self.f_opt = np.min(self.fitness)\n            self.x_opt = self.population[np.argmin(self.fitness)]\n\n        self.best_fitness_history.append(self.f_opt)\n\n        while self.budget > 0:\n            # Voronoi tessellation to estimate search space density\n            vor = Voronoi(self.population)\n            \n            # Update Voronoi weights based on fitness improvements\n            delta_fitness = self.f_opt - self.fitness #Fitness improvement for each individual compared to best\n            delta_fitness = np.clip(delta_fitness, 0, np.inf) #ensure non-negative\n            self.voronoi_weights = np.exp(delta_fitness / np.mean(delta_fitness)) #Boltzmann-like distribution\n            self.voronoi_weights /= np.sum(self.voronoi_weights) #normalize weights\n            \n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size + len(self.archive), 2, replace=False, p=np.concatenate([self.voronoi_weights, np.ones(len(self.archive))/len(self.archive)]))\n\n                # Select x_r1\n                if np.random.rand() < self.p:\n                    x_r1 = self.population[np.argmin(self.fitness)]  # Use best individual\n                elif idxs[0] < self.pop_size:\n                    x_r1 = self.population[idxs[0]]\n                else:\n                    x_r1 = self.archive[idxs[0] - self.pop_size]\n\n                # Select x_r2\n                if idxs[1] < self.pop_size:\n                    x_r2 = self.population[idxs[1]]\n                else:\n                    x_r2 = self.archive[idxs[1] - self.pop_size]\n                    \n                # Adaptive F and CR\n                F = self.memory_F[self.memory_idx]\n                CR = self.memory_CR[self.memory_idx]\n                \n                mutant = self.population[i] + F * (x_r1 - x_r2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.where(np.random.rand(self.dim) < CR, mutant, self.population[i])\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    # Archive update: Mirroring strategy\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        idx_replace = np.random.randint(self.archive_size)\n                        self.archive[idx_replace] = self.population[i].copy()\n\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    self.sf_history.append(F)\n                    self.scr_history.append(CR)\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Update memory\n            if self.sf_history and self.scr_history:\n                self.memory_F[self.memory_idx] = np.mean(np.clip(self.sf_history, 0.1, 0.9))\n                self.memory_CR[self.memory_idx] = np.mean(self.scr_history)\n            else:\n                self.memory_F[self.memory_idx] = 0.5\n                self.memory_CR[self.memory_idx] = 0.9\n                \n            self.sf_history = []\n            self.scr_history = []\n            self.memory_idx = (self.memory_idx + 1) % self.memory_size\n\n            # Stagnation detection and restart\n            if self.f_opt >= self.best_fitness_history[-1]:\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n\n            self.best_fitness_history.append(self.f_opt)\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                # Restart the population - keep best individual\n                best_idx = np.argmin(self.fitness)\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.population[best_idx] = self.x_opt.copy() #Keep best\n                self.fitness = np.array([func(x) for x in self.population])\n                self.budget -= self.pop_size\n                self.stagnation_counter = 0\n                self.voronoi_weights = np.ones(self.pop_size) / self.pop_size #Reset Voronoi weights\n                if np.min(self.fitness) < self.f_opt:\n                    self.f_opt = np.min(self.fitness)\n                    self.x_opt = self.population[np.argmin(self.fitness)]\n                self.best_fitness_history.append(self.f_opt)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "An exception occurred: name 'Voronoi' is not defined.", "error": "", "parent_ids": ["38c95f37-0da8-4d60-9734-0ad95d7c5c65"], "operator": null, "metadata": {}}
{"id": "3c4ea5b2-1129-4241-a847-9ca979ef70a0", "fitness": 0.7083102946280931, "name": "AdaptiveDifferentialEvolution", "description": "Adaptive Differential Evolution with a simplified mutation strategy, orthogonal crossover, and a focused restart mechanism based on stagnation detection.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, Cr=0.9, stagnation_iters=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.Cr = Cr\n        self.stagnation_iters = stagnation_iters\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.no_improvement_count = 0\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.update_best(self.pop, self.fitness)\n\n    def mutate(self):\n        mutated_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.pop[idxs]\n            mutated_pop[i] = x_r1 + self.F * (x_r2 - x_r3)\n\n        return mutated_pop\n\n    def crossover(self, mutated_pop):\n        crossed_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            j_rand = np.random.randint(0, self.dim)\n            u = np.random.rand(self.dim) < self.Cr\n            u[j_rand] = True\n            crossed_pop[i] = np.where(u, mutated_pop[i], self.pop[i])\n        return crossed_pop\n\n    def selection(self, func, crossed_pop):\n        new_fitness = np.array([func(x) for x in crossed_pop])\n        self.eval_count += self.pop_size\n\n        improved = new_fitness < self.fitness\n        self.pop[improved] = crossed_pop[improved]\n        self.fitness[improved] = new_fitness[improved]\n\n        self.update_best(self.pop, self.fitness)\n\n    def restart_population(self, func):\n        new_pop = np.random.normal(loc=self.x_opt, scale=0.1, size=(self.pop_size, self.dim))\n        new_pop = np.clip(new_pop, func.bounds.lb, func.bounds.ub)\n        new_fitness = np.array([func(x) for x in new_pop])\n        self.eval_count += self.pop_size\n\n        self.pop = new_pop\n        self.fitness = new_fitness\n        self.update_best(self.pop, self.fitness)\n\n    def update_best(self, pop, fitness):\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = pop[best_index].copy()\n            self.no_improvement_count = 0\n        else:\n            self.no_improvement_count += 1\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.no_improvement_count = 0\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            mutated_pop = self.mutate()\n            mutated_pop = np.clip(mutated_pop, func.bounds.lb, func.bounds.ub)\n            crossed_pop = self.crossover(mutated_pop)\n            crossed_pop = np.clip(crossed_pop, func.bounds.lb, func.bounds.ub)\n            self.selection(func, crossed_pop)\n\n            if self.no_improvement_count > self.stagnation_iters:\n                self.restart_population(func)\n                self.no_improvement_count = 0\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.708 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["30d0d783-b7a5-44c8-91ac-796c98b07a84"], "operator": null, "metadata": {"aucs": [0.327892760999846, 0.7027359590608302, 0.7014521462304735, 0.8702128987728452, 0.754261009832849, 0.7900527857169907, 0.698048029127715, 0.678981430213389, 0.7402847549850397, 0.6538326571055795, 0.8441727365253954, 0.9968527844532594, 0.6708140411776478, 0.757912215138673, 0.901034972121853, 0.7860634684948462, 0.624312647768213, 0.8550736325768391, 0.29813848644748875, 0.5140764758120886]}}
{"id": "c158c517-d7ac-4a78-9f4b-640dacff858b", "fitness": 0.46594217658359194, "name": "AdaptiveDifferentialEvolution", "description": "Simplified Adaptive Differential Evolution with parent-centric mutation, binomial crossover, and dynamic F parameter adjustment based on success for enhanced exploration.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=40, Cr=0.7, F=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.Cr = Cr\n        self.F = F\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.success_history = []\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.pop[best_index].copy()\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            # Parent-centric mutation\n            idx_p = np.argmin(self.fitness)\n            x_p = self.pop[idx_p]\n            idxs = np.random.choice(self.pop_size, 2, replace=False)\n            x_r1, x_r2 = self.pop[idxs]\n            x_mutated = x_p + self.F * (x_r1 - x_r2)\n            x_mutated = np.clip(x_mutated, func.bounds.lb, func.bounds.ub)\n\n            # Binomial crossover\n            x_trial = np.copy(self.pop[i])\n            j_rand = np.random.randint(0, self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.Cr or j == j_rand:\n                    x_trial[j] = x_mutated[j]\n\n            # Evaluation\n            f_trial = func(x_trial)\n            self.eval_count += 1\n\n            # Selection\n            if f_trial < self.fitness[i]:\n                self.success_history.append(self.fitness[i] - f_trial)\n                self.pop[i] = x_trial\n                self.fitness[i] = f_trial\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = x_trial.copy()\n            else:\n                self.success_history.append(0)\n\n        # Update F adaptively\n        if self.success_history:\n            success_rate = np.mean([1 if succ > 0 else 0 for succ in self.success_history[-self.pop_size:]])\n            if success_rate > 0.2:\n                self.F *= 0.9\n            else:\n                self.F *= 1.1\n            self.F = np.clip(self.F, 0.1, 1.0)\n\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.success_history = []\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.466 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ccb1b87a-4c62-4fc7-9007-30f9ccd2ac9b"], "operator": null, "metadata": {"aucs": [0.13751043660354734, 0.2105583097674466, 0.4176100054248195, 0.3908447608262221, 0.6136741392923901, 0.5312891991799356, 0.3072281565584615, 0.41877878695499793, 0.5472708136749327, 0.20062252415438087, 0.6171336143125294, 0.9970309767960769, 0.3233698164391613, 0.29847423871920087, 0.9082147237493002, 0.33213182824391907, 0.3797489663542456, 0.9653912004125572, 0.20723012435435506, 0.5147309098533596]}}
{"id": "96508b05-e3fa-4ef4-99d0-999bb559bd67", "fitness": 0.751008993574663, "name": "AdaptiveDifferentialEvolution", "description": "Adaptive Differential Evolution with ranking-based mutation and a repair mechanism for constraint handling.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=40, Cr=0.7, F=0.6):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.Cr = Cr\n        self.F = F\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.pop[best_index].copy()\n\n    def repair(self, x, lb, ub):\n        \"\"\"Repair solution to lie within the bounds.\"\"\"\n        return np.clip(x, lb, ub)\n\n    def evolve(self, func):\n        ranked_indices = np.argsort(self.fitness)\n\n        for i in range(self.pop_size):\n            # Mutation: Ranking-based selection of parents\n            p_best = self.pop[ranked_indices[0]]\n            idx_r1 = np.random.choice(ranked_indices[1:]) # Avoid selecting the best individual again\n            idx_r2 = np.random.choice(ranked_indices[1:])\n            x_r1, x_r2 = self.pop[idx_r1], self.pop[idx_r2]\n\n            x_mutated = p_best + self.F * (x_r1 - x_r2)\n            x_mutated = self.repair(x_mutated, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            x_trial = np.copy(self.pop[i])\n            j_rand = np.random.randint(0, self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.Cr or j == j_rand:\n                    x_trial[j] = x_mutated[j]\n\n            x_trial = self.repair(x_trial, func.bounds.lb, func.bounds.ub)\n\n            # Evaluation\n            f_trial = func(x_trial)\n            self.eval_count += 1\n\n            # Selection\n            if f_trial < self.fitness[i]:\n                self.pop[i] = x_trial\n                self.fitness[i] = f_trial\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = x_trial.copy()\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.751 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ccb1b87a-4c62-4fc7-9007-30f9ccd2ac9b"], "operator": null, "metadata": {"aucs": [0.26856296016221115, 0.744399512406447, 0.8740734882873277, 0.935576128862314, 0.8891973714988481, 0.915076058167849, 0.3404366240680968, 0.7983904204922898, 0.8975431302084611, 0.5083426141788367, 0.9478297439233582, 0.997866623425217, 0.6226059171110991, 0.8828431048758208, 0.942496289448157, 0.9136041900827933, 0.8515761331753584, 0.933517360339688, 0.23654806666971973, 0.5196941341093664]}}
{"id": "e453f47b-b503-4931-b85d-12a4265c05d5", "fitness": 0.6708832839779719, "name": "AdaptiveDifferentialEvolution", "description": "Simplified Adaptive Differential Evolution with periodic population resizing, focused parameter adaptation using a ring buffer, and mirroring bounds handling.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size_init=50, Cr=0.5, F=0.5, p_best=0.1, pop_resize_freq=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_init = pop_size_init\n        self.pop_size = pop_size_init\n        self.Cr = Cr\n        self.F = F\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.p_best = p_best\n        self.success_Cr = []\n        self.success_F = []\n        self.memory_size = 10\n        self.memory_Cr = np.full(self.memory_size, Cr)\n        self.memory_F = np.full(self.memory_size, F)\n        self.memory_idx = 0\n        self.pop_resize_freq = pop_resize_freq\n        self.generation = 0\n\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        \n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.pop[best_index].copy()\n\n    def mutate(self, func):\n        mutated_pop = np.zeros_like(self.pop)\n        top_indices = np.argsort(self.fitness)[:int(self.p_best * self.pop_size)]\n\n        for i in range(self.pop_size):\n            pbest_idx = top_indices[np.random.randint(len(top_indices))]\n            idxs = np.random.choice(self.pop_size, 2, replace=False)\n            x_r1, x_r2 = self.pop[idxs]\n            \n            mutated_vector = self.pop[i] + self.F * (self.pop[pbest_idx] - self.pop[i]) + self.F * (x_r1 - x_r2)\n            \n            # Mirroring strategy\n            for j in range(self.dim):\n                if mutated_vector[j] < func.bounds.lb[j]:\n                    mutated_vector[j] = func.bounds.lb[j] + (func.bounds.lb[j] - mutated_vector[j])\n                elif mutated_vector[j] > func.bounds.ub[j]:\n                    mutated_vector[j] = func.bounds.ub[j] - (mutated_vector[j] - func.bounds.ub[j])\n                    \n            mutated_pop[i] = mutated_vector\n        return mutated_pop\n\n    def crossover(self, mutated_pop):\n        crossed_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            j_rand = np.random.randint(0, self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.Cr or j == j_rand:\n                    crossed_pop[i, j] = mutated_pop[i, j]\n                else:\n                    crossed_pop[i, j] = self.pop[i, j]\n        return crossed_pop\n\n    def selection(self, func, crossed_pop):\n        new_fitness = np.array([func(x) for x in crossed_pop])\n        self.eval_count += self.pop_size\n\n        improved = new_fitness < self.fitness\n        \n        # Store successful Cr and F values\n        for i in range(self.pop_size):\n            if improved[i]:\n                self.success_Cr.append(self.Cr)\n                self.success_F.append(self.F)\n        \n        self.pop[improved] = crossed_pop[improved]\n        self.fitness[improved] = new_fitness[improved]\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.pop[best_index].copy()\n            \n    def adapt_parameters(self):\n        # Update Cr and F using success history\n        if self.success_Cr:\n            self.Cr = np.median(self.success_Cr)\n        else:\n            self.Cr = self.memory_Cr[self.memory_idx]\n\n        if self.success_F:\n            self.F = np.median(self.success_F)\n        else:\n            self.F = self.memory_F[self.memory_idx]\n            \n        self.memory_Cr[self.memory_idx] = self.Cr\n        self.memory_F[self.memory_idx] = self.F\n        self.memory_idx = (self.memory_idx + 1) % self.memory_size\n            \n        self.success_Cr = []\n        self.success_F = []\n\n    def resize_population(self, func):\n        if self.eval_count > self.budget // 2:\n            self.pop_size = self.pop_size_init  # Reset to initial size\n\n        best_indices = np.argsort(self.fitness)[:self.pop_size_init]  # Keep best individuals\n\n        new_pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size_init, self.dim))\n        new_fitness = np.full(self.pop_size_init, np.inf)\n\n        new_pop[:len(best_indices)] = self.pop[best_indices]\n        new_fitness[:len(best_indices)] = self.fitness[best_indices]\n            \n        self.pop = new_pop\n        self.fitness = new_fitness\n        self.pop_size = self.pop_size_init\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.initialize_population(func)\n        self.generation = 0\n        \n        while self.eval_count < self.budget:\n            self.generation += 1\n            mutated_pop = self.mutate(func)\n            crossed_pop = self.crossover(mutated_pop)\n            self.selection(func, crossed_pop)\n            self.adapt_parameters()\n\n            if self.generation % self.pop_resize_freq == 0:\n                self.resize_population(func)\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.671 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["aa1940c9-4f60-4d5b-87aa-41da3b9aad82"], "operator": null, "metadata": {"aucs": [0.24393796819018776, 0.5511989015664078, 0.7214644905505807, 0.8814631424660366, 0.7684230492586198, 0.8318918472230858, 0.5805519713528441, 0.6925860781559425, 0.7849480938829811, 0.4834211279650864, 0.8646508292856794, 0.9967096403383596, 0.3756798386072886, 0.7087949312906219, 0.9197251293437768, 0.8013357506923676, 0.5885499911373071, 0.8725437175650543, 0.21971507235952914, 0.5300741083276815]}}
{"id": "8d187161-d149-4c58-9600-b4f290f85023", "fitness": 0.746999598974653, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution with archive for diversity, adaptive F and Cr, and a repair mechanism to keep solutions within bounds.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=40, archive_size=10, Cr_init=0.7, F_init=0.6, F_decay=0.99):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.Cr_init = Cr_init\n        self.F_init = F_init\n        self.F_decay = F_decay\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.archive = []\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.pop[best_index].copy()\n\n    def repair(self, x, lb, ub):\n        \"\"\"Repair a solution to ensure it lies within the bounds.\"\"\"\n        return np.clip(x, lb, ub)\n\n\n    def evolve(self, func):\n        # Adaptive F and Cr\n        F = self.F_init * self.F_decay**(self.eval_count / self.budget)\n        Cr = self.Cr_init\n\n        for i in range(self.pop_size):\n            # Parent Selection\n            idx_p = np.argmin(self.fitness)\n            x_p = self.pop[idx_p]\n\n            # Mutation - use archive for diversity\n            candidates = list(range(self.pop_size))\n            candidates.remove(i)\n            if self.archive:\n                arch_idx = np.random.randint(len(self.archive))\n                x_r1 = self.archive[arch_idx]\n                idxs = np.random.choice(candidates, 1, replace=False)\n                x_r2 = self.pop[idxs[0]]\n\n            else:\n                idxs = np.random.choice(candidates, 2, replace=False)\n                x_r1, x_r2 = self.pop[idxs]\n\n\n            x_mutated = x_p + F * (x_r1 - x_r2)\n            x_mutated = self.repair(x_mutated, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            x_trial = np.copy(self.pop[i])\n            j_rand = np.random.randint(0, self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < Cr or j == j_rand:\n                    x_trial[j] = x_mutated[j]\n\n            x_trial = self.repair(x_trial, func.bounds.lb, func.bounds.ub)\n\n            # Evaluation\n            f_trial = func(x_trial)\n            self.eval_count += 1\n\n            # Selection\n            if f_trial < self.fitness[i]:\n                # Archive management\n                if len(self.archive) < self.archive_size:\n                    self.archive.append(self.pop[i].copy())\n                else:\n                    # Replace a random archive member\n                    replace_idx = np.random.randint(self.archive_size)\n                    self.archive[replace_idx] = self.pop[i].copy()\n\n                self.pop[i] = x_trial\n                self.fitness[i] = f_trial\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = x_trial.copy()\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.archive = []  # Initialize archive\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution scored 0.747 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ccb1b87a-4c62-4fc7-9007-30f9ccd2ac9b"], "operator": null, "metadata": {"aucs": [0.34196650304279397, 0.6895969515639928, 0.8220920636135434, 0.9172786611114434, 0.8484617469792446, 0.8860825551938896, 0.3693510090962906, 0.7915989812674493, 0.8575249454760813, 0.8142588053003259, 0.9100504742891522, 0.9962386735458661, 0.5281969679859573, 0.8136138503546829, 0.9557276583761645, 0.8640473870660685, 0.7654534868360467, 0.9041971151344712, 0.395463824967514, 0.46879031829208007]}}
{"id": "f40c4a00-3b8f-4bf9-8069-6b426133dd61", "fitness": 0.7075345717511173, "name": "AdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution with archive for stagnation avoidance and adaptive mutation factor based on success.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=40, Cr=0.7, F=0.6, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.Cr = Cr\n        self.F = F\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.archive = []\n        self.archive_fitness = []\n        self.archive_size = archive_size\n        self.success_history = []\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.pop[best_index].copy()\n\n    def update_archive(self, x, f):\n        if len(self.archive) < self.archive_size:\n            self.archive.append(x)\n            self.archive_fitness.append(f)\n        else:\n            worst_index = np.argmax(self.archive_fitness)\n            if f < self.archive_fitness[worst_index]:\n                self.archive[worst_index] = x\n                self.archive_fitness[worst_index] = f\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            # Select parent\n            idx_p = np.argmin(self.fitness)\n            x_p = self.pop[idx_p]\n\n            # Adaptive F\n            if self.success_history:\n                self.F = np.clip(np.mean(self.success_history), 0.1, 0.9)\n            \n            # Mutation\n            idxs = np.random.choice(self.pop_size, 2, replace=False)\n            x_r1, x_r2 = self.pop[idxs]\n\n            # Stagnation check: If the improvement is very small, use archive\n            if np.abs(self.fitness[i] - self.f_opt) < 1e-6 and self.archive:\n                idx_a = np.random.randint(0, len(self.archive))\n                x_r1 = self.archive[idx_a]  # Use archive member for mutation\n            \n            x_mutated = x_p + self.F * (x_r1 - x_r2)\n            x_mutated = np.clip(x_mutated, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            x_trial = np.copy(self.pop[i])\n            j_rand = np.random.randint(0, self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.Cr or j == j_rand:\n                    x_trial[j] = x_mutated[j]\n\n            # Evaluation\n            f_trial = func(x_trial)\n            self.eval_count += 1\n\n            # Selection\n            if f_trial < self.fitness[i]:\n                self.success_history.append(self.F)\n                if len(self.success_history) > 10:\n                    self.success_history.pop(0)\n                \n                self.pop[i] = x_trial\n                self.fitness[i] = f_trial\n                self.update_archive(x_trial, f_trial)\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = x_trial.copy()\n            else:\n                self.success_history = [] # Reset success history if no improvement\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.archive = []\n        self.archive_fitness = []\n        self.success_history = []\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.708 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ccb1b87a-4c62-4fc7-9007-30f9ccd2ac9b"], "operator": null, "metadata": {"aucs": [0.2532852844351676, 0.8312062739371435, 0.8802467605503488, 0.922387530768997, 0.31631722409365215, 0.9156306627881174, 0.3390463455005279, 0.8566544057427092, 0.8984617344512061, 0.8814735009858194, 0.9213298829156149, 0.998834171351544, 0.5870967414486361, 0.3642152556445686, 0.9588283843078971, 0.9114560150554729, 0.8420698409415865, 0.3798672717096496, 0.23093899465351264, 0.8613451537401765]}}
{"id": "73e97dab-d775-4d59-901b-dd9bcbfc0062", "fitness": -Infinity, "name": "CMA_DE", "description": "Covariance matrix adaptation in DE to learn the dependencies between variables and accelerate convergence by adapting the search distribution to the problem's landscape, and simplified parameter settings.", "code": "import numpy as np\n\nclass CMA_DE:\n    def __init__(self, budget=10000, dim=10, pop_size=None, Cr=0.7, F=0.6, sigma0=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.Cr = Cr\n        self.F = F\n        self.sigma = sigma0\n        self.mean = None\n        self.C = None  # Covariance matrix\n        self.eval_count = 0\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n        self.path_c = None\n        self.path_sigma = None\n        self.pc_decay = None\n        self.ps_decay = None\n        self.eigenspace = None\n        self.eigenvalues = None\n\n\n    def initialize_population(self, func):\n        self.mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.C = np.eye(self.dim)  # Initialize covariance matrix to identity\n        self.path_c = np.zeros(self.dim)\n        self.path_sigma = np.zeros(self.dim)\n\n        # Learning rate parameters\n        self.pc_decay = 0.1 #1 / np.sqrt(self.dim)\n        self.ps_decay = 0.1 #1 / np.sqrt(self.dim)\n\n    def sample_population(self):\n        z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n        if self.eigenspace is None:\n            y = z\n        else:\n            y = self.eigenspace @ np.diag(np.sqrt(self.eigenvalues)) @ z.T\n            y = y.T\n\n        x = self.mean + self.sigma * y\n        return x\n\n    def update_distribution(self, func, pop, fitness):\n        # Sort population based on fitness\n        idx = np.argsort(fitness)\n        pop = pop[idx]\n        \n        # Calculate new mean (weighted average of top individuals)\n        delta_mean = np.mean(pop[:self.pop_size//2] - self.mean, axis=0)\n        self.mean = self.mean + delta_mean\n\n        # Update evolution path for covariance matrix\n        z = (pop[0] - self.mean) / self.sigma\n        self.path_c = (1 - self.pc_decay) * self.path_c + np.sqrt(self.pc_decay * (2 - self.pc_decay)) * z\n        self.C = (1 - self.ps_decay) * self.C + self.ps_decay * np.outer(self.path_c, self.path_c)\n\n        # Update sigma\n        self.path_sigma = (1 - self.ps_decay) * self.path_sigma + np.sqrt(self.ps_decay * (2 - self.ps_decay)) * delta_mean/self.sigma\n        self.sigma *= np.exp(self.ps_decay/0.5 * (np.linalg.norm(self.path_sigma)**2 - self.dim)/self.dim)\n        self.sigma = max(self.sigma, 1e-10) # Avoid sigma getting too small\n\n        self.eigenvalues, self.eigenspace = np.linalg.eig(self.C)\n\n\n    def evolve(self, func):\n        pop = self.sample_population()\n\n        # Clip individuals to bounds\n        pop = np.clip(pop, func.bounds.lb, func.bounds.ub)\n\n        # Evaluate population\n        fitness = np.array([func(x) for x in pop])\n        self.eval_count += self.pop_size\n\n        # Update best solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = pop[best_index].copy()\n\n        self.update_distribution(func, pop, fitness)\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "An exception occurred: Array must not contain infs or NaNs.", "error": "", "parent_ids": ["ccb1b87a-4c62-4fc7-9007-30f9ccd2ac9b"], "operator": null, "metadata": {}}
{"id": "b2675ad8-e75a-49c2-a4d3-7477f6fdc158", "fitness": 0.352554689022281, "name": "ImprovedAdaptiveDEOrthoRestart", "description": "Improved Adaptive Differential Evolution with orthogonal crossover, covariance matrix adaptation, and a success-history based F/CR adaptation with a jitter strategy and focused stagnation restart.", "code": "import numpy as np\n\nclass ImprovedAdaptiveDEOrthoRestart:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size_factor=2, memory_size=5, stagnation_threshold=50, p_best=0.1, cov_update_interval=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = int(pop_size * archive_size_factor)\n        self.F = 0.5\n        self.CR = 0.9\n        self.archive = []\n        self.memory_F = np.ones(memory_size) * 0.5\n        self.memory_CR = np.ones(memory_size) * 0.9\n        self.memory_idx = 0\n        self.epsilon = 1e-6\n        self.sf_history = []\n        self.scr_history = []\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.cov = np.eye(dim)  # Covariance matrix for mutation\n        self.cov_update_interval = cov_update_interval  # Update covariance matrix every n iterations\n        self.iter_count = 0\n        self.memory_size = memory_size\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.p_best = p_best  # Probability of using best individual in mutation\n\n    def orthogonal_crossover(self, x, mutant):\n        if self.dim in [2, 4]:\n            hadamard_matrix = np.array([[1, 1], [1, -1]]) if self.dim == 2 else np.array([[1, 1, 1, 1], [1, -1, 1, -1], [1, 1, -1, -1], [1, -1, -1, 1]])\n            trial = np.copy(x)\n            for i in range(self.dim):\n                trial[i] = 0.5 * (x[i] + mutant[i]) + 0.5 * hadamard_matrix[i % self.dim][i // (self.dim // hadamard_matrix.shape[1])] * (x[i] - mutant[i])\n            return trial\n        else:\n            return np.where(np.random.rand(self.dim) < self.CR, mutant, x)\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        \n        if np.min(self.fitness) < self.f_opt:\n            self.f_opt = np.min(self.fitness)\n            self.x_opt = self.population[np.argmin(self.fitness)]\n\n        self.best_fitness_history.append(self.f_opt)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size + len(self.archive), 2, replace=False)\n\n                # Select x_r1\n                if np.random.rand() < self.p_best:\n                    x_r1 = self.population[np.argmin(self.fitness)]  # Use best individual\n                elif idxs[0] < self.pop_size:\n                    x_r1 = self.population[idxs[0]]\n                else:\n                    x_r1 = self.archive[idxs[0] - self.pop_size]\n\n                # Select x_r2\n                if idxs[1] < self.pop_size:\n                    x_r2 = self.population[idxs[1]]\n                else:\n                    x_r2 = self.archive[idxs[1] - self.pop_size]\n                    \n                # Adaptive F and CR with jitter\n                F = self.memory_F[self.memory_idx] + np.random.normal(0, 0.05)\n                F = np.clip(F, 0.1, 1.0) # Clip to reasonable range.\n                CR = self.memory_CR[self.memory_idx] + np.random.normal(0, 0.05)\n                CR = np.clip(CR, 0.1, 1.0)\n                \n                mu = np.zeros(self.dim)\n                mutant = x_r1 + F * (x_r2 - self.population[i]) + np.random.multivariate_normal(mu, self.cov)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = self.orthogonal_crossover(self.population[i], mutant)\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    # Archive update: Mirroring strategy\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        idx_replace = np.random.randint(self.archive_size)\n                        self.archive[idx_replace] = self.population[i].copy()\n\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    self.sf_history.append(F)\n                    self.scr_history.append(CR)\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Update memory\n            if self.sf_history and self.scr_history:\n                self.memory_F[self.memory_idx] = np.mean(np.clip(self.sf_history, 0.1, 0.9))\n                self.memory_CR[self.memory_idx] = np.mean(self.scr_history)\n            else:\n                self.memory_F[self.memory_idx] = 0.5\n                self.memory_CR[self.memory_idx] = 0.9\n                \n            self.sf_history = []\n            self.scr_history = []\n            self.memory_idx = (self.memory_idx + 1) % self.memory_size\n            \n            #Update covariance matrix\n            self.iter_count += 1\n            if self.iter_count % self.cov_update_interval == 0:\n                self.cov = np.cov(self.population.T)\n                self.cov += np.eye(self.dim) * 1e-6\n\n            # Stagnation detection and restart\n            if self.f_opt >= self.best_fitness_history[-1]:\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n\n            self.best_fitness_history.append(self.f_opt)\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                # Restart the population around the best solution\n                best_idx = np.argmin(self.fitness)\n                self.population = np.random.normal(self.population[best_idx], scale=0.5, size=(self.pop_size, self.dim))\n                self.population = np.clip(self.population, func.bounds.lb, func.bounds.ub)\n\n                self.fitness = np.array([func(x) for x in self.population])\n                self.budget -= self.pop_size\n                self.stagnation_counter = 0\n                self.cov = np.eye(self.dim)  # Reset covariance matrix\n                if np.min(self.fitness) < self.f_opt:\n                    self.f_opt = np.min(self.fitness)\n                    self.x_opt = self.population[np.argmin(self.fitness)]\n                self.best_fitness_history.append(self.f_opt)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm ImprovedAdaptiveDEOrthoRestart scored 0.353 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["38c95f37-0da8-4d60-9734-0ad95d7c5c65"], "operator": null, "metadata": {"aucs": [0.14402140391497142, 0.22469925583401718, 0.3400723948882556, 0.3900336118271972, 0.2829803435985513, 0.3572468008111752, 0.2742257897857676, 0.29340640250928895, 0.2661954539112916, 0.20043334161073312, 0.308719866359235, 0.9994462899017157, 0.29052674254238775, 0.28347688555768447, 0.6707212502479252, 0.36169141277017414, 0.28919915570513444, 0.41231004454454523, 0.18826510562332766, 0.47342222850224114]}}
{"id": "c0b96142-6fca-4799-a5f2-33792fa43d98", "fitness": 0.6587643749267676, "name": "AdaptiveDEOrthoRestart", "description": "Adaptive Differential Evolution with simplified mutation, orthogonal crossover, and focused restarts, employing a smaller population size and simplified adaptation for efficiency.", "code": "import numpy as np\n\nclass AdaptiveDEOrthoRestart:\n    def __init__(self, budget=10000, dim=10, pop_size=25, stagnation_threshold=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = 0.5\n        self.CR = 0.9\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n        self.p = 0.1\n\n    def orthogonal_crossover(self, x, mutant):\n        if self.dim in [2, 4]:\n            hadamard_matrix = np.array([[1, 1], [1, -1]]) if self.dim == 2 else np.array([[1, 1, 1, 1], [1, -1, 1, -1], [1, 1, -1, -1], [1, -1, -1, 1]])\n            trial = np.copy(x)\n            for i in range(self.dim):\n                trial[i] = 0.5 * (x[i] + mutant[i]) + 0.5 * hadamard_matrix[i % self.dim][i // (self.dim // hadamard_matrix.shape[1])] * (x[i] - mutant[i])\n            return trial\n        else:\n            return np.where(np.random.rand(self.dim) < 0.5, mutant, x)\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        \n        if np.min(self.fitness) < self.f_opt:\n            self.f_opt = np.min(self.fitness)\n            self.x_opt = self.population[np.argmin(self.fitness)]\n\n        self.best_fitness_history.append(self.f_opt)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 2, replace=False)\n                \n                if np.random.rand() < self.p:\n                    x_r1 = self.population[np.argmin(self.fitness)]  # Use best individual\n                else:\n                    x_r1 = self.population[idxs[0]]\n\n                x_r2 = self.population[idxs[1]]\n                    \n                mutant = x_r1 + self.F * (x_r2 - self.population[i])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = self.orthogonal_crossover(self.population[i], mutant)\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Stagnation detection and restart\n            if self.f_opt >= self.best_fitness_history[-1]:\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n\n            self.best_fitness_history.append(self.f_opt)\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                # Restart the population\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.budget -= self.pop_size\n                self.stagnation_counter = 0\n                if np.min(self.fitness) < self.f_opt:\n                    self.f_opt = np.min(self.fitness)\n                    self.x_opt = self.population[np.argmin(self.fitness)]\n                self.best_fitness_history.append(self.f_opt)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveDEOrthoRestart scored 0.659 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["38c95f37-0da8-4d60-9734-0ad95d7c5c65"], "operator": null, "metadata": {"aucs": [0.21290215563409276, 0.5213734087454913, 0.7369446847295476, 0.8604644543837178, 0.8031586925729953, 0.8546907451860948, 0.33035690563093045, 0.729362600928056, 0.8037495348331007, 0.7506105243904967, 0.8758858822956416, 0.9958644104711006, 0.2928592688276197, 0.7487832704348524, 0.8969347417830852, 0.8725256815994494, 0.5712917862355447, 0]}}
{"id": "7e176d3d-661d-48f9-aaad-0f1b5b57b7f1", "fitness": 0.0, "name": "SimplifiedAdaptiveDifferentialEvolution", "description": "Simplified Adaptive Differential Evolution with adaptive F and Cr, focusing on local search and reduced archive size.", "code": "import numpy as np\n\nclass SimplifiedAdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=30, archive_size=5, Cr_init=0.7, F_init=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.Cr_init = Cr_init\n        self.F_init = F_init\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.archive = []\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.pop[best_index].copy()\n\n    def repair(self, x, lb, ub):\n        \"\"\"Repair a solution to ensure it lies within the bounds.\"\"\"\n        return np.clip(x, lb, ub)\n\n\n    def evolve(self, func):\n        F = self.F_init\n        Cr = self.Cr_init\n\n        for i in range(self.pop_size):\n            # Mutation - focused on local search\n            candidates = list(range(self.pop_size))\n            candidates.remove(i)\n            idxs = np.random.choice(candidates, 2, replace=False)\n            x_r1, x_r2 = self.pop[idxs]\n\n            x_mutated = self.pop[i] + F * (x_r1 - x_r2)\n            x_mutated = self.repair(x_mutated, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            x_trial = np.copy(self.pop[i])\n            j_rand = np.random.randint(0, self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < Cr or j == j_rand:\n                    x_trial[j] = x_mutated[j]\n\n            x_trial = self.repair(x_trial, func.bounds.lb, func.bounds.ub)\n\n            # Evaluation\n            f_trial = func(x_trial)\n            self.eval_count += 1\n\n            # Selection\n            if f_trial < self.fitness[i]:\n                # Archive management - simplified\n                if len(self.archive) < self.archive_size:\n                    self.archive.append(self.pop[i].copy())\n                elif self.archive: # Ensure archive is not empty before attempting to replace\n                    replace_idx = np.random.randint(self.archive_size)\n                    self.archive[replace_idx] = self.pop[i].copy()\n\n\n                self.pop[i] = x_trial\n                self.fitness[i] = f_trial\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = x_trial.copy()\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.archive = []  # Initialize archive\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm SimplifiedAdaptiveDifferentialEvolution scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["8d187161-d149-4c58-9600-b4f290f85023"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "cf848f4f-3ac5-4b60-a3a4-6556d37c87b0", "fitness": 0.4339923696199922, "name": "AdaptiveDifferentialEvolution", "description": "Adaptive Differential Evolution with a dynamic population size reduction strategy, orthogonal crossover, and a focused restart mechanism based on stagnation detection to balance exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, Cr=0.9, stagnation_iters=50, reduction_factor=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.initial_pop_size = pop_size\n        self.F = F\n        self.Cr = Cr\n        self.stagnation_iters = stagnation_iters\n        self.reduction_factor = reduction_factor\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.no_improvement_count = 0\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.update_best(self.pop, self.fitness)\n\n    def mutate(self):\n        mutated_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.pop[idxs]\n            mutated_pop[i] = x_r1 + self.F * (x_r2 - x_r3)\n\n        return mutated_pop\n\n    def crossover(self, mutated_pop):\n        crossed_pop = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            j_rand = np.random.randint(0, self.dim)\n            u = np.random.rand(self.dim) < self.Cr\n            u[j_rand] = True\n            crossed_pop[i] = np.where(u, mutated_pop[i], self.pop[i])\n        return crossed_pop\n\n    def selection(self, func, crossed_pop):\n        new_fitness = np.array([func(x) for x in crossed_pop])\n        self.eval_count += self.pop_size\n\n        improved = new_fitness < self.fitness\n        self.pop[improved] = crossed_pop[improved]\n        self.fitness[improved] = new_fitness[improved]\n\n        self.update_best(self.pop, self.fitness)\n\n    def restart_population(self, func):\n        new_pop = np.random.normal(loc=self.x_opt, scale=0.1, size=(self.pop_size, self.dim))\n        new_pop = np.clip(new_pop, func.bounds.lb, func.bounds.ub)\n        new_fitness = np.array([func(x) for x in new_pop])\n        self.eval_count += self.pop_size\n\n        self.pop = new_pop\n        self.fitness = new_fitness\n        self.update_best(self.pop, self.fitness)\n\n    def reduce_population(self):\n        if self.pop_size > 10:  # Minimum population size\n            self.pop_size = int(self.pop_size * self.reduction_factor)\n            \n            #Sort the population based on fitness\n            sorted_indices = np.argsort(self.fitness)\n            self.pop = self.pop[sorted_indices[:self.pop_size]]\n            self.fitness = self.fitness[sorted_indices[:self.pop_size]]\n            print(f\"Population size reduced to: {self.pop_size}\")\n\n    def update_best(self, pop, fitness):\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = pop[best_index].copy()\n            self.no_improvement_count = 0\n        else:\n            self.no_improvement_count += 1\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.no_improvement_count = 0\n        self.pop_size = self.initial_pop_size  # Reset population size\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            mutated_pop = self.mutate()\n            mutated_pop = np.clip(mutated_pop, func.bounds.lb, func.bounds.ub)\n            crossed_pop = self.crossover(mutated_pop)\n            crossed_pop = np.clip(crossed_pop, func.bounds.lb, func.bounds.ub)\n            self.selection(func, crossed_pop)\n\n            if self.no_improvement_count > self.stagnation_iters:\n                self.restart_population(func)\n                self.no_improvement_count = 0\n                self.reduce_population() #Reduce population upon stagnation\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.434 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3c4ea5b2-1129-4241-a847-9ca979ef70a0"], "operator": null, "metadata": {"aucs": [0.3406267870059252, 0.6973145144148944, 0.6980281770591491, 0]}}
{"id": "e390b5ef-3fdf-4b12-ad5d-56234dcaa7f6", "fitness": 0.679847183700214, "name": "SimplifiedAdaptiveDifferentialEvolution", "description": "Simplified Adaptive Differential Evolution with adaptive F and Cr, focusing on efficient exploration and exploitation without an explicit archive.", "code": "import numpy as np\n\nclass SimplifiedAdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=40, Cr_init=0.7, F_init=0.6, F_decay=0.99):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.Cr_init = Cr_init\n        self.F_init = F_init\n        self.F_decay = F_decay\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.pop[best_index].copy()\n\n    def repair(self, x, lb, ub):\n        \"\"\"Repair a solution to ensure it lies within the bounds.\"\"\"\n        return np.clip(x, lb, ub)\n\n    def evolve(self, func):\n        # Adaptive F and Cr\n        F = self.F_init * self.F_decay**(self.eval_count / self.budget)\n        Cr = self.Cr_init\n\n        for i in range(self.pop_size):\n            # Mutation\n            candidates = list(range(self.pop_size))\n            candidates.remove(i)\n            idxs = np.random.choice(candidates, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.pop[idxs]\n\n            x_mutated = self.pop[i] + F * (x_r1 - x_r2 + x_r3 - self.pop[i]) # Simplified DE mutation\n            x_mutated = self.repair(x_mutated, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            x_trial = np.copy(self.pop[i])\n            j_rand = np.random.randint(0, self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < Cr or j == j_rand:\n                    x_trial[j] = x_mutated[j]\n\n            x_trial = self.repair(x_trial, func.bounds.lb, func.bounds.ub)\n\n            # Evaluation\n            f_trial = func(x_trial)\n            self.eval_count += 1\n\n            # Selection\n            if f_trial < self.fitness[i]:\n                self.pop[i] = x_trial\n                self.fitness[i] = f_trial\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = x_trial.copy()\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm SimplifiedAdaptiveDifferentialEvolution scored 0.680 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["8d187161-d149-4c58-9600-b4f290f85023"], "operator": null, "metadata": {"aucs": [0.22500508194449476, 0.62181874840392, 0.6545575158084243, 0.8908194824825935, 0.7622858387667095, 0.8263974026807253, 0.625933276995121, 0.7324118439118128, 0.7901211919322213, 0.5115442569907773, 0.8601624496205995, 0.9977547561221168, 0.48535129019434176, 0.7452307079342229, 0.9049480138436337, 0.8256541775585496, 0.5650221859097773, 0.8608604564511739, 0.21209210197938733, 0.49897289447367643]}}
{"id": "3501c32b-4172-4ba4-adf3-1366a00b2691", "fitness": 0.4111381582405504, "name": "SimplifiedAdaptiveDE", "description": "Simplified Adaptive Differential Evolution with parent-centric mutation, adaptive F, and stochastic ranking.", "code": "import numpy as np\n\nclass SimplifiedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, Cr=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_init = F_init\n        self.Cr = Cr\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.pop[best_index].copy()\n\n    def repair(self, x, lb, ub):\n        \"\"\"Repair a solution to ensure it lies within the bounds.\"\"\"\n        return np.clip(x, lb, ub)\n\n    def evolve(self, func):\n        F = self.F_init  # Fixed F for simplicity\n\n        for i in range(self.pop_size):\n            # Parent Selection (current individual is also a parent)\n            candidates = list(range(self.pop_size))\n            candidates.remove(i)\n            idxs = np.random.choice(candidates, 2, replace=False)\n            x_r1, x_r2 = self.pop[idxs]\n\n            # Mutation (Parent-centric)\n            x_mutated = self.pop[i] + F * (x_r1 - x_r2)\n            x_mutated = self.repair(x_mutated, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            x_trial = np.copy(self.pop[i])\n            j_rand = np.random.randint(0, self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.Cr or j == j_rand:\n                    x_trial[j] = x_mutated[j]\n\n            x_trial = self.repair(x_trial, func.bounds.lb, func.bounds.ub)\n\n            # Evaluation\n            f_trial = func(x_trial)\n            self.eval_count += 1\n\n            # Stochastic Ranking Selection\n            p = 0.9  # Probability of selecting based on fitness\n            if np.random.rand() < p or self.fitness[i] < f_trial:\n                if f_trial < self.fitness[i]:\n                    self.pop[i] = x_trial\n                    self.fitness[i] = f_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial.copy()\n            # Otherwise, keep the old individual\n            \n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm SimplifiedAdaptiveDE scored 0.411 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["8d187161-d149-4c58-9600-b4f290f85023"], "operator": null, "metadata": {"aucs": [0.16049473804635794, 0.2699343253519334, 0.3923870420692587, 0.4392609263410825, 0.30964371613678376, 0.42609045952356805, 0.29004096368420973, 0.34390341457066687, 0.30426571295328897, 0.19923274029499294, 0.5965888282253169, 0.9983300724372034, 0.4052319436502724, 0.3021355396339235, 0.8227399662492206, 0.4015332822920271, 0.327788547241632, 0.5184045004813937, 0.2280448205599005, 0.48671162506797505]}}
{"id": "ad7db588-8d29-4eb0-bc1c-361eba666b85", "fitness": 0.5986874596598917, "name": "AdaptiveDifferentialEvolution", "description": "Adaptive Differential Evolution with a dynamically adjusted mutation factor based on population diversity and successful search direction.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=40, Cr=0.7, F=0.6, F_decay=0.99):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.Cr = Cr\n        self.F = F\n        self.F_decay = F_decay\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.diversity_history = []\n        self.success_history = []\n\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.pop[best_index].copy()\n\n    def repair(self, x, lb, ub):\n        \"\"\"Repair solution to lie within the bounds.\"\"\"\n        return np.clip(x, lb, ub)\n\n    def calculate_diversity(self):\n        \"\"\"Calculate population diversity based on the mean pairwise distance.\"\"\"\n        distances = np.sum((self.pop[:, None, :] - self.pop[None, :, :])**2, axis=2)**0.5\n        diversity = np.mean(distances)\n        return diversity\n\n    def evolve(self, func):\n        ranked_indices = np.argsort(self.fitness)\n        diversity = self.calculate_diversity()\n        self.diversity_history.append(diversity)\n        \n        # Adjust F based on diversity. If diversity is low, reduce F to encourage exploitation\n        if len(self.diversity_history) > 1:\n            if self.diversity_history[-1] < np.mean(self.diversity_history[:-1]):\n                 self.F *= self.F_decay #Reduce F if diversity is decreasing\n\n\n        for i in range(self.pop_size):\n            # Mutation: Ranking-based selection of parents\n            p_best = self.pop[ranked_indices[0]]\n            idx_r1 = np.random.choice(ranked_indices[1:]) # Avoid selecting the best individual again\n            idx_r2 = np.random.choice(ranked_indices[1:])\n            x_r1, x_r2 = self.pop[idx_r1], self.pop[idx_r2]\n\n            x_mutated = p_best + self.F * (x_r1 - x_r2)\n            x_mutated = self.repair(x_mutated, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            x_trial = np.copy(self.pop[i])\n            j_rand = np.random.randint(0, self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.Cr or j == j_rand:\n                    x_trial[j] = x_mutated[j]\n\n            x_trial = self.repair(x_trial, func.bounds.lb, func.bounds.ub)\n\n            # Evaluation\n            f_trial = func(x_trial)\n            self.eval_count += 1\n\n            # Selection\n            if f_trial < self.fitness[i]:\n                self.pop[i] = x_trial\n                self.fitness[i] = f_trial\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = x_trial.copy()\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.diversity_history = [] #Reset diversity history\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.599 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["96508b05-e3fa-4ef4-99d0-999bb559bd67"], "operator": null, "metadata": {"aucs": [0.2287085726277841, 0.1858427200655176, 0.7498323118633008, 0.9548562229078847, 0.9028692647702826, 0.9297428792888982, 0.37558738186610385, 0.6480234270861885, 0.9156727566926135, 0.23360751712289862, 0.9419511549409785, 0.9970450862073882, 0.40958496745306094, 0.39295528177113404, 0.5866230040625839, 0.3909948104830384, 0.454356880686985, 0.94863596276469, 0.2172704503585261, 0.509588540177974]}}
{"id": "78c7352f-3929-4118-89ef-14d5e26f6025", "fitness": 0.7192856364454336, "name": "AdaptiveDifferentialEvolution", "description": "Adaptive Differential Evolution with a learning rate for F, a global best component in mutation, and a Cauchy mutation for escaping local optima.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=40, Cr=0.7, F=0.5, F_lr=0.1, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.Cr = Cr\n        self.F = F\n        self.F_lr = F_lr\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.archive = []\n        self.archive_fitness = []\n        self.archive_size = archive_size\n        self.success_history = []\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 50\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.pop[best_index].copy()\n\n    def update_archive(self, x, f):\n        if len(self.archive) < self.archive_size:\n            self.archive.append(x)\n            self.archive_fitness.append(f)\n        else:\n            worst_index = np.argmax(self.archive_fitness)\n            if f < self.archive_fitness[worst_index]:\n                self.archive[worst_index] = x\n                self.archive_fitness[worst_index] = f\n\n    def evolve(self, func):\n        best_idx = np.argmin(self.fitness)\n        x_best = self.pop[best_idx]\n        \n        for i in range(self.pop_size):\n            # Mutation\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.pop[idxs]\n            \n            # Incorporate global best and Cauchy mutation\n            if np.random.rand() < 0.1: # Probability to use Cauchy mutation\n                x_mutated = self.pop[i] + 0.1 * np.random.standard_cauchy(size=self.dim) # Cauchy mutation\n            else:\n                x_mutated = self.pop[i] + self.F * (x_best - self.pop[i]) + self.F * (x_r1 - x_r2) # Use global best\n                \n            x_mutated = np.clip(x_mutated, func.bounds.lb, func.bounds.ub)\n            \n            # Crossover\n            x_trial = np.copy(self.pop[i])\n            j_rand = np.random.randint(0, self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.Cr or j == j_rand:\n                    x_trial[j] = x_mutated[j]\n\n            # Evaluation\n            f_trial = func(x_trial)\n            self.eval_count += 1\n\n            # Selection\n            if f_trial < self.fitness[i]:\n                self.success_history.append(self.F)\n                if len(self.success_history) > 10:\n                    self.success_history.pop(0)\n                \n                # Update learning rate for F\n                self.F = self.F + self.F_lr * (0.5 - np.random.rand())\n                self.F = np.clip(self.F, 0.1, 1.0)\n\n                self.pop[i] = x_trial\n                self.fitness[i] = f_trial\n                self.update_archive(x_trial, f_trial)\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = x_trial.copy()\n                    self.stagnation_counter = 0\n            else:\n                 self.stagnation_counter += 1\n                 self.success_history = [] # Reset success history if no improvement\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.archive = []\n        self.archive_fitness = []\n        self.success_history = []\n        self.stagnation_counter = 0\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.719 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f40c4a00-3b8f-4bf9-8069-6b426133dd61"], "operator": null, "metadata": {"aucs": [0.20177287544533629, 0.545791395489562, 0.7813520575568307, 0.89398142067646, 0.8113863512420925, 0.8533650198067849, 0.5996412367023957, 0.7290042299058987, 0.7963802808533391, 0.6434499793557347, 0.9087756951398794, 0.9934663918572497, 0.5883286529700698, 0.7030758675670916, 0.9441550858349702, 0.8319081700512263, 0.6469418362278689, 0.8943096507281635, 0.24327746308302167, 0.7753490684146985]}}
{"id": "993fae49-5946-4ce9-b981-5c21eb6442e0", "fitness": 0.5925962007814367, "name": "AdaptiveDifferentialEvolution", "description": "Simplified Adaptive Differential Evolution with adaptive parameter control and bound constraint handling using clipping.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=40, Cr_init=0.5, F_init=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.Cr_init = Cr_init\n        self.F_init = F_init\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.Cr = np.full(pop_size, Cr_init)\n        self.F = np.full(pop_size, F_init)\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.pop[best_index].copy()\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            # Mutation\n            r1, r2, r3 = np.random.choice(self.pop_size, 3, replace=False)\n            x_mutated = self.pop[r1] + self.F[i] * (self.pop[r2] - self.pop[r3])\n\n            # Clip to bounds\n            x_mutated = np.clip(x_mutated, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            x_trial = np.copy(self.pop[i])\n            mask = np.random.rand(self.dim) < self.Cr[i]\n            x_trial[mask] = x_mutated[mask]\n\n            # Evaluation\n            f_trial = func(x_trial)\n            self.eval_count += 1\n\n            # Selection\n            if f_trial < self.fitness[i]:\n                self.pop[i] = x_trial\n                self.fitness[i] = f_trial\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = x_trial.copy()\n                    \n                # Adapt parameters\n                self.Cr[i] = np.random.normal(self.Cr_init, 0.1)\n                self.Cr[i] = np.clip(self.Cr[i], 0.1, 0.9)\n                self.F[i] = np.random.normal(self.F_init, 0.1)\n                self.F[i] = np.clip(self.F[i], 0.1, 0.9)\n            else:\n                 # Adapt parameters (different update if no improvement)\n                self.Cr[i] = np.random.normal(self.Cr_init, 0.05)\n                self.Cr[i] = np.clip(self.Cr[i], 0.1, 0.9)\n                self.F[i] = np.random.normal(self.F_init, 0.05)\n                self.F[i] = np.clip(self.F[i], 0.1, 0.9)\n\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.593 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["96508b05-e3fa-4ef4-99d0-999bb559bd67"], "operator": null, "metadata": {"aucs": [0.18817677673905464, 0.22282257684739804, 0.5788571288862376, 0.7729031840043896, 0.6916536200212557, 0.7457690908256085, 0.5339388038728832, 0.5762999130205967, 0.6870665500484331, 0.5858677680403097, 0.7563892133377393, 0.9928149305240838, 0.27994885597481856, 0.5990235697812769, 0.8397842521024343, 0.7701407346615845, 0.47533127097747796, 0.8216410335870856, 0.2235369947321677, 0.5099577476439008]}}
{"id": "a06dceca-d467-4f43-b329-14e5d430b04d", "fitness": 0.37236294739051273, "name": "AdaptiveDifferentialEvolution", "description": "Adaptive Differential Evolution with archive for stagnation avoidance, adaptive mutation factor based on success, and parent-centric mutation with aging.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=40, Cr=0.7, F=0.6, archive_size=10, aging_rate=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.Cr = Cr\n        self.F = F\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.archive = []\n        self.archive_fitness = []\n        self.archive_size = archive_size\n        self.success_history = []\n        self.aging_rate = aging_rate  # Rate at which fitness degrades\n        self.ages = None  # Individual ages\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.pop[best_index].copy()\n        self.ages = np.zeros(self.pop_size) # Initialize ages\n\n    def update_archive(self, x, f):\n        if len(self.archive) < self.archive_size:\n            self.archive.append(x)\n            self.archive_fitness.append(f)\n        else:\n            worst_index = np.argmax(self.archive_fitness)\n            if f < self.archive_fitness[worst_index]:\n                self.archive[worst_index] = x\n                self.archive_fitness[worst_index] = f\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            # Select parent (current individual)\n            x_p = self.pop[i] # Parent centric mutation\n\n            # Adaptive F\n            if self.success_history:\n                self.F = np.clip(np.mean(self.success_history), 0.1, 0.9)\n            \n            # Mutation\n            idxs = np.random.choice(self.pop_size, 2, replace=False)\n            x_r1, x_r2 = self.pop[idxs]\n\n            # Stagnation check: If the improvement is very small, use archive\n            if np.abs(self.fitness[i] - self.f_opt) < 1e-6 and self.archive:\n                idx_a = np.random.randint(0, len(self.archive))\n                x_r1 = self.archive[idx_a]  # Use archive member for mutation\n            \n            x_mutated = x_p + self.F * (x_r1 - x_r2)\n            x_mutated = np.clip(x_mutated, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            x_trial = np.copy(self.pop[i])\n            j_rand = np.random.randint(0, self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.Cr or j == j_rand:\n                    x_trial[j] = x_mutated[j]\n\n            # Evaluation\n            f_trial = func(x_trial)\n            self.eval_count += 1\n\n            # Selection\n            if f_trial < self.fitness[i]:\n                self.success_history.append(self.F)\n                if len(self.success_history) > 10:\n                    self.success_history.pop(0)\n                \n                self.pop[i] = x_trial\n                self.fitness[i] = f_trial\n                self.update_archive(x_trial, f_trial)\n                self.ages[i] = 0 # Reset age\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = x_trial.copy()\n            else:\n                self.success_history = [] # Reset success history if no improvement\n                self.ages[i] += 1  # Increase age if no improvement\n                self.fitness[i] += self.aging_rate * self.ages[i]  # Degrade fitness\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.archive = []\n        self.archive_fitness = []\n        self.success_history = []\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.372 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f40c4a00-3b8f-4bf9-8069-6b426133dd61"], "operator": null, "metadata": {"aucs": [0.15977350100524945, 0.2504452244739257, 0.34832358166077015, 0.4201797517051913, 0.3045014657811823, 0.380179600866148, 0.2948909674333353, 0.33414349497525786, 0.30626003336084673, 0.20603035429383387, 0.3977032652200654, 0.9962705118441713, 0.2989391102049429, 0.3009900042234742, 0.6666765071449186, 0.3567489023497008, 0.3077404765267677, 0.4593485729230513, 0.18882898915310875, 0.46928463266431175]}}
{"id": "dec2dbef-6e21-4413-91aa-82ab777531e5", "fitness": 0.6782376996252103, "name": "AdaptiveDifferentialEvolution", "description": "Adaptive Differential Evolution with self-adaptive mutation factor F and crossover rate Cr, and a repair mechanism, focusing on stagnation detection and restarting to enhance exploration.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=40, Cr_init=0.7, F_init=0.6, stagnation_threshold=1000):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.Cr_init = Cr_init\n        self.F_init = F_init\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = stagnation_threshold\n        self.Cr = np.full(pop_size, Cr_init)\n        self.F = np.full(pop_size, F_init)\n        self.best_fitness_history = []\n\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.pop[best_index].copy()\n        self.best_fitness_history.append(self.f_opt)\n\n    def repair(self, x, lb, ub):\n        \"\"\"Repair solution to lie within the bounds.\"\"\"\n        return np.clip(x, lb, ub)\n\n    def evolve(self, func):\n        ranked_indices = np.argsort(self.fitness)\n        best_index = ranked_indices[0]\n        best_individual = self.pop[best_index]\n\n        for i in range(self.pop_size):\n            # Adaptive F and Cr\n            self.F[i] = np.clip(np.random.normal(self.F_init, 0.1), 0.1, 1.0)\n            self.Cr[i] = np.clip(np.random.normal(self.Cr_init, 0.1), 0.1, 1.0)\n\n            # Mutation: Ranking-based selection of parents\n            p_best = self.pop[ranked_indices[0]]\n            idx_r1 = np.random.choice(self.pop_size)\n            idx_r2 = np.random.choice(self.pop_size)\n            while idx_r1 == i:\n                idx_r1 = np.random.choice(self.pop_size)\n            while idx_r2 == i or idx_r2 == idx_r1:\n                idx_r2 = np.random.choice(self.pop_size)\n\n            x_r1, x_r2 = self.pop[idx_r1], self.pop[idx_r2]\n\n            x_mutated = p_best + self.F[i] * (x_r1 - x_r2)\n            x_mutated = self.repair(x_mutated, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            x_trial = np.copy(self.pop[i])\n            j_rand = np.random.randint(0, self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.Cr[i] or j == j_rand:\n                    x_trial[j] = x_mutated[j]\n\n            x_trial = self.repair(x_trial, func.bounds.lb, func.bounds.ub)\n\n            # Evaluation\n            f_trial = func(x_trial)\n            self.eval_count += 1\n\n            # Selection\n            if f_trial < self.fitness[i]:\n                self.pop[i] = x_trial\n                self.fitness[i] = f_trial\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = x_trial.copy()\n\n        # Stagnation check and restart\n        self.best_fitness_history.append(self.f_opt)\n        if len(self.best_fitness_history) > self.stagnation_threshold:\n            if self.best_fitness_history[-1] >= self.best_fitness_history[-self.stagnation_threshold]:\n                # Stagnation detected: restart a portion of the population\n                num_to_restart = int(self.pop_size * 0.3)\n                worst_indices = np.argsort(self.fitness)[-num_to_restart:]\n                self.pop[worst_indices] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(num_to_restart, self.dim))\n                self.fitness[worst_indices] = [func(x) for x in self.pop[worst_indices]]\n                self.eval_count += num_to_restart\n                best_index_after_restart = np.argmin(self.fitness)\n                if self.fitness[best_index_after_restart] < self.f_opt:\n                     self.f_opt = self.fitness[best_index_after_restart]\n                     self.x_opt = self.pop[best_index_after_restart].copy()\n                self.best_fitness_history = [self.f_opt]  # Reset history with new best\n            else:\n                self.best_fitness_history.pop(0) #remove the oldest element\n\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.best_fitness_history = []\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.678 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["96508b05-e3fa-4ef4-99d0-999bb559bd67"], "operator": null, "metadata": {"aucs": [0.3567110031143218, 0.3377134841350339, 0.8737484037735122, 0.9303117719221976, 0.8926105882895184, 0.9174281938754859, 0.3764536599037015, 0.856679981201904, 0.8786719572848247, 0.22566040545481691, 0.9387631053799091, 0.9960565165629683, 0.467710210863052, 0.34862577597918853, 0.5857589049457059, 0.9096263363063154, 0.8399086695119646, 0.9476000466734638, 0.38426180019609346, 0.5004531771302275]}}
{"id": "a3655f27-b475-427a-84b9-c424ccc2c125", "fitness": -Infinity, "name": "SelfAdaptiveDE", "description": "Self-adaptive Differential Evolution with a dynamically adjusted population size based on performance and a combined mutation strategy using both current-to-best and random differential vectors.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, Cr=0.7, F=0.6, min_pop_size=10, max_pop_size=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.Cr = Cr\n        self.F = F\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.min_pop_size = min_pop_size\n        self.max_pop_size = max_pop_size\n        self.adaptation_rate = 0.1  # Rate to adjust population size\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.pop[best_index].copy()\n\n    def adjust_population_size(self):\n        # If there's been little improvement, increase pop size; otherwise decrease\n        improvement = self.f_opt - np.min(self.fitness)\n        if improvement < 1e-5:  # Little improvement\n            self.pop_size = min(int(self.pop_size * (1 + self.adaptation_rate)), self.max_pop_size)\n        else:\n            self.pop_size = max(int(self.pop_size * (1 - self.adaptation_rate)), self.min_pop_size)\n        \n        # Ensure population size stays within bounds\n        self.pop_size = np.clip(self.pop_size, self.min_pop_size, self.max_pop_size)\n        \n        # Resize population if necessary\n        if self.pop.shape[0] != self.pop_size:\n            current_size = self.pop.shape[0]\n            if self.pop_size > current_size:\n                # Add new random individuals\n                new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size - current_size, self.dim))\n                self.pop = np.vstack((self.pop, new_individuals))\n                new_fitness = np.array([func(x) for x in new_individuals])\n                self.fitness = np.concatenate((self.fitness, new_fitness))\n                self.eval_count += self.pop_size - current_size\n\n            elif self.pop_size < current_size:\n                # Remove worst individuals\n                indices_to_keep = np.argsort(self.fitness)[:self.pop_size]\n                self.pop = self.pop[indices_to_keep]\n                self.fitness = self.fitness[indices_to_keep]\n\n    def evolve(self, func):\n        self.adjust_population_size()\n        for i in range(self.pop_size):\n            # Select parent\n            x_p = self.pop[i]\n            f_p = self.fitness[i]\n\n            # Mutation: Combined current-to-best and random mutation\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.pop[idxs]\n            x_best = self.pop[np.argmin(self.fitness)]\n\n            x_mutated = x_p + self.F * (x_best - x_p) + self.F * (x_r1 - x_r2)\n            x_mutated = np.clip(x_mutated, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            x_trial = np.copy(x_p)\n            j_rand = np.random.randint(0, self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.Cr or j == j_rand:\n                    x_trial[j] = x_mutated[j]\n\n            # Evaluation\n            f_trial = func(x_trial)\n            self.eval_count += 1\n\n            # Selection\n            if f_trial < f_p:\n                self.pop[i] = x_trial\n                self.fitness[i] = f_trial\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = x_trial.copy()\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "An exception occurred: name 'func' is not defined.", "error": "", "parent_ids": ["f40c4a00-3b8f-4bf9-8069-6b426133dd61"], "operator": null, "metadata": {}}
{"id": "1dc1ed7e-aeb5-49f8-bab6-b6067b5e1bb6", "fitness": 0.7738467675195311, "name": "AdaptiveDifferentialEvolution", "description": "Adaptive Differential Evolution with an improved parent selection mechanism using tournament selection and adaptive crossover rate based on population diversity.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=40, Cr=0.7, F=0.6, archive_size=10, tournament_size=3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.Cr = Cr\n        self.F = F\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.archive = []\n        self.archive_fitness = []\n        self.archive_size = archive_size\n        self.success_history = []\n        self.tournament_size = tournament_size\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.pop[best_index].copy()\n\n    def update_archive(self, x, f):\n        if len(self.archive) < self.archive_size:\n            self.archive.append(x)\n            self.archive_fitness.append(f)\n        else:\n            worst_index = np.argmax(self.archive_fitness)\n            if f < self.archive_fitness[worst_index]:\n                self.archive[worst_index] = x\n                self.archive_fitness[worst_index] = f\n\n    def tournament_selection(self):\n        idxs = np.random.choice(self.pop_size, self.tournament_size, replace=False)\n        tournament_fitness = self.fitness[idxs]\n        winner_index = idxs[np.argmin(tournament_fitness)]\n        return winner_index\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            # Parent selection using tournament selection\n            idx_p = self.tournament_selection()\n            x_p = self.pop[idx_p]\n\n            # Adaptive F\n            if self.success_history:\n                self.F = np.clip(np.mean(self.success_history), 0.1, 0.9)\n            \n            # Mutation\n            idxs = np.random.choice(self.pop_size, 2, replace=False)\n            x_r1, x_r2 = self.pop[idxs]\n\n            # Stagnation check: If the improvement is very small, use archive\n            if np.abs(self.fitness[i] - self.f_opt) < 1e-6 and self.archive:\n                idx_a = np.random.randint(0, len(self.archive))\n                x_r1 = self.archive[idx_a]  # Use archive member for mutation\n            \n            x_mutated = x_p + self.F * (x_r1 - x_r2)\n            x_mutated = np.clip(x_mutated, func.bounds.lb, func.bounds.ub)\n\n            # Adaptive Crossover\n            diversity = np.std(self.fitness)\n            Cr = self.Cr * (1 + diversity)  # Increase Cr when diversity is high\n            Cr = np.clip(Cr, 0.1, 0.9)\n            \n            # Crossover\n            x_trial = np.copy(self.pop[i])\n            j_rand = np.random.randint(0, self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < Cr or j == j_rand:\n                    x_trial[j] = x_mutated[j]\n\n            # Evaluation\n            f_trial = func(x_trial)\n            self.eval_count += 1\n\n            # Selection\n            if f_trial < self.fitness[i]:\n                self.success_history.append(self.F)\n                if len(self.success_history) > 10:\n                    self.success_history.pop(0)\n                \n                self.pop[i] = x_trial\n                self.fitness[i] = f_trial\n                self.update_archive(x_trial, f_trial)\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = x_trial.copy()\n            else:\n                self.success_history = [] # Reset success history if no improvement\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.archive = []\n        self.archive_fitness = []\n        self.success_history = []\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.774 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f40c4a00-3b8f-4bf9-8069-6b426133dd61"], "operator": null, "metadata": {"aucs": [0.37292608994133347, 0.7852077034043798, 0.8040402075589324, 0.9223878241762916, 0.8293779949741262, 0.8773518314569249, 0.7706309757591834, 0.7838076447497312, 0.8435641192700699, 0.801045077642591, 0.9112686132691148, 0.9980192825953372, 0.6740420456667684, 0.8077502696903915, 0.9536893133338884, 0.8637654247121893, 0.7330539732605006, 0.9046871073741699, 0.3124878687655822, 0.527831982789116]}}
