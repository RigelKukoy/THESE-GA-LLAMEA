{"id": "f4fc4e1e-5dc2-4033-994e-c3c4edbf9142", "fitness": -Infinity, "name": "CMAES", "description": "Covariance Matrix Adaptation Evolution Strategy with orthogonal sampling and budget-aware adaptation.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize = None, cs = 0.3, damps = 1, c_mu = 0.3, c_cov = 0.3):\n        self.budget = budget\n        self.dim = dim\n        self.mu = np.zeros(dim)\n        self.sigma = 0.5\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.weights = np.log(self.popsize + 1) - np.log(np.arange(1, self.popsize + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = 1 / np.sum(self.weights**2)\n        self.cs = cs\n        self.damps = damps\n        self.c_mu = c_mu\n        self.c_cov = c_cov\n        self.chiN = self.dim**0.5 * (1 - 1/(4*self.dim) + 1/(21*self.dim**2))\n        self.ps = np.zeros(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.C = np.eye(self.dim)\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n\n    def __call__(self, func):\n        while self.evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            x = self.mu[:, np.newaxis] + self.sigma * np.linalg.cholesky(self.C) @ z\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.evals += self.popsize\n\n            if np.min(f) < self.f_opt:\n                    self.f_opt = np.min(f)\n                    self.x_opt = x[:, np.argmin(f)]\n                    \n            idx = np.argsort(f)\n            x_sorted = x[:, idx]\n\n            mu_old = self.mu.copy()\n            self.mu = np.dot(x_sorted[:, :self.popsize], self.weights)\n            self.ps = (1-self.cs)*self.ps + (self.cs*(2-self.cs)*self.mueff)**0.5 * np.linalg.solve(np.linalg.cholesky(self.C), (self.mu - mu_old) / self.sigma)\n            self.pc = (1-self.damps)*self.pc + (self.damps*(2-self.damps)*self.mueff)**0.5 * ((self.mu - mu_old) / self.sigma)\n            \n            art_C = 1/self.sigma * (x_sorted[:, :self.popsize] - mu_old[:, np.newaxis])\n            self.C = (1-self.c_cov) * self.C + self.c_cov * (self.mueff * np.outer(self.pc, self.pc) + (1 - self.mueff) * np.dot(art_C, np.diag(self.weights) @ art_C.T) )\n            self.sigma = self.sigma * np.exp((self.cs/self.damps) * (np.linalg.norm(self.ps)/self.chiN - 1))\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "An exception occurred: operands could not be broadcast together with shapes (2,6) (2,) (2,) .", "error": "", "parent_ids": [], "operator": null, "metadata": {}}
{"id": "da6ce70b-c673-4c92-baa9-45f0e80dc803", "fitness": -Infinity, "name": "CMAES", "description": "Covariance matrix adaptation evolution strategy with a dynamically adjusted population size and orthogonal sampling.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None):\n        self.budget = budget\n        self.dim = dim\n        self.mu = dim // 2 if popsize is None else popsize // 2  # Number of parents/offspring for recombination\n        self.lamb = 4 + int(3 * np.log(self.dim)) if popsize is None else popsize # Number of offspring\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.ccov1 = (1 / self.mueff) * min(1, (2 * self.mueff - 1) / ((self.dim + 1 + (self.mueff - 1)) + self.mueff))\n        self.ccovmu = min(1 - self.ccov1, (2 * (self.mueff - 2 + 1/self.mueff)) / ((self.dim + 2)**2 + self.mueff))\n        self.chiN = self.dim**0.5 * (1 - 1/(4*self.dim) + 1/(21*self.dim**2))\n        self.count = 0\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.m = np.zeros(self.dim)  # Mean value\n        self.sigma = 0.5 # Step size\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.C = np.eye(self.dim)\n        self.B = np.eye(self.dim)\n        self.D = np.ones(self.dim)\n        self.invC = np.eye(self.dim)\n\n        while self.count < self.budget:\n            # Generate and evaluate lambda offspring\n            x = np.zeros((self.dim, self.lamb))\n            f = np.zeros(self.lamb)\n\n            for k in range(self.lamb):\n                z = np.random.normal(0, 1, self.dim)\n                x[:, k] = self.m + self.sigma * (self.B @ (self.D * z))\n                x[:, k] = np.clip(x[:, k], func.bounds.lb, func.bounds.ub)\n\n                f[k] = func(x[:, k])\n                self.count += 1\n\n                if f[k] < self.f_opt:\n                    self.f_opt = f[k]\n                    self.x_opt = x[:, k]\n\n                if self.count >= self.budget:\n                    break\n\n            if self.count >= self.budget:\n                break\n\n            # Sort by fitness and update the mean value\n            arindex = np.argsort(f)\n            x = x[:, arindex]\n            f = f[arindex]\n\n            xmean = np.sum(x[:, :self.mu] * self.weights, axis=1)\n\n            # Cumulation and step-size control\n            ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * self.invC @ (xmean - self.m) / self.sigma\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.count / self.lamb)) / self.chiN < 1.4 + 2/(self.dim + 1)\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (xmean - self.m) / self.sigma\n\n            # Covariance matrix adaptation\n            artmp = (x[:, :self.mu] - self.m) / self.sigma\n            C1 = self.ccov1 * (self.pc[:, None] @ self.pc[None, :] + (1 - hsig) * self.ccov1 * self.C)\n            Cmu = self.ccovmu * artmp @ np.diag(self.weights) @ artmp.T\n            self.C = (1 - self.ccov1 - self.ccovmu) * self.C + C1 + Cmu\n\n            # Update B and D\n            try:\n                self.D, self.B = np.linalg.eigh(self.C)\n            except np.linalg.LinAlgError:\n                self.D, self.B = np.linalg.eigh(np.eye(self.dim))  # Reset in case of singular matrix\n            self.D = np.sqrt(np.maximum(self.D, 1e-16))\n            self.invC = self.B @ np.diag(self.D**-1) @ self.B.T\n\n            # Adapt step size sigma\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(ps)/self.chiN - 1))\n            self.m = xmean\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "An exception occurred: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 2).", "error": "", "parent_ids": [], "operator": null, "metadata": {}}
{"id": "2441497a-249b-4b62-aa39-2d245568137c", "fitness": 0.6303326779638019, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with Archive and Restart strategy.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, archive_size=10, restart_criterion=1000):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.archive_size = archive_size\n        self.restart_criterion = restart_criterion  # Evaluations without improvement before restart\n        self.archive = []  # Archive for storing potentially useful solutions\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n        self.no_improvement_count = 0\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                mutant = x_r1 + self.F * (x_r2 - x_r3)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        trial[j] = mutant[j]\n\n                # Repair\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i])\n                    else:\n                        # Replace a random element in the archive\n                        replace_index = np.random.randint(self.archive_size)\n                        self.archive[replace_index] = self.population[i]\n                    \n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    # Update best solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.no_improvement_count = 0\n                    else:\n                        self.no_improvement_count += 1\n                else:\n                    self.no_improvement_count += 1\n                    \n                if self.evals >= self.budget:\n                    break\n\n            # Restart if no improvement for a while\n            if self.no_improvement_count > self.restart_criterion:\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.evals += self.pop_size\n                self.best_index = np.argmin(self.fitness)\n                self.f_opt = self.fitness[self.best_index]\n                self.x_opt = self.population[self.best_index]\n                self.no_improvement_count = 0\n                self.archive = []\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDE scored 0.630 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.19286957736080068, 0.5092796507523292, 0.6263622136767013, 0.8595844016300404, 0.713410468454642, 0.7765437128326859, 0.5990292827094716, 0.6022974707675173, 0.6982832592696943, 0.6527178347517206, 0.796047240868194, 0.998890449495403, 0.31282397749027024, 0.645011665288143, 0.883551311907439, 0.7799028900749208, 0.4443475755858548, 0.8061862029508233, 0.20808532693682313, 0.5014290464725635]}}
{"id": "8f8b379a-7269-44c4-8487-9ae90c1dc889", "fitness": 0.6997717275011675, "name": "AdaptiveDE", "description": "An adaptive Differential Evolution strategy with a population-based archive for exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=100, F=0.5, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.population = None\n        self.fitness = None\n        self.archive = []\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n    def ensure_bounds(self, vec, bounds):\n        vec_new = []\n        for i, val in enumerate(vec):\n            if val < bounds.lb[i]:\n                vec_new.append(bounds.lb[i])\n            if val > bounds.ub[i]:\n                vec_new.append(bounds.ub[i])\n            if bounds.lb[i] <= val <= bounds.ub[i]:\n                vec_new.append(val)\n        return np.array(vec_new)\n\n    def mutate(self, bounds):\n        for i in range(self.pop_size):\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = a + self.F * (b - c)\n            mutant = self.ensure_bounds(mutant, bounds)\n            yield mutant\n\n    def crossover(self, mutant, target):\n        trial = []\n        for d in range(self.dim):\n            if np.random.rand() <= self.CR or d == np.random.randint(self.dim):\n                trial.append(mutant[d])\n            else:\n                trial.append(target[d])\n        return np.array(trial)\n\n    def update_archive(self, x):\n        if len(self.archive) < self.archive_size:\n            self.archive.append(x)\n        else:\n            idx = np.random.randint(0, self.archive_size)\n            self.archive[idx] = x\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            mutants = self.mutate(func.bounds)\n\n            for i, mutant in enumerate(mutants):\n                trial = self.crossover(mutant, self.population[i])\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    self.update_archive(trial)\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                elif np.random.rand() < 0.1 and len(self.archive) > 0:  # Probabilistic replacement from archive\n                    archive_idx = np.random.randint(0, len(self.archive))\n                    archived_vector = self.archive[archive_idx]\n                    f_archived = func(archived_vector)\n                    self.budget -=1\n                    if f_archived < self.fitness[i]:\n                        self.population[i] = archived_vector\n                        self.fitness[i] = f_archived\n                        if f_archived < self.f_opt:\n                            self.f_opt = f_archived\n                            self.x_opt = archived_vector\n\n                if self.budget <= 0:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDE scored 0.700 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.3329423998304488, 0.6619486485993051, 0.6984254148378783, 0.8590296312103967, 0.7607722596312877, 0.780748484861791, 0.6904639930381594, 0.6667507912008248, 0.7249951091947808, 0.6386761571655126, 0.8669836624515407, 0.9912284540044483, 0.7039850533205223, 0.7427712678482816, 0.9285834186826539, 0.7740779136370836, 0.6265223767912895, 0.8467577862159779, 0]}}
{"id": "91134a4b-75dc-4f12-bac7-216b570eafc9", "fitness": 0.0, "name": "AdaptiveGradientDescent", "description": "Adaptive Gradient Descent with momentum and dynamic learning rate adjustment based on the function evaluations and gradient estimates.", "code": "import numpy as np\n\nclass AdaptiveGradientDescent:\n    def __init__(self, budget=10000, dim=10, learning_rate=0.1, momentum=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.learning_rate = learning_rate\n        self.momentum = momentum\n        self.v = np.zeros(dim)  # Initialize velocity for momentum\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        x = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)  # Initialize x within bounds\n        \n        eval_count = 0\n        while eval_count < self.budget:\n            # Calculate gradient using finite differences\n            grad = np.zeros(self.dim)\n            for i in range(self.dim):\n                x_plus = x.copy()\n                x_minus = x.copy()\n                delta = 1e-4  # Small perturbation\n                x_plus[i] += delta\n                x_minus[i] -= delta\n\n                x_plus = np.clip(x_plus, func.bounds.lb, func.bounds.ub)\n                x_minus = np.clip(x_minus, func.bounds.lb, func.bounds.ub)\n\n                f_plus = func(x_plus)\n                f_minus = func(x_minus)\n                \n                if np.isinf(f_plus) or np.isnan(f_plus):\n                    f_plus = np.Inf\n                if np.isinf(f_minus) or np.isnan(f_minus):\n                    f_minus = np.Inf\n                \n                eval_count += 2\n                if eval_count > self.budget:\n                    break  # Respect the budget\n\n                grad[i] = (f_plus - f_minus) / (2 * delta)\n            if eval_count > self.budget:\n                break\n                \n            # Update velocity with momentum\n            self.v = self.momentum * self.v - self.learning_rate * grad\n\n            # Update position\n            x = x + self.v\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)  # Clip to bounds\n\n            # Evaluate current position\n            f = func(x)\n            eval_count += 1\n\n            if np.isinf(f) or np.isnan(f):\n                f = np.Inf\n\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x\n\n            # Adjust learning rate (simple decay)\n            self.learning_rate *= 0.999\n            \n            if eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveGradientDescent scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["da6ce70b-c673-4c92-baa9-45f0e80dc803"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "9e1b54ab-2c09-4f0f-bd04-dc7cbc5b7a50", "fitness": -Infinity, "name": "AdaptiveCMAES", "description": "An adaptive CMA-ES variant that dynamically adjusts the covariance matrix update based on the age of the current best solution.", "code": "import numpy as np\n\nclass AdaptiveCMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damps=1, c_mu=0.3, c_cov=0.3, age_factor=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.mu = np.zeros(dim)\n        self.sigma = 0.5\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.weights = np.log(self.popsize + 1) - np.log(np.arange(1, self.popsize + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = 1 / np.sum(self.weights**2)\n        self.cs = cs\n        self.damps = damps\n        self.c_mu = c_mu\n        self.c_cov = c_cov\n        self.chiN = self.dim**0.5 * (1 - 1/(4*self.dim) + 1/(21*self.dim**2))\n        self.ps = np.zeros(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.C = np.eye(self.dim)\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n        self.age = 0\n        self.age_factor = age_factor #Factor to reduce covariance update strength as best solution ages\n\n    def __call__(self, func):\n        while self.evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            x = self.mu + self.sigma * np.linalg.cholesky(self.C) @ z\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.evals += self.popsize\n\n            best_idx = np.argmin(f)\n            if f[best_idx] < self.f_opt:\n                self.f_opt = f[best_idx]\n                self.x_opt = x[:, best_idx]\n                self.age = 0\n            else:\n                self.age += 1  # Increment age if no improvement\n\n            idx = np.argsort(f)\n            x_sorted = x[:, idx]\n\n            mu_old = self.mu.copy()\n            self.mu = np.dot(x_sorted[:, :self.popsize], self.weights)\n            \n            self.ps = (1-self.cs)*self.ps + (self.cs*(2-self.cs)*self.mueff)**0.5 * np.linalg.solve(np.linalg.cholesky(self.C), (self.mu - mu_old) / self.sigma)\n            self.pc = (1-self.damps)*self.pc + (self.damps*(2-self.damps)*self.mueff)**0.5 * ((self.mu - mu_old) / self.sigma)\n\n            art_C = 1/self.sigma * (x_sorted[:, :self.popsize] - mu_old[:, np.newaxis])\n            # Adapt covariance matrix update based on the age of the best solution\n            age_discount = 1.0 / (1.0 + self.age_factor * self.age)\n            self.C = (1-self.c_cov) * self.C + self.c_cov * age_discount * (self.mueff * np.outer(self.pc, self.pc) + (1 - self.mueff) * np.dot(art_C, np.diag(self.weights) @ art_C.T) )\n            self.sigma = self.sigma * np.exp((self.cs/self.damps) * (np.linalg.norm(self.ps)/self.chiN - 1))\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "An exception occurred: operands could not be broadcast together with shapes (2,) (2,6) .", "error": "", "parent_ids": ["f4fc4e1e-5dc2-4033-994e-c3c4edbf9142"], "operator": null, "metadata": {}}
{"id": "527bb82b-f4aa-49a0-975a-3deb664204f7", "fitness": -Infinity, "name": "GPSurrogateOptimization", "description": "Population-based search that uses a Gaussian process to model the fitness landscape and samples new points based on the GP's predicted mean and variance, balancing exploration and exploitation.", "code": "import numpy as np\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n\nclass GPSurrogateOptimization:\n    def __init__(self, budget=10000, dim=10, n_initial_samples=10, n_restarts=5):\n        self.budget = budget\n        self.dim = dim\n        self.n_initial_samples = n_initial_samples\n        self.n_restarts = n_restarts\n        self.X = None\n        self.y = None\n        self.gpr = None\n\n    def _initialize(self, func):\n        # Sample initial points randomly\n        X_init = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.n_initial_samples, self.dim))\n        y_init = np.array([func(x) for x in X_init])\n\n        self.X = X_init\n        self.y = y_init\n        self.f_opt = np.min(self.y)\n        self.x_opt = self.X[np.argmin(self.y)]\n\n    def _train_gp(self):\n        # Define Gaussian Process Regressor\n        kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-3, 1e3))\n        self.gpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=self.n_restarts)\n\n        # Fit Gaussian Process model\n        self.gpr.fit(self.X, self.y)\n\n    def _acquisition_function(self, x, xi=0.01):\n        # Calculate acquisition function (Upper Confidence Bound)\n        mu, sigma = self.gpr.predict(x.reshape(1, -1), return_std=True)\n        return -mu[0] + xi * sigma[0]  # Maximize UCB -> Minimize -UCB\n\n    def _propose_location(self, func, n_iter=10):\n        # Find the next point to sample by maximizing the acquisition function\n        x_start = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(n_iter, self.dim))\n        \n        best_x = None\n        best_acq = np.inf\n        \n        for x0 in x_start:\n            res = self._minimize_lbfgsb(self._acquisition_function, x0, func.bounds.lb, func.bounds.ub)\n            if res['fun'] < best_acq:\n                best_acq = res['fun']\n                best_x = res['x']\n\n        return best_x\n\n    def _minimize_lbfgsb(self, func, x0, lb, ub):\n        from scipy.optimize import minimize\n\n        bounds = [(lb[i], ub[i]) for i in range(self.dim)]\n        res = minimize(func, x0, method='L-BFGS-B', bounds=bounds)\n\n        return res\n    \n    def __call__(self, func):\n        self.count = 0\n        self._initialize(func)\n        self.count += self.n_initial_samples\n\n        while self.count < self.budget:\n            # Train Gaussian Process model\n            self._train_gp()\n\n            # Propose the next sampling location\n            x_next = self._propose_location(func)\n            \n            # Sample the objective function\n            f_next = func(x_next)\n            self.count += 1\n\n            # Update the observations\n            self.X = np.vstack((self.X, x_next))\n            self.y = np.append(self.y, f_next)\n\n            # Update best solution\n            if f_next < self.f_opt:\n                self.f_opt = f_next\n                self.x_opt = x_next\n\n            if self.count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "An exception occurred: name 'C' is not defined.", "error": "", "parent_ids": ["da6ce70b-c673-4c92-baa9-45f0e80dc803"], "operator": null, "metadata": {}}
{"id": "09671e23-81de-4f46-adad-465647ec029f", "fitness": -Infinity, "name": "BudgetAwareCMAES", "description": "An enhanced CMA-ES variant with budget-aware parameter adaptation, dynamic population sizing, and orthogonal sampling to improve exploration and exploitation.", "code": "import numpy as np\n\nclass BudgetAwareCMAES:\n    def __init__(self, budget=10000, dim=10, initial_sigma=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.mu = np.zeros(dim)\n        self.sigma = initial_sigma\n        self.popsize = self._calculate_popsize()\n        self.weights = np.log(self.popsize + 1) - np.log(np.arange(1, self.popsize + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = 1 / np.sum(self.weights**2)\n        self.cs = 0.3\n        self.damps = 1 + (2 * max(0, np.log((self.budget / self.dim) / self.popsize))**0.5)\n        self.c_mu = 0.3\n        self.c_cov = 0.3\n        self.chiN = self.dim**0.5 * (1 - 1/(4*self.dim) + 1/(21*self.dim**2))\n        self.ps = np.zeros(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.C = np.eye(self.dim)\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n        self.min_popsize = 4 \n        self.max_popsize = 50 #avoid too large populations\n\n    def _calculate_popsize(self):\n         #Dynamic popsize, scaling with budget and dim\n        base_popsize = 4 + int(3 * np.log(self.dim))\n        adapted_popsize = min(self.max_popsize, max(self.min_popsize, int(base_popsize * (self.budget / 10000)**0.25)))\n        return adapted_popsize\n\n    def __call__(self, func):\n        while self.evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            x = self.mu + self.sigma * np.linalg.cholesky(self.C) @ z\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.evals += self.popsize\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n\n            idx = np.argsort(f)\n            x_sorted = x[:, idx]\n\n            mu_old = self.mu.copy()\n            self.mu = np.dot(x_sorted[:, :self.popsize], self.weights)\n            self.ps = (1-self.cs)*self.ps + (self.cs*(2-self.cs)*self.mueff)**0.5 * np.linalg.solve(np.linalg.cholesky(self.C), (self.mu - mu_old) / self.sigma)\n            self.pc = (1-self.damps)*self.pc + (self.damps*(2-self.damps)*self.mueff)**0.5 * ((self.mu - mu_old) / self.sigma)\n            \n            art_C = (x_sorted[:, :self.popsize] - mu_old[:, np.newaxis]) / self.sigma\n            self.C = (1-self.c_cov) * self.C + self.c_cov * (self.mueff * np.outer(self.pc, self.pc) + (1 - self.mueff) * np.dot(art_C, np.diag(self.weights) @ art_C.T) )\n\n            self.sigma = self.sigma * np.exp((self.cs/self.damps) * (np.linalg.norm(self.ps)/self.chiN - 1))\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "An exception occurred: 'BudgetAwareCMAES' object has no attribute 'max_popsize'.", "error": "", "parent_ids": ["f4fc4e1e-5dc2-4033-994e-c3c4edbf9142"], "operator": null, "metadata": {}}
{"id": "c330b093-2f54-4435-8e08-c4a395365a0e", "fitness": 0.0, "name": "AdaptiveDELocalSearch", "description": "An Adaptive Differential Evolution strategy that utilizes a self-adaptive mutation factor F and crossover rate CR, along with a local search component to intensify the search around promising solutions.", "code": "import numpy as np\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_init=0.5, CR_init=0.7, local_search_prob=0.1, local_search_radius=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_init = F_init\n        self.CR_init = CR_init\n        self.local_search_prob = local_search_prob\n        self.local_search_radius = local_search_radius\n        self.F = np.full(pop_size, F_init)\n        self.CR = np.full(pop_size, CR_init)\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Adaptation of F and CR\n                self.F[i] = np.clip(np.random.normal(self.F_init, 0.1), 0.1, 1.0)\n                self.CR[i] = np.clip(np.random.normal(self.CR_init, 0.1), 0.1, 1.0)\n                \n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                mutant = x_r1 + self.F[i] * (x_r2 - x_r3)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR[i] or j == j_rand:\n                        trial[j] = mutant[j]\n\n                # Repair\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Local Search\n                if np.random.rand() < self.local_search_prob:\n                    trial = self.local_search(trial, func, self.local_search_radius)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    # Update best solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        \n                if self.evals >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt\n\n    def local_search(self, x, func, radius):\n        x_best = x\n        f_best = func(x)\n        num_neighbors = 10\n        \n        for _ in range(num_neighbors):\n            neighbor = x + np.random.uniform(-radius, radius, size=self.dim)\n            neighbor = np.clip(neighbor, func.bounds.lb, func.bounds.ub)\n            f_neighbor = func(neighbor)\n            self.evals += 1\n\n            if f_neighbor < f_best:\n                f_best = f_neighbor\n                x_best = neighbor\n\n        return x_best", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDELocalSearch scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2441497a-249b-4b62-aa39-2d245568137c"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "17e06b34-8b17-4cec-846d-3b59a3282887", "fitness": 0.0, "name": "AdaptivePopulationSearch", "description": "A population-based algorithm that adapts the search distribution using a combination of global and local search strategies, dynamically adjusting parameters based on the fitness landscape.", "code": "import numpy as np\n\nclass AdaptivePopulationSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=None):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 2 * dim + 1 if pop_size is None else pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population = None\n        self.fitness = None\n        self.best_fitness = np.inf\n        self.best_solution = None\n        self.mutation_rate = 0.1\n        self.crossover_rate = 0.7\n        self.local_search_prob = 0.1\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.zeros(self.pop_size)\n\n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.fitness[i] == 0:\n              self.fitness[i] = func(self.population[i])\n              self.budget -= 1\n\n              if self.fitness[i] < self.best_fitness:\n                  self.best_fitness = self.fitness[i]\n                  self.best_solution = self.population[i]\n              if self.budget <= 0:\n                  break\n\n    def selection(self):\n        # Tournament selection\n        indices = np.random.choice(self.pop_size, size=2, replace=False)\n        if self.fitness[indices[0]] < self.fitness[indices[1]]:\n            return indices[0]\n        else:\n            return indices[1]\n\n    def mutation(self, individual):\n        for i in range(self.dim):\n            if np.random.rand() < self.mutation_rate:\n                individual[i] += np.random.normal(0, 0.1 * (self.ub - self.lb))\n                individual[i] = np.clip(individual[i], self.lb, self.ub)\n        return individual\n\n    def crossover(self, parent1, parent2):\n        if np.random.rand() < self.crossover_rate:\n            crossover_point = np.random.randint(1, self.dim)\n            child1 = np.concatenate((parent1[:crossover_point], parent2[crossover_point:]))\n            child2 = np.concatenate((parent2[:crossover_point], parent1[crossover_point:]))\n            return child1, child2\n        else:\n            return parent1, parent2\n\n    def local_search(self, func, individual):\n      # Perform a simple random step in the vicinity of the individual\n      new_individual = individual + np.random.normal(0, 0.05 * (self.ub - self.lb), size=self.dim)\n      new_individual = np.clip(new_individual, self.lb, self.ub)\n      new_fitness = func(new_individual)\n      self.budget -= 1\n\n      if new_fitness < func(individual):\n          return new_individual, new_fitness\n      else:\n          return individual, func(individual)\n\n    def __call__(self, func):\n        self.initialize_population()\n        self.evaluate_population(func)\n\n        while self.budget > 0:\n            new_population = np.copy(self.population)\n\n            for i in range(self.pop_size):\n                # Selection\n                parent_index1 = self.selection()\n                parent_index2 = self.selection()\n                parent1 = self.population[parent_index1]\n                parent2 = self.population[parent_index2]\n\n                # Crossover\n                child1, child2 = self.crossover(parent1, parent2)\n\n                # Mutation\n                child1 = self.mutation(child1)\n\n                # Local Search\n                if np.random.rand() < self.local_search_prob and self.budget > 0:\n                  child1, fitness_child1 = self.local_search(func, child1)\n                  new_population[i] = child1\n                  if fitness_child1 < self.fitness[i]:\n                      self.fitness[i] = fitness_child1\n                      if fitness_child1 < self.best_fitness:\n                          self.best_fitness = fitness_child1\n                          self.best_solution = child1\n\n                else:\n                  new_population[i] = child1\n                  self.fitness[i] = 0 # mark as unevaluated\n\n\n            self.population = new_population\n            self.evaluate_population(func)\n\n            if self.budget <= 0:\n                break\n\n        return self.best_fitness, self.best_solution", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptivePopulationSearch scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["da6ce70b-c673-4c92-baa9-45f0e80dc803"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "c716a668-5738-4f2c-bb18-3d45b869887e", "fitness": 0.28370638579203833, "name": "AdaptiveDE_CMA", "description": "Adaptive Differential Evolution with self-adaptive parameters, covariance matrix learning, and orthogonal design for enhanced exploration.", "code": "import numpy as np\n\nclass AdaptiveDE_CMA:\n    def __init__(self, budget=10000, dim=10, pop_size=None, F=0.5, CR=0.7, archive_size=10, restart_criterion=1000, cma_learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 5 * dim if pop_size is None else pop_size  # Adapt pop_size based on dimension\n        self.F = F\n        self.CR = CR\n        self.archive_size = archive_size\n        self.restart_criterion = restart_criterion  # Evaluations without improvement before restart\n        self.archive = []  # Archive for storing potentially useful solutions\n        self.cma_learning_rate = cma_learning_rate\n        self.C = np.eye(dim)  # Covariance matrix\n        self.mean = None\n\n    def orthogonal_design(self, n_points):\n        \"\"\"Generates an orthogonal design.\"\"\"\n        if n_points <= 0:\n            return np.array([])\n\n        if n_points == 1:\n            return np.array([[0.5] * self.dim])\n\n        # Simple latin hypercube sampling\n        points = np.zeros((n_points, self.dim))\n        for i in range(self.dim):\n            points[:, i] = (np.random.permutation(n_points) + 0.5) / n_points\n        return points\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n        # Initialize population using orthogonal design\n        design = self.orthogonal_design(self.pop_size)\n        self.population = func.bounds.lb + design * (func.bounds.ub - func.bounds.lb)\n        \n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n        self.no_improvement_count = 0\n        self.mean = self.x_opt.copy() # Initialize mean for CMA\n\n        while self.evals < self.budget:\n            # Adaptive F and CR\n            self.F = np.random.normal(0.5, 0.1)\n            self.CR = np.random.normal(0.7, 0.1)\n            self.F = np.clip(self.F, 0.1, 0.9)\n            self.CR = np.clip(self.CR, 0.1, 0.9)\n            \n            for i in range(self.pop_size):\n                # Mutation with covariance matrix\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n\n                # Sample a perturbation from a multivariate normal distribution\n                z = np.random.normal(0, 1, self.dim)\n                mutation_vector = x_r1 + self.F * (x_r2 - x_r3) + np.sqrt(self.C) @ z\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        trial[j] = mutation_vector[j]\n\n                # Repair\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i])\n                    else:\n                        # Replace a random element in the archive\n                        replace_index = np.random.randint(self.archive_size)\n                        self.archive[replace_index] = self.population[i]\n                    \n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    # Update best solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.no_improvement_count = 0\n                    else:\n                        self.no_improvement_count += 1\n                else:\n                    self.no_improvement_count += 1\n\n                if self.evals >= self.budget:\n                    break\n            \n            # CMA-like update of the covariance matrix\n            diff = self.population[np.argmin(self.fitness)] - self.mean\n            self.C = (1 - self.cma_learning_rate) * self.C + self.cma_learning_rate * np.outer(diff, diff)\n            self.mean = self.population[np.argmin(self.fitness)] # Update mean\n\n            # Restart if no improvement for a while\n            if self.no_improvement_count > self.restart_criterion:\n                design = self.orthogonal_design(self.pop_size)\n                self.population = func.bounds.lb + design * (func.bounds.ub - func.bounds.lb)\n                self.fitness = np.array([func(x) for x in self.population])\n                self.evals += self.pop_size\n                self.best_index = np.argmin(self.fitness)\n                self.f_opt = self.fitness[self.best_index]\n                self.x_opt = self.population[self.best_index]\n                self.no_improvement_count = 0\n                self.archive = []\n                self.C = np.eye(self.dim) # Reset covariance matrix\n                self.mean = self.x_opt.copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDE_CMA scored 0.284 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2441497a-249b-4b62-aa39-2d245568137c"], "operator": null, "metadata": {"aucs": [0.1038924751697552, 0.15752410102679948, 0.28811158028884876, 0.31952739287627785, 0.20875658031870326, 0.22028867793363305, 0.2585588295872653, 0.20679585032068304, 0.20727981612721147, 0.15318025652072143, 0.23323268814945597, 0.9988043292784488, 0.23217838128299284, 0.19782154099697846, 0.5434567182774194, 0.25028844212623946, 0.24508489971255676, 0.23925871976332735, 0.15764259962138638, 0.4524438364620611]}}
{"id": "caa23290-d3d0-4f81-9f6d-133b65716220", "fitness": 0.6010715826336943, "name": "SelfAdaptiveDE", "description": "Self-Adaptive Differential Evolution with a probabilistic archive and orthogonal crossover for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, restart_criterion=1000, p_archive=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.restart_criterion = restart_criterion\n        self.p_archive = p_archive # Probability of using an archived solution\n        self.archive = []\n        self.F = np.full(pop_size, 0.5)  # Initialize F for each individual\n        self.CR = np.full(pop_size, 0.7) # Initialize CR for each individual\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n        self.no_improvement_count = 0\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Self-adapt F and CR\n                if np.random.rand() < 0.1:\n                    self.F[i] = np.random.normal(0.5, 0.3)\n                    self.F[i] = np.clip(self.F[i], 0.1, 1.0)\n                if np.random.rand() < 0.1:\n                    self.CR[i] = np.random.normal(0.7, 0.1)\n                    self.CR[i] = np.clip(self.CR[i], 0.1, 1.0)\n\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                \n                # Use archive with probability p_archive\n                if np.random.rand() < self.p_archive and len(self.archive) > 0:\n                    x_r4 = self.archive[np.random.randint(len(self.archive))]\n                    mutant = x_r1 + self.F[i] * (x_r2 - x_r3) + self.F[i] * (x_r4 - self.population[i])\n                else:\n                    mutant = x_r1 + self.F[i] * (x_r2 - x_r3)\n                \n\n                # Orthogonal Crossover\n                trial = np.copy(self.population[i])\n                num_changes = 0\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR[i]:\n                        trial[j] = mutant[j]\n                        num_changes += 1\n                if num_changes == 0:\n                    j_rand = np.random.randint(self.dim)\n                    trial[j_rand] = mutant[j_rand]\n\n                # Repair\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    # Update archive probabilistically\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i])\n                    else:\n                        if np.random.rand() < 0.5: #Probabilistic archive update\n                            replace_index = np.random.randint(self.archive_size)\n                            self.archive[replace_index] = self.population[i]\n                    \n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    # Update best solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.no_improvement_count = 0\n                    else:\n                        self.no_improvement_count += 1\n                else:\n                    self.no_improvement_count += 1\n                    \n                if self.evals >= self.budget:\n                    break\n\n            # Restart if no improvement for a while\n            if self.no_improvement_count > self.restart_criterion:\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.evals += self.pop_size\n                self.best_index = np.argmin(self.fitness)\n                self.f_opt = self.fitness[self.best_index]\n                self.x_opt = self.population[self.best_index]\n                self.no_improvement_count = 0\n                self.archive = []\n                self.F = np.full(self.pop_size, 0.5)  # Re-initialize F\n                self.CR = np.full(self.pop_size, 0.7)  # Re-initialize CR\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm SelfAdaptiveDE scored 0.601 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2441497a-249b-4b62-aa39-2d245568137c"], "operator": null, "metadata": {"aucs": [0.18519829733885318, 0.27264119629712213, 0.6007656175545723, 0.8282723268399887, 0.6900953391500866, 0.7299210531523767, 0.5630390212797807, 0.5563907981579788, 0.6632940750084857, 0.6352605284255262, 0.7677243394451065, 0.9983589069354412, 0.31361595097808903, 0.5836144713745601, 0.857292332580961, 0.7169290601175047, 0.545838811437231, 0.7939176061015816, 0.2108694192227153, 0.5083925012759252]}}
{"id": "2d907ec0-5026-4ff5-89ca-6c32ada02572", "fitness": 0.5829176742699743, "name": "AdaptiveDE_v2", "description": "An adaptive Differential Evolution strategy that dynamically adjusts F and CR based on population diversity and success rate.", "code": "import numpy as np\n\nclass AdaptiveDE_v2:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_base=0.5, CR_base=0.7, archive_size=10, restart_criterion=1000, F_adapt_rate=0.1, CR_adapt_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_base = F_base  # Mean value for F\n        self.CR_base = CR_base # Mean value for CR\n        self.F = np.full(pop_size, F_base)\n        self.CR = np.full(pop_size, CR_base)\n        self.archive_size = archive_size\n        self.restart_criterion = restart_criterion  # Evaluations without improvement before restart\n        self.archive = []  # Archive for storing potentially useful solutions\n        self.F_adapt_rate = F_adapt_rate\n        self.CR_adapt_rate = CR_adapt_rate\n        self.success_history_F = []\n        self.success_history_CR = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n        self.no_improvement_count = 0\n\n        while self.evals < self.budget:\n            successful_F = []\n            successful_CR = []\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                \n                # Sample F and CR for this individual\n                F_i = np.clip(np.random.normal(self.F[i], self.F_adapt_rate), 0.0, 2.0)\n                CR_i = np.clip(np.random.normal(self.CR[i], self.CR_adapt_rate), 0.0, 1.0)\n                \n                mutant = x_r1 + F_i * (x_r2 - x_r3)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR_i or j == j_rand:\n                        trial[j] = mutant[j]\n\n                # Repair\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i])\n                    else:\n                        # Replace a random element in the archive\n                        replace_index = np.random.randint(self.archive_size)\n                        self.archive[replace_index] = self.population[i]\n                    \n                    successful_F.append(F_i)\n                    successful_CR.append(CR_i)\n                    \n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    # Update best solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.no_improvement_count = 0\n                    else:\n                        self.no_improvement_count += 1\n                else:\n                    self.no_improvement_count += 1\n                    \n                if self.evals >= self.budget:\n                    break\n            \n            # Adapt F and CR based on successful values\n            if successful_F:\n                self.F_base = np.mean(successful_F)\n                self.F = np.clip(np.random.normal(self.F_base, self.F_adapt_rate, size=self.pop_size), 0.0, 2.0)\n            if successful_CR:\n                self.CR_base = np.mean(successful_CR)\n                self.CR = np.clip(np.random.normal(self.CR_base, self.CR_adapt_rate, size=self.pop_size), 0.0, 1.0)\n            \n\n            # Restart if no improvement for a while\n            if self.no_improvement_count > self.restart_criterion:\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.evals += self.pop_size\n                self.best_index = np.argmin(self.fitness)\n                self.f_opt = self.fitness[self.best_index]\n                self.x_opt = self.population[self.best_index]\n                self.no_improvement_count = 0\n                self.archive = []\n                self.F = np.full(self.pop_size, self.F_base)\n                self.CR = np.full(self.pop_size, self.CR_base)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDE_v2 scored 0.583 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2441497a-249b-4b62-aa39-2d245568137c"], "operator": null, "metadata": {"aucs": [0.2569886888286754, 0.32021465248523295, 0.35655456035978284, 0.8667301455299017, 0.5668911800413466, 0.5114643264933367, 0.6896954311414243, 0.5741228200001429, 0.5689556266859961, 0.7487074837760235, 0.9022336907163482, 0.9997453699525636, 0.30177497974638556, 0.3934125980811033, 0.69275043387674, 0.8588648530114512, 0.48212416472507613, 0.8713976690843059, 0.1933931686639666, 0.502331642199682]}}
{"id": "38eec28d-87c3-47e2-a8c0-8dd2093f3b02", "fitness": 0.5579792350533551, "name": "SelfAdaptiveDE", "description": "Self-Adaptive Differential Evolution with a probabilistic local search component to refine promising solutions.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=50, F_min=0.1, F_max=0.9, CR_min=0.1, CR_max=0.9, local_search_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F_min = F_min\n        self.F_max = F_max\n        self.CR_min = CR_min\n        self.CR_max = CR_max\n        self.local_search_prob = local_search_prob\n        self.population = None\n        self.fitness = None\n        self.archive = []\n        self.F = np.random.uniform(F_min, F_max, size=pop_size)\n        self.CR = np.random.uniform(CR_min, CR_max, size=pop_size)\n\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n    def ensure_bounds(self, vec, bounds):\n        vec_new = []\n        for i, val in enumerate(vec):\n            if val < bounds.lb[i]:\n                vec_new.append(bounds.lb[i])\n            if val > bounds.ub[i]:\n                vec_new.append(bounds.ub[i])\n            if bounds.lb[i] <= val <= bounds.ub[i]:\n                vec_new.append(val)\n        return np.array(vec_new)\n\n    def mutate(self, i, bounds):\n        idxs = [idx for idx in range(self.pop_size) if idx != i]\n        a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n        mutant = a + self.F[i] * (b - c)\n        mutant = self.ensure_bounds(mutant, bounds)\n        return mutant\n\n    def crossover(self, mutant, target, i):\n        trial = []\n        for d in range(self.dim):\n            if np.random.rand() <= self.CR[i] or d == np.random.randint(self.dim):\n                trial.append(mutant[d])\n            else:\n                trial.append(target[d])\n        return np.array(trial)\n\n    def update_archive(self, x):\n        if len(self.archive) < self.archive_size:\n            self.archive.append(x)\n        else:\n            idx = np.random.randint(0, self.archive_size)\n            self.archive[idx] = x\n    \n    def local_search(self, x, func, bounds, step_size=0.1):\n        x_new = x.copy()\n        for i in range(self.dim):\n            # Explore in both directions\n            delta = np.random.uniform(-step_size, step_size)\n            x_new[i] = x[i] + delta\n            x_new = self.ensure_bounds(x_new, bounds)\n        f_new = func(x_new)\n        self.budget -= 1\n        return f_new, x_new\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                mutant = self.mutate(i, func.bounds)\n                trial = self.crossover(mutant, self.population[i], i)\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    self.update_archive(trial)\n                    \n                    # Update F and CR adaptively\n                    self.F[i] = np.random.uniform(self.F_min, self.F_max)\n                    self.CR[i] = np.random.uniform(self.CR_min, self.CR_max)\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        \n                    # Local search around the improved solution\n                    if np.random.rand() < self.local_search_prob:\n                        f_local, x_local = self.local_search(trial, func, func.bounds)\n                        if f_local < f_trial:\n                            self.population[i] = x_local\n                            self.fitness[i] = f_local\n                            self.f_opt = f_local\n                            self.x_opt = x_local\n                        \n                if self.budget <= 0:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm SelfAdaptiveDE scored 0.558 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["8f8b379a-7269-44c4-8487-9ae90c1dc889"], "operator": null, "metadata": {"aucs": [0.18125145747522786, 0.2568745473363492, 0.5497938785487089, 0.7856738486394823, 0.6170139859032187, 0.693079322682467, 0.46219320345229264, 0.5117179491399853, 0.6514156675868233, 0.46742712734458713, 0.7127497172956296, 0.9893179358042398, 0.27265268494554773, 0.5532572512123144, 0.8366040573235511, 0.7309236223321289, 0.4034852378895212, 0.7639686792068597, 0.21738357610273296, 0.5028009508454326]}}
{"id": "99aa644a-2fff-4cb6-b9a8-0055b5e09d45", "fitness": -Infinity, "name": "SimplifiedCMAES", "description": "A simplified CMA-ES variant with rank-one covariance matrix update and adaptive step size control based on success rate.", "code": "import numpy as np\n\nclass SimplifiedCMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, step_size=0.5, success_rate=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = np.zeros(dim)\n        self.step_size = step_size\n        self.success_rate = success_rate\n        self.evals = 0\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.pc = np.zeros(dim) # Evolution path\n\n    def __call__(self, func):\n        while self.evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            x = self.mu[:, np.newaxis] + self.step_size * z\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.evals += self.popsize\n            \n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n\n            idx = np.argsort(f)\n            x_sorted = x[:, idx]\n            \n            mu_old = self.mu.copy()\n            self.mu = np.mean(x_sorted[:, :self.popsize // 2], axis=1) # Select best half\n\n            # Update evolution path\n            self.pc = 0.8 * self.pc + 0.2 * (self.mu - mu_old) / self.step_size\n\n            # Adjust step size based on evolution path length\n            if np.linalg.norm(self.pc) > 1.5 * self.success_rate:\n                self.step_size *= 1.2\n            elif np.linalg.norm(self.pc) < 0.5 * self.success_rate:\n                self.step_size *= 0.8\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "An exception occurred: operands could not be broadcast together with shapes (2,6) (2,) (2,) .", "error": "", "parent_ids": ["f4fc4e1e-5dc2-4033-994e-c3c4edbf9142"], "operator": null, "metadata": {}}
{"id": "da338acf-cbf2-4820-ae6f-6b7a3c2ef399", "fitness": -Infinity, "name": "AdaptiveDELocalSearch", "description": "Adaptive Differential Evolution with a self-adaptive mutation factor and a local search operator triggered based on stagnation.", "code": "import numpy as np\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_initial=0.5, CR=0.7, archive_size=10, restart_criterion=1000, local_search_prob=0.1, local_search_radius=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_initial\n        self.CR = CR\n        self.archive_size = archive_size\n        self.restart_criterion = restart_criterion  # Evaluations without improvement before restart\n        self.archive = []  # Archive for storing potentially useful solutions\n        self.local_search_prob = local_search_prob\n        self.local_search_radius = local_search_radius\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n        self.no_improvement_count = 0\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                \n                # Self-adaptive F\n                self.F = np.random.normal(0.5, 0.3)\n                self.F = np.clip(self.F, 0.1, 1.0)\n                \n                mutant = x_r1 + self.F * (x_r2 - x_r3)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        trial[j] = mutant[j]\n\n                # Repair\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i])\n                    else:\n                        # Replace a random element in the archive\n                        replace_index = np.random.randint(self.archive_size)\n                        self.archive[replace_index] = self.population[i]\n                    \n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    # Update best solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.no_improvement_count = 0\n                    else:\n                        self.no_improvement_count += 1\n                else:\n                    self.no_improvement_count += 1\n                    \n                if self.evals >= self.budget:\n                    break\n\n            # Local Search\n            if self.no_improvement_count > self.restart_criterion * 0.5 and np.random.rand() < self.local_search_prob:\n                # Apply local search to the best solution\n                x_local = np.copy(self.x_opt)\n                for j in range(self.dim):\n                    x_local[j] += np.random.uniform(-self.local_search_radius, self.local_search_radius)\n                    x_local[j] = np.clip(x_local[j], func.bounds.lb, func.bounds.ub)\n\n                f_local = func(x_local)\n                self.evals += 1\n\n                if f_local < self.f_opt:\n                    self.f_opt = f_local\n                    self.x_opt = x_local\n                    self.no_improvement_count = 0\n\n            # Restart if no improvement for a while\n            if self.no_improvement_count > self.restart_criterion:\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.evals += self.pop_size\n                self.best_index = np.argmin(self.fitness)\n                self.f_opt = self.fitness[self.best_index]\n                self.x_opt = self.population[self.best_index]\n                self.no_improvement_count = 0\n                self.archive = []\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "An exception occurred: setting an array element with a sequence..", "error": "", "parent_ids": ["2441497a-249b-4b62-aa39-2d245568137c"], "operator": null, "metadata": {}}
{"id": "015cf366-8e99-45e1-ab97-e65579852437", "fitness": -Infinity, "name": "AdaptiveDE_LocalSearch", "description": "Adaptive Differential Evolution with a diversity-based mutation strategy and a local search phase using Nelder-Mead simplex.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDE_LocalSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_base=0.5, CR_base=0.7, local_search_freq=100, local_search_iters=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.F = np.full(pop_size, F_base)\n        self.CR = np.full(pop_size, CR_base)\n        self.local_search_freq = local_search_freq\n        self.local_search_iters = local_search_iters\n        self.evals_since_ls = 0\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Diversity-based mutation\n                if np.random.rand() < 0.5:  # Exploit: Standard DE mutation\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutant = x_r1 + self.F[i] * (x_r2 - x_r3)\n                else:  # Explore: Mutation with best and random vector\n                    x_best = self.population[self.best_index]\n                    x_r1 = self.population[np.random.choice(self.pop_size)]\n                    mutant = x_best + self.F[i] * (x_r1 - self.population[i])\n                \n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR[i] or j == j_rand:\n                        trial[j] = mutant[j]\n\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.best_index = i\n\n                if self.evals >= self.budget:\n                    break\n\n            # Local search\n            self.evals_since_ls += self.pop_size\n            if self.evals_since_ls >= self.local_search_freq:\n                self.evals_since_ls = 0\n                \n                # Apply local search to the best solution\n                x_local = self.x_opt.copy()\n                \n                def obj_func(x):\n                  return func(x)\n\n                res = minimize(obj_func, x_local, method='Nelder-Mead', bounds=func.bounds, options={'maxiter': self.local_search_iters})\n                \n                if res.fun < self.f_opt:\n                    self.f_opt = res.fun\n                    self.x_opt = res.x\n                    self.population[self.best_index] = res.x\n                    self.fitness[self.best_index] = res.fun\n                self.evals += res.nit # nit contains the number of function evaluations made\n            \n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "An exception occurred: name 'minimize' is not defined.", "error": "", "parent_ids": ["2d907ec0-5026-4ff5-89ca-6c32ada02572"], "operator": null, "metadata": {}}
{"id": "6ddb605e-c6c6-430c-b124-6cb301f18c33", "fitness": -Infinity, "name": "CauchyDE", "description": "A differential evolution strategy that incorporates a Cauchy mutation operator and a dynamic population size adjustment based on stagnation detection.", "code": "import numpy as np\n\nclass CauchyDE:\n    def __init__(self, budget=10000, dim=10, pop_size_init=50, archive_size=10, restart_criterion=1000, stagnation_threshold=0.001):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_init = pop_size_init\n        self.pop_size = pop_size_init\n        self.archive_size = archive_size\n        self.restart_criterion = restart_criterion\n        self.stagnation_threshold = stagnation_threshold\n        self.archive = []\n        self.F = 0.5\n        self.CR = 0.7\n        self.min_pop_size = 10\n        self.max_pop_size = 100\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n        self.no_improvement_count = 0\n        self.previous_best_fitness = self.f_opt\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation using Cauchy distribution\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                \n                # Cauchy mutation\n                cauchy_values = np.random.standard_cauchy(size=self.dim)\n                mutant = x_r1 + self.F * cauchy_values * (x_r2 - x_r3)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR:\n                        trial[j] = mutant[j]\n\n                # Repair\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    # Update archive probabilistically\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i])\n                    else:\n                        if np.random.rand() < 0.5: #Probabilistic archive update\n                            replace_index = np.random.randint(self.archive_size)\n                            self.archive[replace_index] = self.population[i]\n                    \n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    # Update best solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.no_improvement_count = 0\n                    else:\n                        self.no_improvement_count += 1\n                else:\n                    self.no_improvement_count += 1\n                    \n                if self.evals >= self.budget:\n                    break\n            \n            # Dynamic population size adjustment based on stagnation\n            improvement = self.previous_best_fitness - self.f_opt\n            if improvement < self.stagnation_threshold:\n                # Stagnation detected, increase population size\n                self.pop_size = min(self.pop_size + 5, self.max_pop_size)\n            else:\n                # Improvement detected, decrease population size\n                self.pop_size = max(self.pop_size - 5, self.min_pop_size)\n            \n            self.previous_best_fitness = self.f_opt\n            \n            # Restart if no improvement for a while\n            if self.no_improvement_count > self.restart_criterion:\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.evals += self.pop_size\n                self.best_index = np.argmin(self.fitness)\n                self.f_opt = self.fitness[self.best_index]\n                self.x_opt = self.population[self.best_index]\n                self.no_improvement_count = 0\n                self.archive = []\n                self.pop_size = self.pop_size_init # Reset population size\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "An exception occurred: index 52 is out of bounds for axis 0 with size 50.", "error": "", "parent_ids": ["caa23290-d3d0-4f81-9f6d-133b65716220"], "operator": null, "metadata": {}}
{"id": "9b6ee5eb-1a62-4499-81a1-504cdd04abdf", "fitness": -Infinity, "name": "DynamicDE_LocalSearch", "description": "Differential Evolution with a dynamically adjusted population size and a local search powered by a gradient-free optimization method, namely Nelder-Mead.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicDE_LocalSearch:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=None, F=0.5, CR=0.7, local_search_frequency=100, pop_size_adjust_freq = 500):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = 5 * dim if initial_pop_size is None else initial_pop_size\n        self.pop_size = self.initial_pop_size\n        self.F = F\n        self.CR = CR\n        self.local_search_frequency = local_search_frequency\n        self.pop_size_adjust_freq = pop_size_adjust_freq\n        self.evals = 0\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n        # Initialize population\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n        self.generation = 0\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                mutation_vector = x_r1 + self.F * (x_r2 - x_r3)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        trial[j] = mutation_vector[j]\n\n                # Repair\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                if self.evals >= self.budget:\n                    break\n            \n            # Local Search\n            if self.generation % self.local_search_frequency == 0:\n                best_x = self.population[np.argmin(self.fitness)]\n                \n                # Nelder-Mead local search\n                res = minimize(func, best_x, method='Nelder-Mead',\n                               bounds=list(zip([func.bounds.lb] * self.dim, [func.bounds.ub] * self.dim)),\n                               options={'maxfev': self.budget - self.evals if self.evals < self.budget else 0})\n                \n                if res.success and res.fun < self.f_opt:\n                    self.f_opt = res.fun\n                    self.x_opt = res.x\n                self.evals += res.nfev # crucial to keep track of function evaluations during local search\n\n            # Adjust population size dynamically\n            if self.generation % self.pop_size_adjust_freq == 0:\n                if np.std(self.fitness) < 1e-3: # If population is too similar, increase diversity\n                    self.pop_size = int(self.pop_size * 1.2)\n                else:\n                    self.pop_size = int(self.initial_pop_size) # Revert to initial population size\n                self.pop_size = min(self.pop_size, self.budget // 2) # Prevents from exceeding budget dramatically\n                \n                # Reinitialize population\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.evals += self.pop_size\n                self.best_index = np.argmin(self.fitness)\n                self.f_opt = self.fitness[self.best_index]\n                self.x_opt = self.population[self.best_index]\n            \n            self.generation += 1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "An exception occurred: name 'minimize' is not defined.", "error": "", "parent_ids": ["c716a668-5738-4f2c-bb18-3d45b869887e"], "operator": null, "metadata": {}}
{"id": "3516e87e-fdae-40f6-84ad-48969d4df135", "fitness": -Infinity, "name": "GP_DE", "description": "A Differential Evolution strategy that integrates a local search based on a Gaussian Process surrogate model to refine the best solutions found.", "code": "import numpy as np\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n\nclass GP_DE:\n    def __init__(self, budget=10000, dim=10, pop_size=None, F=0.5, CR=0.7, local_search_points=10, gp_n_restarts=3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 5 * dim if pop_size is None else pop_size\n        self.F = F\n        self.CR = CR\n        self.local_search_points = local_search_points\n        self.gp_n_restarts = gp_n_restarts\n        self.X_train = None\n        self.y_train = None\n        self.gp = None\n\n    def init_gp(self):\n        kernel = C(1.0, constant_value_bounds=\"fixed\") * RBF(1.0, length_scale_bounds=\"fixed\")\n        self.gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=self.gp_n_restarts)\n\n    def update_gp(self, x, f):\n        if self.X_train is None:\n            self.X_train = x.reshape(1, -1)\n            self.y_train = np.array([f])\n        else:\n            self.X_train = np.vstack((self.X_train, x))\n            self.y_train = np.append(self.y_train, f)\n        \n        # Train GP if there are enough points\n        if len(self.X_train) > self.dim + 1:\n            try:\n                self.gp.fit(self.X_train, self.y_train)\n            except Exception as e:\n                print(f\"GP fit failed: {e}\")\n                self.init_gp() # Reinitialize if fitting fails\n                self.gp.fit(self.X_train, self.y_train)\n\n    def local_search(self, func, x_best):\n        \"\"\"Refines the solution using a Gaussian Process surrogate model.\"\"\"\n        if self.gp is None or len(self.X_train) <= self.dim + 1:\n            return x_best, func(x_best)  # Return if GP not initialized or not enough training data\n\n        best_x = x_best\n        best_f = func(x_best)\n        \n        # Sample points around the current best solution\n        for _ in range(self.local_search_points):\n            x_sample = x_best + np.random.normal(0, 0.1, self.dim)  # Small perturbation\n            x_sample = np.clip(x_sample, func.bounds.lb, func.bounds.ub) # Clip\n\n            try:\n                f_predicted = self.gp.predict(x_sample.reshape(1, -1))[0]\n                # Select the point if its predicted value is promising\n                if f_predicted < best_f: # Comparing predicted values\n                    f_actual = func(x_sample)\n                    if f_actual < best_f: # Comparing actual values\n                         best_x = x_sample\n                         best_f = f_actual\n                         self.update_gp(x_sample, f_actual) # Update GP after finding a better actual solution\n            except Exception as e:\n                 print(f\"GP prediction failed: {e}\")\n                 break\n\n        return best_x, best_f\n            \n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.init_gp() # Initialize GP at the start\n        \n        # Initialize population\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n\n        # Initial GP training with initial population\n        for x, f in zip(self.population, self.fitness):\n            self.update_gp(x, f)\n\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                mutation_vector = x_r1 + self.F * (x_r2 - x_r3)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        trial[j] = mutation_vector[j]\n\n                # Repair\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n                self.update_gp(trial, f_trial)\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Local search around the best solution\n            self.x_opt, self.f_opt = self.local_search(func, self.x_opt)\n            self.evals += self.local_search_points  # Approximation; actual number might be less due to GP exceptions\n            if self.evals >= self.budget:\n                break\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "An exception occurred: name 'C' is not defined.", "error": "", "parent_ids": ["c716a668-5738-4f2c-bb18-3d45b869887e"], "operator": null, "metadata": {}}
{"id": "3afce9ff-0918-4d24-a839-fea56f7b4552", "fitness": 0.0, "name": "MirroredSamplingDE", "description": "A self-adaptive differential evolution algorithm that uses a mirrored sampling strategy and a dynamically updated archive to enhance exploration and exploitation.", "code": "import numpy as np\n\nclass MirroredSamplingDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, restart_criterion=1000, p_archive=0.1, mirror_rate=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.restart_criterion = restart_criterion\n        self.p_archive = p_archive\n        self.mirror_rate = mirror_rate\n        self.archive = []\n        self.F = np.full(pop_size, 0.5)\n        self.CR = np.full(pop_size, 0.7)\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n        self.no_improvement_count = 0\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Self-adapt F and CR\n                if np.random.rand() < 0.1:\n                    self.F[i] = np.random.normal(0.5, 0.3)\n                    self.F[i] = np.clip(self.F[i], 0.1, 1.0)\n                if np.random.rand() < 0.1:\n                    self.CR[i] = np.random.normal(0.7, 0.1)\n                    self.CR[i] = np.clip(self.CR[i], 0.1, 1.0)\n\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n\n                # Use archive with probability p_archive\n                if np.random.rand() < self.p_archive and len(self.archive) > 0:\n                    x_r4 = self.archive[np.random.randint(len(self.archive))]\n                    mutant = x_r1 + self.F[i] * (x_r2 - x_r3) + self.F[i] * (x_r4 - self.population[i])\n                else:\n                    mutant = x_r1 + self.F[i] * (x_r2 - x_r3)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR[i]:\n                        trial[j] = mutant[j]\n\n                # Mirrored Sampling: Generate a mirrored candidate\n                if np.random.rand() < self.mirror_rate:\n                    mirrored_trial = 2 * np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim) - trial\n                    mirrored_trial = np.clip(mirrored_trial, func.bounds.lb, func.bounds.ub)\n                    f_mirrored_trial = func(mirrored_trial)\n                    self.evals += 1\n                    if f_mirrored_trial < func(trial):\n                        trial = mirrored_trial\n                \n\n                # Repair\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    # Update archive probabilistically\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i])\n                    else:\n                        if np.random.rand() < 0.5:\n                            replace_index = np.random.randint(self.archive_size)\n                            self.archive[replace_index] = self.population[i]\n                    \n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    # Update best solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.no_improvement_count = 0\n                    else:\n                        self.no_improvement_count += 1\n                else:\n                    self.no_improvement_count += 1\n\n                if self.evals >= self.budget:\n                    break\n\n            # Restart if no improvement for a while\n            if self.no_improvement_count > self.restart_criterion:\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.evals += self.pop_size\n                self.best_index = np.argmin(self.fitness)\n                self.f_opt = self.fitness[self.best_index]\n                self.x_opt = self.population[self.best_index]\n                self.no_improvement_count = 0\n                self.archive = []\n                self.F = np.full(self.pop_size, 0.5)\n                self.CR = np.full(self.pop_size, 0.7)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm MirroredSamplingDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["caa23290-d3d0-4f81-9f6d-133b65716220"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "2c47796d-3158-458c-9f78-b1a7e13d2f17", "fitness": -Infinity, "name": "EnhancedAdaptiveDE", "description": "Enhanced Adaptive Differential Evolution with dynamic population size adjustment, a pool of mutation strategies, and a local search phase.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size_min=20, pop_size_max=100, F_base=0.5, CR_base=0.7, archive_size=10, restart_criterion=1000, F_adapt_rate=0.1, CR_adapt_rate=0.1, local_search_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_min = pop_size_min\n        self.pop_size_max = pop_size_max\n        self.pop_size = pop_size_max  # Initial population size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.F = np.full(self.pop_size, F_base)\n        self.CR = np.full(self.pop_size, CR_base)\n        self.archive_size = archive_size\n        self.restart_criterion = restart_criterion\n        self.archive = []\n        self.F_adapt_rate = F_adapt_rate\n        self.CR_adapt_rate = CR_adapt_rate\n        self.success_history_F = []\n        self.success_history_CR = []\n        self.local_search_prob = local_search_prob\n        self.mutation_strategies = [self.mutation_rand1, self.mutation_current_to_best, self.mutation_rand2]\n\n\n    def mutation_rand1(self, population, i):\n        indices = np.random.choice(len(population), 3, replace=False)\n        x_r1, x_r2, x_r3 = population[indices]\n        return x_r1 + self.F[i] * (x_r2 - x_r3)\n\n    def mutation_current_to_best(self, population, i, best_index):\n         indices = np.random.choice(len(population), 2, replace=False)\n         x_r1, x_r2 = population[indices]\n         return population[i] + self.F[i] * (self.population[best_index] - population[i]) + self.F[i] * (x_r1 - x_r2)\n\n    def mutation_rand2(self, population, i):\n        indices = np.random.choice(len(population), 5, replace=False)\n        x_r1, x_r2, x_r3, x_r4, x_r5 = population[indices]\n        return x_r1 + self.F[i] * (x_r2 - x_r3) + self.F[i] * (x_r4 - x_r5)\n\n    def local_search(self, x, func, bounds):\n        # Apply a small perturbation to each dimension\n        x_new = x + np.random.normal(0, 0.01, size=self.dim)\n        x_new = np.clip(x_new, bounds.lb, bounds.ub)\n        f_new = func(x_new)\n        return x_new, f_new\n\n    def adjust_population_size(self):\n        # Dynamically adjust population size based on the algorithm's progress\n        if self.no_improvement_count > self.restart_criterion / 2 and self.pop_size > self.pop_size_min:\n            self.pop_size = max(self.pop_size_min, int(self.pop_size * 0.9))  # Reduce population size\n            print(f\"Reducing population size to {self.pop_size}\")\n            self.population = self.population[:self.pop_size]\n            self.fitness = self.fitness[:self.pop_size]\n            self.F = self.F[:self.pop_size]\n            self.CR = self.CR[:self.pop_size]\n        elif self.no_improvement_count < self.restart_criterion / 4 and self.pop_size < self.pop_size_max:\n            self.pop_size = min(self.pop_size_max, int(self.pop_size * 1.1))  # Increase population size\n            print(f\"Increasing population size to {self.pop_size}\")\n            new_population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size - len(self.population), self.dim))\n            new_fitness = np.array([func(x) for x in new_population])\n            self.population = np.concatenate([self.population, new_population])\n            self.fitness = np.concatenate([self.fitness, new_fitness])\n            self.F = np.concatenate([self.F, np.full(self.pop_size - len(self.F), self.F_base)])\n            self.CR = np.concatenate([self.CR, np.full(self.pop_size - len(self.CR), self.CR_base)])\n            \n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n        self.no_improvement_count = 0\n\n        while self.evals < self.budget:\n            successful_F = []\n            successful_CR = []\n\n            for i in range(self.pop_size):\n                # Mutation Strategy Selection\n                mutation_strategy = np.random.choice(self.mutation_strategies)\n                mutant = mutation_strategy(self.population, i)\n\n                # Sample F and CR for this individual\n                F_i = np.clip(np.random.normal(self.F[i], self.F_adapt_rate), 0.0, 2.0)\n                CR_i = np.clip(np.random.normal(self.CR[i], self.CR_adapt_rate), 0.0, 1.0)\n                self.F[i] = F_i\n                self.CR[i] = CR_i\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR_i or j == j_rand:\n                        trial[j] = mutant[j]\n\n                # Repair\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i])\n                    else:\n                        # Replace a random element in the archive\n                        replace_index = np.random.randint(self.archive_size)\n                        self.archive[replace_index] = self.population[i]\n\n                    successful_F.append(F_i)\n                    successful_CR.append(CR_i)\n\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    # Update best solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.no_improvement_count = 0\n                    else:\n                        self.no_improvement_count += 1\n                else:\n                    self.no_improvement_count += 1\n\n                # Local Search\n                if np.random.rand() < self.local_search_prob:\n                    x_local, f_local = self.local_search(self.population[i], func, func.bounds)\n                    self.evals += 1\n                    if f_local < self.fitness[i]:\n                        self.fitness[i] = f_local\n                        self.population[i] = x_local\n                        if f_local < self.f_opt:\n                            self.f_opt = f_local\n                            self.x_opt = x_local\n                            self.no_improvement_count = 0\n\n                if self.evals >= self.budget:\n                    break\n            # Adapt F and CR based on successful values\n            if successful_F:\n                self.F_base = np.mean(successful_F)\n                self.F = np.clip(np.random.normal(self.F_base, self.F_adapt_rate, size=self.pop_size), 0.0, 2.0)\n            if successful_CR:\n                self.CR_base = np.mean(successful_CR)\n                self.CR = np.clip(np.random.normal(self.CR_base, self.CR_adapt_rate, size=self.pop_size), 0.0, 1.0)\n\n            # Adjust Population size dynamically\n            self.adjust_population_size()\n\n            # Restart if no improvement for a while\n            if self.no_improvement_count > self.restart_criterion:\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.evals += self.pop_size\n                self.best_index = np.argmin(self.fitness)\n                self.f_opt = self.fitness[self.best_index]\n                self.x_opt = self.population[self.best_index]\n                self.no_improvement_count = 0\n                self.archive = []\n                self.F = np.full(self.pop_size, self.F_base)\n                self.CR = np.full(self.pop_size, self.CR_base)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "An exception occurred: EnhancedAdaptiveDE.mutation_current_to_best() missing 1 required positional argument: 'best_index'.", "error": "", "parent_ids": ["2d907ec0-5026-4ff5-89ca-6c32ada02572"], "operator": null, "metadata": {}}
{"id": "3ac57c88-e09c-4b79-b0a1-8302b62442aa", "fitness": 0.6295103367502565, "name": "MirroredSamplingDE", "description": "A Differential Evolution variant that uses a mirrored sampling strategy to enhance boundary exploration and a simplified self-adaptation mechanism.", "code": "import numpy as np\n\nclass MirroredSamplingDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, mirror_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.mirror_rate = mirror_rate\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                mutant = x_r1 + self.F * (x_r2 - x_r3)\n\n                # Mirrored Sampling\n                for j in range(self.dim):\n                    if np.random.rand() < self.mirror_rate:\n                        if mutant[j] < func.bounds.lb[j]:\n                            mutant[j] = 2 * func.bounds.lb[j] - mutant[j]\n                        elif mutant[j] > func.bounds.ub[j]:\n                            mutant[j] = 2 * func.bounds.ub[j] - mutant[j]\n                \n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR:\n                        trial[j] = mutant[j]\n\n                # Repair\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                if self.evals >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm MirroredSamplingDE scored 0.630 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["caa23290-d3d0-4f81-9f6d-133b65716220"], "operator": null, "metadata": {"aucs": [0.20620905016667745, 0.5602265249302819, 0.608849923885318, 0.8048590484334981, 0.6757139545118102, 0.7539905245967659, 0.6287022850164188, 0.605959105265361, 0.6843900748644668, 0.6303864747252513, 0.7709028277714816, 0.9861613816619716, 0.3090268299808253, 0.6359433735152417, 0.8773174065174956, 0.7650196065407202, 0.5373204676284573, 0.8184535982212, 0.22469940855407144, 0.5060748682178141]}}
{"id": "29a16fc3-16f0-434c-81cb-a7dfc7b8668f", "fitness": 0.4495333492812633, "name": "DynamicPopulationDE", "description": "Differential Evolution with a dynamically adjusted population size and a Cauchy mutation operator.", "code": "import numpy as np\n\nclass DynamicPopulationDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, F=0.5, CR=0.7, pop_size_reduction_factor=0.9, cauchy_gamma=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.initial_pop_size = initial_pop_size\n        self.F = F\n        self.CR = CR\n        self.pop_size_reduction_factor = pop_size_reduction_factor\n        self.cauchy_gamma = cauchy_gamma\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n    def ensure_bounds(self, vec, bounds):\n        vec_new = []\n        for i, val in enumerate(vec):\n            if val < bounds.lb[i]:\n                vec_new.append(bounds.lb[i])\n            if val > bounds.ub[i]:\n                vec_new.append(bounds.ub[i])\n            else:\n                vec_new.append(val)\n        return np.array(vec_new)\n\n    def cauchy_mutation(self, i, bounds):\n        idxs = [idx for idx in range(self.pop_size) if idx != i]\n        a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n        \n        # Cauchy mutation\n        cauchy_values = np.random.standard_cauchy(size=self.dim) * self.cauchy_gamma\n        mutant = a + self.F * (b - c) + cauchy_values\n        \n        mutant = self.ensure_bounds(mutant, bounds)\n        return mutant\n\n    def crossover(self, mutant, target):\n        trial = []\n        for d in range(self.dim):\n            if np.random.rand() <= self.CR or d == np.random.randint(self.dim):\n                trial.append(mutant[d])\n            else:\n                trial.append(target[d])\n        return np.array(trial)\n\n    def reduce_population(self):\n        # Reduce population size if stagnation is detected (e.g., no improvement in f_opt for a while)\n        # For simplicity, we reduce every time, but in a real scenario, a stagnation check would be better.\n        new_pop_size = int(self.pop_size * self.pop_size_reduction_factor)\n        if new_pop_size < 10:  # minimum population size\n            return\n\n        # Select best individuals to keep\n        best_indices = np.argsort(self.fitness)[:new_pop_size]\n        self.population = self.population[best_indices]\n        self.fitness = self.fitness[best_indices]\n        self.pop_size = new_pop_size\n        print(f\"Population size reduced to {self.pop_size}\")\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        generation = 0\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                mutant = self.cauchy_mutation(i, func.bounds)\n                trial = self.crossover(mutant, self.population[i])\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n            \n            if generation % 10 == 0 and self.pop_size > 10: # Reduce every 10 generations\n               self.reduce_population()\n               #Re-evaluate population to update f_opt accurately\n               self.fitness = np.array([func(x) for x in self.population])\n               self.budget -= self.pop_size\n               self.f_opt = np.min(self.fitness)\n               self.x_opt = self.population[np.argmin(self.fitness)]\n\n            generation += 1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm DynamicPopulationDE scored 0.450 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["38eec28d-87c3-47e2-a8c0-8dd2093f3b02"], "operator": null, "metadata": {"aucs": [0.19518688562195352, 0.3514988324886059, 0.41698726916263573, 0.7610214356698284, 0.3409748095202093, 0.43582675676979465, 0.3159751751452853, 0.3600610561826293, 0.3547165349595597, 0.29687844065933533, 0.5989369549215671, 0.9996690555623948, 0.3247354701961417, 0.35458629998487246, 0.7785341394312058, 0.4431809386544241, 0.34623342949034797, 0.5242778526055034, 0.287569498775721, 0.5038161498232505]}}
{"id": "6adee4d4-39ae-460c-80a9-dc31a4a1423b", "fitness": 0.6155573158467743, "name": "AdaptiveDE_v3", "description": "Differential Evolution with periodic fitness landscape analysis to detect stagnation and trigger population rejuvenation based on a Gaussian mutation strategy.", "code": "import numpy as np\n\nclass AdaptiveDE_v3:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_base=0.5, CR_base=0.7, archive_size=10, restart_criterion=1000, F_adapt_rate=0.1, CR_adapt_rate=0.1, stagnation_analysis_interval=200, rejuvenation_rate=0.3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_base = F_base  # Mean value for F\n        self.CR_base = CR_base # Mean value for CR\n        self.F = np.full(pop_size, F_base)\n        self.CR = np.full(pop_size, CR_base)\n        self.archive_size = archive_size\n        self.restart_criterion = restart_criterion  # Evaluations without improvement before restart\n        self.archive = []  # Archive for storing potentially useful solutions\n        self.F_adapt_rate = F_adapt_rate\n        self.CR_adapt_rate = CR_adapt_rate\n        self.success_history_F = []\n        self.success_history_CR = []\n        self.stagnation_analysis_interval = stagnation_analysis_interval\n        self.rejuvenation_rate = rejuvenation_rate\n        self.fitness_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n        self.no_improvement_count = 0\n        self.fitness_history = []\n\n        while self.evals < self.budget:\n            successful_F = []\n            successful_CR = []\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                \n                # Sample F and CR for this individual\n                F_i = np.clip(np.random.normal(self.F[i], self.F_adapt_rate), 0.0, 2.0)\n                CR_i = np.clip(np.random.normal(self.CR[i], self.CR_adapt_rate), 0.0, 1.0)\n                \n                mutant = x_r1 + F_i * (x_r2 - x_r3)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR_i or j == j_rand:\n                        trial[j] = mutant[j]\n\n                # Repair\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i])\n                    else:\n                        # Replace a random element in the archive\n                        replace_index = np.random.randint(self.archive_size)\n                        self.archive[replace_index] = self.population[i]\n                    \n                    successful_F.append(F_i)\n                    successful_CR.append(CR_i)\n                    \n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    # Update best solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.no_improvement_count = 0\n                    else:\n                        self.no_improvement_count += 1\n                else:\n                    self.no_improvement_count += 1\n                    \n                if self.evals >= self.budget:\n                    break\n            \n            # Adapt F and CR based on successful values\n            if successful_F:\n                self.F_base = np.mean(successful_F)\n                self.F = np.clip(np.random.normal(self.F_base, self.F_adapt_rate, size=self.pop_size), 0.0, 2.0)\n            if successful_CR:\n                self.CR_base = np.mean(successful_CR)\n                self.CR = np.clip(np.random.normal(self.CR_base, self.CR_adapt_rate, size=self.pop_size), 0.0, 1.0)\n            \n            self.fitness_history.append(self.f_opt)\n\n            # Stagnation analysis and rejuvenation\n            if self.evals % self.stagnation_analysis_interval == 0:\n                if len(self.fitness_history) > self.stagnation_analysis_interval:\n                    recent_fitness = self.fitness_history[-self.stagnation_analysis_interval:]\n                    fitness_std = np.std(recent_fitness)\n                    \n                    if fitness_std < 1e-6:  # Stagnation detected\n                        # Rejuvenate a portion of the population with Gaussian mutation\n                        num_rejuvenate = int(self.rejuvenation_rate * self.pop_size)\n                        indices_to_rejuvenate = np.random.choice(self.pop_size, num_rejuvenate, replace=False)\n                        \n                        for idx in indices_to_rejuvenate:\n                            self.population[idx] = np.clip(self.population[idx] + np.random.normal(0, 0.5, self.dim), func.bounds.lb, func.bounds.ub)  # Gaussian mutation\n                            self.fitness[idx] = func(self.population[idx])\n                            self.evals += 1\n                            if self.fitness[idx] < self.f_opt:\n                                self.f_opt = self.fitness[idx]\n                                self.x_opt = self.population[idx]\n\n            # Restart if no improvement for a while\n            if self.no_improvement_count > self.restart_criterion:\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.evals += self.pop_size\n                self.best_index = np.argmin(self.fitness)\n                self.f_opt = self.fitness[self.best_index]\n                self.x_opt = self.population[self.best_index]\n                self.no_improvement_count = 0\n                self.archive = []\n                self.F = np.full(self.pop_size, self.F_base)\n                self.CR = np.full(self.pop_size, self.CR_base)\n                self.fitness_history = [] # Reset fitness history\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDE_v3 scored 0.616 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2d907ec0-5026-4ff5-89ca-6c32ada02572"], "operator": null, "metadata": {"aucs": [0.2622795388525291, 0.39185307834494365, 0.5281291110326338, 0.8798802909568507, 0.7755985427763109, 0.8052777228092128, 0.7414816924371834, 0.42186840281983173, 0.7784496265200159, 0.7324763442094973, 0.40633480980791115, 0.9893968410804461, 0.29519300973464946, 0.5612572758535253, 0.6974918784835233, 0.820036737805737, 0.5464627460827408, 0.8576858685969219, 0.326138897965916, 0.4938539007651067]}}
{"id": "96f519e9-04c1-4864-b7aa-7598f78650dc", "fitness": 0.4001506852257369, "name": "AdaptiveDE_Automaton", "description": "An adaptive Differential Evolution strategy that utilizes a distance-based mutation operator and a learning automaton to select between different crossover strategies.", "code": "import numpy as np\n\nclass AdaptiveDE_Automaton:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, archive_size=10, restart_criterion=1000):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.archive_size = archive_size\n        self.restart_criterion = restart_criterion\n        self.archive = []\n        self.no_improvement_count = 0\n\n        # Learning Automaton parameters\n        self.num_crossover_strategies = 3\n        self.probabilities = np.ones(self.num_crossover_strategies) / self.num_crossover_strategies\n        self.rewards = np.zeros(self.num_crossover_strategies)\n        self.learning_rate = 0.1\n\n    def distance_based_mutation(self, x_i):\n        distances = np.linalg.norm(self.population - x_i, axis=1)\n        distances[distances == 0] = np.inf  # Avoid selecting itself\n        probabilities = 1 / distances\n        probabilities /= np.sum(probabilities)\n        indices = np.random.choice(self.pop_size, 2, replace=False, p=probabilities)\n        x_r1, x_r2 = self.population[indices]\n        mutant = x_i + self.F * (x_r1 - x_r2)\n        return mutant\n\n    def crossover_exp(self, mutant, target):\n        trial = np.copy(target)\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.CR or j == j_rand:\n                trial[j] = mutant[j]\n        return trial\n\n    def crossover_bin(self, mutant, target):\n        trial = np.copy(target)\n        for j in range(self.dim):\n            if np.random.rand() < self.CR:\n                trial[j] = mutant[j]\n        return trial\n\n    def crossover_rand(self, mutant, target):\n        trial = np.copy(target)\n        indices = np.random.choice(self.dim, int(self.CR * self.dim), replace=False)\n        trial[indices] = mutant[indices]\n        return trial\n\n    def update_probabilities(self, chosen_strategy, reward):\n        self.rewards *= 0\n        self.rewards[chosen_strategy] = reward\n        self.probabilities = (1 - self.learning_rate) * self.probabilities + self.learning_rate * self.rewards\n        self.probabilities /= np.sum(self.probabilities)\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                mutant = self.distance_based_mutation(self.population[i])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover - Learning Automaton selects the strategy\n                chosen_strategy = np.random.choice(self.num_crossover_strategies, p=self.probabilities)\n                if chosen_strategy == 0:\n                    trial = self.crossover_exp(mutant, self.population[i])\n                elif chosen_strategy == 1:\n                    trial = self.crossover_bin(mutant, self.population[i])\n                else:\n                    trial = self.crossover_rand(mutant, self.population[i])\n\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i])\n                    else:\n                        replace_index = np.random.randint(self.archive_size)\n                        self.archive[replace_index] = self.population[i]\n\n                    #Update learning automaton\n                    self.update_probabilities(chosen_strategy, 1)\n\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.no_improvement_count = 0\n                    else:\n                        self.no_improvement_count += 1\n                else:\n                    self.no_improvement_count += 1\n                    self.update_probabilities(chosen_strategy, 0)\n\n\n                if self.evals >= self.budget:\n                    break\n\n            if self.no_improvement_count > self.restart_criterion:\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.evals += self.pop_size\n                self.best_index = np.argmin(self.fitness)\n                self.f_opt = self.fitness[self.best_index]\n                self.x_opt = self.population[self.best_index]\n                self.no_improvement_count = 0\n                self.archive = []\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDE_Automaton scored 0.400 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2d907ec0-5026-4ff5-89ca-6c32ada02572"], "operator": null, "metadata": {"aucs": [0.14540240994327358, 0.27125110141253184, 0.35355787631167956, 0.68683865037839, 0.27377108765836056, 0.473596628510063, 0.2872008154444533, 0.3524741935766791, 0.2897856448337256, 0.17765724629353263, 0.39712656057368323, 0.9982166064907978, 0.28287977958841826, 0.2983901750168896, 0.7560279270397086, 0.4013985750135739, 0.32344537999510237, 0.5686469392335571, 0.1729263254469453, 0.4924197817533723]}}
{"id": "b99817b1-c968-49c4-ad24-c63809350a21", "fitness": 0.6149228816018695, "name": "DynamicAgingDE", "description": "Self-Adaptive Differential Evolution with a dynamic population size and aging mechanism to promote diversity and convergence.", "code": "import numpy as np\n\nclass DynamicAgingDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, min_pop_size=10, archive_size=10, aging_limit=500, p_archive=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = initial_pop_size\n        self.min_pop_size = min_pop_size\n        self.pop_size = initial_pop_size\n        self.archive_size = archive_size\n        self.aging_limit = aging_limit\n        self.p_archive = p_archive\n        self.archive = []\n        self.F = np.full(initial_pop_size, 0.5)\n        self.CR = np.full(initial_pop_size, 0.7)\n        self.ages = np.zeros(initial_pop_size)  # Initialize ages for each individual\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n        self.no_improvement_count = 0\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Self-adapt F and CR\n                if np.random.rand() < 0.1:\n                    self.F[i] = np.random.normal(0.5, 0.3)\n                    self.F[i] = np.clip(self.F[i], 0.1, 1.0)\n                if np.random.rand() < 0.1:\n                    self.CR[i] = np.random.normal(0.7, 0.1)\n                    self.CR[i] = np.clip(self.CR[i], 0.1, 1.0)\n\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                \n                # Use archive with probability p_archive\n                if np.random.rand() < self.p_archive and len(self.archive) > 0:\n                    x_r4 = self.archive[np.random.randint(len(self.archive))]\n                    mutant = x_r1 + self.F[i] * (x_r2 - x_r3) + self.F[i] * (x_r4 - self.population[i])\n                else:\n                    mutant = x_r1 + self.F[i] * (x_r2 - x_r3)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR[i]:\n                        trial[j] = mutant[j]\n\n                # Repair\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    # Update archive probabilistically\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i])\n                    else:\n                        if np.random.rand() < 0.5:\n                            replace_index = np.random.randint(self.archive_size)\n                            self.archive[replace_index] = self.population[i]\n                            \n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n                    self.ages[i] = 0  # Reset age\n\n                    # Update best solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.no_improvement_count = 0\n                    else:\n                        self.no_improvement_count += 1\n                else:\n                    self.ages[i] += 1\n                    self.no_improvement_count += 1\n\n                if self.evals >= self.budget:\n                    break\n\n            # Aging mechanism: remove old individuals\n            to_remove = self.ages >= self.aging_limit\n            num_to_remove = np.sum(to_remove)\n\n            if num_to_remove > 0 and self.pop_size > self.min_pop_size:\n                indices_to_remove = np.where(to_remove)[0]\n                indices_to_remove = indices_to_remove[:min(num_to_remove, self.pop_size - self.min_pop_size)] #remove at most pop_size - min_pop_size individuals\n\n                self.population = np.delete(self.population, indices_to_remove, axis=0)\n                self.fitness = np.delete(self.fitness, indices_to_remove)\n                self.F = np.delete(self.F, indices_to_remove)\n                self.CR = np.delete(self.CR, indices_to_remove)\n                self.ages = np.delete(self.ages, indices_to_remove)\n                self.pop_size = len(self.population)\n                \n                # Add new random individuals to maintain population size\n                num_to_add = num_to_remove\n                new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(num_to_add, self.dim))\n                new_fitness = np.array([func(x) for x in new_individuals])\n                self.evals += num_to_add\n\n                self.population = np.concatenate((self.population, new_individuals), axis=0)\n                self.fitness = np.concatenate((self.fitness, new_fitness))\n                self.F = np.concatenate((self.F, np.full(num_to_add, 0.5)))\n                self.CR = np.concatenate((self.CR, np.full(num_to_add, 0.7)))\n                self.ages = np.concatenate((self.ages, np.zeros(num_to_add)))\n                self.pop_size = len(self.population)\n\n                self.best_index = np.argmin(self.fitness)\n                self.f_opt = self.fitness[self.best_index]\n                self.x_opt = self.population[self.best_index]\n                \n            if self.no_improvement_count > self.budget/10: #if no improvement for 10% of the budget, restart\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.evals += self.pop_size\n                self.best_index = np.argmin(self.fitness)\n                self.f_opt = self.fitness[self.best_index]\n                self.x_opt = self.population[self.best_index]\n                self.no_improvement_count = 0\n                self.archive = []\n                self.F = np.full(self.pop_size, 0.5)\n                self.CR = np.full(self.pop_size, 0.7)\n                self.ages = np.zeros(self.pop_size)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm DynamicAgingDE scored 0.615 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["caa23290-d3d0-4f81-9f6d-133b65716220"], "operator": null, "metadata": {"aucs": [0.20182579329603723, 0.4282060531144589, 0.595743426847298, 0.8415000677124572, 0.7139611416065252, 0.7546664164289262, 0.5579245814719012, 0.5649436252323641, 0.6814061727734789, 0.604368042195087, 0.7682013309023047, 0.9967317966311436, 0.329981323309662, 0.6024240295465426, 0.8900394314018637, 0.7558938727161896, 0.503220264327486, 0.7824874681042289, 0.2042736398768371, 0.5206591545425983]}}
{"id": "9c8c63b6-5785-4c41-9b53-3dd1b301cbe7", "fitness": 0.6200604849903641, "name": "DynamicSelfAdaptiveDE", "description": "A differential evolution strategy with a dynamic population size, adaptive parameter control using a success-history based adaptation, and a local search operator to refine promising solutions.", "code": "import numpy as np\n\nclass DynamicSelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, min_pop_size=10, max_pop_size=100, archive_size=20, p_archive=0.1, local_search_freq=0.1, F_init=0.5, CR_init=0.7, sf_init=0.5, scr_init=0.5, tau1=0.1, tau2=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = initial_pop_size\n        self.min_pop_size = min_pop_size\n        self.max_pop_size = max_pop_size\n        self.pop_size = initial_pop_size\n        self.archive_size = archive_size\n        self.p_archive = p_archive\n        self.local_search_freq = local_search_freq\n        self.archive = []\n        self.F = np.full(initial_pop_size, F_init)\n        self.CR = np.full(initial_pop_size, CR_init)\n        self.SF = np.full(archive_size, sf_init) #memory for F\n        self.SCR = np.full(archive_size, scr_init) #memory for CR\n        self.memory_index = 0\n        self.tau1 = tau1\n        self.tau2 = tau2\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n        self.success_F = []\n        self.success_CR = []\n\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Adaptation of F and CR using success history\n                sf = self.SF[np.random.randint(self.archive_size)]\n                scr = self.SCR[np.random.randint(self.archive_size)]\n\n                self.F[i] = sf + self.tau1 * np.random.randn()\n                self.CR[i] = scr + self.tau2 * np.random.randn()\n\n                self.F[i] = np.clip(self.F[i], 0.1, 1.0)\n                self.CR[i] = np.clip(self.CR[i], 0.0, 1.0)\n\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n\n                # Use archive with probability p_archive\n                if np.random.rand() < self.p_archive and len(self.archive) > 0:\n                    x_r4 = self.archive[np.random.randint(len(self.archive))]\n                    mutant = x_r1 + self.F[i] * (x_r2 - x_r3) + self.F[i] * (x_r4 - self.population[i])\n                else:\n                    mutant = x_r1 + self.F[i] * (x_r2 - x_r3)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR[i]:\n                        trial[j] = mutant[j]\n\n                # Repair\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    # Success\n                    self.success_F.append(self.F[i])\n                    self.success_CR.append(self.CR[i])\n\n                    # Update archive probabilistically\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i])\n                    else:\n                        if np.random.rand() < 0.5:\n                            replace_index = np.random.randint(self.archive_size)\n                            self.archive[replace_index] = self.population[i]\n\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    # Update best solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                \n                # Local search\n                if np.random.rand() < self.local_search_freq:\n                    trial_ls = trial + 0.01 * np.random.normal(0, 1, self.dim)\n                    trial_ls = np.clip(trial_ls, func.bounds.lb, func.bounds.ub)\n                    f_trial_ls = func(trial_ls)\n                    self.evals += 1\n                    if f_trial_ls < f_trial:\n                        self.fitness[i] = f_trial_ls\n                        self.population[i] = trial_ls\n                        if f_trial_ls < self.f_opt:\n                            self.f_opt = f_trial_ls\n                            self.x_opt = trial_ls\n\n                if self.evals >= self.budget:\n                    break\n            \n            # Update memory of successful F and CR values\n            if len(self.success_F) > 0:\n                self.SF[self.memory_index] = np.mean(self.success_F)\n                self.SCR[self.memory_index] = np.mean(self.success_CR)\n                self.memory_index = (self.memory_index + 1) % self.archive_size\n                self.success_F = []\n                self.success_CR = []\n\n            # Dynamic population size adjustment (example: linear decrease)\n            remaining_evals = self.budget - self.evals\n            self.pop_size = int(self.min_pop_size + (self.initial_pop_size - self.min_pop_size) * (remaining_evals / self.budget))\n            self.pop_size = np.clip(self.pop_size, self.min_pop_size, self.max_pop_size)\n\n            #Resize population\n            if self.pop_size != self.population.shape[0]:\n                if self.pop_size > self.population.shape[0]:\n                    #Add random individuals\n                    num_new = self.pop_size - self.population.shape[0]\n                    new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(num_new, self.dim))\n                    new_fitness = np.array([func(x) for x in new_individuals])\n                    self.evals += num_new\n                    self.population = np.vstack((self.population, new_individuals))\n                    self.fitness = np.concatenate((self.fitness, new_fitness))\n                else:\n                    #Remove worst individuals\n                    num_remove = self.population.shape[0] - self.pop_size\n                    worst_indices = np.argsort(self.fitness)[-num_remove:]\n                    keep_indices = np.setdiff1d(np.arange(self.population.shape[0]), worst_indices)\n                    self.population = self.population[keep_indices]\n                    self.fitness = self.fitness[keep_indices]\n                \n                self.F = np.full(self.pop_size, 0.5)\n                self.CR = np.full(self.pop_size, 0.7)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm DynamicSelfAdaptiveDE scored 0.620 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["caa23290-d3d0-4f81-9f6d-133b65716220"], "operator": null, "metadata": {"aucs": [0.2533755053775202, 0.5788853947713102, 0.5447334013361547, 0.7847550774963219, 0.6527556462278087, 0.7454222271421023, 0.6235951026641748, 0.6261095106365093, 0.6707503880408263, 0.6038678610757657, 0.7163725365739184, 0.994901221048254, 0.32272860582549867, 0.6660535092548263, 0.8204083824981423, 0.7601281737247707, 0.5204858859214574, 0.7687665776183131, 0.2420377675990384, 0.5050769249745686]}}
{"id": "986d37b0-b011-4508-bbe0-2d197b48bd6b", "fitness": 0.0, "name": "EnhancedDE", "description": "Differential Evolution with dynamic F and CR, combined with a local search refinement using a Gaussian perturbation on the best solution found so far.", "code": "import numpy as np\n\nclass EnhancedDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n\n        while self.evals < self.budget:\n            # Dynamic F and CR\n            F = np.random.uniform(0.3, 0.9)\n            CR = np.random.uniform(0.1, 0.9)\n\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                mutant = x_r1 + F * (x_r2 - x_r3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < CR:\n                        trial[j] = mutant[j]\n\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n            \n            # Local Search around the best solution\n            if self.evals < self.budget:\n                x_local = self.x_opt + np.random.normal(0, 0.05, self.dim)  # Gaussian perturbation\n                x_local = np.clip(x_local, func.bounds.lb, func.bounds.ub)\n                f_local = func(x_local)\n                self.evals += 1\n\n                if f_local < self.f_opt:\n                    self.f_opt = f_local\n                    self.x_opt = x_local\n\n            if self.evals >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm EnhancedDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3ac57c88-e09c-4b79-b0a1-8302b62442aa"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "e59330f7-9c05-4cf5-9b20-75d45212bc50", "fitness": 0.0, "name": "OrthogonalCauchyDE", "description": "Differential Evolution with a self-adaptive population size, orthogonal learning, and a Cauchy mutation operator for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass OrthogonalCauchyDE:\n    def __init__(self, budget=10000, dim=10, pop_size_base=50, F=0.5, CR=0.7, pop_size_adapt_freq=50, pop_size_reduction_factor=0.9, orthogonal_learning_trials=5, cauchy_mutation_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_base = pop_size_base\n        self.pop_size = pop_size_base\n        self.F = F\n        self.CR = CR\n        self.pop_size_adapt_freq = pop_size_adapt_freq\n        self.pop_size_reduction_factor = pop_size_reduction_factor\n        self.orthogonal_learning_trials = orthogonal_learning_trials\n        self.cauchy_mutation_prob = cauchy_mutation_prob\n        self.evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                if np.random.rand() < self.cauchy_mutation_prob:\n                    # Cauchy mutation\n                    mutant = self.population[i] + np.random.standard_cauchy(size=self.dim) * 0.1  # Scale Cauchy distribution\n                else:\n                    # Standard DE mutation\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutant = x_r1 + self.F * (x_r2 - x_r3)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        trial[j] = mutant[j]\n\n                # Repair\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Orthogonal learning\n                best_trial = trial\n                best_f_trial = func(trial)\n                self.evals += 1\n\n                for _ in range(self.orthogonal_learning_trials):\n                    orthogonal_trial = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                    f_orthogonal_trial = func(orthogonal_trial)\n                    self.evals += 1\n\n                    if f_orthogonal_trial < best_f_trial:\n                        best_f_trial = f_orthogonal_trial\n                        best_trial = orthogonal_trial\n\n                # Selection\n                f_trial = best_f_trial\n                trial = best_trial\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            # Adapt population size\n            if self.evals % self.pop_size_adapt_freq == 0:\n                if self.pop_size > 10:  # Minimum population size\n                    self.pop_size = int(self.pop_size * self.pop_size_reduction_factor)\n                    self.pop_size = max(10, self.pop_size)  # Ensure minimum size\n                    # Resize population, keep best individuals\n                    sorted_indices = np.argsort(self.fitness)\n                    self.population = self.population[sorted_indices[:self.pop_size]]\n                    self.fitness = self.fitness[sorted_indices[:self.pop_size]]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm OrthogonalCauchyDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6adee4d4-39ae-460c-80a9-dc31a4a1423b"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "5f12b6f1-dbda-43d7-8631-b27af6a15fa4", "fitness": 0.0, "name": "OrthogonalShrinkingHypersphereDE", "description": "A Differential Evolution variant with orthogonal learning and a shrinking hypersphere local search to intensify search around the best solution.", "code": "import numpy as np\n\nclass OrthogonalShrinkingHypersphereDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_base=0.5, CR_base=0.7, hypersphere_radius=0.1, shrink_factor=0.99, local_search_interval=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.hypersphere_radius = hypersphere_radius\n        self.shrink_factor = shrink_factor\n        self.local_search_interval = local_search_interval\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                mutant = x_r1 + self.F_base * (x_r2 - x_r3)\n                \n                # Crossover\n                trial = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR_base or j == j_rand:\n                        trial[j] = mutant[j]\n\n                # Repair\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Orthogonal Learning\n                orthogonal_point = self.x_opt + np.random.normal(0, 0.01, self.dim) # Small perturbation around the best\n                orthogonal_point = np.clip(orthogonal_point, func.bounds.lb, func.bounds.ub)\n                \n                trial = 0.5 * trial + 0.5 * orthogonal_point\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        \n            # Shrinking Hypersphere Local Search\n            if self.evals % self.local_search_interval == 0:\n                for _ in range(5):\n                    # Generate a random point within the hypersphere\n                    r = np.random.uniform(0, self.hypersphere_radius)\n                    u = np.random.normal(0, 1, self.dim)\n                    norm_u = np.linalg.norm(u)\n                    x_local = self.x_opt + r * u / norm_u\n                    x_local = np.clip(x_local, func.bounds.lb, func.bounds.ub)\n                    \n                    f_local = func(x_local)\n                    self.evals += 1\n                    \n                    if f_local < self.f_opt:\n                        self.f_opt = f_local\n                        self.x_opt = x_local\n                \n                self.hypersphere_radius *= self.shrink_factor\n                self.hypersphere_radius = max(self.hypersphere_radius, 1e-6) # Minimum radius\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm OrthogonalShrinkingHypersphereDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6adee4d4-39ae-460c-80a9-dc31a4a1423b"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "e0d79df6-2873-40f5-98a3-dc0a2d368805", "fitness": 0.0, "name": "SelfOrganizingDE", "description": "A Differential Evolution strategy with a self-organizing search range, adjusting its exploration based on the population's distribution and success rate, alongside a gradient-based local search.", "code": "import numpy as np\n\nclass SelfOrganizingDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, local_search_prob=0.1, learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.local_search_prob = local_search_prob\n        self.learning_rate = learning_rate\n        self.lb = None\n        self.ub = None\n\n    def __call__(self, func):\n        self.lb = func.bounds.lb\n        self.ub = func.bounds.ub\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n        # Initialize population and search range\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n\n        # Initialize adaptive search range\n        self.search_lb = np.full(self.dim, self.lb)\n        self.search_ub = np.full(self.dim, self.ub)\n\n        while self.evals < self.budget:\n            # Update search range based on population distribution\n            self.search_lb = np.min(self.population, axis=0)\n            self.search_ub = np.max(self.population, axis=0)\n\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                mutant = x_r1 + self.F * (x_r2 - x_r3)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR:\n                        trial[j] = mutant[j]\n\n                # Repair and adjust within self-organized search range\n                trial = np.clip(trial, self.search_lb, self.search_ub)\n                trial = np.clip(trial, self.lb, self.ub) #Ensure bounds are still respected.\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    # Success: Expand search range slightly\n                    self.search_lb = np.minimum(self.search_lb, trial - self.learning_rate * np.abs(trial))\n                    self.search_ub = np.maximum(self.search_ub, trial + self.learning_rate * np.abs(trial))\n                    self.search_lb = np.clip(self.search_lb, self.lb, self.ub)\n                    self.search_ub = np.clip(self.search_ub, self.lb, self.ub)\n\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                # Local search with gradient estimation\n                if np.random.rand() < self.local_search_prob:\n                    # Estimate gradient using finite differences\n                    gradient = np.zeros(self.dim)\n                    delta = 1e-3  # Small perturbation\n                    for j in range(self.dim):\n                        x_plus = np.copy(trial)\n                        x_minus = np.copy(trial)\n                        x_plus[j] += delta\n                        x_minus[j] -= delta\n                        x_plus = np.clip(x_plus, self.lb, self.ub)\n                        x_minus = np.clip(x_minus, self.lb, self.ub)\n\n                        f_plus = func(x_plus)\n                        f_minus = func(x_minus)\n                        self.evals += 2\n\n                        gradient[j] = (f_plus - f_minus) / (2 * delta)\n\n                    # Move against the gradient\n                    trial_ls = trial - 0.01 * gradient\n                    trial_ls = np.clip(trial_ls, self.lb, self.ub)\n                    f_trial_ls = func(trial_ls)\n                    self.evals += 1\n                    if f_trial_ls < self.fitness[i]:\n                        self.fitness[i] = f_trial_ls\n                        self.population[i] = trial_ls\n                        if f_trial_ls < self.f_opt:\n                            self.f_opt = f_trial_ls\n                            self.x_opt = trial_ls\n\n                if self.evals >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm SelfOrganizingDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9c8c63b6-5785-4c41-9b53-3dd1b301cbe7"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "40a9f6ed-857d-4f40-9c50-b024719bc3f5", "fitness": 0.10239006224229985, "name": "SelfOrganizingDE", "description": "A self-organizing differential evolution algorithm that dynamically adjusts population diversity and mutation strategies based on performance feedback and local landscape characteristics.", "code": "import numpy as np\n\nclass SelfOrganizingDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, local_search_prob=0.1, diversity_threshold=0.1, F_adapt_rate=0.1, CR_adapt_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.local_search_prob = local_search_prob\n        self.diversity_threshold = diversity_threshold\n        self.F_adapt_rate = F_adapt_rate\n        self.CR_adapt_rate = CR_adapt_rate\n        self.population = None\n        self.fitness = None\n        self.evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.no_improvement_count = 0\n        self.success_history_F = []\n        self.success_history_CR = []\n        self.archive = []\n\n    def __call__(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n        self.no_improvement_count = 0\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                mutant = x_r1 + self.F * (x_r2 - x_r3)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        trial[j] = mutant[j]\n\n                # Repair\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Local Search (optional)\n                if np.random.rand() < self.local_search_prob:\n                    trial = self.local_search(trial, func, func.bounds.lb, func.bounds.ub)  # Apply local search\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n                    self.success_history_F.append(self.F)\n                    self.success_history_CR.append(self.CR)\n                    \n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.no_improvement_count = 0\n                    else:\n                        self.no_improvement_count += 1\n                else:\n                    self.no_improvement_count += 1\n\n                if self.evals >= self.budget:\n                    break\n\n            # Adapt F and CR\n            if self.success_history_F:\n                self.F = np.clip(np.mean(self.success_history_F) * (1 + np.random.normal(0, self.F_adapt_rate)), 0.1, 1.0)\n                self.CR = np.clip(np.mean(self.success_history_CR) * (1 + np.random.normal(0, self.CR_adapt_rate)), 0.1, 0.9)\n                self.success_history_F = []\n                self.success_history_CR = []\n            \n            # Dynamic diversity control using euclidean distance\n            distances = np.sum((self.population - np.mean(self.population, axis=0))**2, axis=1)\n            diversity = np.mean(distances)\n            \n            if diversity < self.diversity_threshold:\n                # Increase diversity by perturbing the population\n                for i in range(self.pop_size):\n                     self.population[i] = np.clip(self.population[i] + np.random.normal(0, 0.1, self.dim), func.bounds.lb, func.bounds.ub)\n                     self.fitness[i] = func(self.population[i])\n                     self.evals += 1\n                     if self.fitness[i] < self.f_opt:\n                         self.f_opt = self.fitness[i]\n                         self.x_opt = self.population[i]\n\n            # Restart if no improvement\n            if self.no_improvement_count > self.budget // 10:\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.evals += self.pop_size\n                self.best_index = np.argmin(self.fitness)\n                self.f_opt = self.fitness[self.best_index]\n                self.x_opt = self.population[self.best_index]\n                self.no_improvement_count = 0\n        return self.f_opt, self.x_opt\n\n    def local_search(self, x, func, lb, ub, radius=0.1, iterations=5):\n        \"\"\"Simple local search around x.\"\"\"\n        best_x = x\n        best_f = func(x)\n        self.evals += 1\n\n        for _ in range(iterations):\n            new_x = x + np.random.uniform(-radius, radius, size=self.dim)\n            new_x = np.clip(new_x, lb, ub)\n            new_f = func(new_x)\n            self.evals += 1\n\n            if new_f < best_f:\n                best_f = new_f\n                best_x = new_x\n\n        return best_x", "configspace": "", "generation": 3, "feedback": "The algorithm SelfOrganizingDE scored 0.102 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6adee4d4-39ae-460c-80a9-dc31a4a1423b"], "operator": null, "metadata": {"aucs": [0.2047801244845997, 0]}}
{"id": "fd40fef7-0a39-4ca2-9dfc-f6151c0249c9", "fitness": 0.409748024129008, "name": "CauchyRankDE", "description": "A Differential Evolution variant that incorporates a Cauchy mutation operator for increased exploration and a rank-based selection mechanism to improve convergence.", "code": "import numpy as np\n\nclass CauchyRankDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, cauchy_scale=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.cauchy_scale = cauchy_scale\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n\n        while self.evals < self.budget:\n            ranked_indices = np.argsort(self.fitness)\n\n            for i in range(self.pop_size):\n                # Mutation (Cauchy)\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                cauchy_values = np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n                mutant = x_r1 + self.F * (x_r2 - x_r3) + cauchy_values\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR:\n                        trial[j] = mutant[j]\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection (Rank-based)\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                if self.evals >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm CauchyRankDE scored 0.410 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3ac57c88-e09c-4b79-b0a1-8302b62442aa"], "operator": null, "metadata": {"aucs": [0.18390412519082078, 0.30772854115012493, 0.3909528880865861, 0.4960227405116481, 0.33239763098131747, 0.4435041166682857, 0.312241012054855, 0.35771386896878843, 0.32910897045645127, 0.2731317135717829, 0.4558537568225275, 0.9935981480531385, 0.2670312601121263, 0.3364365946031864, 0.7673016602681207, 0.4117723398451081, 0.327918686233184, 0.5013333123468209, 0.20589063039813704, 0.5011184862571492]}}
{"id": "936bceda-0fb6-4235-9766-bd2a7915a3c2", "fitness": 0.448920357094884, "name": "VelocityArchiveDE", "description": "Differential Evolution with a velocity-based mutation and a dynamic archive to store potentially useful solutions, enhancing exploration and exploitation.", "code": "import numpy as np\n\nclass VelocityArchiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=20, F=0.5, CR=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.CR = CR\n        self.archive = []\n        self.archive_fitness = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n        self.velocity = np.zeros_like(self.population)  # Initialize velocity\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Velocity update\n                indices = np.random.choice(self.pop_size, 2, replace=False)\n                x_r1, x_r2 = self.population[indices]\n                self.velocity[i] = self.F * (x_r1 - x_r2)\n\n                # Mutation with velocity\n                mutant = self.population[i] + self.velocity[i]\n                \n                # Add archive component\n                if len(self.archive) > 0 and np.random.rand() < 0.1: # Probability to use archive\n                    archive_index = np.random.randint(len(self.archive))\n                    mutant += self.F * (self.archive[archive_index] - self.population[i])\n                \n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR:\n                        trial[j] = mutant[j]\n\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i])\n                        self.archive_fitness.append(self.fitness[i])\n                    else:\n                        worst_archive_index = np.argmax(self.archive_fitness)\n                        if self.fitness[i] < self.archive_fitness[worst_archive_index]:\n                            self.archive[worst_archive_index] = self.population[i]\n                            self.archive_fitness[worst_archive_index] = self.fitness[i]\n\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                if self.evals >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm VelocityArchiveDE scored 0.449 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3ac57c88-e09c-4b79-b0a1-8302b62442aa"], "operator": null, "metadata": {"aucs": [0.15762400601573112, 0.2646029128839025, 0.4344636775475207, 0.6261965443350133, 0.3724016954721502, 0.5197184905133576, 0.3075943340775862, 0.410329861328509, 0.38396019713315765, 0.21481006871839659, 0.6651770924185914, 0.9968933536861021, 0.33042358326089283, 0.3563043545160346, 0.798404709696876, 0.4962652132436427, 0.3502260410555893, 0.6095862194394394, 0.19317492324468466, 0.4902498633105016]}}
{"id": "9fbd16b2-1729-4612-bcf4-f2a3dcfb4849", "fitness": 0.5759668523648813, "name": "OrthogonalCauchyDE", "description": "Differential Evolution with orthogonal learning to enhance exploration and exploitation by probabilistically incorporating information from multiple parents, and a Cauchy mutation for intensified local search.", "code": "import numpy as np\n\nclass OrthogonalCauchyDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, orthogonal_components=5, cauchy_scale=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.orthogonal_components = orthogonal_components\n        self.cauchy_scale = cauchy_scale\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Orthogonal Learning\n                indices = np.random.choice(self.pop_size, self.orthogonal_components + 1, replace=False)\n                x_target = self.population[i]\n                x_parents = self.population[indices[:-1]]\n                x_base = self.population[indices[-1]]\n\n                mutant = np.copy(x_base)\n                for j in range(self.dim):\n                    # Probabilistically choose from parents to create mutant\n                    parent_idx = np.random.randint(self.orthogonal_components)\n                    mutant[j] = x_base[j] + self.F * (x_parents[parent_idx][j] - x_target[j])\n\n                # Cauchy Mutation (intensified local search)\n                if np.random.rand() < 0.1:  # Apply Cauchy mutation with a probability\n                    cauchy_noise = self.cauchy_scale * np.random.standard_cauchy(size=self.dim)\n                    mutant += cauchy_noise\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        trial[j] = mutant[j]\n\n                # Repair\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                if self.evals >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm OrthogonalCauchyDE scored 0.576 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6adee4d4-39ae-460c-80a9-dc31a4a1423b"], "operator": null, "metadata": {"aucs": [0.20284437002534095, 0.3327809462870025, 0.5610174062055391, 0.8534847806626517, 0.6400386947569412, 0.7203470376608403, 0.4285445706793535, 0.5579048302988481, 0.6735215518134325, 0.5405827076377145, 0.7349421344855058, 0.9994261532715597, 0.27840014003846214, 0.5502169431497907, 0.8022942288718544, 0.7071987329657987, 0.43528812732516, 0.7758081037108815, 0.21233800864388208, 0.5123575788070684]}}
{"id": "fd4f8e04-869c-4ad4-aaab-2ee2a1a57e92", "fitness": 0.3963221288775657, "name": "StochasticRankingAgingDE", "description": "Differential Evolution with a stochastic ranking-based selection, aging, and periodic population replacement with a focus on exploration.", "code": "import numpy as np\n\nclass StochasticRankingAgingDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, min_pop_size=10, archive_size=10, aging_limit=500, p_archive=0.1, p_rank=0.45, restart_trigger=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = initial_pop_size\n        self.min_pop_size = min_pop_size\n        self.pop_size = initial_pop_size\n        self.archive_size = archive_size\n        self.aging_limit = aging_limit\n        self.p_archive = p_archive\n        self.p_rank = p_rank  # Probability for stochastic ranking\n        self.archive = []\n        self.F = np.full(initial_pop_size, 0.5)\n        self.CR = np.full(initial_pop_size, 0.7)\n        self.ages = np.zeros(initial_pop_size)  # Initialize ages for each individual\n        self.restart_trigger = restart_trigger # Percentage of budget before triggering restart\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n        self.no_improvement_count = 0\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Self-adapt F and CR\n                if np.random.rand() < 0.1:\n                    self.F[i] = np.random.normal(0.5, 0.3)\n                    self.F[i] = np.clip(self.F[i], 0.1, 1.0)\n                if np.random.rand() < 0.1:\n                    self.CR[i] = np.random.normal(0.7, 0.1)\n                    self.CR[i] = np.clip(self.CR[i], 0.1, 1.0)\n\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                \n                # Use archive with probability p_archive\n                if np.random.rand() < self.p_archive and len(self.archive) > 0:\n                    x_r4 = self.archive[np.random.randint(len(self.archive))]\n                    mutant = x_r1 + self.F[i] * (x_r2 - x_r3) + self.F[i] * (x_r4 - self.population[i])\n                else:\n                    mutant = x_r1 + self.F[i] * (x_r2 - x_r3)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR[i]:\n                        trial[j] = mutant[j]\n\n                # Repair\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection using Stochastic Ranking\n                f_trial = func(trial)\n                self.evals += 1\n\n                # Stochastic Ranking Selection\n                if np.random.rand() < self.p_rank or self.fitness[i] < f_trial:  # Changed comparison\n                    if f_trial < self.fitness[i]:\n                        # Update archive probabilistically\n                        if len(self.archive) < self.archive_size:\n                            self.archive.append(self.population[i])\n                        else:\n                            if np.random.rand() < 0.5:\n                                replace_index = np.random.randint(self.archive_size)\n                                self.archive[replace_index] = self.population[i]\n                                \n                        self.fitness[i] = f_trial\n                        self.population[i] = trial\n                        self.ages[i] = 0  # Reset age\n\n                        # Update best solution\n                        if f_trial < self.f_opt:\n                            self.f_opt = f_trial\n                            self.x_opt = trial\n                            self.no_improvement_count = 0\n                        else:\n                            self.no_improvement_count += 1\n                    else:\n                        self.ages[i] += 1\n                        self.no_improvement_count += 1\n\n                else:\n                    self.ages[i] += 1\n                    self.no_improvement_count += 1\n\n                if self.evals >= self.budget:\n                    break\n\n            # Aging mechanism: remove old individuals\n            to_remove = self.ages >= self.aging_limit\n            num_to_remove = np.sum(to_remove)\n\n            if num_to_remove > 0 and self.pop_size > self.min_pop_size:\n                indices_to_remove = np.where(to_remove)[0]\n                indices_to_remove = indices_to_remove[:min(num_to_remove, self.pop_size - self.min_pop_size)] #remove at most pop_size - min_pop_size individuals\n\n                self.population = np.delete(self.population, indices_to_remove, axis=0)\n                self.fitness = np.delete(self.fitness, indices_to_remove)\n                self.F = np.delete(self.F, indices_to_remove)\n                self.CR = np.delete(self.CR, indices_to_remove)\n                self.ages = np.delete(self.ages, indices_to_remove)\n                self.pop_size = len(self.population)\n                \n                # Add new random individuals to maintain population size\n                num_to_add = num_to_remove\n                new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(num_to_add, self.dim))\n                new_fitness = np.array([func(x) for x in new_individuals])\n                self.evals += num_to_add\n\n                self.population = np.concatenate((self.population, new_individuals), axis=0)\n                self.fitness = np.concatenate((self.fitness, new_fitness))\n                self.F = np.concatenate((self.F, np.full(num_to_add, 0.5)))\n                self.CR = np.concatenate((self.CR, np.full(num_to_add, 0.7)))\n                self.ages = np.concatenate((self.ages, np.zeros(num_to_add)))\n                self.pop_size = len(self.population)\n\n                self.best_index = np.argmin(self.fitness)\n                self.f_opt = self.fitness[self.best_index]\n                self.x_opt = self.population[self.best_index]\n                \n            # Periodic population replacement based on exploration\n            if self.evals > self.restart_trigger * self.budget:\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.evals += self.pop_size\n                self.best_index = np.argmin(self.fitness)\n                self.f_opt = self.fitness[self.best_index]\n                self.x_opt = self.population[self.best_index]\n                self.no_improvement_count = 0\n                self.archive = []\n                self.F = np.full(self.pop_size, 0.5)\n                self.CR = np.full(self.pop_size, 0.7)\n                self.ages = np.zeros(self.pop_size)\n                self.restart_trigger = 1.1 #Ensure it doesnt restart again\n        \n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm StochasticRankingAgingDE scored 0.396 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b99817b1-c968-49c4-ad24-c63809350a21"], "operator": null, "metadata": {"aucs": [0.16089483599178, 0.25088473470535033, 0.3718191080032832, 0.5368527089160251, 0.32012986431095414, 0.4191439533820638, 0.2860818092225703, 0.3620513791404605, 0.34031722153050825, 0.22011449954737605, 0.39050762057880994, 0.9969406419105116, 0.2962706632587586, 0.3400860721853194, 0.7324821937400814, 0.4225981452779183, 0.32749498992293224, 0.4949994298163848, 0.18028999973316417, 0.4764827063770617]}}
{"id": "5d351d23-657e-4a3b-b2a8-bcbda17cefdb", "fitness": 0.4864286376678432, "name": "CauchyADE", "description": "Differential Evolution with a Cauchy mutation operator and adaptive parameter control using a rank-based adaptation strategy.", "code": "import numpy as np\n\nclass CauchyADE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, F_adapt_rate=0.1, CR_adapt_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.F_adapt_rate = F_adapt_rate\n        self.CR_adapt_rate = CR_adapt_rate\n        self.memory_F = np.ones(5) * 0.5\n        self.memory_CR = np.ones(5) * 0.7\n        self.memory_idx = 0\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n\n        while self.evals < self.budget:\n            ranked_indices = np.argsort(self.fitness)\n            for i in range(self.pop_size):\n                # Parameter Adaptation\n                F = self.memory_F[self.memory_idx] + self.F_adapt_rate * np.random.normal(0, 1)\n                CR = self.memory_CR[self.memory_idx] + self.CR_adapt_rate * np.random.normal(0, 1)\n                F = np.clip(F, 0.1, 1.0)\n                CR = np.clip(CR, 0.1, 1.0)\n                \n                # Mutation (Cauchy)\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                mutant = x_r1 + F * np.random.standard_cauchy(size=self.dim) * (x_r2 - x_r3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < CR:\n                        trial[j] = mutant[j]\n\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        \n                    # Update Memory based on Rank\n                    rank = np.where(ranked_indices == i)[0][0]\n                    if rank < self.pop_size / 4: \n                        self.memory_F[self.memory_idx] = F\n                        self.memory_CR[self.memory_idx] = CR\n                        self.memory_idx = (self.memory_idx + 1) % 5\n\n                if self.evals >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm CauchyADE scored 0.486 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3ac57c88-e09c-4b79-b0a1-8302b62442aa"], "operator": null, "metadata": {"aucs": [0.23964400527817864, 0.5021517665223149, 0.45091160667111585, 0.6217597871627913, 0.488614570283759, 0.6027144178941926, 0.37647588803070886, 0.4449858061039468, 0.4672249846281029, 0.42652159927877953, 0.35760800986522756, 0.9986447657968675, 0.2819254365299213, 0.35612638700336374, 0.7129881863682035, 0.663177377040244, 0.36012726809220663, 0.6087588041507842, 0.25482596348916553, 0.5133861231669877]}}
{"id": "12ef64fe-589a-402e-a948-fcda4042c1a1", "fitness": 0.30573844722530313, "name": "RankAgingCauchyDE", "description": "A Differential Evolution variant that uses a combination of best-so-far and random mutation strategies, along with a Cauchy distribution for generating the scaling factor F, and an aging mechanism based on fitness rank.", "code": "import numpy as np\n\nclass RankAgingCauchyDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, CR=0.7, aging_rate=0.05, cauchy_loc=0.0, cauchy_scale=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.aging_rate = aging_rate\n        self.cauchy_loc = cauchy_loc\n        self.cauchy_scale = cauchy_scale\n        self.age = np.zeros(pop_size)\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n\n        while self.evals < self.budget:\n            # Rank the population\n            ranked_indices = np.argsort(self.fitness)\n\n            for i in range(self.pop_size):\n                # Mutation: Use a combination of best-so-far and random mutation\n                if np.random.rand() < 0.5:  # Probability to use best-so-far\n                    x_best = self.population[self.best_index]\n                    indices = np.random.choice(self.pop_size, 2, replace=False)\n                    x_r1, x_r2 = self.population[indices]\n                    # Cauchy distribution for F\n                    F = np.abs(np.random.standard_cauchy() * self.cauchy_scale + self.cauchy_loc)\n                    mutant = x_best + F * (x_r1 - x_r2)\n                else:  # Use random mutation\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    # Cauchy distribution for F\n                    F = np.abs(np.random.standard_cauchy() * self.cauchy_scale + self.cauchy_loc)\n                    mutant = x_r1 + F * (x_r2 - x_r3)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        trial[j] = mutant[j]\n\n                # Repair\n                trial = np.clip(trial, self.lb, self.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n                    self.age[i] = 0  # Reset age\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.best_index = i\n                else:\n                     # Aging: Increase age based on rank\n                    rank = np.where(ranked_indices == i)[0][0]\n                    self.age[i] += self.aging_rate * (rank / self.pop_size)\n\n                if self.evals >= self.budget:\n                    break\n            # Replace old individuals\n            for i in range(self.pop_size):\n                if self.age[i] > 1.0:\n                    self.population[i] = np.random.uniform(self.lb, self.ub, size=self.dim)\n                    self.fitness[i] = func(self.population[i])\n                    self.evals += 1\n                    self.age[i] = 0  # Reset age\n                    if self.fitness[i] < self.f_opt:\n                        self.f_opt = self.fitness[i]\n                        self.x_opt = self.population[i]\n                        self.best_index = i\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm RankAgingCauchyDE scored 0.306 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6adee4d4-39ae-460c-80a9-dc31a4a1423b"], "operator": null, "metadata": {"aucs": [0.20467929404520513, 0.2222801782266952, 0.7959943166293122, 0]}}
{"id": "ae29f0c6-62e4-4159-9bc6-ba29a0f6bf14", "fitness": 0.30559269216937857, "name": "AdaptiveDE", "description": "Differential Evolution with a modified mutation strategy using a combination of best-guided and random components, along with a dynamically adjusted crossover rate based on fitness improvement.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation: Combining best-guided and random components\n                indices = np.random.choice(self.pop_size, 2, replace=False)\n                x_r1, x_r2 = self.population[indices]\n                x_best = self.population[self.best_index]\n                mutant = x_best + self.F * (x_r1 - x_r2) \n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n\n                # Adaptive Crossover\n                delta_f = np.abs(self.fitness[i] - self.f_opt)\n                cr = self.CR * np.exp(-delta_f / (self.f_opt + 1e-8)) # Dynamic CR\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < cr:\n                        trial[j] = mutant[j]\n\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.best_index = i\n\n                if self.evals >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDE scored 0.306 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3ac57c88-e09c-4b79-b0a1-8302b62442aa"], "operator": null, "metadata": {"aucs": [0.13379630072920767, 0.19627691190317487, 0.2959856833266856, 0.26640788852088015, 0.22192385591015784, 0.2604050129322716, 0.24968931509143477, 0.2414278962694787, 0.23483756986226223, 0.19615443055122495, 0.2209562447205946, 0.9982561959965256, 0.24461775451338574, 0.25018377970216377, 0.5808996960900277, 0.30622432428606117, 0.2549961512884432, 0.30923592590628546, 0.18704495878660576, 0.46253394700070005]}}
{"id": "53f3d838-faef-408b-896a-52bb2ea9b9b2", "fitness": -Infinity, "name": "AdaptiveCauchyGaussianDE", "description": "Adaptive Differential Evolution with a combination of Cauchy and Gaussian mutations, dynamically adjusting mutation parameters based on population diversity.", "code": "import numpy as np\n\nclass AdaptiveCauchyGaussianDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, \n                 cauchy_scale_init=0.1, gaussian_scale_init=0.05, diversity_threshold=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.cauchy_scale = cauchy_scale_init\n        self.gaussian_scale = gaussian_scale_init\n        self.diversity_threshold = diversity_threshold\n        self.population = None\n        self.fitness = None\n        self.evals = 0\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n\n        while self.evals < self.budget:\n            # Calculate population diversity\n            diversity = np.std(self.fitness)\n\n            for i in range(self.pop_size):\n                # Mutation: adaptive Cauchy/Gaussian mixture\n                if diversity > self.diversity_threshold:\n                    # High diversity: use Cauchy for exploration\n                    cauchy_values = np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n                    mutant = self.population[i] + self.F * cauchy_values\n                else:\n                    # Low diversity: use Gaussian for exploitation\n                    gaussian_values = np.random.normal(0, self.gaussian_scale, size=self.dim)\n                    mutant = self.population[i] + self.F * gaussian_values\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR:\n                        trial[j] = mutant[j]\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                if self.evals >= self.budget:\n                    break\n            \n            # Adaptive parameter control (example: adjust Cauchy scale)\n            self.cauchy_scale = self.cauchy_scale_init * np.exp(-diversity)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "An exception occurred: 'AdaptiveCauchyGaussianDE' object has no attribute 'cauchy_scale_init'.", "error": "", "parent_ids": ["fd40fef7-0a39-4ca2-9dfc-f6151c0249c9"], "operator": null, "metadata": {}}
{"id": "3d08551c-60b1-48c2-9a38-fef0c78674d7", "fitness": 0.12391906558230126, "name": "AdaptiveLvyDE", "description": "Adaptive Differential Evolution with Lvy flight mutation and a self-regulating population size.", "code": "import numpy as np\n\nclass AdaptiveLvyDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, pop_size_reduction_factor=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.pop_size_reduction_factor = pop_size_reduction_factor\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation (Lvy Flight)\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                \n                levy_step = self.levy_flight(beta=1.5, size=self.dim)\n                mutant = x_r1 + self.F * levy_step * (x_r2 - x_r3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR:\n                        trial[j] = mutant[j]\n\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n            \n            # Reduce population size if stagnant\n            if self.evals % (self.pop_size * 5) == 0:\n                if np.std(self.fitness) < 1e-6:\n                    self.pop_size = int(self.pop_size * self.pop_size_reduction_factor)\n                    self.pop_size = max(10, self.pop_size)  # Ensure minimum population size\n                    \n                    # Repopulate with new random solutions\n                    new_population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                    new_fitness = np.array([func(x) for x in new_population])\n                    self.evals += self.pop_size\n                    \n                    self.population = new_population\n                    self.fitness = new_fitness\n                    \n                    self.best_index = np.argmin(self.fitness)\n                    self.f_opt = self.fitness[self.best_index]\n                    self.x_opt = self.population[self.best_index]\n            \n            if self.evals >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt\n\n    def levy_flight(self, beta=1.5, size=None):\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) / (np.math.gamma((1 + beta) / 2) * beta * (2 ** ((beta - 1) / 2)))) ** (1 / beta)\n        u = np.random.normal(0, sigma, size=size)\n        v = np.random.normal(0, 1, size=size)\n        step = u / (abs(v) ** (1 / beta))\n        return step", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveLvyDE scored 0.124 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5d351d23-657e-4a3b-b2a8-bcbda17cefdb"], "operator": null, "metadata": {"aucs": [0.24783813116460252, 0]}}
{"id": "3dea26bf-0202-4ffa-8892-2448f7c1b825", "fitness": 0.18271025523462162, "name": "DynamicGaussianDE", "description": "Differential Evolution with a dynamically adjusted population size based on the function evaluation landscape and a self-adaptive mutation strategy using a Gaussian distribution.", "code": "import numpy as np\n\nclass DynamicGaussianDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, F=0.5, CR=0.7, pop_size_reduction_factor=0.9, gaussian_scale_init=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.initial_pop_size = initial_pop_size\n        self.F = F\n        self.CR = CR\n        self.pop_size_reduction_factor = pop_size_reduction_factor\n        self.gaussian_scale_init = gaussian_scale_init\n        self.gaussian_scale = gaussian_scale_init\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n\n        generation = 0\n        while self.evals < self.budget:\n            generation += 1\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                mutant = x_r1 + self.F * (x_r2 - x_r3)\n                \n                # Gaussian Mutation (Self-adaptive)\n                gaussian_noise = self.gaussian_scale * np.random.normal(size=self.dim)\n                mutant += gaussian_noise\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        trial[j] = mutant[j]\n\n                # Repair\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Dynamic Population Size Adjustment\n            if generation % 10 == 0:\n                std_fitness = np.std(self.fitness)\n                if std_fitness < 1e-3:  # If fitness diversity is low, reduce population size\n                    self.pop_size = int(self.pop_size * self.pop_size_reduction_factor)\n                    self.pop_size = max(10, self.pop_size)  # Ensure minimum population size\n                    self.population = self.population[:self.pop_size]\n                    self.fitness = self.fitness[:self.pop_size]\n                elif std_fitness > 0.1:\n                    self.gaussian_scale = min(1.0, self.gaussian_scale * 1.1)\n                else:\n                    self.gaussian_scale = max(0.001, self.gaussian_scale * 0.9)\n\n                if self.evals + self.pop_size > self.budget:\n                    self.pop_size = max(10, self.budget - self.evals)  # Adjust pop size if close to budget\n                \n                if self.pop_size != len(self.fitness):\n                    self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                    self.fitness = np.array([func(x) for x in self.population])\n                    self.evals += self.pop_size\n                    \n                self.best_index = np.argmin(self.fitness)\n                self.f_opt = self.fitness[self.best_index]\n                self.x_opt = self.population[self.best_index]\n\n                if self.evals >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm DynamicGaussianDE scored 0.183 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9fbd16b2-1729-4612-bcf4-f2a3dcfb4849"], "operator": null, "metadata": {"aucs": [0.1834183230942975, 0.36471244260956737, 0]}}
{"id": "77e38f3f-6f64-4080-a64f-c93285c2e309", "fitness": 0.43710843095758134, "name": "AdaptiveLocalSearchDE", "description": "Differential Evolution with a self-adaptive mutation strategy based on fitness improvement and a local search operator triggered with a probability.", "code": "import numpy as np\n\nclass AdaptiveLocalSearchDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, local_search_prob=0.05, local_search_radius=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.local_search_prob = local_search_prob\n        self.local_search_radius = local_search_radius\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n        self.mutation_factors = np.full(self.pop_size, self.F) # Initialize mutation factors\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                mutant = self.population[i] + self.mutation_factors[i] * (x_r1 - x_r2) + self.mutation_factors[i] * (x_r3 - self.population[i])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR:\n                        trial[j] = mutant[j]\n\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Local Search\n                if np.random.rand() < self.local_search_prob:\n                    trial = self.local_search(trial, func)\n                    trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    # Adapt mutation factor\n                    if f_trial < self.fitness[i]:\n                         self.mutation_factors[i] = np.clip(self.mutation_factors[i] * 1.1, 0.1, 1.0) # Increase if improvement\n                    else:\n                         self.mutation_factors[i] = np.clip(self.mutation_factors[i] * 0.9, 0.1, 1.0) # Decrease if no improvement\n\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                if self.evals >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt\n\n    def local_search(self, x, func):\n        # Simple local search around x\n        x_new = x + np.random.uniform(-self.local_search_radius, self.local_search_radius, size=self.dim)\n        return x_new", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveLocalSearchDE scored 0.437 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["936bceda-0fb6-4235-9766-bd2a7915a3c2"], "operator": null, "metadata": {"aucs": [0.16676933748399492, 0.29408252240861676, 0.4259758909995638, 0.6198064704183629, 0.3580707892647014, 0.5047451628166744, 0.3138365939007014, 0.3760963228452189, 0.40575454612649486, 0.25174644728589, 0.5270506428440741, 0.9992472643473677, 0.3137489840181126, 0.3320292684294478, 0.7275970910861598, 0.5017637351077624, 0.35443756058654186, 0.5598900834316017, 0.2171049293178059, 0.4924149764325353]}}
{"id": "4ab3022d-bebe-4bc9-8e59-5e6f2615aa27", "fitness": 0.6107016427525671, "name": "LevySelfAdaptiveDE", "description": "Differential Evolution with a Lvy flight mutation operator and a self-adaptive strategy to control the mutation strength and crossover rate based on the success rate of previous generations.", "code": "import numpy as np\n\nclass LevySelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_init=0.5, CR_init=0.7, alpha=1.5, beta=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.CR = CR_init\n        self.alpha = alpha  # Levy flight parameter\n        self.beta = beta # self-adaptive learning rate\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n\n    def levy_flight(self, size, alpha=1.5):\n        \"\"\"\n        Generate Levy distribution samples.\n        \"\"\"\n        u = np.random.randn(size)\n        v = np.random.randn(size)\n        sigma = (np.math.gamma(1 + alpha) * np.sin(np.pi * alpha / 2) / (np.math.gamma((1 + alpha) / 2) * alpha * 2 ** ((alpha - 1) / 2))) ** (1 / alpha)\n        step = sigma * u / abs(v) ** (1 / alpha)\n        return step\n\n    def __call__(self, func):\n        # Initialization\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n\n        successful_F = []\n        successful_CR = []\n\n        # Optimization loop\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation (Levy flight)\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                levy_steps = self.levy_flight(self.dim, self.alpha)\n                mutant = x_r1 + self.F * levy_steps * (x_r2 - x_r3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR:\n                        trial[j] = mutant[j]\n\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    successful_F.append(self.F)\n                    successful_CR.append(self.CR)\n\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            # Self-adaptive parameter control\n            if successful_F:\n                mean_F = np.mean(successful_F)\n                self.F = self.F + self.beta * (mean_F - self.F)\n                self.F = np.clip(self.F, 0.1, 1.0)\n\n                mean_CR = np.mean(successful_CR)\n                self.CR = self.CR + self.beta * (mean_CR - self.CR)\n                self.CR = np.clip(self.CR, 0.1, 1.0)\n                \n            successful_F = []\n            successful_CR = []\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm LevySelfAdaptiveDE scored 0.611 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5d351d23-657e-4a3b-b2a8-bcbda17cefdb"], "operator": null, "metadata": {"aucs": [0.21544072953969962, 0.4184965183120152, 0.5204274392295714, 0.826355006553145, 0.6803490380533623, 0.7424371548430585, 0.6149453775759863, 0.531660957813006, 0.663369527766078, 0.5713529151171899, 0.7783264598265579, 0.9977523229120383, 0.25738884557814834, 0.6178390047233682, 0.8432412832162505, 0.7398138474014317, 0.48856093689098357, 0.7889401411113219, 0.3973994935514613, 0.5199358550366668]}}
{"id": "9b35df8a-475d-40f2-a6fa-4e0f47d67be5", "fitness": 0.5155452022721873, "name": "RepulsiveLevyDE", "description": "Differential Evolution with a repulsive force from the worst solution and a mutation based on Lvy flights for enhanced exploration.", "code": "import numpy as np\n\nclass RepulsiveLevyDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, levy_exponent=1.5, repulsion_strength=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.levy_exponent = levy_exponent\n        self.repulsion_strength = repulsion_strength\n\n    def levy_flight(self, size):\n        \"\"\"\n        Generate Levy distributed steps.\n        \"\"\"\n        u = np.random.randn(size)\n        v = np.random.randn(size)\n        step = u / np.power(np.abs(v), (1.0 / self.levy_exponent))\n        sigma = np.power(np.math.gamma(1 + self.levy_exponent) * np.sin(np.pi * self.levy_exponent / 2) / (np.math.gamma((1 + self.levy_exponent) / 2) * self.levy_exponent * np.power(2, (self.levy_exponent - 1) / 2)), (1 / self.levy_exponent))\n        return sigma * step\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n\n        while self.evals < self.budget:\n            worst_index = np.argmax(self.fitness)\n            x_worst = self.population[worst_index]\n\n            for i in range(self.pop_size):\n                # Differential Evolution Mutation with Levy Flight and Repulsion\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n\n                levy_steps = self.levy_flight(self.dim)\n                mutant = x_r1 + self.F * (x_r2 - x_r3) + self.repulsion_strength * (x_r1 - x_worst) + 0.01 * levy_steps # Add levy flight and repulsion\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        trial[j] = mutant[j]\n\n                # Repair\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                if self.evals >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm RepulsiveLevyDE scored 0.516 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9fbd16b2-1729-4612-bcf4-f2a3dcfb4849"], "operator": null, "metadata": {"aucs": [0.19655946842231087, 0.3887382229059412, 0.46705264613558495, 0.8245285376255356, 0.4898494251782567, 0.5535054827109477, 0.3595549809724674, 0.44905079020817995, 0.4637932898543763, 0.3700553260852537, 0.7573333810459223, 0.9926106350375226, 0.31217360313557496, 0.4547622479088612, 0.8780384162628395, 0.5672156606234602, 0.40669399370747283, 0.6580329620406857, 0.20493883232332877, 0.516416143259222]}}
{"id": "d86972af-89c0-4419-95af-a4c53d4c702f", "fitness": 0.35655827842000426, "name": "NeighborhoodAdaptiveDE", "description": "Self-Adaptive Differential Evolution with Neighborhood-based Mutation, where mutation parameters adapt based on the success within a local neighborhood of solutions.", "code": "import numpy as np\n\nclass NeighborhoodAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, neighborhood_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.neighborhood_size = neighborhood_size\n        self.F = np.full(pop_size, 0.5)  # Initialize F for each individual\n        self.CR = np.full(pop_size, 0.7)  # Initialize CR for each individual\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Neighborhood Selection\n                indices = np.random.choice(self.pop_size, self.neighborhood_size, replace=False)\n                neighborhood = self.population[indices]\n                neighborhood_fitness = self.fitness[indices]\n\n                # Mutation\n                donor_vector = self.mutate(i, neighborhood)\n                donor_vector = np.clip(donor_vector, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial_vector = self.crossover(i, donor_vector)\n                trial_vector = np.clip(trial_vector, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial_vector)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    # Update individual\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial_vector\n\n                    # Adapt parameters based on success\n                    self.adapt_parameters(i, f_trial, neighborhood_fitness)\n\n                    # Update best solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial_vector\n\n                if self.evals >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt\n\n    def mutate(self, i, neighborhood):\n        # Select three distinct individuals from the neighborhood\n        indices = np.random.choice(self.neighborhood_size, 3, replace=False)\n        x_r1, x_r2, x_r3 = neighborhood[indices]\n        return self.population[i] + self.F[i] * (x_r1 - x_r2) + self.F[i] * (self.x_opt - x_r3)\n\n    def crossover(self, i, donor_vector):\n        trial_vector = np.copy(self.population[i])\n        for j in range(self.dim):\n            if np.random.rand() < self.CR[i]:\n                trial_vector[j] = donor_vector[j]\n        return trial_vector\n\n    def adapt_parameters(self, i, f_trial, neighborhood_fitness):\n         # Adjust F and CR based on the improvement in the neighborhood\n        improvement = np.mean(neighborhood_fitness) - f_trial\n        if improvement > 0:\n            self.F[i] = np.clip(self.F[i] * (1 + 0.1 * improvement), 0.1, 0.9)\n            self.CR[i] = np.clip(self.CR[i] * (1 + 0.1 * improvement), 0.1, 0.9)\n        else:\n            self.F[i] = np.clip(self.F[i] * 0.9, 0.1, 0.9)\n            self.CR[i] = np.clip(self.CR[i] * 0.9, 0.1, 0.9)", "configspace": "", "generation": 4, "feedback": "The algorithm NeighborhoodAdaptiveDE scored 0.357 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["fd40fef7-0a39-4ca2-9dfc-f6151c0249c9"], "operator": null, "metadata": {"aucs": [0.1362776186509218, 0.19896034709974153, 0.358350651797976, 0.3638160642584459, 0.2806676260621336, 0.365852794228001, 0.28332114120370133, 0.28156356666050253, 0.27818340981041856, 0.19701625506051446, 0.4064706017405729, 0.9996762679662405, 0.28221294021603693, 0.28725546847196226, 0.6763931293524142, 0.3864438790660326, 0.2768838590770287, 0.42283102022145314, 0.16971975622812874, 0.4792691712278584]}}
{"id": "bc7c4e5e-5e80-4c6a-8400-d89554138178", "fitness": 0.474653804519712, "name": "SelfAdaptiveLaplacianDE", "description": "Differential Evolution with a self-adaptive strategy for both F and CR, combined with a Laplacian mutation for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass SelfAdaptiveLaplacianDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_mean=0.5, CR_mean=0.7, laplacian_scale=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_mean = F_mean\n        self.CR_mean = CR_mean\n        self.laplacian_scale = laplacian_scale\n\n        self.F_memory = np.ones(self.pop_size) * self.F_mean\n        self.CR_memory = np.ones(self.pop_size) * self.CR_mean\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Generate F and CR values\n                F = np.random.normal(self.F_mean, 0.1)\n                F = np.clip(F, 0.1, 1.0)\n                CR = np.random.normal(self.CR_mean, 0.1)\n                CR = np.clip(CR, 0.1, 1.0)\n\n                # Mutation (Laplacian)\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                laplacian_values = np.random.laplace(0, self.laplacian_scale, size=self.dim)\n                mutant = x_r1 + F * (x_r2 - x_r3) + laplacian_values\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < CR:\n                        trial[j] = mutant[j]\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n                    self.F_memory[i] = F\n                    self.CR_memory[i] = CR\n\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n\n                    self.F_mean = np.mean(self.F_memory)\n                    self.CR_mean = np.mean(self.CR_memory)\n\n                if self.evals >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm SelfAdaptiveLaplacianDE scored 0.475 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["fd40fef7-0a39-4ca2-9dfc-f6151c0249c9"], "operator": null, "metadata": {"aucs": [0.20243345673653912, 0.3887302458421993, 0.44010979586053467, 0.8477719055856704, 0.39000580184947053, 0.4619376848351441, 0.327745011406292, 0.37872550122754767, 0.3771778924133423, 0.332935866730387, 0.607860228915315, 0.9960065402090632, 0.30150464680909317, 0.38857974148183616, 0.800399354861375, 0.5370346578126501, 0.37189666185174153, 0.5741545461366937, 0.2580867369363198, 0.509979812893025]}}
{"id": "743fa5a7-cb9b-444f-8fa4-0f97ac504793", "fitness": 0.4024551898641821, "name": "SelfAdaptiveDE", "description": "Differential Evolution with a self-adaptive strategy that adjusts mutation strength and crossover rate based on the success rate of previous generations, promoting a balance between exploration and exploitation.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_init=0.5, CR_init=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = np.full(pop_size, F_init)\n        self.CR = np.full(pop_size, CR_init)\n        self.F_init = F_init\n        self.CR_init = CR_init\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n        self.success_F = []\n        self.success_CR = []\n        self.success_count = 0\n\n    def __call__(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n        \n        while self.evals < self.budget:\n            new_population = np.copy(self.population)\n            new_fitness = np.copy(self.fitness)\n            \n            successful_mutations_F = []\n            successful_mutations_CR = []\n            successful_mutations = 0\n\n            for i in range(self.pop_size):\n                # Adaptation of F and CR\n                if np.random.rand() < 0.1: # occasional random reset\n                    self.F[i] = self.F_init\n                    self.CR[i] = self.CR_init\n                else:\n                    self.F[i] = np.clip(np.random.normal(self.F[i], 0.1), 0.1, 1.0)\n                    self.CR[i] = np.clip(np.random.normal(self.CR[i], 0.1), 0.1, 1.0)\n\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                mutant = self.population[i] + self.F[i] * (x_r1 - x_r2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR[i] or j == j_rand:\n                        trial[j] = mutant[j]\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n                \n                if f_trial < self.fitness[i]:\n                    new_fitness[i] = f_trial\n                    new_population[i] = trial\n                    \n                    successful_mutations_F.append(self.F[i])\n                    successful_mutations_CR.append(self.CR[i])\n                    successful_mutations += 1\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                \n                if self.evals >= self.budget:\n                    break\n            \n            self.population = new_population\n            self.fitness = new_fitness\n            \n            if successful_mutations > 0:\n              self.success_F.extend(successful_mutations_F)\n              self.success_CR.extend(successful_mutations_CR)\n              self.success_count += successful_mutations\n              \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm SelfAdaptiveDE scored 0.402 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["936bceda-0fb6-4235-9766-bd2a7915a3c2"], "operator": null, "metadata": {"aucs": [0.16218572359476124, 0.2748054473667243, 0.3885824588898522, 0.515621065619318, 0.29815965255685695, 0.42859020789227786, 0.29850520895195043, 0.3407792577986445, 0.33897571701194473, 0.20775133377842847, 0.4832839567255445, 0.9990732511114145, 0.3387455110569003, 0.30756905646561006, 0.7507636022932416, 0.4171249399090109, 0.324489338021013, 0.49522194801715846, 0.18827931250494456, 0.4905968077180478]}}
{"id": "14602a10-7381-40db-b49f-e7d61e538805", "fitness": 0.5466171300437682, "name": "SHADE", "description": "Differential Evolution with a self-adaptive strategy for both mutation factor and crossover rate, guided by the fitness landscape using a success-history based adaptation.", "code": "import numpy as np\n\nclass SHADE:\n    def __init__(self, budget=10000, dim=10, pop_size=100, memory_size=10, archive_size=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.memory_size = memory_size\n        self.archive_size = archive_size\n        self.memory_CR = np.full(self.memory_size, 0.5)\n        self.memory_F = np.full(self.memory_size, 0.5)\n        self.archive = None\n        self.p_best_rate = 0.11\n        self.mu_CR = 0.5\n        self.mu_F = 0.5\n        self.sf_CR = np.full(self.memory_size, 0.5)\n        self.sf_F = np.full(self.memory_size, 0.5)\n        self.archive = []\n        self.H = self.memory_size\n        self.p = self.p_best_rate\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n        self.archive = []\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Memory selection\n                memory_index = np.random.randint(self.memory_size)\n                current_CR = self.memory_CR[memory_index]\n                current_F = self.memory_F[memory_index]\n\n                # Mutation\n                p_best_index = np.random.choice(np.argsort(self.fitness)[:max(1, int(self.p * self.pop_size))])\n                x_pbest = self.population[p_best_index]\n\n                indices = np.random.choice(self.pop_size, 2, replace=False)\n                x_r1, x_r2 = self.population[indices]\n\n                mutant = self.population[i] + current_F * (x_pbest - self.population[i]) + current_F * (x_r1 - x_r2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < current_CR or j == j_rand:\n                        trial[j] = mutant[j]\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.archive.append(self.population[i].copy())\n\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                \n                if self.evals >= self.budget:\n                    break\n\n            # Memory update\n            if len(self.archive) > 0:\n                self.archive = self.archive[-self.archive_size:]\n                \n                successful_CRs = []\n                successful_Fs = []\n                \n                for i in range(self.pop_size):\n                  if self.fitness[i] < self.fitness[i]:\n                    delta_f = np.abs(self.fitness[i] - self.fitness[i])\n\n                for memory_index in range(self.memory_size):\n                  self.memory_CR[memory_index] = np.mean(self.sf_CR) if len(self.sf_CR) > 0 else 0.5\n                  self.memory_F[memory_index] = np.mean(self.sf_F) if len(self.sf_F) > 0 else 0.5\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm SHADE scored 0.547 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["fd40fef7-0a39-4ca2-9dfc-f6151c0249c9"], "operator": null, "metadata": {"aucs": [0.18729053794524686, 0.3146860536703384, 0.5474841811487352, 0.7940920522051397, 0.5642864894571678, 0.7015336039487712, 0.3736360245846315, 0.4987943155918879, 0.5978626307958481, 0.30494412299229023, 0.7716808105526165, 0.9984062709524354, 0.3175765196400525, 0.5244764397797325, 0.8692094100913357, 0.6626570098212491, 0.44699506561281954, 0.7465153258751078, 0.20615841373247845, 0.5040573224774811]}}
{"id": "ea8b1456-54ce-4d93-b189-484f9c17046c", "fitness": 0.5784903444432408, "name": "LaplacianArchiveDE", "description": "Self-adaptive Differential Evolution with a Laplacian mutation operator and a dynamic archive of successful solutions for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass LaplacianArchiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, F=0.5, CR=0.7, F_adapt_rate=0.1, CR_adapt_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.CR = CR\n        self.F_adapt_rate = F_adapt_rate\n        self.CR_adapt_rate = CR_adapt_rate\n        self.archive = []\n        self.memory_F = np.ones(5) * 0.5\n        self.memory_CR = np.ones(5) * 0.7\n        self.memory_idx = 0\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n\n        while self.evals < self.budget:\n            ranked_indices = np.argsort(self.fitness)\n            for i in range(self.pop_size):\n                # Parameter Adaptation\n                F = self.memory_F[self.memory_idx] + self.F_adapt_rate * np.random.normal(0, 1)\n                CR = self.memory_CR[self.memory_idx] + self.CR_adapt_rate * np.random.normal(0, 1)\n                F = np.clip(F, 0.1, 1.0)\n                CR = np.clip(CR, 0.1, 1.0)\n                \n                # Mutation (Laplacian)\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                \n                # Sample from Laplacian distribution\n                scale = F * np.sum(np.abs(x_r2 - x_r3)) / self.dim # Heuristic for Laplacian scale\n                laplacian_noise = np.random.laplace(0, scale, size=self.dim)\n                mutant = x_r1 + laplacian_noise\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < CR:\n                        trial[j] = mutant[j]\n\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        \n                    # Update Memory based on Rank\n                    rank = np.where(ranked_indices == i)[0][0]\n                    if rank < self.pop_size / 4: \n                        self.memory_F[self.memory_idx] = F\n                        self.memory_CR[self.memory_idx] = CR\n                        self.memory_idx = (self.memory_idx + 1) % 5\n\n                    # Archive Update\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                    else:\n                        # Replace a random archive member\n                        replace_index = np.random.randint(0, self.archive_size)\n                        self.archive[replace_index] = trial\n\n                if self.evals >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm LaplacianArchiveDE scored 0.578 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5d351d23-657e-4a3b-b2a8-bcbda17cefdb"], "operator": null, "metadata": {"aucs": [0.21485841564476638, 0.5026157815483696, 0.505042203154827, 0.7784202296321443, 0.6741824632788418, 0.6464176904693826, 0.3190890796526291, 0.636546238655189, 0.7094319563815084, 0.5947213594234828, 0.7062205921902022, 0.9924296943633464, 0.27073168960211513, 0.5927691346807691, 0.8095652794103877, 0.6465114293792669, 0.49595773202159976, 0.746498530267848, 0.20243764460806812, 0.5253597445000744]}}
{"id": "53d4e789-f8dd-4fd7-b5b8-71db4a503009", "fitness": 0.3798951856351975, "name": "LevySelfAdaptiveDE", "description": "Differential Evolution with a Lvy flight mutation and a self-adaptive strategy for dynamically adjusting parameters based on population diversity.", "code": "import numpy as np\n\nclass LevySelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_init=0.5, CR_init=0.7, levy_exponent=1.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_init = F_init\n        self.CR_init = CR_init\n        self.levy_exponent = levy_exponent\n        self.F = np.full(pop_size, F_init)\n        self.CR = np.full(pop_size, CR_init)\n\n\n    def levy_flight(self, size, exponent):\n        sigma = (np.math.gamma(1 + exponent) * np.sin(np.pi * exponent / 2) / (np.math.gamma((1 + exponent) / 2) * exponent * 2**((exponent - 1) / 2)))**(1 / exponent)\n        u = np.random.normal(0, sigma, size=size)\n        v = np.random.normal(0, 1, size=size)\n        step = u / abs(v)**(1 / exponent)\n        return step\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n\n        while self.evals < self.budget:\n            # Calculate population diversity\n            diversity = np.std(self.population)\n\n            for i in range(self.pop_size):\n                # Self-adaptive F and CR\n                if np.random.rand() < 0.1:\n                    self.F[i] = np.clip(np.random.normal(self.F_init, 0.1), 0.1, 0.9)\n                if np.random.rand() < 0.1:\n                    self.CR[i] = np.clip(np.random.normal(self.CR_init, 0.1), 0.1, 0.9)\n\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                levy_steps = self.levy_flight(self.dim, self.levy_exponent)\n                mutant = self.population[i] + self.F[i] * (x_r1 - x_r2) + levy_steps * (self.x_opt - self.population[i])\n\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR[i] or j == j_rand:\n                        trial[j] = mutant[j]\n\n                # Repair\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                if self.evals >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm LevySelfAdaptiveDE scored 0.380 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9fbd16b2-1729-4612-bcf4-f2a3dcfb4849"], "operator": null, "metadata": {"aucs": [0.157306070106777, 0.21602541454510438, 0.3564159947258959, 0.3564030155237835, 0.3465089912023913, 0.44083199057123434, 0.2987389155901349, 0.3312836537821201, 0.30809168042630686, 0.21705947859175456, 0.3575849614343174, 0.9946513673621613, 0.25735979938576825, 0.3147556660845461, 0.6936357782029952, 0.4495295174649231, 0.288271849134753, 0.5330865159918388, 0.18891293541359733, 0.49145011716354725]}}
{"id": "f9270467-7cf3-46fb-9d0c-6c1bca0b3920", "fitness": 0.0, "name": "AgingCauchyDE", "description": "Differential Evolution with a Cauchy mutation operator and an aging mechanism to promote diversity by replacing individuals that have stagnated for a certain number of generations.", "code": "import numpy as np\n\nclass AgingCauchyDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, aging_limit=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.aging_limit = aging_limit\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n        self.age = None  # Track age of each individual\n\n    def cauchy_mutation(self, x_r1, x_r2, x_r3, F):\n        \"\"\"\n        Cauchy mutation operator.\n        \"\"\"\n        return x_r1 + F * np.random.standard_cauchy(size=self.dim) * (x_r2 - x_r3)\n\n\n    def __call__(self, func):\n        # Initialization\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n        self.age = np.zeros(self.pop_size)\n\n        # Optimization loop\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation (Cauchy)\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                mutant = self.cauchy_mutation(x_r1, x_r2, x_r3, self.F)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR:\n                        trial[j] = mutant[j]\n\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n                    self.age[i] = 0  # Reset age\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                else:\n                    self.age[i] += 1 # Increment age if not improved\n\n                if self.evals >= self.budget:\n                    break\n            \n            # Aging mechanism: replace old individuals\n            for i in range(self.pop_size):\n                if self.age[i] > self.aging_limit:\n                    self.population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                    self.fitness[i] = func(self.population[i])\n                    self.evals += 1\n                    self.age[i] = 0\n                    if self.fitness[i] < self.f_opt:\n                        self.f_opt = self.fitness[i]\n                        self.x_opt = self.population[i]\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AgingCauchyDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["4ab3022d-bebe-4bc9-8e59-5e6f2615aa27"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "126a1dc0-68df-4e14-ac76-322fa5262693", "fitness": 0.07841332192282163, "name": "CauchyVelocityDE", "description": "Self-adaptive Differential Evolution with a Cauchy mutation operator, velocity-based exploration, and a restart mechanism for escaping local optima.", "code": "import numpy as np\n\nclass CauchyVelocityDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, restart_prob=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.restart_prob = restart_prob\n        self.population = None\n        self.fitness = None\n        self.velocities = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.velocities = np.zeros((self.pop_size, self.dim))\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation (Cauchy)\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                \n                # Cauchy noise\n                cauchy_noise = self.F * np.random.standard_cauchy(size=self.dim)\n                mutant = x_r1 + cauchy_noise * (x_r2 - x_r3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                \n                # Velocity update\n                self.velocities[i] = 0.5 * self.velocities[i] + 0.5 * (mutant - self.population[i])\n                \n                # Crossover\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR:\n                        trial[j] = mutant[j] #+ self.velocities[i][j]\n                    #else:\n                    #    trial[j] = self.population[i][j] + self.velocities[i][j]\n                    \n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                # Restart mechanism\n                if np.random.rand() < self.restart_prob:\n                    self.population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                    self.fitness[i] = func(self.population[i])\n                    self.evals += 1\n                    if self.fitness[i] < self.f_opt:\n                        self.f_opt = self.fitness[i]\n                        self.x_opt = self.population[i]\n\n                if self.evals >= self.budget:\n                    break\n                    \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm CauchyVelocityDE scored 0.078 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ea8b1456-54ce-4d93-b189-484f9c17046c"], "operator": null, "metadata": {"aucs": [0.15682664384564327, 0]}}
{"id": "35f722b8-a3be-4289-aa8d-0bf6ede1dde5", "fitness": 0.0, "name": "CauchyLocalSearchDE", "description": "A Differential Evolution strategy that uses a Cauchy mutation operator and dynamically adjusts its parameters based on the success rate of individuals, while also incorporating a local search around the best solution to refine convergence.", "code": "import numpy as np\n\nclass CauchyLocalSearchDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, local_search_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.local_search_prob = local_search_prob\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n\n        success_history_F = []\n        success_history_CR = []\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Parameter Adaptation (based on success history)\n                if success_history_F:\n                    self.F = np.mean(success_history_F)\n                if success_history_CR:\n                    self.CR = np.mean(success_history_CR)\n                self.F = np.clip(self.F, 0.1, 1.0)\n                self.CR = np.clip(self.CR, 0.1, 1.0)\n                \n                # Mutation (Cauchy)\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                \n                # Sample from Cauchy distribution\n                cauchy_noise = self.F * np.random.standard_cauchy(size=self.dim)\n                mutant = x_r1 + cauchy_noise * (x_r2 - x_r3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR:\n                        trial[j] = mutant[j]\n\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        \n                    # Update success history\n                    success_history_F.append(self.F)\n                    success_history_CR.append(self.CR)\n                    if len(success_history_F) > 10:\n                        success_history_F.pop(0)\n                        success_history_CR.pop(0)\n\n            # Local Search around best solution\n            if np.random.rand() < self.local_search_prob:\n                x_local = np.random.normal(self.x_opt, scale=0.05, size=self.dim)\n                x_local = np.clip(x_local, func.bounds.lb, func.bounds.ub)\n                f_local = func(x_local)\n                self.evals += 1\n                if f_local < self.f_opt:\n                    self.f_opt = f_local\n                    self.x_opt = x_local\n                    self.fitness[self.best_index] = f_local # Update fitness of best index if better solution is found.\n                    self.population[self.best_index] = x_local\n\n\n                # Update the best_index in the main population.\n                self.best_index = np.argmin(self.fitness)\n                self.f_opt = self.fitness[self.best_index]\n                self.x_opt = self.population[self.best_index]\n\n\n                if self.evals >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm CauchyLocalSearchDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ea8b1456-54ce-4d93-b189-484f9c17046c"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "83a2fbd1-010a-490b-bf8f-7809cff02774", "fitness": -Infinity, "name": "CauchyOrthogonalDE", "description": "Self-adaptive Differential Evolution with a Cauchy mutation operator and orthogonal learning to enhance population diversity.", "code": "import numpy as np\n\nclass CauchyOrthogonalDE:\n    def __init__(self, budget=10000, dim=10, pop_size=100, memory_size=10, archive_size=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.memory_size = memory_size\n        self.archive_size = archive_size\n        self.memory_CR = np.full(self.memory_size, 0.5)\n        self.memory_F = np.full(self.memory_size, 0.5)\n        self.archive = []\n        self.p_best_rate = 0.11\n        self.sf_CR = []\n        self.sf_F = []\n        self.p = self.p_best_rate\n        self.H = self.memory_size\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n        self.archive = []\n\n        while self.evals < self.budget:\n            sf_CR = []\n            sf_F = []\n\n            for i in range(self.pop_size):\n                # Memory selection\n                memory_index = np.random.randint(self.memory_size)\n                current_CR = self.memory_CR[memory_index]\n                current_F = self.memory_F[memory_index]\n\n                # Mutation (Cauchy)\n                p_best_index = np.random.choice(np.argsort(self.fitness)[:max(1, int(self.p * self.pop_size))])\n                x_pbest = self.population[p_best_index]\n\n                indices = np.random.choice(self.pop_size, 2, replace=False)\n                x_r1, x_r2 = self.population[indices]\n\n                # Cauchy mutation\n                cauchy_value = current_F * np.random.standard_cauchy()\n                mutant = self.population[i] + cauchy_value * (x_pbest - self.population[i]) + current_F * (x_r1 - x_r2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < current_CR or j == j_rand:\n                        trial[j] = mutant[j]\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Orthogonal learning\n                orthogonal_point = self.orthogonal_design(self.population[i], trial, func.bounds.lb, func.bounds.ub)\n                f_orthogonal = func(orthogonal_point)\n                self.evals += 1\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i] and f_trial <= f_orthogonal:\n                    self.archive.append(self.population[i].copy())\n                    sf_CR.append(current_CR)\n                    sf_F.append(current_F)\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                elif f_orthogonal < self.fitness[i] and f_orthogonal < f_trial:\n                    self.archive.append(self.population[i].copy())\n                    sf_CR.append(current_CR)\n                    sf_F.append(current_F)\n                    self.fitness[i] = f_orthogonal\n                    self.population[i] = orthogonal_point\n                    if f_orthogonal < self.f_opt:\n                        self.f_opt = f_orthogonal\n                        self.x_opt = orthogonal_point\n\n                if self.evals >= self.budget:\n                    break\n\n            # Memory update\n            if len(self.archive) > 0:\n                self.archive = self.archive[-self.archive_size:]\n\n                if sf_CR:\n                    mean_CR = np.mean(sf_CR)\n                    for memory_index in range(self.memory_size):\n                        self.memory_CR[memory_index] = mean_CR\n                if sf_F:\n                    mean_F = np.mean(sf_F)\n                    for memory_index in range(self.memory_size):\n                        self.memory_F[memory_index] = mean_F\n\n        return self.f_opt, self.x_opt\n    \n    def orthogonal_design(self, x1, x2, lb, ub):\n        levels = 4  # Number of levels for each factor\n        design = np.array([\n            [-1, -1],\n            [-1, 1],\n            [1, -1],\n            [1, 1]\n        ])\n\n        factors = np.zeros((self.dim, levels))\n        for i in range(self.dim):\n            factors[i, 0] = x1[i]\n            factors[i, 1] = x2[i]\n            factors[i, 2] = lb + (ub - lb) * np.random.rand()\n            factors[i, 3] = lb + (ub - lb) * np.random.rand()\n            factors[i, :] = np.clip(factors[i, :], lb, ub)\n\n        orthogonal_point = np.zeros(self.dim)\n        for i in range(self.dim):\n            level_index = np.random.randint(levels)\n            orthogonal_point[i] = factors[i, level_index]\n        return orthogonal_point", "configspace": "", "generation": 5, "feedback": "An exception occurred: setting an array element with a sequence..", "error": "", "parent_ids": ["14602a10-7381-40db-b49f-e7d61e538805"], "operator": null, "metadata": {}}
{"id": "ea52e7ea-7e6b-4cd4-8cd7-86dd092b3a1f", "fitness": -Infinity, "name": "AdaptiveOrthoDE", "description": "Adaptive Differential Evolution with orthogonal learning and a dynamic population size adjustment based on optimization progress.", "code": "import numpy as np\n\nclass AdaptiveOrthoDE:\n    def __init__(self, budget=10000, dim=10, pop_size_init=50, F_init=0.5, CR_init=0.7, ortho_group_size=5, pop_size_min=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size_init\n        self.pop_size_min = pop_size_min\n        self.F = F_init\n        self.CR = CR_init\n        self.ortho_group_size = ortho_group_size\n        self.archive_factor = 2.0\n        self.archive = []\n\n    def orthogonal_learning(self, population, fitnesses, group_size):\n        \"\"\"\n        Perform orthogonal learning on a subset of the population.\n        \"\"\"\n        best_index = np.argmin(fitnesses)\n        best_individual = population[best_index]\n        \n        indices = np.random.choice(len(population), min(group_size, len(population)), replace=False)\n        group = population[indices]\n\n        # Calculate centroid of the group\n        centroid = np.mean(group, axis=0)\n\n        # Generate an orthogonal vector\n        orthogonal_vector = best_individual - centroid\n        orthogonal_vector /= np.linalg.norm(orthogonal_vector)  # Normalize\n\n        # Create new candidate by moving along the orthogonal direction\n        candidate = best_individual + 0.1 * orthogonal_vector * np.random.randn(self.dim) # Scale and add noise\n        \n        return candidate\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n        self.archive = []\n\n        success_history_F = []\n        success_history_CR = []\n        memory_size = 10\n\n        def update_memory(memory, value):\n            memory.append(value)\n            if len(memory) > memory_size:\n                memory.pop(0)\n            return memory\n\n        while self.evals < self.budget:\n            # Adaptive F and CR\n            if success_history_F:\n                self.F = np.clip(np.random.choice(success_history_F), 0.1, 1.0)\n            else:\n                self.F = 0.5\n            if success_history_CR:\n                self.CR = np.clip(np.random.choice(success_history_CR), 0.1, 1.0)\n            else:\n                self.CR = 0.7\n\n            for i in range(self.pop_size):\n                # Differential Evolution Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                mutant = self.population[i] + self.F * (x_r1 - x_r2)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        trial[j] = mutant[j]\n\n                # Orthogonal learning\n                candidate_ortho = self.orthogonal_learning(self.population, self.fitness, self.ortho_group_size)\n                \n                # Randomly choose between trial and orthogonal candidate\n                if np.random.rand() < 0.5:\n                    trial = candidate_ortho\n\n                # Repair\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    success_history_F = update_memory(success_history_F, self.F)\n                    success_history_CR = update_memory(success_history_CR, self.CR)\n                    \n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        \n                    # Update archive\n                    self.archive.append(self.population[i])\n                    if len(self.archive) > self.archive_factor * self.pop_size:\n                        self.archive.pop(0)\n\n            # Dynamic Population Size Adjustment\n            if self.evals > self.budget * 0.5:\n                if np.random.rand() < 0.1 and self.pop_size > self.pop_size_min:\n                    self.pop_size = max(self.pop_size_min, int(self.pop_size * 0.9)) # Reduce population size\n\n                    # Remove worst individuals\n                    worst_indices = np.argsort(self.fitness)[- (len(self.population) - self.pop_size):]\n                    self.population = np.delete(self.population, worst_indices, axis=0)\n                    self.fitness = np.delete(self.fitness, worst_indices)\n                    \n                    # Refill population with archive or random\n                    num_to_refill = self.pop_size_init - self.pop_size\n                    if self.archive:\n                        archive_indices = np.random.choice(len(self.archive), min(num_to_refill, len(self.archive)), replace=True)\n                        new_individuals = [self.archive[i] for i in archive_indices]\n                    else:\n                        new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(num_to_refill, self.dim))\n                    \n                    self.population = np.concatenate((self.population, new_individuals), axis=0)\n                    new_fitness = np.array([func(x) for x in self.population[self.pop_size:]])\n                    self.fitness = np.concatenate((self.fitness, new_fitness))\n                    self.evals += len(new_fitness)\n                    self.pop_size = len(self.population)\n\n\n                self.best_index = np.argmin(self.fitness)\n                self.f_opt = self.fitness[self.best_index]\n                self.x_opt = self.population[self.best_index]\n                \n            if self.evals >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "An exception occurred: 'AdaptiveOrthoDE' object has no attribute 'pop_size_init'.", "error": "", "parent_ids": ["9b35df8a-475d-40f2-a6fa-4e0f47d67be5"], "operator": null, "metadata": {}}
{"id": "a3e2ffd8-75b2-44d9-9e42-6be142d75c9a", "fitness": 0.10219128942238602, "name": "CMAES_DE", "description": "A hybrid algorithm combining Differential Evolution with a Gaussian mutation and a covariance matrix adaptation strategy for improved search direction and step size control.", "code": "import numpy as np\n\nclass CMAES_DE:\n    def __init__(self, budget=10000, dim=10, pop_size=None, F=0.5, CR=0.7, initial_sigma=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.F = F\n        self.CR = CR\n        self.initial_sigma = initial_sigma\n        self.mean = None\n        self.sigma = None\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.chiN = None\n        self.evals = 0\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n\n    def initialize(self, func):\n        self.mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.sigma = self.initial_sigma\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n    def sample_population(self):\n        z = np.random.randn(self.pop_size, self.dim)\n        population = self.mean + self.sigma * np.dot(z, np.linalg.cholesky(self.C).T)\n        return population\n    \n    def repair(self, x, func):\n        return np.clip(x, func.bounds.lb, func.bounds.ub)\n\n\n    def __call__(self, func):\n        self.initialize(func)\n        \n        c_sigma = (self.pop_size + 2) / (self.dim + 3) / 3\n        c_c = 4 / (self.dim + 4)\n        c_1 = 2 / ((self.dim + np.sqrt(2))**2)\n        c_mu = 2 / ((self.dim + 2)**2)\n        d_sigma = 1 + 2 * max(0, np.sqrt((c_1 + c_mu - 1)))\n        mu = self.pop_size // 2\n\n\n        while self.evals < self.budget:\n            population = self.sample_population()\n            population = np.array([self.repair(x, func) for x in population])\n            fitness = np.array([func(x) for x in population])\n            self.evals += self.pop_size\n\n            if np.min(fitness) < self.f_opt:\n                self.f_opt = np.min(fitness)\n                self.x_opt = population[np.argmin(fitness)]\n\n\n            indices = np.argsort(fitness)\n            best_individuals = population[indices[:mu]]\n            \n            \n            weights = np.array([np.log(mu + 0.5) - np.log(i + 1) for i in range(mu)])\n            weights /= np.sum(weights)\n            \n\n            mean_old = self.mean.copy()\n            self.mean = np.sum(weights[:, None] * best_individuals, axis=0)\n            \n            \n            z = (self.mean - mean_old) / self.sigma\n            self.ps = (1 - c_sigma) * self.ps + np.sqrt(c_sigma * (2 - c_sigma)) * z\n            self.pc = (1 - c_c) * self.pc + np.sqrt(c_c * (2 - c_c)) * (self.mean - mean_old)\n\n            hsig = np.linalg.norm(self.ps) / np.sqrt(1 - (1 - c_sigma)**(self.evals / self.pop_size)) / self.chiN < 1.4 + 2 / (self.dim + 1)\n            self.C = (1 - c_1 - c_mu) * self.C + c_1 * (self.pc[:, None] @ self.pc[None, :])\n            self.C += c_mu * np.sum(weights[:, None, None] * ((best_individuals - mean_old)[:, :, None] @ (best_individuals - mean_old)[:, None, :]), axis=0)\n\n\n            self.sigma *= np.exp((c_sigma / d_sigma) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            \n            if self.evals >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm CMAES_DE scored 0.102 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9b35df8a-475d-40f2-a6fa-4e0f47d67be5"], "operator": null, "metadata": {"aucs": [0.007073351078243251, 0.009899979817028948, 0.1996677637481915, 0.0619006474698246, 0.1427203843706859, 0.14651123903238017, 0.17355327839552104, 0.07815349181536235, 0.023731207200797133, 0.10384847236011974, 0.12385493123599645, 0.16847752487670953, 9.999999999998899e-05, 0.007299609055362977, 0.09413668339251924, 0.1526950353445884, 0.19850163915899355, 0.11942154057153187, 0.09782965141408151, 0.1344493581097822]}}
{"id": "856c1d67-a94b-43c2-a5b9-4d56a4b8889b", "fitness": 0.266174692736027, "name": "SelfOrganizingPSO", "description": "A self-organizing particle swarm optimization with velocity clamping and adaptive exploration-exploitation balance using the fitness landscape.", "code": "import numpy as np\n\nclass SelfOrganizingPSO:\n    def __init__(self, budget=10000, dim=10, pop_size=50, inertia=0.7, c1=1.5, c2=1.5, v_max=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.inertia = inertia\n        self.c1 = c1\n        self.c2 = c2\n        self.v_max = v_max\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = None\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n\n    def __call__(self, func):\n        # Initialization\n        self.particles = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.random.uniform(-self.v_max, self.v_max, size=(self.pop_size, self.dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_fitness = np.array([func(x) for x in self.particles])\n        self.evals = self.pop_size\n\n        self.global_best_position = self.personal_best_positions[np.argmin(self.personal_best_fitness)]\n        self.global_best_fitness = np.min(self.personal_best_fitness)\n        \n        while self.evals < self.budget:\n            # Calculate exploration tendency\n            exploration_tendency = np.std(self.particles, axis=0)\n            exploration_tendency = exploration_tendency / np.sum(exploration_tendency) # Normalize\n            \n            # Adaptive adjustment of PSO parameters\n            adaptive_c1 = self.c1 * (1 + exploration_tendency)\n            adaptive_c2 = self.c2 * (1 - exploration_tendency)\n            \n            for i in range(self.pop_size):\n                # Update velocity\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                \n                self.velocities[i] = (self.inertia * self.velocities[i] +\n                                    adaptive_c1 * r1 * (self.personal_best_positions[i] - self.particles[i]) +\n                                    adaptive_c2 * r2 * (self.global_best_position - self.particles[i]))\n                \n                # Velocity clamping\n                self.velocities[i] = np.clip(self.velocities[i], -self.v_max, self.v_max)\n                \n                # Update particle position\n                new_position = self.particles[i] + self.velocities[i]\n                new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n\n                # Evaluate fitness\n                new_fitness = func(new_position)\n                self.evals += 1\n                \n                # Update personal best\n                if new_fitness < self.personal_best_fitness[i]:\n                    self.personal_best_fitness[i] = new_fitness\n                    self.personal_best_positions[i] = new_position\n                    \n                    # Update global best\n                    if new_fitness < self.global_best_fitness:\n                        self.global_best_fitness = new_fitness\n                        self.global_best_position = new_position\n                        \n            if self.evals >= self.budget:\n                break\n                \n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 5, "feedback": "The algorithm SelfOrganizingPSO scored 0.266 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ea8b1456-54ce-4d93-b189-484f9c17046c"], "operator": null, "metadata": {"aucs": [0.10788373273291185, 0.20635568409766625, 0.27396125769006396, 0.16416305400643905, 0.1778121100754132, 0.1829497653901453, 0.19553290370361454, 0.19946909179231265, 0.22494820733553655, 0.1635320291774317, 0.17990840342796055, 0.9906146309035947, 0.24923452095276688, 0.167287722670165, 0.5455687217765178, 0.2612352605620166, 0.20554711465131004, 0.21960799995958824, 0.14191704112214942, 0.4659646026929357]}}
{"id": "327d2504-2cc4-49a0-89b2-09d57e3264d1", "fitness": 0.3841726978860882, "name": "CauchySelfAdaptiveDE", "description": "Differential Evolution with a Cauchy mutation operator and a self-adaptive strategy that adjusts both the location and scale parameters of the Cauchy distribution based on the improvement rate.", "code": "import numpy as np\n\nclass CauchySelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, loc_init=0.0, scale_init=1.0, CR_init=0.7, learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.loc = loc_init  # Cauchy location parameter\n        self.scale = scale_init  # Cauchy scale parameter\n        self.CR = CR_init\n        self.learning_rate = learning_rate\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n        self.archive = []\n\n    def cauchy_mutation(self, size, loc, scale):\n        \"\"\"\n        Generate Cauchy distribution samples.\n        \"\"\"\n        return np.random.standard_cauchy(size) * scale + loc\n\n    def __call__(self, func):\n        # Initialization\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n\n        successful_loc = []\n        successful_scale = []\n        successful_CR = []\n\n        # Optimization loop\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation (Cauchy distribution)\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                cauchy_steps = self.cauchy_mutation(self.dim, self.loc, self.scale)\n                mutant = x_r1 + cauchy_steps * (x_r2 - x_r3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR:\n                        trial[j] = mutant[j]\n\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    successful_loc.append(self.loc)\n                    successful_scale.append(self.scale)\n                    successful_CR.append(self.CR)\n                    \n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                if self.evals >= self.budget:\n                    break\n            \n            # Self-adaptive parameter control\n            if successful_loc:\n                self.loc = np.mean(successful_loc) + self.learning_rate * np.random.normal(0, 0.1)\n                self.scale = np.mean(successful_scale) * np.exp(self.learning_rate * np.random.normal(0, 0.1))\n                self.scale = np.clip(self.scale, 0.01, 2.0)\n\n                mean_CR = np.mean(successful_CR) if successful_CR else self.CR\n                self.CR = mean_CR + self.learning_rate * np.random.normal(0, 0.1)\n                self.CR = np.clip(self.CR, 0.1, 1.0)\n\n            successful_loc = []\n            successful_scale = []\n            successful_CR = []\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm CauchySelfAdaptiveDE scored 0.384 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["4ab3022d-bebe-4bc9-8e59-5e6f2615aa27"], "operator": null, "metadata": {"aucs": [0.14750442398590435, 0.19988892808573488, 0.35088857862659006, 0.3906792362888758, 0.36306656819646854, 0.41412176332899386, 0.312300440272275, 0.34433204665799255, 0.34510160143139845, 0.22521873679399018, 0.31793004392870816, 0.9979945586233909, 0.2665630217456393, 0.34159060714618017, 0.7068761605212259, 0.44625809049513177, 0.3280653032316364, 0.5148452704937909, 0.19027670693304288, 0.4799518709347935]}}
{"id": "54194f87-881b-45b7-9051-a20511a580f9", "fitness": 0.6455134007884826, "name": "NeighborhoodDE", "description": "Neighborhood-based mutation in Differential Evolution, where the mutation is guided by the best solution in the local neighborhood of each individual.", "code": "import numpy as np\n\nclass NeighborhoodDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, neighborhood_size=10, F=0.5, CR=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.neighborhood_size = neighborhood_size\n        self.F = F\n        self.CR = CR\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Define neighborhood\n                indices = np.arange(self.pop_size)\n                distances = np.linalg.norm(self.population - self.population[i], axis=1)\n                neighborhood_indices = indices[np.argsort(distances)][:self.neighborhood_size]\n\n                # Find best solution in neighborhood\n                best_neighbor_index = neighborhood_indices[np.argmin(self.fitness[neighborhood_indices])]\n\n                # Mutation\n                indices = np.random.choice(self.pop_size, 2, replace=False)\n                x_r1, x_r2 = self.population[indices]\n                mutant = self.population[i] + self.F * (self.population[best_neighbor_index] - self.population[i]) + self.F * (x_r1 - x_r2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR:\n                        trial[j] = mutant[j]\n\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                \n                if self.evals >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm NeighborhoodDE scored 0.646 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ea8b1456-54ce-4d93-b189-484f9c17046c"], "operator": null, "metadata": {"aucs": [0.20461426900621338, 0.5461796381065931, 0.6919006601301554, 0.8358314520163493, 0.7347259223074233, 0.8110156641552911, 0.47971642401968917, 0.6682985327524762, 0.7814265402254148, 0.21444633886890418, 0.867510479133686, 0.996628523228185, 0.4399735216533698, 0.6806240446983327, 0.9224420523623195, 0.8018202260311307, 0.593155976155703, 0.8625345680897507, 0.2747574124937263, 0.5026657703349393]}}
{"id": "438f4f48-3827-47e4-ad14-7fdb7764c1a2", "fitness": 0.47345110960940673, "name": "CauchyVelocityDE", "description": "Differential Evolution with a Cauchy mutation operator, velocity-based parameter adaptation, and a local search component triggered when stagnation is detected.", "code": "import numpy as np\n\nclass CauchyVelocityDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, stagnation_threshold=1000, local_search_radius=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.stagnation_threshold = stagnation_threshold\n        self.local_search_radius = local_search_radius\n        self.velocity_F = np.zeros(self.pop_size)\n        self.velocity_CR = np.zeros(self.pop_size)\n        self.best_fitness_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Velocity-based Parameter Adaptation\n                self.velocity_F[i] = 0.9 * self.velocity_F[i] + 0.1 * np.random.normal(0, 0.1)\n                self.velocity_CR[i] = 0.9 * self.velocity_CR[i] + 0.1 * np.random.normal(0, 0.1)\n                F = np.clip(self.F + self.velocity_F[i], 0.1, 1.0)\n                CR = np.clip(self.CR + self.velocity_CR[i], 0.1, 1.0)\n\n                # Mutation (Cauchy)\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                cauchy_noise = F * np.random.standard_cauchy(size=self.dim)\n                mutant = x_r1 + cauchy_noise * (x_r2 - x_r3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < CR:\n                        trial[j] = mutant[j]\n\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n            \n            self.best_fitness_history.append(self.f_opt)\n\n            # Stagnation Detection and Local Search\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if np.std(self.best_fitness_history[-self.stagnation_threshold:]) < 1e-6:  #Stagnation\n                    # Perform local search around the best solution\n                    for i in range(self.pop_size):\n                        neighbor = self.x_opt + np.random.uniform(-self.local_search_radius, self.local_search_radius, size=self.dim)\n                        neighbor = np.clip(neighbor, func.bounds.lb, func.bounds.ub)\n                        f_neighbor = func(neighbor)\n                        self.evals += 1\n\n                        if f_neighbor < self.f_opt:\n                            self.f_opt = f_neighbor\n                            self.x_opt = neighbor\n                        \n                        if f_neighbor < self.fitness[i]:\n                            self.fitness[i] = f_neighbor\n                            self.population[i] = neighbor\n\n            if self.evals >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm CauchyVelocityDE scored 0.473 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ea8b1456-54ce-4d93-b189-484f9c17046c"], "operator": null, "metadata": {"aucs": [0.17772989859778443, 0.2265867845559073, 0.4562522253151642, 0.6823681111552989, 0.5374133191076891, 0.5790739333659431, 0.3678196442032563, 0.43590411771599924, 0.45942752676416365, 0.38417583253615994, 0.5071098649520615, 0.9931703489437828, 0.2241588123580306, 0.4150714426918687, 0.7214265383929839, 0.5873095552368408, 0.37284471736873215, 0.6303462585289452, 0.19449721013773946, 0.5163360502597832]}}
{"id": "e8f17d77-8b3e-4490-bb63-a6b747600a30", "fitness": 0.0, "name": "CauchySelfAdaptiveDE", "description": "Differential Evolution with a Cauchy mutation operator, self-adaptive parameter control using a memory of past successful values, and a local search strategy to refine promising solutions.", "code": "import numpy as np\n\nclass CauchySelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_init=0.5, CR_init=0.7, memory_size=10, local_search_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.CR = CR_init\n        self.memory_size = memory_size\n        self.local_search_prob = local_search_prob\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n        self.F_memory = []\n        self.CR_memory = []\n\n    def cauchy_mutation(self, size):\n        \"\"\"\n        Generate Cauchy distribution samples.\n        \"\"\"\n        return np.random.standard_cauchy(size=size)\n\n    def local_search(self, x, func, radius=0.1):\n        \"\"\"\n        Perform local search around x.\n        \"\"\"\n        x_new = x + np.random.uniform(-radius, radius, size=self.dim)\n        x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n        f_new = func(x_new)\n        self.evals += 1\n        if f_new < func(x):\n            return x_new, f_new\n        else:\n            return x, func(x)\n\n    def __call__(self, func):\n        # Initialization\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n\n        # Optimization loop\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation (Cauchy mutation)\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                cauchy_steps = self.cauchy_mutation(self.dim)\n                mutant = x_r1 + self.F * cauchy_steps * (x_r2 - x_r3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR:\n                        trial[j] = mutant[j]\n\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    # Store successful F and CR values\n                    if len(self.F_memory) < self.memory_size:\n                        self.F_memory.append(self.F)\n                        self.CR_memory.append(self.CR)\n                    else:\n                        self.F_memory[np.random.randint(self.memory_size)] = self.F\n                        self.CR_memory[np.random.randint(self.memory_size)] = self.CR\n\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        # Local search around best solution\n                        if np.random.rand() < self.local_search_prob:\n                             self.x_opt, self.f_opt = self.local_search(self.x_opt, func)\n                            \n\n                if self.evals >= self.budget:\n                    break\n\n            # Self-adaptive parameter control\n            if self.F_memory:\n                self.F = np.mean(self.F_memory)\n                self.F = np.clip(self.F, 0.1, 1.0)\n\n                self.CR = np.mean(self.CR_memory)\n                self.CR = np.clip(self.CR, 0.1, 1.0)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm CauchySelfAdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["4ab3022d-bebe-4bc9-8e59-5e6f2615aa27"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "ac4401b5-c7da-42f3-be7e-1602a70355a6", "fitness": 0.3326701378629508, "name": "OrthogonalAdaptiveDE", "description": "Adaptive Differential Evolution with orthogonal learning to improve population diversity and convergence speed.", "code": "import numpy as np\n\nclass OrthogonalAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=100, memory_size=10, archive_size=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.memory_size = memory_size\n        self.archive_size = archive_size\n        self.memory_CR = np.full(self.memory_size, 0.5)\n        self.memory_F = np.full(self.memory_size, 0.5)\n        self.archive = None\n        self.p_best_rate = 0.11\n        self.mu_CR = 0.5\n        self.mu_F = 0.5\n        self.sf_CR = []\n        self.sf_F = []\n        self.archive = []\n        self.H = self.memory_size\n        self.p = self.p_best_rate\n        self.orthogonal_sample_size = 5  # Number of samples for orthogonal design\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n        self.archive = []\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Memory selection\n                memory_index = np.random.randint(self.memory_size)\n                current_CR = self.memory_CR[memory_index]\n                current_F = self.memory_F[memory_index]\n\n                # Mutation\n                p_best_index = np.random.choice(np.argsort(self.fitness)[:max(1, int(self.p * self.pop_size))])\n                x_pbest = self.population[p_best_index]\n\n                indices = np.random.choice(self.pop_size, 2, replace=False)\n                x_r1, x_r2 = self.population[indices]\n\n                mutant = self.population[i] + current_F * (x_pbest - self.population[i]) + current_F * (x_r1 - x_r2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < current_CR or j == j_rand:\n                        trial[j] = mutant[j]\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Orthogonal Learning\n                orthogonal_matrix = self.generate_orthogonal_array(self.orthogonal_sample_size)\n                orthogonal_trials = []\n                for k in range(self.orthogonal_sample_size):\n                    orthogonal_trial = np.copy(self.population[i])\n                    for j in range(self.dim):\n                        if orthogonal_matrix[k, j % orthogonal_matrix.shape[1]] == 1:\n                            orthogonal_trial[j] = trial[j] # use the trial vector components\n\n                    orthogonal_trial = np.clip(orthogonal_trial, func.bounds.lb, func.bounds.ub)\n                    orthogonal_trials.append(orthogonal_trial)\n\n                orthogonal_fitnesses = [func(x) for x in orthogonal_trials]\n                self.evals += self.orthogonal_sample_size\n                best_orthogonal_index = np.argmin(orthogonal_fitnesses)\n                best_orthogonal_trial = orthogonal_trials[best_orthogonal_index]\n                f_orthogonal = orthogonal_fitnesses[best_orthogonal_index]\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_orthogonal < min(f_trial, self.fitness[i]):\n                    if self.fitness[i] < f_trial:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        self.archive.append(trial.copy())\n\n                    self.fitness[i] = f_orthogonal\n                    self.population[i] = best_orthogonal_trial\n\n                    if f_orthogonal < self.f_opt:\n                        self.f_opt = f_orthogonal\n                        self.x_opt = best_orthogonal_trial\n                elif f_trial < self.fitness[i]:\n                    self.archive.append(self.population[i].copy())\n\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n\n                if self.evals >= self.budget:\n                    break\n\n            # Memory update\n            if len(self.archive) > 0:\n                self.archive = self.archive[-self.archive_size:]\n                \n                successful_CRs = []\n                successful_Fs = []\n                \n                for i in range(self.pop_size):\n                  if self.fitness[i] < self.fitness[i]:\n                    delta_f = np.abs(self.fitness[i] - self.fitness[i])\n                \n                if self.sf_CR:\n                    self.memory_CR = np.mean(self.sf_CR)\n                if self.sf_F:\n                    self.memory_F = np.mean(self.sf_F)\n\n                # Reset sf_CR and sf_F\n                self.sf_CR = []\n                self.sf_F = []\n\n\n        return self.f_opt, self.x_opt\n\n    def generate_orthogonal_array(self, n):\n      # generates an orthogonal array of strength 2 and size n x log2(n)\n      k = int(np.ceil(np.log2(n)))\n      orthogonal_array = np.zeros((n, k), dtype=int)\n      for j in range(k):\n        for i in range(n):\n          if (i // (2**j)) % 2 == 1:\n            orthogonal_array[i, j] = 1\n      return orthogonal_array", "configspace": "", "generation": 5, "feedback": "The algorithm OrthogonalAdaptiveDE scored 0.333 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["14602a10-7381-40db-b49f-e7d61e538805"], "operator": null, "metadata": {"aucs": [0.16681386155230737, 0.21202718771942675, 0.33628037563958924, 0.32298989417774415, 0.24406078449580626, 0.3229329762841362, 0.2746008560677129, 0.2910320175546244, 0.2543590849453923, 0.19451596277698002, 0.29069962769994095, 0.9458878299179989, 0.2591042748934044, 0.24752292743296922, 0.6804621346664745, 0.31597007135727606, 0.2823834946615795, 0.3600861071995729, 0.17279404519321417, 0.47887924302286633]}}
{"id": "5676e55c-9b0a-485e-83bb-8ca90e8cfc15", "fitness": -Infinity, "name": "GaussianDistanceAdaptiveDE", "description": "Differential Evolution with a Gaussian mutation operator, velocity-based parameter adaptation, and a distance-based self-adaptive strategy that adjusts the mutation strength based on the average distance to the population.", "code": "import numpy as np\n\nclass GaussianDistanceAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_init=0.5, CR_init=0.7, learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.CR = CR_init\n        self.learning_rate = learning_rate\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n        self.archive = []\n\n    def gaussian_mutation(self, size, loc, scale):\n        \"\"\"\n        Generate Gaussian distribution samples.\n        \"\"\"\n        return np.random.normal(loc, scale, size)\n\n    def __call__(self, func):\n        # Initialization\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n\n        successful_F = []\n        successful_CR = []\n\n        # Optimization loop\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation (Gaussian distribution)\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                mutant = x_r1 + self.F * (x_r2 - x_r3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR:\n                        trial[j] = mutant[j]\n\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    successful_F.append(self.F)\n                    successful_CR.append(self.CR)\n                    \n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                if self.evals >= self.budget:\n                    break\n            \n            # Self-adaptive parameter control\n            if successful_F:\n                self.F = np.mean(successful_F) + self.learning_rate * np.random.normal(0, 0.1)\n                self.F = np.clip(self.F, 0.1, 1.0)\n\n                mean_CR = np.mean(successful_CR) if successful_CR else self.CR\n                self.CR = mean_CR + self.learning_rate * np.random.normal(0, 0.1)\n                self.CR = np.clip(self.CR, 0.1, 1.0)\n            else:\n                # If no successful mutations, increase F slightly to explore more\n                self.F = min(self.F + 0.05, 1.0)\n                self.CR = min(self.CR + 0.05, 1.0)\n\n\n            # Distance-based F adaptation\n            distances = np.linalg.norm(self.population - np.mean(self.population, axis=0), axis=1)\n            avg_distance = np.mean(distances)\n\n            # Adjust F based on average distance: higher distance, higher exploration\n            self.F = self.F_init + 0.5 * (avg_distance / (func.bounds.ub[0] - func.bounds.lb[0]))\n            self.F = np.clip(self.F, 0.1, 1.0)\n            \n            successful_F = []\n            successful_CR = []\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "An exception occurred: 'GaussianDistanceAdaptiveDE' object has no attribute 'F_init'.", "error": "", "parent_ids": ["327d2504-2cc4-49a0-89b2-09d57e3264d1"], "operator": null, "metadata": {}}
{"id": "a6d17a66-9e44-4ca5-aaff-0d4825014350", "fitness": -Infinity, "name": "OrthogonalCovarianceAdaptiveDE", "description": "Differential Evolution with orthogonal crossover and self-adaptive covariance matrix adaptation for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass OrthogonalCovarianceAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, CR_init=0.7, F_init=0.5, learning_rate=0.1, orthogonal_dim=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR_init\n        self.F = F_init\n        self.learning_rate = learning_rate\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n        self.orthogonal_dim = orthogonal_dim\n        self.covariance_matrix = np.eye(dim)  # Initialize covariance matrix\n\n    def orthogonal_crossover(self, x, mutant):\n        \"\"\"\n        Performs orthogonal crossover between x and mutant.\n        \"\"\"\n        orthogonal_indices = np.random.choice(self.dim, size=self.orthogonal_dim, replace=False)\n        trial = np.copy(x)\n        trial[orthogonal_indices] = mutant[orthogonal_indices]\n        return trial\n        \n    def generate_mutant(self):\n        return np.random.multivariate_normal(np.zeros(self.dim), self.covariance_matrix)\n\n    def __call__(self, func):\n        # Initialization\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n        \n        successful_F = []\n        successful_CR = []\n        \n        # Optimization loop\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n\n                mutant = x_r1 + self.F * (x_r2 - x_r3) + self.generate_mutant() # Add covariance adaptation for better exploration\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Orthogonal Crossover\n                trial = self.orthogonal_crossover(self.population[i], mutant)\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR:\n                        trial[j] = mutant[j]\n\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    successful_F.append(self.F)\n                    successful_CR.append(self.CR)\n\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                if self.evals >= self.budget:\n                    break\n            \n            # Self-adaptive parameter control\n            if successful_F:\n                self.F = np.mean(successful_F) + self.learning_rate * np.random.normal(0, 0.1)\n                self.F = np.clip(self.F, 0.1, 1.0)\n            else:\n                self.F = self.F * np.exp(self.learning_rate * np.random.normal(0, 0.1)) # Increase exploration when no success\n                self.F = np.clip(self.F, 0.1, 1.0)\n\n            if successful_CR:\n                self.CR = np.mean(successful_CR) + self.learning_rate * np.random.normal(0, 0.1)\n                self.CR = np.clip(self.CR, 0.1, 1.0)\n            else:\n                self.CR = self.CR * np.exp(self.learning_rate * np.random.normal(0, 0.1)) # Increase exploration when no success\n                self.CR = np.clip(self.CR, 0.1, 1.0)\n\n            successful_F = []\n            successful_CR = []\n\n            # Covariance Matrix Adaptation\n            elite_indices = np.argsort(self.fitness)[:self.pop_size // 5]  # Select top 20% as elite\n            elite_population = self.population[elite_indices]\n            \n            if len(elite_population) > 1:\n                covariance = np.cov(elite_population.T)\n                # Regularize covariance matrix\n                self.covariance_matrix = (1 - self.learning_rate) * self.covariance_matrix + self.learning_rate * covariance\n                self.covariance_matrix += np.eye(self.dim) * 1e-6  # Add small value to diagonal for stability\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "An exception occurred: Cannot take a larger sample than population when 'replace=False'.", "error": "", "parent_ids": ["327d2504-2cc4-49a0-89b2-09d57e3264d1"], "operator": null, "metadata": {}}
{"id": "24158742-38aa-45c4-82e6-07867c51d722", "fitness": 0.0, "name": "DynamicPopulationDE", "description": "Differential Evolution with a dynamic population size and a learning-based mutation strategy that adapts based on recent successes.", "code": "import numpy as np\n\nclass DynamicPopulationDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, F=0.5, CR=0.7, pop_size_reduction_factor=0.9, success_history_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.initial_pop_size = initial_pop_size\n        self.F = F\n        self.CR = CR\n        self.pop_size_reduction_factor = pop_size_reduction_factor\n        self.success_history_size = success_history_size\n        self.success_history_F = np.ones(success_history_size) * F\n        self.success_history_CR = np.ones(success_history_size) * CR\n        self.success_count = 0\n        self.history_index = 0\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                F = self.success_history_F.mean()\n                CR = self.success_history_CR.mean()\n                F = np.clip(np.random.normal(F, 0.1), 0.1, 1.0)\n                CR = np.clip(np.random.normal(CR, 0.1), 0.1, 1.0)\n\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                mutant = x_r1 + F * (x_r2 - x_r3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < CR:\n                        trial[j] = mutant[j]\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.success_history_F[self.history_index] = F\n                    self.success_history_CR[self.history_index] = CR\n                    self.history_index = (self.history_index + 1) % self.success_history_size\n\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Dynamic population size adjustment\n            if self.evals > self.budget * 0.5 and np.random.rand() < 0.1:\n                 new_pop_size = int(self.pop_size * self.pop_size_reduction_factor)\n                 if new_pop_size > 10:\n                     self.pop_size = new_pop_size\n                     indices = np.argsort(self.fitness)[:self.pop_size]\n                     self.population = self.population[indices]\n                     self.fitness = self.fitness[indices]\n                \n            if self.evals >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm DynamicPopulationDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["54194f87-881b-45b7-9051-a20511a580f9"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "ee46a31d-fbdc-47ec-8f64-9da9469431b8", "fitness": 0.09911838772063075, "name": "AdaptiveDistanceLocalSearchDE", "description": "Differential Evolution with a self-adaptive strategy for both the mutation factor and crossover rate, and a distance-based local search to escape local optima.", "code": "import numpy as np\n\nclass AdaptiveDistanceLocalSearchDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_init=0.5, CR_init=0.7, local_search_prob=0.1, local_search_radius=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_init = F_init\n        self.CR_init = CR_init\n        self.local_search_prob = local_search_prob\n        self.local_search_radius = local_search_radius\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n\n        self.F = np.full(self.pop_size, self.F_init)\n        self.CR = np.full(self.pop_size, self.CR_init)\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Self-adaptive parameters\n                self.F[i] = np.clip(np.random.normal(self.F_init, 0.1), 0.1, 1.0)\n                self.CR[i] = np.clip(np.random.normal(self.CR_init, 0.1), 0.1, 1.0)\n\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                mutant = self.population[i] + self.F[i] * (x_r1 - x_r2) + self.F[i] * (x_r3 - self.population[i])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR[i]:\n                        trial[j] = mutant[j]\n\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                \n                # Distance-based Local Search\n                if np.random.rand() < self.local_search_prob:\n                    # Calculate distance to the best solution\n                    distance = np.linalg.norm(self.population[i] - self.x_opt)\n                    \n                    # Define a local search range based on the distance\n                    radius = self.local_search_radius * distance \n                    \n                    # Generate a new solution within the local search range\n                    local_trial = self.population[i] + np.random.uniform(-radius, radius, size=self.dim)\n                    local_trial = np.clip(local_trial, func.bounds.lb, func.bounds.ub)\n                    \n                    f_local_trial = func(local_trial)\n                    self.evals += 1\n                    \n                    if f_local_trial < self.fitness[i]:\n                        self.fitness[i] = f_local_trial\n                        self.population[i] = local_trial\n                        \n                        if f_local_trial < self.f_opt:\n                            self.f_opt = f_local_trial\n                            self.x_opt = local_trial\n\n                if self.evals >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveDistanceLocalSearchDE scored 0.099 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["54194f87-881b-45b7-9051-a20511a580f9"], "operator": null, "metadata": {"aucs": [0.1982367754412615, 0]}}
{"id": "50d8e0cb-bb86-433a-952f-bc877ed560c3", "fitness": 0.0, "name": "LevyMemoryAdaptiveDE", "description": "Differential Evolution with Lvy flight mutation, adaptive scaling, and a memory-based restart mechanism to escape local optima.", "code": "import numpy as np\n\nclass LevyMemoryAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, levy_exponent=1.5, CR_init=0.7, F_init=0.5, memory_size=10, learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.levy_exponent = levy_exponent\n        self.CR = CR_init\n        self.F = F_init\n        self.learning_rate = learning_rate\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n        self.memory_size = memory_size\n        self.memory = []  # Store best solutions encountered\n        self.memory_fitness = [] #Store fitness of best solutions\n\n    def levy_flight(self, size, exponent):\n        \"\"\"\n        Generate Levy distribution samples.\n        \"\"\"\n        num = np.random.gamma(1 + exponent) * np.sin(np.pi * exponent / 2)\n        den = np.random.gamma((1 + exponent) / 2) * exponent * 2**((exponent - 1) / 2)\n        sigma = (num / den)**(1 / exponent)\n        u = np.random.normal(0, sigma, size)\n        v = np.random.normal(0, 1, size)\n        return u / (np.abs(v)**(1 / exponent))\n\n    def __call__(self, func):\n        # Initialization\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n        \n        # Initialize memory with initial best solution\n        self.memory.append(self.x_opt.copy())\n        self.memory_fitness.append(self.f_opt)\n\n        successful_CR = []\n        successful_F = []\n\n        # Optimization loop\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation (Levy flight)\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                levy_steps = self.levy_flight(self.dim, self.levy_exponent)\n                mutant = x_r1 + self.F * levy_steps * (x_r2 - x_r3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR:\n                        trial[j] = mutant[j]\n\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    successful_CR.append(self.CR)\n                    successful_F.append(self.F)\n\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        \n                        # Update memory if new best is found\n                        if len(self.memory) < self.memory_size:\n                            self.memory.append(self.x_opt.copy())\n                            self.memory_fitness.append(self.f_opt)\n                        else:\n                            worst_mem_idx = np.argmax(self.memory_fitness)\n                            if self.f_opt < self.memory_fitness[worst_mem_idx]:\n                                self.memory[worst_mem_idx] = self.x_opt.copy()\n                                self.memory_fitness[worst_mem_idx] = self.f_opt\n                \n                if self.evals >= self.budget:\n                    break\n\n            # Adaptive parameter control\n            if successful_CR:\n                self.CR = np.mean(successful_CR) + self.learning_rate * np.random.normal(0, 0.1)\n                self.CR = np.clip(self.CR, 0.1, 1.0)\n                self.F = np.mean(successful_F) + self.learning_rate * np.random.normal(0, 0.1)\n                self.F = np.clip(self.F, 0.1, 1.0)\n\n            else:\n                # If no improvement, explore more\n                self.CR = np.random.uniform(0.5, 1.0)\n                self.F = np.random.uniform(0.1, 0.9)\n\n            # Restart mechanism: replace worst individual with a solution from memory\n            worst_idx = np.argmax(self.fitness)\n            best_mem_idx = np.argmin(self.memory_fitness)\n            \n            if self.fitness[worst_idx] > self.memory_fitness[best_mem_idx]: #Only restart if worst is worse than the best in memory\n                self.population[worst_idx] = self.memory[best_mem_idx].copy()\n                self.fitness[worst_idx] = func(self.population[worst_idx])\n                self.evals += 1\n\n                if self.fitness[worst_idx] < self.f_opt:\n                    self.f_opt = self.fitness[worst_idx]\n                    self.x_opt = self.population[worst_idx]\n                    \n            successful_CR = []\n            successful_F = []\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm LevyMemoryAdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["327d2504-2cc4-49a0-89b2-09d57e3264d1"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "b66b10c1-8ee9-4cb9-81cf-21794e255f2e", "fitness": -Infinity, "name": "DynamicNeighborhoodDE", "description": "Differential Evolution with a dynamically adjusted neighborhood size and a local search based on the Nelder-Mead simplex method when the population stagnates.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DynamicNeighborhoodDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, initial_neighborhood_size=10, F=0.5, CR=0.7, stagnation_threshold=1e-6, stagnation_iterations=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.neighborhood_size = initial_neighborhood_size\n        self.initial_neighborhood_size = initial_neighborhood_size\n        self.F = F\n        self.CR = CR\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_iterations = stagnation_iterations\n        self.best_fitness_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n        self.best_fitness_history.append(self.f_opt)\n        self.iteration = 0\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Define neighborhood\n                indices = np.arange(self.pop_size)\n                distances = np.linalg.norm(self.population - self.population[i], axis=1)\n                neighborhood_indices = indices[np.argsort(distances)][:self.neighborhood_size]\n\n                # Find best solution in neighborhood\n                best_neighbor_index = neighborhood_indices[np.argmin(self.fitness[neighborhood_indices])]\n\n                # Mutation\n                indices = np.random.choice(self.pop_size, 2, replace=False)\n                x_r1, x_r2 = self.population[indices]\n                mutant = self.population[i] + self.F * (self.population[best_neighbor_index] - self.population[i]) + self.F * (x_r1 - x_r2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR:\n                        trial[j] = mutant[j]\n\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n            \n            # Stagnation check and local search\n            self.best_fitness_history.append(self.f_opt)\n            if self.iteration > self.stagnation_iterations:\n                if np.abs(self.best_fitness_history[-1] - self.best_fitness_history[-self.stagnation_iterations-1]) < self.stagnation_threshold:\n                    # Perform local search around the best solution\n                    bounds = [(func.bounds.lb, func.bounds.ub) for _ in range(self.dim)]\n                    res = minimize(func, self.x_opt, method='Nelder-Mead', bounds=bounds, options={'maxfev': self.budget - self.evals})\n                    if res.success:\n                        f_local = res.fun\n                        x_local = res.x\n                        if f_local < self.f_opt:\n                            self.f_opt = f_local\n                            self.x_opt = x_local\n                        self.evals += res.nfev\n\n                    # Increase neighborhood size\n                    self.neighborhood_size = min(self.pop_size, self.neighborhood_size + 2)\n            else:\n                # Reduce neighborhood size to explore more if not stagnated\n                self.neighborhood_size = self.initial_neighborhood_size\n\n\n            self.iteration += 1\n\n            if self.evals >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "An exception occurred: name 'minimize' is not defined.", "error": "", "parent_ids": ["54194f87-881b-45b7-9051-a20511a580f9"], "operator": null, "metadata": {}}
{"id": "4b66e839-dcfb-45b0-89b9-d8ca17062551", "fitness": 0.25076441312460857, "name": "SelfOrganizingScouts", "description": "Self-organizing scouts with adaptive step size control and a memory of promising regions to enhance exploration and exploitation.", "code": "import numpy as np\n\nclass SelfOrganizingScouts:\n    def __init__(self, budget=10000, dim=10, num_scouts=20, memory_size=10, initial_step_size=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.num_scouts = num_scouts\n        self.memory_size = memory_size\n        self.initial_step_size = initial_step_size\n        self.step_sizes = np.full(self.num_scouts, self.initial_step_size)\n        self.memory = []  # Store promising locations\n        self.memory_fitness = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.scouts = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.num_scouts, self.dim))\n        self.fitness = np.array([func(x) for x in self.scouts])\n        self.evals = self.num_scouts\n\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.scouts[self.best_index]\n        \n        while self.evals < self.budget:\n            for i in range(self.num_scouts):\n                # Adaptive step size\n                step_size = self.step_sizes[i]\n\n                # Scout moves\n                new_location = self.scouts[i] + np.random.normal(0, step_size, self.dim)\n                new_location = np.clip(new_location, func.bounds.lb, func.bounds.ub)\n                f_new = func(new_location)\n                self.evals += 1\n\n                # Memory-guided exploration\n                if len(self.memory) > 0 and np.random.rand() < 0.2: # 20% chance to use memory\n                    memory_index = np.random.randint(len(self.memory))\n                    memory_location = self.memory[memory_index]\n                    new_location = self.scouts[i] + 0.5 * (memory_location - self.scouts[i]) + np.random.normal(0, step_size, self.dim)\n                    new_location = np.clip(new_location, func.bounds.lb, func.bounds.ub)\n                    f_new = func(new_location)\n                    self.evals += 1\n\n                # Update scout location\n                if f_new < self.fitness[i]:\n                    self.fitness[i] = f_new\n                    self.scouts[i] = new_location\n                    self.step_sizes[i] *= 1.1  # Increase step size if improving\n                else:\n                    self.step_sizes[i] *= 0.9  # Decrease step size if not improving\n                \n                self.step_sizes[i] = np.clip(self.step_sizes[i], self.initial_step_size/10, self.initial_step_size*10) #limit the step size\n\n                # Update global best\n                if self.fitness[i] < self.f_opt:\n                    self.f_opt = self.fitness[i]\n                    self.x_opt = self.scouts[i]\n\n                # Update memory\n                if len(self.memory) < self.memory_size:\n                    self.memory.append(self.scouts[i].copy())\n                    self.memory_fitness.append(self.fitness[i])\n                else:\n                    worst_memory_index = np.argmax(self.memory_fitness)\n                    if self.fitness[i] < self.memory_fitness[worst_memory_index]:\n                        self.memory[worst_memory_index] = self.scouts[i].copy()\n                        self.memory_fitness[worst_memory_index] = self.fitness[i]\n\n                if self.evals >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm SelfOrganizingScouts scored 0.251 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ac4401b5-c7da-42f3-be7e-1602a70355a6"], "operator": null, "metadata": {"aucs": [0.10636618128701936, 0.20476379943134193, 0.515978665720402, 0.1675432652776655, 0.18817902753871085, 0.5725199526171205, 0]}}
{"id": "f5b24cfd-df84-4c5a-8ae8-91480a79ad7f", "fitness": 0.3140905627399829, "name": "GaussianVelocityDERestart", "description": "Differential Evolution with Gaussian mutation, velocity-based parameter adaptation, and a restart mechanism triggered by stagnation.", "code": "import numpy as np\n\nclass GaussianVelocityDERestart:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_init=0.5, CR_init=0.7, learning_rate=0.1, stagnation_threshold=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.CR = CR_init\n        self.learning_rate = learning_rate\n        self.stagnation_threshold = stagnation_threshold\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n        self.stagnation_counter = 0\n        self.archive = []\n\n    def gaussian_mutation(self, size, F):\n        \"\"\"\n        Generate Gaussian distribution samples.\n        \"\"\"\n        return np.random.normal(0, F, size)\n\n    def __call__(self, func):\n        # Initialization\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n        self.best_fitness_history = [self.f_opt]\n\n        # Optimization loop\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation (Gaussian distribution)\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                gaussian_steps = self.gaussian_mutation(self.dim, self.F)\n                mutant = x_r1 + gaussian_steps * (x_r2 - x_r3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR:\n                        trial[j] = mutant[j]\n\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.stagnation_counter = 0 # Reset stagnation counter\n\n                if self.evals >= self.budget:\n                    break\n            \n            # Adaptive parameter control\n            self.F = self.F + self.learning_rate * np.random.normal(0, 0.1)\n            self.F = np.clip(self.F, 0.1, 1.0)\n            self.CR = self.CR + self.learning_rate * np.random.normal(0, 0.1)\n            self.CR = np.clip(self.CR, 0.1, 1.0)\n\n            # Stagnation Check\n            self.best_fitness_history.append(self.f_opt)\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                self.best_fitness_history.pop(0)  # Remove the oldest fitness value\n                if abs(self.best_fitness_history[-1] - self.best_fitness_history[0]) < 1e-6:  # Define a small tolerance\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0 # Reset counter if there is progress\n\n            # Restart mechanism\n            if self.stagnation_counter > self.stagnation_threshold:\n                # Re-initialize the population (excluding the best)\n                new_population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size - 1, self.dim))\n                self.population[:-1] = new_population\n                self.population[-1] = self.x_opt.copy() # Keep the best solution\n                self.fitness = np.array([func(x) for x in self.population])\n                self.evals += self.pop_size - 1\n                self.best_index = np.argmin(self.fitness)\n                self.f_opt = self.fitness[self.best_index]\n                self.x_opt = self.population[self.best_index]\n                self.stagnation_counter = 0 # Reset the counter after restart\n                self.best_fitness_history = [self.f_opt] # Reset history\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm GaussianVelocityDERestart scored 0.314 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["327d2504-2cc4-49a0-89b2-09d57e3264d1"], "operator": null, "metadata": {"aucs": [0.21467429042737274, 0.43418300274081423, 0.6075049577917448, 0]}}
{"id": "4075841e-1f14-437f-aefe-df6fe75d196b", "fitness": 0.4806207035038101, "name": "CrowdAdaptiveDE", "description": "Population-based Adaptive Mutation with Elitism and Diversity Preservation using a Crowding Distance mechanism.", "code": "import numpy as np\n\nclass CrowdAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=100, F=0.5, CR=0.9, elite_percent=0.1, crowding_epsilon=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.elite_count = int(elite_percent * pop_size)\n        self.crowding_epsilon = crowding_epsilon\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n\n        while self.evals < self.budget:\n            # Sort population by fitness (for elitism and crowding distance)\n            sorted_indices = np.argsort(self.fitness)\n            self.population = self.population[sorted_indices]\n            self.fitness = self.fitness[sorted_indices]\n\n            if self.fitness[0] < self.f_opt:\n                self.f_opt = self.fitness[0]\n                self.x_opt = self.population[0]\n\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros_like(self.fitness)\n            \n            # Elitism: Keep the best individuals\n            new_population[:self.elite_count] = self.population[:self.elite_count]\n            new_fitness[:self.elite_count] = self.fitness[:self.elite_count]\n            \n            # Adaptive Mutation and Crossover for the rest of the population\n            for i in range(self.elite_count, self.pop_size):\n                # Adaptive F and CR\n                adaptive_F = np.random.normal(self.F, 0.1)\n                adaptive_F = np.clip(adaptive_F, 0.1, 1.0)\n                adaptive_CR = np.random.normal(self.CR, 0.1)\n                adaptive_CR = np.clip(adaptive_CR, 0.1, 1.0)\n                \n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                mutant = self.population[i] + adaptive_F * (x_r1 - x_r2) + adaptive_F * (x_r3 - self.population[i])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < adaptive_CR or j == j_rand:\n                        trial[j] = mutant[j]\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n                \n                # Evaluate Trial Vector\n                f_trial = func(trial)\n                self.evals += 1\n\n                # Selection based on crowding distance\n                if f_trial < self.fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                else:\n                    # Crowding distance mechanism: Keep the original if it is not too close to better individuals\n                    distance_to_better = np.min(np.linalg.norm(self.population[:i] - self.population[i], axis=1))\n                    if distance_to_better > self.crowding_epsilon:\n                        new_population[i] = self.population[i]\n                        new_fitness[i] = self.fitness[i]\n                    else:\n                        new_population[i] = trial #replace with trial vector\n                        new_fitness[i] = f_trial #update the fitness as well\n\n\n            self.population = new_population\n            self.fitness = new_fitness\n\n            if self.evals >= self.budget:\n                break\n        \n        sorted_indices = np.argsort(self.fitness)\n        self.population = self.population[sorted_indices]\n        self.fitness = self.fitness[sorted_indices]\n        if self.fitness[0] < self.f_opt:\n            self.f_opt = self.fitness[0]\n            self.x_opt = self.population[0]\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm CrowdAdaptiveDE scored 0.481 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ac4401b5-c7da-42f3-be7e-1602a70355a6"], "operator": null, "metadata": {"aucs": [0.19058814596027562, 0.378948086654741, 0.3735563770593955, 0.7197537505627217, 0.4304858583970089, 0.49678516203191614, 0.3322701826586286, 0.3914956184801557, 0.4283955452668815, 0.21037543832834793, 0.7909160388627832, 0.9994932033284301, 0.4663450781641708, 0.3985943162700748, 0.8309513627515599, 0.49507188152352233, 0.38578125079432213, 0.6013343822129729, 0.20378482950022136, 0.487487561268072]}}
{"id": "25a07271-1adb-4d8a-88e5-abfe5c48748e", "fitness": 0.598541856900292, "name": "LevyAdaptiveRestartDE", "description": "Differential Evolution with Lvy flight mutation, adaptive parameters based on success rate, and a restart mechanism triggered by stagnation.", "code": "import numpy as np\n\nclass LevyAdaptiveRestartDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, stagnation_threshold=1000, levy_exponent=1.5, restart_probability=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.stagnation_threshold = stagnation_threshold\n        self.levy_exponent = levy_exponent\n        self.restart_probability = restart_probability\n        self.success_history_F = []\n        self.success_history_CR = []\n        self.best_fitness_history = []\n\n    def levy_flight(self, size, exponent):\n        u = np.random.randn(size)\n        v = np.random.randn(size)\n        sigma = (np.math.gamma(1 + exponent) * np.sin(np.pi * exponent / 2) / (np.math.gamma((1 + exponent) / 2) * exponent * (2 ** ((exponent - 1) / 2)))) ** (1 / exponent)\n        step = sigma * u / (np.abs(v) ** (1 / exponent))\n        return step\n    \n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Adaptive Parameter Control\n                if self.success_history_F:\n                    self.F = np.clip(np.mean(self.success_history_F), 0.1, 1.0)\n                if self.success_history_CR:\n                    self.CR = np.clip(np.mean(self.success_history_CR), 0.1, 1.0)\n                \n                # Mutation (Levy Flight)\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                levy_steps = self.levy_flight(size=self.dim, exponent=self.levy_exponent)\n                mutant = x_r1 + self.F * levy_steps * (x_r2 - x_r3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR:\n                        trial[j] = mutant[j]\n\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.success_history_F.append(self.F)\n                    self.success_history_CR.append(self.CR)\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            self.best_fitness_history.append(self.f_opt)\n\n            # Stagnation Detection and Restart Mechanism\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if np.std(self.best_fitness_history[-self.stagnation_threshold:]) < 1e-6:\n                    # Restart the population with a small probability\n                    if np.random.rand() < self.restart_probability:\n                        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                        self.fitness = np.array([func(x) for x in self.population])\n                        self.evals += self.pop_size\n                        best_index = np.argmin(self.fitness)\n                        if self.fitness[best_index] < self.f_opt:\n                            self.f_opt = self.fitness[best_index]\n                            self.x_opt = self.population[best_index]\n                        self.success_history_F = []\n                        self.success_history_CR = []\n                        self.best_fitness_history = []\n\n            if self.evals >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm LevyAdaptiveRestartDE scored 0.599 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["438f4f48-3827-47e4-ad14-7fdb7764c1a2"], "operator": null, "metadata": {"aucs": [0.20151078381021348, 0.3532728500466934, 0.5358339066034074, 0.8176786172934054, 0.7169154710184302, 0.746356943380486, 0.5996623395240019, 0.5679302279557047, 0.6343914522252008, 0.6080147983235882, 0.7290496881340047, 0.997082497879668, 0.2542224415134128, 0.5776255694496507, 0.7290125625627282, 0.7269055775351689, 0.5035219598383969, 0.771018310634407, 0.36196915053455925, 0.5388619897427152]}}
{"id": "a05c8456-5015-4b94-aabd-36ac852731a9", "fitness": 0.24404657570552565, "name": "DynamicNeighborhoodRestartDE", "description": "Differential Evolution with a dynamically adjusted neighborhood size and a restart mechanism to escape local optima.", "code": "import numpy as np\n\nclass DynamicNeighborhoodRestartDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, initial_neighborhood_size=10, F=0.5, CR=0.7, restart_prob=0.05, neighborhood_adapt_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.neighborhood_size = initial_neighborhood_size\n        self.initial_neighborhood_size = initial_neighborhood_size\n        self.F = F\n        self.CR = CR\n        self.restart_prob = restart_prob\n        self.neighborhood_adapt_rate = neighborhood_adapt_rate\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Define neighborhood\n                indices = np.arange(self.pop_size)\n                distances = np.linalg.norm(self.population - self.population[i], axis=1)\n                neighborhood_indices = indices[np.argsort(distances)][:int(self.neighborhood_size)]\n\n                # Find best solution in neighborhood\n                best_neighbor_index = neighborhood_indices[np.argmin(self.fitness[neighborhood_indices])]\n\n                # Mutation\n                indices = np.random.choice(self.pop_size, 2, replace=False)\n                x_r1, x_r2 = self.population[indices]\n                mutant = self.population[i] + self.F * (self.population[best_neighbor_index] - self.population[i]) + self.F * (x_r1 - x_r2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR:\n                        trial[j] = mutant[j]\n\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                    # Adapt neighborhood size (increase if improvement)\n                    self.neighborhood_size = min(self.pop_size, self.neighborhood_size * (1 + self.neighborhood_adapt_rate))\n                else:\n                    # Adapt neighborhood size (decrease if no improvement)\n                    self.neighborhood_size = max(1, self.neighborhood_size * (1 - self.neighborhood_adapt_rate))\n\n                # Restart mechanism\n                if np.random.rand() < self.restart_prob:\n                    self.population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                    self.fitness[i] = func(self.population[i])\n                    self.evals += 1\n                    if self.fitness[i] < self.f_opt:\n                        self.f_opt = self.fitness[i]\n                        self.x_opt = self.population[i]\n                \n                if self.evals >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm DynamicNeighborhoodRestartDE scored 0.244 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["54194f87-881b-45b7-9051-a20511a580f9"], "operator": null, "metadata": {"aucs": [0.13877359566185865, 0.20694014658789983, 0.31833404401015486, 0.3998241745090725, 0.2776309683029057, 0.33409986134612646, 0.2656129777110132, 0.29738929670610303, 0.2663406889850417, 0.17956657894060613, 0]}}
{"id": "d7b57c88-4ff4-46be-954c-c17ce1595a75", "fitness": 0.7532113945011866, "name": "EnsembleAdaptiveDE", "description": "Differential Evolution with an ensemble of mutation strategies and self-adaptive selection probabilities for each strategy.", "code": "import numpy as np\n\nclass EnsembleAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, CR=0.7, F=0.5, learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.F = F\n        self.learning_rate = learning_rate\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n\n        # Define multiple mutation strategies\n        self.mutation_strategies = [\n            self.mutation_strategy_1,  # DE/rand/1\n            self.mutation_strategy_2,  # DE/current-to-best/1\n            self.mutation_strategy_3   # DE/best/1\n        ]\n        self.num_strategies = len(self.mutation_strategies)\n\n        # Initialize selection probabilities for each strategy\n        self.strategy_probabilities = np.ones(self.num_strategies) / self.num_strategies\n        self.successful_strategies = []\n\n    def mutation_strategy_1(self, population, i): # DE/rand/1\n        indices = np.random.choice(self.pop_size, 5, replace=False)\n        x_r1, x_r2, x_r3, x_r4, x_r5 = population[indices]\n        return x_r1 + self.F * (x_r2 - x_r3) + self.F * (x_r4 - x_r5)\n    \n    def mutation_strategy_2(self, population, i, best_index): # DE/current-to-best/1\n        indices = np.random.choice(self.pop_size, 3, replace=False)\n        x_r1, x_r2, x_r3 = population[indices]\n        return population[i] + self.F * (population[best_index] - population[i]) + self.F * (x_r1 - x_r2)\n\n    def mutation_strategy_3(self, population, i, best_index): # DE/best/1\n        indices = np.random.choice(self.pop_size, 3, replace=False)\n        x_r1, x_r2, x_r3 = population[indices]\n        return self.population[best_index] + self.F * (x_r1 - x_r2)\n\n    def __call__(self, func):\n        # Initialization\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n\n\n        # Optimization loop\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Strategy Selection\n                strategy_index = np.random.choice(self.num_strategies, p=self.strategy_probabilities)\n                mutation_strategy = self.mutation_strategies[strategy_index]\n\n                # Mutation\n                if mutation_strategy == self.mutation_strategy_1:\n                    mutant = mutation_strategy(self.population, i)\n                else:\n                    mutant = mutation_strategy(self.population, i, self.best_index)\n                    \n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR:\n                        trial[j] = mutant[j]\n\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.successful_strategies.append(strategy_index)\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.best_index = np.argmin(self.fitness)\n\n                if self.evals >= self.budget:\n                    break\n\n            # Update strategy probabilities\n            if self.successful_strategies:\n                strategy_counts = np.bincount(self.successful_strategies, minlength=self.num_strategies)\n                success_rates = strategy_counts / np.sum(strategy_counts)\n                self.strategy_probabilities = (1 - self.learning_rate) * self.strategy_probabilities + self.learning_rate * success_rates\n                self.strategy_probabilities /= np.sum(self.strategy_probabilities)  # Normalize\n\n            self.successful_strategies = []\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm EnsembleAdaptiveDE scored 0.753 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["327d2504-2cc4-49a0-89b2-09d57e3264d1"], "operator": null, "metadata": {"aucs": [0.30929140746654826, 0.7955683734876139, 0.8227192970229351, 0.90391198380083, 0.8547417858981421, 0.8851540485646643, 0.3403075934878893, 0.8076760462111129, 0.8495245329765861, 0.7865647772101094, 0.9239909305438856, 0.9989041130396963, 0.5616650447251925, 0.8126223768922113, 0.94070672510359, 0.8719913438664535, 0.7757801921187273, 0.9141796330567785, 0.3733584790070754, 0.5355692055436874]}}
{"id": "488e75f3-163d-42a4-9dbc-2b1cec3b90f9", "fitness": -Infinity, "name": "SOMAdaptiveDE", "description": "A differential evolution variant that incorporates a self-organizing map (SOM) to adapt the mutation operator and parameters based on the population distribution.", "code": "import numpy as np\nfrom minisom import MiniSom\n\nclass SOMAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, CR=0.7, F=0.5, som_grid_size=10, learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.F = F\n        self.som_grid_size = som_grid_size\n        self.learning_rate = learning_rate\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n        self.som = None\n\n    def initialize_som(self):\n        \"\"\"Initializes the Self-Organizing Map.\"\"\"\n        self.som = MiniSom(self.som_grid_size, self.som_grid_size, self.dim, sigma=0.3, learning_rate=0.5)\n        self.som.random_weights_init(self.population)\n        self.som.train_random(self.population, 100)  # Train for a short initial period\n\n\n    def adapt_mutation_parameters(self, individual):\n        \"\"\"Adapts mutation parameters based on the SOM unit the individual belongs to.\"\"\"\n        winner = self.som.winner(individual)\n        # Example: Modify F and CR based on SOM unit coordinates\n        F = 0.1 + (winner[0] / self.som_grid_size) * 0.8  # F between 0.1 and 0.9\n        CR = 0.2 + (winner[1] / self.som_grid_size) * 0.7 # CR between 0.2 and 0.9\n        return F, CR\n\n    def mutation(self, population, i, F):\n        \"\"\"DE/rand/1 mutation.\"\"\"\n        indices = np.random.choice(self.pop_size, 3, replace=False)\n        x_r1, x_r2, x_r3 = population[indices]\n        mutant = x_r1 + F * (x_r2 - x_r3)\n        return mutant\n\n    def __call__(self, func):\n        # Initialization\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n\n        self.initialize_som()\n\n        # Optimization loop\n        while self.evals < self.budget:\n            # Train SOM every few generations\n            if self.evals % (self.pop_size * 5) == 0:\n                self.som.train_random(self.population, 50)\n\n            for i in range(self.pop_size):\n                # Adapt mutation parameters\n                F, CR = self.adapt_mutation_parameters(self.population[i])\n\n                # Mutation\n                mutant = self.mutation(self.population, i, F)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < CR:\n                        trial[j] = mutant[j]\n\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.best_index = np.argmin(self.fitness)\n\n                if self.evals >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "An exception occurred: No module named 'minisom'.", "error": "", "parent_ids": ["d7b57c88-4ff4-46be-954c-c17ce1595a75"], "operator": null, "metadata": {}}
{"id": "04b3e111-4773-4af5-a74e-141d0f455c00", "fitness": -Infinity, "name": "OrthogonalAdaptiveDE", "description": "Differential Evolution with orthogonal learning, adaptive crossover rate, and an archive to maintain diversity and prevent premature convergence.", "code": "import numpy as np\n\nclass OrthogonalAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, learning_rate=0.1, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.learning_rate = learning_rate\n        self.archive_size = archive_size\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n        self.archive = []\n\n        # Adaptive Crossover Rate parameters\n        self.CR_mean = 0.5\n        self.CR_memory = []\n        self.CR_memory_size = 10\n\n\n    def orthogonal_design(self, n, k):\n        \"\"\"Generates an orthogonal design for n factors and k levels.\"\"\"\n        if n == 2 and k == 2:\n            return np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n        \n        design = np.zeros((n*k, k), dtype=int)\n        for i in range(k):\n            for j in range(n):\n                design[i*n:(i+1)*n,j] = np.tile(np.repeat(np.arange(n),1),k)[i*n:(i+1)*n]\n\n        return design % 2\n\n    def __call__(self, func):\n        # Initialization\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n\n        # Optimization loop\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                mutant = self.population[i] + self.F * (x_r1 - x_r2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                \n                # Adaptive Crossover Rate\n                CR = np.random.normal(self.CR_mean, 0.1)\n                CR = np.clip(CR, 0, 1)\n\n                # Orthogonal Crossover\n                orthogonal_design_size = min(self.dim, 4) # Choose a small design size for efficiency\n                orthogonal_matrix = self.orthogonal_design(orthogonal_design_size, 2)\n                \n                trial = np.copy(self.population[i])\n                \n                for j in range(self.dim):\n                    if np.random.rand() < CR or j < orthogonal_design_size:\n                        if j < orthogonal_design_size:\n                            if orthogonal_matrix[i % orthogonal_matrix.shape[0], j % orthogonal_design_size] == 1:\n                                trial[j] = mutant[j]\n                        else:\n                            trial[j] = mutant[j]\n                \n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    # Archive update\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i])\n                    else:\n                        idx_to_replace = np.random.randint(0, self.archive_size)\n                        self.archive[idx_to_replace] = self.population[i]\n                        \n                    # Update CR memory\n                    self.CR_memory.append(CR)\n                    if len(self.CR_memory) > self.CR_memory_size:\n                        self.CR_memory.pop(0)\n\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.best_index = np.argmin(self.fitness)\n\n                if self.evals >= self.budget:\n                    break\n\n            # Update CR_mean using the successful CR values\n            if self.CR_memory:\n                self.CR_mean = np.mean(self.CR_memory)\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "An exception occurred: index 2 is out of bounds for axis 1 with size 2.", "error": "", "parent_ids": ["d7b57c88-4ff4-46be-954c-c17ce1595a75"], "operator": null, "metadata": {}}
{"id": "6feef66a-872b-4a1e-9b90-6d56cb0ab622", "fitness": 0.0, "name": "DistanceAdaptiveShakeDE", "description": "Differential Evolution with Self-Adaptive Mutation based on the distance to the best solution and a periodic shaking mechanism to escape local optima.", "code": "import numpy as np\n\nclass DistanceAdaptiveShakeDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, CR=0.7, shake_frequency=500, shake_intensity=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.shake_frequency = shake_frequency\n        self.shake_intensity = shake_intensity\n        self.population = None\n        self.fitness = None\n        self.best_fitness = np.inf\n        self.best_solution = None\n        self.evaluations = 0\n        self.F = 0.5  # Initial mutation factor\n\n    def __call__(self, func):\n        # Initialization\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evaluations = self.pop_size\n\n        best_index = np.argmin(self.fitness)\n        self.best_fitness = self.fitness[best_index]\n        self.best_solution = self.population[best_index].copy()\n\n        # Optimization loop\n        while self.evaluations < self.budget:\n            for i in range(self.pop_size):\n                # Adaptive mutation factor based on distance to best solution\n                distance = np.linalg.norm(self.population[i] - self.best_solution)\n                self.F = np.clip(0.1 + distance / (np.linalg.norm(func.bounds.ub - func.bounds.lb) * self.dim), 0.1, 1.0)\n\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                mutant = x_r1 + self.F * (x_r2 - x_r3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR:\n                        trial[j] = mutant[j]\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Evaluation\n                f_trial = func(trial)\n                self.evaluations += 1\n\n                # Selection\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    # Update best solution\n                    if f_trial < self.best_fitness:\n                        self.best_fitness = f_trial\n                        self.best_solution = trial.copy()\n\n            # Shaking mechanism\n            if self.evaluations % self.shake_frequency == 0:\n                for i in range(self.pop_size):\n                    random_vector = np.random.uniform(-1, 1, size=self.dim)\n                    self.population[i] += self.shake_intensity * random_vector\n                    self.population[i] = np.clip(self.population[i], func.bounds.lb, func.bounds.ub)\n\n                self.fitness = np.array([func(x) for x in self.population])\n                self.evaluations += self.pop_size // 2 # Approximate cost.\n\n                best_index = np.argmin(self.fitness)\n                if self.fitness[best_index] < self.best_fitness:\n                        self.best_fitness = self.fitness[best_index]\n                        self.best_solution = self.population[best_index].copy()\n\n        return self.best_fitness, self.best_solution", "configspace": "", "generation": 7, "feedback": "The algorithm DistanceAdaptiveShakeDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["25a07271-1adb-4d8a-88e5-abfe5c48748e"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "9ee6ef2e-4692-4ef5-b14d-88210642e930", "fitness": -Infinity, "name": "SOMAdaptiveDE", "description": "Differential Evolution with a self-organizing map to adapt mutation strategies and parameters based on the location of individuals in the search space.", "code": "import numpy as np\nfrom minisom import MiniSom\n\nclass SOMAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=100, som_grid_size=10, F=0.5, CR=0.9, learning_rate=0.1, sigma=0.3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.som_grid_size = som_grid_size\n        self.F = F\n        self.CR = CR\n        self.learning_rate = learning_rate\n        self.sigma = sigma\n        self.som = MiniSom(som_grid_size, som_grid_size, dim, sigma=sigma, learning_rate=learning_rate)\n        self.mutation_strategies = [\n            lambda x, xr1, xr2, xr3, F: x + F * (xr1 - xr2),  # DE/rand/1\n            lambda x, xr1, xr2, xr3, F: xr1 + F * (xr2 - xr3),  # DE/best/1\n            lambda x, xr1, xr2, xr3, F: x + F * (xr1 - xr2) + F * (xr3 - x),  #DE/rand/2\n        ]\n        self.strategy_probs = np.ones(len(self.mutation_strategies)) / len(self.mutation_strategies)\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n\n        # Initial SOM training\n        self.som.train(self.population, 1000)\n\n        while self.evals < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros_like(self.fitness)\n\n            for i in range(self.pop_size):\n                # Find the best matching unit (BMU) for the individual\n                bmu = self.som.winner(self.population[i])\n                bmu_index = np.ravel_multi_index(bmu, (self.som_grid_size, self.som_grid_size))\n\n                # Adapt mutation strategy based on BMU\n                strategy_index = np.random.choice(len(self.mutation_strategies), p=self.strategy_probs)\n                mutation_strategy = self.mutation_strategies[strategy_index]\n\n                # Adaptive F and CR (can be BMU-dependent)\n                adaptive_F = np.random.normal(self.F, 0.1)\n                adaptive_F = np.clip(adaptive_F, 0.1, 1.0)\n                adaptive_CR = np.random.normal(self.CR, 0.1)\n                adaptive_CR = np.clip(adaptive_CR, 0.1, 1.0)\n                \n\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                mutant = mutation_strategy(self.population[i], x_r1, x_r2, x_r3, adaptive_F)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < adaptive_CR or j == j_rand:\n                        trial[j] = mutant[j]\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Evaluate Trial Vector\n                f_trial = func(trial)\n                self.evals += 1\n\n                # Selection\n                if f_trial < self.fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                else:\n                    new_population[i] = self.population[i]\n                    new_fitness[i] = self.fitness[i]\n\n            self.population = new_population\n            self.fitness = new_fitness\n\n            # SOM Adaptation (retrain every few iterations)\n            if self.evals % self.pop_size * 5 == 0:\n                self.som.train(self.population, self.pop_size)\n\n\n            # Update best solution\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "An exception occurred: No module named 'minisom'.", "error": "", "parent_ids": ["4075841e-1f14-437f-aefe-df6fe75d196b"], "operator": null, "metadata": {}}
{"id": "b4b40121-6f2d-40db-bbfa-1f28f3fb0d47", "fitness": -Infinity, "name": "RingTopologyAdaptiveDE", "description": "Differential Evolution with a ring topology, self-adaptive parameters based on the success rate of mutation and crossover, and a periodic local search using Nelder-Mead Simplex.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass RingTopologyAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_init=0.5, CR_init=0.7, learning_rate=0.1, local_search_interval=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.CR = CR_init\n        self.learning_rate = learning_rate\n        self.local_search_interval = local_search_interval\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n        self.success_F = []\n        self.success_CR = []\n        self.archive = []\n        self.local_search_counter = 0\n\n    def ring_mutation(self, i):\n        \"\"\"\n        Ring topology mutation: uses neighbors in the population.\n        \"\"\"\n        left = (i - 1) % self.pop_size\n        right = (i + 1) % self.pop_size\n        x_r1 = self.population[left]\n        x_r2 = self.population[right]\n        x_r3 = self.population[np.random.choice(self.pop_size)] # Pick a random other individual\n\n        return x_r1, x_r2, x_r3\n\n    def self_adaptive_parameters(self, success_F, success_CR):\n        \"\"\"\n        Update F and CR based on the success rate.\n        \"\"\"\n        if success_F:\n            self.F = np.mean(success_F)\n        if success_CR:\n            self.CR = np.mean(success_CR)\n        self.F = np.clip(self.F, 0.1, 1.0)\n        self.CR = np.clip(self.CR, 0.1, 1.0)\n\n    def local_search(self, func, x):\n        \"\"\"\n        Perform a local search around x using Nelder-Mead.\n        \"\"\"\n        bounds = func.bounds\n        def obj_func(x_):\n            return func(x_)\n\n        res = minimize(obj_func, x, method='Nelder-Mead', bounds=[(bounds.lb[i], bounds.ub[i]) for i in range(self.dim)])\n        self.evals += res.nfev\n        return res.fun, res.x\n\n    def __call__(self, func):\n        # Initialization\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n\n        # Optimization loop\n        while self.evals < self.budget:\n            success_F = []\n            success_CR = []\n\n            for i in range(self.pop_size):\n                # Mutation (Ring topology)\n                x_r1, x_r2, x_r3 = self.ring_mutation(i)\n                mutant = self.population[i] + self.F * (x_r1 - x_r2) + self.F * (x_r3 - self.population[i])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR:\n                        trial[j] = mutant[j]\n\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    success_F.append(self.F)\n                    success_CR.append(self.CR)\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            # Adaptive parameter control\n            self.self_adaptive_parameters(success_F, success_CR)\n\n            # Local Search (Nelder-Mead)\n            self.local_search_counter += 1\n            if self.local_search_counter >= self.local_search_interval:\n                f_local, x_local = self.local_search(func, self.x_opt)\n                if f_local < self.f_opt:\n                    self.f_opt = f_local\n                    self.x_opt = x_local\n                    self.fitness[self.best_index] = f_local\n                    self.population[self.best_index] = x_local\n\n                self.local_search_counter = 0\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "An exception occurred: name 'minimize' is not defined.", "error": "", "parent_ids": ["f5b24cfd-df84-4c5a-8ae8-91480a79ad7f"], "operator": null, "metadata": {}}
{"id": "4ed2e865-9325-4560-961e-743bd4c3ecbd", "fitness": -Infinity, "name": "SOMAdaptiveDE", "description": "Differential Evolution with a self-organizing map (SOM) to cluster individuals and apply different mutation strategies based on cluster assignment.", "code": "import numpy as np\nfrom minisom import MiniSom\n\nclass SOMAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, CR=0.7, F=0.5, som_grid_size=5, learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.F = F\n        self.som_grid_size = som_grid_size\n        self.learning_rate = learning_rate\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n\n        # Define multiple mutation strategies\n        self.mutation_strategies = [\n            self.mutation_strategy_1,  # DE/rand/1\n            self.mutation_strategy_2,  # DE/current-to-best/1\n            self.mutation_strategy_3   # DE/best/1\n        ]\n        self.num_strategies = len(self.mutation_strategies)\n\n        # Initialize SOM\n        self.som = MiniSom(som_grid_size, som_grid_size, dim, sigma=0.3, learning_rate=0.1)\n        self.strategy_usage = np.zeros((som_grid_size, som_grid_size, self.num_strategies))\n\n    def mutation_strategy_1(self, population, i): # DE/rand/1\n        indices = np.random.choice(self.pop_size, 5, replace=False)\n        x_r1, x_r2, x_r3, x_r4, x_r5 = population[indices]\n        return x_r1 + self.F * (x_r2 - x_r3) + self.F * (x_r4 - x_r5)\n    \n    def mutation_strategy_2(self, population, i, best_index): # DE/current-to-best/1\n        indices = np.random.choice(self.pop_size, 3, replace=False)\n        x_r1, x_r2, x_r3 = population[indices]\n        return population[i] + self.F * (population[best_index] - population[i]) + self.F * (x_r1 - x_r2)\n\n    def mutation_strategy_3(self, population, i, best_index): # DE/best/1\n        indices = np.random.choice(self.pop_size, 3, replace=False)\n        x_r1, x_r2, x_r3 = population[indices]\n        return self.population[best_index] + self.F * (x_r1 - x_r2)\n\n    def __call__(self, func):\n        # Initialization\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n\n        # Train SOM initially\n        self.som.train_random(self.population, 1000)\n\n        # Optimization loop\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Find the best matching unit (BMU) for the individual\n                w_x, w_y = self.som.winner(self.population[i])\n\n                # Select mutation strategy based on SOM node\n                probabilities = self.strategy_usage[w_x, w_y]\n                if np.sum(probabilities) == 0:\n                    probabilities = np.ones(self.num_strategies) / self.num_strategies\n                else:\n                    probabilities = probabilities / np.sum(probabilities)\n                strategy_index = np.random.choice(self.num_strategies, p=probabilities)\n                mutation_strategy = self.mutation_strategies[strategy_index]\n\n                # Mutation\n                if mutation_strategy == self.mutation_strategy_1:\n                    mutant = mutation_strategy(self.population, i)\n                else:\n                    mutant = mutation_strategy(self.population, i, self.best_index)\n                    \n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR:\n                        trial[j] = mutant[j]\n\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n                    # Update SOM\n                    self.som.update(self.population[i], self.som.winner(self.population[i]), self.learning_rate)\n                    self.strategy_usage[w_x, w_y, strategy_index] += 1\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.best_index = np.argmin(self.fitness)\n\n                if self.evals >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "An exception occurred: No module named 'minisom'.", "error": "", "parent_ids": ["d7b57c88-4ff4-46be-954c-c17ce1595a75"], "operator": null, "metadata": {}}
{"id": "0b1b292a-2368-4d53-b7b7-03ce0a5b42d5", "fitness": -Infinity, "name": "DynamicEnsembleAdaptiveDE", "description": "Differential Evolution with dynamic ensemble of mutation strategies, adaptive strategy selection using a weighted average of past success and current performance, and a diversity maintenance mechanism based on distance to the nearest neighbor.", "code": "import numpy as np\n\nclass DynamicEnsembleAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, CR=0.7, F=0.5, learning_rate=0.1, diversity_weight=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.F = F\n        self.learning_rate = learning_rate\n        self.diversity_weight = diversity_weight\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n\n        # Define multiple mutation strategies\n        self.mutation_strategies = [\n            self.mutation_strategy_1,  # DE/rand/1\n            self.mutation_strategy_2,  # DE/current-to-best/1\n            self.mutation_strategy_3   # DE/best/1\n        ]\n        self.num_strategies = len(self.mutation_strategies)\n\n        # Initialize selection probabilities for each strategy\n        self.strategy_probabilities = np.ones(self.num_strategies) / self.num_strategies\n        self.successful_strategies = []\n        self.strategy_success_rates = np.zeros(self.num_strategies) # track immediate success\n        self.min_distance = np.zeros(self.pop_size)\n\n\n    def mutation_strategy_1(self, population, i): # DE/rand/1\n        indices = np.random.choice(self.pop_size, 5, replace=False)\n        x_r1, x_r2, x_r3, x_r4, x_r5 = population[indices]\n        return x_r1 + self.F * (x_r2 - x_r3) + self.F * (x_r4 - x_r5)\n    \n    def mutation_strategy_2(self, population, i, best_index): # DE/current-to-best/1\n        indices = np.random.choice(self.pop_size, 3, replace=False)\n        x_r1, x_r2, x_r3 = population[indices]\n        return population[i] + self.F * (population[best_index] - population[i]) + self.F * (x_r1 - x_r2)\n\n    def mutation_strategy_3(self, population, i, best_index): # DE/best/1\n        indices = np.random.choice(self.pop_size, 3, replace=False)\n        x_r1, x_r2, x_r3 = population[indices]\n        return self.population[best_index] + self.F * (x_r1 - x_r2)\n\n    def calculate_min_distance(self):\n        for i in range(self.pop_size):\n            distances = np.linalg.norm(self.population - self.population[i], axis=1)\n            distances[i] = np.inf  # Exclude itself\n            self.min_distance[i] = np.min(distances)\n\n    def __call__(self, func):\n        # Initialization\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n\n        # Optimization loop\n        while self.evals < self.budget:\n            self.calculate_min_distance()\n            diversity_score = np.mean(self.min_distance)\n\n            for i in range(self.pop_size):\n                # Strategy Selection: Roulette wheel selection weighted by success rate and diversity\n                normalized_success_rates = self.strategy_success_rates / np.sum(self.strategy_success_rates) if np.sum(self.strategy_success_rates) > 0 else np.ones(self.num_strategies) / self.num_strategies\n                strategy_weights = (1 - self.diversity_weight) * normalized_success_rates + self.diversity_weight * diversity_score # Combine success and diversity\n                strategy_index = np.random.choice(self.num_strategies, p=strategy_weights)\n                mutation_strategy = self.mutation_strategies[strategy_index]\n\n                # Mutation\n                if mutation_strategy == self.mutation_strategy_1:\n                    mutant = mutation_strategy(self.population, i)\n                else:\n                    mutant = mutation_strategy(self.population, i, self.best_index)\n                    \n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR:\n                        trial[j] = mutant[j]\n\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.successful_strategies.append(strategy_index)\n                    self.strategy_success_rates[strategy_index] +=1\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.best_index = np.argmin(self.fitness)\n\n                if self.evals >= self.budget:\n                    break\n            \n            # Update strategy probabilities - using exponential smoothing\n            if self.successful_strategies:\n                strategy_counts = np.bincount(self.successful_strategies, minlength=self.num_strategies)\n                success_rates = strategy_counts / np.sum(strategy_counts)\n                self.strategy_probabilities = (1 - self.learning_rate) * self.strategy_probabilities + self.learning_rate * success_rates\n                self.strategy_probabilities /= np.sum(self.strategy_probabilities)  # Normalize\n\n            self.successful_strategies = []\n            self.strategy_success_rates = np.zeros(self.num_strategies)\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "An exception occurred: probabilities do not sum to 1.", "error": "", "parent_ids": ["d7b57c88-4ff4-46be-954c-c17ce1595a75"], "operator": null, "metadata": {}}
{"id": "583a84f9-ea83-458a-b839-11f3b537b9a7", "fitness": 0.45208678212519293, "name": "SOMAdaptiveDE", "description": "Differential Evolution with a self-organizing map (SOM) for population diversification and adaptive parameter control based on SOM node fitness.", "code": "import numpy as np\n\nclass SOMAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, som_grid_size=10, F_init=0.5, CR_init=0.7, learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.som_grid_size = som_grid_size\n        self.F = F_init\n        self.CR = CR_init\n        self.learning_rate = learning_rate\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n        self.som = None  # Self-Organizing Map\n        self.node_assignments = None  # Mapping of population members to SOM nodes\n\n    def initialize_som(self):\n        \"\"\"Initialize the Self-Organizing Map (SOM).\"\"\"\n        self.som = np.random.uniform(-1, 1, size=(self.som_grid_size, self.som_grid_size, self.dim))\n\n    def find_closest_node(self, x):\n        \"\"\"Find the closest SOM node to a given individual.\"\"\"\n        distances = np.sum((self.som.reshape(-1, self.dim) - x) ** 2, axis=1)\n        return np.argmin(distances) // self.som_grid_size, np.argmin(distances) % self.som_grid_size\n\n    def update_som(self, x, node, learning_rate):\n        \"\"\"Update the SOM based on the individual and learning rate.\"\"\"\n        distance = np.linalg.norm(np.array(node) - np.array([self.som_grid_size // 2, self.som_grid_size // 2]))\n        influence = np.exp(-distance ** 2 / (2 * (self.som_grid_size / 3) ** 2)) # Gaussian neighborhood function\n        self.som[node[0], node[1]] += learning_rate * influence * (x - self.som[node[0], node[1]])\n\n    def assign_nodes(self):\n        \"\"\"Assign each individual in the population to its closest SOM node.\"\"\"\n        self.node_assignments = [self.find_closest_node(x) for x in self.population]\n\n    def adjust_parameters(self):\n        \"\"\"Adjust F and CR based on SOM node fitness.\"\"\"\n        node_fitnesses = {}\n        for i, node in enumerate(self.node_assignments):\n            if node not in node_fitnesses:\n                node_fitnesses[node] = []\n            node_fitnesses[node].append(self.fitness[i])\n\n        for node in node_fitnesses:\n            node_fitnesses[node] = np.mean(node_fitnesses[node])\n\n        # Adjust F and CR based on how fit individuals assigned to the node are.\n        for i, node in enumerate(self.node_assignments):\n            if node in node_fitnesses:\n                if self.fitness[i] < node_fitnesses[node]: # Individual better than average node fitness\n                    self.F = min(1.0, self.F + self.learning_rate * 0.1)\n                    self.CR = min(1.0, self.CR + self.learning_rate * 0.1)\n                else:\n                    self.F = max(0.1, self.F - self.learning_rate * 0.1)\n                    self.CR = max(0.1, self.CR - self.learning_rate * 0.1)\n\n    def __call__(self, func):\n        # Initialization\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n\n        self.initialize_som()\n        self.assign_nodes()\n\n        # Optimization loop\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                mutant = x_r1 + self.F * (x_r2 - x_r3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR:\n                        trial[j] = mutant[j]\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n                    self.update_som(trial, self.node_assignments[i], self.learning_rate)  # Update SOM\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            self.assign_nodes()  # Reassign nodes after population changes\n            self.adjust_parameters() # Adapt parameters\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm SOMAdaptiveDE scored 0.452 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f5b24cfd-df84-4c5a-8ae8-91480a79ad7f"], "operator": null, "metadata": {"aucs": [0.17055356453258008, 0.28073982374358053, 0.43060031219693595, 0.6852882628958044, 0.43557647858842397, 0.46731328432613317, 0.3416532692519144, 0.4156468648426844, 0.417392566335857, 0.22224030855089516, 0.5102261948632134, 0.9960734117823062, 0.2860424712307532, 0.38966437109615903, 0.8191376771323239, 0.5320707669708775, 0.3716427795421723, 0.5899397932433036, 0.1823918030783621, 0.49754163829957887]}}
{"id": "ea1fe576-a6d4-4195-99ff-3101431ae7ca", "fitness": 0.2759899899248714, "name": "OrthogonalAdaptiveDE", "description": "Adaptive Differential Evolution with orthogonal learning and a decaying exploration rate to balance exploration and exploitation.", "code": "import numpy as np\n\nclass OrthogonalAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, CR=0.7, F=0.5, learning_rate=0.1, exploration_rate=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.F = F\n        self.learning_rate = learning_rate\n        self.exploration_rate = exploration_rate\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n\n        # Orthogonal Learning Matrix\n        self.orthogonal_matrix = self.create_orthogonal_matrix(dim)\n\n    def create_orthogonal_matrix(self, dim):\n        # A simple Hadamard matrix construction for power-of-2 dimensions.\n        # Can be extended to other dimensions using more sophisticated methods.\n        n = 2**int(np.ceil(np.log2(dim)))\n        H = np.ones((1, 1))\n        while H.shape[0] < n:\n            H = np.vstack((np.hstack((H, H)), np.hstack((H, -H))))\n        return H[:dim, :dim]\n\n\n    def __call__(self, func):\n        # Initialization\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n        \n        # Scaling factor decay\n        decay_rate = 0.999\n\n        # Optimization loop\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                if np.random.rand() < self.exploration_rate:\n                    # Exploration: Orthogonal Learning\n                    basis_vector_indices = np.random.choice(self.dim, size=min(self.dim, 5), replace=False)  # Selecting a subset of basis vectors\n                    orthogonal_vector = np.zeros(self.dim)\n\n                    for j in basis_vector_indices:\n                         orthogonal_vector[j] = np.sum(self.population[:, j]) / self.pop_size  # Simplified mean projection\n\n                    mutant = self.population[i] + self.F * orthogonal_vector\n                else:\n                    # Exploitation: Standard DE mutation (DE/rand/1)\n                    indices = np.random.choice(self.pop_size, 5, replace=False)\n                    x_r1, x_r2, x_r3, x_r4, x_r5 = self.population[indices]\n                    mutant = x_r1 + self.F * (x_r2 - x_r3) + self.F * (x_r4 - x_r5)\n                \n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR:\n                        trial[j] = mutant[j]\n\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.best_index = np.argmin(self.fitness)\n\n                if self.evals >= self.budget:\n                    break\n            \n            #Adaptive Exploration Rate\n            self.exploration_rate *= decay_rate\n            self.exploration_rate = max(0.01, self.exploration_rate)\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm OrthogonalAdaptiveDE scored 0.276 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d7b57c88-4ff4-46be-954c-c17ce1595a75"], "operator": null, "metadata": {"aucs": [0.11631527066111269, 0.18930778999946007, 0.2764551588783968, 0.19661012596489913, 0.2295748020771885, 0.22604040284663962, 0.22719904402244107, 0.1980022657680559, 0.22154518911492438, 0.16374802610354766, 0.27369022216653527, 0.7925932275341432, 0.21967337474172577, 0.23200691636066817, 0.5799315368113303, 0.280061470337297, 0.2112850397647804, 0.269815586574373, 0.1603127206991638, 0.455631628070747]}}
{"id": "cb3e26e7-2bf3-46e6-b2b5-e53d8a574597", "fitness": 0.0, "name": "SelfOrganizingDE", "description": "A self-organizing differential evolution algorithm with dynamic population sizing and adaptive parameter control based on fitness landscape analysis.", "code": "import numpy as np\n\nclass SelfOrganizingDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, min_pop_size=10, max_pop_size=100,\n                 CR=0.7, F=0.5, landscape_analysis_interval=500, stagnation_threshold=1e-6):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = initial_pop_size\n        self.min_pop_size = min_pop_size\n        self.max_pop_size = max_pop_size\n        self.CR = CR\n        self.F = F\n        self.landscape_analysis_interval = landscape_analysis_interval\n        self.stagnation_threshold = stagnation_threshold\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n        self.pop_size = initial_pop_size\n        self.past_f_opt = np.Inf\n        self.stagnation_count = 0\n        self.lb = None\n        self.ub = None\n\n    def __call__(self, func):\n        self.lb = func.bounds.lb\n        self.ub = func.bounds.ub\n        # Initialization\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n        self.past_f_opt = self.f_opt\n\n\n        # Optimization loop\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation (DE/rand/1)\n                indices = np.random.choice(self.pop_size, 5, replace=False)\n                x_r1, x_r2, x_r3, x_r4, x_r5 = self.population[indices]\n                mutant = x_r1 + self.F * (x_r2 - x_r3) + self.F * (x_r4 - x_r5)\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR:\n                        trial[j] = mutant[j]\n\n                trial = np.clip(trial, self.lb, self.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.best_index = np.argmin(self.fitness)\n\n                if self.evals >= self.budget:\n                    break\n\n            # Landscape Analysis and Population Size Adjustment\n            if self.evals % self.landscape_analysis_interval == 0:\n                # Stagnation Detection\n                if abs(self.f_opt - self.past_f_opt) < self.stagnation_threshold:\n                    self.stagnation_count += 1\n                else:\n                    self.stagnation_count = 0\n\n                self.past_f_opt = self.f_opt\n\n                # Population Size Adjustment\n                if self.stagnation_count >= 3:  # Stagnation detected\n                    self.pop_size = max(self.min_pop_size, int(self.pop_size * 0.75))  # Reduce population\n                    self.stagnation_count = 0  # Reset stagnation counter\n                elif np.std(self.fitness) < self.stagnation_threshold: # Low diversity\n                    self.pop_size = min(self.max_pop_size, int(self.pop_size * 1.25))  # Increase population\n                else:\n                    pass  # Maintain population size\n\n                # Repopulate if population size changed\n                if self.pop_size != self.population.shape[0]:\n                    # Keep the best individuals\n                    best_indices = np.argsort(self.fitness)[:min(self.pop_size, self.population.shape[0])]\n                    best_individuals = self.population[best_indices]\n                    \n                    self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n                    \n                    self.population[:len(best_individuals)] = best_individuals\n                    self.fitness = np.array([func(x) for x in self.population])\n                    self.evals += self.pop_size - len(best_individuals)\n                    self.best_index = np.argmin(self.fitness)\n                    self.f_opt = self.fitness[self.best_index]\n                    self.x_opt = self.population[self.best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm SelfOrganizingDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d7b57c88-4ff4-46be-954c-c17ce1595a75"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "1364e88c-47bf-47b5-a276-721959df401d", "fitness": -Infinity, "name": "CooperativeOrthogonalDE", "description": "Cooperative Differential Evolution with orthogonal learning and aging population, where solutions are replaced if they stagnate for a period, combined with an orthogonal array to guide the search.", "code": "import numpy as np\n\nclass CooperativeOrthogonalDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, stagnation_threshold=50, orthogonal_sample=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.stagnation_threshold = stagnation_threshold\n        self.orthogonal_sample = orthogonal_sample\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n        self.stagnation_counters = None  # Individual stagnation counters\n        self.archive = []\n\n    def generate_orthogonal_array(self, n, k, l):\n        \"\"\"\n        Generates an orthogonal array of strength 2.\n        n: Number of runs\n        k: Number of factors (<= (n-1)/(l-1))\n        l: Number of levels\n        \"\"\"\n        if n == 4 and k == 3 and l == 2:\n            return np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 0]])\n        elif n == 9 and k == 4 and l == 3:\n            return np.array([\n                [0, 0, 0, 0],\n                [0, 1, 1, 1],\n                [0, 2, 2, 2],\n                [1, 0, 1, 2],\n                [1, 1, 2, 0],\n                [1, 2, 0, 1],\n                [2, 0, 2, 1],\n                [2, 1, 0, 2],\n                [2, 2, 1, 0]\n            ])\n        else:\n            raise ValueError(\"Orthogonal array not implemented for these parameters.\")\n\n    def __call__(self, func):\n        # Initialization\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n        self.stagnation_counters = np.zeros(self.pop_size)\n\n        # Optimization loop\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Differential Evolution Mutation and Crossover\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                mutant = x_r1 + self.F * (x_r2 - x_r3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR:\n                        trial[j] = mutant[j]\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n                    self.stagnation_counters[i] = 0  # Reset stagnation counter\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                else:\n                    self.stagnation_counters[i] += 1\n                    \n                if self.evals >= self.budget:\n                    break\n\n            # Orthogonal Learning\n            for i in range(self.pop_size):\n                if self.evals + self.orthogonal_sample <= self.budget:\n                    # Select dimensions for orthogonal learning\n                    dimensions = np.random.choice(self.dim, min(self.dim, 4), replace=False)\n                    num_factors = len(dimensions)\n\n                    if num_factors > 0:\n                        # Generate Orthogonal Array based on number of factors\n                        if num_factors == 3 and self.orthogonal_sample >= 4:\n                            orthogonal_array = self.generate_orthogonal_array(4, 3, 2)\n                            num_runs = 4\n                            levels = 2\n                        elif num_factors <= 4 and self.orthogonal_sample >= 9:\n                            orthogonal_array = self.generate_orthogonal_array(9, 4, 3)\n                            num_runs = 9\n                            levels = 3\n                        else:\n                             continue # Skip orthogonal learning if not enough runs or factors\n\n                        best_fitness_oa = self.fitness[i]\n                        best_solution_oa = self.population[i].copy()\n                            \n                        # Iterate over orthogonal array runs\n                        for run in range(num_runs):\n                            trial_solution = self.population[i].copy()\n                            for factor_index, dimension in enumerate(dimensions):\n                                # Map level to the search space\n                                level = orthogonal_array[run][factor_index]\n                                if levels == 2:\n                                    trial_solution[dimension] = func.bounds.lb + level * (func.bounds.ub - func.bounds.lb)\n                                else: # levels == 3\n                                    trial_solution[dimension] = func.bounds.lb + level * (func.bounds.ub - func.bounds.lb) / 2\n\n                            trial_solution = np.clip(trial_solution, func.bounds.lb, func.bounds.ub)\n                            f_trial = func(trial_solution)\n                            self.evals += 1\n\n                            if f_trial < best_fitness_oa:\n                                best_fitness_oa = f_trial\n                                best_solution_oa = trial_solution\n\n                        # Replace current solution if a better one is found\n                        if best_fitness_oa < self.fitness[i]:\n                            self.fitness[i] = best_fitness_oa\n                            self.population[i] = best_solution_oa\n                            if best_fitness_oa < self.f_opt:\n                                self.f_opt = best_fitness_oa\n                                self.x_opt = best_solution_oa\n                \n            # Aging Population/Restart\n            for i in range(self.pop_size):\n                if self.stagnation_counters[i] > self.stagnation_threshold:\n                    self.population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                    self.fitness[i] = func(self.population[i])\n                    self.evals += 1\n                    self.stagnation_counters[i] = 0\n                    if self.fitness[i] < self.f_opt:\n                        self.f_opt = self.fitness[i]\n                        self.x_opt = self.population[i]\n            \n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "An exception occurred: setting an array element with a sequence..", "error": "", "parent_ids": ["f5b24cfd-df84-4c5a-8ae8-91480a79ad7f"], "operator": null, "metadata": {}}
{"id": "5a7c20a9-8403-4915-9ec5-5ba725a2b57b", "fitness": 0.28832515210431403, "name": "CMAEnsembleAdaptiveDE", "description": "Differential Evolution with a dynamic ensemble of mutation strategies, adaptive strategy selection using a softmax function based on past performance, and covariance matrix adaptation for step size control.", "code": "import numpy as np\n\nclass CMAEnsembleAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, CR=0.7, initial_F=0.5, learning_rate=0.1, cma_learning_rate=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.initial_F = initial_F\n        self.learning_rate = learning_rate\n        self.cma_learning_rate = cma_learning_rate\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n\n        # Define multiple mutation strategies\n        self.mutation_strategies = [\n            self.mutation_strategy_1,  # DE/rand/1\n            self.mutation_strategy_2,  # DE/current-to-best/1\n            self.mutation_strategy_3   # DE/best/1\n        ]\n        self.num_strategies = len(self.mutation_strategies)\n\n        # Initialize strategy preferences (logits) for softmax\n        self.strategy_logits = np.zeros(self.num_strategies)\n        self.strategy_selection_counts = np.zeros(self.num_strategies)\n        self.successful_strategies = []\n\n        # CMA-ES like step size adaptation\n        self.C = np.eye(self.dim)  # Covariance matrix\n        self.pc = np.zeros(self.dim)  # Evolution path for C\n        self.sigma = 1.0  # Overall step size\n\n        self.c_sigma = self.cma_learning_rate  # Learning rate for sigma\n        self.c_c = self.cma_learning_rate  # Learning rate for C\n        self.d_sigma = 1 + self.c_sigma\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (16 * self.dim**2)) # expectation of ||N(0,I)||\n\n\n    def mutation_strategy_1(self, population, i): # DE/rand/1\n        indices = np.random.choice(self.pop_size, 5, replace=False)\n        x_r1, x_r2, x_r3, x_r4, x_r5 = population[indices]\n        return x_r1 + self.sigma * self.initial_F * (x_r2 - x_r3) + self.sigma * self.initial_F * (x_r4 - x_r5)\n    \n    def mutation_strategy_2(self, population, i, best_index): # DE/current-to-best/1\n        indices = np.random.choice(self.pop_size, 3, replace=False)\n        x_r1, x_r2, x_r3 = population[indices]\n        return population[i] + self.sigma * self.initial_F * (population[best_index] - population[i]) + self.sigma * self.initial_F * (x_r1 - x_r2)\n\n    def mutation_strategy_3(self, population, i, best_index): # DE/best/1\n        indices = np.random.choice(self.pop_size, 3, replace=False)\n        x_r1, x_r2, x_r3 = population[indices]\n        return self.population[best_index] + self.sigma * self.initial_F * (x_r1 - x_r2)\n\n    def select_strategy(self):\n        # Softmax selection based on logits\n        probabilities = np.exp(self.strategy_logits - np.max(self.strategy_logits))\n        probabilities /= np.sum(probabilities)\n        return np.random.choice(self.num_strategies, p=probabilities)\n\n    def update_strategy_logits(self, selected_strategy, reward):\n        # Update logits based on reward (e.g., 1 for success, 0 for failure)\n        self.strategy_logits[selected_strategy] += self.learning_rate * reward\n        # Decay other logits slightly to encourage exploration\n        for i in range(self.num_strategies):\n            if i != selected_strategy:\n                self.strategy_logits[i] -= self.learning_rate * 0.01\n        self.strategy_logits -= np.max(self.strategy_logits) # stabilize\n\n    def __call__(self, func):\n        # Initialization\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n\n        # Optimization loop\n        while self.evals < self.budget:\n            sorted_indices = np.argsort(self.fitness) # for CMA update\n            old_C = self.C.copy()\n\n            for i in range(self.pop_size):\n                # Strategy Selection\n                strategy_index = self.select_strategy()\n                mutation_strategy = self.mutation_strategies[strategy_index]\n\n                # Mutation\n                if mutation_strategy == self.mutation_strategy_1:\n                    z = np.random.multivariate_normal(np.zeros(self.dim), self.C)\n                    mutant = self.population[i] + mutation_strategy(self.population, i)\n                else:\n                    z = np.random.multivariate_normal(np.zeros(self.dim), self.C)\n                    mutant = self.population[i] + mutation_strategy(self.population, i, self.best_index)\n                    \n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR:\n                        trial[j] = mutant[j]\n\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    reward = 1\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.best_index = np.argmin(self.fitness)\n                else:\n                    reward = 0\n                \n                self.update_strategy_logits(strategy_index, reward)\n\n\n                if self.evals >= self.budget:\n                    break\n\n            # CMA-ES adaptation \n            y = (self.population[sorted_indices[0]] - self.population[sorted_indices[self.pop_size//2]])/self.sigma\n            self.pc = (1 - self.c_c) * self.pc + np.sqrt(self.c_c * (2 - self.c_c)) * y\n            self.sigma = self.sigma * np.exp((self.c_sigma / self.d_sigma) * (np.linalg.norm(self.pc) / self.chiN - 1))\n            self.C = (1 - self.c_c) * self.C + self.c_c * (np.outer(self.pc, self.pc))\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm CMAEnsembleAdaptiveDE scored 0.288 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d7b57c88-4ff4-46be-954c-c17ce1595a75"], "operator": null, "metadata": {"aucs": [0.1295728938204771, 0.16394053451224389, 0.27597292819409536, 0.22059104191819467, 0.23290548339566008, 0.2357924191743327, 0.23863101997338154, 0.20311872992875846, 0.18618166969734673, 0.18027025044927725, 0.19503770168408385, 0.9999108166432357, 0.22697567423619114, 0.2523518322561549, 0.6063315757990042, 0.2923457872056102, 0.2232845325602827, 0.2825008836106867, 0.16820567325218994, 0.45258159377507323]}}
{"id": "7c13b77a-8be1-45a9-93f9-16cd73279be9", "fitness": 0.0, "name": "NeighborhoodAdaptiveMutation", "description": "Neighborhood Search with Adaptive Radius and Mutation based on the fitness landscape.", "code": "import numpy as np\n\nclass NeighborhoodAdaptiveMutation:\n    def __init__(self, budget=10000, dim=10, initial_radius=0.5, radius_decay=0.99, mutation_rate=0.1, mutation_scale=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.radius = initial_radius\n        self.radius_decay = radius_decay\n        self.mutation_rate = mutation_rate\n        self.mutation_scale = mutation_scale\n        self.x_opt = None\n        self.f_opt = np.Inf\n        self.evals = 0\n\n    def __call__(self, func):\n        # Initialization\n        x = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.x_opt = x\n        self.f_opt = func(x)\n        self.evals += 1\n\n        while self.evals < self.budget:\n            # Generate a neighbor within the radius\n            neighbor = self.x_opt + np.random.uniform(-self.radius, self.radius, size=self.dim)\n            neighbor = np.clip(neighbor, func.bounds.lb, func.bounds.ub)\n            f_neighbor = func(neighbor)\n            self.evals += 1\n\n            # Adaptive Mutation: Apply random mutation with probability mutation_rate\n            if np.random.rand() < self.mutation_rate:\n                mutation = np.random.normal(0, self.mutation_scale, size=self.dim)\n                mutated_neighbor = neighbor + mutation\n                mutated_neighbor = np.clip(mutated_neighbor, func.bounds.lb, func.bounds.ub)\n                f_mutated_neighbor = func(mutated_neighbor)\n                self.evals += 1\n                \n                if f_mutated_neighbor < f_neighbor:\n                    neighbor = mutated_neighbor\n                    f_neighbor = f_mutated_neighbor\n\n            # Accept the neighbor if it's better\n            if f_neighbor < self.f_opt:\n                self.x_opt = neighbor\n                self.f_opt = f_neighbor\n                # Increase radius if improvement is significant\n                self.radius *= 1.05  \n            else:\n                # Decrease the radius if no improvement\n                self.radius *= self.radius_decay\n\n            self.radius = max(self.radius, 1e-6)  # Ensure radius doesn't become too small\n            if self.evals >= self.budget:\n                break\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm NeighborhoodAdaptiveMutation scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["583a84f9-ea83-458a-b839-11f3b537b9a7"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "957e7288-daa2-4f3e-819b-04b9b89b862e", "fitness": -Infinity, "name": "MirroredArchiveAdaptiveDE", "description": "Adaptive Differential Evolution with a mirrored boundary handling strategy, an archive of past solutions, and a self-adaptive mutation factor based on the success rate of previous mutations.", "code": "import numpy as np\n\nclass MirroredArchiveAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, F_init=0.5, CR=0.7, success_rate_memory=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F_init\n        self.CR = CR\n        self.success_rate_memory = success_rate_memory\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n        self.archive = []\n        self.archive_fitness = []\n        self.success_history_F = []  # Store successful F values\n        self.success_history_CR = []  # Store successful CR values\n        self.mutation_successes = 0\n        self.mutation_attempts = 0\n\n    def mirrored_boundary_handling(self, x, lb, ub):\n        \"\"\"Handles boundaries by mirroring the exceeding value back into the search space.\"\"\"\n        for i in range(len(x)):\n            if x[i] < lb:\n                x[i] = lb + (lb - x[i])  # Mirror from lower bound\n            elif x[i] > ub:\n                x[i] = ub - (x[i] - ub)  # Mirror from upper bound\n        return x\n\n    def update_archive(self, x, fx):\n        \"\"\"Update the archive with new solutions.\"\"\"\n        if len(self.archive) < self.archive_size:\n            self.archive.append(x)\n            self.archive_fitness.append(fx)\n        else:\n            # Replace the worst solution in the archive\n            worst_index = np.argmax(self.archive_fitness)\n            if fx < self.archive_fitness[worst_index]:\n                self.archive[worst_index] = x\n                self.archive_fitness[worst_index] = fx\n\n    def adjust_F(self):\n        \"\"\"Adjust F based on the success rate of previous mutations.\"\"\"\n        if self.mutation_attempts > 0:\n            success_rate = self.mutation_successes / self.mutation_attempts\n            if len(self.success_history_F) > self.success_rate_memory:\n                self.success_history_F.pop(0)  # Remove oldest entry\n            self.success_history_F.append(success_rate)\n            \n            avg_success_rate = np.mean(self.success_history_F)\n            if avg_success_rate > 0.5:\n                self.F = max(0.1, self.F - 0.05)  # Decrease F if success rate is high\n            elif avg_success_rate < 0.2:\n                self.F = min(1.0, self.F + 0.05)  # Increase F if success rate is low\n        \n    def __call__(self, func):\n        # Initialization\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n        \n        # Initialize Archive with initial population\n        for i in range(self.pop_size):\n            self.update_archive(self.population[i], self.fitness[i])\n\n        # Optimization loop\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n\n                # Select a random vector from the archive with a small probability\n                if np.random.rand() < 0.1 and len(self.archive) > 0:\n                    arch_index = np.random.randint(len(self.archive))\n                    x_r1 = self.archive[arch_index]\n                \n                mutant = x_r1 + self.F * (x_r2 - x_r3)\n                mutant = self.mirrored_boundary_handling(mutant, lb, ub) # Boundary Handling\n                \n\n                # Crossover\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR:\n                        trial[j] = mutant[j]\n                trial = self.mirrored_boundary_handling(trial, lb, ub) # Boundary Handling\n\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n                self.mutation_attempts += 1\n\n                if f_trial < self.fitness[i]:\n                    self.mutation_successes += 1\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n                    self.update_archive(trial, f_trial)\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                \n                if self.evals >= self.budget:\n                    break\n            \n            self.adjust_F()\n            self.mutation_successes = 0 # Reset for next generation\n            self.mutation_attempts = 0\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "An exception occurred: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all().", "error": "", "parent_ids": ["583a84f9-ea83-458a-b839-11f3b537b9a7"], "operator": null, "metadata": {}}
{"id": "40716eca-a8cb-46b4-8b97-abd6722b413d", "fitness": -Infinity, "name": "DynamicPopulationAdaptiveDE", "description": "Differential Evolution with dynamic population size adjustment based on success rate and a self-adaptive local search operator triggered by stagnation.", "code": "import numpy as np\n\nclass DynamicPopulationAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, CR=0.7, F=0.5, pop_size_factor=0.2, stagnation_threshold=1000):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = initial_pop_size\n        self.pop_size = initial_pop_size\n        self.CR = CR\n        self.F = F\n        self.pop_size_factor = pop_size_factor\n        self.stagnation_threshold = stagnation_threshold\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n        self.success_rate = 0.5  # Initial success rate\n        self.success_history = []\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n\n    def adjust_population_size(self):\n        if len(self.success_history) > 10:\n            recent_success_rate = np.mean(self.success_history[-10:])\n            if recent_success_rate > self.success_rate:\n                self.pop_size = min(int(self.pop_size * (1 + self.pop_size_factor)), 2 * self.initial_pop_size)  # Increase population size\n                self.success_rate = recent_success_rate\n            elif recent_success_rate < self.success_rate:\n                self.pop_size = max(int(self.pop_size * (1 - self.pop_size_factor)), self.initial_pop_size // 4)  # Decrease population size\n                self.success_rate = recent_success_rate\n\n    def local_search(self, x, func, radius=0.1):\n        # Perturb each dimension with a small random value\n        x_perturbed = x + np.random.uniform(-radius, radius, size=self.dim)\n        x_perturbed = np.clip(x_perturbed, func.bounds.lb, func.bounds.ub)\n        f_perturbed = func(x_perturbed)\n        self.evals +=1 \n        if f_perturbed < self.f_opt:\n            self.f_opt = f_perturbed\n            self.x_opt = x_perturbed\n        return f_perturbed, x_perturbed\n\n    def __call__(self, func):\n        # Initialization\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.initial_pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.initial_pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n        # Optimization loop\n        while self.evals < self.budget:\n            new_population = np.zeros((self.pop_size, self.dim))\n            new_fitness = np.zeros(self.pop_size)\n            success_count = 0\n\n            for i in range(self.initial_pop_size): # changed to initial_pop_size for speed\n                # Mutation\n                indices = np.random.choice(self.initial_pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                mutant = self.population[i] + self.F * (x_r2 - x_r3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR:\n                        trial[j] = mutant[j]\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    new_fitness[i] = f_trial\n                    new_population[i] = trial\n                    success_count += 1\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                else:\n                    new_fitness[i] = self.fitness[i]\n                    new_population[i] = self.population[i]\n\n                if self.evals >= self.budget:\n                    break\n\n            self.success_history.append(success_count / self.initial_pop_size) #changed\n\n            # Stagnation check and local search\n            if abs(self.f_opt - self.best_fitness_history[-1]) < 1e-8:\n                self.stagnation_counter += 1\n                if self.stagnation_counter > self.stagnation_threshold:\n                    f_local, x_local = self.local_search(self.x_opt, func)\n                    if f_local < self.f_opt:\n                        self.f_opt = f_local\n                        self.x_opt = x_local\n                    self.stagnation_counter = 0 # Reset after triggering\n\n            else:\n                self.stagnation_counter = 0\n\n            self.best_fitness_history.append(self.f_opt)\n\n\n            self.population[:self.initial_pop_size] = new_population[:self.initial_pop_size]\n            self.fitness[:self.initial_pop_size] = new_fitness[:self.initial_pop_size]\n\n            # Population adjustment\n            self.adjust_population_size()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "An exception occurred: index 40 is out of bounds for axis 0 with size 40.", "error": "", "parent_ids": ["5a7c20a9-8403-4915-9ec5-5ba725a2b57b"], "operator": null, "metadata": {}}
{"id": "4e6b3f34-a5e4-49c4-958a-13acfccbf806", "fitness": 0.2943436287099878, "name": "VelocityCauchyDE", "description": "Differential Evolution with a velocity-based mutation and a Cauchy mutation operator to promote exploration and exploitation.", "code": "import numpy as np\n\nclass VelocityCauchyDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, CR=0.7, F=0.5, velocity_decay=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.F = F\n        self.velocity_decay = velocity_decay\n        self.population = None\n        self.fitness = None\n        self.best_fitness = np.inf\n        self.best_solution = None\n        self.evaluations = 0\n        self.velocity = None\n\n    def __call__(self, func):\n        # Initialization\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evaluations = self.pop_size\n\n        best_index = np.argmin(self.fitness)\n        self.best_fitness = self.fitness[best_index]\n        self.best_solution = self.population[best_index].copy()\n\n        self.velocity = np.zeros_like(self.population)\n\n        # Optimization loop\n        while self.evaluations < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n\n                # Velocity update\n                self.velocity[i] = self.velocity_decay * self.velocity[i] + self.F * (x_r1 - self.population[i]) + self.F * (x_r2 - x_r3)\n                mutant = self.population[i] + self.velocity[i]\n\n                # Cauchy mutation to promote exploration\n                if np.random.rand() < 0.1:\n                    mutant += 0.01 * np.random.standard_cauchy(size=self.dim)\n\n                mutant = np.clip(mutant, lb, ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR:\n                        trial[j] = mutant[j]\n                trial = np.clip(trial, lb, ub)\n\n                # Evaluation\n                f_trial = func(trial)\n                self.evaluations += 1\n\n                # Selection\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    # Update best solution\n                    if f_trial < self.best_fitness:\n                        self.best_fitness = f_trial\n                        self.best_solution = trial.copy()\n\n        return self.best_fitness, self.best_solution", "configspace": "", "generation": 8, "feedback": "The algorithm VelocityCauchyDE scored 0.294 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6feef66a-872b-4a1e-9b90-6d56cb0ab622"], "operator": null, "metadata": {"aucs": [0.1247863391337698, 0.18563541750544887, 0.2812229119899593, 0.2295045080610868, 0.21421942141317807, 0.24508919611932978, 0.2389441584090054, 0.21437740670531413, 0.1943341281358283, 0.16099825700835024, 0.2227720568062298, 0.9993057641622932, 0.2402236064698443, 0.24074088617994405, 0.635795184357844, 0.3023500544173926, 0.23433096563162803, 0.28455979897262984, 0.18111222307561903, 0.45657028964506086]}}
{"id": "77af5716-74b2-439d-af42-aa207da06976", "fitness": 0.503311290765336, "name": "CooperativeSwarmAdaptivePareto", "description": "Cooperative Swarm Optimization with Adaptive Pareto Frontier Exploration, employing multiple swarms that share information through a dynamically updated Pareto frontier to balance exploration and exploitation.", "code": "import numpy as np\n\nclass CooperativeSwarmAdaptivePareto:\n    def __init__(self, budget=10000, dim=10, num_swarms=5, swarm_size=20, inertia=0.7, cognitive_coeff=1.4, social_coeff=1.4, pareto_size=20, learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = num_swarms\n        self.swarm_size = swarm_size\n        self.inertia = inertia\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.pareto_size = pareto_size\n        self.learning_rate = learning_rate\n        self.swarms = []\n        self.velocities = []\n        self.personal_best_positions = []\n        self.personal_best_fitness = []\n        self.global_best_position = None\n        self.global_best_fitness = np.Inf\n        self.evals = 0\n        self.pareto_frontier = []  # Store solutions in the Pareto frontier\n\n    def initialize_swarms(self, func):\n        \"\"\"Initialize multiple swarms.\"\"\"\n        for _ in range(self.num_swarms):\n            swarm = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.swarm_size, self.dim))\n            velocity = np.random.uniform(-1, 1, size=(self.swarm_size, self.dim))\n            self.swarms.append(swarm)\n            self.velocities.append(velocity)\n\n            fitness = np.array([func(x) for x in swarm])\n            self.evals += self.swarm_size\n            self.personal_best_positions.append(np.copy(swarm))\n            self.personal_best_fitness.append(fitness)\n\n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.global_best_fitness:\n                self.global_best_fitness = fitness[best_index]\n                self.global_best_position = np.copy(swarm[best_index])\n\n    def update_pareto_frontier(self, position, fitness):\n        \"\"\"Update the Pareto frontier with a new solution.\"\"\"\n        # Add the new solution to the Pareto frontier\n        self.pareto_frontier.append((position, fitness))\n\n        # Remove dominated solutions\n        new_pareto = []\n        for sol, fit in self.pareto_frontier:\n            is_dominated = False\n            for other_sol, other_fit in self.pareto_frontier:\n                if fit > other_fit:  # Assuming minimization\n                    is_dominated = True\n                    break\n            if not is_dominated:\n                new_pareto.append((sol, fit))\n\n        self.pareto_frontier = new_pareto\n\n        # Limit the size of the Pareto frontier\n        if len(self.pareto_frontier) > self.pareto_size:\n            # Remove the worst solution based on crowding distance (or a random one)\n            self.pareto_frontier.sort(key=lambda x: x[1]) # Sort by fitness\n            self.pareto_frontier.pop()\n\n\n    def __call__(self, func):\n        # Initialization\n        self.initialize_swarms(func)\n        f_opt = self.global_best_fitness\n        x_opt = self.global_best_position\n\n        # Optimization loop\n        while self.evals < self.budget:\n            for swarm_idx in range(self.num_swarms):\n                swarm = self.swarms[swarm_idx]\n                velocity = self.velocities[swarm_idx]\n                personal_best_position = self.personal_best_positions[swarm_idx]\n                personal_best_fitness = self.personal_best_fitness[swarm_idx]\n\n                for i in range(self.swarm_size):\n                    # Update velocity\n                    r1 = np.random.rand(self.dim)\n                    r2 = np.random.rand(self.dim)\n\n                    # Select a random solution from the Pareto frontier\n                    if self.pareto_frontier:\n                        pareto_solution, pareto_fitness = self.pareto_frontier[np.random.randint(len(self.pareto_frontier))]\n                    else:\n                        pareto_solution = self.global_best_position # Fallback\n\n                    velocity[i] = self.inertia * velocity[i] + \\\n                                  self.cognitive_coeff * r1 * (personal_best_position[i] - swarm[i]) + \\\n                                  self.social_coeff * r2 * (pareto_solution - swarm[i]) # Attract to pareto solution\n\n                    # Update position\n                    swarm[i] = swarm[i] + velocity[i]\n                    swarm[i] = np.clip(swarm[i], func.bounds.lb, func.bounds.ub)\n\n                    # Evaluate fitness\n                    fitness = func(swarm[i])\n                    self.evals += 1\n\n                    # Update personal best\n                    if fitness < personal_best_fitness[i]:\n                        personal_best_fitness[i] = fitness\n                        personal_best_position[i] = np.copy(swarm[i])\n\n                    # Update global best\n                    if fitness < self.global_best_fitness:\n                        self.global_best_fitness = fitness\n                        self.global_best_position = np.copy(swarm[i])\n                        f_opt = self.global_best_fitness\n                        x_opt = self.global_best_position\n\n                    # Update Pareto frontier\n                    self.update_pareto_frontier(np.copy(swarm[i]), fitness)\n\n\n                self.swarms[swarm_idx] = swarm\n                self.velocities[swarm_idx] = velocity\n                self.personal_best_positions[swarm_idx] = personal_best_position\n                self.personal_best_fitness[swarm_idx] = personal_best_fitness\n\n                if self.evals >= self.budget:\n                    break\n\n        return f_opt, x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm CooperativeSwarmAdaptivePareto scored 0.503 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["583a84f9-ea83-458a-b839-11f3b537b9a7"], "operator": null, "metadata": {"aucs": [0.19603127447580082, 0.28915324323228364, 0.5295812953133228, 0.8506989763504225, 0.2717476293605854, 0.6687829697681669, 0.35880141972236756, 0.5150091029579681, 0.6073335985471123, 0.19263075800842377, 0.7752519157861948, 0.9913156543636652, 0.24031632465240216, 0.2909089631162348, 0.7332616921026234, 0.6557905412349379, 0.38355917660350025, 0.7717532346215414, 0.22515374079891393, 0.5191443042902515]}}
{"id": "a183b0bb-956a-4d60-a3ef-4f5f68b26bbf", "fitness": 0.3373627404979612, "name": "AdaptiveCauchyGaussianDE", "description": "A Differential Evolution variant that uses a combination of Cauchy and Gaussian mutations, adaptively choosing between them based on success rate, and includes a restart mechanism when stagnation is detected.", "code": "import numpy as np\n\nclass AdaptiveCauchyGaussianDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, CR=0.7, F=0.5, cauchy_prob=0.5, learning_rate=0.1, stagnation_threshold=1000):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.F = F\n        self.cauchy_prob = cauchy_prob\n        self.learning_rate = learning_rate\n        self.stagnation_threshold = stagnation_threshold\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n        self.stagnation_counter = 0\n        self.last_improvement = 0\n\n        # Cauchy mutation parameter\n        self.cauchy_scale = 0.1\n\n        # Success counters for Cauchy and Gaussian mutations\n        self.cauchy_success = 0\n        self.gaussian_success = 0\n        self.total_cauchy = 0\n        self.total_gaussian = 0\n\n\n    def mutate_cauchy(self, population, i):\n        indices = np.random.choice(self.pop_size, 3, replace=False)\n        x_r1, x_r2, x_r3 = population[indices]\n        return population[i] + self.F * (x_r1 - x_r2) + np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n\n    def mutate_gaussian(self, population, i):\n        indices = np.random.choice(self.pop_size, 3, replace=False)\n        x_r1, x_r2, x_r3 = population[indices]\n        return population[i] + self.F * (x_r1 - x_r2) + np.random.normal(0, 0.1, size=self.dim)\n\n\n    def __call__(self, func):\n        # Initialization\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n        self.last_improvement = 0\n        self.stagnation_counter = 0\n\n\n        # Optimization loop\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Choose mutation type adaptively\n                if np.random.rand() < self.cauchy_prob:\n                    mutant = self.mutate_cauchy(self.population, i)\n                    self.total_cauchy += 1\n                else:\n                    mutant = self.mutate_gaussian(self.population, i)\n                    self.total_gaussian += 1\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR:\n                        trial[j] = mutant[j]\n\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.best_index = np.argmin(self.fitness)\n                        self.last_improvement = self.evals\n\n                    if np.random.rand() < self.cauchy_prob:\n                        self.cauchy_success += 1\n                    else:\n                        self.gaussian_success += 1\n\n                if self.evals >= self.budget:\n                    break\n            \n            # Update Cauchy probability based on success\n            if self.total_cauchy > 0 and self.total_gaussian > 0:\n              success_rate_cauchy = self.cauchy_success / self.total_cauchy\n              success_rate_gaussian = self.gaussian_success / self.total_gaussian\n              self.cauchy_prob = 0.5 + self.learning_rate * (success_rate_cauchy - success_rate_gaussian)\n              self.cauchy_prob = np.clip(self.cauchy_prob, 0.1, 0.9)\n\n            # Stagnation check and restart\n            if self.evals - self.last_improvement > self.stagnation_threshold:\n                self.stagnation_counter += 1\n                if self.stagnation_counter > 3:\n                  # Restart the population\n                  self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                  self.fitness = np.array([func(x) for x in self.population])\n                  self.evals += self.pop_size\n                  self.best_index = np.argmin(self.fitness)\n                  self.f_opt = self.fitness[self.best_index]\n                  self.x_opt = self.population[self.best_index]\n                  self.last_improvement = self.evals\n                  self.stagnation_counter = 0\n                  self.cauchy_success = 0\n                  self.gaussian_success = 0\n                  self.total_cauchy = 0\n                  self.total_gaussian = 0\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveCauchyGaussianDE scored 0.337 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5a7c20a9-8403-4915-9ec5-5ba725a2b57b"], "operator": null, "metadata": {"aucs": [0.13026235779601025, 0.2119866106409488, 0.3602810362943447, 0.3925228977139448, 0.2651805027867784, 0.3707617032729379, 0.29915105591627966, 0.2924489244678229, 0.292490439862358, 0.18614001577254868, 0.3519503196132281, 0.9875900748829284, 0.27849032047013356, 0.2731427305179691, 0.7054048579591463, 0]}}
{"id": "119d60e1-0652-4142-b811-48fde8480d84", "fitness": 0.5957146855499751, "name": "DynamicPopulationAdaptiveDE", "description": "Adaptive Differential Evolution with dynamic population sizing, where the population size adapts based on the fitness improvement rate and a neighborhood-based mutation to promote local search.", "code": "import numpy as np\n\nclass DynamicPopulationAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, min_pop_size=10, max_pop_size=100, CR=0.7, F=0.5, learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = initial_pop_size\n        self.min_pop_size = min_pop_size\n        self.max_pop_size = max_pop_size\n        self.pop_size = initial_pop_size\n        self.CR = CR\n        self.F = F\n        self.learning_rate = learning_rate\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n        self.fitness_history = []\n\n    def __call__(self, func):\n        # Initialization\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n        self.fitness_history.append(self.f_opt)\n\n        # Optimization loop\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation: Neighborhood-based mutation\n                neighbors = np.random.choice(self.pop_size, 4, replace=False) # Choose 4 neighbors\n                x_r1, x_r2, x_r3, x_r4 = self.population[neighbors]\n                mutant = self.population[i] + self.F * (x_r1 - x_r2) + self.F * (x_r3 - x_r4)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR:\n                        trial[j] = mutant[j]\n\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.best_index = np.argmin(self.fitness)\n\n                if self.evals >= self.budget:\n                    break\n            \n            # Dynamic population sizing\n            improvement_rate = 0\n            if len(self.fitness_history) > 1:\n                improvement_rate = (self.fitness_history[-1] - self.f_opt) / abs(self.fitness_history[-1]) if self.fitness_history[-1] != 0 else 0\n\n            self.fitness_history.append(self.f_opt)\n\n            if improvement_rate > 0.01:  # Significant improvement\n                self.pop_size = min(self.pop_size + 1, self.max_pop_size)\n                # Add a new individual (randomly initialized)\n                new_individual = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                self.population = np.vstack((self.population, new_individual))\n                self.fitness = np.append(self.fitness, func(new_individual))\n                self.evals += 1\n            elif improvement_rate < 0.001 and self.pop_size > self.min_pop_size:  # Stagnation\n                self.pop_size = max(self.pop_size - 1, self.min_pop_size)\n                # Remove the worst individual\n                worst_index = np.argmax(self.fitness)\n                self.population = np.delete(self.population, worst_index, axis=0)\n                self.fitness = np.delete(self.fitness, worst_index)\n            \n            self.best_index = np.argmin(self.fitness)\n            self.f_opt = self.fitness[self.best_index]\n            self.x_opt = self.population[self.best_index]\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm DynamicPopulationAdaptiveDE scored 0.596 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ea1fe576-a6d4-4195-99ff-3101431ae7ca"], "operator": null, "metadata": {"aucs": [0.1665672214569438, 0.3565182403360606, 0.6718547858343398, 0.8258053151622429, 0.727402902989457, 0.757738867364282, 0.33452226352938064, 0.642566464391052, 0.7741114980945062, 0.20188392085989904, 0.8406317805237181, 0.9983043954222794, 0.45279397778937647, 0.4980338161476272, 0.8619859393537328, 0.7699980109501674, 0.5062302874341873, 0.8123985308316599, 0.20502736193357118, 0.5099181305950204]}}
{"id": "5abc35d1-460a-474d-af04-f0528016534e", "fitness": 0.35755218663838073, "name": "MemoryAdaptiveDE", "description": "Differential Evolution with a crow search inspired memory and a dynamic adaptation of the scaling factor F based on the success rate of previous iterations.", "code": "import numpy as np\n\nclass MemoryAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, CR=0.7, memory_size=10, initial_F=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.memory_size = memory_size\n        self.memory = None\n        self.initial_F = initial_F\n        self.F = initial_F\n        self.population = None\n        self.fitness = None\n        self.best_fitness = np.inf\n        self.best_solution = None\n        self.evaluations = 0\n        self.success_history = []\n\n    def __call__(self, func):\n        # Initialization\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evaluations = self.pop_size\n\n        best_index = np.argmin(self.fitness)\n        self.best_fitness = self.fitness[best_index]\n        self.best_solution = self.population[best_index].copy()\n\n        self.memory = np.copy(self.population)  # Initialize memory with current population\n\n        # Optimization loop\n        while self.evaluations < self.budget:\n            successful_mutations = 0\n            for i in range(self.pop_size):\n                # Mutation: Crow Search inspired memory usage\n                crow_index = np.random.randint(0, self.pop_size)\n                awareness_prob = np.random.rand()\n                if awareness_prob > 0.1:  # Crow is unaware, follows memory\n                    x_r1 = self.memory[crow_index]  # Follow crow's memory\n                else:\n                    x_r1 = self.population[np.random.choice(self.pop_size)] # Crow is aware, random search\n\n                indices = np.random.choice(self.pop_size, 2, replace=False)\n                x_r2, x_r3 = self.population[indices]\n                mutant = self.population[i] + self.F * (x_r1 - x_r2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR:\n                        trial[j] = mutant[j]\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Evaluation\n                f_trial = func(trial)\n                self.evaluations += 1\n\n                # Selection\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n                    self.memory[i] = trial  # Update memory\n                    successful_mutations += 1\n\n                    # Update best solution\n                    if f_trial < self.best_fitness:\n                        self.best_fitness = f_trial\n                        self.best_solution = trial.copy()\n                else:\n                    # If trial fails, crow updates its location\n                    if np.random.rand() < 0.1:\n                        self.memory[i] = np.random.uniform(func.bounds.lb, func.bounds.ub)\n\n            # Adaptive F based on success rate\n            success_rate = successful_mutations / self.pop_size if self.pop_size > 0 else 0.0\n            self.success_history.append(success_rate)\n            if len(self.success_history) > self.memory_size:\n                self.success_history.pop(0)\n\n            avg_success = np.mean(self.success_history)\n            self.F = np.clip(self.initial_F * (1 + avg_success), 0.1, 1.0) # Adapt scaling factor F\n\n        return self.best_fitness, self.best_solution", "configspace": "", "generation": 8, "feedback": "The algorithm MemoryAdaptiveDE scored 0.358 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6feef66a-872b-4a1e-9b90-6d56cb0ab622"], "operator": null, "metadata": {"aucs": [0.15083007993998343, 0.220782358798008, 0.35445900361048877, 0.3651443041314357, 0.26755640812786785, 0.3720950843609677, 0.2773420626239027, 0.30116908358401606, 0.27266701203525145, 0.18143594570261223, 0.38104912804556346, 0.9961855147244557, 0.2861160470211892, 0.27716784457528365, 0.6729852057798693, 0.37182202640791695, 0.29933296071858084, 0.433280325287579, 0.18799636648965456, 0.48162697080298766]}}
{"id": "fbeb7fc8-a7b3-4f6f-97bd-f8b9a83611e3", "fitness": 0.34372021426205374, "name": "AdaptiveCauchyShrinkingDE", "description": "Adaptive Differential Evolution with a Cauchy mutation operator, a shrinking population, and an aging mechanism to encourage exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveCauchyShrinkingDE:\n    def __init__(self, budget=10000, dim=10, pop_size=100, CR=0.7, F=0.5, shrink_factor=0.95, age_limit=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.F = F\n        self.shrink_factor = shrink_factor\n        self.age_limit = age_limit\n        self.population = None\n        self.fitness = None\n        self.ages = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n\n    def __call__(self, func):\n        # Initialization\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.ages = np.zeros(self.pop_size, dtype=int)\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n\n        # Optimization loop\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation using Cauchy distribution\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                \n                # Cauchy mutation\n                mutant = self.population[i] + self.F * (x_r1 - x_r2) + np.random.standard_cauchy(size=self.dim) * 0.01  # Adding Cauchy noise\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR:\n                        trial[j] = mutant[j]\n\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n                    self.ages[i] = 0  # Reset age\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.best_index = np.argmin(self.fitness)\n                else:\n                    self.ages[i] += 1 # Increment age if not improving\n\n                if self.evals >= self.budget:\n                    break\n            \n            # Aging and population shrinking\n            if np.max(self.ages) > self.age_limit:\n                # Remove the oldest individual\n                worst_index = np.argmax(self.ages)\n                \n                # Replace with a random individual\n                self.population[worst_index] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                self.fitness[worst_index] = func(self.population[worst_index])\n                self.ages[worst_index] = 0\n                self.evals += 1\n\n                # Shrink population size\n                self.pop_size = int(self.pop_size * self.shrink_factor)\n                if self.pop_size < 10:\n                    self.pop_size = 10 # Ensure minimum population size\n                \n                # Resize population and fitness arrays\n                self.population = self.population[np.argsort(self.fitness)[:self.pop_size]]\n                self.fitness = self.fitness[np.argsort(self.fitness)[:self.pop_size]]\n                self.ages = self.ages[np.argsort(self.fitness)[:self.pop_size]]\n                \n                self.best_index = np.argmin(self.fitness)\n                self.f_opt = self.fitness[self.best_index]\n                self.x_opt = self.population[self.best_index]\n                    \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveCauchyShrinkingDE scored 0.344 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5a7c20a9-8403-4915-9ec5-5ba725a2b57b"], "operator": null, "metadata": {"aucs": [0.143241314282317, 0.23378620856812493, 0.33559287991410325, 0.31962279489858225, 0.26445638990988907, 0.3326421063492707, 0.2775997653211353, 0.2888520987409928, 0.2556059008960052, 0.18980035411565255, 0.325598154214789, 0.9925523731356962, 0.29756983993338004, 0.2786750638021632, 0.6828378996757001, 0.3419088299905644, 0.2791568312620454, 0.38262273264293134, 0.1770048247074243, 0.47527792288030635]}}
{"id": "dc106085-54f5-4fbc-958a-b971bed89cb0", "fitness": 0.6094643243509139, "name": "RepulsiveEntropyDE", "description": "Differential Evolution with a repulsive force from the worst-performing individuals, and a population entropy-based diversity maintenance strategy.", "code": "import numpy as np\n\nclass RepulsiveEntropyDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, CR=0.7, F=0.5, repulsion_strength=0.1, entropy_threshold=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.F = F\n        self.repulsion_strength = repulsion_strength\n        self.entropy_threshold = entropy_threshold\n        self.population = None\n        self.fitness = None\n        self.best_fitness = np.inf\n        self.best_solution = None\n        self.evaluations = 0\n\n    def __call__(self, func):\n        # Initialization\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evaluations = self.pop_size\n\n        best_index = np.argmin(self.fitness)\n        self.best_fitness = self.fitness[best_index]\n        self.best_solution = self.population[best_index].copy()\n\n        # Optimization loop\n        while self.evaluations < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                mutant = x_r1 + self.F * (x_r2 - x_r3)\n\n                # Repulsion from worst individuals\n                worst_index = np.argmax(self.fitness)\n                repulsion_vector = self.population[i] - self.population[worst_index]\n                mutant = mutant + self.repulsion_strength * repulsion_vector\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR:\n                        trial[j] = mutant[j]\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Evaluation\n                f_trial = func(trial)\n                self.evaluations += 1\n\n                # Selection\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    # Update best solution\n                    if f_trial < self.best_fitness:\n                        self.best_fitness = f_trial\n                        self.best_solution = trial.copy()\n\n            # Population entropy-based diversity maintenance\n            entropy = self.calculate_population_entropy()\n            if entropy < self.entropy_threshold:\n                # Reset a portion of the population to random positions\n                num_to_reset = int(self.pop_size * 0.2)\n                indices_to_reset = np.random.choice(self.pop_size, num_to_reset, replace=False)\n                self.population[indices_to_reset] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(num_to_reset, self.dim))\n                self.fitness[indices_to_reset] = np.array([func(x) for x in self.population[indices_to_reset]])\n                self.evaluations += num_to_reset\n                \n                best_index = np.argmin(self.fitness)\n                self.best_fitness = self.fitness[best_index]\n                self.best_solution = self.population[best_index].copy()\n\n        return self.best_fitness, self.best_solution\n\n    def calculate_population_entropy(self):\n        # Discretize the population into bins\n        num_bins = 10\n        histogram, _ = np.histogramdd(self.population, bins=num_bins, range=[(0,1)]*self.dim)  # Normalize to 0-1 range\n\n        # Normalize the histogram to get probabilities\n        probabilities = histogram / np.sum(histogram)\n\n        # Calculate entropy\n        entropy = -np.sum(probabilities * np.log(probabilities + 1e-10))  # Add a small constant to avoid log(0)\n\n        # Normalize entropy to the range [0, 1]\n        max_entropy = np.log(num_bins**self.dim)\n        normalized_entropy = entropy / max_entropy\n\n        return normalized_entropy", "configspace": "", "generation": 8, "feedback": "The algorithm RepulsiveEntropyDE scored 0.609 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6feef66a-872b-4a1e-9b90-6d56cb0ab622"], "operator": null, "metadata": {"aucs": [0.21214716679968726, 0.3434763905886472, 0.572590322125179, 0.7903028307890942, 0.6985747287933988, 0.7437761795293772, 0.5881695770728042, 0.554850322744264, 0.6946523448066679, 0.5312138444027938, 0.8072248077223302, 0.9997508482063828, 0.30538982987185503, 0.609532842708436, 0.8875236046719323, 0.7527746979504886, 0.47730697074221384, 0.8140037685118446, 0.2749976329860351, 0.5310277759948476]}}
{"id": "1bb850ca-8c22-4081-9898-ad4b0e7e9864", "fitness": 0.6089449782396998, "name": "ArchivedAdaptiveDE", "description": "Differential Evolution with a diversity archive, adaptive exploration rate based on archive fitness, and a local search operator triggered by stagnation.", "code": "import numpy as np\n\nclass ArchivedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=25, CR=0.7, F=0.5, learning_rate=0.1, exploration_rate=1.0, stagnation_threshold=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.CR = CR\n        self.F = F\n        self.learning_rate = learning_rate\n        self.exploration_rate = exploration_rate\n        self.stagnation_threshold = stagnation_threshold\n        self.population = None\n        self.fitness = None\n        self.archive = None\n        self.archive_fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n        self.stagnation_counter = 0\n\n    def __call__(self, func):\n        # Initialization\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index]\n\n        # Initialize Archive\n        self.archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        self.archive_fitness = np.array([func(x) for x in self.archive])\n        self.evals += self.archive_size\n\n        # Scaling factor decay\n        decay_rate = 0.999\n\n        # Optimization loop\n        while self.evals < self.budget:\n            f_opt_prev = self.f_opt  # Store previous best fitness\n\n            for i in range(self.pop_size):\n                # Mutation\n                if np.random.rand() < self.exploration_rate:\n                    # Exploration: Mutate using a random archive member\n                    if self.archive.shape[0] > 0: #check archive not empty\n                        archive_index = np.random.randint(self.archive.shape[0])\n                        mutant = self.population[i] + self.F * (self.archive[archive_index] - self.population[i])\n                    else: #fall back to standard DE if archive is empty\n                        indices = np.random.choice(self.pop_size, 3, replace=False)\n                        x_r1, x_r2, x_r3 = self.population[indices]\n                        mutant = x_r1 + self.F * (x_r2 - x_r3)\n                        \n                else:\n                    # Exploitation: Standard DE mutation (DE/rand/1)\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutant = x_r1 + self.F * (x_r2 - x_r3)\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR:\n                        trial[j] = mutant[j]\n\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.best_index = np.argmin(self.fitness)\n\n                if self.evals >= self.budget:\n                    break\n            \n            # Archive Update: Replace worst archive member with a diverse solution\n            if self.archive.shape[0] > 0: #archive not empty\n                worst_archive_index = np.argmax(self.archive_fitness)\n                \n                #Pick a random solution from the population\n                rand_index = np.random.randint(self.pop_size)\n\n                self.archive[worst_archive_index] = self.population[rand_index]\n                self.archive_fitness[worst_archive_index] = self.fitness[rand_index]\n                \n            #Adaptive Exploration Rate\n            if self.archive.shape[0] > 0:\n                 archive_fitness_mean = np.mean(self.archive_fitness)\n                 self.exploration_rate = 1.0 / (1.0 + np.exp(archive_fitness_mean - self.f_opt)) # Adjust exploration based on the archive fitness\n            else:\n                 self.exploration_rate *= decay_rate\n                 self.exploration_rate = max(0.01, self.exploration_rate)\n\n            # Stagnation Check and Local Search\n            if self.f_opt == f_opt_prev:\n                self.stagnation_counter += 1\n                if self.stagnation_counter > self.stagnation_threshold:\n                    # Local Search around the best solution\n                    self.local_search(func)\n                    self.stagnation_counter = 0\n            else:\n                self.stagnation_counter = 0\n\n        return self.f_opt, self.x_opt\n\n    def local_search(self, func, num_iterations=10):\n        # Perturb the current best solution with small random steps\n        for _ in range(num_iterations):\n            perturbation = np.random.normal(0, 0.05, self.dim)  # Small random perturbation\n            new_x = self.x_opt + perturbation\n            new_x = np.clip(new_x, func.bounds.lb, func.bounds.ub)\n            new_f = func(new_x)\n            self.evals += 1\n\n            if new_f < self.f_opt:\n                self.f_opt = new_f\n                self.x_opt = new_x\n                self.best_index = np.argmin(self.fitness) #update best index (necessary?)", "configspace": "", "generation": 8, "feedback": "The algorithm ArchivedAdaptiveDE scored 0.609 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ea1fe576-a6d4-4195-99ff-3101431ae7ca"], "operator": null, "metadata": {"aucs": [0.22967605813820902, 0.536239583738593, 0.5450903878099322, 0.8157925105935655, 0.685316682228417, 0.7348382593936766, 0.5967276044531182, 0.5245726465017945, 0.6623827062313477, 0.5939906343167951, 0.7922636218901109, 0.9928785538604459, 0.3186044689463885, 0.6380849281469476, 0.8442213493134987, 0.6978869480108341, 0.4922457611315445, 0.768260819265651, 0.21268697008789472, 0.497139070735234]}}
{"id": "ca5ea78f-2f18-4ca0-bde3-6f5ce3f34fdf", "fitness": 0.24434871348338533, "name": "RepulsiveLocalSearchDE", "description": "Adaptive Differential Evolution with a repulsion mechanism to increase diversity and a local search operator to enhance exploitation.", "code": "import numpy as np\n\nclass RepulsiveLocalSearchDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, CR=0.7, F=0.5, repulsion_factor=0.1, local_search_probability=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.F = F\n        self.repulsion_factor = repulsion_factor\n        self.local_search_probability = local_search_probability\n        self.population = None\n        self.fitness = None\n        self.best_fitness = np.inf\n        self.best_solution = None\n        self.evaluations = 0\n\n    def __call__(self, func):\n        # Initialization\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evaluations = self.pop_size\n\n        best_index = np.argmin(self.fitness)\n        self.best_fitness = self.fitness[best_index]\n        self.best_solution = self.population[best_index].copy()\n\n        # Optimization loop\n        while self.evaluations < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                mutant = x_r1 + self.F * (x_r2 - x_r3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR:\n                        trial[j] = mutant[j]\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Repulsion Mechanism\n                repulsion_vector = np.zeros(self.dim)\n                for j in range(self.pop_size):\n                    if i != j:\n                        repulsion_vector += (self.population[i] - self.population[j])\n\n                trial = trial - self.repulsion_factor * repulsion_vector\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Local Search\n                if np.random.rand() < self.local_search_probability:\n                    # Perform a small random step around the trial vector\n                    local_step = np.random.uniform(-0.01, 0.01, size=self.dim)\n                    trial = trial + local_step\n                    trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Evaluation\n                f_trial = func(trial)\n                self.evaluations += 1\n\n                # Selection\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    # Update best solution\n                    if f_trial < self.best_fitness:\n                        self.best_fitness = f_trial\n                        self.best_solution = trial.copy()\n\n        return self.best_fitness, self.best_solution", "configspace": "", "generation": 8, "feedback": "The algorithm RepulsiveLocalSearchDE scored 0.244 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6feef66a-872b-4a1e-9b90-6d56cb0ab622"], "operator": null, "metadata": {"aucs": [0.06643817036625355, 0.1298163404101066, 0.22660864649450774, 0.19474907251493856, 0.13961495646731015, 0.1958310716447752, 0.1871848425885395, 0.19456737824343406, 0.15882761068432882, 0.1286678681477733, 0.17970202774386201, 0.9990456909475496, 0.21131457862429293, 0.21199113114185297, 0.5444769492973955, 0.22591842373717552, 0.15953684575211446, 0.18819035353058666, 0.13197297956314413, 0.4125193317677659]}}
