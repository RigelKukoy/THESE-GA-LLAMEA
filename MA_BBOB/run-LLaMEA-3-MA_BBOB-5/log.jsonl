{"id": "dde74d15-9cc4-435c-a60f-5e74dac768c0", "fitness": -Infinity, "name": "CMAES", "description": "Covariance matrix adaptation evolution strategy with adaptive population size and restart mechanism.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize = None, cs = 0.3, c_cov = 0.1):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        \n        self.cs = cs\n        self.damps = 1 + 2 * np.max([0, np.sqrt((self.mueff-1)/(self.dim+1)) - 1]) + self.cs\n        self.c_cov = c_cov\n        self.chiN = self.dim**0.5 * (1 - (1/(4*self.dim)) + 1/(21*self.dim**2))\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize variables\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = 0.3  # Initial step size\n        C = np.eye(self.dim)  # Covariance matrix\n        \n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        \n        restart_iter = 0\n\n        evals = 0\n        while evals < self.budget:\n            # Generate and evaluate lambda offspring\n            Z = np.random.multivariate_normal(np.zeros(self.dim), C, size=self.popsize)\n            X = mean + sigma * Z\n            \n            # Clip X to respect bounds\n            X = np.clip(X, func.bounds.lb, func.bounds.ub)\n            \n            F = np.array([func(x) for x in X])\n            evals += self.popsize\n            \n            # Sort by fitness\n            idx = np.argsort(F)\n            X = X[idx]\n            F = F[idx]\n            \n            if F[0] < self.f_opt:\n                self.f_opt = F[0]\n                self.x_opt = X[0]\n            \n            # Update mean\n            mean_old = mean\n            mean = np.sum(X[:self.mu].T * self.weights, axis=1)\n            \n            # Update evolution path\n            ps = (1-self.cs) * ps + np.sqrt(self.cs * (2-self.cs) * self.mueff) * (mean - mean_old) / sigma @ np.linalg.inv(np.linalg.cholesky(C))\n            \n            # Update covariance matrix\n            pc = (1-self.c_cov) * pc + np.sqrt(self.c_cov * (2-self.c_cov) * self.mueff) * (mean - mean_old) / sigma\n            \n            C = (1-self.c_cov) * C + self.c_cov * (self.mueff/ np.sum(self.weights**2))*(pc[:, None] @ pc[None, :]) + self.c_cov * (1-self.mueff/ np.sum(self.weights**2)) * (Z[:self.mu].T @ np.diag(self.weights) @ Z[:self.mu])\n            \n            # Update step size\n            sigma = sigma * np.exp((self.cs/self.damps) * (np.linalg.norm(ps)/self.chiN - 1))\n            \n            # Check for stagnation and restart if necessary\n            if np.max(np.diag(C)) > 1e7 or sigma < 1e-7:\n                restart_iter +=1\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                sigma = 0.3\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "An exception occurred: Matrix is not positive definite.", "error": "", "parent_ids": [], "operator": null, "metadata": {}}
{"id": "53ddcf3c-1690-4f88-b1d9-97994c24c64d", "fitness": -Infinity, "name": "CMAES", "description": "Covariance Matrix Adaptation Evolution Strategy with a population size of 4 + floor(3*log(D)), where D is the dimension.", "code": "import numpy as np\nimport math\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, sigma0=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.popsize = 4 + math.floor(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n\n    def __call__(self, func):\n        # Initialization\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)  # Covariance matrix\n        \n        # Evolution path\n        ps = np.zeros(self.dim)\n        pc = np.zeros(self.dim)\n\n        # Parameters (commonly used values)\n        c_sigma = (math.sqrt(self.mu / (self.dim + self.mu))) / sigma\n        d_sigma = 1 + 2 * max(0, math.sqrt((self.mu - 1) / (self.dim + 1)) - 1) + c_sigma\n        c_c = (math.sqrt(self.mu / (self.dim + self.mu))) \n        c_1 = 2 / ((self.dim + 1.3)**2 + self.mu)\n        c_mu = min(1 - c_1, 2 * (self.mu - 1 + 1/self.mu) / ((self.dim + 2)**2 + self.mu))\n        eigeneval = 0\n        \n        f_opt = np.Inf\n        x_opt = None\n        \n        used_budget = 0\n\n        # Optimization loop\n        while used_budget < self.budget:\n            # Generate lambda offsprings\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            \n            try:\n                A = np.linalg.cholesky(C)  # C = A @ A.T\n                y = A @ z\n                x = mean[:, np.newaxis] + sigma * y\n                x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            except np.linalg.LinAlgError:\n                #Handle non-positive definite covariance matrix\n                C = C + 1e-6 * np.eye(self.dim) \n                try:\n                    A = np.linalg.cholesky(C)\n                    y = A @ z\n                    x = mean[:, np.newaxis] + sigma * y\n                    x = np.clip(x, func.bounds.lb, func.bounds.ub)\n                except:\n                    continue  # Retry if even after adding a small identity matrix it is not positive definite.\n\n            \n            f = np.array([func(x[:,i]) for i in range(self.popsize)])\n            used_budget += self.popsize\n\n            if np.min(f) < f_opt:\n                f_opt = np.min(f)\n                x_opt = x[:, np.argmin(f)].copy()\n\n            # Sort by fitness\n            fitness_ranks = np.argsort(f)\n            x_sorted = x[:, fitness_ranks]\n            y_sorted = y[:, fitness_ranks]\n\n            # Update mean\n            mean_new = np.mean(x_sorted[:, :self.mu], axis=1)\n            y_w = y_sorted[:, :self.mu]\n            \n            mean = mean_new.copy()\n\n            # Update evolution paths\n            ps = (1 - c_sigma) * ps + np.sqrt(c_sigma * (2 - c_sigma) * self.mu) * np.mean(y_sorted[:, :self.mu], axis=1)\n            hsig = (np.linalg.norm(ps) / math.sqrt(1 - (1 - c_sigma)**(2 * (used_budget // self.popsize + 1))) / math.sqrt(self.dim)) < (1.4 + 2/(self.dim+1))\n            \n            pc = (1 - c_c) * pc + hsig * np.sqrt(c_c * (2 - c_c) * self.mu) * np.mean(y_sorted[:, :self.mu], axis=1)\n\n            # Update covariance matrix\n            C = (1 - c_1 - c_mu) * C + c_1 * (pc[:, np.newaxis] @ pc[np.newaxis, :]) \\\n                + c_mu * np.sum(y_sorted[:, :self.mu] @ np.diag(np.ones(self.mu)) @ y_sorted[:, :self.mu].T, axis=1)\n\n            # Update step size\n            sigma = sigma * np.exp((c_sigma / d_sigma) * (np.linalg.norm(ps) / math.sqrt(self.dim) - 1))\n            sigma = max(sigma, 1e-10)\n            \n        return f_opt, x_opt", "configspace": "", "generation": 0, "feedback": "An exception occurred: name 'math' is not defined.", "error": "", "parent_ids": [], "operator": null, "metadata": {}}
{"id": "3e26151b-bd4f-450d-8f6c-0a73d5c4d6f7", "fitness": -Infinity, "name": "CMAES_with_Restarts_and_Archive", "description": "Covariance matrix adaptation evolution strategy with restarts and archive for diversity.", "code": "import numpy as np\n\nclass CMAES_with_Restarts_and_Archive:\n    def __init__(self, budget=10000, dim=10, popsize=None, sigma0=0.2, restarts=5, archive_size=100):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(dim))\n        self.sigma0 = sigma0\n        self.restarts = restarts\n        self.archive_size = archive_size\n        self.archive_x = []\n        self.archive_f = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        evals = 0\n\n        for restart in range(self.restarts):\n            # Initialization\n            mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n            sigma = self.sigma0\n            C = np.eye(self.dim)\n            pc = np.zeros(self.dim)\n            ps = np.zeros(self.dim)\n            chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))  # expectation of ||N(0,I)||\n\n            # Parameters (lambda is popsize)\n            lambda_ = self.popsize\n            mu = lambda_ // 2  # Number of parents\n            weights = np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))\n            weights = weights / np.sum(weights)\n            mueff = np.sum(weights)**2 / np.sum(weights**2)\n\n            # Learning rates\n            cc = (4 + mueff / self.dim) / (self.dim + 4 + 2 * mueff / self.dim)\n            cs = (mueff + 2) / (self.dim + mueff + 5)\n            c1 = 2 / ((self.dim + 1.3)**2 + mueff)\n            cmu = min(1 - c1, 2 * (mueff - 2 + 1 / mueff) / ((self.dim + 2)**2 + mueff))\n            damps = 1 + 2 * max(0, np.sqrt((mueff - 1) / (self.dim + 1)) - 1) + cs\n\n            while evals < self.budget / self.restarts:  # Budget per restart\n                # Sampling\n                z = np.random.randn(self.dim, lambda_)\n                x = mean[:, np.newaxis] + sigma * np.dot(np.linalg.cholesky(C), z)\n                x = np.clip(x, func.bounds.lb, func.bounds.ub)  # Clip to bounds\n\n                # Evaluation\n                f = np.array([func(x[:, i]) for i in range(lambda_)])\n                evals += lambda_\n                \n                # Archive update\n                for i in range(lambda_):\n                    if len(self.archive_x) < self.archive_size:\n                        self.archive_x.append(x[:, i])\n                        self.archive_f.append(f[i])\n                    else:\n                        max_f_index = np.argmax(self.archive_f)\n                        if f[i] < self.archive_f[max_f_index]:\n                            self.archive_x[max_f_index] = x[:, i]\n                            self.archive_f[max_f_index] = f[i]\n\n                # Selection and Recombination\n                idx = np.argsort(f)\n                x_sorted = x[:, idx]\n                z_sorted = z[:, idx]\n                mean_old = mean.copy()\n                mean = np.dot(x_sorted[:, :mu], weights)\n\n                # Update evolution path\n                ps = (1 - cs) * ps + np.sqrt(cs * (2 - cs) * mueff) * np.dot(np.linalg.inv(np.linalg.cholesky(C)), (mean - mean_old) / sigma)\n                hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - cs)**(2 * evals / lambda_)) < chiN * (1.4 + 2 / (self.dim + 1))\n                pc = (1 - cc) * pc + hsig * np.sqrt(cc * (2 - cc) * mueff) * (mean - mean_old) / sigma\n\n                # Update covariance matrix\n                C = (1 - c1 - cmu) * C + c1 * (np.outer(pc, pc) + (1 - hsig) * cc * (2 - cc) * C) + cmu * np.dot(z_sorted[:, :mu] * weights, z_sorted[:, :mu].T)\n\n                # Update step size\n                sigma = sigma * np.exp((cs / damps) * (np.linalg.norm(ps) / chiN - 1))\n\n                # Eigenvalue repair\n                C = np.triu(C) + np.triu(C, 1).T\n                C = np.linalg.eigvalsh(C)\n                C = np.diag(C.real)\n                C = np.maximum(C, 1e-10) # Prevent singular matrix\n                \n                C = C / np.trace(C) * np.trace(np.eye(self.dim)) # Normalize C\n                \n                # Best solution update\n                if f[idx[0]] < self.f_opt:\n                    self.f_opt = f[idx[0]]\n                    self.x_opt = x[:, idx[0]]\n\n            # Restart strategy: Check if stagnation has occurred\n            if restart < self.restarts - 1:\n                # Reset mean to a random point from the archive\n                if len(self.archive_x) > 0:\n                    idx = np.argmin(self.archive_f)\n                    mean = self.archive_x[idx].copy()\n                else:\n                     mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n\n                # Optional: Increase sigma0 for the next restart\n                sigma = self.sigma0 * (1 + restart * 0.1) \n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "An exception occurred: operands could not be broadcast together with shapes (2,6) (2,) (2,) .", "error": "", "parent_ids": [], "operator": null, "metadata": {}}
{"id": "f4f1f245-8463-4766-9395-8ac453ebe40f", "fitness": 0.32172747865246676, "name": "AdaptiveCovarianceSampling", "description": "Adaptively samples new points based on the covariance matrix of the best solutions found so far, while also incorporating a diversity mechanism.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.1, diversity_threshold=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n        self.diversity_threshold = diversity_threshold\n\n        self.lb = -5.0\n        self.ub = 5.0\n\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = []\n        self.best_f = []\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            # Covariance matrix not positive semi-definite, fall back to sampling from the bounds\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        \n        # Clip to bounds\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solutions\n        indices = np.argsort(f)\n        self.best_x.extend(x[indices[:min(10, self.initial_samples)]])\n        self.best_f.extend(f[indices[:min(10, self.initial_samples)]])\n\n        if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n        \n        \n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n            \n            # Update best solutions\n            indices = np.argsort(f)\n            self.best_x.extend(x[indices[:min(10, num_samples)]])\n            self.best_f.extend(f[indices[:min(10, num_samples)]])\n           \n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n\n            # Adaptation of mean and covariance\n            num_best = min(len(self.best_x), 50) # Consider only a limited number of best solutions\n            \n            if num_best > 1: # Need at least two points to calculate covariance\n                \n                # Diversity check\n                distances = np.linalg.norm(np.array(self.best_x)[:num_best] - np.mean(np.array(self.best_x)[:num_best], axis=0), axis=1)\n                if np.std(distances) > self.diversity_threshold:\n                    # Diverse solutions, update mean and covariance\n                    self.mean = (1 - self.adaptation_rate) * self.mean + self.adaptation_rate * np.mean(np.array(self.best_x)[:num_best], axis=0)\n                    try:\n                        self.covariance = (1 - self.adaptation_rate) * self.covariance + self.adaptation_rate * np.cov(np.array(self.best_x)[:num_best].T)\n                    except:\n                        pass # keep covariance as is\n\n                else:\n                    # Solutions are too similar, increase variance along each dimension\n                    self.covariance = self.covariance + self.adaptation_rate * np.diag(np.var(np.random.uniform(self.lb, self.ub, size=(100, self.dim)), axis=0)) # Add the variance of random samples\n\n            # Remove worst solutions if the list gets too long\n            if len(self.best_x) > 100:\n                indices = np.argsort(self.best_f)\n                self.best_x = list(np.array(self.best_x)[indices[:100]])\n                self.best_f = list(np.array(self.best_f)[indices[:100]])\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.322 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.14022860742313037, 0.29611763978582517, 0.3632841395899614, 0.2787955221931866, 0.22026724755836047, 0.26850838047708026, 0.2712747318243056, 0.26971129539103256, 0.262062302510659, 0.1764197655817319, 0.2361438497874877, 0.9942753790736584, 0.28212319689295107, 0.24127837107203887, 0.5731687692057956, 0.31667531797450876, 0.2848812897460663, 0.2923671325815076, 0.1923145568177983, 0.47465207756225003]}}
{"id": "25b8bb95-bf97-4bd7-bf88-1914f65e31f3", "fitness": -Infinity, "name": "CMAES", "description": "CMA-ES with simplified updates, adaptive covariance regularization, and budget management.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, sigma0=0.5, popsize=None):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.popsize = 4 + int(3 * np.log(self.dim)) if popsize is None else popsize\n        self.mu = self.popsize // 2\n        self.c_sigma = np.sqrt(self.mu / (self.dim + self.mu))\n        self.c_c = np.sqrt(self.mu / (self.dim + self.mu))\n        self.c_1 = 2 / ((self.dim + 1.3)**2 + self.mu)\n        self.c_mu = min(1 - self.c_1, 2 * (self.mu - 1 + 1/self.mu) / ((self.dim + 2)**2 + self.mu))\n        self.d_sigma = 1 + 2 * max(0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1) + self.c_sigma\n        self.min_sigma = 1e-10\n\n    def __call__(self, func):\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)\n        ps = np.zeros(self.dim)\n        pc = np.zeros(self.dim)\n        f_opt = np.inf\n        x_opt = None\n        used_budget = 0\n\n        while used_budget < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            try:\n                A = np.linalg.cholesky(C)\n                y = A @ z\n                x = mean[:, np.newaxis] + sigma * y\n                x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            except np.linalg.LinAlgError:\n                C = C + 1e-6 * np.eye(self.dim)\n                try:\n                    A = np.linalg.cholesky(C)\n                    y = A @ z\n                    x = mean[:, np.newaxis] + sigma * y\n                    x = np.clip(x, func.bounds.lb, func.bounds.ub)\n                except np.linalg.LinAlgError:\n                    continue\n\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            used_budget += self.popsize\n\n            if np.min(f) < f_opt:\n                f_opt = np.min(f)\n                x_opt = x[:, np.argmin(f)].copy()\n\n            fitness_ranks = np.argsort(f)\n            y_sorted = y[:, fitness_ranks]\n            mean = np.mean(x[:, fitness_ranks[:self.mu]], axis=1)\n\n            ps = (1 - self.c_sigma) * ps + np.sqrt(self.c_sigma * (2 - self.c_sigma) * self.mu) * np.mean(y_sorted[:, :self.mu], axis=1)\n            hsig = (np.linalg.norm(ps) / np.sqrt(1 - (1 - self.c_sigma)**(2 * (used_budget // self.popsize + 1))) / np.sqrt(self.dim)) < (1.4 + 2/(self.dim+1))\n            pc = (1 - self.c_c) * pc + (self.c_c * (2 - self.c_c) * self.mu)**0.5 * np.mean(y_sorted[:, :self.mu], axis=1) * hsig\n\n            C = (1 - self.c_1 - self.c_mu) * C + self.c_1 * np.outer(pc, pc) + self.c_mu * np.sum([y_sorted[:, i] @ y_sorted[:, i].T for i in range(self.mu)], axis=0)\n\n            sigma *= np.exp((self.c_sigma / self.d_sigma) * (np.linalg.norm(ps) / np.sqrt(self.dim) - 1))\n            sigma = max(sigma, self.min_sigma)\n\n        return f_opt, x_opt", "configspace": "", "generation": 1, "feedback": "An exception occurred: operands could not be broadcast together with shapes (2,6) (2,) (2,) .", "error": "", "parent_ids": ["53ddcf3c-1690-4f88-b1d9-97994c24c64d"], "operator": null, "metadata": {}}
{"id": "52f83cf7-a211-4d30-a782-85315ddfd153", "fitness": -Infinity, "name": "CMAES", "description": "CMA-ES with a simplified update and handling of non-positive definite covariance matrices.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, sigma0=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.popsize = 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.c_sigma = (self.mu / (self.dim + self.mu))**0.5\n        self.d_sigma = 1 + 2 * max(0, ((self.mu - 1) / (self.dim + 1))**0.5 - 1) + self.c_sigma\n        self.c_c = (self.mu / (self.dim + self.mu))**0.5\n        self.c_1 = 2 / ((self.dim + 1.3)**2 + self.mu)\n        self.c_mu = min(1 - self.c_1, 2 * (self.mu - 1 + 1/self.mu) / ((self.dim + 2)**2 + self.mu))\n\n    def __call__(self, func):\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)\n        ps = np.zeros(self.dim)\n        pc = np.zeros(self.dim)\n        f_opt = np.inf\n        x_opt = None\n        used_budget = 0\n\n        while used_budget < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            try:\n                A = np.linalg.cholesky(C)\n                y = A @ z\n                x = mean[:, np.newaxis] + sigma * y\n                x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            except np.linalg.LinAlgError:\n                C += 1e-6 * np.eye(self.dim)\n                try:\n                    A = np.linalg.cholesky(C)\n                    y = A @ z\n                    x = mean[:, np.newaxis] + sigma * y\n                    x = np.clip(x, func.bounds.lb, func.bounds.ub)\n                except np.linalg.LinAlgError:\n                    continue\n            \n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            used_budget += self.popsize\n            \n            if np.min(f) < f_opt:\n                f_opt = np.min(f)\n                x_opt = x[:, np.argmin(f)].copy()\n                \n            fitness_ranks = np.argsort(f)\n            x_sorted = x[:, fitness_ranks]\n            y_sorted = y[:, fitness_ranks]\n            mean_new = np.mean(x_sorted[:, :self.mu], axis=1)\n            mean = mean_new.copy()\n\n            ps = (1 - self.c_sigma) * ps + (self.c_sigma * (2 - self.c_sigma) * self.mu)**0.5 * np.mean(y_sorted[:, :self.mu], axis=1)\n            hsig = (np.linalg.norm(ps) / (1 - (1 - self.c_sigma)**(2 * (used_budget // self.popsize + 1)))**0.5 / self.dim**0.5) < (1.4 + 2/(self.dim+1))\n            pc = (1 - self.c_c) * pc + hsig * (self.c_c * (2 - self.c_c) * self.mu)**0.5 * np.mean(y_sorted[:, :self.mu], axis=1)\n            C = (1 - self.c_1 - self.c_mu) * C + self.c_1 * np.outer(pc, pc) + self.c_mu * np.sum([y_sorted[:, i] @ y_sorted[:, i].T for i in range(self.mu)], axis=0)\n\n            sigma *= np.exp((self.c_sigma / self.d_sigma) * (np.linalg.norm(ps) / self.dim**0.5 - 1))\n            sigma = max(sigma, 1e-10)\n\n        return f_opt, x_opt", "configspace": "", "generation": 1, "feedback": "An exception occurred: operands could not be broadcast together with shapes (2,6) (2,) (2,) .", "error": "", "parent_ids": ["53ddcf3c-1690-4f88-b1d9-97994c24c64d"], "operator": null, "metadata": {}}
{"id": "9aac7450-2fb9-4986-a264-cbe7903542b6", "fitness": 0.3229863713794, "name": "AdaptiveCovarianceSampling", "description": "Adaptively samples new points based on the covariance matrix of the best solutions found so far, using a simplified adaptation rule and noise injection for exploration.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n\n        self.lb = -5.0\n        self.ub = 5.0\n\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = []\n        self.best_f = []\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            # Covariance matrix not positive semi-definite, fall back to sampling from the bounds\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        \n        # Clip to bounds\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solutions\n        indices = np.argsort(f)\n        self.best_x.extend(x[indices[:min(10, self.initial_samples)]])\n        self.best_f.extend(f[indices[:min(10, self.initial_samples)]])\n\n        if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n        \n        \n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n            \n            # Update best solutions\n            indices = np.argsort(f)\n            self.best_x.extend(x[indices[:min(10, num_samples)]])\n            self.best_f.extend(f[indices[:min(10, num_samples)]])\n           \n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n\n            # Adaptation of mean and covariance\n            num_best = min(len(self.best_x), 50) # Consider only a limited number of best solutions\n            \n            if num_best > 1: # Need at least two points to calculate covariance\n                best_x_arr = np.array(self.best_x)[:num_best]\n                self.mean = (1 - self.adaptation_rate) * self.mean + self.adaptation_rate * np.mean(best_x_arr, axis=0)\n                try:\n                    self.covariance = (1 - self.adaptation_rate) * self.covariance + self.adaptation_rate * np.cov(best_x_arr.T)\n                    # Add a small amount of noise to the covariance matrix to ensure exploration\n                    self.covariance += np.eye(self.dim) * 1e-6 \n                except:\n                    pass # keep covariance as is\n\n\n            # Remove worst solutions if the list gets too long\n            if len(self.best_x) > 100:\n                indices = np.argsort(self.best_f)\n                self.best_x = list(np.array(self.best_x)[indices[:100]])\n                self.best_f = list(np.array(self.best_f)[indices[:100]])\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.323 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f4f1f245-8463-4766-9395-8ac453ebe40f"], "operator": null, "metadata": {"aucs": [0.12743264021962863, 0.2882791801625946, 0.3762166446899763, 0.2702898637580843, 0.21163917026315238, 0.31256095382637195, 0.2760068965688466, 0.31067141449680524, 0.2838705962779111, 0.1914367300149572, 0.2471964926760155, 1.0, 0.2907593810668252, 0.27543803600186645, 0.5250661055162833, 0.30375934585806286, 0.30360241820521083, 0.2135760025758049, 0.18866808730835538, 0.4632574681012468]}}
{"id": "b6725372-df01-4293-92c1-e426636f34a5", "fitness": 0.3269790775700828, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with simplified adaptation and a better covariance handling strategy.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n\n        self.lb = -5.0\n        self.ub = 5.0\n\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = []\n        self.best_f = []\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            # Covariance matrix not positive semi-definite, fall back to sampling from the bounds\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        \n        # Clip to bounds\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solutions\n        indices = np.argsort(f)\n        self.best_x.extend(x[indices[:min(10, self.initial_samples)]])\n        self.best_f.extend(f[indices[:min(10, self.initial_samples)]])\n\n        if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n        \n        \n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n            \n            # Update best solutions\n            indices = np.argsort(f)\n            self.best_x.extend(x[indices[:min(10, num_samples)]])\n            self.best_f.extend(f[indices[:min(10, num_samples)]])\n           \n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n\n            # Adaptation of mean and covariance\n            num_best = min(len(self.best_x), 50) # Consider only a limited number of best solutions\n            \n            if num_best > 1: # Need at least two points to calculate covariance\n                \n                self.mean = (1 - self.adaptation_rate) * self.mean + self.adaptation_rate * np.mean(np.array(self.best_x)[:num_best], axis=0)\n                \n                # Robust covariance estimation using shrinkage\n                X = np.array(self.best_x)[:num_best]\n                \n                sample_covariance = np.cov(X.T)\n                \n                # Shrinkage factor (can be tuned)\n                shrinkage = 0.1\n                \n                self.covariance = (1 - shrinkage) * sample_covariance + shrinkage * np.eye(self.dim) * np.var(X)\n\n            # Remove worst solutions if the list gets too long\n            if len(self.best_x) > 100:\n                indices = np.argsort(self.best_f)\n                self.best_x = list(np.array(self.best_x)[indices[:100]])\n                self.best_f = list(np.array(self.best_f)[indices[:100]])\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.327 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f4f1f245-8463-4766-9395-8ac453ebe40f"], "operator": null, "metadata": {"aucs": [0.1488015141268283, 0.25769173753057706, 0.3677880204915652, 0.2954665161895005, 0.23494438317629296, 0.2993027762949373, 0.27022025948754824, 0.29044110350198604, 0.24943670032987963, 0.18042675907206063, 0.27880057860048857, 0.9986333402197547, 0.2894230011147846, 0.2215201357637846, 0.5951109262349378, 0.30370619896408335, 0.2897080864956235, 0.3035431783206822, 0.18749821461170435, 0.47711812087463856]}}
{"id": "dcccaa99-bc5f-45ae-8bb4-97eca5a540be", "fitness": 0.32507754081242657, "name": "AdaptiveCovarianceSampling", "description": "Adaptively samples new points based on the covariance matrix of the best solutions found so far, while also incorporating a simplified diversity mechanism via covariance regularization.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.1, covariance_regularization=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n        self.covariance_regularization = covariance_regularization\n\n        self.lb = -5.0\n        self.ub = 5.0\n\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = []\n        self.best_f = []\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            # Covariance matrix not positive semi-definite, fall back to sampling from the bounds\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        \n        # Clip to bounds\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(x_) for x_ in x])\n        self.budget -= self.initial_samples\n\n        # Update best solutions\n        indices = np.argsort(f)\n        self.best_x.extend(x[indices[:min(10, self.initial_samples)]])\n        self.best_f.extend(f[indices[:min(10, self.initial_samples)]])\n\n        if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n        \n        \n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(x_) for x_ in x])\n            self.budget -= num_samples\n            \n            # Update best solutions\n            indices = np.argsort(f)\n            self.best_x.extend(x[indices[:min(10, num_samples)]])\n            self.best_f.extend(f[indices[:min(10, num_samples)]])\n           \n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n\n            # Adaptation of mean and covariance\n            num_best = min(len(self.best_x), 50) # Consider only a limited number of best solutions\n            \n            if num_best > 1: # Need at least two points to calculate covariance\n                self.mean = (1 - self.adaptation_rate) * self.mean + self.adaptation_rate * np.mean(np.array(self.best_x)[:num_best], axis=0)\n                try:\n                    covariance = np.cov(np.array(self.best_x)[:num_best].T)\n                    # Regularize covariance matrix\n                    covariance = covariance + self.covariance_regularization * np.eye(self.dim)\n                    self.covariance = (1 - self.adaptation_rate) * self.covariance + self.adaptation_rate * covariance\n\n                except:\n                    pass # keep covariance as is\n\n\n            # Remove worst solutions if the list gets too long\n            if len(self.best_x) > 100:\n                indices = np.argsort(self.best_f)\n                self.best_x = list(np.array(self.best_x)[indices[:100]])\n                self.best_f = list(np.array(self.best_f)[indices[:100]])\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.325 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f4f1f245-8463-4766-9395-8ac453ebe40f"], "operator": null, "metadata": {"aucs": [0.1562770709171185, 0.25899536660780353, 0.36649810188287113, 0.3209644084083244, 0.24943034215987336, 0.27943285898071357, 0.2687549642148409, 0.2873690843175005, 0.26832081530170737, 0.18565104905174168, 0.2935360113510299, 0.8998062387163023, 0.2815222954467065, 0.24826844876657206, 0.4989302722376525, 0.3241784947359734, 0.29568507070826877, 0.35468852111391114, 0.1900631218228912, 0.47317827950672975]}}
{"id": "c8842d61-0d44-4883-b134-6ab817141dfd", "fitness": 0.3291255721664273, "name": "AdaptiveCovarianceSamplingSimplified", "description": "Adaptively samples based on a simplified covariance matrix (diagonal only) and adds a random exploration component.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSamplingSimplified:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.1, exploration_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n        self.exploration_rate = exploration_rate\n\n        self.lb = -5.0\n        self.ub = 5.0\n\n        self.mean = np.zeros(dim)\n        self.variance = np.ones(dim)  # Diagonal covariance\n        self.best_x = []\n        self.best_f = []\n\n    def sample(self, num_samples):\n        # Sample from normal distribution with diagonal covariance\n        x = np.random.normal(self.mean, np.sqrt(self.variance), size=(num_samples, self.dim))\n        \n        # Exploration: Randomly sample some dimensions\n        mask = np.random.rand(num_samples, self.dim) < self.exploration_rate\n        x[mask] = np.random.uniform(self.lb, self.ub, size=x[mask].shape)\n        \n        # Clip to bounds\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solutions\n        indices = np.argsort(f)\n        self.best_x.extend(x[indices[:min(10, self.initial_samples)]])\n        self.best_f.extend(f[indices[:min(10, self.initial_samples)]])\n\n        if np.min(f) < self.f_opt:\n            self.f_opt = np.min(f)\n            self.x_opt = x[np.argmin(f)]\n        \n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n            \n            # Update best solutions\n            indices = np.argsort(f)\n            self.best_x.extend(x[indices[:min(10, num_samples)]])\n            self.best_f.extend(f[indices[:min(10, num_samples)]])\n           \n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n\n            # Adaptation of mean and variance\n            num_best = min(len(self.best_x), 50)\n            if num_best > 0:\n                best_x_arr = np.array(self.best_x)[:num_best]\n                self.mean = (1 - self.adaptation_rate) * self.mean + self.adaptation_rate * np.mean(best_x_arr, axis=0)\n                self.variance = (1 - self.adaptation_rate) * self.variance + self.adaptation_rate * np.var(best_x_arr, axis=0)\n                self.variance = np.maximum(self.variance, 0.01) # Ensure variance is not zero\n\n            # Remove worst solutions if the list gets too long\n            if len(self.best_x) > 100:\n                indices = np.argsort(self.best_f)\n                self.best_x = list(np.array(self.best_x)[indices[:100]])\n                self.best_f = list(np.array(self.best_f)[indices[:100]])\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveCovarianceSamplingSimplified scored 0.329 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f4f1f245-8463-4766-9395-8ac453ebe40f"], "operator": null, "metadata": {"aucs": [0.12792046918020716, 0.24507438952311333, 0.3708178645937975, 0.3017298197007994, 0.23752890069212795, 0.31202440168827306, 0.2648027229123774, 0.29193400532607217, 0.2746167721340479, 0.17365477838273258, 0.2640300651656692, 0.9803202204457074, 0.26658539640325807, 0.24036978681831644, 0.5799437260197793, 0.31674332040257147, 0.3016274235658857, 0.3584700862694016, 0.19618081379313645, 0.47813648031127054]}}
{"id": "09a97fde-cca8-43b8-bdf8-3f1ad5fb3532", "fitness": 0.3703500830548084, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with simplified diversity control and covariance updates, focusing on stability and reducing complexity.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.1, diversity_threshold=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n        self.diversity_threshold = diversity_threshold\n\n        self.lb = -5.0\n        self.ub = 5.0\n\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = []\n        self.best_f = []\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            # Covariance matrix not positive semi-definite, fall back to sampling from the bounds\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        \n        # Clip to bounds\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solutions\n        indices = np.argsort(f)\n        self.best_x.extend(x[indices[:min(10, self.initial_samples)]])\n        self.best_f.extend(f[indices[:min(10, self.initial_samples)]])\n\n        if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n        \n        \n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n            \n            # Update best solutions\n            indices = np.argsort(f)\n            self.best_x.extend(x[indices[:min(10, num_samples)]])\n            self.best_f.extend(f[indices[:min(10, num_samples)]])\n           \n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n\n            # Adaptation of mean and covariance\n            num_best = min(len(self.best_x), 20) # Reduced number of best solutions\n\n            if num_best > 1:\n                self.mean = (1 - self.adaptation_rate) * self.mean + self.adaptation_rate * np.mean(np.array(self.best_x)[:num_best], axis=0)\n                try:\n                    # Simplified covariance update\n                    self.covariance = (1 - self.adaptation_rate) * self.covariance + self.adaptation_rate * np.cov(np.array(self.best_x)[:num_best].T)\n                except:\n                    pass # keep covariance as is\n\n\n            # Remove worst solutions if the list gets too long\n            if len(self.best_x) > 50:  # Reduced best solutions size\n                indices = np.argsort(self.best_f)\n                self.best_x = list(np.array(self.best_x)[indices[:50]])\n                self.best_f = list(np.array(self.best_f)[indices[:50]])\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.370 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f4f1f245-8463-4766-9395-8ac453ebe40f"], "operator": null, "metadata": {"aucs": [0.15994176832131957, 0.3182975072470946, 0.38088930699731816, 0.45711807794565007, 0.26052731343407465, 0.3821169672115221, 0.2664943646359197, 0.32410530829289796, 0.30594400182364667, 0.18836505882607701, 0.4479992123255183, 0.9957440768454398, 0.2720481171426673, 0.25896248258478427, 0.6200981227627306, 0.3257694639244517, 0.33182144801936897, 0.43632277859339896, 0.19614178038344554, 0.4782945037788422]}}
{"id": "79df7d10-e9b2-48c0-9ad6-0aea3a3a2998", "fitness": 0.3331552342423172, "name": "AdaptiveCovarianceSampling", "description": "Adaptively samples new points based on the covariance matrix of the best solutions found so far, while incorporating a simplified diversity mechanism based on covariance matrix diagonality.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.1, diversity_threshold=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n        self.diversity_threshold = diversity_threshold\n\n        self.lb = -5.0\n        self.ub = 5.0\n\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = []\n        self.best_f = []\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            # Covariance matrix not positive semi-definite, fall back to sampling from the bounds\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        \n        # Clip to bounds\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solutions\n        indices = np.argsort(f)\n        self.best_x.extend(x[indices[:min(10, self.initial_samples)]])\n        self.best_f.extend(f[indices[:min(10, self.initial_samples)]])\n\n        if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n        \n        \n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n            \n            # Update best solutions\n            indices = np.argsort(f)\n            self.best_x.extend(x[indices[:min(10, num_samples)]])\n            self.best_f.extend(f[indices[:min(10, num_samples)]])\n           \n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n\n            # Adaptation of mean and covariance\n            num_best = min(len(self.best_x), 50) # Consider only a limited number of best solutions\n            \n            if num_best > 1: # Need at least two points to calculate covariance\n                \n                self.mean = (1 - self.adaptation_rate) * self.mean + self.adaptation_rate * np.mean(np.array(self.best_x)[:num_best], axis=0)\n                try:\n                    cov = np.cov(np.array(self.best_x)[:num_best].T)\n                    #Diversity check: encourage diagonal covariance\n                    if np.sum(np.abs(np.diag(cov) - np.diag(self.covariance))) > self.diversity_threshold:\n                        self.covariance = (1 - self.adaptation_rate) * self.covariance + self.adaptation_rate * cov\n                    else:\n                        self.covariance = self.covariance + self.adaptation_rate * np.diag(np.var(np.random.uniform(self.lb, self.ub, size=(100, self.dim)), axis=0))\n                except:\n                    pass # keep covariance as is\n\n            # Remove worst solutions if the list gets too long\n            if len(self.best_x) > 100:\n                indices = np.argsort(self.best_f)\n                self.best_x = list(np.array(self.best_x)[indices[:100]])\n                self.best_f = list(np.array(self.best_f)[indices[:100]])\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.333 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f4f1f245-8463-4766-9395-8ac453ebe40f"], "operator": null, "metadata": {"aucs": [0.11880532221803419, 0.29119536323506756, 0.36709902715599396, 0.3197208408986505, 0.24719822365039823, 0.3090420795186315, 0.27522770396228735, 0.29408249171335044, 0.2590137821052054, 0.18576580476701576, 0.25481598025111607, 0.9973897153372229, 0.27006793038945676, 0.22562079857969963, 0.640913320716252, 0.31601016146169947, 0.31188809557827, 0.3100610298116442, 0.19449643207963752, 0.47469058141670983]}}
{"id": "ec678b40-0ef1-43d6-a3b3-a75764cb4a2f", "fitness": 0.30528835446078084, "name": "CMAES", "description": "Robust CMA-ES implementation with eigenvalue decomposition to handle non-positive definite covariance matrices and simplified update rules.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize = None, cs = 0.3, c_cov = 0.1):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        \n        self.cs = cs\n        self.damps = 1 + 2 * np.max([0, np.sqrt((self.mueff-1)/(self.dim+1)) - 1]) + self.cs\n        self.c_cov = c_cov\n        self.chiN = self.dim**0.5 * (1 - (1/(4*self.dim)) + 1/(21*self.dim**2))\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize variables\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = 0.3  # Initial step size\n        C = np.eye(self.dim)  # Covariance matrix\n        \n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        \n        restart_iter = 0\n\n        evals = 0\n        while evals < self.budget:\n            # Generate and evaluate lambda offspring\n            Z = np.random.multivariate_normal(np.zeros(self.dim), C, size=self.popsize)\n            X = mean + sigma * Z\n            \n            # Clip X to respect bounds\n            X = np.clip(X, func.bounds.lb, func.bounds.ub)\n            \n            F = np.array([func(x) for x in X])\n            evals += self.popsize\n            \n            # Sort by fitness\n            idx = np.argsort(F)\n            X = X[idx]\n            F = F[idx]\n            \n            if F[0] < self.f_opt:\n                self.f_opt = F[0]\n                self.x_opt = X[0]\n            \n            # Update mean\n            mean_old = mean\n            mean = np.sum(X[:self.mu].T * self.weights, axis=1)\n            \n            # Update evolution path\n            ps = (1-self.cs) * ps + np.sqrt(self.cs * (2-self.cs) * self.mueff) * (mean - mean_old) / sigma\n            pc = (1-self.c_cov) * pc + np.sqrt(self.c_cov * (2-self.c_cov) * self.mueff) * (mean - mean_old) / sigma\n\n            # Update covariance matrix\n            C = (1-self.c_cov) * C + self.c_cov * (pc[:, None] @ pc[None, :]) + self.c_cov * (0.1 + self.mueff / np.sum(self.weights**2)) * (ps[:, None] @ ps[None, :])\n\n            # Ensure C is positive definite\n            try:\n                np.linalg.cholesky(C)\n            except np.linalg.LinAlgError:\n                # If not positive definite, perform eigenvalue decomposition and clip eigenvalues\n                eigenvalues, eigenvectors = np.linalg.eigh(C)\n                eigenvalues = np.clip(eigenvalues, 1e-10, None)  # Ensure eigenvalues are positive\n                C = eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T\n            \n            # Update step size\n            sigma = sigma * np.exp((self.cs/self.damps) * (np.linalg.norm(ps)/self.chiN - 1))\n            \n            # Check for stagnation and restart if necessary\n            if np.max(np.diag(C)) > 1e7 or sigma < 1e-7:\n                restart_iter +=1\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                sigma = 0.3\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm CMAES scored 0.305 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["dde74d15-9cc4-435c-a60f-5e74dac768c0"], "operator": null, "metadata": {"aucs": [0.12288895762853613, 0.1831092558123254, 0.28365600759705345, 0.28046494456752724, 0.235328198951687, 0.2562694000060901, 0.24879658968519802, 0.2410970128210329, 0.20563284101326085, 0.1649170334476363, 0.26936495513818204, 0.9961190166476446, 0.25548346573503067, 0.26372228572077383, 0.6271313445574026, 0.3094573603771692, 0.2515957296242418, 0.30859692949017603, 0.145288685068361, 0.4568470753262882]}}
{"id": "9776532a-be8f-4bd7-8747-b6e590195dd5", "fitness": 0.23050505852446487, "name": "SimplifiedCMAES", "description": "Simplified CMA-ES with eigenvalue clipping to maintain positive definiteness of the covariance matrix and reduced complexity.", "code": "import numpy as np\n\nclass SimplifiedCMAES:\n    def __init__(self, budget=10000, dim=10, popsize = None, cs = 0.3, c_cov = 0.1):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        \n        self.cs = cs\n        self.damps = 1 + 2 * np.max([0, np.sqrt((self.mueff-1)/(self.dim+1)) - 1]) + self.cs\n        self.c_cov = c_cov\n        self.chiN = self.dim**0.5 * (1 - (1/(4*self.dim)) + 1/(21*self.dim**2))\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initialize variables\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = 0.3  # Initial step size\n        C = np.eye(self.dim)  # Covariance matrix\n        \n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        \n        restart_iter = 0\n\n        evals = 0\n        while evals < self.budget:\n            # Generate and evaluate lambda offspring\n            Z = np.random.multivariate_normal(np.zeros(self.dim), C, size=self.popsize)\n            X = mean + sigma * Z\n            \n            # Clip X to respect bounds\n            X = np.clip(X, func.bounds.lb, func.bounds.ub)\n            \n            F = np.array([func(x) for x in X])\n            evals += self.popsize\n            \n            # Sort by fitness\n            idx = np.argsort(F)\n            X = X[idx]\n            F = F[idx]\n            \n            if F[0] < self.f_opt:\n                self.f_opt = F[0]\n                self.x_opt = X[0]\n            \n            # Update mean\n            mean_old = mean\n            mean = np.sum(X[:self.mu].T * self.weights, axis=1)\n            \n            # Update evolution path\n            ps = (1-self.cs) * ps + np.sqrt(self.cs * (2-self.cs) * self.mueff) * (mean - mean_old) / sigma\n            \n            # Update covariance matrix\n            pc = (1-self.c_cov) * pc + np.sqrt(self.c_cov * (2-self.c_cov) * self.mueff) * (mean - mean_old) / sigma\n            \n            C = (1-self.c_cov) * C + self.c_cov * (self.mueff/ np.sum(self.weights**2))*(pc[:, None] @ pc[None, :]) + self.c_cov * (1-self.mueff/ np.sum(self.weights**2)) * (Z[:self.mu].T @ np.diag(self.weights) @ Z[:self.mu])\n\n            # Ensure positive definiteness by eigenvalue clipping\n            try:\n                L, Q = np.linalg.eig(C)\n                L = np.clip(L, 1e-10, None)  # Clip eigenvalues to be positive\n                C = Q @ np.diag(L) @ Q.T\n            except np.linalg.LinAlgError:\n                C = np.eye(self.dim)  # Reset C if eigenvalue decomposition fails\n\n            # Update step size\n            sigma = sigma * np.exp((self.cs/self.damps) * (np.linalg.norm(ps)/self.chiN - 1))\n            \n            # Check for stagnation and restart if necessary\n            if np.max(np.diag(C)) > 1e7 or sigma < 1e-7:\n                restart_iter +=1\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                sigma = 0.3\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm SimplifiedCMAES scored 0.231 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["dde74d15-9cc4-435c-a60f-5e74dac768c0"], "operator": null, "metadata": {"aucs": [0.10600419075388867, 0.19585233342375874, 0.2501832667059827, 0.16146616201015662, 0.16146730706858292, 0.18288879619557308, 0.06657653209123127, 0.19916684989766031, 0.20084879665337552, 0.14639097728875927, 0.17926854997499153, 0.8947031983764192, 0.3041656593389557, 0.19946690574309456, 0.30532071862408083, 0.21908804030011408, 0.1544059864724422, 0.17507790595789285, 0.1357753538150277, 0.37198363979731]}}
{"id": "242a8802-e196-4e23-a1d6-29588af04455", "fitness": -Infinity, "name": "CMAES", "description": "CMA-ES with simplified covariance update and step-size adaptation, handling budget constraints and covariance matrix repair.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, sigma0=0.5, popsize_factor=3):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.popsize = 4 + int(popsize_factor * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.c_sigma = None\n        self.d_sigma = None\n        self.c_c = None\n        self.c_1 = None\n        self.c_mu = None\n\n    def __call__(self, func):\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)\n\n        ps = np.zeros(self.dim)\n        pc = np.zeros(self.dim)\n\n        # Initialize CMA-ES parameters inside the __call__ method.\n        if self.c_sigma is None: # Check if parameters are not initialized\n          self.c_sigma = (np.sqrt(self.mu / (self.dim + self.mu))) / sigma\n          self.d_sigma = 1 + 2 * max(0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1) + self.c_sigma\n          self.c_c = (np.sqrt(self.mu / (self.dim + self.mu)))\n          self.c_1 = 2 / ((self.dim + 1.3)**2 + self.mu)\n          self.c_mu = min(1 - self.c_1, 2 * (self.mu - 1 + 1/self.mu) / ((self.dim + 2)**2 + self.mu))\n    \n\n        f_opt = np.Inf\n        x_opt = None\n        used_budget = 0\n\n        while used_budget < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            try:\n                A = np.linalg.cholesky(C)\n                y = A @ z\n                x = mean[:, np.newaxis] + sigma * y\n                x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            except np.linalg.LinAlgError:\n                C = C + 1e-6 * np.eye(self.dim)\n                try:\n                    A = np.linalg.cholesky(C)\n                    y = A @ z\n                    x = mean[:, np.newaxis] + sigma * y\n                    x = np.clip(x, func.bounds.lb, func.bounds.ub)\n                except np.linalg.LinAlgError:\n                    continue\n\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            used_budget += self.popsize\n\n            if np.min(f) < f_opt:\n                f_opt = np.min(f)\n                x_opt = x[:, np.argmin(f)].copy()\n\n            fitness_ranks = np.argsort(f)\n            y_sorted = y[:, fitness_ranks]\n\n            mean = np.mean(x[:, fitness_ranks[:self.mu]], axis=1)\n\n            ps = (1 - self.c_sigma) * ps + np.sqrt(self.c_sigma * (2 - self.c_sigma) * self.mu) * np.mean(y_sorted[:, :self.mu], axis=1)\n            hsig = (np.linalg.norm(ps) / np.sqrt(1 - (1 - self.c_sigma)**(2 * (used_budget // self.popsize + 1))) / np.sqrt(self.dim)) < (1.4 + 2/(self.dim+1))\n\n            pc = (1 - self.c_c) * pc + hsig * np.sqrt(self.c_c * (2 - self.c_c) * self.mu) * np.mean(y_sorted[:, :self.mu], axis=1)\n\n            C = (1 - self.c_1 - self.c_mu) * C + self.c_1 * np.outer(pc, pc) + self.c_mu * np.sum([y_sorted[:, i] @ y_sorted[:, i].T for i in range(self.mu)], axis=0)\n\n\n            sigma *= np.exp((self.c_sigma / self.d_sigma) * (np.linalg.norm(ps) / np.sqrt(self.dim) - 1))\n            sigma = max(sigma, 1e-10)\n\n        return f_opt, x_opt", "configspace": "", "generation": 1, "feedback": "An exception occurred: operands could not be broadcast together with shapes (2,6) (2,) (2,) .", "error": "", "parent_ids": ["53ddcf3c-1690-4f88-b1d9-97994c24c64d"], "operator": null, "metadata": {}}
{"id": "a269f779-6394-4b2d-aeaa-2cb1fe10b60f", "fitness": -Infinity, "name": "CMAES_with_Restarts_and_Archive", "description": "CMA-ES with restarts, archive, and eigenvalue clipping for covariance matrix repair, improved boundary handling and simplified update rules.", "code": "import numpy as np\n\nclass CMAES_with_Restarts_and_Archive:\n    def __init__(self, budget=10000, dim=10, popsize=None, sigma0=0.2, restarts=5, archive_size=100):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(dim))\n        self.sigma0 = sigma0\n        self.restarts = restarts\n        self.archive_size = archive_size\n        self.archive_x = []\n        self.archive_f = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        evals = 0\n\n        for restart in range(self.restarts):\n            # Initialization\n            mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n            sigma = self.sigma0\n            C = np.eye(self.dim)\n            pc = np.zeros(self.dim)\n            ps = np.zeros(self.dim)\n            chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n            # Parameters\n            lambda_ = self.popsize\n            mu = lambda_ // 2\n            weights = np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))\n            weights = weights / np.sum(weights)\n            mueff = np.sum(weights)**2 / np.sum(weights**2)\n\n            # Learning rates\n            cc = (4 + mueff / self.dim) / (self.dim + 4 + 2 * mueff / self.dim)\n            cs = (mueff + 2) / (self.dim + mueff + 5)\n            c1 = 2 / ((self.dim + 1.3)**2 + mueff)\n            cmu = min(1 - c1, 2 * (mueff - 2 + 1 / mueff) / ((self.dim + 2)**2 + mueff))\n            damps = 1 + 2 * max(0, np.sqrt((mueff - 1) / (self.dim + 1)) - 1) + cs\n\n            while evals < self.budget / self.restarts:\n                # Sampling\n                z = np.random.randn(self.dim, lambda_)\n                x = mean[:, np.newaxis] + sigma * np.dot(np.linalg.cholesky(C), z)\n                x = np.clip(x, func.bounds.lb, func.bounds.ub)\n\n                # Evaluation\n                f = np.array([func(x[:, i]) for i in range(lambda_)])\n                evals += lambda_\n\n                # Archive update\n                for i in range(lambda_):\n                    if len(self.archive_x) < self.archive_size:\n                        self.archive_x.append(x[:, i])\n                        self.archive_f.append(f[i])\n                    else:\n                        max_f_index = np.argmax(self.archive_f)\n                        if f[i] < self.archive_f[max_f_index]:\n                            self.archive_x[max_f_index] = x[:, i]\n                            self.archive_f[max_f_index] = f[i]\n\n                # Selection and Recombination\n                idx = np.argsort(f)\n                x_sorted = x[:, idx]\n                z_sorted = z[:, idx]\n                mean_old = mean.copy()\n                mean = np.dot(x_sorted[:, :mu], weights)\n\n                # Update evolution path\n                ps = (1 - cs) * ps + np.sqrt(cs * (2 - cs) * mueff) * (mean - mean_old) / sigma\n                hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - cs)**(2 * evals / lambda_)) < chiN * (1.4 + 2 / (self.dim + 1))\n                pc = (1 - cc) * pc + hsig * np.sqrt(cc * (2 - cc) * mueff) * (mean - mean_old) / sigma\n\n                # Update covariance matrix\n                C = (1 - c1 - cmu) * C + c1 * (np.outer(pc, pc) + (1 - hsig) * cc * (2 - cc) * C) + cmu * np.dot(z_sorted[:, :mu] * weights, z_sorted[:, :mu].T)\n\n                # Update step size\n                sigma = sigma * np.exp((cs / damps) * (np.linalg.norm(ps) / chiN - 1))\n\n                # Eigenvalue repair\n                D, B = np.linalg.eig(C)\n                D = np.maximum(D.real, 1e-10)\n                C = B @ np.diag(D) @ B.T\n                \n                # Best solution update\n                if f[idx[0]] < self.f_opt:\n                    self.f_opt = f[idx[0]]\n                    self.x_opt = x[:, idx[0]]\n\n            # Restart strategy\n            if restart < self.restarts - 1:\n                if len(self.archive_x) > 0:\n                    idx = np.argmin(self.archive_f)\n                    mean = self.archive_x[idx].copy()\n                else:\n                    mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                sigma = self.sigma0 * (1 + restart * 0.1)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "An exception occurred: operands could not be broadcast together with shapes (2,6) (2,) (2,) .", "error": "", "parent_ids": ["3e26151b-bd4f-450d-8f6c-0a73d5c4d6f7"], "operator": null, "metadata": {}}
{"id": "733d546a-032b-428c-852b-c8ee6e391ffc", "fitness": -Infinity, "name": "AdaptiveCovarianceSamplingSimplified", "description": "Simplified adaptive covariance sampling with focused exploration and robust variance control.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSamplingSimplified:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.1, exploration_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n        self.exploration_rate = exploration_rate\n\n        self.lb = -5.0\n        self.ub = 5.0\n\n        self.mean = np.zeros(dim)\n        self.variance = np.ones(dim)\n        self.best_x = []\n        self.best_f = []\n        self.num_best_kept = 20\n\n    def sample(self, num_samples):\n        x = np.random.normal(self.mean, np.sqrt(self.variance), size=(num_samples, self.dim))\n        \n        # Focused exploration around the mean\n        mask = np.random.rand(num_samples, self.dim) < self.exploration_rate\n        x[mask] = self.mean[None, mask.argmax(axis=1)] + np.random.uniform(-1.0, 1.0, size=x[mask].shape) # explore around the mean\n        \n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        indices = np.argsort(f)\n        self.best_x.extend(x[indices[:min(self.num_best_kept, self.initial_samples)]])\n        self.best_f.extend(f[indices[:min(self.num_best_kept, self.initial_samples)]])\n\n        if np.min(f) < self.f_opt:\n            self.f_opt = np.min(f)\n            self.x_opt = x[np.argmin(f)]\n        \n        while self.budget > 0:\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n            \n            indices = np.argsort(f)\n            self.best_x.extend(x[indices[:min(self.num_best_kept, num_samples)]])\n            self.best_f.extend(f[indices[:min(self.num_best_kept, num_samples)]])\n           \n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n\n            # Adaptation of mean and variance\n            num_best = min(len(self.best_x), self.num_best_kept * 2) # consider more for adaptation\n            if num_best > 0:\n                best_x_arr = np.array(self.best_x)[:num_best]\n                self.mean = (1 - self.adaptation_rate) * self.mean + self.adaptation_rate * np.mean(best_x_arr, axis=0)\n                self.variance = (1 - self.adaptation_rate) * self.variance + self.adaptation_rate * np.var(best_x_arr, axis=0)\n                self.variance = np.maximum(self.variance, 0.01)\n\n            self.best_x = self.best_x[-self.num_best_kept:]\n            self.best_f = self.best_f[-self.num_best_kept:]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "An exception occurred: operands could not be broadcast together with shapes (1,100) (53,) .", "error": "", "parent_ids": ["c8842d61-0d44-4883-b134-6ab817141dfd"], "operator": null, "metadata": {}}
{"id": "3a57e6a2-db4a-426a-a4c4-114d3106cef6", "fitness": 0.35837781499182136, "name": "AdaptiveCovarianceSamplingSimplified", "description": "Simplified adaptive covariance sampling with decay-based mean adaptation and dynamic exploration rate adjustment.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSamplingSimplified:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.1, exploration_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n        self.exploration_rate = exploration_rate\n\n        self.lb = -5.0\n        self.ub = 5.0\n\n        self.mean = np.zeros(dim)\n        self.variance = np.ones(dim)\n        self.best_x = []\n        self.best_f = []\n        self.age = 0\n\n    def sample(self, num_samples):\n        # Sample from normal distribution with diagonal covariance\n        x = np.random.normal(self.mean, np.sqrt(self.variance), size=(num_samples, self.dim))\n        \n        # Exploration: Randomly sample some dimensions, decaying exploration\n        effective_exploration_rate = self.exploration_rate / (1 + self.age / 1000)\n        mask = np.random.rand(num_samples, self.dim) < effective_exploration_rate\n        x[mask] = np.random.uniform(self.lb, self.ub, size=x[mask].shape)\n        \n        # Clip to bounds\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solutions\n        indices = np.argsort(f)\n        self.best_x.extend(x[indices[:min(10, self.initial_samples)]])\n        self.best_f.extend(f[indices[:min(10, self.initial_samples)]])\n\n        if np.min(f) < self.f_opt:\n            self.f_opt = np.min(f)\n            self.x_opt = x[np.argmin(f)]\n        \n        while self.budget > 0:\n            self.age += 1\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n            \n            # Update best solutions\n            indices = np.argsort(f)\n            self.best_x.extend(x[indices[:min(10, num_samples)]])\n            self.best_f.extend(f[indices[:min(10, num_samples)]])\n           \n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n\n            # Adaptation of mean and variance, simplified mean adaptation\n            if self.best_x:\n                best_x_arr = np.array(self.best_x)\n                self.mean = (1 - self.adaptation_rate) * self.mean + self.adaptation_rate * best_x_arr[np.argmin(np.array(self.best_f))] # Move mean towards best solution\n                self.variance = (1 - self.adaptation_rate) * self.variance + self.adaptation_rate * np.var(best_x_arr, axis=0)\n                self.variance = np.maximum(self.variance, 0.01) # Ensure variance is not zero\n\n            # Remove worst solutions if the list gets too long\n            if len(self.best_x) > 100:\n                indices = np.argsort(self.best_f)\n                self.best_x = list(np.array(self.best_x)[indices[:100]])\n                self.best_f = list(np.array(self.best_f)[indices[:100]])\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveCovarianceSamplingSimplified scored 0.358 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c8842d61-0d44-4883-b134-6ab817141dfd"], "operator": null, "metadata": {"aucs": [0.15760127157062176, 0.2621577300500211, 0.38681676031103396, 0.3675254931623496, 0.26590282704109314, 0.3482895675729908, 0.27244866184918215, 0.30454789643982316, 0.28968278654476887, 0.17964712363952606, 0.3808868993497986, 0.9981179480874315, 0.2890968367963491, 0.2795548654165132, 0.6536579493926923, 0.3417135002774683, 0.30672893317101557, 0.405658782562204, 0.1913183432873128, 0.4862021233142302]}}
{"id": "8736e89a-13a2-455d-a113-2412507ac863", "fitness": 0.33163525383122233, "name": "AdaptiveCovarianceSamplingSimplified", "description": "Simplified adaptive covariance sampling with dynamic exploration rate annealing and focused variance adaptation.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSamplingSimplified:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.1, initial_exploration_rate=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n        self.exploration_rate = initial_exploration_rate  # Initial exploration rate\n        self.exploration_decay = 0.995 # Decay rate for exploration\n        self.lb = -5.0\n        self.ub = 5.0\n\n        self.mean = np.zeros(dim)\n        self.variance = np.ones(dim)\n        self.best_x = []\n        self.best_f = []\n\n    def sample(self, num_samples):\n        x = np.random.normal(self.mean, np.sqrt(self.variance), size=(num_samples, self.dim))\n        \n        # Exploration with decaying exploration rate\n        mask = np.random.rand(num_samples, self.dim) < self.exploration_rate\n        x[mask] = np.random.uniform(self.lb, self.ub, size=x[mask].shape)\n        \n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solutions\n        indices = np.argsort(f)\n        best_idx = indices[:min(10, self.initial_samples)]\n        self.best_x.extend(x[best_idx])\n        self.best_f.extend(f[best_idx])\n\n        if np.min(f) < self.f_opt:\n            self.f_opt = np.min(f)\n            self.x_opt = x[np.argmin(f)]\n        \n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n            \n            # Update best solutions\n            indices = np.argsort(f)\n            best_idx = indices[:min(10, num_samples)]\n            self.best_x.extend(x[best_idx])\n            self.best_f.extend(f[best_idx])\n           \n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n\n            # Adaptation of mean and variance\n            num_best = min(len(self.best_x), 50)\n            if num_best > 0:\n                best_x_arr = np.array(self.best_x)[:num_best]\n                self.mean = (1 - self.adaptation_rate) * self.mean + self.adaptation_rate * np.mean(best_x_arr, axis=0)\n                # Focus adaptation on dimensions where best solutions deviate more from the mean\n                deviation = best_x_arr - self.mean\n                self.variance = (1 - self.adaptation_rate) * self.variance + self.adaptation_rate * np.var(deviation, axis=0)\n                self.variance = np.maximum(self.variance, 0.001)\n\n            # Reduce exploration rate\n            self.exploration_rate *= self.exploration_decay\n\n            # Remove worst solutions if the list gets too long, keep the best 100\n            if len(self.best_x) > 100:\n                indices = np.argsort(self.best_f)\n                self.best_x = list(np.array(self.best_x)[indices[:100]])\n                self.best_f = list(np.array(self.best_f)[indices[:100]])\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveCovarianceSamplingSimplified scored 0.332 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c8842d61-0d44-4883-b134-6ab817141dfd"], "operator": null, "metadata": {"aucs": [0.1419584008307727, 0.2429472472739328, 0.35857789732047884, 0.32233793327979443, 0.23893581312116452, 0.32178218427403205, 0.28632577518631286, 0.2858001677394695, 0.2765484375291898, 0.17658713338840515, 0.2409896376658479, 0.9979345050006954, 0.26754842062789774, 0.24779472800277313, 0.591443871982388, 0.31835402775546173, 0.29032810476840676, 0.3569669118925852, 0.1893325740040367, 0.4802113049808011]}}
{"id": "b9bbea71-67c4-45b7-8c5d-7d1bbeb1dedd", "fitness": 0.4166325994896972, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with a more aggressive adaptation rate and diagonal covariance matrix, enhancing exploration by simplifying the covariance estimation.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.2, diversity_threshold=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n        self.diversity_threshold = diversity_threshold\n\n        self.lb = -5.0\n        self.ub = 5.0\n\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = []\n        self.best_f = []\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            # Covariance matrix not positive semi-definite, fall back to sampling from the bounds\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        \n        # Clip to bounds\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solutions\n        indices = np.argsort(f)\n        self.best_x.extend(x[indices[:min(10, self.initial_samples)]])\n        self.best_f.extend(f[indices[:min(10, self.initial_samples)]])\n\n        if np.min(f) < self.f_opt:\n            self.f_opt = np.min(f)\n            self.x_opt = x[np.argmin(f)]\n        \n        \n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n            \n            # Update best solutions\n            indices = np.argsort(f)\n            self.best_x.extend(x[indices[:min(10, num_samples)]])\n            self.best_f.extend(f[indices[:min(10, num_samples)]])\n           \n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n\n            # Adaptation of mean and covariance\n            num_best = min(len(self.best_x), 20) # Reduced number of best solutions\n\n            if num_best > 1:\n                self.mean = (1 - self.adaptation_rate) * self.mean + self.adaptation_rate * np.mean(np.array(self.best_x)[:num_best], axis=0)\n                try:\n                    # Diagonal covariance update: focus on individual dimension variances\n                    variances = np.var(np.array(self.best_x)[:num_best], axis=0)\n                    self.covariance = (1 - self.adaptation_rate) * self.covariance + self.adaptation_rate * np.diag(variances)\n\n                except:\n                    pass # keep covariance as is\n\n            # Remove worst solutions if the list gets too long\n            if len(self.best_x) > 50:  # Reduced best solutions size\n                indices = np.argsort(self.best_f)\n                self.best_x = list(np.array(self.best_x)[indices[:50]])\n                self.best_f = list(np.array(self.best_f)[indices[:50]])\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.417 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["09a97fde-cca8-43b8-bdf8-3f1ad5fb3532"], "operator": null, "metadata": {"aucs": [0.14011464186919953, 0.3381802131630056, 0.4114244487439691, 0.6190702740761673, 0.35801805654294283, 0.466567028218299, 0.3199908443650128, 0.3933953256708288, 0.4107342017761344, 0.1882753165573362, 0.3281087651697515, 0.9716860240485485, 0.2647008105901508, 0.3722975274698349, 0.7124322605914041, 0.4218002632851495, 0.3630514863543264, 0.5507691071606349, 0.19785579403908515, 0.5041796001021646]}}
{"id": "4d73a50a-8ec0-426e-a403-e9af67208d26", "fitness": 0.25535690178238507, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with rank-based updates of mean and covariance, promoting exploration and exploitation balance.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n\n        self.lb = -5.0\n        self.ub = 5.0\n\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = []\n        self.best_f = []\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            # Covariance matrix not positive semi-definite, fall back to sampling from the bounds\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        \n        # Clip to bounds\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solutions\n        indices = np.argsort(f)\n        self.best_x = list(x[indices[:10]])\n        self.best_f = list(f[indices[:10]])\n\n        if np.min(f) < self.f_opt:\n            self.f_opt = np.min(f)\n            self.x_opt = x[np.argmin(f)]\n        \n        \n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n            \n            # Update best solutions\n            indices = np.argsort(f)\n            self.best_x.extend(x[indices[:5]])\n            self.best_f.extend(f[indices[:5]])\n           \n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n\n            # Rank-based adaptation of mean and covariance\n            num_best = min(len(self.best_x), 20)\n            \n            ranked_indices = np.argsort(self.best_f)\n            \n            selected_x = np.array(self.best_x)[ranked_indices[:num_best]]\n\n            if num_best > 1:\n                delta_mean = np.mean(selected_x, axis=0) - self.mean\n                self.mean = (1 - self.adaptation_rate) * self.mean + self.adaptation_rate * delta_mean\n                try:\n                    covariance_update = np.cov(selected_x.T)\n                    self.covariance = (1 - self.adaptation_rate) * self.covariance + self.adaptation_rate * covariance_update\n                except:\n                    pass\n\n\n            # Keep only the best solutions\n            combined_x = np.array(self.best_x)\n            combined_f = np.array(self.best_f)\n            \n            sort_idx = np.argsort(combined_f)\n            self.best_x = list(combined_x[sort_idx[:20]])\n            self.best_f = list(combined_f[sort_idx[:20]])\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.255 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["09a97fde-cca8-43b8-bdf8-3f1ad5fb3532"], "operator": null, "metadata": {"aucs": [0.09946097581119118, 0.21033115197593, 0.32871273585125027, 0.1679022174936372, 0.16911965628204417, 0.18601106685293, 0.22339152116328365, 0.2295092012650286, 0.19466336656166117, 0.17102502259250763, 0.2027749858334017, 0.9989985934239269, 0.2722839545591207, 0.16774969236178916, 0.15171357536925534, 0.2648615520066401, 0.22586728552669333, 0.19799487492000534, 0.18901278029077306, 0.45575382550663257]}}
{"id": "0dffdabe-822a-48dc-b705-72dbf1f28319", "fitness": 0.24652604428276342, "name": "AdaptiveCovarianceSampling", "description": "Adaptively samples based on covariance, focusing on exploration by periodically resetting the mean and covariance and using a simpler covariance update.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.1, reset_interval=500):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n        self.reset_interval = reset_interval  # Resetting the mean and covariance\n\n        self.lb = -5.0\n        self.ub = 5.0\n\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = []\n        self.best_f = []\n        self.eval_count = 0\n\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            # Covariance matrix not positive semi-definite, fall back to sampling from the bounds\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        \n        # Clip to bounds\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n        self.eval_count += self.initial_samples\n\n        # Update best solutions\n        indices = np.argsort(f)\n        self.best_x.extend(x[indices[:min(10, self.initial_samples)]])\n        self.best_f.extend(f[indices[:min(10, self.initial_samples)]])\n\n        if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n        \n        \n        while self.budget > 0:\n            # Resetting mechanism\n            if self.eval_count > self.reset_interval:\n                self.mean = np.zeros(self.dim)\n                self.covariance = np.eye(self.dim)\n                self.eval_count = 0\n                self.best_x = []\n                self.best_f = []\n\n\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n            self.eval_count += num_samples\n            \n            # Update best solutions\n            indices = np.argsort(f)\n            self.best_x.extend(x[indices[:min(10, num_samples)]])\n            self.best_f.extend(f[indices[:min(10, num_samples)]])\n           \n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n\n            # Adaptation of mean and covariance\n            num_best = min(len(self.best_x), 10)\n\n            if num_best > 1:\n                self.mean = (1 - self.adaptation_rate) * self.mean + self.adaptation_rate * np.mean(np.array(self.best_x)[:num_best], axis=0)\n                # Simplified covariance update using only diagonal elements\n                std = np.std(np.array(self.best_x)[:num_best], axis=0)\n                self.covariance = (1 - self.adaptation_rate) * self.covariance + self.adaptation_rate * np.diag(std**2 + 1e-8)\n\n\n            # Remove worst solutions if the list gets too long\n            if len(self.best_x) > 30:\n                indices = np.argsort(self.best_f)\n                self.best_x = list(np.array(self.best_x)[indices[:30]])\n                self.best_f = list(np.array(self.best_f)[indices[:30]])\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.247 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["09a97fde-cca8-43b8-bdf8-3f1ad5fb3532"], "operator": null, "metadata": {"aucs": [0.10195274385595854, 0.18895971049886062, 0.3202758149869426, 0.1679292759453812, 0.16094433325069457, 0.1702062423118199, 0.23356318008627208, 0.20354872617921693, 0.19147337824055988, 0.15548866566185915, 0.18159006315662862, 0.9967043032745968, 0.24997190203962372, 0.16189458127402645, 0.15951097763196065, 0.2728743958131308, 0.2305465212184048, 0.17759773496288567, 0.17002392399415533, 0.43546441127229074]}}
{"id": "c372a95b-0d02-4578-84f1-7056c98bcd9e", "fitness": 0.3057771505514612, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with rank-based mean update and covariance regularization for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.1, rank_adaptation_rate=0.05, covariance_decay=0.001):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n        self.rank_adaptation_rate = rank_adaptation_rate\n        self.covariance_decay = covariance_decay # Regularization parameter\n\n        self.lb = -5.0\n        self.ub = 5.0\n\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.archive_x = []\n        self.archive_f = []\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            # Covariance matrix not positive semi-definite, fall back to sampling from the bounds\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        \n        # Clip to bounds\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update archive\n        indices = np.argsort(f)\n        self.archive_x.extend(x[indices[:min(10, self.initial_samples)]])\n        self.archive_f.extend(f[indices[:min(10, self.initial_samples)]])\n\n        if np.min(f) < self.f_opt:\n            self.f_opt = np.min(f)\n            self.x_opt = x[np.argmin(f)]\n        \n        \n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n            \n            # Update archive\n            indices = np.argsort(f)\n            self.archive_x.extend(x[indices[:min(10, num_samples)]])\n            self.archive_f.extend(f[indices[:min(10, num_samples)]])\n           \n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n\n            # Select top solutions for adaptation\n            num_best = min(len(self.archive_x), 20)\n            indices = np.argsort(self.archive_f)\n            best_x = np.array(self.archive_x)[indices[:num_best]]\n\n            # Rank-based mean update\n            weights = np.arange(num_best, 0, -1)\n            weights = weights / np.sum(weights)\n            self.mean = (1 - self.rank_adaptation_rate) * self.mean + self.rank_adaptation_rate * np.sum(best_x * weights[:, np.newaxis], axis=0)\n\n            # Covariance update with regularization\n            try:\n                covariance_update = np.cov(best_x.T)\n                self.covariance = (1 - self.adaptation_rate) * self.covariance + self.adaptation_rate * covariance_update + self.covariance_decay * np.eye(self.dim)  # Regularization\n            except:\n                self.covariance += self.covariance_decay * np.eye(self.dim) # keep covariance as is, but add regularization\n\n\n            # Archive management: Keep only the best\n            archive_size = 50\n            if len(self.archive_x) > archive_size:\n                indices = np.argsort(self.archive_f)\n                self.archive_x = list(np.array(self.archive_x)[indices[:archive_size]])\n                self.archive_f = list(np.array(self.archive_f)[indices[:archive_size]])\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.306 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["09a97fde-cca8-43b8-bdf8-3f1ad5fb3532"], "operator": null, "metadata": {"aucs": [0.12504425402920682, 0.24503573454641847, 0.3432763282747442, 0.2032987452385402, 0.2251171760417826, 0.19477448480666082, 0.23957430074347563, 0.22110449792690479, 0.21179761766158367, 0.17948522798611355, 0.2890737293707023, 0.9970062541892611, 0.27409158415534385, 0.20975017617332448, 0.6009157390852331, 0.2869216505629447, 0.27056926725957897, 0.317448579143195, 0.20436358668301968, 0.4768940771511905]}}
{"id": "2522720d-7169-40ed-88d4-97ed5ad6a85d", "fitness": 0.3257438793163744, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with a simplified covariance adaptation using exponential moving average and diagonal dominance.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.1, diagonal_decay=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n        self.diagonal_decay = diagonal_decay\n\n        self.lb = -5.0\n        self.ub = 5.0\n\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = []\n        self.best_f = []\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            # Covariance matrix not positive semi-definite, fall back to sampling from the bounds\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        \n        # Clip to bounds\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solutions\n        indices = np.argsort(f)\n        self.best_x.extend(x[indices[:min(10, self.initial_samples)]])\n        self.best_f.extend(f[indices[:min(10, self.initial_samples)]])\n\n        if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n        \n        \n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n            \n            # Update best solutions\n            indices = np.argsort(f)\n            self.best_x.extend(x[indices[:min(10, num_samples)]])\n            self.best_f.extend(f[indices[:min(10, num_samples)]])\n           \n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n\n            # Adaptation of mean and covariance\n            num_best = min(len(self.best_x), 50) # Consider only a limited number of best solutions\n            \n            if num_best > 1: # Need at least two points to calculate covariance\n                \n                self.mean = (1 - self.adaptation_rate) * self.mean + self.adaptation_rate * np.mean(np.array(self.best_x)[:num_best], axis=0)\n                \n                # Robust covariance estimation using exponential moving average\n                X = np.array(self.best_x)[:num_best]\n                sample_covariance = np.cov(X.T)\n\n                # Diagonal dominance and EMA update\n                self.covariance = (1 - self.adaptation_rate) * self.covariance + self.adaptation_rate * sample_covariance\n                self.covariance = self.diagonal_decay * self.covariance + (1 - self.diagonal_decay) * np.diag(np.diag(self.covariance))\n\n            # Remove worst solutions if the list gets too long\n            if len(self.best_x) > 100:\n                indices = np.argsort(self.best_f)\n                self.best_x = list(np.array(self.best_x)[indices[:100]])\n                self.best_f = list(np.array(self.best_f)[indices[:100]])\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.326 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b6725372-df01-4293-92c1-e426636f34a5"], "operator": null, "metadata": {"aucs": [0.12132725681556111, 0.25382822509033154, 0.3633086457657756, 0.24308472167608486, 0.23390625727889824, 0.3200876374194861, 0.2667217798296079, 0.29437351314637994, 0.2666345103143506, 0.183720736124664, 0.28204329815183593, 0.9940947962764795, 0.2710779408358125, 0.23912755508705463, 0.523225311453778, 0.32411850409306897, 0.29904879733941114, 0.3710591935707287, 0.18383457454128482, 0.48025433151689456]}}
{"id": "1aa57f62-c9c4-4905-a6a8-640e0026101d", "fitness": 0.3126945414093733, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with simplified diversity control, adaptive learning rate, and covariance shrinkage for stability.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.1, diversity_threshold=0.01, shrinkage_factor=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n        self.diversity_threshold = diversity_threshold\n        self.shrinkage_factor = shrinkage_factor\n\n        self.lb = -5.0\n        self.ub = 5.0\n\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = []\n        self.best_f = []\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            # Covariance matrix not positive semi-definite, fall back to sampling from the bounds\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        \n        # Clip to bounds\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solutions\n        indices = np.argsort(f)\n        self.best_x.extend(x[indices[:min(10, self.initial_samples)]])\n        self.best_f.extend(f[indices[:min(10, self.initial_samples)]])\n\n        if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n        \n        \n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n            \n            # Update best solutions\n            indices = np.argsort(f)\n            self.best_x.extend(x[indices[:min(10, num_samples)]])\n            self.best_f.extend(f[indices[:min(10, num_samples)]])\n           \n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n\n            # Adaptation of mean and covariance\n            num_best = min(len(self.best_x), 50) # Consider only a limited number of best solutions\n            \n            if num_best > 1: # Need at least two points to calculate covariance\n                \n                best_x_array = np.array(self.best_x)[:num_best]\n                mean_old = self.mean.copy()\n                self.mean = (1 - self.adaptation_rate) * self.mean + self.adaptation_rate * np.mean(best_x_array, axis=0)\n                \n                try:\n                    cov = np.cov(best_x_array.T)\n                    \n                    # Shrinkage\n                    cov = (1 - self.shrinkage_factor) * cov + self.shrinkage_factor * np.diag(np.diag(cov))\n\n                    # Adapt learning rate based on the change in mean\n                    mean_change = np.linalg.norm(self.mean - mean_old)\n                    adaptive_adaptation_rate = self.adaptation_rate / (1 + mean_change)\n                    \n                    #Diversity check: encourage diagonal covariance\n                    if np.sum(np.abs(np.diag(cov) - np.diag(self.covariance))) > self.diversity_threshold:\n                        self.covariance = (1 - adaptive_adaptation_rate) * self.covariance + adaptive_adaptation_rate * cov\n                    else:\n                         self.covariance = (1 - adaptive_adaptation_rate) * self.covariance + adaptive_adaptation_rate * np.diag(np.var(np.random.uniform(self.lb, self.ub, size=(100, self.dim)), axis=0))\n\n                except:\n                    pass # keep covariance as is\n\n            # Remove worst solutions if the list gets too long\n            if len(self.best_x) > 100:\n                indices = np.argsort(self.best_f)\n                self.best_x = list(np.array(self.best_x)[indices[:100]])\n                self.best_f = list(np.array(self.best_f)[indices[:100]])\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.313 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["79df7d10-e9b2-48c0-9ad6-0aea3a3a2998"], "operator": null, "metadata": {"aucs": [0.13790456825448405, 0.263261851565708, 0.3704615221601373, 0.2467054007077898, 0.2489011998145616, 0.3207573579539019, 0.27743859818521055, 0.29680472708117, 0.26554675810427164, 0.18540822914877675, 0.26716262786533107, 0.9976893869142002, 0.2890840408153573, 0.21952111491194148, 0.2017654061969011, 0.31465047041988425, 0.29402468708044227, 0.39389225342239564, 0.19491050099288942, 0.4680001265921131]}}
{"id": "6635e60f-2c6d-4a24-977b-97031fffe1f2", "fitness": 0.28828164654165067, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with rank-based selection and covariance regularization for stability and exploration.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=50, adaptation_rate=0.1, rank_cutoff=10):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n        self.rank_cutoff = rank_cutoff  # Number of top solutions to consider for covariance update\n\n        self.lb = -5.0\n        self.ub = 5.0\n\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = []\n        self.best_f = []\n        self.n_evals = 0  # Keep track of the number of evaluations\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            # Covariance matrix not positive semi-definite, fall back to sampling from the bounds\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        \n        # Clip to bounds\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.n_evals += self.initial_samples\n        self.budget -= self.initial_samples\n\n        # Update best solutions\n        indices = np.argsort(f)\n        self.best_x.extend(x[indices[:min(self.rank_cutoff, self.initial_samples)]])\n        self.best_f.extend(f[indices[:min(self.rank_cutoff, self.initial_samples)]])\n\n        if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n        \n        \n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.n_evals += num_samples\n            self.budget -= num_samples\n            \n            # Update best solutions\n            indices = np.argsort(f)\n            self.best_x.extend(x[indices[:min(self.rank_cutoff, num_samples)]])\n            self.best_f.extend(f[indices[:min(self.rank_cutoff, num_samples)]])\n           \n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n\n            # Adaptation of mean and covariance\n            num_best = min(len(self.best_x), self.rank_cutoff) # Consider only a limited number of best solutions\n            \n            if num_best > 1: # Need at least two points to calculate covariance\n                \n                self.mean = (1 - self.adaptation_rate) * self.mean + self.adaptation_rate * np.mean(np.array(self.best_x)[:num_best], axis=0)\n                \n                # Robust covariance estimation using shrinkage\n                X = np.array(self.best_x)[:num_best]\n                \n                sample_covariance = np.cov(X.T)\n                \n                # Regularize covariance matrix\n                reg = 1e-6  # Regularization factor (can be tuned)\n                self.covariance = sample_covariance + reg * np.eye(self.dim)\n            else:\n                # If there's not enough data, reset the covariance to identity\n                self.covariance = np.eye(self.dim)\n\n            # Remove worst solutions if the list gets too long\n            if len(self.best_x) > 100:\n                indices = np.argsort(self.best_f)\n                self.best_x = list(np.array(self.best_x)[indices[:100]])\n                self.best_f = list(np.array(self.best_f)[indices[:100]])\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.288 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b6725372-df01-4293-92c1-e426636f34a5"], "operator": null, "metadata": {"aucs": [0.11052106324004618, 0.23098311118870274, 0.2889799532850812, 0.2474238905963111, 0.20355781160560416, 0.2242856879241084, 0.2352957450076426, 0.19583884712661903, 0.2083760853948735, 0.1731653975436911, 0.24070662395996778, 0.9968754686633027, 0.29215282185532776, 0.19923248380305358, 0.4639227276686181, 0.3126458848347632, 0.22660684220352145, 0.19376538895122664, 0.25518368341612696, 0.4661134125644256]}}
{"id": "3b3afd82-ae51-4a55-9e42-c92113aae195", "fitness": 0.3941996715495542, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with mean adaptation and covariance update using only the best point to enhance convergence speed.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n\n        self.lb = -5.0\n        self.ub = 5.0\n\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.inf\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            # Covariance matrix not positive semi-definite, fall back to sampling from the bounds\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        \n        # Clip to bounds\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n        \n        best_index = np.argmin(f)\n        if f[best_index] < self.best_f:\n            self.best_f = f[best_index]\n            self.best_x = x[best_index]\n                \n        if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n        \n        \n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n            \n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n                \n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n\n            # Adaptation of mean and covariance\n            if self.best_x is not None:                \n                self.mean = (1 - self.adaptation_rate) * self.mean + self.adaptation_rate * self.best_x\n                \n                # Update covariance based on the direction of the best solution\n                diff = self.best_x - self.mean\n                self.covariance = (1 - self.adaptation_rate) * self.covariance + self.adaptation_rate * np.outer(diff, diff) + self.adaptation_rate * np.eye(self.dim) * 1e-6\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.394 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b6725372-df01-4293-92c1-e426636f34a5"], "operator": null, "metadata": {"aucs": [0.13235183095023384, 0.27004104506692894, 0.41725036846006847, 0.17639569744124273, 0.33226751024459167, 0.4299857964614471, 0.31120323290303054, 0.3588584327825052, 0.36954592351247706, 0.18641852820767824, 0.6044872373529877, 0.9999121907833786, 0.2769577388101312, 0.31583998041114414, 0.7703123098642034, 0.39254414140508975, 0.35840301631797455, 0.4982369138165794, 0.19662172452710913, 0.48635981167228237]}}
{"id": "dc987c6f-5ee4-4cf4-ae9d-c204a613966e", "fitness": 0.37681302367133157, "name": "AdaptiveCovarianceSamplingSimplified", "description": "Adaptively samples using a simplified (diagonal) covariance matrix, focusing on efficient variance control and a reduced memory footprint for past best solutions.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSamplingSimplified:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.1, exploration_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n        self.exploration_rate = exploration_rate\n\n        self.lb = -5.0\n        self.ub = 5.0\n\n        self.mean = np.zeros(dim)\n        self.variance = np.ones(dim)  # Diagonal covariance\n        self.best_x = None # Store only the single best solution\n        self.best_f = np.inf\n\n    def sample(self, num_samples):\n        # Sample from normal distribution with diagonal covariance\n        x = np.random.normal(self.mean, np.sqrt(self.variance), size=(num_samples, self.dim))\n        \n        # Exploration: Randomly sample some dimensions\n        mask = np.random.rand(num_samples, self.dim) < self.exploration_rate\n        x[mask] = np.random.uniform(self.lb, self.ub, size=x[mask].shape)\n        \n        # Clip to bounds\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solution\n        best_idx = np.argmin(f)\n        if f[best_idx] < self.best_f:\n            self.best_f = f[best_idx]\n            self.best_x = x[best_idx]\n        \n        if np.min(f) < self.f_opt:\n            self.f_opt = np.min(f)\n            self.x_opt = x[np.argmin(f)]\n        \n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n            \n            # Update best solution\n            best_idx = np.argmin(f)\n            if f[best_idx] < self.best_f:\n                self.best_f = f[best_idx]\n                self.best_x = x[best_idx]\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n\n            # Adaptation of mean and variance\n            if self.best_x is not None:\n                self.mean = (1 - self.adaptation_rate) * self.mean + self.adaptation_rate * self.best_x\n                self.variance = (1 - self.adaptation_rate) * self.variance + self.adaptation_rate * (self.best_x - self.mean)**2\n                self.variance = np.maximum(self.variance, 0.01) # Ensure variance is not zero\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveCovarianceSamplingSimplified scored 0.377 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c8842d61-0d44-4883-b134-6ab817141dfd"], "operator": null, "metadata": {"aucs": [0.14991798605383932, 0.23420236422233254, 0.39388487359720004, 0.4197025015797178, 0.3051251176683407, 0.3714156812431939, 0.2835573651761095, 0.3174448086174576, 0.309989798176192, 0.19913618855414872, 0.447855813770765, 0.9931767681285263, 0.2781850990742454, 0.30753359525321133, 0.6753218416469293, 0.3640374169052555, 0.3268605171253677, 0.4347272046351849, 0.23799215337338153, 0.48619337862523304]}}
{"id": "1a7b7704-16f7-4ef0-b919-d94cb682eb10", "fitness": 0.3445683439764413, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with simplified covariance update using exponential moving average and added noise scaling based on function evaluations.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.1, noise_multiplier=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n        self.noise_multiplier = noise_multiplier\n\n        self.lb = -5.0\n        self.ub = 5.0\n\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.inf\n        self.eval_count = 0\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            # Covariance matrix not positive semi-definite, fall back to sampling from the bounds\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        \n        # Clip to bounds\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n        self.eval_count += self.initial_samples\n        \n        best_index = np.argmin(f)\n        if f[best_index] < self.best_f:\n            self.best_f = f[best_index]\n            self.best_x = x[best_index]\n                \n        if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n        \n        \n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n            self.eval_count += num_samples\n            \n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n                \n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n\n            # Adaptation of mean and covariance\n            if self.best_x is not None:                \n                self.mean = (1 - self.adaptation_rate) * self.mean + self.adaptation_rate * self.best_x\n                \n                # Update covariance based on the direction of the best solution using exponential moving average\n                diff = self.best_x - self.mean\n                outer_product = np.outer(diff, diff)\n                self.covariance = (1 - self.adaptation_rate) * self.covariance + self.adaptation_rate * outer_product\n                \n                # Add scaled noise based on evaluation count\n                noise_scale = self.noise_multiplier * (1 - (self.eval_count / 10000))\n                self.covariance += noise_scale * np.eye(self.dim) # Added noise\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.345 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3b3afd82-ae51-4a55-9e42-c92113aae195"], "operator": null, "metadata": {"aucs": [0.12445069198232173, 0.23421828633388764, 0.34887961405461443, 0.3489484853547058, 0.2574965977104714, 0.32009529852650165, 0.27301205965513076, 0.2838763637364685, 0.2606714026966823, 0.20063613776689615, 0.33348825859472186, 0.9993275869542183, 0.2899887408849938, 0.2612871926898148, 0.6934847139461828, 0.33299701440370166, 0.2865496985761692, 0.37323939630809366, 0.18020462714337504, 0.48851471220987464]}}
{"id": "2abbaf32-2b51-4f04-ad4b-e2f92ff64ce5", "fitness": 0.33434516020395905, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with simplified covariance update using exponential moving average and dynamic exploration noise.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.1, exploration_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n        self.exploration_rate = exploration_rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.inf\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            # Covariance matrix not positive semi-definite, fall back to sampling from the bounds\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        \n        # Clip to bounds\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n        \n        best_index = np.argmin(f)\n        if f[best_index] < self.best_f:\n            self.best_f = f[best_index]\n            self.best_x = x[best_index]\n                \n        if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n        \n        \n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n            \n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n                \n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n\n            # Adaptation of mean and covariance\n            if self.best_x is not None:                \n                self.mean = (1 - self.adaptation_rate) * self.mean + self.adaptation_rate * self.best_x\n                \n                # Update covariance based on the direction of the best solution\n                diff = self.best_x - self.mean\n                \n                # Simplified covariance update using exponential moving average\n                self.covariance = (1 - self.adaptation_rate) * self.covariance + self.adaptation_rate * np.outer(diff, diff) \n                \n                # Add dynamic exploration noise\n                self.covariance += self.exploration_rate * np.eye(self.dim)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.334 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3b3afd82-ae51-4a55-9e42-c92113aae195"], "operator": null, "metadata": {"aucs": [0.1464263013639917, 0.2217051254291843, 0.3370102669895667, 0.284293012027531, 0.25914424159361305, 0.31123347752236197, 0.26750693099587997, 0.27192527077536466, 0.2587365364391202, 0.1818778153318843, 0.3129270623108872, 0.9973758372525997, 0.25588129080590327, 0.2563051680007753, 0.688860901613978, 0.32581892519742317, 0.28282275518282096, 0.3641526921524071, 0.18012048678822412, 0.48277910630566456]}}
{"id": "4df89f3a-ab91-4023-8658-52b5f33f781a", "fitness": 0.3412655807106476, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with a simplified rank-one covariance update and decaying mean adaptation for enhanced exploration and exploitation balance.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.1, decay_rate=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n        self.decay_rate = decay_rate  # Decay rate for mean adaptation\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.inf\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solution\n        best_index = np.argmin(f)\n        if f[best_index] < self.best_f:\n            self.best_f = f[best_index]\n            self.best_x = x[best_index]\n            self.f_opt = self.best_f\n            self.x_opt = self.best_x\n        \n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n            \n            # Update best solution\n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n                self.f_opt = self.best_f\n                self.x_opt = self.best_x\n\n            # Adaptation of mean and covariance using rank-one update\n            if self.best_x is not None:\n                # Decaying mean adaptation\n                self.mean = (1 - self.decay_rate) * self.mean + self.decay_rate * self.best_x\n\n                # Rank-one covariance update\n                diff = (self.best_x - self.mean).reshape(-1, 1)\n                self.covariance = (1 - self.adaptation_rate) * self.covariance + self.adaptation_rate * (diff @ diff.T)\n                \n                # Ensure covariance is positive semi-definite (add small value to diagonal)\n                self.covariance += 1e-8 * np.eye(self.dim)\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.341 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b9bbea71-67c4-45b7-8c5d-7d1bbeb1dedd"], "operator": null, "metadata": {"aucs": [0.1246754996806807, 0.1782259747918069, 0.38186754276496915, 0.16867376014832358, 0.2716250112741635, 0.3436131524525914, 0.27478564542326656, 0.32125810624915807, 0.29402733920108515, 0.18198166308732755, 0.4308648573957863, 0.9690814636635346, 0.2934042578409982, 0.2523386901809781, 0.6756286909513953, 0.2825068818829556, 0.33362003000170626, 0.4205073949466924, 0.1757109834910261, 0.4509146687845058]}}
{"id": "606f4d6f-b0da-4fdc-8a91-16de66f8ca36", "fitness": 0.36143964741644974, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with simplified rank-one covariance updates and adaptive learning rate, balancing exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.1, learning_rate_decay=0.99):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n        self.learning_rate_decay = learning_rate_decay\n\n        self.lb = -5.0\n        self.ub = 5.0\n\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.inf\n        self.t = 0 #iteration counter\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            # Covariance matrix not positive semi-definite, fall back to sampling from the bounds\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        \n        # Clip to bounds\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n        self.t += self.initial_samples\n        \n        best_index = np.argmin(f)\n        if f[best_index] < self.best_f:\n            self.best_f = f[best_index]\n            self.best_x = x[best_index]\n                \n        if np.min(f) < self.f_opt:\n            self.f_opt = np.min(f)\n            self.x_opt = x[np.argmin(f)]\n        \n        \n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n            self.t += num_samples\n            \n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n                \n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n\n            # Adaptation of mean and covariance\n            if self.best_x is not None:\n                #Decay the learning rate over time\n                adaptive_rate = self.adaptation_rate * (self.learning_rate_decay ** (self.t / self.initial_samples))\n                \n                diff = self.best_x - self.mean\n                self.mean = (1 - adaptive_rate) * self.mean + adaptive_rate * self.best_x\n                \n                # Rank-one update for covariance\n                self.covariance = (1 - adaptive_rate) * self.covariance + adaptive_rate * np.outer(diff, diff)\n                \n                #Ensure covariance is positive definite\n                self.covariance += np.eye(self.dim) * 1e-6\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.361 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3b3afd82-ae51-4a55-9e42-c92113aae195"], "operator": null, "metadata": {"aucs": [0.12572956685013492, 0.2474467386167697, 0.3839229018019239, 0.19519366173080022, 0.30772042372138275, 0.3666957322809973, 0.26894736324731383, 0.3256061471680253, 0.3031983132473086, 0.2041573408005657, 0.4222737879904416, 0.9765183358661508, 0.31038041684000506, 0.28364469578817875, 0.7029456438476623, 0.3593451672243222, 0.32436548393936293, 0.4446457053374815, 0.1997893862439094, 0.47626613578625865]}}
{"id": "7de6636e-6ceb-40c7-8b4c-6ce0ecd2e8c9", "fitness": 0.5018864729482647, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with dynamic adaptation rate based on fitness improvement and simplified covariance estimation using only the best solution.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.Inf\n        self.success_rate = 0.0\n        self.success_history = []\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solution\n        best_index = np.argmin(f)\n        if f[best_index] < self.best_f:\n            self.best_f = f[best_index]\n            self.best_x = x[best_index]\n\n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n\n            # Update best solution\n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n                self.success_history.append(1)\n            else:\n                 self.success_history.append(0)\n                 \n            # Dynamic adaptation rate based on success\n            window_size = min(len(self.success_history), 20)\n            self.success_rate = np.mean(self.success_history[-window_size:]) if self.success_history else 0.0\n\n            if self.success_rate > 0.2:\n                self.adaptation_rate = min(self.adaptation_rate * 1.1, 0.5)  # Increase rate if doing well\n            else:\n                self.adaptation_rate = max(self.adaptation_rate * 0.9, 0.01)   # Decrease if not improving\n          \n            # Adaptation of mean and covariance using only the best\n            self.mean = (1 - self.adaptation_rate) * self.mean + self.adaptation_rate * self.best_x\n            \n            # Diagonal covariance update focusing on best solution's deviation\n            deviation = self.best_x - self.mean\n            self.covariance = (1 - self.adaptation_rate) * self.covariance + self.adaptation_rate * np.diag(deviation**2)\n\n        return self.best_f, self.best_x", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.502 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b9bbea71-67c4-45b7-8c5d-7d1bbeb1dedd"], "operator": null, "metadata": {"aucs": [0.1434890194147065, 0.21730309478518184, 0.7868899211329057, 0.6475861725842931, 0.2399866932080862, 0.7744116204379016, 0.3156278192779167, 0.41985183511332025, 0.3173474810413037, 0.18280081242279933, 0.8147042198792209, 0.9988163883302565, 0.25594187290343884, 0.7582231420837429, 0.7001171431073259, 0.32772446534519817, 0.3791331003351739, 0.8206958271870536, 0.4572515825458937, 0.4798272478295762]}}
{"id": "a8496a14-e89a-455e-868a-4581a86590e9", "fitness": 0.31344864667837014, "name": "AdaptiveCovarianceSamplingSimplified", "description": "Simplified adaptive covariance sampling with decaying exploration and adaptive variance thresholding to enhance robustness.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSamplingSimplified:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.1, exploration_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n        self.exploration_rate = exploration_rate\n\n        self.lb = -5.0\n        self.ub = 5.0\n\n        self.mean = np.zeros(dim)\n        self.variance = np.ones(dim)\n        self.best_x = []\n        self.best_f = []\n        self.age = 0\n        self.min_variance = 0.01 # Minimum variance threshold\n\n    def sample(self, num_samples):\n        # Sample from normal distribution with diagonal covariance\n        x = np.random.normal(self.mean, np.sqrt(self.variance), size=(num_samples, self.dim))\n        \n        # Exploration: Randomly sample some dimensions, decaying exploration\n        effective_exploration_rate = self.exploration_rate / (1 + self.age / 1000)\n        mask = np.random.rand(num_samples, self.dim) < effective_exploration_rate\n        x[mask] = np.random.uniform(self.lb, self.ub, size=x[mask].shape)\n        \n        # Clip to bounds\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solution\n        best_index = np.argmin(f)\n        if f[best_index] < self.f_opt:\n            self.f_opt = f[best_index]\n            self.x_opt = x[best_index]\n        \n        while self.budget > 0:\n            self.age += 1\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n            \n            # Update best solution\n            best_index = np.argmin(f)\n            if f[best_index] < self.f_opt:\n                self.f_opt = f[best_index]\n                self.x_opt = x[best_index]\n\n            # Adaptation of mean and variance, simplified mean adaptation\n            self.mean = (1 - self.adaptation_rate) * self.mean + self.adaptation_rate * self.x_opt # Move mean towards best solution\n            self.variance = (1 - self.adaptation_rate) * self.variance + self.adaptation_rate * np.var(x, axis=0)\n            self.variance = np.maximum(self.variance, self.min_variance) # Ensure variance is not zero\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveCovarianceSamplingSimplified scored 0.313 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3a57e6a2-db4a-426a-a4c4-114d3106cef6"], "operator": null, "metadata": {"aucs": [0.13275344765298158, 0.19890910762414737, 0.2984261504851453, 0.26521901033475304, 0.2270725684631757, 0.2885686169182552, 0.26211469867450843, 0.23697993067736256, 0.23435022972425246, 0.16065070954020033, 0.27274907684114624, 0.9992187267951682, 0.2602382019334155, 0.23675463887023385, 0.6673914981749509, 0.30529146600069756, 0.25642011243005114, 0.32205591625909225, 0.17007626236564388, 0.47373256380222084]}}
{"id": "06f8b842-0024-44b2-a2b5-592fcb5c858b", "fitness": 0.3513230370017587, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with simplified covariance update using a running average of the best solution's displacement and an exponentially decaying adaptation rate.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.2, decay_rate=0.99):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n        self.decay_rate = decay_rate\n\n        self.lb = -5.0\n        self.ub = 5.0\n\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.inf\n        self.displacement_avg = np.zeros(dim)\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n        \n        best_index = np.argmin(f)\n        if f[best_index] < self.best_f:\n            self.best_f = f[best_index]\n            self.best_x = x[best_index]\n            \n        if np.min(f) < self.f_opt:\n            self.f_opt = np.min(f)\n            self.x_opt = x[np.argmin(f)]\n            \n        while self.budget > 0:\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n            \n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n                \n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n\n            if self.best_x is not None:\n                displacement = self.best_x - self.mean\n                self.displacement_avg = (1 - self.adaptation_rate) * self.displacement_avg + self.adaptation_rate * displacement\n                self.mean = self.mean + self.adaptation_rate * self.displacement_avg\n                \n                self.covariance = (1 - self.adaptation_rate) * self.covariance + self.adaptation_rate * np.outer(self.displacement_avg, self.displacement_avg) + np.eye(self.dim) * 1e-6\n\n                self.adaptation_rate *= self.decay_rate\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.351 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3b3afd82-ae51-4a55-9e42-c92113aae195"], "operator": null, "metadata": {"aucs": [0.12820324334442357, 0.2045708479430811, 0.4546109523073606, 0.1915146599672647, 0.33538951204751033, 0.3339085111483663, 0.26023735593214936, 0.36475111444317276, 0.42695387938808826, 0.17763966903642836, 0.42277862954349743, 0.9974011573269252, 0.3217045791480636, 0.28431909020154966, 0.5490401372282911, 0.34935594771565115, 0.2761798377476762, 0.3358243143308528, 0.16702611838878456, 0.44505118284603695]}}
{"id": "18f6690e-59ed-41e4-8d7e-0ab7c0ff0fa8", "fitness": 0.3680797928047905, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with simplified covariance update based on the best point's movement, combined with a dynamic adaptation rate based on the success history.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.1, success_history=10):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n        self.success_history_length = success_history\n\n        self.lb = -5.0\n        self.ub = 5.0\n\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.inf\n        self.success_history = []\n\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            # Covariance matrix not positive semi-definite, fall back to sampling from the bounds\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        \n        # Clip to bounds\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n        \n        best_index = np.argmin(f)\n        if f[best_index] < self.best_f:\n            self.best_f = f[best_index]\n            self.best_x = x[best_index]\n            self.success_history.append(1)\n        else:\n            self.success_history.append(0)\n                \n        if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n        \n        \n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n            \n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n                self.success_history.append(1)\n            else:\n                self.success_history.append(0)\n\n                \n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n\n            # Adaptation of mean and covariance\n            if self.best_x is not None:                \n                self.mean = (1 - self.adaptation_rate) * self.mean + self.adaptation_rate * self.best_x\n                \n                # Update covariance based on the direction of the best solution\n                diff = self.best_x - self.mean\n                self.covariance = (1 - self.adaptation_rate) * self.covariance + self.adaptation_rate * np.outer(diff, diff) + np.eye(self.dim) * 1e-8 #removed the multiplication by adaptation_rate\n\n            if len(self.success_history) > self.success_history_length:\n                self.success_history = self.success_history[-self.success_history_length:]\n            \n            success_rate = np.mean(self.success_history) if self.success_history else 0.5  # Default to 0.5 if history is empty\n            self.adaptation_rate = min(0.5, max(0.01, 0.1 * success_rate / 0.5)) # Adjust adaptation rate dynamically\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.368 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3b3afd82-ae51-4a55-9e42-c92113aae195"], "operator": null, "metadata": {"aucs": [0.11510232131441067, 0.21844435386632888, 0.3890521986479012, 0.43657428151588007, 0.2673727739814131, 0.365447647840479, 0.2840883069828215, 0.31518510788772724, 0.28315976787775043, 0.18142908429410787, 0.7072569137646574, 0.9982534766405554, 0.2566188869980235, 0.27491505464579435, 0.5498515459536177, 0.3459790862608607, 0.29890454730679716, 0.4036319501319765, 0.18550856615799138, 0.4848199840267148]}}
{"id": "58f1461a-6cfc-4d38-a1b1-d72a3cb22e6f", "fitness": 0.3262211035716344, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with simplified diagonal covariance update based on a moving average of past variances for enhanced stability and exploration.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.1, variance_memory=10):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n        self.variance_memory = variance_memory\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.variances_history = []\n        self.best_x = []\n        self.best_f = []\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        indices = np.argsort(f)\n        self.best_x.extend(x[indices[:min(10, self.initial_samples)]])\n        self.best_f.extend(f[indices[:min(10, self.initial_samples)]])\n\n        if np.min(f) < self.f_opt:\n            self.f_opt = np.min(f)\n            self.x_opt = x[np.argmin(f)]\n        \n        while self.budget > 0:\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n            \n            indices = np.argsort(f)\n            self.best_x.extend(x[indices[:min(5, num_samples)]])  # Keep fewer new best samples\n            self.best_f.extend(f[indices[:min(5, num_samples)]])\n           \n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n\n            num_best = min(len(self.best_x), 15) # Reduced number of best solutions\n\n            if num_best > 1:\n                best_x_arr = np.array(self.best_x)[:num_best]\n                self.mean = (1 - self.adaptation_rate) * self.mean + self.adaptation_rate * np.mean(best_x_arr, axis=0)\n                \n                variances = np.var(best_x_arr, axis=0)\n                self.variances_history.append(variances)\n                \n                if len(self.variances_history) > self.variance_memory:\n                    self.variances_history.pop(0)\n                \n                # Moving average of variances\n                smoothed_variances = np.mean(np.array(self.variances_history), axis=0)\n                self.covariance = np.diag(smoothed_variances)\n\n\n            if len(self.best_x) > 30:  # Reduced best solutions size\n                indices = np.argsort(self.best_f)\n                self.best_x = list(np.array(self.best_x)[indices[:30]])\n                self.best_f = list(np.array(self.best_f)[indices[:30]])\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.326 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b9bbea71-67c4-45b7-8c5d-7d1bbeb1dedd"], "operator": null, "metadata": {"aucs": [0.14962871559828805, 0.28144387491771816, 0.35754092610603705, 0.30786201798313295, 0.23409651291115874, 0.31291049312040053, 0.27907521881888453, 0.2563125346111732, 0.24413336557897514, 0.18016236206218583, 0.2726908890468619, 0.9968613662301948, 0.26672894518857604, 0.2316160276248187, 0.5661962619029437, 0.31861556745745634, 0.29284838054752593, 0.2992189459517294, 0.18951452300747718, 0.48696514276715075]}}
{"id": "69b98c43-6d72-4723-b495-f75fdde0aded", "fitness": 0.46881824050216203, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with dynamic adaptation rate based on success, simplified diagonal covariance, and focused best solution management.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.2, success_threshold=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n        self.success_threshold = success_threshold\n\n        self.lb = -5.0\n        self.ub = 5.0\n\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = []\n        self.best_f = []\n        self.success_rate = 0.0\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            # Covariance matrix not positive semi-definite, fall back to sampling from the bounds\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        \n        # Clip to bounds\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solutions\n        indices = np.argsort(f)\n        num_new_best = min(10, self.initial_samples)\n        self.best_x.extend(x[indices[:num_new_best]])\n        self.best_f.extend(f[indices[:num_new_best]])\n\n        if np.min(f) < self.f_opt:\n            self.f_opt = np.min(f)\n            self.x_opt = x[np.argmin(f)]\n        \n        successful_iterations = 0\n        total_iterations = 0\n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n            total_iterations += 1\n            \n            # Update best solutions\n            indices = np.argsort(f)\n            num_new_best = min(10, num_samples)\n            new_best_x = x[indices[:num_new_best]]\n            new_best_f = f[indices[:num_new_best]]\n            \n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n                successful_iterations += 1\n\n            self.best_x.extend(new_best_x)\n            self.best_f.extend(new_best_f)\n\n            # Adaptation of mean and covariance\n            num_best = min(len(self.best_x), 20)\n\n            if num_best > 1:\n                best_x_arr = np.array(self.best_x)[:num_best]\n                self.mean = (1 - self.adaptation_rate) * self.mean + self.adaptation_rate * np.mean(best_x_arr, axis=0)\n                \n                # Diagonal covariance update with regularization\n                variances = np.var(best_x_arr, axis=0)\n                self.covariance = (1 - self.adaptation_rate) * self.covariance + self.adaptation_rate * np.diag(variances + 1e-8) # Regularization\n\n            # Remove worst solutions if the list gets too long, keep only best 30\n            if len(self.best_x) > 30:\n                indices = np.argsort(self.best_f)\n                self.best_x = list(np.array(self.best_x)[indices[:30]])\n                self.best_f = list(np.array(self.best_f)[indices[:30]])\n            \n            # Dynamic adaptation rate\n            self.success_rate = successful_iterations / total_iterations if total_iterations > 0 else 0.0\n            if self.success_rate > self.success_threshold:\n                self.adaptation_rate = min(self.adaptation_rate * 1.1, 0.5)  # Increase adaptation if successful\n            else:\n                self.adaptation_rate = max(self.adaptation_rate * 0.9, 0.05)   # Decrease adaptation if unsuccessful\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.469 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b9bbea71-67c4-45b7-8c5d-7d1bbeb1dedd"], "operator": null, "metadata": {"aucs": [0.13762719973940007, 0.5568839020924516, 0.39298054777159164, 0.747216870866741, 0.5146297398365196, 0.5936003251810098, 0.4367379588104453, 0.3654381079511867, 0.5890118987541544, 0.20964435868162468, 0.2700887820542447, 0.9939108935099502, 0.26320517455126746, 0.4749396001806866, 0.6741124479424803, 0.3545167573629364, 0.501277005637784, 0.6109534174671645, 0.17884159311851966, 0.5107482285330804]}}
{"id": "540afc6e-8718-49f4-97e4-838b60bf6044", "fitness": 0.2970812480091187, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with simplified rank-one covariance updates and learning rate adaptation based on success, balancing exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.1, success_threshold=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n        self.success_threshold = success_threshold\n\n        self.lb = -5.0\n        self.ub = 5.0\n\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.inf\n        self.success_rate = 0.0\n\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            # Covariance matrix not positive semi-definite, fall back to sampling from the bounds\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        \n        # Clip to bounds\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n        \n        best_index = np.argmin(f)\n        if f[best_index] < self.best_f:\n            self.best_f = f[best_index]\n            self.best_x = x[best_index]\n                \n        if np.min(f) < self.f_opt:\n            self.f_opt = np.min(f)\n            self.x_opt = x[np.argmin(f)]\n        \n        num_success = 0\n        num_iterations = 0\n\n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n            \n            best_index = np.argmin(f)\n            \n            if f[best_index] < self.best_f:\n                num_success +=1\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n                \n                # Adaptation of mean and covariance\n                diff = self.best_x - self.mean\n                self.mean = (1 - self.adaptation_rate) * self.mean + self.adaptation_rate * self.best_x\n                \n                # Rank-one update of covariance\n                self.covariance = (1 - self.adaptation_rate) * self.covariance + self.adaptation_rate * np.outer(diff, diff)\n            else:\n                # Dampen covariance if no improvement is seen\n                self.covariance = (1 - self.adaptation_rate/10) * self.covariance + (self.adaptation_rate/10) * np.eye(self.dim) * 1e-6   \n            \n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n            \n            num_iterations += 1\n            self.success_rate = num_success / num_iterations if num_iterations > 0 else 0.0\n            \n            # Adjust adaptation rate based on success\n            if self.success_rate > self.success_threshold:\n                self.adaptation_rate = min(self.adaptation_rate * 1.1, 0.5) # Increase if successful\n            else:\n                self.adaptation_rate = max(self.adaptation_rate * 0.9, 0.01)  # Decrease if not\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.297 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3b3afd82-ae51-4a55-9e42-c92113aae195"], "operator": null, "metadata": {"aucs": [0.11516509899888572, 0.20938591572985255, 0.5717178496872368, 0.1514784725640198, 0.16005504872687415, 0.571354961046423, 0.21118856194045899, 0.19128881634192896, 0.18819938149828908, 0.18499277223311605, 0.1852191625779549, 0.9945171213431873, 0.28682139933564144, 0.1742285352167473, 0.19075318256178608, 0.4731356319470613, 0.21600580472321695, 0.20178047354431305, 0.22461786653864613, 0.4397189036267347]}}
{"id": "927e4d66-613a-4e9c-927c-7a6a2828063b", "fitness": 0.346731671377252, "name": "AdaptiveCovarianceSamplingSimplified", "description": "Adaptive covariance sampling with rank-based mean adaptation and dynamic variance clamping to balance exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSamplingSimplified:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.1, exploration_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n        self.exploration_rate = exploration_rate\n\n        self.lb = -5.0\n        self.ub = 5.0\n\n        self.mean = np.zeros(dim)\n        self.variance = np.ones(dim)\n        self.best_x = []\n        self.best_f = []\n        self.age = 0\n        self.clamp_threshold = 1.0\n\n    def sample(self, num_samples):\n        # Sample from normal distribution with diagonal covariance\n        x = np.random.normal(self.mean, np.sqrt(self.variance), size=(num_samples, self.dim))\n        \n        # Exploration: Randomly sample some dimensions, decaying exploration\n        effective_exploration_rate = self.exploration_rate / (1 + self.age / 1000)\n        mask = np.random.rand(num_samples, self.dim) < effective_exploration_rate\n        x[mask] = np.random.uniform(self.lb, self.ub, size=x[mask].shape)\n        \n        # Clip to bounds\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solutions\n        indices = np.argsort(f)\n        self.best_x.extend(x[indices[:min(10, self.initial_samples)]])\n        self.best_f.extend(f[indices[:min(10, self.initial_samples)]])\n\n        if np.min(f) < self.f_opt:\n            self.f_opt = np.min(f)\n            self.x_opt = x[np.argmin(f)]\n        \n        while self.budget > 0:\n            self.age += 1\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n            \n            # Update best solutions\n            indices = np.argsort(f)\n            self.best_x.extend(x[indices[:min(10, num_samples)]])\n            self.best_f.extend(f[indices[:min(10, num_samples)]])\n           \n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n\n            # Adaptation of mean and variance, simplified mean adaptation\n            if self.best_x:\n                best_x_arr = np.array(self.best_x)\n                ranks = np.argsort(self.best_f)\n                \n                # Rank-based mean adaptation, using top 10%\n                top_indices = ranks[:max(1, int(0.1 * len(ranks)))]  # At least one\n                mean_update = np.mean(best_x_arr[top_indices], axis=0)\n                self.mean = (1 - self.adaptation_rate) * self.mean + self.adaptation_rate * mean_update\n                \n                # Variance adaptation\n                self.variance = (1 - self.adaptation_rate) * self.variance + self.adaptation_rate * np.var(best_x_arr, axis=0)\n                \n                # Clamp variance to prevent premature convergence\n                self.variance = np.clip(self.variance, 0.01, self.clamp_threshold)\n\n\n            # Remove worst solutions if the list gets too long\n            if len(self.best_x) > 100:\n                indices = np.argsort(self.best_f)\n                self.best_x = list(np.array(self.best_x)[indices[:100]])\n                self.best_f = list(np.array(self.best_f)[indices[:100]])\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveCovarianceSamplingSimplified scored 0.347 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3a57e6a2-db4a-426a-a4c4-114d3106cef6"], "operator": null, "metadata": {"aucs": [0.1458102432074897, 0.26526858431292066, 0.37542672943677047, 0.30912595665266085, 0.2607073619397573, 0.34132046770579083, 0.28161206753802304, 0.2958159378472034, 0.28632410975357736, 0.17693392781443584, 0.3609224454744355, 0.9345133090095841, 0.27965554047152985, 0.2608467419355093, 0.6443952613834705, 0.3390454337232419, 0.3119728547309544, 0.4134356186982383, 0.17127806924553657, 0.48022276666391095]}}
{"id": "aef72581-7c25-4c8e-bda7-f9e9a04657f7", "fitness": 0.4362232186181396, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with simplified covariance and mean updates based on the best solution, using a decaying learning rate for smoother adaptation.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, learning_rate=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.learning_rate = learning_rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.Inf\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solution\n        best_index = np.argmin(f)\n        if f[best_index] < self.best_f:\n            self.best_f = f[best_index]\n            self.best_x = x[best_index]\n\n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n\n            # Update best solution\n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n\n            # Adaptation of mean and covariance using only the best\n            self.mean = (1 - self.learning_rate) * self.mean + self.learning_rate * self.best_x\n            \n            # Diagonal covariance update focusing on best solution's deviation\n            deviation = self.best_x - self.mean\n            self.covariance = (1 - self.learning_rate) * self.covariance + self.learning_rate * np.diag(deviation**2)\n            \n            # Decay the learning rate\n            self.learning_rate *= 0.99\n\n        return self.best_f, self.best_x", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.436 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["7de6636e-6ceb-40c7-8b4c-6ce0ecd2e8c9"], "operator": null, "metadata": {"aucs": [0.12413832868607355, 0.21121844336991347, 0.46107718279195276, 0.7107815161947009, 0.2530962655542216, 0.4552425530219605, 0.3044998810131936, 0.3892404380219585, 0.407878999913624, 0.1951138780372903, 0.695698936430931, 0.9998285148023432, 0.3024200523629321, 0.3710286209937428, 0.7665264982182021, 0.427257066166869, 0.3997313683447391, 0.5686963327595473, 0.1938395631311921, 0.48714993254740513]}}
{"id": "9ce603d4-c4f9-4d12-8802-d94d3a73d158", "fitness": 0.380491405720435, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with simplified covariance update based on the best point's movement and a decaying average adaptation rate.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n\n        self.lb = -5.0\n        self.ub = 5.0\n\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.inf\n\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            # Covariance matrix not positive semi-definite, fall back to sampling from the bounds\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        \n        # Clip to bounds\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n        \n        best_index = np.argmin(f)\n        if f[best_index] < self.best_f:\n            self.best_f = f[best_index]\n            self.best_x = x[best_index]\n                \n        if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n        \n        \n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n            \n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n\n                \n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n\n            # Adaptation of mean and covariance\n            if self.best_x is not None:                \n                self.mean = (1 - self.adaptation_rate) * self.mean + self.adaptation_rate * self.best_x\n                \n                # Update covariance based on the direction of the best solution\n                diff = self.best_x - self.mean\n                self.covariance = (1 - self.adaptation_rate) * self.covariance + self.adaptation_rate * np.outer(diff, diff) + np.eye(self.dim) * 1e-8\n\n            self.adaptation_rate *= 0.99  # Decay the adaptation rate\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.380 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["18f6690e-59ed-41e4-8d7e-0ab7c0ff0fa8"], "operator": null, "metadata": {"aucs": [0.12911587498438803, 0.24533784966017058, 0.391602621841188, 0.4607870603273665, 0.3164872740666186, 0.3753068086380513, 0.27891585978609035, 0.3390412175783709, 0.32205973735963656, 0.15401293355248857, 0.4274628074522825, 0.99599526247595, 0.2814301717996892, 0.3016920177681953, 0.6919475252459867, 0.36245849220117843, 0.33105548953524144, 0.4660651752939904, 0.2534639053523984, 0.4855900294894172]}}
{"id": "27d11aaa-0d70-4e7d-ad00-946dacf51b80", "fitness": 0.336830608771521, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with simplified rank-one covariance updates and adaptive learning rate, focusing on covariance regularization for stability.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.1, covariance_decay=0.99):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n        self.covariance_decay = covariance_decay  # Decay factor for covariance\n\n        self.lb = -5.0\n        self.ub = 5.0\n\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.inf\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            # Covariance matrix not positive semi-definite, fall back to sampling from the bounds\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n\n        # Clip to bounds\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        best_index = np.argmin(f)\n        if f[best_index] < self.best_f:\n            self.best_f = f[best_index]\n            self.best_x = x[best_index]\n\n        if np.min(f) < self.f_opt:\n            self.f_opt = np.min(f)\n            self.x_opt = x[np.argmin(f)]\n\n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n\n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n\n            # Adaptation of mean and covariance\n            if self.best_x is not None:\n                diff = self.best_x - self.mean\n                self.mean = (1 - self.adaptation_rate) * self.mean + self.adaptation_rate * self.best_x\n\n                # Rank-one update for covariance\n                self.covariance = self.covariance_decay * self.covariance + (1 - self.covariance_decay) * np.outer(diff, diff)\n                \n                # Regularize covariance to ensure positive definiteness\n                self.covariance += np.eye(self.dim) * 1e-6\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.337 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["606f4d6f-b0da-4fdc-8a91-16de66f8ca36"], "operator": null, "metadata": {"aucs": [0.11367379581889014, 0.23241878520961767, 0.3450502187442943, 0.32401421744987513, 0.2525230546620679, 0.29181651906721484, 0.2619726354578997, 0.28354283402174474, 0.2622238042992915, 0.17965121219770497, 0.3260062442540984, 0.9810230083300929, 0.26785106011229554, 0.2573052626579324, 0.7135740868482207, 0.3266078438849185, 0.28236730072914995, 0.36228606364000915, 0.19049931828408995, 0.4822049097610117]}}
{"id": "e3c38a27-b104-4e52-abdb-8b4967486bda", "fitness": 0.3839845001716996, "name": "AdaptiveCovarianceSampling", "description": "Simplified adaptive covariance sampling with dynamic adaptation rate based on a moving average of fitness improvements and diagonal covariance update.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.Inf\n        self.fitness_history = []\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solution\n        best_index = np.argmin(f)\n        if f[best_index] < self.best_f:\n            self.best_f = f[best_index]\n            self.best_x = x[best_index]\n\n        self.fitness_history.append(self.best_f)\n\n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n\n            # Update best solution\n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n\n            self.fitness_history.append(self.best_f)\n\n            # Dynamic adaptation rate based on fitness improvement\n            window_size = min(len(self.fitness_history), 20)\n            if len(self.fitness_history) > 1:\n                improvement = self.fitness_history[-2] - self.fitness_history[-1]\n                if improvement > 0:\n                    self.adaptation_rate = min(self.adaptation_rate * 1.1, 0.5)\n                else:\n                    self.adaptation_rate = max(self.adaptation_rate * 0.9, 0.01)\n          \n            # Adaptation of mean and covariance using only the best\n            self.mean = (1 - self.adaptation_rate) * self.mean + self.adaptation_rate * self.best_x\n            \n            # Diagonal covariance update focusing on best solution's deviation\n            deviation = self.best_x - self.mean\n            self.covariance = (1 - self.adaptation_rate) * self.covariance + self.adaptation_rate * np.diag(deviation**2)\n\n        return self.best_f, self.best_x", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.384 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["7de6636e-6ceb-40c7-8b4c-6ce0ecd2e8c9"], "operator": null, "metadata": {"aucs": [0.21249289862164478, 0.18599067188778595, 0.4381029860835409, 0.2311277628865791, 0.25054776453389405, 0.32772676598949746, 0.28776106756373176, 0.40453495096808134, 0.7245235395390696, 0.15441500964228383, 0.494394847761384, 0.9685353285636161, 0.25186475949669673, 0.3291337898052189, 0.6728952140612031, 0.33228810672066245, 0.30666195006830854, 0.4049157263949754, 0.20869496195830584, 0.493081900887513]}}
{"id": "ebdd12f2-c65a-48bb-bcf0-40f0fd4ec25a", "fitness": 0.36815210971388324, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with simplified updates, learning rate decay, and dynamic covariance regularization.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.1, learning_rate_decay=0.99):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n        self.learning_rate_decay = learning_rate_decay\n\n        self.lb = -5.0\n        self.ub = 5.0\n\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.inf\n        self.t = 0 #iteration counter\n        self.min_eig = 1e-6\n\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            # Covariance matrix not positive semi-definite, fall back to sampling from the bounds\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        \n        # Clip to bounds\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n        self.t += self.initial_samples\n        \n        best_index = np.argmin(f)\n        self.best_f = f[best_index]\n        self.best_x = x[best_index]\n        self.f_opt = self.best_f\n        self.x_opt = self.best_x\n\n        \n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n            self.t += num_samples\n            \n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n                self.f_opt = self.best_f\n                self.x_opt = self.best_x\n\n            # Adaptation of mean and covariance\n            adaptive_rate = self.adaptation_rate * (self.learning_rate_decay ** (self.t / self.initial_samples))\n            \n            diff = self.best_x - self.mean\n            self.mean = (1 - adaptive_rate) * self.mean + adaptive_rate * self.best_x\n            \n            # Rank-one update for covariance\n            self.covariance = (1 - adaptive_rate) * self.covariance + adaptive_rate * np.outer(diff, diff)\n                \n            # Ensure covariance is positive definite\n            eigenvalues = np.linalg.eigvalsh(self.covariance)\n            min_eigenvalue = np.min(eigenvalues)\n            if min_eigenvalue < self.min_eig:\n                self.covariance += (self.min_eig - min_eigenvalue) * np.eye(self.dim)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.368 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["606f4d6f-b0da-4fdc-8a91-16de66f8ca36"], "operator": null, "metadata": {"aucs": [0.12933348843994807, 0.24665497685807025, 0.3863232114002666, 0.4418392871140675, 0.2960467529153177, 0.3710414115838161, 0.28744540720166567, 0.3333139404361407, 0.3135194172478445, 0.15676310018566963, 0.42056676484053945, 0.9804922592383425, 0.2768750462305486, 0.2832415949041346, 0.6634975050724469, 0.3645083762865371, 0.32638637828517636, 0.3765021809945843, 0.21902292507900234, 0.48966816996354534]}}
{"id": "588779f7-a5d3-4ef3-be7c-3d062d6d594e", "fitness": 0.3199330410936111, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with simplified rank-one covariance updates, adaptive learning rate based on success, and bounds respecting sampling.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.2, success_threshold=0.1, exploration_factor=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n        self.success_threshold = success_threshold\n        self.exploration_factor = exploration_factor\n\n        self.lb = -5.0\n        self.ub = 5.0\n\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.inf\n        self.success_rate = 0.0\n\n    def sample(self, num_samples):\n        while True:\n            try:\n                x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n                break\n            except np.linalg.LinAlgError:\n                # Add regularization in case covariance is not positive semi-definite\n                self.covariance += np.eye(self.dim) * 1e-6\n\n        # Clip to bounds\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        \n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solution\n        best_index = np.argmin(f)\n        if f[best_index] < self.best_f:\n            self.best_f = f[best_index]\n            self.best_x = x[best_index]\n        \n        successful_iterations = 0\n        total_iterations = 0\n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n            total_iterations += 1\n            \n            # Update best solution\n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n                successful_iterations += 1\n\n            # Adaptation of mean and covariance\n            if successful_iterations > 0:\n                # Rank-one update using the best solution\n                delta = self.best_x - self.mean\n                self.mean = (1 - self.adaptation_rate) * self.mean + self.adaptation_rate * self.best_x\n\n                # Rank-one covariance update with exploration factor\n                self.covariance = (1 - self.adaptation_rate) * self.covariance + self.adaptation_rate * np.outer(delta, delta) + np.eye(self.dim) * self.exploration_factor\n\n            # Dynamic adaptation rate\n            self.success_rate = successful_iterations / total_iterations if total_iterations > 0 else 0.0\n            if self.success_rate > self.success_threshold:\n                self.adaptation_rate = min(self.adaptation_rate * 1.1, 0.5)  # Increase adaptation if successful\n                self.exploration_factor = max(self.exploration_factor * 0.9, 1e-3) # Decrease exploration if successful\n            else:\n                self.adaptation_rate = max(self.adaptation_rate * 0.9, 0.05)   # Decrease adaptation if unsuccessful\n                self.exploration_factor = min(self.exploration_factor * 1.1, 0.2) # Increase exploration if unsuccessful\n\n        return self.best_f, self.best_x", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.320 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["69b98c43-6d72-4723-b495-f75fdde0aded"], "operator": null, "metadata": {"aucs": [0.11936982951456143, 0.35455396407351536, 0.44479194015578727, 0.16389070484785406, 0.16811823036650198, 0.4580361057095511, 0.25354435297637423, 0.3783721995438192, 0.2847587416477585, 0.19715565524222967, 0.19756491727354453, 0.9975549327972196, 0.29570075520963224, 0.1805234542843258, 0.15278440332959486, 0.4977170426445672, 0.3721108653839522, 0.1832518546137164, 0.23495925269626206, 0.4639016195614547]}}
{"id": "c44d43c9-a2f6-4ebf-a782-d1c9bae6999f", "fitness": 0.37276535480524287, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with rank-one covariance updates and adaptive learning rate, simplifying the update and focusing on recent best solutions to balance exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.1, learning_rate_decay=0.99):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n        self.learning_rate_decay = learning_rate_decay\n\n        self.lb = -5.0\n        self.ub = 5.0\n\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.inf\n        self.t = 0 #iteration counter\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            # Covariance matrix not positive semi-definite, fall back to sampling from the bounds\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        \n        # Clip to bounds\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n        self.t += self.initial_samples\n        \n        best_index = np.argmin(f)\n        if f[best_index] < self.best_f:\n            self.best_f = f[best_index]\n            self.best_x = x[best_index]\n                \n        if np.min(f) < self.f_opt:\n            self.f_opt = np.min(f)\n            self.x_opt = x[np.argmin(f)]\n        \n        \n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n            self.t += num_samples\n            \n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n                \n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n\n            # Adaptation of mean and covariance\n            if self.best_x is not None:\n                #Decay the learning rate over time, but limit it to a minimum\n                adaptive_rate = max(0.01, self.adaptation_rate * (self.learning_rate_decay ** (self.t / self.initial_samples)))\n                \n                diff = self.best_x - self.mean\n                self.mean = (1 - adaptive_rate) * self.mean + adaptive_rate * self.best_x\n                \n                # Rank-one update for covariance\n                diff = diff.reshape(-1, 1)\n                self.covariance = (1 - adaptive_rate) * self.covariance + adaptive_rate * np.dot(diff, diff.T)\n\n                #Ensure covariance is positive definite\n                self.covariance += np.eye(self.dim) * 1e-8\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.373 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["606f4d6f-b0da-4fdc-8a91-16de66f8ca36"], "operator": null, "metadata": {"aucs": [0.12621485328491833, 0.2094688115701473, 0.3894802581259351, 0.42340554696474886, 0.3290163012902044, 0.3843184745614656, 0.28674472768490644, 0.3290744725695026, 0.3219394250292309, 0.1634870552198885, 0.4037241759213335, 0.9986810638200813, 0.2762688094607373, 0.2946548649114388, 0.6485265584476173, 0.3643660145762265, 0.3186331590963011, 0.4663100791370953, 0.22803157927751216, 0.4929608651555666]}}
{"id": "ed3fb03e-45e8-43b1-84b0-95a089c2a1d6", "fitness": 0.3801688170686966, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with simplified covariance rank-one updates, dynamic learning rate decay, and covariance regularization for stability.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.2, learning_rate_decay=0.995, covariance_min=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n        self.learning_rate_decay = learning_rate_decay\n        self.covariance_min = covariance_min\n\n        self.lb = -5.0\n        self.ub = 5.0\n\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.inf\n        self.t = 0 #iteration counter\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            # Covariance matrix not positive semi-definite, fall back to sampling from the bounds\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        \n        # Clip to bounds\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n        self.t += self.initial_samples\n        \n        best_index = np.argmin(f)\n        if f[best_index] < self.best_f:\n            self.best_f = f[best_index]\n            self.best_x = x[best_index]\n                \n        if np.min(f) < self.f_opt:\n            self.f_opt = np.min(f)\n            self.x_opt = x[np.argmin(f)]\n        \n        \n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n            self.t += num_samples\n            \n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n                \n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n\n            # Adaptation of mean and covariance\n            if self.best_x is not None:\n                #Decay the learning rate over time\n                adaptive_rate = self.adaptation_rate * (self.learning_rate_decay ** (self.t / self.initial_samples))\n                \n                diff = self.best_x - self.mean\n                self.mean = (1 - adaptive_rate) * self.mean + adaptive_rate * self.best_x\n                \n                # Rank-one update for covariance\n                self.covariance = (1 - adaptive_rate) * self.covariance + adaptive_rate * np.outer(diff, diff)\n                \n                #Ensure covariance is positive definite and regularize\n                self.covariance += np.eye(self.dim) * self.covariance_min\n                \n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.380 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["606f4d6f-b0da-4fdc-8a91-16de66f8ca36"], "operator": null, "metadata": {"aucs": [0.1268667667752248, 0.304516105678898, 0.3835737746564011, 0.41952722248498586, 0.30732479984043537, 0.3796416516998662, 0.29246426521342617, 0.326600250274781, 0.30421123924428295, 0.1956727437380228, 0.4537400827418687, 0.9986245445616805, 0.2868725233313908, 0.30243482888426787, 0.7447544338189811, 0.29925109686923124, 0.3292899618557553, 0.43594848739277337, 0.22456039438509956, 0.48750116792655873]}}
{"id": "646df123-5236-47f1-975e-6a9c332f9048", "fitness": 0.2809451357215895, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with dynamic adaptation rate based on fitness improvement, simplified covariance estimation using the best solution's movement, and learning rate annealing.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.2, success_threshold=0.1, learning_rate_annealing=0.99):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n        self.success_threshold = success_threshold\n        self.learning_rate_annealing = learning_rate_annealing\n\n        self.lb = -5.0\n        self.ub = 5.0\n\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.Inf\n        self.previous_best_x = None\n\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            # Covariance matrix not positive semi-definite, fall back to sampling from the bounds\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        \n        # Clip to bounds\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solution\n        best_index = np.argmin(f)\n        if f[best_index] < self.f_opt:\n            self.f_opt = f[best_index]\n            self.x_opt = x[best_index]\n            self.previous_best_x = np.copy(self.x_opt)\n        \n        successful_iterations = 0\n        total_iterations = 0\n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n            total_iterations += 1\n            \n            # Update best solution\n            best_index = np.argmin(f)\n            if f[best_index] < self.f_opt:\n                self.f_opt = f[best_index]\n                self.x_opt = x[best_index]\n                successful_iterations += 1\n\n                # Adaptation of mean and covariance\n                if self.previous_best_x is not None:\n                    direction = self.x_opt - self.previous_best_x\n                    self.mean = (1 - self.adaptation_rate) * self.mean + self.adaptation_rate * self.x_opt #np.mean(best_x_arr, axis=0)\n                    self.covariance = (1 - self.adaptation_rate) * self.covariance + self.adaptation_rate * np.outer(direction, direction) + np.eye(self.dim) * 1e-8\n                else:\n                    self.mean = (1 - self.adaptation_rate) * self.mean + self.adaptation_rate * self.x_opt #np.mean(best_x_arr, axis=0)\n                    self.covariance = (1 - self.adaptation_rate) * self.covariance + np.eye(self.dim) * 1e-8\n\n\n                self.previous_best_x = np.copy(self.x_opt)\n\n\n            # Dynamic adaptation rate\n            success_rate = successful_iterations / total_iterations if total_iterations > 0 else 0.0\n            if success_rate > self.success_threshold:\n                self.adaptation_rate = min(self.adaptation_rate * 1.1, 0.5)  # Increase adaptation if successful\n            else:\n                self.adaptation_rate = max(self.adaptation_rate * 0.9, 0.05)   # Decrease adaptation if unsuccessful\n            \n            # Learning rate annealing\n            self.adaptation_rate *= self.learning_rate_annealing\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.281 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["69b98c43-6d72-4723-b495-f75fdde0aded"], "operator": null, "metadata": {"aucs": [0.10688877480313419, 0.41952943113620034, 0.4205711103973143, 0.15616407152907053, 0.15193300405701826, 0.1728283374883931, 0.21126503572307243, 0.20555554734774772, 0.17857195388344715, 0.15904267664530836, 0.4115727276662937, 0.9985073550135638, 0.25458901091379427, 0.16729436288564592, 0.15002975182996492, 0.32474624001351216, 0.32048503533774875, 0.1824701566987319, 0.20089441084278026, 0.42596372021904894]}}
{"id": "333861f7-018c-4778-a35d-5bc9be0274d2", "fitness": 0.33407388072509275, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with exponentially smoothed mean and covariance updates, prioritizing exploration and robustness using a dynamic adaptation rate based on success history.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.1, success_history=10, exploration_factor=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n        self.success_history_length = success_history\n        self.exploration_factor = exploration_factor # Adds exploration\n\n        self.lb = -5.0\n        self.ub = 5.0\n\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.inf\n        self.success_history = []\n\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            # Covariance matrix not positive semi-definite, fall back to sampling from the bounds\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        \n        # Clip to bounds\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n        \n        best_index = np.argmin(f)\n        if f[best_index] < self.best_f:\n            self.best_f = f[best_index]\n            self.best_x = x[best_index]\n            self.success_history.append(1)\n        else:\n            self.success_history.append(0)\n                \n        if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n        \n        \n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n            \n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n                self.success_history.append(1)\n            else:\n                self.success_history.append(0)\n\n                \n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n\n            # Adaptation of mean and covariance\n            if self.best_x is not None:                \n                self.mean = (1 - self.adaptation_rate) * self.mean + self.adaptation_rate * self.best_x\n                \n                # Update covariance based on the direction of the best solution\n                diff = self.best_x - self.mean\n                self.covariance = (1 - self.adaptation_rate) * self.covariance + self.adaptation_rate * (np.outer(diff, diff) + np.eye(self.dim) * self.exploration_factor)\n\n            if len(self.success_history) > self.success_history_length:\n                self.success_history = self.success_history[-self.success_history_length:]\n            \n            success_rate = np.mean(self.success_history) if self.success_history else 0.5  # Default to 0.5 if history is empty\n            self.adaptation_rate = min(0.5, max(0.01, 0.1 * success_rate / 0.5)) # Adjust adaptation rate dynamically\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.334 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["18f6690e-59ed-41e4-8d7e-0ab7c0ff0fa8"], "operator": null, "metadata": {"aucs": [0.11491472241577594, 0.1941623409409574, 0.3707080339447715, 0.18434382608972433, 0.26122279488223954, 0.3275156578928079, 0.2678756156017028, 0.3048065305511103, 0.2782410338803393, 0.17335247865145087, 0.39350897343618374, 0.9962797179932861, 0.26940994546152264, 0.272164720660672, 0.5608774497927076, 0.3315046488167779, 0.3018044687752852, 0.39128834461761475, 0.1964693613575097, 0.49102694873941566]}}
{"id": "943fab1a-10b6-443e-8701-cbda5196d811", "fitness": 0.3441276807986352, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with exponentially decaying learning rate and covariance regularization to improve exploration and convergence.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.2, learning_rate_decay=0.98, covariance_decay=0.99):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n        self.learning_rate_decay = learning_rate_decay\n        self.covariance_decay = covariance_decay\n\n        self.lb = -5.0\n        self.ub = 5.0\n\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.inf\n        self.t = 0 #iteration counter\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            # Covariance matrix not positive semi-definite, fall back to sampling from the bounds\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        \n        # Clip to bounds\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n        self.t += self.initial_samples\n        \n        best_index = np.argmin(f)\n        if f[best_index] < self.best_f:\n            self.best_f = f[best_index]\n            self.best_x = x[best_index]\n                \n        if np.min(f) < self.f_opt:\n            self.f_opt = np.min(f)\n            self.x_opt = x[np.argmin(f)]\n        \n        \n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n            self.t += num_samples\n            \n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n                \n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n\n            # Adaptation of mean and covariance\n            if self.best_x is not None:\n                #Decay the learning rate over time\n                adaptive_rate = self.adaptation_rate * (self.learning_rate_decay ** (self.t / self.initial_samples))\n                \n                diff = self.best_x - self.mean\n                self.mean = (1 - adaptive_rate) * self.mean + adaptive_rate * self.best_x\n                \n                # Rank-one update for covariance\n                self.covariance = (self.covariance_decay) * self.covariance + (1-self.covariance_decay) * np.outer(diff, diff) + np.eye(self.dim) * 1e-6\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.344 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["606f4d6f-b0da-4fdc-8a91-16de66f8ca36"], "operator": null, "metadata": {"aucs": [0.1271084246828299, 0.2619936571484067, 0.3412489346480041, 0.28726625304508724, 0.2554286393834285, 0.3099755011020051, 0.2832880161544479, 0.2850455789898304, 0.2675233326652351, 0.16933834771203182, 0.3570675624243126, 0.9922646404567151, 0.29464010739034285, 0.2584377547174417, 0.7025771626851773, 0.3251184696046313, 0.28661699863705936, 0.3728304651654154, 0.22629643767509566, 0.4784873316852063]}}
{"id": "cc8e4b88-7bbf-4047-b8d4-b674940d13c7", "fitness": 0.44535948503348005, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with exponential moving average for mean and covariance, and dynamic learning rate based on best solution improvement.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.2, learning_rate_decay=0.995):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n        self.learning_rate_decay = learning_rate_decay\n\n        self.lb = -5.0\n        self.ub = 5.0\n\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.inf\n        self.t = 0  # iteration counter\n        self.success_history = []\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            # Covariance matrix not positive semi-definite, fall back to sampling from the bounds\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n\n        # Clip to bounds\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n        self.t += self.initial_samples\n\n        best_index = np.argmin(f)\n        if f[best_index] < self.best_f:\n            self.best_f = f[best_index]\n            self.best_x = x[best_index]\n            self.success_history.append(1)\n        else:\n            self.success_history.append(0)\n\n        if np.min(f) < self.f_opt:\n            self.f_opt = np.min(f)\n            self.x_opt = x[np.argmin(f)]\n\n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n            self.t += num_samples\n\n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n                self.success_history.append(1)\n            else:\n                self.success_history.append(0)\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n\n            # Adaptation of mean and covariance\n            if self.best_x is not None:\n                # Decay the learning rate over time\n                adaptive_rate = self.adaptation_rate * (self.learning_rate_decay ** (self.t / self.initial_samples))\n\n                diff = self.best_x - self.mean\n                self.mean = (1 - adaptive_rate) * self.mean + adaptive_rate * self.best_x\n\n                # Rank-one update for covariance (exponential moving average)\n                self.covariance = (1 - adaptive_rate) * self.covariance + adaptive_rate * (np.outer(diff, diff) + np.eye(self.dim) * 1e-8)  # Ensure positive definite\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.445 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["606f4d6f-b0da-4fdc-8a91-16de66f8ca36"], "operator": null, "metadata": {"aucs": [0.20857563958033054, 0.25989907024541614, 0.4991701536426665, 0.1958466830838793, 0.30847710579615273, 0.5180648539110158, 0.28057430622280455, 0.4267888485458221, 0.4829566272180351, 0.19108513203508626, 0.770053226047414, 0.9965349185077954, 0.2691068574705454, 0.4489895146667804, 0.8180510094484548, 0.5084852409745688, 0.4380006287258057, 0.6096676933970013, 0.19524457807531548, 0.48161761307470896]}}
{"id": "fc30c8f2-97b9-4763-b819-e31c6fa4ff1d", "fitness": -Infinity, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with simplified mean and covariance updates based on best solution improvement, using a decaying adaptation rate and eigenvalue decomposition for robust covariance handling.", "code": "import numpy as np\nfrom scipy.linalg import eigh\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.2, learning_rate_decay=0.995, min_eig=1e-8):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n        self.learning_rate_decay = learning_rate_decay\n        self.min_eig = min_eig #Minimum eigenvalue for regularization\n\n        self.lb = -5.0\n        self.ub = 5.0\n\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.inf\n        self.t = 0  # iteration counter\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            # Covariance matrix not positive semi-definite, fall back to sampling from the bounds\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n\n        # Clip to bounds\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def ensure_pos_def(self, covariance):\n        \"\"\"Ensures the covariance matrix is positive definite by adjusting eigenvalues.\"\"\"\n        w, v = eigh(covariance)\n        w = np.maximum(w, self.min_eig)  # Ensure eigenvalues are above the minimum\n        return v @ np.diag(w) @ v.T\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n        self.t += self.initial_samples\n\n        best_index = np.argmin(f)\n        if f[best_index] < self.best_f:\n            self.best_f = f[best_index]\n            self.best_x = x[best_index]\n\n        self.f_opt = np.min(f)\n        self.x_opt = x[np.argmin(f)]\n\n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n            self.t += num_samples\n\n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n\n                # Adaptation of mean and covariance\n                # Decay the learning rate over time\n                adaptive_rate = self.adaptation_rate * (self.learning_rate_decay ** (self.t / self.initial_samples))\n\n                diff = self.best_x - self.mean\n                self.mean = (1 - adaptive_rate) * self.mean + adaptive_rate * self.best_x\n\n                # Rank-one update for covariance (exponential moving average)\n                self.covariance = (1 - adaptive_rate) * self.covariance + adaptive_rate * np.outer(diff, diff)\n                self.covariance = self.ensure_pos_def(self.covariance)\n\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "An exception occurred: name 'eigh' is not defined.", "error": "", "parent_ids": ["cc8e4b88-7bbf-4047-b8d4-b674940d13c7"], "operator": null, "metadata": {}}
{"id": "d31825bd-5a24-45ac-b633-9e86fe13acfc", "fitness": 0.450495972422191, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with dynamic covariance rank reduction and prioritized mean update for faster convergence.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, learning_rate=0.2, rank_update=1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.learning_rate = learning_rate\n        self.rank_update = rank_update  # Number of components to update in covariance\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.Inf\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solution\n        best_index = np.argmin(f)\n        if f[best_index] < self.best_f:\n            self.best_f = f[best_index]\n            self.best_x = x[best_index]\n\n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n\n            # Update best solution\n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n\n            # Adaptation of mean and covariance using only the best\n            deviation = self.best_x - self.mean\n            self.mean = (1 - self.learning_rate) * self.mean + self.learning_rate * self.best_x #self.best_x\n\n            # Rank-one covariance update using the deviation, clip learning rate\n            limited_learning_rate = min(self.learning_rate, 1.0)\n            self.covariance = (1 - limited_learning_rate) * self.covariance + limited_learning_rate * np.outer(deviation, deviation)\n            \n            # Decay the learning rate\n            self.learning_rate *= 0.99\n\n        return self.best_f, self.best_x", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.450 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["aef72581-7c25-4c8e-bda7-f9e9a04657f7"], "operator": null, "metadata": {"aucs": [0.13428987637315182, 0.396714508521285, 0.4783067651523961, 0.772037285912976, 0.25880905311532176, 0.49000473549011936, 0.32152898418647247, 0.4030690760978779, 0.43169690388781223, 0.19766059506272382, 0.5524034802373516, 0.9837996817457174, 0.30352316199452356, 0.40552692982032157, 0.838818357469259, 0.352352646514031, 0.4136712652233071, 0.5810395366656436, 0.19225668846348287, 0.5024099165100459]}}
{"id": "21ee5c97-da85-4c74-95a8-6bb3fc52c83e", "fitness": 0.34658024070013227, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with a simplified rank-one covariance update based on the difference between the current best and the mean, combined with decaying learning rates for both mean and covariance adaptation.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, learning_rate_mean=0.1, learning_rate_cov=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.learning_rate_mean = learning_rate_mean\n        self.learning_rate_cov = learning_rate_cov\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.Inf\n        self.epsilon = 1e-8  # Small constant to avoid singularity\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            # If covariance is not positive definite, sample uniformly\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solution\n        best_index = np.argmin(f)\n        if f[best_index] < self.best_f:\n            self.best_f = f[best_index]\n            self.best_x = x[best_index]\n\n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n\n            # Update best solution\n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n\n            # Adaptation of mean and covariance using only the best\n            diff = self.best_x - self.mean\n            self.mean = self.mean + self.learning_rate_mean * diff\n            \n            # Rank-one covariance update\n            self.covariance = (1 - self.learning_rate_cov) * self.covariance + self.learning_rate_cov * np.outer(diff, diff) + self.epsilon * np.eye(self.dim)\n\n            # Decay learning rates\n            self.learning_rate_mean *= 0.99\n            self.learning_rate_cov *= 0.99\n\n        return self.best_f, self.best_x", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.347 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["aef72581-7c25-4c8e-bda7-f9e9a04657f7"], "operator": null, "metadata": {"aucs": [0.1732010457687746, 0.2962788241716363, 0.37725801946100956, 0.1690440458161464, 0.2780803982805845, 0.34238837494571794, 0.27189094141810966, 0.28727986407556805, 0.2851154298925821, 0.18213644181899913, 0.38362248826741785, 0.9940623059360267, 0.2837001629748681, 0.25969775587081645, 0.634695850647889, 0.34968712782470024, 0.3081815881953923, 0.38842204800464497, 0.1822823984810058, 0.48457970215075563]}}
{"id": "9d9d1e20-0a2d-4b13-a5df-f8032240670e", "fitness": 0.4662533980950473, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with simplified covariance and mean updates, adaptive learning rate based on fitness improvement, and covariance regularization.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, learning_rate=0.2, regularization_factor=1e-6):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.learning_rate = learning_rate\n        self.regularization_factor = regularization_factor\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.Inf\n        self.previous_best_f = np.Inf\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solution\n        best_index = np.argmin(f)\n        if f[best_index] < self.best_f:\n            self.best_f = f[best_index]\n            self.best_x = x[best_index]\n\n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n\n            # Update best solution\n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n                \n                # Adjust learning rate based on improvement\n                improvement = self.previous_best_f - self.best_f\n                if improvement > 0:\n                    self.learning_rate = min(0.5, self.learning_rate * 1.1) # Increase if improving\n                else:\n                    self.learning_rate = max(0.05, self.learning_rate * 0.9) # Decrease if not improving\n            \n            self.previous_best_f = self.best_f\n\n            # Adaptation of mean and covariance using only the best\n            self.mean = (1 - self.learning_rate) * self.mean + self.learning_rate * self.best_x\n            \n            # Diagonal covariance update focusing on best solution's deviation\n            deviation = self.best_x - self.mean\n            self.covariance = (1 - self.learning_rate) * self.covariance + self.learning_rate * np.diag(deviation**2)\n            \n            # Regularize covariance matrix\n            self.covariance += self.regularization_factor * np.eye(self.dim)\n\n        return self.best_f, self.best_x", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.466 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["aef72581-7c25-4c8e-bda7-f9e9a04657f7"], "operator": null, "metadata": {"aucs": [0.2374223820834318, 0.1857453055088344, 0.5985855799258151, 0.2424704207449413, 0.6065079527720664, 0.6121954371461862, 0.36474081863154595, 0.5202278330661386, 0.5955942864267195, 0.1901623301525932, 0.37082794564151644, 0.997607053596264, 0.25172892047726014, 0.6027935796060606, 0.7120786617615751, 0.3479772434745497, 0.393800448484497, 0.7897651011693967, 0.21012391461576685, 0.49471274661578435]}}
{"id": "7de90ee6-1e8f-48fb-934d-ae7559d5a2e5", "fitness": 0.43040441314567807, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with dynamic adaptation rate based on recent fitness improvements and a simplified covariance update using only the best solution's deviation from the mean.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.Inf\n        self.fitness_history = []\n        self.min_adaptation_rate = 0.01\n        self.max_adaptation_rate = 0.5\n\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solution\n        best_index = np.argmin(f)\n        if f[best_index] < self.best_f:\n            self.best_f = f[best_index]\n            self.best_x = x[best_index]\n\n        self.fitness_history.append(self.best_f)\n\n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n\n            # Update best solution\n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n\n            self.fitness_history.append(self.best_f)\n\n            # Dynamic adaptation rate based on fitness improvement\n            if len(self.fitness_history) > 1:\n                improvement = self.fitness_history[-2] - self.fitness_history[-1]\n                if improvement > 0:\n                    self.adaptation_rate = min(self.adaptation_rate * 1.1, self.max_adaptation_rate)\n                else:\n                    self.adaptation_rate = max(self.adaptation_rate * 0.9, self.min_adaptation_rate)\n\n            # Adaptation of mean and covariance using only the best\n            deviation = self.best_x - self.mean\n            self.mean = (1 - self.adaptation_rate) * self.mean + self.adaptation_rate * self.best_x\n\n            # Diagonal covariance update focusing on best solution's deviation\n            self.covariance = (1 - self.adaptation_rate) * self.covariance + self.adaptation_rate * np.diag(deviation**2)\n\n        return self.best_f, self.best_x", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.430 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e3c38a27-b104-4e52-abdb-8b4967486bda"], "operator": null, "metadata": {"aucs": [0.18848699599683383, 0.18497051117698782, 0.3969998798193157, 0.2292809240023238, 0.270530285286553, 0.7560300071535003, 0.28156031339986554, 0.33388312562547595, 0.3467954219103587, 0.16896470736427072, 0.8119808400450254, 0.9988326236051379, 0.24708080918447017, 0.651114296943147, 0.6969821678365747, 0.474816443927254, 0.4254050915005183, 0.4010231025667108, 0.2563094968664402, 0.48704121870279826]}}
{"id": "0b7ee236-d0c4-4664-b29f-6448629b43d5", "fitness": 0.3913747857593891, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with exponentially smoothed mean and covariance updates based on the best solution, along with dynamic adaptation rate adjustment.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.Inf\n        self.fitness_history = []\n        self.smoothing = 0.9  # Smoothing factor for adaptation rate\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solution\n        best_index = np.argmin(f)\n        if f[best_index] < self.best_f:\n            self.best_f = f[best_index]\n            self.best_x = x[best_index]\n\n        self.fitness_history.append(self.best_f)\n\n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n\n            # Update best solution\n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n\n            self.fitness_history.append(self.best_f)\n\n            # Dynamic adaptation rate based on fitness improvement and smoothing\n            if len(self.fitness_history) > 1:\n                improvement = self.fitness_history[-2] - self.fitness_history[-1]\n                if improvement > 0:\n                    self.adaptation_rate = self.smoothing * self.adaptation_rate + (1 - self.smoothing) * min(self.adaptation_rate * 1.1, 0.4)\n                else:\n                    self.adaptation_rate = self.smoothing * self.adaptation_rate + (1 - self.smoothing) * max(self.adaptation_rate * 0.9, 0.01)\n\n            # Adaptation of mean and covariance using only the best, with smoothing\n            self.mean = self.smoothing * self.mean + (1 - self.smoothing) * self.best_x\n            deviation = self.best_x - self.mean\n            self.covariance = self.smoothing * self.covariance + (1 - self.smoothing) * np.diag(deviation**2)\n\n        return self.best_f, self.best_x", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.391 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e3c38a27-b104-4e52-abdb-8b4967486bda"], "operator": null, "metadata": {"aucs": [0.12346479705193758, 0.22555050035096513, 0.4216203461404634, 0.5904623148804932, 0.3252316245053737, 0.38486652488296036, 0.29868256851397357, 0.33136385262616175, 0.3267591616613845, 0.1964905250311324, 0.5665221204847511, 0.9763414855708996, 0.2907431239663879, 0.275427231685072, 0.7305322135760846, 0.30336922818446244, 0.33795940120711954, 0.47617506237323537, 0.19208844578539352, 0.45384518670953033]}}
{"id": "6bffa5c8-0a5e-4fc8-abcf-b21e3a9b43dd", "fitness": 0.3748757940861397, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with simplified rank-one covariance update using a decaying learning rate and a fixed exploration noise.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.2, exploration_noise=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n        self.exploration_noise = exploration_noise\n\n        self.lb = -5.0\n        self.ub = 5.0\n\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.inf\n\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            # Covariance matrix not positive semi-definite, fall back to sampling from the bounds\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        \n        # Clip to bounds\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n        \n        best_index = np.argmin(f)\n        self.best_f = f[best_index]\n        self.best_x = x[best_index]\n        self.f_opt = self.best_f\n        self.x_opt = self.best_x\n        \n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n            \n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n                \n                if self.best_f < self.f_opt:\n                    self.f_opt = self.best_f\n                    self.x_opt = self.best_x\n\n            # Adaptation of mean and covariance\n            diff = self.best_x - self.mean\n            self.mean = (1 - self.adaptation_rate) * self.mean + self.adaptation_rate * self.best_x\n            \n            # Rank-one update of covariance\n            self.covariance = (1 - self.adaptation_rate) * self.covariance + self.adaptation_rate * np.outer(diff, diff) + np.eye(self.dim) * self.exploration_noise\n\n            self.adaptation_rate *= 0.99  # Decay the adaptation rate\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.375 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9ce603d4-c4f9-4d12-8802-d94d3a73d158"], "operator": null, "metadata": {"aucs": [0.12801790853359163, 0.27094499562829943, 0.3833276674881384, 0.46463831726889526, 0.32123320778428766, 0.35881480104661134, 0.302052069904591, 0.3292472547101628, 0.29195006156777836, 0.1931111640902744, 0.4363105961834419, 0.9988219004416327, 0.31360084563814616, 0.3040692085592869, 0.5602520021373351, 0.3344006908140731, 0.3175231760702998, 0.43520638855098914, 0.2646373609185202, 0.48935626438643864]}}
{"id": "ad763ca8-41d8-47b6-af4d-b3c2b5469716", "fitness": 0.36672909280898647, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with momentum-based mean update and simplified, regularized covariance update focusing on recent best solutions and decaying adaptation rate.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.1, momentum=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n        self.momentum = momentum\n\n        self.lb = -5.0\n        self.ub = 5.0\n\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.inf\n        self.mean_velocity = np.zeros(dim) # Initialize momentum\n\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            # Covariance matrix not positive semi-definite, fall back to sampling from the bounds\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        \n        # Clip to bounds\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n        \n        best_index = np.argmin(f)\n        if f[best_index] < self.best_f:\n            self.best_f = f[best_index]\n            self.best_x = x[best_index]\n                \n        if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n        \n        \n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n            \n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n\n                \n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n\n            # Adaptation of mean and covariance\n            if self.best_x is not None:                \n                # Momentum-based mean update\n                diff = self.best_x - self.mean\n                self.mean_velocity = self.momentum * self.mean_velocity + (1 - self.momentum) * diff\n                self.mean = self.mean + self.adaptation_rate * self.mean_velocity\n                \n                # Simplified covariance update with regularization\n                diff = self.best_x - self.mean\n                self.covariance = (1 - self.adaptation_rate) * self.covariance + self.adaptation_rate * np.outer(diff, diff) + np.eye(self.dim) * 1e-6\n\n            self.adaptation_rate *= 0.99  # Decay the adaptation rate\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.367 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9ce603d4-c4f9-4d12-8802-d94d3a73d158"], "operator": null, "metadata": {"aucs": [0.11838315358326634, 0.1998424824290499, 0.3895738238759796, 0.42198387340393395, 0.28853340971883057, 0.3644040573696429, 0.28818100135022073, 0.3204880837507421, 0.31392138386323953, 0.200502119199233, 0.41686343168810347, 0.9943795566403275, 0.2847193655722996, 0.29414956162784034, 0.7006860833313407, 0.32105913208375636, 0.30266795058296025, 0.4020708673346717, 0.2255837924985814, 0.48658872627570804]}}
{"id": "0d6f3d4b-0183-489d-b318-82d44c3a4f23", "fitness": 0.3302756461863834, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with simplified covariance and mean updates, and a dynamic learning rate based on a moving average of recent successes.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.2, success_history_length=10):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n        self.success_history_length = success_history_length\n\n        self.lb = -5.0\n        self.ub = 5.0\n\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.inf\n        self.t = 0  # iteration counter\n        self.success_history = []\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            # Covariance matrix not positive semi-definite, fall back to sampling from the bounds\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n\n        # Clip to bounds\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n        self.t += self.initial_samples\n\n        best_index = np.argmin(f)\n        if f[best_index] < self.best_f:\n            self.best_f = f[best_index]\n            self.best_x = x[best_index]\n            self.success_history.append(1)\n        else:\n            self.success_history.append(0)\n\n        if np.min(f) < self.f_opt:\n            self.f_opt = np.min(f)\n            self.x_opt = x[np.argmin(f)]\n\n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n            self.t += num_samples\n\n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n                self.success_history.append(1)\n            else:\n                self.success_history.append(0)\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n\n            # Adaptation of mean and covariance\n            if self.best_x is not None:\n                # Calculate success rate\n                success_rate = np.mean(self.success_history[-self.success_history_length:] if len(self.success_history) > 0 else 0)\n\n                # Dynamic adaptation rate\n                adaptive_rate = self.adaptation_rate * success_rate\n\n                diff = self.best_x - self.mean\n                self.mean = (1 - adaptive_rate) * self.mean + adaptive_rate * self.best_x\n\n                # Simplified covariance update (only diagonal)\n                self.covariance = (1 - adaptive_rate) * self.covariance + adaptive_rate * np.diag(diff**2)  # Diagonal update\n\n                # Ensure positive definite\n                self.covariance += np.eye(self.dim) * 1e-8\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.330 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["cc8e4b88-7bbf-4047-b8d4-b674940d13c7"], "operator": null, "metadata": {"aucs": [0.14813913143470725, 0.23960151909660432, 0.3729967759713677, 0.21957013611247866, 0.2625955197391059, 0.30208815185094495, 0.2683098565914871, 0.2756302735806365, 0.26566760400368417, 0.15052181635123585, 0.3455771224095924, 0.9618014472357637, 0.26763671537073974, 0.25182272403504313, 0.638111278050706, 0.3352032260064657, 0.26731428215233566, 0.35923647372283807, 0.1936461811357667, 0.4800426888761633]}}
{"id": "b0de5fbd-e671-4e28-97b4-122210c7946f", "fitness": 0.3809625445346938, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with simplified covariance update using a rank-one update based on the best solution found so far and a decreasing learning rate, with covariance regularization.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n        self.min_adaptation_rate = 0.01 # Minimum adaptation rate\n\n        self.lb = -5.0\n        self.ub = 5.0\n\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.inf\n        self.exploration_factor = 0.01  # Adjust for exploration\n\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            # Covariance matrix not positive semi-definite, fall back to sampling from the bounds\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        \n        # Clip to bounds\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n        \n        best_index = np.argmin(f)\n        if f[best_index] < self.best_f:\n            self.best_f = f[best_index]\n            self.best_x = x[best_index]\n                \n        if np.min(f) < self.f_opt:\n            self.f_opt = np.min(f)\n            self.x_opt = x[np.argmin(f)]\n        \n        \n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n            \n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n\n                if np.min(f) < self.f_opt:\n                    self.f_opt = np.min(f)\n                    self.x_opt = x[np.argmin(f)]\n\n            # Adaptation of mean and covariance\n            if self.best_x is not None:                \n                diff = self.best_x - self.mean\n                self.mean = (1 - self.adaptation_rate) * self.mean + self.adaptation_rate * self.best_x\n                \n                # Rank-one update for covariance\n                self.covariance = (1 - self.adaptation_rate) * self.covariance + self.adaptation_rate * np.outer(diff, diff)\n                \n                # Regularize covariance\n                self.covariance += np.eye(self.dim) * self.exploration_factor\n                \n            # Decay the adaptation rate, but not below a minimum value\n            self.adaptation_rate = max(self.adaptation_rate * 0.99, self.min_adaptation_rate)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.381 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9ce603d4-c4f9-4d12-8802-d94d3a73d158"], "operator": null, "metadata": {"aucs": [0.18331526489425864, 0.25009911004030283, 0.3806819491428225, 0.4494563283771268, 0.2857027152587873, 0.36349530813234543, 0.29308190740904727, 0.3148724100188478, 0.3131897258635786, 0.18392253887451038, 0.4482782921599946, 0.995933218425477, 0.26916469333812354, 0.3180973969517046, 0.7797086498450527, 0.3391066103179222, 0.3080511257664048, 0.4615045611383568, 0.21441363000500002, 0.46717545473421196]}}
{"id": "0bf84a18-402c-42a8-b563-edc68795299f", "fitness": 0.36792502697514096, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with a simplified rank-one covariance update, adaptive learning rate based on success rate, and dynamic covariance damping to maintain positive definiteness.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.2, learning_rate_decay=0.995, damping_factor=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n        self.learning_rate_decay = learning_rate_decay\n        self.damping_factor = damping_factor\n\n        self.lb = -5.0\n        self.ub = 5.0\n\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.inf\n        self.t = 0  # iteration counter\n        self.success_rate = 0.0\n        self.success_history = []\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            # Covariance matrix not positive semi-definite, fall back to sampling from the bounds\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n\n        # Clip to bounds\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n        self.t += self.initial_samples\n\n        best_index = np.argmin(f)\n        if f[best_index] < self.best_f:\n            self.best_f = f[best_index]\n            self.best_x = x[best_index]\n            self.success_history.append(1)\n        else:\n            self.success_history.append(0)\n\n        if np.min(f) < self.f_opt:\n            self.f_opt = np.min(f)\n            self.x_opt = x[np.argmin(f)]\n\n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n            self.t += num_samples\n\n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n                self.success_history.append(1)\n            else:\n                self.success_history.append(0)\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n\n            # Adaptation of mean and covariance\n            if self.best_x is not None:\n                # Update success rate\n                self.success_rate = np.mean(self.success_history[-min(self.t, 100):])  # Track recent success\n\n                # Adaptive learning rate based on success rate\n                adaptive_rate = self.adaptation_rate * (1 + 2 * self.success_rate -1)\n\n                diff = self.best_x - self.mean\n                self.mean = (1 - adaptive_rate) * self.mean + adaptive_rate * self.best_x\n\n                # Simplified rank-one update with covariance damping\n                self.covariance = (1 - adaptive_rate) * self.covariance + adaptive_rate * np.outer(diff, diff)\n                self.covariance += np.eye(self.dim) * self.damping_factor # Damping\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.368 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["cc8e4b88-7bbf-4047-b8d4-b674940d13c7"], "operator": null, "metadata": {"aucs": [0.13676821257402205, 0.3207942293127255, 0.4003728423824823, 0.18045459554943333, 0.3049540563546129, 0.397477635394053, 0.2834300085967508, 0.3199803419753069, 0.29993824331892316, 0.15748082043955725, 0.3783966567045183, 0.973458833968259, 0.3232496445843943, 0.3207451797351353, 0.7432754190474808, 0.32271350272139054, 0.3174764486655708, 0.4948136294438089, 0.1911363583088198, 0.49158388042557466]}}
{"id": "b95afe76-4c8b-4a7e-b4fb-aadb62cb6338", "fitness": 0.3734926661476733, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with rank-one covariance update using the best point's movement and a decaying adaptation rate, further simplified by removing initial sampling and covariance regularization.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, adaptation_rate=0.1, initial_covariance=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.adaptation_rate = adaptation_rate\n        self.initial_covariance = initial_covariance\n\n        self.lb = -5.0\n        self.ub = 5.0\n\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim) * self.initial_covariance\n        self.best_x = None\n        self.best_f = np.inf\n\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            # Covariance matrix not positive semi-definite, fall back to sampling from the bounds\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        \n        # Clip to bounds\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        \n        num_samples = min(self.budget, int(np.ceil(0.01 * self.budget))) # Reduce sample size\n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, num_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n            \n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n\n                if self.best_f < self.f_opt:\n                    self.f_opt = self.best_f\n                    self.x_opt = self.best_x\n\n\n            # Adaptation of mean and covariance\n            if self.best_x is not None:                \n                diff = self.best_x - self.mean\n                self.mean = (1 - self.adaptation_rate) * self.mean + self.adaptation_rate * self.best_x                \n                self.covariance = (1 - self.adaptation_rate) * self.covariance + self.adaptation_rate * np.outer(diff, diff)\n\n            self.adaptation_rate *= 0.99  # Decay the adaptation rate\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.373 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9ce603d4-c4f9-4d12-8802-d94d3a73d158"], "operator": null, "metadata": {"aucs": [0.12561626482377874, 0.20487555432121285, 0.39848412274722267, 0.19233593848901598, 0.28869182922256387, 0.3792629201162806, 0.3007894838593326, 0.3268750891592206, 0.3180672799869413, 0.19582054285443096, 0.5853300245327722, 0.9776929982965828, 0.3307399817078991, 0.2891467109417022, 0.7145204106237345, 0.37718839451453967, 0.3233336204040751, 0.4651248084763111, 0.19214943849711552, 0.48380790937873375]}}
{"id": "07b21daf-3054-471e-b0f5-cf12f1b34b87", "fitness": 0.3805902779506374, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with simplified covariance update using only the best solution's deviation from the mean and adaptive adaptation rate for exploration-exploitation balance.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.Inf\n        self.fitness_history = []\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solution\n        best_index = np.argmin(f)\n        if f[best_index] < self.best_f:\n            self.best_f = f[best_index]\n            self.best_x = x[best_index]\n\n        self.fitness_history.append(self.best_f)\n\n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n\n            # Update best solution\n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n\n            self.fitness_history.append(self.best_f)\n\n            # Dynamic adaptation rate based on fitness improvement\n            if len(self.fitness_history) > 1:\n                improvement = self.fitness_history[-2] - self.fitness_history[-1]\n                if improvement > 0:\n                    self.adaptation_rate = min(self.adaptation_rate * 1.1, 0.4)\n                else:\n                    self.adaptation_rate = max(self.adaptation_rate * 0.9, 0.01)\n\n            # Adaptation of mean and covariance using only the best\n            self.mean = 0.9 * self.mean + 0.1 * self.best_x\n            deviation = self.best_x - self.mean\n            self.covariance = 0.9 * self.covariance + 0.1 * np.diag(deviation**2)\n\n\n        return self.best_f, self.best_x", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.381 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0b7ee236-d0c4-4664-b29f-6448629b43d5"], "operator": null, "metadata": {"aucs": [0.20000463251113454, 0.22523843181090264, 0.4281371383505672, 0.31738870213257797, 0.3339455345512471, 0.373385004119152, 0.30321248213874064, 0.33661150796943207, 0.3318466005977132, 0.19445849482681887, 0.444382300541615, 0.9992521331206483, 0.26344886701838033, 0.30527496629332984, 0.7155845569904098, 0.3045677552150473, 0.34247436808368925, 0.4427688237013061, 0.2812966958597183, 0.46852656318031705]}}
{"id": "5c112d1c-596c-49cb-82fd-8cf2250e5d4a", "fitness": 0.49313246284882944, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with simplified rank-one covariance update, adaptive learning rate based on fitness improvement, and covariance regularization using the identity matrix.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, learning_rate=0.2, regularization_factor=1e-6):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.learning_rate = learning_rate\n        self.regularization_factor = regularization_factor\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.Inf\n        self.previous_best_f = np.Inf\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solution\n        best_index = np.argmin(f)\n        if f[best_index] < self.best_f:\n            self.best_f = f[best_index]\n            self.best_x = x[best_index]\n\n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n\n            # Update best solution\n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n                \n                # Adjust learning rate based on improvement\n                improvement = self.previous_best_f - self.best_f\n                if improvement > 0:\n                    self.learning_rate = min(0.5, self.learning_rate * 1.1) # Increase if improving\n                else:\n                    self.learning_rate = max(0.05, self.learning_rate * 0.9) # Decrease if not improving\n            \n            self.previous_best_f = self.best_f\n\n            # Adaptation of mean and covariance using only the best\n            s = self.best_x - self.mean\n            self.mean = (1 - self.learning_rate) * self.mean + self.learning_rate * self.best_x\n            \n            # Rank-one covariance update\n            self.covariance = (1 - self.learning_rate) * self.covariance + self.learning_rate * np.outer(s, s)\n\n            # Regularize covariance matrix\n            self.covariance += self.regularization_factor * np.eye(self.dim)\n\n        return self.best_f, self.best_x", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.493 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9d9d1e20-0a2d-4b13-a5df-f8032240670e"], "operator": null, "metadata": {"aucs": [0.1554123060704996, 0.25032669210788505, 0.5890900403172686, 0.5482811678594437, 0.549728879348226, 0.6561185844300865, 0.33331731940038256, 0.5038090477457425, 0.5774670484642696, 0.16776647415273005, 0.8691790787017639, 0.9972793833081267, 0.29279575149236636, 0.2829219337036353, 0.9138101069654737, 0.3420328150774321, 0.33250602134209206, 0.7913218656123684, 0.20438493086644516, 0.5050998100103513]}}
{"id": "ed7e0f16-2892-4efa-8384-6db593bd9940", "fitness": 0.3880388298374644, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with simplified covariance update and dynamic adaptation rate adjustment based on recent fitness improvements.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.Inf\n        self.fitness_history = []\n        self.smoothing = 0.9  # Smoothing factor for adaptation rate\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solution\n        best_index = np.argmin(f)\n        if f[best_index] < self.best_f:\n            self.best_f = f[best_index]\n            self.best_x = x[best_index]\n\n        self.fitness_history.append(self.best_f)\n\n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n\n            # Update best solution\n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n\n            self.fitness_history.append(self.best_f)\n\n            # Dynamic adaptation rate based on fitness improvement and smoothing\n            if len(self.fitness_history) > 1:\n                improvement = self.fitness_history[-2] - self.fitness_history[-1]\n                if improvement > 0:\n                    self.adaptation_rate = self.smoothing * self.adaptation_rate + (1 - self.smoothing) * min(self.adaptation_rate * 1.1, 0.4)\n                else:\n                    self.adaptation_rate = self.smoothing * self.adaptation_rate + (1 - self.smoothing) * max(self.adaptation_rate * 0.9, 0.01)\n\n            # Adaptation of mean and covariance using only the best\n            self.mean = self.smoothing * self.mean + (1 - self.smoothing) * self.best_x\n            deviation = self.best_x - self.mean\n            self.covariance = self.smoothing * self.covariance + (1 - self.smoothing) * (np.outer(deviation, deviation) + 1e-8 * np.eye(self.dim))\n\n\n        return self.best_f, self.best_x", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.388 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0b7ee236-d0c4-4664-b29f-6448629b43d5"], "operator": null, "metadata": {"aucs": [0.1849107277515143, 0.20398736532644102, 0.42500683383442006, 0.18198145825157352, 0.3403966229169133, 0.42449473290130757, 0.30968857852002707, 0.3553948063570186, 0.355051096473335, 0.1903370020244184, 0.5692864047711622, 0.9937676509798762, 0.2650830772620506, 0.32976073767450376, 0.6956713078975402, 0.34735424472297216, 0.3327615469064422, 0.5066604302136098, 0.24779354128251463, 0.5013884306816476]}}
{"id": "271addca-dbf8-429c-a97e-8de55bc90216", "fitness": 0.3749837324959122, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with simplified mean and covariance updates based on the best solution and a dynamic adaptation rate, aiming for faster convergence and robustness.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.Inf\n        self.fitness_history = []\n        self.smoothing = 0.9  # Smoothing factor for adaptation rate\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solution\n        best_index = np.argmin(f)\n        if f[best_index] < self.best_f:\n            self.best_f = f[best_index]\n            self.best_x = x[best_index]\n\n        self.fitness_history.append(self.best_f)\n\n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n\n            # Update best solution\n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n\n            self.fitness_history.append(self.best_f)\n\n            # Dynamic adaptation rate based on fitness improvement and smoothing\n            if len(self.fitness_history) > 1:\n                improvement = self.fitness_history[-2] - self.fitness_history[-1]\n                if improvement > 0:\n                    self.adaptation_rate = min(self.adaptation_rate * 1.1, 0.4)\n                else:\n                    self.adaptation_rate = max(self.adaptation_rate * 0.9, 0.01)\n\n            # Adaptation of mean and covariance using only the best\n            self.mean = self.smoothing * self.mean + (1 - self.smoothing) * self.best_x\n            deviation = self.best_x - self.mean\n            self.covariance = self.smoothing * self.covariance + (1 - self.smoothing) * np.diag(deviation**2)\n\n        return self.best_f, self.best_x", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.375 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0b7ee236-d0c4-4664-b29f-6448629b43d5"], "operator": null, "metadata": {"aucs": [0.12026347796057035, 0.19759432355273088, 0.41809730251116617, 0.453770463784092, 0.3107045890455722, 0.385175010085535, 0.28814021114386235, 0.33041789133954746, 0.3273374802118829, 0.20822473847302014, 0.39434078062438527, 0.9980787986815937, 0.27320455364353824, 0.3074706270001143, 0.7141501342865706, 0.315369117156417, 0.31199183416434795, 0.4275999903938691, 0.2225479487452564, 0.49519537711417205]}}
{"id": "92761222-83d7-4b8a-a7ae-af610b2fcf0f", "fitness": 0.33558393224192146, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with simplified rank-one covariance update, adaptive learning rate, and dynamic regularization for faster adaptation.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, learning_rate=0.2, regularization_factor=1e-6):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.learning_rate = learning_rate\n        self.regularization_factor = regularization_factor\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.Inf\n        self.previous_best_f = np.Inf\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solution\n        best_index = np.argmin(f)\n        if f[best_index] < self.best_f:\n            self.best_f = f[best_index]\n            self.best_x = x[best_index]\n\n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n\n            # Update best solution\n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n                \n                # Adjust learning rate based on improvement\n                improvement = self.previous_best_f - self.best_f\n                if improvement > 0:\n                    self.learning_rate = min(0.5, self.learning_rate * 1.1) # Increase if improving\n                else:\n                    self.learning_rate = max(0.05, self.learning_rate * 0.9) # Decrease if not improving\n                \n                # Adaptation of mean and covariance using only the best\n                y = self.best_x - self.mean\n                self.mean = self.mean + self.learning_rate * y\n\n                # Rank-one covariance update\n                y = y.reshape(-1, 1)\n                self.covariance = (1 - self.learning_rate) * self.covariance + self.learning_rate * (y @ y.T)\n                \n                # Regularize covariance matrix\n                self.covariance = self.covariance + self.regularization_factor * np.eye(self.dim)\n            \n            self.previous_best_f = self.best_f\n\n        return self.best_f, self.best_x", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.336 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9d9d1e20-0a2d-4b13-a5df-f8032240670e"], "operator": null, "metadata": {"aucs": [0.1256253756614002, 0.19734348007724178, 0.4885443068449825, 0.1880691770035806, 0.17768715739274088, 0.18248263747035265, 0.32553244433241824, 0.47736625746791683, 0.5471991242776284, 0.201688193452905, 0.8397911151561068, 0.6973105425853157, 0.3203239949586918, 0.27105183946865063, 0.1418998303259299, 0.400551760586758, 0.31590811308218203, 0.1959438126658536, 0.18448812447555252, 0.43287135755222084]}}
{"id": "4e36d815-bf90-4cf4-86c0-b9a75925b004", "fitness": 0.3641914586639988, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with simplified covariance update based on moving average and mean adaptation, and dynamic learning rate adjustment, further simplified.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.Inf\n        self.fitness_history = []\n        self.min_adaptation_rate = 0.01\n        self.max_adaptation_rate = 0.5\n        self.moving_avg_deviation = np.zeros(dim) # Initialize moving average\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solution\n        best_index = np.argmin(f)\n        if f[best_index] < self.best_f:\n            self.best_f = f[best_index]\n            self.best_x = x[best_index]\n\n        self.fitness_history.append(self.best_f)\n\n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n\n            # Update best solution\n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n\n            self.fitness_history.append(self.best_f)\n\n            # Dynamic adaptation rate based on fitness improvement\n            if len(self.fitness_history) > 1:\n                improvement = self.fitness_history[-2] - self.fitness_history[-1]\n                if improvement > 0:\n                    self.adaptation_rate = min(self.adaptation_rate * 1.1, self.max_adaptation_rate)\n                else:\n                    self.adaptation_rate = max(self.adaptation_rate * 0.9, self.min_adaptation_rate)\n\n            # Adaptation of mean and covariance using only the best\n            deviation = self.best_x - self.mean\n            self.mean = (1 - self.adaptation_rate) * self.mean + self.adaptation_rate * self.best_x\n\n            # Moving average update\n            self.moving_avg_deviation = (1 - self.adaptation_rate) * self.moving_avg_deviation + self.adaptation_rate * deviation\n\n            # Diagonal covariance update using moving average of deviation\n            self.covariance = (1 - self.adaptation_rate) * self.covariance + self.adaptation_rate * np.diag(self.moving_avg_deviation**2)\n\n        return self.best_f, self.best_x", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.364 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["7de90ee6-1e8f-48fb-934d-ae7559d5a2e5"], "operator": null, "metadata": {"aucs": [0.14426402201243693, 0.23272812360124173, 0.40783015466187855, 0.19509311512543004, 0.22944828508576576, 0.41614643582838073, 0.2651050920471333, 0.30847023808507945, 0.5149693708896208, 0.1824522656527271, 0.4181075069519846, 0.996662249915613, 0.2840842397302695, 0.28485031039711894, 0.663436750229736, 0.3404034917071713, 0.31024042986732736, 0.4143307157857967, 0.21310934619203759, 0.46209702951322607]}}
{"id": "14c57b00-7eb9-4be3-8554-380c6b488f97", "fitness": 0.4767570795916892, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with simplified rank-one covariance update and momentum-based mean adaptation for efficient exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=50, learning_rate=0.1, regularization_factor=1e-8, momentum=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.learning_rate = learning_rate\n        self.regularization_factor = regularization_factor\n        self.momentum = momentum\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.Inf\n        self.mean_velocity = np.zeros(dim) # Momentum for mean update\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solution\n        best_index = np.argmin(f)\n        if f[best_index] < self.best_f:\n            self.best_f = f[best_index]\n            self.best_x = x[best_index]\n\n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n\n            # Update best solution\n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n\n            # Adaptation of mean and covariance using only the best\n            # Momentum-based mean update\n            self.mean_velocity = self.momentum * self.mean_velocity + (1 - self.momentum) * (self.best_x - self.mean)\n            self.mean = self.mean + self.learning_rate * self.mean_velocity\n\n            # Rank-one covariance update using only the best solution's deviation\n            deviation = self.best_x - self.mean\n            self.covariance = (1 - self.learning_rate) * self.covariance + self.learning_rate * np.outer(deviation, deviation)\n            \n\n            # Regularize covariance matrix\n            self.covariance += self.regularization_factor * np.eye(self.dim)\n            \n            # Ensure covariance is positive definite (optional, but recommended)\n            try:\n                np.linalg.cholesky(self.covariance)\n            except np.linalg.LinAlgError:\n                # If not positive definite, add a bit more regularization\n                self.covariance += 1e-6 * np.eye(self.dim)\n\n\n        return self.best_f, self.best_x", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.477 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9d9d1e20-0a2d-4b13-a5df-f8032240670e"], "operator": null, "metadata": {"aucs": [0.15381820215821862, 0.18695161069832533, 0.5303767789631737, 0.7945466863298608, 0.4802331339106005, 0.5858199416739143, 0.28192536183903116, 0.44610720278324656, 0.525866503323873, 0.191944672144008, 0.755335661475775, 0.9844768557870294, 0.2879018545792348, 0.4858946991040468, 0.7699031985156802, 0.38104735655395927, 0.4024725334784446, 0.6522707152105729, 0.14722276630339748, 0.49102585700139234]}}
{"id": "50bdde79-2c57-4ed3-9ede-5612fe5971bd", "fitness": 0.4985660287816483, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with dynamic learning rate, covariance regularization, and focused covariance update based on the best solution's recent movements.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, learning_rate=0.2, regularization_factor=1e-6, improvement_threshold=1e-3):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.learning_rate = learning_rate\n        self.regularization_factor = regularization_factor\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.Inf\n        self.previous_best_x = None\n        self.recent_improvements = []\n        self.improvement_threshold = improvement_threshold\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solution\n        best_index = np.argmin(f)\n        self.best_f = f[best_index]\n        self.best_x = x[best_index]\n        self.previous_best_x = self.best_x.copy()\n\n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n\n            # Update best solution\n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n\n                # Adjust learning rate based on recent improvements\n                improvement = np.linalg.norm(self.previous_best_x - self.best_x)\n                self.recent_improvements.append(improvement)\n                if len(self.recent_improvements) > 10:\n                    self.recent_improvements.pop(0)\n\n                if np.mean(self.recent_improvements) < self.improvement_threshold:\n                     self.learning_rate = max(0.05, self.learning_rate * 0.9)\n                else:\n                    self.learning_rate = min(0.5, self.learning_rate * 1.1)\n\n                self.previous_best_x = self.best_x.copy()\n\n            # Adaptation of mean and covariance using only the best's movement\n            movement = self.best_x - self.mean\n            self.mean = (1 - self.learning_rate) * self.mean + self.learning_rate * self.best_x # Keep mean in updated position\n            self.covariance = (1 - self.learning_rate) * self.covariance + self.learning_rate * np.diag(movement**2)\n\n            # Regularize covariance matrix\n            self.covariance += self.regularization_factor * np.eye(self.dim)\n\n        return self.best_f, self.best_x", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.499 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9d9d1e20-0a2d-4b13-a5df-f8032240670e"], "operator": null, "metadata": {"aucs": [0.2346087331957979, 0.6635048464406634, 0.5807180682445492, 0.19562020038431227, 0.2637376743647656, 0.6283229818019167, 0.3287506207076528, 0.47301112385525457, 0.5624223069622463, 0.21025631837950864, 0.4681308185893396, 0.9966411490892081, 0.2556236288412268, 0.58253343508289, 0.7930113506047135, 0.5790814275093992, 0.5235145952293456, 0.7663124491712139, 0.37111071113599825, 0.49440813604296396]}}
{"id": "cd9d1443-31b7-48ed-8c76-affbbb817366", "fitness": 0.5034504422505764, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with simplified covariance rank-1 update based on the best solution and adaptive learning rate using a simplified moving average of fitness improvement.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.2, history_length=10):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.Inf\n        self.fitness_history = []\n        self.improvement_history = []\n        self.history_length = history_length\n        self.min_adaptation_rate = 0.01\n        self.max_adaptation_rate = 0.5\n        self.min_eig = 1e-6  # Minimum eigenvalue for covariance matrix regularization\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solution\n        best_index = np.argmin(f)\n        if f[best_index] < self.best_f:\n            self.best_f = f[best_index]\n            self.best_x = x[best_index]\n\n        self.fitness_history.append(self.best_f)\n\n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n\n            # Update best solution\n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n\n            self.fitness_history.append(self.best_f)\n\n            # Dynamic adaptation rate based on fitness improvement\n            if len(self.fitness_history) > 1:\n                improvement = self.fitness_history[-2] - self.fitness_history[-1]\n                self.improvement_history.append(improvement)\n                if len(self.improvement_history) > self.history_length:\n                    self.improvement_history.pop(0)\n\n                avg_improvement = np.mean(self.improvement_history)\n\n                if avg_improvement > 0:\n                    self.adaptation_rate = min(self.adaptation_rate * 1.1, self.max_adaptation_rate)\n                else:\n                    self.adaptation_rate = max(self.adaptation_rate * 0.9, self.min_adaptation_rate)\n\n            # Adaptation of mean and covariance using only the best\n            deviation = self.best_x - self.mean\n            self.mean = (1 - self.adaptation_rate) * self.mean + self.adaptation_rate * self.best_x\n\n            # Rank-1 covariance update focusing on best solution's deviation\n            deviation = deviation.reshape(-1, 1)  # Ensure deviation is a column vector\n            self.covariance = (1 - self.adaptation_rate) * self.covariance + self.adaptation_rate * (deviation @ deviation.T)\n\n            # Regularize covariance matrix\n            U, S, V = np.linalg.svd(self.covariance)\n            S = np.maximum(S, self.min_eig)  # Ensure eigenvalues are not too small\n            self.covariance = U @ np.diag(S) @ V\n\n        return self.best_f, self.best_x", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.503 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["7de90ee6-1e8f-48fb-934d-ae7559d5a2e5"], "operator": null, "metadata": {"aucs": [0.14757031004819487, 0.20904764843222834, 0.6045937117446605, 0.8576901577240744, 0.53350991789198, 0.6695708643463725, 0.29890249337301844, 0.5047415107101397, 0.629289762326383, 0.22433871824126128, 0.8569934329105608, 0.9826658437430346, 0.3170720000934866, 0.22888814490242737, 0.7814938465141649, 0.3717652307933157, 0.4039355724244734, 0.798989744897334, 0.15173005337365852, 0.4962198805207615]}}
{"id": "f32727e4-68ab-4148-b0bc-066f9ff34c3a", "fitness": 0.3265817982078266, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with simplified covariance update based on the best solution's deviation, momentum-based mean update, and dynamic adaptation rate based on consecutive improvements.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.2, momentum=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.Inf\n        self.fitness_history = []\n        self.min_adaptation_rate = 0.01\n        self.max_adaptation_rate = 0.5\n        self.momentum = momentum\n        self.mean_velocity = np.zeros(dim)\n\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solution\n        best_index = np.argmin(f)\n        if f[best_index] < self.best_f:\n            self.best_f = f[best_index]\n            self.best_x = x[best_index]\n\n        self.fitness_history.append(self.best_f)\n\n        consecutive_improvements = 0\n\n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n\n            # Update best solution\n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n                consecutive_improvements += 1\n            else:\n                consecutive_improvements = 0\n\n\n            self.fitness_history.append(self.best_f)\n\n            # Dynamic adaptation rate based on consecutive improvements\n            if consecutive_improvements > 5:\n                self.adaptation_rate = min(self.adaptation_rate * 1.2, self.max_adaptation_rate)\n                consecutive_improvements = 0  # Reset counter after rate increase\n            else:\n                self.adaptation_rate = max(self.adaptation_rate * 0.95, self.min_adaptation_rate)\n\n\n            # Adaptation of mean and covariance using only the best and momentum\n            deviation = self.best_x - self.mean\n            self.mean_velocity = self.momentum * self.mean_velocity + (1 - self.momentum) * deviation\n            self.mean = self.mean + self.adaptation_rate * self.mean_velocity\n\n\n            # Simplified covariance update focusing on best solution's deviation\n            self.covariance = (1 - self.adaptation_rate) * self.covariance + self.adaptation_rate * np.diag(deviation**2)\n\n\n        return self.best_f, self.best_x", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.327 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["7de90ee6-1e8f-48fb-934d-ae7559d5a2e5"], "operator": null, "metadata": {"aucs": [0.13282175939868135, 0.24193440370307628, 0.3735289732370537, 0.16905878771115113, 0.23847148878619817, 0.29870451241335627, 0.28411161751849745, 0.2653449279147956, 0.26881116131620497, 0.17004046314362786, 0.31822468100700885, 0.9995795948485583, 0.291781879961157, 0.24096167003693114, 0.6554819730561265, 0.27509844850004406, 0.27300157049042595, 0.3737883036974743, 0.1903286590357115, 0.4705610883804504]}}
{"id": "8373af6d-4bba-4c92-8480-d0f2358c4573", "fitness": 0.2865183234583886, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with simplified covariance update and adaptive learning rate annealing based on successful steps.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, learning_rate=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.learning_rate = learning_rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.Inf\n        self.success_rate = 0.0\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solution\n        best_index = np.argmin(f)\n        if f[best_index] < self.best_f:\n            self.best_f = f[best_index]\n            self.best_x = x[best_index]\n            self.success_rate = 1.0\n\n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n\n            # Update best solution\n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n                self.success_rate = 0.8  # Increased success rate\n                \n                # Adaptation of mean and covariance using only the best\n                deviation = self.best_x - self.mean\n                self.mean = (1 - self.learning_rate) * self.mean + self.learning_rate * self.best_x\n\n                # Simplified covariance update\n                self.covariance = (1 - self.learning_rate) * self.covariance + self.learning_rate * np.outer(deviation, deviation)\n\n                # Adjust learning rate if successful\n                self.learning_rate *= 1.1\n            else:\n                self.success_rate *= 0.5  # Decreased success rate\n                # Reduce learning rate if not successful\n                self.learning_rate *= 0.9\n\n            # Clip learning rate\n            self.learning_rate = np.clip(self.learning_rate, 0.01, 0.5)\n            \n        return self.best_f, self.best_x", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.287 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d31825bd-5a24-45ac-b633-9e86fe13acfc"], "operator": null, "metadata": {"aucs": [0.15138254211707425, 0.22278703775933273, 0.3612275026674493, 0.17837991800260022, 0.376788758433263, 0.18227913582870392, 0.2134480394065691, 0.2954422527585804, 0.2061897190386982, 0.1564877662609393, 0.2847432612366533, 0.9950204149692149, 0.27844557927309255, 0.19770506721495817, 0.17723844364549746, 0.28698580326360246, 0.3212769048586992, 0.20711097173569293, 0.19307237443555136, 0.44435497626159925]}}
{"id": "164b5d07-e99c-4243-bcdd-af6baf7d1ce5", "fitness": 0.35593367946057086, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with simplified rank-one covariance update and dynamic learning rate based on fitness improvement, further regularized.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, learning_rate=0.2, regularization=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.learning_rate = learning_rate\n        self.regularization = regularization\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.Inf\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solution\n        best_index = np.argmin(f)\n        if f[best_index] < self.best_f:\n            self.best_f = f[best_index]\n            self.best_x = x[best_index]\n\n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n\n            # Update best solution\n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n                improvement = True\n            else:\n                improvement = False\n\n            # Adaptation of mean and covariance using only the best\n            deviation = self.best_x - self.mean\n            self.mean = (1 - self.learning_rate) * self.mean + self.learning_rate * self.best_x\n\n            # Rank-one covariance update using the deviation\n            self.covariance = (1 - self.learning_rate) * self.covariance + self.learning_rate * np.outer(deviation, deviation)\n\n            # Regularize covariance\n            self.covariance += self.regularization * np.eye(self.dim)\n\n            # Dynamic learning rate\n            if improvement:\n                self.learning_rate *= 1.1\n            else:\n                self.learning_rate *= 0.9\n\n            self.learning_rate = np.clip(self.learning_rate, 0.01, 0.3)\n\n\n        return self.best_f, self.best_x", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.356 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d31825bd-5a24-45ac-b633-9e86fe13acfc"], "operator": null, "metadata": {"aucs": [0.17548517857946644, 0.17688420700436813, 0.386881184596255, 0.39822756771912704, 0.27393730840950525, 0.38221388275468327, 0.26678221456627127, 0.30499875855233516, 0.2995811196561442, 0.18410313922828203, 0.3857563110109262, 0.9810127404504485, 0.2668720788797996, 0.30621492152727336, 0.45350838065358823, 0.41135535858775907, 0.3034551922516512, 0.4804210856301574, 0.20997188430664016, 0.4710110748467353]}}
{"id": "c39cf3fb-e952-4319-bb67-244569e3e6dd", "fitness": 0.43456206444046347, "name": "AdaptiveCovarianceSampling", "description": "Simplified adaptive covariance sampling with momentum-based mean adaptation and diagonal covariance for faster computation and reduced parameter count.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=50, learning_rate=0.1, regularization_factor=1e-8, momentum=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.learning_rate = learning_rate\n        self.regularization_factor = regularization_factor\n        self.momentum = momentum\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mean = np.zeros(dim)\n        self.covariance = np.ones(dim)  # Diagonal covariance\n        self.best_x = None\n        self.best_f = np.Inf\n        self.mean_velocity = np.zeros(dim) # Momentum for mean update\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.normal(self.mean, np.sqrt(self.covariance), size=(num_samples, self.dim))\n        except:\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solution\n        best_index = np.argmin(f)\n        if f[best_index] < self.best_f:\n            self.best_f = f[best_index]\n            self.best_x = x[best_index]\n\n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n\n            # Update best solution\n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n\n            # Adaptation of mean and covariance using only the best\n            # Momentum-based mean update\n            self.mean_velocity = self.momentum * self.mean_velocity + (1 - self.momentum) * (self.best_x - self.mean)\n            self.mean = self.mean + self.learning_rate * self.mean_velocity\n\n            # Rank-one covariance update using only the best solution's deviation\n            deviation = self.best_x - self.mean\n            self.covariance = (1 - self.learning_rate) * self.covariance + self.learning_rate * deviation**2\n\n            # Regularize covariance matrix\n            self.covariance += self.regularization_factor\n            self.covariance = np.maximum(self.covariance, 1e-6) # Ensure covariance is positive\n\n        return self.best_f, self.best_x", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.435 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["14c57b00-7eb9-4be3-8554-380c6b488f97"], "operator": null, "metadata": {"aucs": [0.14292609738548057, 0.20314266525310498, 0.5070676077722356, 0.1967376138109782, 0.47790433279830025, 0.5504871482850175, 0.31603342734083617, 0.42326181793505024, 0.47856370258371905, 0.194769804377543, 0.6775039009627221, 0.9981102015038814, 0.2740893490538665, 0.2855720022596453, 0.731701466764851, 0.4557815548101708, 0.4509631363974177, 0.6504177607883064, 0.21058148299800794, 0.46562621572813534]}}
{"id": "1ab2907a-fa78-4321-8ac8-e799c8e3fad6", "fitness": 0.285967684283019, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with simplified rank-one covariance update, adaptive learning rate based on fitness improvement, and covariance regularization using a fixed small value to ensure positive definiteness.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, learning_rate=0.2, regularization_factor=1e-8):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.learning_rate = learning_rate\n        self.regularization_factor = regularization_factor\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.Inf\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solution\n        best_index = np.argmin(f)\n        self.best_f = f[best_index]\n        self.best_x = x[best_index]\n\n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n\n            # Update best solution\n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n\n                # Adaptation of mean and covariance using only the best solution\n                movement = self.best_x - self.mean\n                self.mean = (1 - self.learning_rate) * self.mean + self.learning_rate * self.best_x\n                \n                # Rank-1 update with regularization\n                movement = movement.reshape(-1, 1)\n                self.covariance = (1 - self.learning_rate) * self.covariance + self.learning_rate * (movement @ movement.T) + self.regularization_factor * np.eye(self.dim)\n\n\n        return self.best_f, self.best_x", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.286 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["50bdde79-2c57-4ed3-9ede-5612fe5971bd"], "operator": null, "metadata": {"aucs": [0.1247266049714697, 0.18050031424797075, 0.3647411472073592, 0.1579622632322173, 0.26549346125339357, 0.28343390414709224, 0.2748050095724851, 0.30059220481591087, 0.2519534189548728, 0.17285451454962475, 0.38543919940010973, 0.997366647869491, 0.27695745585526665, 0.1683663143396339, 0.1504669884961649, 0.29326680497398794, 0.2236025091644288, 0.1864761087945488, 0.1767624414870601, 0.48358637232729285]}}
{"id": "4d7ef7fc-f096-4315-ab89-053a1007792f", "fitness": 0.48329387659033085, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with simplified rank-one covariance update and adaptive learning rate based on fitness improvement, with covariance regularization, and simplified mean update.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, learning_rate=0.2, regularization_factor=1e-6):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.learning_rate = learning_rate\n        self.regularization_factor = regularization_factor\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.Inf\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solution\n        best_index = np.argmin(f)\n        if f[best_index] < self.best_f:\n            self.best_f = f[best_index]\n            self.best_x = x[best_index]\n\n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n\n            # Update best solution\n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                improvement = self.best_f - f[best_index]\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n                \n                # Adjust learning rate based on improvement\n                if improvement > 0:\n                    self.learning_rate = min(0.5, self.learning_rate * 1.1) # Increase if improving\n                else:\n                    self.learning_rate = max(0.05, self.learning_rate * 0.9) # Decrease if not improving\n            else:\n                 improvement = 0\n                \n\n            # Adaptation of mean and covariance using only the best\n            s = self.best_x - self.mean\n            self.mean = self.mean + self.learning_rate * s # Simplified mean update\n            \n            # Rank-one covariance update\n            self.covariance = (1 - self.learning_rate) * self.covariance + self.learning_rate * np.outer(s, s)\n\n            # Regularize covariance matrix\n            self.covariance += self.regularization_factor * np.eye(self.dim)\n\n        return self.best_f, self.best_x", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.483 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5c112d1c-596c-49cb-82fd-8cf2250e5d4a"], "operator": null, "metadata": {"aucs": [0.19438289343523374, 0.32556201147560426, 0.59979767450592, 0.2584721328439482, 0.24817832748306634, 0.6539055402915821, 0.3135688205069619, 0.41964970061373263, 0.6257418340275002, 0.2059433146245524, 0.8765999667991439, 0.992167128873066, 0.3087253247217264, 0.2843479520613592, 0.8013590083314751, 0.4572319311247336, 0.5192414677093741, 0.789057588934499, 0.28933031059800474, 0.502614602845133]}}
{"id": "29c48d76-cdfe-4ced-8e27-f3eca817760b", "fitness": 0.36573865500955666, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with simplified rank-one covariance update and dynamic adjustment of exploration-exploitation balance using fitness variance.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, learning_rate=0.2, regularization_factor=1e-8, variance_threshold=1e-4):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.learning_rate = learning_rate\n        self.regularization_factor = regularization_factor\n        self.variance_threshold = variance_threshold\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.Inf\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solution\n        best_index = np.argmin(f)\n        if f[best_index] < self.best_f:\n            self.best_f = f[best_index]\n            self.best_x = x[best_index]\n\n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n\n            # Update best solution\n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n\n            # Adaptation of mean and covariance using only the best\n            s = self.best_x - self.mean\n            self.mean = (1 - self.learning_rate) * self.mean + self.learning_rate * self.best_x\n\n            # Rank-one covariance update\n            self.covariance = (1 - self.learning_rate) * self.covariance + self.learning_rate * np.outer(s, s)\n\n            # Regularize covariance matrix\n            self.covariance += self.regularization_factor * np.eye(self.dim)\n            \n            # Adjust learning rate based on fitness variance\n            fitness_variance = np.var(f)\n            if fitness_variance < self.variance_threshold:\n                self.learning_rate = min(0.5, self.learning_rate * 1.1)  # Increase exploration\n            else:\n                self.learning_rate = max(0.05, self.learning_rate * 0.9)  # Increase exploitation\n\n        return self.best_f, self.best_x", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.366 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5c112d1c-596c-49cb-82fd-8cf2250e5d4a"], "operator": null, "metadata": {"aucs": [0.13740437055642307, 0.21086592003072624, 0.3895811901504833, 0.42438568981282854, 0.2885339910506888, 0.36510553265623735, 0.298247675204949, 0.3120661413904773, 0.3015785274845021, 0.19246138005871727, 0.4143056205922566, 0.9985852749866709, 0.27007267143327507, 0.28854101875482985, 0.6648382631943182, 0.3123528610072217, 0.3198760011516397, 0.41232730459025546, 0.23150709826301774, 0.4821365678216163]}}
{"id": "d29b67c2-9e99-4e21-965f-1a6c47c8249f", "fitness": 0.46776037306974017, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with simplified rank-one covariance update, adaptive learning rate based on fitness improvement, covariance regularization, and a simplified mean update for efficient exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=50, learning_rate=0.1, regularization_factor=1e-8):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.learning_rate = learning_rate\n        self.regularization_factor = regularization_factor\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.Inf\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solution\n        best_index = np.argmin(f)\n        self.best_f = f[best_index]\n        self.best_x = x[best_index]\n\n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n\n            # Update best solution\n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n\n            # Adaptation of mean and covariance using only the best\n            # Simplified mean update\n            self.mean = (1 - self.learning_rate) * self.mean + self.learning_rate * self.best_x\n\n            # Rank-one covariance update using only the best solution's deviation\n            deviation = self.best_x - self.mean\n            self.covariance = (1 - self.learning_rate) * self.covariance + self.learning_rate * np.outer(deviation, deviation)\n            \n\n            # Regularize covariance matrix\n            self.covariance += self.regularization_factor * np.eye(self.dim)\n            \n            # Ensure covariance is positive definite (optional, but recommended)\n            try:\n                np.linalg.cholesky(self.covariance)\n            except np.linalg.LinAlgError:\n                # If not positive definite, add a bit more regularization\n                self.covariance += 1e-6 * np.eye(self.dim)\n\n\n        return self.best_f, self.best_x", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.468 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["14c57b00-7eb9-4be3-8554-380c6b488f97"], "operator": null, "metadata": {"aucs": [0.15412350406531916, 0.24605293575979048, 0.5022382080264617, 0.7420228848714928, 0.29771631288071354, 0.5712911602929636, 0.31401039373366435, 0.44496763222460645, 0.5143881157474184, 0.1771505740385304, 0.8040352699768764, 0.9885832022494913, 0.3392404814800203, 0.37439095954259005, 0.7106846215687301, 0.3678665893515688, 0.46753298731924187, 0.6692812528812957, 0.19518754214824163, 0.4744428332357856]}}
{"id": "b6691be2-ca33-4144-860b-7b55c17ccd4e", "fitness": 0.3433039480520605, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with simplified covariance update using only the best solution's movement, adaptive learning rate based on successful steps, and covariance regularization.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, learning_rate=0.2, regularization_factor=1e-6):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.learning_rate = learning_rate\n        self.regularization_factor = regularization_factor\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.Inf\n        self.success_rate = 0.0\n        self.success_history = []\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solution\n        best_index = np.argmin(f)\n        self.best_f = f[best_index]\n        self.best_x = x[best_index]\n\n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n\n            # Update best solution\n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n                self.success_history.append(1)\n\n                # Adaptation of mean and covariance using only the best's movement\n                movement = self.best_x - self.mean\n                self.mean = (1 - self.learning_rate) * self.mean + self.learning_rate * self.best_x\n                self.covariance = (1 - self.learning_rate) * self.covariance + self.learning_rate * np.outer(movement, movement)\n                \n                # Adjust learning rate based on success\n                self.success_rate = np.mean(self.success_history[-10:]) if len(self.success_history) > 0 else 0\n                self.learning_rate = min(0.5, self.learning_rate * (1 + 0.2 * (self.success_rate - 0.2))) # Dampen learning rate changes.\n            else:\n                self.success_history.append(0)\n\n            # Regularize covariance matrix\n            self.covariance += self.regularization_factor * np.eye(self.dim)\n\n        return self.best_f, self.best_x", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.343 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["50bdde79-2c57-4ed3-9ede-5612fe5971bd"], "operator": null, "metadata": {"aucs": [0.12028686625002716, 0.22194503743505534, 0.3873335489617733, 0.16585924440657873, 0.18432785769469895, 0.18234066349260603, 0.30047942729567745, 0.41979858318163454, 0.19757034694624964, 0.18956276541998773, 0.8595101002559107, 0.9994187662158679, 0.2802323741663949, 0.4491044327529097, 0.4471616514790713, 0.3534620378438391, 0.30863697980760585, 0.1851107725897747, 0.17385904445242772, 0.44007846039312015]}}
{"id": "8c4e099a-38c1-402a-8436-cbaf4b2848d2", "fitness": 0.4735594998631608, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with simplified rank-one covariance update and adaptive learning rate based on recent fitness improvements for efficient exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.2, history_length=10):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.Inf\n        self.fitness_history = []\n        self.improvement_history = []\n        self.history_length = history_length\n        self.min_adaptation_rate = 0.01\n        self.max_adaptation_rate = 0.5\n        self.min_eig = 1e-6  # Minimum eigenvalue for covariance matrix regularization\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solution\n        best_index = np.argmin(f)\n        if f[best_index] < self.best_f:\n            self.best_f = f[best_index]\n            self.best_x = x[best_index]\n\n        self.fitness_history.append(self.best_f)\n\n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n\n            # Update best solution\n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n\n            self.fitness_history.append(self.best_f)\n\n            # Dynamic adaptation rate based on fitness improvement\n            if len(self.fitness_history) > 1:\n                improvement = self.fitness_history[-2] - self.fitness_history[-1]\n                self.improvement_history.append(improvement)\n                if len(self.improvement_history) > self.history_length:\n                    self.improvement_history.pop(0)\n\n                avg_improvement = np.mean(self.improvement_history)\n\n                if avg_improvement > 0:\n                    self.adaptation_rate = min(self.adaptation_rate * 1.1, self.max_adaptation_rate)\n                else:\n                    self.adaptation_rate = max(self.adaptation_rate * 0.9, self.min_adaptation_rate)\n\n            # Adaptation of mean and covariance using only the best\n            deviation = self.best_x - self.mean\n            self.mean = (1 - self.adaptation_rate) * self.mean + self.adaptation_rate * self.best_x\n\n            # Rank-1 covariance update focusing on best solution's deviation\n            deviation = deviation.reshape(-1, 1)  # Ensure deviation is a column vector\n            self.covariance = (1 - self.adaptation_rate) * self.covariance + self.adaptation_rate * (deviation @ deviation.T)\n            \n            # Regularize covariance matrix using a simpler approach\n            self.covariance = (1 - self.min_eig) * self.covariance + self.min_eig * np.eye(self.dim)\n\n        return self.best_f, self.best_x", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.474 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["cd9d1443-31b7-48ed-8c76-affbbb817366"], "operator": null, "metadata": {"aucs": [0.21622132559725937, 0.1841859121545084, 0.5821401753066737, 0.843210201598375, 0.625453600702326, 0.6579411672978046, 0.2878730824341975, 0.24461032392046267, 0.6106188361847499, 0.18204131948446267, 0.878113022860793, 0.9990059583061338, 0.33520462126857653, 0.3820900932783453, 0.8666080914117116, 0.36068817763551475, 0.2995980560712417, 0.22208395596667985, 0.19121390474797983, 0.5022881710354214]}}
{"id": "cdc09197-736b-4d79-a6ce-6c8a196c2b75", "fitness": 0.5244933553664769, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with simplified rank-one covariance update and adaptive learning rate based on fitness improvement, further simplified by removing the previous best fitness check and adjusting the learning rate directly based on the current best solution.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, learning_rate=0.2, regularization_factor=1e-6):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.learning_rate = learning_rate\n        self.regularization_factor = regularization_factor\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.Inf\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solution\n        best_index = np.argmin(f)\n        if f[best_index] < self.best_f:\n            self.best_f = f[best_index]\n            self.best_x = x[best_index]\n\n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n\n            # Update best solution\n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                # Adjust learning rate based on improvement\n                if self.best_x is not None:\n                    improvement = self.best_f - f[best_index]\n                    if improvement > 0:\n                        self.learning_rate = min(0.5, self.learning_rate * 1.1) # Increase if improving\n                    else:\n                        self.learning_rate = max(0.05, self.learning_rate * 0.9) # Decrease if not improving\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n\n            # Adaptation of mean and covariance using only the best\n            s = self.best_x - self.mean\n            self.mean = (1 - self.learning_rate) * self.mean + self.learning_rate * self.best_x\n            \n            # Rank-one covariance update\n            self.covariance = (1 - self.learning_rate) * self.covariance + self.learning_rate * np.outer(s, s)\n\n            # Regularize covariance matrix\n            self.covariance += self.regularization_factor * np.eye(self.dim)\n\n        return self.best_f, self.best_x", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.524 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5c112d1c-596c-49cb-82fd-8cf2250e5d4a"], "operator": null, "metadata": {"aucs": [0.1538981210858441, 0.1789559498890647, 0.5879880027732599, 0.789733516232993, 0.5980695358433596, 0.6580611939151697, 0.2864245823871392, 0.5188958972271871, 0.600661056314937, 0.21382132427548972, 0.8693613997126142, 0.9979811123886816, 0.26490175652836423, 0.5946477202528556, 0.8341649441660524, 0.32074376910595903, 0.5182944249694668, 0.7753187584789559, 0.26104708695146306, 0.46689695483068117]}}
{"id": "33eb0048-445b-44ac-9cf7-4575746aae33", "fitness": 0.34552590792763577, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with dynamic adaptation rate based on fitness improvement, simplified covariance update using a moving average of past best solutions, and covariance regularization.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.2, history_length=10):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.Inf\n        self.fitness_history = []\n        self.improvement_history = []\n        self.history_length = history_length\n        self.min_adaptation_rate = 0.01\n        self.max_adaptation_rate = 0.5\n        self.min_eig = 1e-6\n        self.past_best_x = []\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solution\n        best_index = np.argmin(f)\n        if f[best_index] < self.best_f:\n            self.best_f = f[best_index]\n            self.best_x = x[best_index]\n            self.past_best_x.append(self.best_x)\n            if len(self.past_best_x) > self.history_length:\n                self.past_best_x.pop(0)\n\n        self.fitness_history.append(self.best_f)\n\n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n\n            # Update best solution\n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n                self.past_best_x.append(self.best_x)\n                if len(self.past_best_x) > self.history_length:\n                    self.past_best_x.pop(0)\n\n            self.fitness_history.append(self.best_f)\n\n            # Dynamic adaptation rate based on fitness improvement\n            if len(self.fitness_history) > 1:\n                improvement = self.fitness_history[-2] - self.fitness_history[-1]\n                self.improvement_history.append(improvement)\n                if len(self.improvement_history) > self.history_length:\n                    self.improvement_history.pop(0)\n\n                avg_improvement = np.mean(self.improvement_history)\n\n                if avg_improvement > 0:\n                    self.adaptation_rate = min(self.adaptation_rate * 1.1, self.max_adaptation_rate)\n                else:\n                    self.adaptation_rate = max(self.adaptation_rate * 0.9, self.min_adaptation_rate)\n\n            # Adaptation of mean and covariance using a moving average of past best solutions\n            if self.past_best_x:\n                self.mean = np.mean(self.past_best_x, axis=0)\n\n            # Simplified covariance update\n            self.covariance = (1 - self.adaptation_rate) * self.covariance + self.adaptation_rate * np.eye(self.dim) * np.var(np.array(self.past_best_x), axis=0).mean()\n\n            # Regularize covariance matrix\n            U, S, V = np.linalg.svd(self.covariance)\n            S = np.maximum(S, self.min_eig)\n            self.covariance = U @ np.diag(S) @ V\n\n        return self.best_f, self.best_x", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.346 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["cd9d1443-31b7-48ed-8c76-affbbb817366"], "operator": null, "metadata": {"aucs": [0.13791448720854915, 0.24787759159349254, 0.3637255515741301, 0.19760374995767638, 0.3645624075493413, 0.22839546690523904, 0.26137983856413316, 0.37407312755578237, 0.29352835279995426, 0.1728058646258228, 0.40106402781784845, 0.9967795915257848, 0.25129860089174494, 0.24393217171158632, 0.5834237533254176, 0.32172519490350493, 0.3234294011600287, 0.4796757392828387, 0.20360872141913178, 0.46371451818070764]}}
{"id": "83065016-91c3-4a66-97f5-b54b0219fdcd", "fitness": 0.3491903509618891, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with simplified rank-one covariance update, adaptive learning rate, and covariance regularization, focusing on computational efficiency and parameter reduction.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.1, history_length=5):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.Inf\n        self.fitness_history = []\n        self.history_length = history_length\n        self.min_adaptation_rate = 0.01\n        self.min_eig = 1e-6\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solution\n        best_index = np.argmin(f)\n        if f[best_index] < self.best_f:\n            self.best_f = f[best_index]\n            self.best_x = x[best_index]\n\n        self.fitness_history.append(self.best_f)\n\n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n\n            # Update best solution\n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n\n            self.fitness_history.append(self.best_f)\n\n            # Dynamic adaptation rate based on fitness improvement\n            if len(self.fitness_history) > self.history_length:\n                improvement = np.mean(np.diff(self.fitness_history[-self.history_length:]))\n                self.adaptation_rate = np.clip(self.adaptation_rate * np.exp(improvement), self.min_adaptation_rate, 0.5)\n\n            # Simplified adaptation of mean and covariance using only the best\n            deviation = self.best_x - self.mean\n            self.mean = (1 - self.adaptation_rate) * self.mean + self.adaptation_rate * self.best_x\n\n            # Rank-1 covariance update focusing on best solution's deviation\n            deviation = deviation.reshape(-1, 1)\n            self.covariance = (1 - self.adaptation_rate) * self.covariance + self.adaptation_rate * (deviation @ deviation.T)\n\n            # Regularize covariance matrix\n            U, S, V = np.linalg.svd(self.covariance)\n            S = np.maximum(S, self.min_eig)\n            self.covariance = U @ np.diag(S) @ V\n\n        return self.best_f, self.best_x", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.349 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["cd9d1443-31b7-48ed-8c76-affbbb817366"], "operator": null, "metadata": {"aucs": [0.13888792588413834, 0.22582384581131587, 0.406035394511031, 0.3562883713597955, 0.27082963139044913, 0.29914950133624585, 0.27618322831989095, 0.2963961362956645, 0.26966451109455314, 0.18881434382359708, 0.3219912584428465, 0.9792666229544791, 0.270258271882988, 0.2590602806386896, 0.7134967727196406, 0.3094614391498547, 0.29978315501865216, 0.4283545309110537, 0.1893861462827725, 0.48467565141012336]}}
{"id": "c39f7919-c6af-43cf-9981-5140db194ab7", "fitness": 0.5185815619123931, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with simplified rank-one covariance update based on the best solution's movement and adaptive learning rate based on the success rate of improvements.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, learning_rate=0.2, regularization_factor=1e-6, success_threshold=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.learning_rate = learning_rate\n        self.regularization_factor = regularization_factor\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.Inf\n        self.successes = 0\n        self.trials = 0\n        self.success_threshold = success_threshold\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solution\n        best_index = np.argmin(f)\n        self.best_f = f[best_index]\n        self.best_x = x[best_index]\n\n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n\n            # Update best solution\n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n                self.successes += 1\n\n            self.trials += 1\n\n            # Adjust learning rate based on success rate\n            success_rate = self.successes / self.trials if self.trials > 0 else 0\n            if success_rate > self.success_threshold:\n                self.learning_rate = min(0.5, self.learning_rate * 1.1)\n            else:\n                self.learning_rate = max(0.05, self.learning_rate * 0.9)\n\n            # Adaptation of mean and covariance using only the best's movement\n            movement = self.best_x - self.mean\n            self.mean = (1 - self.learning_rate) * self.mean + self.learning_rate * self.best_x\n            self.covariance = (1 - self.learning_rate) * self.covariance + self.learning_rate * np.outer(movement, movement)\n\n            # Regularize covariance matrix\n            self.covariance += self.regularization_factor * np.eye(self.dim)\n\n        return self.best_f, self.best_x", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.519 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["50bdde79-2c57-4ed3-9ede-5612fe5971bd"], "operator": null, "metadata": {"aucs": [0.13655292788579831, 0.18316411653637732, 0.6010784481905279, 0.7392682116191973, 0.3397513217910283, 0.6675573102275627, 0.3095416238287406, 0.5065055329417752, 0.6169488687011782, 0.31060478340784503, 0.8509633114389119, 0.9990042923005996, 0.3257678622878598, 0.22322661171142577, 0.8011135669312582, 0.6381834693267248, 0.5270565141772808, 0.7844093245491766, 0.31579609559195876, 0.49513704480263654]}}
{"id": "e81c7915-a979-4d36-9ea3-a03a74c6b70d", "fitness": 0.48538624911740647, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with rank-one covariance update and dynamic learning rate adjusted by exponential moving average of fitness improvement.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.2, ema_alpha=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.Inf\n        self.ema_improvement = 0.0  # Exponential moving average of improvement\n        self.ema_alpha = ema_alpha\n        self.min_adaptation_rate = 0.01\n        self.max_adaptation_rate = 0.5\n        self.min_eig = 1e-6\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solution\n        best_index = np.argmin(f)\n        if f[best_index] < self.best_f:\n            self.best_f = f[best_index]\n            self.best_x = x[best_index]\n\n        prev_best_f = self.best_f  # Store previous best fitness\n\n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n\n            # Update best solution\n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n\n            # Dynamic adaptation rate based on fitness improvement\n            improvement = prev_best_f - self.best_f  # Use previous best f for improvement\n            self.ema_improvement = self.ema_alpha * improvement + (1 - self.ema_alpha) * self.ema_improvement\n\n            if self.ema_improvement > 0:\n                self.adaptation_rate = min(self.adaptation_rate * 1.1, self.max_adaptation_rate)\n            else:\n                self.adaptation_rate = max(self.adaptation_rate * 0.9, self.min_adaptation_rate)\n\n            # Adaptation of mean and covariance using only the best\n            deviation = self.best_x - self.mean\n            self.mean = (1 - self.adaptation_rate) * self.mean + self.adaptation_rate * self.best_x\n\n            # Rank-1 covariance update focusing on best solution's deviation\n            deviation = deviation.reshape(-1, 1)\n            self.covariance = (1 - self.adaptation_rate) * self.covariance + self.adaptation_rate * (deviation @ deviation.T)\n\n            # Regularize covariance matrix\n            U, S, V = np.linalg.svd(self.covariance)\n            S = np.maximum(S, self.min_eig)\n            self.covariance = U @ np.diag(S) @ V\n            \n            prev_best_f = self.best_f\n\n        return self.best_f, self.best_x", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.485 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["cd9d1443-31b7-48ed-8c76-affbbb817366"], "operator": null, "metadata": {"aucs": [0.19522800915785543, 0.20914046067140013, 0.6053503995923875, 0.2569592362498295, 0.4151322971839815, 0.6718027128582021, 0.5217469604397276, 0.516793876186412, 0.6110121960058438, 0.2115515327843016, 0.8772845483623654, 0.9993986805824812, 0.2665750965590248, 0.2888662916671343, 0.7298220815332418, 0.434523667651028, 0.4313942009915037, 0.7653675503920743, 0.19622223832187713, 0.5035529451574581]}}
{"id": "037634ee-8cdd-4206-8d51-bd12ab7d42fc", "fitness": 0.4792061410648486, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with simplified rank-one covariance update and adaptive learning rate, regularized covariance, and simplified mean update, further simplified by removing explicit success counting and directly adjusting the learning rate based on the relative fitness improvement.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, learning_rate=0.2, regularization_factor=1e-6):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.learning_rate = learning_rate\n        self.regularization_factor = regularization_factor\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.Inf\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solution\n        best_index = np.argmin(f)\n        self.best_f = f[best_index]\n        self.best_x = x[best_index]\n\n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n\n            # Update best solution\n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                # Adjust learning rate based on fitness improvement\n                improvement_ratio = (self.best_f - f[best_index]) / self.best_f if self.best_f != 0 else 1.0  # Avoid division by zero\n                self.learning_rate = min(0.5, self.learning_rate * (1 + improvement_ratio))\n\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n            else:\n                self.learning_rate = max(0.05, self.learning_rate * 0.95)\n\n            # Adaptation of mean and covariance using only the best's movement\n            movement = self.best_x - self.mean\n            self.mean = (1 - self.learning_rate) * self.mean + self.learning_rate * self.best_x\n            self.covariance = (1 - self.learning_rate) * self.covariance + self.learning_rate * np.outer(movement, movement)\n\n            # Regularize covariance matrix\n            self.covariance += self.regularization_factor * np.eye(self.dim)\n\n        return self.best_f, self.best_x", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.479 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c39f7919-c6af-43cf-9981-5140db194ab7"], "operator": null, "metadata": {"aucs": [0.1805329159928789, 0.2209029245324482, 0.5871693518010146, 0.8429050133904882, 0.2561319097818362, 0.6573820327298627, 0.28373670672139517, 0.47736355015235754, 0.6019489831844465, 0.20966282026169336, 0.9012803739139719, 0.9958771260819008, 0.29090977708613064, 0.21397321483633425, 0.7928360465376955, 0.34813564067908076, 0.2983768907551917, 0.7158020017244768, 0.2189772814531683, 0.49021825968059873]}}
{"id": "b052580e-dd4b-40ac-8085-c2c3ea1ad06f", "fitness": 0.4446894170895087, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with simplified rank-one covariance update and adaptive learning rate based on fitness improvement, further simplifying learning rate adjustment and covariance update.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, learning_rate=0.2, regularization_factor=1e-6):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.learning_rate = learning_rate\n        self.regularization_factor = regularization_factor\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.Inf\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solution\n        best_index = np.argmin(f)\n        if f[best_index] < self.best_f:\n            self.best_f = f[best_index]\n            self.best_x = x[best_index]\n\n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n\n            # Update best solution\n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                improvement = self.best_f - f[best_index]\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n                \n                # Adjust learning rate based on improvement\n                self.learning_rate = min(0.5, self.learning_rate * (1 + 0.1 * (improvement > 0)))\n            else:\n                self.learning_rate = max(0.05, self.learning_rate * 0.9)\n\n            # Adaptation of mean and covariance using only the best\n            s = self.best_x - self.mean\n            self.mean = self.mean + self.learning_rate * s # Simplified mean update\n            \n            # Rank-one covariance update\n            self.covariance = (1 - self.learning_rate) * self.covariance + self.learning_rate * np.outer(s, s)\n\n            # Regularize covariance matrix\n            self.covariance += self.regularization_factor * np.eye(self.dim)\n\n        return self.best_f, self.best_x", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.445 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["4d7ef7fc-f096-4315-ab89-053a1007792f"], "operator": null, "metadata": {"aucs": [0.1692712707572337, 0.2159381066044299, 0.44665690299279215, 0.5923270533821121, 0.5628747375456424, 0.45642987943410995, 0.2926092410786647, 0.4241505973062638, 0.31071918370154994, 0.19010957184281507, 0.7390683033653617, 0.9842533064297239, 0.27230060879683593, 0.49443207032904446, 0.6931279008401638, 0.499448699886302, 0.3530916465168571, 0.4999490908213252, 0.2106925620710315, 0.4863376080879148]}}
{"id": "af0f3443-d466-427f-94a0-c3b060c3f30f", "fitness": 0.38201563713755216, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with simplified rank-one covariance update, adaptive learning rate, and covariance damping for exploration.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, learning_rate=0.2, damping_factor=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.learning_rate = learning_rate\n        self.damping_factor = damping_factor\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.Inf\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solution\n        best_index = np.argmin(f)\n        self.best_f = f[best_index]\n        self.best_x = x[best_index]\n\n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n\n            # Update best solution\n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n                self.learning_rate = min(0.5, self.learning_rate * 1.05) # Increase if improving\n            else:\n                self.learning_rate = max(0.05, self.learning_rate * 0.95) # Decrease if not improving\n\n            # Adaptation of mean and covariance using only the best\n            s = self.best_x - self.mean\n            self.mean = self.mean + self.learning_rate * s\n\n            # Rank-one covariance update\n            self.covariance = (1 - self.learning_rate) * self.covariance + self.learning_rate * np.outer(s, s)\n\n            # Damping covariance matrix to prevent premature convergence\n            self.covariance = (1 - self.damping_factor) * self.covariance + self.damping_factor * np.eye(self.dim)\n\n        return self.best_f, self.best_x", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.382 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["4d7ef7fc-f096-4315-ab89-053a1007792f"], "operator": null, "metadata": {"aucs": [0.18018335508296768, 0.26414133099211434, 0.3980175069150971, 0.43543739762079703, 0.24238355357981012, 0.36837382950407094, 0.2854157511678045, 0.328861826840644, 0.2969929566104531, 0.17973630758362158, 0.44955560580896015, 0.9832252227899201, 0.26540645415848996, 0.3010825152798685, 0.7894241749485749, 0.38637481347817004, 0.3297373351946682, 0.4616189912905361, 0.1936450463311351, 0.5006987675733393]}}
{"id": "2e009cab-5a27-4df0-8135-a27389010741", "fitness": 0.6087309942884123, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with simplified rank-one covariance update and adaptive learning rate based on fitness improvement, dynamically adjusting regularization based on covariance condition number.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, learning_rate=0.2, regularization_factor=1e-6, condition_number_threshold=1e6):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.learning_rate = learning_rate\n        self.regularization_factor = regularization_factor\n        self.condition_number_threshold = condition_number_threshold\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.Inf\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solution\n        best_index = np.argmin(f)\n        if f[best_index] < self.best_f:\n            self.best_f = f[best_index]\n            self.best_x = x[best_index]\n\n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n\n            # Update best solution\n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                improvement = self.best_f - f[best_index]\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n                \n                # Adjust learning rate based on improvement\n                if improvement > 0:\n                    self.learning_rate = min(0.5, self.learning_rate * 1.1) # Increase if improving\n                else:\n                    self.learning_rate = max(0.05, self.learning_rate * 0.9) # Decrease if not improving\n            else:\n                 improvement = 0\n                \n\n            # Adaptation of mean and covariance using only the best\n            s = self.best_x - self.mean\n            self.mean = self.mean + self.learning_rate * s # Simplified mean update\n            \n            # Rank-one covariance update\n            self.covariance = (1 - self.learning_rate) * self.covariance + self.learning_rate * np.outer(s, s)\n\n            # Dynamic Regularization\n            condition_number = np.linalg.cond(self.covariance)\n            if condition_number > self.condition_number_threshold:\n                self.regularization_factor *= 1.1  # Increase regularization\n            else:\n                self.regularization_factor *= 0.9  # Decrease regularization\n\n            # Regularize covariance matrix\n            self.covariance += self.regularization_factor * np.eye(self.dim)\n\n        return self.best_f, self.best_x", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.609 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["4d7ef7fc-f096-4315-ab89-053a1007792f"], "operator": null, "metadata": {"aucs": [0.1934166349799149, 0.7377411501643848, 0.681872912573052, 0.8686509481099017, 0.6841012240827569, 0.7503900750452555, 0.3379746318350212, 0.45164801709699987, 0.6932466996739901, 0.20445547187467628, 0.880152372913535, 0.992437713649306, 0.3347834047202105, 0.7075216490745531, 0.7296227404347106, 0.7484320302637838, 0.641775114755086, 0.8074610291238761, 0.2248790881494267, 0.5040569772478044]}}
{"id": "3b53ec93-01b5-4eab-8cd9-df39b08ce5a8", "fitness": 0.5205100525434723, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with simplified rank-one covariance update and adaptive learning rate based on fitness improvement, using a moving average for improvement and simplified covariance update.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, learning_rate=0.2, regularization_factor=1e-6, improvement_window=10):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.learning_rate = learning_rate\n        self.regularization_factor = regularization_factor\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.Inf\n        self.improvement_history = []\n        self.improvement_window = improvement_window\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solution\n        best_index = np.argmin(f)\n        if f[best_index] < self.best_f:\n            self.best_f = f[best_index]\n            self.best_x = x[best_index]\n\n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n\n            # Update best solution\n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                improvement = self.best_f - f[best_index]\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n                self.improvement_history.append(improvement)\n\n            else:\n                self.improvement_history.append(0)\n\n            if len(self.improvement_history) > self.improvement_window:\n                self.improvement_history = self.improvement_history[-self.improvement_window:]\n\n            avg_improvement = np.mean(self.improvement_history)\n\n            # Adjust learning rate based on improvement\n            if avg_improvement > 0:\n                self.learning_rate = min(0.5, self.learning_rate * 1.1) # Increase if improving\n            else:\n                self.learning_rate = max(0.05, self.learning_rate * 0.9) # Decrease if not improving\n\n\n            # Adaptation of mean and covariance using only the best\n            s = self.best_x - self.mean\n            self.mean = self.mean + self.learning_rate * s # Simplified mean update\n\n            # Rank-one covariance update (simplified)\n            self.covariance = (1 - self.learning_rate) * self.covariance + self.learning_rate * np.outer(s, s) + self.regularization_factor * np.eye(self.dim)\n\n        return self.best_f, self.best_x", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.521 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["4d7ef7fc-f096-4315-ab89-053a1007792f"], "operator": null, "metadata": {"aucs": [0.1599634180206404, 0.2723454140778653, 0.5846361959348318, 0.8534767691195844, 0.26549605438690493, 0.6248743382541546, 0.3250639613212737, 0.5023772525908972, 0.618733132401472, 0.21265846064169724, 0.8419501837212171, 0.9988052814378823, 0.3365016210537132, 0.23461036448636552, 0.8477116541107903, 0.6454898220243683, 0.5341055135160895, 0.7810046483363978, 0.27615260468880654, 0.49424436074449285]}}
{"id": "04c49ab2-ce38-462b-9092-696594792db5", "fitness": 0.47074015297445326, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with simplified rank-one covariance update and adaptive learning rate based on fitness improvement, using a decaying learning rate schedule and covariance regularization to balance exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, learning_rate=0.2, regularization_factor=1e-6):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.learning_rate = learning_rate\n        self.regularization_factor = regularization_factor\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.Inf\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solution\n        best_index = np.argmin(f)\n        self.best_f = f[best_index]\n        self.best_x = x[best_index]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n\n            # Update best solution\n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n\n            # Adaptation of mean and covariance using only the best's movement\n            movement = self.best_x - self.mean\n            self.mean = (1 - self.learning_rate) * self.mean + self.learning_rate * self.best_x\n            self.covariance = (1 - self.learning_rate) * self.covariance + self.learning_rate * np.outer(movement, movement)\n\n            # Regularize covariance matrix\n            self.covariance += self.regularization_factor * np.eye(self.dim)\n\n            # Decay learning rate\n            self.learning_rate = self.learning_rate * 0.999\n\n        return self.best_f, self.best_x", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.471 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c39f7919-c6af-43cf-9981-5140db194ab7"], "operator": null, "metadata": {"aucs": [0.14674324180528242, 0.24341697921139527, 0.5012452594832351, 0.7792555120201652, 0.34031812456289057, 0.5135897644664948, 0.29457887371803515, 0.4324311925149237, 0.4832947009930776, 0.19761227028699135, 0.7480265913746604, 0.9951426648965651, 0.2654006016251065, 0.4817311864837528, 0.835872845696648, 0.34808023611364036, 0.4511072581895873, 0.5994719769245398, 0.2814177357764862, 0.47606604334558755]}}
{"id": "77bcf951-909d-4822-ace2-49a3c703a650", "fitness": 0.4701044268683917, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with simplified rank-one covariance update based on the best solution's movement and adaptive learning rate adjusted by exponential moving average of fitness improvement.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, learning_rate=0.2, regularization_factor=1e-6, ema_alpha=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.learning_rate = learning_rate\n        self.regularization_factor = regularization_factor\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.Inf\n        self.ema_fitness_improvement = 0.0\n        self.ema_alpha = ema_alpha\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solution\n        best_index = np.argmin(f)\n        self.best_f = f[best_index]\n        self.best_x = x[best_index]\n\n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n\n            # Update best solution\n            best_index = np.argmin(f)\n            new_best_f = f[best_index]\n            new_best_x = x[best_index]\n\n            if new_best_f < self.best_f:\n                fitness_improvement = self.best_f - new_best_f\n                self.best_f = new_best_f\n                self.best_x = new_best_x\n            else:\n                fitness_improvement = 0.0\n\n            # Adjust learning rate based on EMA of fitness improvement\n            self.ema_fitness_improvement = (\n                self.ema_alpha * fitness_improvement + (1 - self.ema_alpha) * self.ema_fitness_improvement\n            )\n\n            if self.ema_fitness_improvement > 0:\n                self.learning_rate = min(0.5, self.learning_rate * 1.1)\n            else:\n                self.learning_rate = max(0.05, self.learning_rate * 0.9)\n\n            # Adaptation of mean and covariance using only the best's movement\n            movement = self.best_x - self.mean\n            self.mean = (1 - self.learning_rate) * self.mean + self.learning_rate * self.best_x\n            self.covariance = (1 - self.learning_rate) * self.covariance + self.learning_rate * np.outer(movement, movement)\n\n            # Regularize covariance matrix\n            self.covariance += self.regularization_factor * np.eye(self.dim)\n\n        return self.best_f, self.best_x", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.470 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c39f7919-c6af-43cf-9981-5140db194ab7"], "operator": null, "metadata": {"aucs": [0.17706457651008256, 0.23177868690540648, 0.5866780590954515, 0.19682020636975794, 0.32350605492044393, 0.6586342953716545, 0.33841485942844285, 0.507473914717445, 0.6084264778826247, 0.19303893045809617, 0.8421412397533217, 0.9969820747865524, 0.2910861368732207, 0.3047432739917132, 0.8318169029330942, 0.3059605457147715, 0.5410640008637286, 0.7809538026164521, 0.18050672722105499, 0.5049977709545186]}}
{"id": "6db82118-7bf6-4a88-9e63-cdd5290a630b", "fitness": 0.49824246970911146, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with simplified rank-one covariance update and adaptive learning rate based on recent fitness improvement, using a moving average of the objective function values to guide adaptation.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.2, ema_alpha=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.Inf\n        self.ema_f = np.Inf  # Exponential moving average of objective function value\n        self.ema_alpha = ema_alpha\n        self.min_adaptation_rate = 0.01\n        self.max_adaptation_rate = 0.5\n        self.min_eig = 1e-6\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solution and EMA\n        best_index = np.argmin(f)\n        if f[best_index] < self.best_f:\n            self.best_f = f[best_index]\n            self.best_x = x[best_index]\n        self.ema_f = self.ema_alpha * np.min(f) + (1 - self.ema_alpha) * self.ema_f\n\n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n\n            # Update best solution and EMA\n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n            self.ema_f = self.ema_alpha * np.min(f) + (1 - self.ema_alpha) * self.ema_f\n\n            # Dynamic adaptation rate based on EMA\n            if self.best_f < self.ema_f:\n                self.adaptation_rate = min(self.adaptation_rate * 1.1, self.max_adaptation_rate)\n            else:\n                self.adaptation_rate = max(self.adaptation_rate * 0.9, self.min_adaptation_rate)\n\n            # Adaptation of mean and covariance using only the best\n            deviation = self.best_x - self.mean\n            self.mean = (1 - self.adaptation_rate) * self.mean + self.adaptation_rate * self.best_x\n\n            # Rank-1 covariance update focusing on best solution's deviation\n            deviation = deviation.reshape(-1, 1)\n            self.covariance = (1 - self.adaptation_rate) * self.covariance + self.adaptation_rate * (deviation @ deviation.T)\n\n            # Regularize covariance matrix\n            U, S, V = np.linalg.svd(self.covariance)\n            S = np.maximum(S, self.min_eig)\n            self.covariance = U @ np.diag(S) @ V\n\n        return self.best_f, self.best_x", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.498 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e81c7915-a979-4d36-9ea3-a03a74c6b70d"], "operator": null, "metadata": {"aucs": [0.12020037468040778, 0.7245533537377088, 0.6126019044357065, 0.19703464295004647, 0.290253915248295, 0.6654714129119993, 0.2538299036143965, 0.5245614990924489, 0.6385394044768646, 0.20177544952000204, 0.8773437585155405, 0.9955021050402647, 0.2723316591323687, 0.27545813778414596, 0.8619191971765263, 0.6712556794416094, 0.3317667476984558, 0.7828411320458266, 0.19410155406151375, 0.47350756261810323]}}
{"id": "d697bf78-8fa9-4c21-9858-79851ac3d5cc", "fitness": 0.2921033421933172, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with simplified covariance update using only the best solution's deviation, adaptive learning rate based on normalized fitness improvement, and covariance regularization, further simplified with mean decay and early stopping based on diminishing improvement.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.2, ema_alpha=0.1, tolerance=1e-6):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.Inf\n        self.ema_improvement = 0.0\n        self.ema_alpha = ema_alpha\n        self.min_adaptation_rate = 0.01\n        self.max_adaptation_rate = 0.5\n        self.min_eig = 1e-6\n        self.tolerance = tolerance\n        self.last_improvement = np.Inf\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solution\n        best_index = np.argmin(f)\n        if f[best_index] < self.best_f:\n            self.best_f = f[best_index]\n            self.best_x = x[best_index]\n\n        prev_best_f = self.best_f\n\n        while self.budget > 0 and self.last_improvement > self.tolerance:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n\n            # Update best solution\n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n\n                # Dynamic adaptation rate based on normalized fitness improvement\n                improvement = (prev_best_f - self.best_f) / max(abs(prev_best_f), 1e-8)\n                self.ema_improvement = self.ema_alpha * improvement + (1 - self.ema_alpha) * self.ema_improvement\n\n                if self.ema_improvement > 0:\n                    self.adaptation_rate = min(self.adaptation_rate * 1.1, self.max_adaptation_rate)\n                else:\n                    self.adaptation_rate = max(self.adaptation_rate * 0.9, self.min_adaptation_rate)\n\n                # Adaptation of mean and covariance using only the best\n                deviation = self.best_x - self.mean\n                self.mean = 0.9 * self.mean + 0.1 * self.best_x # decaying mean\n\n                # Rank-1 covariance update focusing on best solution's deviation\n                deviation = deviation.reshape(-1, 1)\n                self.covariance = (1 - self.adaptation_rate) * self.covariance + self.adaptation_rate * (deviation @ deviation.T)\n\n                # Regularize covariance matrix\n                U, S, V = np.linalg.svd(self.covariance)\n                S = np.maximum(S, self.min_eig)\n                self.covariance = U @ np.diag(S) @ V\n\n                self.last_improvement = abs(prev_best_f - self.best_f)\n                prev_best_f = self.best_f\n\n        return self.best_f, self.best_x", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.292 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e81c7915-a979-4d36-9ea3-a03a74c6b70d"], "operator": null, "metadata": {"aucs": [0.10933793373681633, 0.18032277896381188, 0.40089239047384784, 0.16960343591700056, 0.26113142801981637, 0.2757760561616903, 0.21726309065975924, 0.3000854279748597, 0.2633163695026718, 0.17528078856139961, 0.5467115939385483, 0.9976215481617413, 0.27287894990796613, 0.1697262934731587, 0.14216815456332987, 0.2820551579994207, 0.22118992153551986, 0.22322024683695896, 0.18589102956281467, 0.44759424791521174]}}
{"id": "2db896fb-c1f0-48a7-a921-d5c634f19091", "fitness": 0.34911230601820026, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with simplified rank-one covariance update and dynamic learning rate based on fitness improvement, using a more aggressive covariance update based on the best solution's displacement and early covariance regularization.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.2, ema_alpha=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.Inf\n        self.ema_improvement = 0.0\n        self.ema_alpha = ema_alpha\n        self.min_adaptation_rate = 0.01\n        self.max_adaptation_rate = 0.5\n        self.min_eig = 1e-6\n        self.covariance_decay = 0.99 \n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        # Initial sampling and covariance regularization\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        best_index = np.argmin(f)\n        self.best_f = f[best_index]\n        self.best_x = x[best_index]\n\n        # Regularize covariance early\n        U, S, V = np.linalg.svd(self.covariance)\n        S = np.maximum(S, self.min_eig)\n        self.covariance = U @ np.diag(S) @ V\n\n        prev_best_f = self.best_f\n\n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n\n            # Update best solution\n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n\n                # Dynamic adaptation rate based on fitness improvement\n                improvement = prev_best_f - self.best_f\n                self.ema_improvement = self.ema_alpha * improvement + (1 - self.ema_alpha) * self.ema_improvement\n\n                if self.ema_improvement > 0:\n                    self.adaptation_rate = min(self.adaptation_rate * 1.1, self.max_adaptation_rate)\n                else:\n                    self.adaptation_rate = max(self.adaptation_rate * 0.9, self.min_adaptation_rate)\n            else:\n                self.adaptation_rate = max(self.adaptation_rate * 0.99, self.min_adaptation_rate)\n\n            # Adaptation of mean and covariance using only the best\n            deviation = self.best_x - self.mean\n            self.mean = (1 - self.adaptation_rate) * self.mean + self.adaptation_rate * self.best_x\n\n            # Rank-1 covariance update focusing on best solution's deviation, more aggressive update\n            deviation = deviation.reshape(-1, 1)\n            self.covariance = self.covariance_decay * self.covariance + (1 - self.covariance_decay) * (deviation @ deviation.T)\n\n            # Regularize covariance matrix\n            U, S, V = np.linalg.svd(self.covariance)\n            S = np.maximum(S, self.min_eig)\n            self.covariance = U @ np.diag(S) @ V\n            \n            prev_best_f = self.best_f\n\n        return self.best_f, self.best_x", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.349 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e81c7915-a979-4d36-9ea3-a03a74c6b70d"], "operator": null, "metadata": {"aucs": [0.1599970195200533, 0.3339590928530227, 0.33469153018680253, 0.3409265132108943, 0.25085407371415347, 0.3157758882969295, 0.27556649903026653, 0.2764865797761078, 0.2573681483257332, 0.16519680513729895, 0.36048185426075197, 0.9991713365470413, 0.3109072709988504, 0.26678483658702223, 0.6797622828621124, 0.33777235754875823, 0.28634888474652087, 0.36529570780437215, 0.1781594176648703, 0.48674002129244254]}}
{"id": "4f0fd67a-ece5-41f8-bbaf-edb8b8ff3dc2", "fitness": 0.4022562496562208, "name": "AdaptiveCovarianceSampling", "description": "Adaptive covariance sampling with simplified rank-one covariance update, adaptive learning rate based on fitness improvement, and covariance damping to enhance exploration.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, learning_rate=0.2, damping_factor=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.learning_rate = learning_rate\n        self.damping_factor = damping_factor\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.Inf\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solution\n        best_index = np.argmin(f)\n        if f[best_index] < self.best_f:\n            self.best_f = f[best_index]\n            self.best_x = x[best_index]\n\n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n\n            # Update best solution\n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                improvement = self.best_f - f[best_index]\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n                \n                # Adjust learning rate based on improvement\n                if improvement > 0:\n                    self.learning_rate = min(0.5, self.learning_rate * 1.1) # Increase if improving\n                else:\n                    self.learning_rate = max(0.05, self.learning_rate * 0.9) # Decrease if not improving\n\n            # Adaptation of mean and covariance using only the best\n            s = self.best_x - self.mean\n            self.mean = self.mean + self.learning_rate * s # Simplified mean update\n            \n            # Rank-one covariance update\n            self.covariance = (1 - self.learning_rate) * self.covariance + self.learning_rate * np.outer(s, s)\n\n            # Damping to maintain exploration\n            self.covariance = (1 - self.damping_factor) * self.covariance + self.damping_factor * np.eye(self.dim)\n\n\n        return self.best_f, self.best_x", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.402 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["4d7ef7fc-f096-4315-ab89-053a1007792f"], "operator": null, "metadata": {"aucs": [0.1346355320796624, 0.19380212281312525, 0.4305747763761435, 0.19703762849326045, 0.28687940432063763, 0.43232402750192045, 0.307908855646683, 0.3413079233232198, 0.3588732336150907, 0.2767842894423437, 0.6648777164938342, 0.9989669296451886, 0.260942900640037, 0.3581396590238992, 0.7914128802115485, 0.372077740887634, 0.3598039097410357, 0.534684252498052, 0.244295804720156, 0.49979540565094527]}}
{"id": "8ace578e-78c8-4bf0-a811-d681b09661de", "fitness": 0.4220629048603536, "name": "AdaptiveCovarianceSampling", "description": "Simplified adaptive covariance sampling with rank-one update, adaptive learning rate, and covariance damping.", "code": "import numpy as np\n\nclass AdaptiveCovarianceSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, learning_rate=0.2, damping_factor=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.initial_samples = initial_samples\n        self.learning_rate = learning_rate\n        self.damping_factor = damping_factor\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim)\n        self.best_x = None\n        self.best_f = np.Inf\n\n    def sample(self, num_samples):\n        try:\n            x = np.random.multivariate_normal(self.mean, self.covariance, num_samples)\n        except np.linalg.LinAlgError:\n            x = np.random.uniform(self.lb, self.ub, size=(num_samples, self.dim))\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        # Initial sampling\n        x = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        f = np.array([func(xi) for xi in x])\n        self.budget -= self.initial_samples\n\n        # Update best solution\n        best_index = np.argmin(f)\n        self.best_f = f[best_index]\n        self.best_x = x[best_index]\n\n        while self.budget > 0:\n            # Adaptive sampling\n            num_samples = min(self.budget, self.initial_samples)\n            x = self.sample(num_samples)\n            f = np.array([func(xi) for xi in x])\n            self.budget -= num_samples\n\n            # Update best solution\n            best_index = np.argmin(f)\n            if f[best_index] < self.best_f:\n                improvement = self.best_f - f[best_index]\n                self.learning_rate = min(0.5, 1.1 * self.learning_rate) if improvement > 0 else max(0.05, 0.9 * self.learning_rate)\n                self.best_f = f[best_index]\n                self.best_x = x[best_index]\n\n            # Adaptation of mean and covariance using only the best\n            s = self.best_x - self.mean\n            self.mean = (1 - self.learning_rate) * self.mean + self.learning_rate * self.best_x\n            \n            # Rank-one covariance update with damping\n            self.covariance = (1 - self.learning_rate) * self.covariance + self.learning_rate * np.outer(s, s) + self.damping_factor * np.eye(self.dim)\n\n        return self.best_f, self.best_x", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveCovarianceSampling scored 0.422 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["cdc09197-736b-4d79-a6ce-6c8a196c2b75"], "operator": null, "metadata": {"aucs": [0.1987775779384401, 0.22021723892814637, 0.409516685432163, 0.5137668392944859, 0.3611343562743412, 0.42553229546336924, 0.29550609808340367, 0.351952353883193, 0.34821694816863824, 0.21659866643392667, 0.6196381719348794, 0.9950605042882303, 0.2998316706463866, 0.3479625282282238, 0.8231620750966192, 0.33544294223825366, 0.3651991372994513, 0.5279580232891503, 0.30668602466151595, 0.47909795962425517]}}
