{"id": "04ee848b-d480-4b68-8bae-d9e906d98cb2", "fitness": -Infinity, "name": "HybridPSO_DE_NM", "description": "Population-based algorithm that combines elements of particle swarm optimization and differential evolution with a local search strategy using Nelder-Mead simplex.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridPSO_DE_NM:\n    def __init__(self, budget=10000, dim=10, pop_size=20, pso_weight=0.7, de_cross_rate=0.9, nm_iterations=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.pso_weight = pso_weight\n        self.de_cross_rate = de_cross_rate\n        self.nm_iterations = nm_iterations\n\n    def __call__(self, func):\n        # Initialize population\n        pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in pop])\n        self.budget -= self.pop_size\n        \n        # Initialize velocities for PSO\n        velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))\n\n        # Initialize best positions and fitness\n        personal_best_positions = pop.copy()\n        personal_best_fitness = fitness.copy()\n        global_best_index = np.argmin(fitness)\n        global_best_position = pop[global_best_index].copy()\n        global_best_fitness = fitness[global_best_index]\n        \n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # PSO update\n                inertia = self.pso_weight * velocities[i]\n                cognitive = np.random.rand() * (personal_best_positions[i] - pop[i])\n                social = np.random.rand() * (global_best_position - pop[i])\n                velocities[i] = inertia + cognitive + social\n                pop[i] += velocities[i]\n                pop[i] = np.clip(pop[i], func.bounds.lb, func.bounds.ub)\n                \n                # Differential Evolution mutation\n                if np.random.rand() < self.de_cross_rate:\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = pop[indices[0]], pop[indices[1]], pop[indices[2]]\n                    mutation = x_r1 + 0.5 * (x_r2 - x_r3)\n                    \n                    # Crossover\n                    crossover_mask = np.random.rand(self.dim) < self.de_cross_rate\n                    pop[i] = np.where(crossover_mask, mutation, pop[i])\n                    pop[i] = np.clip(pop[i], func.bounds.lb, func.bounds.ub)\n                \n                # Evaluate new position\n                f = func(pop[i])\n                self.budget -= 1\n                \n                if f < personal_best_fitness[i]:\n                    personal_best_fitness[i] = f\n                    personal_best_positions[i] = pop[i].copy()\n\n                    if f < global_best_fitness:\n                        global_best_fitness = f\n                        global_best_position = pop[i].copy()\n\n                # Nelder-Mead local search (applied to the best particle)\n                if i == global_best_index and self.budget > 0:\n                    nm_result = minimize(func, global_best_position, method='Nelder-Mead',\n                                          options={'maxiter': self.nm_iterations, 'maxfev': self.budget})\n\n                    if nm_result.success and nm_result.fun < global_best_fitness:\n                        global_best_fitness = nm_result.fun\n                        global_best_position = nm_result.x\n                        \n                    self.budget -= nm_result.nfev\n            \n            global_best_index = np.argmin(personal_best_fitness)\n            global_best_position = personal_best_positions[global_best_index].copy()\n            global_best_fitness = personal_best_fitness[global_best_index]\n\n        return global_best_fitness, global_best_position", "configspace": "", "generation": 0, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 64, in __call__\nNameError: name 'minimize' is not defined\n.", "error": "", "parent_ids": [], "operator": null, "metadata": {}}
{"id": "c573ffeb-224d-474d-8890-07242998d2da", "fitness": 0.6208266503443493, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with a population size that scales with the problem dimension and a self-adaptive mutation factor.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_multiplier=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = dim * pop_multiplier\n        self.F = 0.5  # Initial mutation factor\n        self.CR = 0.9 # Crossover rate\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                crossover = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                \n                elif self.budget > 0 and np.random.rand() < 0.1:  # Occasionally perturb F and CR\n                    self.F = np.clip(np.random.normal(0.5, 0.3), 0.1, 1.0) # Adapt F\n                    self.CR = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0) # Adapt CR\n\n                if self.budget <=0:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDE scored 0.621 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.19372374431107697, 0.318688722002786, 0.46613917389498416, 0.9388308489184886, 0.822534967510708, 0.8881861363408347, 0.3676492009803465, 0.8104159022988154, 0.6713776936261928, 0.8564441878718966, 0.9172674701672708, 0.9835144683083851, 0.2793680939979134, 0.552911273921888, 0.7876873722464716, 0.6553472793666604, 0.3921158546787711, 0.6710728268411204, 0.3215032524352728, 0.5217545371671015]}}
{"id": "ee6dc2fa-c843-4dd3-8042-35bfff865b87", "fitness": 0.5534347040537596, "name": "AdaptiveExplorationOptimization", "description": "Population-based algorithm with adaptive exploration and exploitation using a combination of differential evolution and local search.", "code": "import numpy as np\n\nclass AdaptiveExplorationOptimization:\n    def __init__(self, budget=10000, dim=10, pop_size=20, de_rate=0.7, local_rate=0.1):\n        \"\"\"\n        Args:\n            budget (int): The evaluation budget.\n            dim (int): The dimension of the problem.\n            pop_size (int): The population size.\n            de_rate (float): Probability of performing differential evolution.\n            local_rate (float): Probability of performing local search.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.de_rate = de_rate\n        self.local_rate = local_rate\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        \"\"\"\n        Optimizes the given function using the allocated budget.\n\n        Args:\n            func (callable): The function to optimize.\n\n        Returns:\n            tuple: A tuple containing the best function value found and the corresponding solution vector.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update the best solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index].copy()\n\n        # Optimization loop\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Adaptive strategy selection\n                if np.random.rand() < self.de_rate:\n                    # Differential Evolution\n                    idxs = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = population[idxs]\n                    mutant = population[i] + 0.8 * (x_r1 - x_r2) \n                    \n                    # Ensure mutant stays within bounds\n                    mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                    # Crossover\n                    cross_points = np.random.rand(self.dim) < 0.9\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, population[i])\n                elif np.random.rand() < self.local_rate:\n                    # Local Search (perturbation)\n                    trial = population[i] + 0.1 * np.random.normal(0, 1, self.dim) * (func.bounds.ub - func.bounds.lb)\n                    trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n                else:\n                    # Global Search (random)\n                    trial = np.random.uniform(func.bounds.lb, func.bounds.ub)\n\n                # Evaluation\n                f_trial = func(trial)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = trial.copy()\n\n                    # Update the best solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial.copy()\n                \n                if self.budget <= 0:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveExplorationOptimization scored 0.553 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.18350565860612755, 0.43196094982134625, 0.5156166496789863, 0.779696050883691, 0.5407818956810508, 0.6211948168088268, 0.35388426886009305, 0.46192626110457036, 0.6054245466794631, 0.464685054405654, 0.8283798387773346, 0.9979510285610217, 0.4076432653068074, 0.5233724336277423, 0.8845232482609113, 0.5805209418690463, 0.43513955503862367, 0.7572639931826889, 0.20038912860161073, 0.49483449531959434]}}
{"id": "f54bc68e-b246-4aca-9137-1490f48440cc", "fitness": 0.6987657805855565, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with stochastic ranking and archive, adjusting parameters based on success rate.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = 0.5\n        self.CR = 0.9\n        self.archive = []\n        self.success_F = []\n        self.success_CR = []\n        self.success_delta_f = []\n        self.p = 0.1 # probability for stochastic ranking\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        \n        for i in range(self.pop_size):\n            if fitness[i] < self.f_opt:\n                self.f_opt = fitness[i]\n                self.x_opt = self.population[i]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                mutant = x_1 + self.F * (x_2 - x_3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                crossover = np.random.uniform(size=self.dim) < self.CR\n                trial = np.where(crossover, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n                \n                # Stochastic Ranking\n                if (fitness[i] < 0 and f_trial < 0) or np.random.rand() < self.p:\n                    if f_trial < fitness[i]:\n                        self.success_F.append(self.F)\n                        self.success_CR.append(self.CR)\n                        self.success_delta_f.append(np.abs(f_trial - fitness[i]))\n                        \n                        fitness[i] = f_trial\n                        self.population[i] = trial\n                        \n                        if len(self.archive) < self.archive_size:\n                            self.archive.append(self.population[i].copy())\n                        else:\n                            idx_to_replace = np.random.randint(0, self.archive_size)\n                            self.archive[idx_to_replace] = self.population[i].copy()\n                else:\n                     if f_trial < fitness[i]:\n                        fitness[i] = f_trial\n                        self.population[i] = trial\n                        \n                        if len(self.archive) < self.archive_size:\n                            self.archive.append(self.population[i].copy())\n                        else:\n                            idx_to_replace = np.random.randint(0, self.archive_size)\n                            self.archive[idx_to_replace] = self.population[i].copy()\n\n\n            # Parameter Adaptation\n            if self.success_F:\n                self.F = np.mean(self.success_F)\n                self.CR = np.mean(self.success_CR)\n\n                # Reset success history\n                self.success_F = []\n                self.success_CR = []\n                self.success_delta_f = []\n            \n            self.F = np.clip(self.F, 0.1, 1.0)\n            self.CR = np.clip(self.CR, 0.1, 1.0)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDE scored 0.699 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.3085445245161407, 0.610751374391401, 0.6619140864913118, 0.863477483891024, 0.7424560391972992, 0.7713607617267026, 0.6133321780620071, 0.6575493647413273, 0.7377746921635338, 0.7199185487522382, 0.8491741006846228, 0.9984345343272114, 0.6471637420682363, 0.7111123993692763, 0.891928663893114, 0.7724852166506508, 0.568535858483611, 0.8293372048263772, 0.41391972854293246, 0.6061451089321136]}}
{"id": "057587f5-47e9-4447-a479-26540672981b", "fitness": -Infinity, "name": "AdaptiveDELearning", "description": "An adaptive Differential Evolution strategy with a dynamically adjusted population size and a learning-based mutation operator, focusing on balancing exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDELearning:\n    def __init__(self, budget=10000, dim=10, pop_multiplier=5, learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_initial = dim * pop_multiplier\n        self.pop_size = self.pop_size_initial\n        self.population = None\n        self.fitness = None\n        self.F = 0.5  # Initial mutation factor\n        self.CR = 0.9 # Crossover rate\n        self.learning_rate = learning_rate\n        self.archive = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            self.adapt_population_size()\n            for i in range(self.pop_size):\n                # Mutation with learning\n                mutant = self.mutation(i, func)\n\n                # Crossover\n                crossover = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < self.fitness[i]:\n                    self.archive.append(self.population[i].copy())\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                else:\n                    self.archive.append(trial.copy())\n\n\n                if self.budget <=0:\n                    break\n\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n    def mutation(self, i, func):\n        idxs = [idx for idx in range(self.pop_size) if idx != i]\n        a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n        \n        if self.archive:\n            donor = self.archive[np.random.choice(len(self.archive))]\n            mutant = np.clip(a + self.F * (b - c + donor - self.population[i]), func.bounds.lb, func.bounds.ub) #Using archive as learning\n        else:\n             mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n\n        return mutant\n\n    def adapt_parameters(self):\n         # Adapt F and CR based on success in previous generation\n        successful_F = []\n        successful_CR = []\n        for i in range(self.pop_size):\n            if len(self.archive) > 0:\n                if np.random.rand() < 0.1:\n                    self.F = np.clip(np.random.normal(0.5, 0.3), 0.1, 1.0)\n                    self.CR = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0)\n\n    def adapt_population_size(self):\n        if self.budget > 0:\n            success_rate = sum(1 for i in range(self.pop_size) if self.archive and self.fitness[i] < func(self.population[i])) / self.pop_size if self.pop_size > 0 else 0\n            if success_rate > 0.2 and self.pop_size < 2*self.pop_size_initial:\n                self.pop_size = min(2*self.pop_size_initial, self.pop_size + 1)\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population]) # re-evaluate fitness with larger population\n            elif success_rate < 0.1 and self.pop_size > self.dim + 2:\n                self.pop_size = max(self.dim + 2, self.pop_size - 1)\n                self.population = self.population[:self.pop_size]\n                self.fitness = self.fitness[:self.pop_size]\n\n\nif __name__ == '__main__':\n    # Example Usage (replace with your actual function)\n    class DummyFunction:\n        def __init__(self, dim):\n            self.dim = dim\n            self.bounds = Bounds(-5, 5)\n\n        def __call__(self, x):\n            return np.sum(x**2)  # Example function\n\n    class Bounds:\n        def __init__(self, lb, ub):\n            self.lb = np.array([lb])\n            self.ub = np.array([ub])\n\n    dim = 10\n    budget = 10000\n    func = DummyFunction(dim)\n    optimizer = AdaptiveDELearning(budget=budget, dim=dim)\n    f_opt, x_opt = optimizer(func)\n    print(f\"Optimal value: {f_opt}\")\n    print(f\"Optimal solution: {x_opt}\")", "configspace": "", "generation": 1, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 22, in __call__\n  File \"<string>\", line 81, in adapt_population_size\n  File \"<string>\", line 81, in <genexpr>\nNameError: name 'func' is not defined\n.", "error": "", "parent_ids": ["c573ffeb-224d-474d-8890-07242998d2da"], "operator": null, "metadata": {}}
{"id": "c8866829-03f7-4966-9515-fb33e9f30c15", "fitness": 0.0, "name": "AdaptiveDE_Orthogonal", "description": "Adaptive Differential Evolution with orthogonal learning and a Cauchy mutation operator.", "code": "import numpy as np\n\nclass AdaptiveDE_Orthogonal:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = 0.5\n        self.CR = 0.9\n        self.archive = []\n        self.success_F = []\n        self.success_CR = []\n        self.success_delta_f = []\n        self.p = 0.1 # probability for stochastic ranking\n        self.levy_exponent = 1.5 # Parameter for Levy flight\n\n    def levy_flight(self, beta):\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) / (np.math.gamma((1 + beta) / 2) * beta * (2 ** ((beta - 1) / 2)))) ** (1 / beta)\n        u = np.random.randn(self.dim) * sigma\n        v = np.random.randn(self.dim)\n        step = u / abs(v) ** (1 / beta)\n        return step\n\n    def orthogonal_learning(self, population, fitness, num_samples=5):\n        \"\"\"Performs orthogonal learning to generate new candidate solutions.\"\"\"\n        best_idx = np.argmin(fitness)\n        best_individual = population[best_idx].copy()\n        \n        new_candidates = []\n        for _ in range(num_samples):\n            # Randomly select dimensions to modify\n            dims_to_modify = np.random.choice(self.dim, size=int(self.dim/2), replace=False) \n            new_candidate = best_individual.copy()\n            \n            # Generate random values for the selected dimensions\n            new_values = np.random.uniform(low=-5.0, high=5.0, size=len(dims_to_modify))\n            new_candidate[dims_to_modify] = new_values\n            new_candidates.append(new_candidate)\n            \n        return new_candidates\n        \n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        \n        for i in range(self.pop_size):\n            if fitness[i] < self.f_opt:\n                self.f_opt = fitness[i]\n                self.x_opt = self.population[i]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation (Cauchy)\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                # Cauchy mutation\n                mutant = x_1 + self.F * (x_2 - x_3) + 0.01 * np.random.standard_cauchy(size=self.dim)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                crossover = np.random.uniform(size=self.dim) < self.CR\n                trial = np.where(crossover, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n                \n                # Stochastic Ranking\n                if (fitness[i] < 0 and f_trial < 0) or np.random.rand() < self.p:\n                    if f_trial < fitness[i]:\n                        self.success_F.append(self.F)\n                        self.success_CR.append(self.CR)\n                        self.success_delta_f.append(np.abs(f_trial - fitness[i]))\n                        \n                        fitness[i] = f_trial\n                        self.population[i] = trial\n                        \n                        if len(self.archive) < self.archive_size:\n                            self.archive.append(self.population[i].copy())\n                        else:\n                            idx_to_replace = np.random.randint(0, self.archive_size)\n                            self.archive[idx_to_replace] = self.population[i].copy()\n                else:\n                     if f_trial < fitness[i]:\n                        fitness[i] = f_trial\n                        self.population[i] = trial\n                        \n                        if len(self.archive) < self.archive_size:\n                            self.archive.append(self.population[i].copy())\n                        else:\n                            idx_to_replace = np.random.randint(0, self.archive_size)\n                            self.archive[idx_to_replace] = self.population[i].copy()\n\n            # Orthogonal Learning\n            new_candidates = self.orthogonal_learning(self.population, fitness)\n            for candidate in new_candidates:\n                f_candidate = func(candidate)\n                self.budget -= 1\n\n                if f_candidate < self.f_opt:\n                    self.f_opt = f_candidate\n                    self.x_opt = candidate\n\n                # Replace worst individual with the new candidate if it's better\n                worst_idx = np.argmax(fitness)\n                if f_candidate < fitness[worst_idx]:\n                    fitness[worst_idx] = f_candidate\n                    self.population[worst_idx] = candidate\n\n\n            # Parameter Adaptation\n            if self.success_F:\n                self.F = np.mean(self.success_F)\n                self.CR = np.mean(self.success_CR)\n\n                # Reset success history\n                self.success_F = []\n                self.success_CR = []\n                self.success_delta_f = []\n            \n            self.F = np.clip(self.F, 0.1, 1.0)\n            self.CR = np.clip(self.CR, 0.1, 1.0)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDE_Orthogonal scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f54bc68e-b246-4aca-9137-1490f48440cc"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "22544753-63b6-476a-89d5-32116584e630", "fitness": 0.0, "name": "AdaptiveDE_OL", "description": "Differential Evolution with a dynamically adjusted population size based on the success rate of finding improved solutions and orthogonal learning.", "code": "import numpy as np\n\nclass AdaptiveDE_OL:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, archive_size=10, min_pop_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.min_pop_size = min_pop_size\n        self.archive_size = archive_size\n        self.F = 0.5\n        self.CR = 0.9\n        self.archive = []\n        self.success_F = []\n        self.success_CR = []\n        self.success_delta_f = []\n        self.p = 0.1 # probability for stochastic ranking\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        return self.fitness\n\n    def orthogonal_learning(self, func, individual, num_samples=5):\n        \"\"\"\n        Performs orthogonal learning to generate new candidate solutions.\n        \"\"\"\n        candidates = []\n        for _ in range(num_samples):\n            # Generate a new solution by randomly perturbing the current individual\n            mutant = individual + np.random.normal(0, 0.1, size=self.dim)  # Adjust stddev as needed\n            mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n            candidates.append(mutant)\n\n        # Evaluate the candidate solutions\n        candidate_fitnesses = [func(candidate) for candidate in candidates]\n        self.budget -= num_samples\n\n        # Select the best candidate solution\n        best_candidate_index = np.argmin(candidate_fitnesses)\n        best_candidate = candidates[best_candidate_index]\n        best_candidate_fitness = candidate_fitnesses[best_candidate_index]\n\n        return best_candidate, best_candidate_fitness\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.fitness = self.initialize_population(func)\n        \n        for i in range(self.pop_size):\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.population[i]\n\n        num_improvements = 0\n\n        while self.budget > 0:\n            new_population = np.copy(self.population)\n            new_fitness = np.copy(self.fitness)\n\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                mutant = x_1 + self.F * (x_2 - x_3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                crossover = np.random.uniform(size=self.dim) < self.CR\n                trial = np.where(crossover, mutant, self.population[i])\n\n                # Orthogonal Learning\n                trial, f_trial_ol = self.orthogonal_learning(func, trial)\n                f_trial = f_trial_ol #func(trial) # orthogonal learning already evaluated this\n                #self.budget -= 1 #orthogonal learning already uses budget\n\n                \n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n                    num_improvements += 1\n                \n                # Stochastic Ranking\n                if (self.fitness[i] < 0 and f_trial < 0) or np.random.rand() < self.p:\n                    if f_trial < self.fitness[i]:\n                        self.success_F.append(self.F)\n                        self.success_CR.append(self.CR)\n                        self.success_delta_f.append(np.abs(f_trial - self.fitness[i]))\n                        \n                        new_fitness[i] = f_trial\n                        new_population[i] = trial\n                        \n                        if len(self.archive) < self.archive_size:\n                            self.archive.append(new_population[i].copy())\n                        else:\n                            idx_to_replace = np.random.randint(0, self.archive_size)\n                            self.archive[idx_to_replace] = new_population[i].copy()\n                else:\n                     if f_trial < self.fitness[i]:\n                        new_fitness[i] = f_trial\n                        new_population[i] = trial\n                        \n                        if len(self.archive) < self.archive_size:\n                            self.archive.append(new_population[i].copy())\n                        else:\n                            idx_to_replace = np.random.randint(0, self.archive_size)\n                            self.archive[idx_to_replace] = new_population[i].copy()\n\n            self.population = new_population\n            self.fitness = new_fitness\n\n\n            # Parameter Adaptation\n            if self.success_F:\n                self.F = np.mean(self.success_F)\n                self.CR = np.mean(self.success_CR)\n\n                # Reset success history\n                self.success_F = []\n                self.success_CR = []\n                self.success_delta_f = []\n            \n            self.F = np.clip(self.F, 0.1, 1.0)\n            self.CR = np.clip(self.CR, 0.1, 1.0)\n            \n            # Adjust population size\n            if num_improvements > 0.1 * self.pop_size and self.pop_size < 2 * self.dim:  # Dynamic population size adjustment\n                self.pop_size = min(self.pop_size + 5, 2 * self.dim)\n                \n                # Initialize new individuals\n                new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(5, self.dim))\n                new_fitnesses = np.array([func(x) for x in new_individuals])\n                self.budget -= 5\n                \n                self.population = np.vstack((self.population, new_individuals))\n                self.fitness = np.concatenate((self.fitness, new_fitnesses))\n            elif num_improvements < 0.01 * self.pop_size and self.pop_size > self.min_pop_size:\n                 self.pop_size = max(self.pop_size - 5, self.min_pop_size)\n                 self.population = self.population[:self.pop_size]\n                 self.fitness = self.fitness[:self.pop_size]\n                \n            num_improvements = 0\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDE_OL scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f54bc68e-b246-4aca-9137-1490f48440cc"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "9a05c634-240a-4af6-85dd-405a61c14bc7", "fitness": 0.0, "name": "AdaptiveDE_OL", "description": "Adaptive Differential Evolution with orthogonal learning, stochastic ranking and archive, adjusting parameters based on success rate.", "code": "import numpy as np\n\nclass AdaptiveDE_OL:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = 0.5\n        self.CR = 0.9\n        self.archive = []\n        self.success_F = []\n        self.success_CR = []\n        self.success_delta_f = []\n        self.p = 0.1  # probability for stochastic ranking\n        self.orthogonal_sample_size = 5  # Number of samples for orthogonal learning\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        for i in range(self.pop_size):\n            if fitness[i] < self.f_opt:\n                self.f_opt = fitness[i]\n                self.x_opt = self.population[i]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                mutant = x_1 + self.F * (x_2 - x_3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                crossover = np.random.uniform(size=self.dim) < self.CR\n                trial = np.where(crossover, mutant, self.population[i])\n\n                # Orthogonal Learning\n                orthogonal_samples = []\n                for _ in range(self.orthogonal_sample_size):\n                    orthogonal_sample = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                    orthogonal_samples.append(orthogonal_sample)\n                orthogonal_samples = np.array(orthogonal_samples)\n\n                orthogonal_fitnesses = np.array([func(x) for x in orthogonal_samples])\n                self.budget -= self.orthogonal_sample_size\n\n                best_orthogonal_index = np.argmin(orthogonal_fitnesses)\n                best_orthogonal_sample = orthogonal_samples[best_orthogonal_index]\n\n                # Combine trial vector with best orthogonal sample\n                trial = 0.5 * trial + 0.5 * best_orthogonal_sample\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                # Stochastic Ranking\n                if (fitness[i] < 0 and f_trial < 0) or np.random.rand() < self.p:\n                    if f_trial < fitness[i]:\n                        self.success_F.append(self.F)\n                        self.success_CR.append(self.CR)\n                        self.success_delta_f.append(np.abs(f_trial - fitness[i]))\n\n                        fitness[i] = f_trial\n                        self.population[i] = trial\n\n                        if len(self.archive) < self.archive_size:\n                            self.archive.append(self.population[i].copy())\n                        else:\n                            idx_to_replace = np.random.randint(0, self.archive_size)\n                            self.archive[idx_to_replace] = self.population[i].copy()\n                else:\n                    if f_trial < fitness[i]:\n                        fitness[i] = f_trial\n                        self.population[i] = trial\n\n                        if len(self.archive) < self.archive_size:\n                            self.archive.append(self.population[i].copy())\n                        else:\n                            idx_to_replace = np.random.randint(0, self.archive_size)\n                            self.archive[idx_to_replace] = self.population[i].copy()\n\n            # Parameter Adaptation\n            if self.success_F:\n                self.F = np.mean(self.success_F)\n                self.CR = np.mean(self.success_CR)\n\n                # Reset success history\n                self.success_F = []\n                self.success_CR = []\n                self.success_delta_f = []\n\n            self.F = np.clip(self.F, 0.1, 1.0)\n            self.CR = np.clip(self.CR, 0.1, 1.0)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDE_OL scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f54bc68e-b246-4aca-9137-1490f48440cc"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "f3935806-46a2-477b-be38-9bcf3a4ef0b6", "fitness": -Infinity, "name": "AdaptiveDE_Restart_OL", "description": "Adaptive Differential Evolution with a separate population for exploration and exploitation, using a restart strategy and orthogonal learning.", "code": "import numpy as np\n\nclass AdaptiveDE_Restart_OL:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, explore_ratio=0.5, restart_patience=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.explore_ratio = explore_ratio\n        self.restart_patience = restart_patience\n        self.F = 0.5\n        self.CR = 0.9\n        self.archive = []\n        self.success_F = []\n        self.success_CR = []\n        self.success_delta_f = []\n        self.p = 0.1  # probability for stochastic ranking\n        self.explore_pop_size = int(self.pop_size * self.explore_ratio)\n        self.exploit_pop_size = self.pop_size - self.explore_pop_size\n        self.restart_counter = 0\n        self.best_fitness_history = []\n\n    def orthogonal_design(self, n, k):\n        # Generate an orthogonal array using a simple method.  Can be replaced with a more robust OA generator.\n        H = np.zeros((n, k))\n        for i in range(n):\n            for j in range(k):\n                H[i, j] = (i * j) % n\n        return H\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n        # Initialize exploration and exploitation populations\n        self.explore_population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.explore_pop_size, self.dim))\n        self.exploit_population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.exploit_pop_size, self.dim))\n        self.population = np.concatenate((self.explore_population, self.exploit_population), axis=0)\n        \n        fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        for i in range(self.pop_size):\n            if fitness[i] < self.f_opt:\n                self.f_opt = fitness[i]\n                self.x_opt = self.population[i]\n                self.best_fitness_history.append(self.f_opt)\n\n        while self.budget > 0:\n            # Adaptive DE for Exploration Population\n            for i in range(self.explore_pop_size):\n                idxs = np.random.choice(self.explore_pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.explore_population[idxs]\n                mutant = x_1 + self.F * (x_2 - x_3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover = np.random.uniform(size=self.dim) < self.CR\n                trial = np.where(crossover, mutant, self.explore_population[i])\n\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n                    self.best_fitness_history.append(self.f_opt)\n\n                if (fitness[i] < 0 and f_trial < 0) or np.random.rand() < self.p:\n                    if f_trial < fitness[i]:\n                        self.success_F.append(self.F)\n                        self.success_CR.append(self.CR)\n                        self.success_delta_f.append(np.abs(f_trial - fitness[i]))\n\n                        fitness[i] = f_trial\n                        self.explore_population[i] = trial\n                else:\n                    if f_trial < fitness[i]:\n                        fitness[i] = f_trial\n                        self.explore_population[i] = trial\n\n            # Adaptive DE for Exploitation Population\n            for i in range(self.exploit_pop_size):\n                idxs = np.random.choice(self.pop_size, 3, replace=False) #draw from whole pop\n                x_1, x_2, x_3 = self.population[idxs]\n                mutant = x_1 + self.F * (x_2 - x_3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover = np.random.uniform(size=self.dim) < self.CR\n                trial = np.where(crossover, mutant, self.exploit_population[i]) #exploit population\n\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n                    self.best_fitness_history.append(self.f_opt)\n\n                if (fitness[i + self.explore_pop_size] < 0 and f_trial < 0) or np.random.rand() < self.p:\n                    if f_trial < fitness[i + self.explore_pop_size]:\n                        self.success_F.append(self.F)\n                        self.success_CR.append(self.CR)\n                        self.success_delta_f.append(np.abs(f_trial - fitness[i + self.explore_pop_size]))\n\n                        fitness[i + self.explore_pop_size] = f_trial\n                        self.exploit_population[i] = trial\n                else:\n                    if f_trial < fitness[i + self.explore_pop_size]:\n                        fitness[i + self.explore_pop_size] = f_trial\n                        self.exploit_population[i] = trial\n            \n            self.population = np.concatenate((self.explore_population, self.exploit_population), axis=0)\n            \n            # Parameter Adaptation\n            if self.success_F:\n                self.F = np.mean(self.success_F)\n                self.CR = np.mean(self.success_CR)\n                self.success_F = []\n                self.success_CR = []\n                self.success_delta_f = []\n\n            self.F = np.clip(self.F, 0.1, 1.0)\n            self.CR = np.clip(self.CR, 0.1, 1.0)\n            \n            # Restart Mechanism\n            if len(self.best_fitness_history) > self.restart_patience:\n                if self.best_fitness_history[-1] >= self.best_fitness_history[-self.restart_patience]:\n                    self.restart_counter += 1\n                    if self.restart_counter >= self.restart_patience:\n                        # Restart: Re-initialize exploration population\n                        self.explore_population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.explore_pop_size, self.dim))\n                        self.population = np.concatenate((self.explore_population, self.exploit_population), axis=0)\n                        fitness = np.array([func(x) for x in self.population])\n                        \n                        for i in range(self.pop_size):\n                            if fitness[i] < self.f_opt:\n                                self.f_opt = fitness[i]\n                                self.x_opt = self.population[i]\n                                self.best_fitness_history.append(self.f_opt)\n\n                        self.restart_counter = 0\n                else:\n                    self.restart_counter = 0\n            \n            # Orthogonal Learning:  Apply to the exploitation population.  Uses evaluations.\n            if self.budget > self.dim * (self.exploit_pop_size + 1):\n                oa = self.orthogonal_design(self.exploit_pop_size, self.dim)\n                for j in range(self.dim):\n                    vals = np.linspace(func.bounds.lb, func.bounds.ub, self.exploit_pop_size)\n                    for i in range(self.exploit_pop_size):\n                         trial = self.exploit_population[i].copy()\n                         trial[j] = vals[int(oa[i,j])]\n                         f_trial = func(trial)\n                         self.budget -= 1\n                         if f_trial < self.f_opt:\n                            self.f_opt = f_trial\n                            self.x_opt = trial\n                            self.best_fitness_history.append(self.f_opt)\n                         if f_trial < fitness[i + self.explore_pop_size]:\n                             fitness[i + self.explore_pop_size] = f_trial\n                             self.exploit_population[i] = trial\n                                          \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "An exception occured: TypeError: only size-1 arrays can be converted to Python scalars\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 144, in evaluate\n    algorithm(f_new)\n  File \"<string>\", line 152, in __call__\nValueError: setting an array element with a sequence.\n.", "error": "", "parent_ids": ["f54bc68e-b246-4aca-9137-1490f48440cc"], "operator": null, "metadata": {}}
{"id": "373556c9-2ce8-4f13-819e-73bf90ef9fae", "fitness": -Infinity, "name": "AdaptiveDE_OL", "description": "Adaptive Differential Evolution with orthogonal learning to enhance population diversity and convergence speed.", "code": "import numpy as np\n\nclass AdaptiveDE_OL:\n    def __init__(self, budget=10000, dim=10, pop_multiplier=5, orthogonal_trials=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = dim * pop_multiplier\n        self.F = 0.5  # Initial mutation factor\n        self.CR = 0.9 # Crossover rate\n        self.orthogonal_trials = orthogonal_trials # Number of orthogonal design trials\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                crossover = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                \n                # Orthogonal Learning\n                else:\n                    orthogonal_matrix = self.generate_orthogonal_array(self.dim, self.orthogonal_trials)\n                    best_orthogonal_f = np.inf\n                    best_orthogonal_x = None\n\n                    for j in range(self.orthogonal_trials):\n                        orthogonal_x = self.population[i].copy()\n                        for k in range(self.dim):\n                            if orthogonal_matrix[j,k] == 1:\n                                orthogonal_x[k] = np.clip(self.population[i][k] + 0.1 * (np.random.rand() - 0.5), func.bounds.lb, func.bounds.ub)\n                            else:\n                                orthogonal_x[k] = np.clip(self.population[i][k] - 0.1 * (np.random.rand() - 0.5), func.bounds.lb, func.bounds.ub)\n\n                        orthogonal_f = func(orthogonal_x)\n                        self.budget -= 1\n\n                        if orthogonal_f < best_orthogonal_f:\n                            best_orthogonal_f = orthogonal_f\n                            best_orthogonal_x = orthogonal_x\n\n                        if self.budget <= 0:\n                            break\n\n                    if best_orthogonal_f < self.fitness[i]:\n                         self.fitness[i] = best_orthogonal_f\n                         self.population[i] = best_orthogonal_x\n\n                         if best_orthogonal_f < self.f_opt:\n                            self.f_opt = best_orthogonal_f\n                            self.x_opt = best_orthogonal_x\n\n                if self.budget > 0 and np.random.rand() < 0.1:  # Occasionally perturb F and CR\n                    self.F = np.clip(np.random.normal(0.5, 0.3), 0.1, 1.0) # Adapt F\n                    self.CR = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0) # Adapt CR\n\n                if self.budget <=0:\n                    break\n\n        return self.f_opt, self.x_opt\n\n    def generate_orthogonal_array(self, n, k):\n          # generates a L_k(2^n) orthogonal array.\n          # In this implementation, we use a very simple (non-optimized) version, sufficient for small n and k\n          array = np.random.randint(0, 2, size=(k, n)) # Simple random array for demonstration purposes\n          return array", "configspace": "", "generation": 1, "feedback": "An exception occured: TypeError: only size-1 arrays can be converted to Python scalars\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 53, in __call__\nValueError: setting an array element with a sequence.\n.", "error": "", "parent_ids": ["c573ffeb-224d-474d-8890-07242998d2da"], "operator": null, "metadata": {}}
{"id": "597d6519-1be2-4830-b97b-46cf1a364c0e", "fitness": 0.0, "name": "AdaptiveHybridOptimization", "description": "An adaptive population-based algorithm using a combination of differential evolution, covariance matrix adaptation evolution strategy (CMA-ES)-inspired adaptation, and a restart mechanism to enhance exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimization:\n    def __init__(self, budget=10000, dim=10, pop_size=None, de_rate=0.7, cma_rate=0.2, restart_trigger=0.01):\n        \"\"\"\n        Args:\n            budget (int): The evaluation budget.\n            dim (int): The dimension of the problem.\n            pop_size (int): The population size. If None, it's set to 4 + int(3 * np.log(dim)).\n            de_rate (float): Probability of performing differential evolution.\n            cma_rate (float): Probability of performing CMA-ES-like adaptation.\n            restart_trigger (float): Threshold for triggering a population restart based on fitness stagnation.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 4 + int(3 * np.log(dim)) if pop_size is None else pop_size\n        self.de_rate = de_rate\n        self.cma_rate = cma_rate\n        self.restart_trigger = restart_trigger\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.mean = None  # CMA-ES like mean\n        self.sigma = 0.5  # CMA-ES like step size\n        self.cov = None # Covariance matrix\n\n    def __call__(self, func):\n        \"\"\"\n        Optimizes the given function using the allocated budget.\n\n        Args:\n            func (callable): The function to optimize.\n\n        Returns:\n            tuple: A tuple containing the best function value found and the corresponding solution vector.\n        \"\"\"\n        # Initialize population within bounds\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.mean = np.mean(population, axis=0)\n        self.cov = np.eye(self.dim) # Initialize covariance matrix\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Update the best solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index].copy()\n            self.best_fit_history = [self.f_opt]\n\n        # Optimization loop\n        while self.budget > 0:\n            new_population = np.zeros_like(population)\n            new_fitness = np.zeros_like(fitness)\n\n            for i in range(self.pop_size):\n                if np.random.rand() < self.de_rate:\n                    # Differential Evolution\n                    idxs = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = population[idxs]\n                    mutant = population[i] + 0.8 * (x_r1 - x_r2)\n                    \n                    # Ensure mutant stays within bounds\n                    mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                    # Crossover\n                    cross_points = np.random.rand(self.dim) < 0.9\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, population[i])\n                elif np.random.rand() < self.cma_rate:\n                    # CMA-ES-like adaptation\n                    try:\n                        z = np.random.multivariate_normal(np.zeros(self.dim), self.cov)\n                        trial = self.mean + self.sigma * z\n                    except: # Handle singular covariance matrix\n                        trial = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                    trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n                else:\n                    # Global Search (random)\n                    trial = np.random.uniform(func.bounds.lb, func.bounds.ub)\n\n                # Evaluation\n                f_trial = func(trial)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    new_fitness[i] = f_trial\n                    new_population[i] = trial.copy()\n                else:\n                    new_fitness[i] = fitness[i]\n                    new_population[i] = population[i].copy()\n\n                # Update the best solution\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial.copy()\n                \n                if self.budget <= 0:\n                    break\n            \n            population = new_population\n            fitness = new_fitness\n\n            # CMA-ES adaptation\n            self.mean = np.mean(population, axis=0)\n            C = np.cov(population.T)\n            if np.linalg.det(C) > 0: # Check determinant\n                self.cov = C\n\n            # Restart mechanism\n            self.best_fit_history.append(self.f_opt)\n            if len(self.best_fit_history) > 100:\n                self.best_fit_history.pop(0)\n                if np.std(self.best_fit_history) < self.restart_trigger:\n                    # Restart population\n                    population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                    fitness = np.array([func(x) for x in population])\n                    self.mean = np.mean(population, axis=0)\n                    self.cov = np.eye(self.dim)\n                    best_index = np.argmin(fitness)\n                    if fitness[best_index] < self.f_opt:\n                        self.f_opt = fitness[best_index]\n                        self.x_opt = population[best_index].copy()\n                    self.sigma = 0.5\n                    self.best_fit_history = [self.f_opt]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveHybridOptimization scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ee6dc2fa-c843-4dd3-8042-35bfff865b87"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "e637f0cc-38a9-4943-844a-1d5830364564", "fitness": 0.3618449008189887, "name": "PSO_SA", "description": "Combines Particle Swarm Optimization with Simulated Annealing for global search and fine-tuning, with adaptive temperature schedule.", "code": "import numpy as np\nimport copy\n\nclass PSO_SA:\n    def __init__(self, budget=10000, dim=10, pop_size=20, inertia=0.7, cognitive_coeff=1.4, social_coeff=1.4, initial_temp=1.0, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.inertia = inertia\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.initial_temp = initial_temp\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        # Initialize population\n        pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in pop])\n        self.budget -= self.pop_size\n\n        # Initialize velocities\n        velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))\n\n        # Initialize personal best positions and fitness\n        personal_best_positions = pop.copy()\n        personal_best_fitness = fitness.copy()\n\n        # Initialize global best position and fitness\n        global_best_index = np.argmin(fitness)\n        global_best_position = pop[global_best_index].copy()\n        global_best_fitness = fitness[global_best_index]\n\n        # Simulated Annealing parameters\n        temperature = self.initial_temp\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # PSO update\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.cognitive_coeff * np.random.rand() * (personal_best_positions[i] - pop[i]) +\n                                 self.social_coeff * np.random.rand() * (global_best_position - pop[i]))\n\n                pop[i] += velocities[i]\n                pop[i] = np.clip(pop[i], func.bounds.lb, func.bounds.ub)\n\n                # Evaluate new position\n                f = func(pop[i])\n                self.budget -= 1\n\n                # Simulated Annealing acceptance criterion\n                delta_e = f - personal_best_fitness[i]\n\n                if delta_e < 0:\n                    # Accept the new solution if it's better\n                    personal_best_fitness[i] = f\n                    personal_best_positions[i] = pop[i].copy()\n\n                    if f < global_best_fitness:\n                        global_best_fitness = f\n                        global_best_position = pop[i].copy()\n                else:\n                    # Accept the new solution with a probability based on temperature\n                    acceptance_prob = np.exp(-delta_e / temperature)\n                    if np.random.rand() < acceptance_prob:\n                        personal_best_fitness[i] = f\n                        personal_best_positions[i] = pop[i].copy()\n\n            # Cooling\n            temperature *= self.cooling_rate\n\n        return global_best_fitness, global_best_position", "configspace": "", "generation": 1, "feedback": "The algorithm PSO_SA scored 0.362 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["04ee848b-d480-4b68-8bae-d9e906d98cb2"], "operator": null, "metadata": {"aucs": [0.12554916325849785, 0.29747958344109027, 0.8303150060691702, 0.2313509021729211, 0.2648503796054581, 0.28107172440136297, 0.2445075534425658, 0.3928192416815637, 0.283389527670619, 0.15608240537680462, 0.2318605228201449, 0.9984572598600249, 0.34607931541690085, 0.2650835065796151, 0.5433142122525194, 0.36569315023141025, 0.30127863922052434, 0.3793871595564883, 0.2196401645014885, 0.4786885988206029]}}
{"id": "d7579762-3ff0-46ce-b447-539068fbbfd5", "fitness": 0.41656229282558865, "name": "ReinforcedAdaptiveDE", "description": "An adaptive differential evolution algorithm with a modified mutation strategy based on fitness landscape analysis and dynamic parameter control using a reinforcement learning approach.", "code": "import numpy as np\n\nclass ReinforcedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, learning_rate=0.1, gamma=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.learning_rate = learning_rate\n        self.gamma = gamma  # Discount factor for future rewards\n        self.F = 0.5\n        self.CR = 0.9\n        self.archive = []\n        self.success_F = []\n        self.success_CR = []\n        self.success_delta_f = []\n        self.p = 0.1  # probability for stochastic ranking\n        self.Q_F = {0.1: 0.0, 0.5: 0.0, 0.9: 0.0} # Q-table for F values\n        self.Q_CR = {0.1: 0.0, 0.5: 0.0, 0.9: 0.0} # Q-table for CR values\n        self.possible_F = [0.1, 0.5, 0.9]\n        self.possible_CR = [0.1, 0.5, 0.9]\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        for i in range(self.pop_size):\n            if fitness[i] < self.f_opt:\n                self.f_opt = fitness[i]\n                self.x_opt = self.population[i]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Parameter Selection using Epsilon-Greedy\n                if np.random.rand() < 0.1: # Epsilon = 0.1\n                    self.F = np.random.choice(self.possible_F)\n                    self.CR = np.random.choice(self.possible_CR)\n                else:\n                    self.F = max(self.Q_F, key=self.Q_F.get)\n                    self.CR = max(self.Q_CR, key=self.Q_CR.get)\n\n\n                # Mutation: Adaptive based on landscape\n                if np.random.rand() < 0.5: # Exploration: DE mutation\n                    idxs = np.random.choice(self.pop_size, 3, replace=False)\n                    x_1, x_2, x_3 = self.population[idxs]\n                    mutant = x_1 + self.F * (x_2 - x_3)\n                else:  # Exploitation: Gradient-based mutation (simplified)\n                    # Approximate gradient direction using a random pair\n                    idxs = np.random.choice(self.pop_size, 2, replace=False)\n                    x_1, x_2 = self.population[idxs]\n                    mutant = self.population[i] + self.F * (x_1 - x_2) # Simplified gradient approx.\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n\n                # Crossover\n                crossover = np.random.uniform(size=self.dim) < self.CR\n                trial = np.where(crossover, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                # Stochastic Ranking\n                if (fitness[i] < 0 and f_trial < 0) or np.random.rand() < self.p:\n                    if f_trial < fitness[i]:\n                        reward = fitness[i] - f_trial # Reward is the improvement in fitness\n                        \n                        #Update Q-Table\n                        self.Q_F[self.F] = self.Q_F[self.F] + self.learning_rate * (reward + self.gamma * max(self.Q_F.values()) - self.Q_F[self.F])\n                        self.Q_CR[self.CR] = self.Q_CR[self.CR] + self.learning_rate * (reward + self.gamma * max(self.Q_CR.values()) - self.Q_CR[self.CR])\n                        \n                        fitness[i] = f_trial\n                        self.population[i] = trial\n\n                        if len(self.archive) < self.archive_size:\n                            self.archive.append(self.population[i].copy())\n                        else:\n                            idx_to_replace = np.random.randint(0, self.archive_size)\n                            self.archive[idx_to_replace] = self.population[i].copy()\n                else:\n                    if f_trial < fitness[i]:\n                        fitness[i] = f_trial\n                        self.population[i] = trial\n\n                        if len(self.archive) < self.archive_size:\n                            self.archive.append(self.population[i].copy())\n                        else:\n                            idx_to_replace = np.random.randint(0, self.archive_size)\n                            self.archive[idx_to_replace] = self.population[i].copy()\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm ReinforcedAdaptiveDE scored 0.417 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f54bc68e-b246-4aca-9137-1490f48440cc"], "operator": null, "metadata": {"aucs": [0.14348925945083102, 0.2629151312913014, 0.321298775720978, 0.2866250802207274, 0.588705705683879, 0.33566544689164635, 0.38758130419093817, 0.48856485002302785, 0.26710922220787126, 0.1843105992767935, 0.5794408346480666, 0.963889678743193, 0.28571105612956427, 0.4277562249339427, 0.6952936901838824, 0.34121824693821423, 0.2952070221957318, 0.7581047041593652, 0.22613661481812286, 0.4922224088036963]}}
{"id": "6eed06d4-c781-49b2-bab4-812d8f5a5b78", "fitness": 0.3093016658560006, "name": "HybridPSODE_SA", "description": "A population-based algorithm that combines aspects of PSO, DE, and Simulated Annealing (SA) for global search with adaptive parameter control and SA-based local refinement.", "code": "import numpy as np\n\nclass HybridPSODE_SA:\n    def __init__(self, budget=10000, dim=10, pop_size=20, pso_weight=0.7, de_cross_rate=0.9, initial_temp=1.0, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.pso_weight = pso_weight\n        self.de_cross_rate = de_cross_rate\n        self.initial_temp = initial_temp\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        # Initialize population\n        pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in pop])\n        self.budget -= self.pop_size\n        \n        # Initialize velocities for PSO\n        velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))\n\n        # Initialize best positions and fitness\n        personal_best_positions = pop.copy()\n        personal_best_fitness = fitness.copy()\n        global_best_index = np.argmin(fitness)\n        global_best_position = pop[global_best_index].copy()\n        global_best_fitness = fitness[global_best_index]\n        \n        temp = self.initial_temp  # Initialize temperature for SA\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # PSO update\n                inertia = self.pso_weight * velocities[i]\n                cognitive = np.random.rand() * (personal_best_positions[i] - pop[i])\n                social = np.random.rand() * (global_best_position - pop[i])\n                velocities[i] = inertia + cognitive + social\n                pop[i] += velocities[i]\n                pop[i] = np.clip(pop[i], func.bounds.lb, func.bounds.ub)\n                \n                # Differential Evolution mutation\n                if np.random.rand() < self.de_cross_rate:\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = pop[indices[0]], pop[indices[1]], pop[indices[2]]\n                    mutation = x_r1 + 0.5 * (x_r2 - x_r3)  #DE mutation\n                    \n                    # Crossover\n                    crossover_mask = np.random.rand(self.dim) < self.de_cross_rate\n                    pop[i] = np.where(crossover_mask, mutation, pop[i])\n                    pop[i] = np.clip(pop[i], func.bounds.lb, func.bounds.ub)\n                \n                # Evaluate new position\n                f = func(pop[i])\n                self.budget -= 1\n                \n                # Simulated Annealing acceptance criterion\n                delta_e = f - personal_best_fitness[i]\n                if delta_e < 0:\n                    personal_best_fitness[i] = f\n                    personal_best_positions[i] = pop[i].copy()\n                    if f < global_best_fitness:\n                        global_best_fitness = f\n                        global_best_position = pop[i].copy()\n                else:\n                    acceptance_prob = np.exp(-delta_e / temp)\n                    if np.random.rand() < acceptance_prob:\n                        personal_best_fitness[i] = f\n                        personal_best_positions[i] = pop[i].copy()\n                        pop[i] = pop[i].copy()  # SA accepted, update pop\n                \n            # Cooling the temperature\n            temp *= self.cooling_rate\n            \n            global_best_index = np.argmin(personal_best_fitness)\n            global_best_position = personal_best_positions[global_best_index].copy()\n            global_best_fitness = personal_best_fitness[global_best_index]\n\n        return global_best_fitness, global_best_position", "configspace": "", "generation": 1, "feedback": "The algorithm HybridPSODE_SA scored 0.309 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["04ee848b-d480-4b68-8bae-d9e906d98cb2"], "operator": null, "metadata": {"aucs": [0.12287219269180927, 0.23432739997790863, 0.2779327173597047, 0.28000276993302564, 0.23981404523337801, 0.27576665014404733, 0.255359708224434, 0.22482826352681007, 0.22337796982315572, 0.18118621452443096, 0.24704260400786393, 0.9928602961738642, 0.2731668489349257, 0.25543886360276147, 0.6353939157745594, 0.2879195814674457, 0.24606505662813938, 0.30051820278534036, 0.16288961155270665, 0.4692704047537002]}}
{"id": "707be948-3733-45ca-bec4-f4a213d00e5f", "fitness": 0.4963454081389285, "name": "AdaptiveDECMA", "description": "Adaptive Differential Evolution with covariance matrix adaptation for mutation and a restart strategy.", "code": "import numpy as np\n\nclass AdaptiveDECMA:\n    def __init__(self, budget=10000, dim=10, pop_multiplier=5, initial_F=0.5, initial_CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = dim * pop_multiplier\n        self.F = initial_F  # Initial mutation factor\n        self.CR = initial_CR # Crossover rate\n        self.mean = np.zeros(dim) # Mean of the population\n        self.C = np.eye(dim) # Covariance matrix\n        self.c_learn = 0.1 # Learning rate for covariance matrix\n        self.mu = self.pop_size // 4 # Number of individuals for updating CMA\n        self.restart_trigger = 100 # Number of iterations without improvement before restart\n        self.no_improvement_counter = 0\n        self.best_fitness_ever = np.inf\n        self.lb = -5.0\n        self.ub = 5.0\n        self.x_opt = None\n        self.f_opt = np.inf\n\n\n    def __call__(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        while self.budget > 0:\n            # Sort population by fitness\n            sorted_indices = np.argsort(self.fitness)\n            self.population = self.population[sorted_indices]\n            self.fitness = self.fitness[sorted_indices]\n\n            if self.fitness[0] < self.best_fitness_ever:\n                self.best_fitness_ever = self.fitness[0]\n                self.x_opt = self.population[0]\n                self.f_opt = self.fitness[0]\n                self.no_improvement_counter = 0\n            else:\n                self.no_improvement_counter += 1\n\n            if self.no_improvement_counter > self.restart_trigger:\n                # Restart strategy\n                self.mean = np.zeros(self.dim)\n                self.C = np.eye(self.dim)\n                self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.budget -= self.pop_size\n                self.no_improvement_counter = 0\n                continue\n\n            for i in range(self.pop_size):\n                # Mutation using CMA\n                z = np.random.multivariate_normal(np.zeros(self.dim), self.C)\n                mutant = self.population[i] + self.F * z\n                mutant = np.clip(mutant, self.lb, self.ub)\n                \n                # Crossover\n                crossover = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                if self.budget <= 0:\n                    break\n\n            # Update CMA\n            self.mean = np.mean(self.population[:self.mu], axis=0)\n            diff = self.population[:self.mu] - self.mean\n            self.C = (1 - self.c_learn) * self.C + self.c_learn * (1/self.mu) * np.sum([np.outer(diff[i], diff[i]) for i in range(self.mu)], axis=0)\n            # Ensure C is positive definite\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDECMA scored 0.496 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c573ffeb-224d-474d-8890-07242998d2da"], "operator": null, "metadata": {"aucs": [0.11256213423653882, 0.2474700717242304, 0.5771990066274048, 0.7445141153000572, 0.3857840988383515, 0.6979271125493105, 0.3006250177515122, 0.4986324860635072, 0.4326342268827382, 0.17572778703703062, 0.8032864151620459, 0.977964031974407, 0.5174151671631921, 0.29166535074631894, 0.8749936761373136, 0.5650436328499178, 0.3642443103099027, 0.6882863734267405, 0.1755167607923338, 0.4954163872057171]}}
{"id": "541bb1bc-1380-4491-b822-168b81deabcc", "fitness": 0.42882754960688774, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with a distance-based mutation strategy and orthogonal learning to enhance exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = 0.5\n        self.CR = 0.9\n        self.archive = []\n        self.success_F = []\n        self.success_CR = []\n        self.success_delta_f = []\n        self.p = 0.1 # probability for stochastic ranking\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        \n        for i in range(self.pop_size):\n            if fitness[i] < self.f_opt:\n                self.f_opt = fitness[i]\n                self.x_opt = self.population[i]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation based on distance\n                distances = np.linalg.norm(self.population - self.population[i], axis=1)\n                probabilities = np.exp(-distances / np.std(distances))\n                probabilities[i] = 0  # Avoid selecting the current individual\n                probabilities /= np.sum(probabilities)\n                \n                idxs = np.random.choice(self.pop_size, 2, replace=False, p=probabilities)\n                x_1, x_2 = self.population[idxs]\n                \n                # Select a third random individual (avoiding the other two)\n                remaining_indices = np.array([j for j in range(self.pop_size) if j not in idxs and j != i])\n                x_3 = self.population[np.random.choice(remaining_indices)]\n\n\n                mutant = self.population[i] + self.F * (x_1 - x_2)  + 0.01*np.random.normal(0,1, self.dim)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                crossover = np.random.uniform(size=self.dim) < self.CR\n                trial = np.where(crossover, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n                \n                # Stochastic Ranking\n                if (fitness[i] < 0 and f_trial < 0) or np.random.rand() < self.p:\n                    if f_trial < fitness[i]:\n                        self.success_F.append(self.F)\n                        self.success_CR.append(self.CR)\n                        self.success_delta_f.append(np.abs(f_trial - fitness[i]))\n                        \n                        fitness[i] = f_trial\n                        self.population[i] = trial\n                        \n                        if len(self.archive) < self.archive_size:\n                            self.archive.append(self.population[i].copy())\n                        else:\n                            idx_to_replace = np.random.randint(0, self.archive_size)\n                            self.archive[idx_to_replace] = self.population[i].copy()\n                else:\n                     if f_trial < fitness[i]:\n                        fitness[i] = f_trial\n                        self.population[i] = trial\n                        \n                        if len(self.archive) < self.archive_size:\n                            self.archive.append(self.population[i].copy())\n                        else:\n                            idx_to_replace = np.random.randint(0, self.archive_size)\n                            self.archive[idx_to_replace] = self.population[i].copy()\n\n\n            # Parameter Adaptation\n            if self.success_F:\n                self.F = np.mean(self.success_F)\n                self.CR = np.mean(self.success_CR)\n\n                # Reset success history\n                self.success_F = []\n                self.success_CR = []\n                self.success_delta_f = []\n            \n            self.F = np.clip(self.F, 0.1, 1.0)\n            self.CR = np.clip(self.CR, 0.1, 1.0)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDE scored 0.429 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f54bc68e-b246-4aca-9137-1490f48440cc"], "operator": null, "metadata": {"aucs": [0.16148616339899968, 0.2950527936945936, 0.41177192150715736, 0.6633546647773623, 0.32371080299462396, 0.45532140883779626, 0.2973754445059885, 0.3707243159641823, 0.31684455916827314, 0.21379853958140727, 0.6147426365073863, 0.9875070532855798, 0.33927334909301554, 0.33690573248660927, 0.7802878683012927, 0.42638977105622666, 0.3448733294615377, 0.5548636017905662, 0.19542318260962532, 0.48684385311553025]}}
{"id": "f7921322-b45c-40f4-bfbd-4091e41bbe46", "fitness": -Infinity, "name": "AdaptiveDE_NM", "description": "Hybrid algorithm combining Differential Evolution with a Nelder-Mead local search, adaptively adjusting the frequency of local search based on performance.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDE_NM:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, CR=0.7, local_search_prob=0.1, local_search_scale=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.local_search_prob = local_search_prob\n        self.local_search_scale = local_search_scale\n\n    def __call__(self, func):\n        # Initialize population\n        pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in pop])\n        self.budget -= self.pop_size\n\n        # Find best initial solution\n        best_index = np.argmin(fitness)\n        best_fitness = fitness[best_index]\n        best_solution = pop[best_index].copy()\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Differential Evolution mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = pop[a] + self.F * (pop[b] - pop[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = pop[i].copy()\n                mask = np.random.rand(self.dim) < self.CR\n                trial[mask] = mutant[mask]\n\n                # Evaluate trial vector\n                f = func(trial)\n                self.budget -= 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    pop[i] = trial.copy()\n\n                    # Update best solution\n                    if f < best_fitness:\n                        best_fitness = f\n                        best_solution = trial.copy()\n\n            # Adaptive Local Search using Nelder-Mead\n            if np.random.rand() < self.local_search_prob:\n                index = np.random.randint(self.pop_size)\n                initial_x = pop[index].copy()\n\n                def obj_func(x):\n                    return func(x)\n\n                result = minimize(obj_func, initial_x, method='Nelder-Mead', bounds=func.bounds)\n\n                if result.fun < fitness[index]:\n                    fitness[index] = result.fun\n                    pop[index] = result.x.copy()\n                    \n                    if result.fun < best_fitness:\n                        best_fitness = result.fun\n                        best_solution = result.x.copy()\n\n                self.budget -= result.nfev  # Account for function evaluations in Nelder-Mead\n                if self.budget <= 0:\n                    break\n            \n        return best_fitness, best_solution", "configspace": "", "generation": 2, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 60, in __call__\nNameError: name 'minimize' is not defined\n.", "error": "", "parent_ids": ["e637f0cc-38a9-4943-844a-1d5830364564"], "operator": null, "metadata": {}}
{"id": "d4e84dca-c527-4b4a-b476-30565c5593f0", "fitness": -Infinity, "name": "DE_NM", "description": "Hybridizes Differential Evolution with a Nelder-Mead simplex search for local refinement, adapting the scaling factor based on the search stage.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DE_NM:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, CR=0.7, local_search_frequency=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Differential evolution scaling factor\n        self.CR = CR  # Crossover rate\n        self.local_search_frequency = local_search_frequency # Frequency of Nelder-Mead application\n\n    def __call__(self, func):\n        # Initialize population\n        pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in pop])\n        self.budget -= self.pop_size\n\n        # Find the best initial solution\n        best_index = np.argmin(fitness)\n        best_fitness = fitness[best_index]\n        best_solution = pop[best_index].copy()\n        \n        generation = 0\n\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Differential Evolution mutation and crossover\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                \n                # Adapt scaling factor F during the search\n                adaptive_F = self.F * (0.5 + 0.5 * (self.budget / 10000)) \n                mutant = pop[a] + adaptive_F * (pop[b] - pop[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = pop[i].copy()\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial[cross_points] = mutant[cross_points]\n\n                # Evaluate trial vector\n                f = func(trial)\n                self.budget -= 1\n\n                # Selection\n                if f < fitness[i]:\n                    fitness[i] = f\n                    pop[i] = trial\n                    \n                    if f < best_fitness:\n                        best_fitness = f\n                        best_solution = trial.copy()\n                        \n\n            # Apply Nelder-Mead local search every local_search_frequency generations\n            if generation % self.local_search_frequency == 0:\n                for i in range(self.pop_size):\n                    if self.budget <= 0:\n                        break\n                        \n                    # Perform local search around the individual\n                    res = minimize(func, pop[i], method='Nelder-Mead', \n                                   bounds=np.column_stack((np.full(self.dim, func.bounds.lb), np.full(self.dim, func.bounds.ub))),\n                                   options={'maxfev': min(50, self.budget)}) # Limit the function evaluations\n\n                    if res.success: #Check if optimization was successful\n                        if res.fun < fitness[i]:\n                            fitness[i] = res.fun\n                            pop[i] = res.x\n                            \n                            if res.fun < best_fitness:\n                                best_fitness = res.fun\n                                best_solution = res.x.copy()\n                    \n                    self.budget -= res.nfev #Reduce the budget based on function evaluations in Nelder-Mead\n\n        return best_fitness, best_solution", "configspace": "", "generation": 2, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 144, in evaluate\n    algorithm(f_new)\n  File \"<string>\", line 66, in __call__\nNameError: name 'minimize' is not defined\n.", "error": "", "parent_ids": ["e637f0cc-38a9-4943-844a-1d5830364564"], "operator": null, "metadata": {}}
{"id": "f9c7c501-24d9-4f0c-a4ac-39de957e47d2", "fitness": -Infinity, "name": "AdaptiveDEOA", "description": "An adaptive Differential Evolution strategy with orthogonal array-based mutation and a local search fine-tuning step.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEOA:\n    def __init__(self, budget=10000, dim=10, pop_multiplier=5, initial_F=0.5, initial_CR=0.9, oa_design_points=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = dim * pop_multiplier\n        self.F = initial_F  # Initial mutation factor\n        self.CR = initial_CR # Crossover rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.x_opt = None\n        self.f_opt = np.inf\n        self.oa_design_points = oa_design_points # Number of design points for Orthogonal Array\n        self.local_search_frequency = 10 # Frequency of local search\n        self.local_search_radius = 0.1 # Radius for local search\n\n    def generate_orthogonal_array(self, n, k, levels):\n        \"\"\"Generates an orthogonal array using a simple method.\"\"\"\n        oa = np.zeros((n, k), dtype=int)\n        for i in range(n):\n            for j in range(k):\n                oa[i, j] = i % levels\n        return oa\n\n    def __call__(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        iteration = 0\n\n        while self.budget > 0:\n            # Sort population by fitness\n            sorted_indices = np.argsort(self.fitness)\n            self.population = self.population[sorted_indices]\n            self.fitness = self.fitness[sorted_indices]\n\n            if self.fitness[0] < self.f_opt:\n                self.f_opt = self.fitness[0]\n                self.x_opt = self.population[0]\n\n            for i in range(self.pop_size):\n                # Mutation using Orthogonal Array\n                oa = self.generate_orthogonal_array(self.oa_design_points, self.dim, 3)\n                mutant = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if oa[i % self.oa_design_points, j] == 1:\n                        mutant[j] = self.population[np.random.randint(self.pop_size)][j]  # Random individual\n                    elif oa[i % self.oa_design_points, j] == 2:\n                        mutant[j] = self.population[np.random.randint(self.pop_size)][j] + self.F * (self.population[np.random.randint(self.pop_size)][j] - self.population[i][j]) # DE Mutation\n\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                crossover = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                if self.budget <= 0:\n                    break\n            \n            iteration += 1\n            if iteration % self.local_search_frequency == 0:\n                # Local search around the best solution\n                best_index = np.argmin(self.fitness)\n                x_best = self.population[best_index].copy()\n                \n                # Define bounds for local search\n                bounds = [(max(self.lb, x_best[i] - self.local_search_radius), min(self.ub, x_best[i] + self.local_search_radius)) for i in range(self.dim)]\n                \n                # Perform local search\n                res = minimize(func, x_best, method='L-BFGS-B', bounds=bounds, options={'maxfun': min(self.budget, self.pop_size)})\n                \n                if res.fun < self.f_opt:\n                        self.f_opt = res.fun\n                        self.x_opt = res.x\n\n                self.budget -= min(self.budget, self.pop_size)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 144, in evaluate\n    algorithm(f_new)\n  File \"<string>\", line 79, in __call__\nNameError: name 'minimize' is not defined\n.", "error": "", "parent_ids": ["707be948-3733-45ca-bec4-f4a213d00e5f"], "operator": null, "metadata": {}}
{"id": "15362e89-d415-4076-b737-8bf2604d3ee4", "fitness": -Infinity, "name": "HybridDE_NM", "description": "A hybrid algorithm combining Differential Evolution with a Nelder-Mead simplex search, adaptively switching between global exploration and local exploitation based on stagnation detection.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridDE_NM:\n    def __init__(self, budget=10000, dim=10, pop_multiplier=5, initial_F=0.5, initial_CR=0.9, nm_restarts=2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = dim * pop_multiplier\n        self.F = initial_F\n        self.CR = initial_CR\n        self.lb = -5.0\n        self.ub = 5.0\n        self.x_opt = None\n        self.f_opt = np.inf\n        self.stagnation_limit = 50\n        self.stagnation_counter = 0\n        self.exploration_phase = True\n        self.nm_restarts = nm_restarts\n\n    def differential_evolution(self, func):\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = population[indices]\n                mutant = population[i] + self.F * (x2 - x3)\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                crossover = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover, mutant, population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n            # Update best solution\n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n                self.stagnation_counter = 0\n            else:\n                self.stagnation_counter += 1\n\n            if self.stagnation_counter > self.stagnation_limit:\n                self.exploration_phase = False\n                break  # Switch to Nelder-Mead\n\n        return population, fitness\n\n\n    def nelder_mead(self, func, x0):\n        bounds = ((self.lb, self.ub),) * self.dim\n        result = minimize(func, x0, method='Nelder-Mead', bounds=bounds, options={'maxfev': self.budget})\n        self.budget -= result.nfev\n        if result.fun < self.f_opt:\n             self.f_opt = result.fun\n             self.x_opt = result.x\n        return result.x, result.fun\n\n    def __call__(self, func):\n        # Initial Differential Evolution phase\n        population, fitness = self.differential_evolution(func)\n\n        # Nelder-Mead phase if DE stagnates\n        if not self.exploration_phase:\n            for _ in range(self.nm_restarts):\n              if self.budget <= 0:\n                break\n              best_index = np.argmin(fitness)\n              x0 = population[best_index].copy() # start NM from DE's best\n              _, _ = self.nelder_mead(func, x0)\n\n        # Nelder-Mead refinement from the best point found in DE if there is budget left and DE didn't stagnate\n        if self.exploration_phase and self.budget > 0:\n            best_index = np.argmin(fitness)\n            x0 = population[best_index].copy() # start NM from DE's best\n            _, _ = self.nelder_mead(func, x0)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 144, in evaluate\n    algorithm(f_new)\n  File \"<string>\", line 83, in __call__\n  File \"<string>\", line 65, in nelder_mead\nNameError: name 'minimize' is not defined\n.", "error": "", "parent_ids": ["707be948-3733-45ca-bec4-f4a213d00e5f"], "operator": null, "metadata": {}}
{"id": "41d87f40-c9fe-44bd-9d04-d629d813477d", "fitness": 0.0, "name": "CooperativeSwarmEnhancedDE", "description": "Cooperative Swarm-Enhanced Differential Evolution: Integrates multiple interacting swarms with a DE framework, where swarms specialize in exploration or exploitation and periodically exchange information to improve convergence.", "code": "import numpy as np\n\nclass CooperativeSwarmEnhancedDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, num_swarms=3, w=0.7, c1=1.5, c2=1.5, F=0.5, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.num_swarms = num_swarms\n        self.w = w  # Inertia weight for PSO\n        self.c1 = c1 # Cognitive coefficient for PSO\n        self.c2 = c2 # Social coefficient for PSO\n        self.F = F    # Mutation factor for DE\n        self.CR = CR   # Crossover rate for DE\n        self.swarms = []\n        self.swarm_best_fitness = []\n        self.swarm_best_positions = []\n        self.global_best_fitness = np.Inf\n        self.global_best_position = None\n\n        for _ in range(self.num_swarms):\n            self.swarms.append(np.random.uniform(-5, 5, size=(self.pop_size, self.dim)))\n            self.swarm_best_fitness.append(np.inf * np.ones(self.pop_size))\n            self.swarm_best_positions.append(np.zeros((self.pop_size, self.dim)))\n\n    def __call__(self, func):\n\n        # Initialize velocities for each swarm\n        velocities = [np.zeros_like(swarm) for swarm in self.swarms]\n\n        # Evaluate initial population\n        for i in range(self.num_swarms):\n            for j in range(self.pop_size):\n                fitness = func(self.swarms[i][j])\n                self.budget -= 1\n                if fitness < self.swarm_best_fitness[i][j]:\n                    self.swarm_best_fitness[i][j] = fitness\n                    self.swarm_best_positions[i][j] = self.swarms[i][j].copy()\n                if fitness < self.global_best_fitness:\n                    self.global_best_fitness = fitness\n                    self.global_best_position = self.swarms[i][j].copy()\n\n                if self.budget <= 0:\n                    return self.global_best_fitness, self.global_best_position\n\n        while self.budget > 0:\n            for i in range(self.num_swarms):\n                # Perform PSO update\n                r1 = np.random.rand(self.pop_size, self.dim)\n                r2 = np.random.rand(self.pop_size, self.dim)\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (self.swarm_best_positions[i] - self.swarms[i]) +\n                                 self.c2 * r2 * (np.tile(self.global_best_position, (self.pop_size, 1)) - self.swarms[i]))\n                self.swarms[i] += velocities[i]\n                self.swarms[i] = np.clip(self.swarms[i], -5, 5)\n\n\n                # Perform DE mutation\n                for j in range(self.pop_size):\n                    idxs = np.random.choice(self.pop_size, 3, replace=False)\n                    x_1, x_2, x_3 = self.swarms[i][idxs]\n                    mutant = x_1 + self.F * (x_2 - x_3)\n                    mutant = np.clip(mutant, -5, 5)\n\n                    # Perform DE crossover\n                    crossover = np.random.uniform(size=self.dim) < self.CR\n                    trial = np.where(crossover, mutant, self.swarms[i][j])\n                    trial = np.clip(trial, -5, 5)\n\n                    # Evaluate trial vector\n                    fitness = func(trial)\n                    self.budget -= 1\n                    if fitness < self.swarm_best_fitness[i][j]:\n                        self.swarm_best_fitness[i][j] = fitness\n                        self.swarm_best_positions[i][j] = trial.copy()\n                    if fitness < self.global_best_fitness:\n                        self.global_best_fitness = fitness\n                        self.global_best_position = trial.copy()\n                    \n                    # Selection: replace population[i] with trial only if trial is better\n                    if fitness < func(self.swarms[i][j]): # Budget already decremented!\n                       self.swarms[i][j] = trial.copy() # Replace with trial vector\n\n                    if self.budget <= 0:\n                        return self.global_best_fitness, self.global_best_position\n            \n            # Swarm information exchange (periodically)\n            if (self.budget % (self.pop_size * self.num_swarms)) < self.pop_size : # Example: every generation\n                # Sort swarms by best fitness\n                sorted_swarms_indices = np.argsort([np.min(fitness) for fitness in self.swarm_best_fitness])\n                best_swarm_idx = sorted_swarms_indices[0]\n                worst_swarm_idx = sorted_swarms_indices[-1]\n\n                # Replace a portion of the worst swarm with the best swarm\n                num_to_replace = int(0.2 * self.pop_size)\n                worst_idxs = np.argsort(self.swarm_best_fitness[worst_swarm_idx])[::-1][:num_to_replace] # Worst fitness in worst swarm\n                best_idxs = np.argsort(self.swarm_best_fitness[best_swarm_idx])[:num_to_replace] # Best fitness in best swarm\n\n                self.swarms[worst_swarm_idx][worst_idxs] = self.swarms[best_swarm_idx][best_idxs].copy()  #Copy best positions from best swarm to worst swarm\n                self.swarm_best_fitness[worst_swarm_idx][worst_idxs] = self.swarm_best_fitness[best_swarm_idx][best_idxs].copy()\n                self.swarm_best_positions[worst_swarm_idx][worst_idxs] = self.swarm_best_positions[best_swarm_idx][best_idxs].copy()\n        \n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 2, "feedback": "The algorithm CooperativeSwarmEnhancedDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d7579762-3ff0-46ce-b447-539068fbbfd5"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "b08b4939-5d05-4b92-abbb-1563d86023df", "fitness": -Infinity, "name": "DE_NM", "description": "Hybrid optimization combining Differential Evolution's exploration with Nelder-Mead's local search for efficient optimization.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DE_NM:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, CR=0.7, nm_iterations=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.nm_iterations = nm_iterations\n\n    def __call__(self, func):\n        # Initialize population\n        pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in pop])\n        self.budget -= self.pop_size\n\n        # Find initial best\n        best_index = np.argmin(fitness)\n        best_x = pop[best_index].copy()\n        best_f = fitness[best_index]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Differential Evolution mutation and crossover\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = pop[a] + self.F * (pop[b] - pop[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                trial = np.copy(pop[i])\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial[cross_points] = mutant[cross_points]\n\n                # Evaluate trial vector\n                f = func(trial)\n                self.budget -= 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    pop[i] = trial\n                    # Nelder-Mead local search around the improved solution\n                    if self.budget > 0 and self.nm_iterations > 0:\n                      nm_result = minimize(func, pop[i], method='Nelder-Mead', options={'maxiter': self.nm_iterations})\n                      if nm_result.success:\n                        f_nm = nm_result.fun\n                        x_nm = nm_result.x\n                        \n                        num_evals = nm_result.nfev\n                        self.budget -= num_evals\n\n                        if f_nm < fitness[i]:\n                            fitness[i] = f_nm\n                            pop[i] = x_nm\n                            f = f_nm # Update f to the improved fitness for DE update\n\n                # Update best solution\n                if f < best_f:\n                    best_f = f\n                    best_x = pop[i].copy()\n\n        return best_f, best_x", "configspace": "", "generation": 2, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 46, in __call__\nNameError: name 'minimize' is not defined\n.", "error": "", "parent_ids": ["e637f0cc-38a9-4943-844a-1d5830364564"], "operator": null, "metadata": {}}
{"id": "13a9ba14-19d2-45d7-9b26-ce9d6b1c2bbf", "fitness": 0.277334256081064, "name": "SelfOrganizingScoutPSO", "description": "A self-organizing scout particle swarm optimization with velocity clamping and dynamic population size.", "code": "import numpy as np\n\nclass SelfOrganizingScoutPSO:\n    def __init__(self, budget=10000, dim=10, pop_size_min=20, pop_size_max=100, scout_rate=0.1, inertia=0.7, cognitive_coeff=1.4, social_coeff=1.4, velocity_clamp=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_min = pop_size_min\n        self.pop_size_max = pop_size_max\n        self.pop_size = pop_size_max #start with a bigger population and decrease it if needed\n        self.scout_rate = scout_rate\n        self.inertia = inertia\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.velocity_clamp = velocity_clamp\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.swarm = None\n        self.velocities = None\n        self.local_best_positions = None\n        self.local_best_fitness = None\n        self.global_best_position = None\n        self.global_best_fitness = np.Inf\n\n    def initialize_swarm(self, func):\n        self.swarm = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.random.uniform(-self.velocity_clamp, self.velocity_clamp, size=(self.pop_size, self.dim))\n        self.local_best_positions = self.swarm.copy()\n        self.local_best_fitness = np.array([func(x) for x in self.swarm])\n        self.budget -= self.pop_size\n\n        self.global_best_position = self.local_best_positions[np.argmin(self.local_best_fitness)].copy()\n        self.global_best_fitness = np.min(self.local_best_fitness)\n\n        if self.global_best_fitness < self.f_opt:\n            self.f_opt = self.global_best_fitness\n            self.x_opt = self.global_best_position.copy()\n\n    def update_velocity(self, i):\n        r1 = np.random.rand(self.dim)\n        r2 = np.random.rand(self.dim)\n        cognitive_component = self.cognitive_coeff * r1 * (self.local_best_positions[i] - self.swarm[i])\n        social_component = self.social_coeff * r2 * (self.global_best_position - self.swarm[i])\n        self.velocities[i] = self.inertia * self.velocities[i] + cognitive_component + social_component\n        self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)\n\n    def update_position(self, i, func):\n        new_position = self.swarm[i] + self.velocities[i]\n        new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n        return new_position\n\n    def scout(self, func):\n        # Replace a percentage of worst performing particles with new random ones\n        num_scouts = int(self.scout_rate * self.pop_size)\n        worst_indices = np.argsort(self.local_best_fitness)[-num_scouts:] # Indices of worst particles\n\n        for i in worst_indices:\n            self.swarm[i] = np.random.uniform(func.bounds.lb, func.bounds.ub)\n            self.velocities[i] = np.random.uniform(-self.velocity_clamp, self.velocity_clamp, self.dim)\n            fitness = func(self.swarm[i])\n            self.budget -= 1\n\n            if fitness < self.f_opt:\n                self.f_opt = fitness\n                self.x_opt = self.swarm[i].copy()\n\n            self.local_best_positions[i] = self.swarm[i].copy()\n            self.local_best_fitness[i] = fitness\n\n            if fitness < self.global_best_fitness:\n                    self.global_best_fitness = fitness\n                    self.global_best_position = self.swarm[i].copy()\n\n\n    def adjust_population_size(self):\n        # Dynamically adjust population size based on stagnation\n        if np.std(self.local_best_fitness) < 1e-6 and self.pop_size > self.pop_size_min:\n            self.pop_size = max(self.pop_size_min, int(self.pop_size * 0.9))  # Reduce population size\n            self.swarm = self.swarm[:self.pop_size]\n            self.velocities = self.velocities[:self.pop_size]\n            self.local_best_positions = self.local_best_positions[:self.pop_size]\n            self.local_best_fitness = self.local_best_fitness[:self.pop_size]\n        elif np.std(self.local_best_fitness) > 0.1 and self.pop_size < self.pop_size_max:\n            self.pop_size = min(self.pop_size_max, int(self.pop_size * 1.1))\n\n    def __call__(self, func):\n        self.initialize_swarm(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                self.update_velocity(i)\n                new_position = self.update_position(i, func)\n                \n                fitness = func(new_position)\n                self.budget -= 1\n\n                if fitness < self.f_opt:\n                    self.f_opt = fitness\n                    self.x_opt = new_position.copy()\n\n                if fitness < self.local_best_fitness[i]:\n                    self.local_best_fitness[i] = fitness\n                    self.local_best_positions[i] = new_position.copy()\n\n                    if fitness < self.global_best_fitness:\n                        self.global_best_fitness = fitness\n                        self.global_best_position = new_position.copy()\n                        \n            self.scout(func) # Scout for new regions\n            self.adjust_population_size()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm SelfOrganizingScoutPSO scored 0.277 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d7579762-3ff0-46ce-b447-539068fbbfd5"], "operator": null, "metadata": {"aucs": [0.11874805348897954, 0.16449677255051498, 0.2828827975393988, 0.22946951360109336, 0.20895907257067314, 0.24721389595481935, 0.23979996397047754, 0.19489955078658117, 0.2038515691155297, 0.1630099574644499, 0.1958964751606591, 0.9984737968735413, 0.2512710236652953, 0.20283854743122476, 0.4611174514399742, 0.26161098671739735, 0.2585043775491156, 0.24811335422029202, 0.15897494885407504, 0.4565530126671865]}}
{"id": "c0662e45-9654-48b9-a311-f6140bdf8e50", "fitness": 0.44243185514848504, "name": "BanditAdaptiveDE", "description": "A self-adjusting differential evolution algorithm that dynamically tunes its parameters based on the success rate of different parameter configurations using a multi-armed bandit approach.", "code": "import numpy as np\n\nclass BanditAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, F_values=[0.1, 0.5, 0.9], CR_values=[0.1, 0.5, 0.9]):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F_values = F_values\n        self.CR_values = CR_values\n        self.F_rewards = {F: 0.0 for F in F_values}\n        self.CR_rewards = {CR: 0.0 for CR in CR_values}\n        self.F_counts = {F: 0 for F in F_values}\n        self.CR_counts = {CR: 0 for CR in CR_values}\n        self.archive = []\n        self.p = 0.1 \n        self.epsilon = 0.1 # Exploration rate for the bandit\n\n    def select_parameter(self, rewards, counts, values):\n        if np.random.rand() < self.epsilon:\n            return np.random.choice(values)\n        else:\n            avg_rewards = {v: rewards[v] / (counts[v] + 1e-6) for v in values}\n            return max(avg_rewards, key=avg_rewards.get)\n            \n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        for i in range(self.pop_size):\n            if fitness[i] < self.f_opt:\n                self.f_opt = fitness[i]\n                self.x_opt = self.population[i]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Bandit-based parameter selection\n                F = self.select_parameter(self.F_rewards, self.F_counts, self.F_values)\n                CR = self.select_parameter(self.CR_rewards, self.CR_counts, self.CR_values)\n                \n\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                mutant = x_1 + F * (x_2 - x_3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                crossover = np.random.uniform(size=self.dim) < CR\n                trial = np.where(crossover, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n                    \n                if (fitness[i] < 0 and f_trial < 0) or np.random.rand() < self.p:\n                    if f_trial < fitness[i]:\n                        reward = fitness[i] - f_trial\n                        \n                        # Update Bandit Rewards\n                        self.F_rewards[F] += reward\n                        self.CR_rewards[CR] += reward\n                        \n                        # Update Bandit Counts\n                        self.F_counts[F] += 1\n                        self.CR_counts[CR] += 1\n\n                        fitness[i] = f_trial\n                        self.population[i] = trial\n                        \n                        if len(self.archive) < self.archive_size:\n                            self.archive.append(self.population[i].copy())\n                        else:\n                            idx_to_replace = np.random.randint(0, self.archive_size)\n                            self.archive[idx_to_replace] = self.population[i].copy()\n\n                else:\n                    if f_trial < fitness[i]:\n                        fitness[i] = f_trial\n                        self.population[i] = trial\n\n                        if len(self.archive) < self.archive_size:\n                            self.archive.append(self.population[i].copy())\n                        else:\n                            idx_to_replace = np.random.randint(0, self.archive_size)\n                            self.archive[idx_to_replace] = self.population[i].copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm BanditAdaptiveDE scored 0.442 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d7579762-3ff0-46ce-b447-539068fbbfd5"], "operator": null, "metadata": {"aucs": [0.22651890246746476, 0.2954634276615593, 0.31822617728465474, 0.227667564131671, 0.3165291341597496, 0.5038392606181539, 0.32994320039076297, 0.4938128174850629, 0.42694708534103065, 0.6319090016434428, 0.2989279545223372, 0.9965258471394851, 0.31046340880273937, 0.3092323799268091, 0.7667228315796546, 0.5513140449742269, 0.6000993144597815, 0.4941350542374966, 0.23388166176330027, 0.5164780343803187]}}
{"id": "ab5619df-0f27-436b-a61e-757c243f1076", "fitness": 0.5385000659331107, "name": "AdaptiveDEMix", "description": "An adaptive differential evolution algorithm with a self-adaptive mutation strategy using a mixture of different mutation operators and a Cauchy distribution for exploration.", "code": "import numpy as np\n\nclass AdaptiveDEMix:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = 0.5\n        self.CR = 0.9\n        self.archive = []\n        self.success_F = []\n        self.success_CR = []\n        self.success_delta_f = []\n        self.p = 0.1 # probability for stochastic ranking\n        self.mutation_probs = np.array([0.3, 0.3, 0.4])  # Probabilities for each mutation operator\n        self.mutation_options = ['current_to_rand_1', 'rand_1', 'current_to_best_1']\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        \n        for i in range(self.pop_size):\n            if fitness[i] < self.f_opt:\n                self.f_opt = fitness[i]\n                self.x_opt = self.population[i]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation Strategy Selection\n                mutation_type = np.random.choice(self.mutation_options, p=self.mutation_probs)\n\n                if mutation_type == 'current_to_rand_1':\n                    idxs = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[idxs]\n                    mutant = self.population[i] + self.F * (x_r1 - x_r2) + self.F * (self.population[i] - x_r3)\n\n                elif mutation_type == 'rand_1':\n                    idxs = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[idxs]\n                    mutant = x_r1 + self.F * (x_r2 - x_r3)\n\n                elif mutation_type == 'current_to_best_1':\n                    best_idx = np.argmin(fitness)\n                    x_best = self.population[best_idx]\n                    idxs = np.random.choice(self.pop_size, 2, replace=False)\n                    x_r1, x_r2 = self.population[idxs]\n                    mutant = self.population[i] + self.F * (x_best - self.population[i]) + self.F * (x_r1 - x_r2)\n\n                # Cauchy Mutation\n                mutant += 0.01 * np.random.standard_cauchy(size=self.dim)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                crossover = np.random.uniform(size=self.dim) < self.CR\n                trial = np.where(crossover, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n                \n                # Stochastic Ranking\n                if (fitness[i] < 0 and f_trial < 0) or np.random.rand() < self.p:\n                    if f_trial < fitness[i]:\n                        self.success_F.append(self.F)\n                        self.success_CR.append(self.CR)\n                        self.success_delta_f.append(np.abs(f_trial - fitness[i]))\n                        \n                        fitness[i] = f_trial\n                        self.population[i] = trial\n                        \n                        if len(self.archive) < self.archive_size:\n                            self.archive.append(self.population[i].copy())\n                        else:\n                            idx_to_replace = np.random.randint(0, self.archive_size)\n                            self.archive[idx_to_replace] = self.population[i].copy()\n                else:\n                     if f_trial < fitness[i]:\n                        fitness[i] = f_trial\n                        self.population[i] = trial\n                        \n                        if len(self.archive) < self.archive_size:\n                            self.archive.append(self.population[i].copy())\n                        else:\n                            idx_to_replace = np.random.randint(0, self.archive_size)\n                            self.archive[idx_to_replace] = self.population[i].copy()\n\n            # Parameter Adaptation\n            if self.success_F:\n                self.F = np.mean(self.success_F)\n                self.CR = np.mean(self.success_CR)\n\n                # Reset success history\n                self.success_F = []\n                self.success_CR = []\n                self.success_delta_f = []\n            \n            self.F = np.clip(self.F, 0.1, 1.0)\n            self.CR = np.clip(self.CR, 0.1, 1.0)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDEMix scored 0.539 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["541bb1bc-1380-4491-b822-168b81deabcc"], "operator": null, "metadata": {"aucs": [0.223884556761895, 0.4870015136822494, 0.485906053450451, 0.8600479886780138, 0.5028804397908219, 0.5619430738927869, 0.33932199925810025, 0.4342153251282642, 0.4616452454185611, 0.39499116209933427, 0.8368931000779849, 0.991377264335656, 0.36617810584125143, 0.47698384170666, 0.9134140128348603, 0.565560950544723, 0.43559385811136264, 0.6772033796464814, 0.2409992195002485, 0.5139602279025106]}}
{"id": "7a9ec00d-11e3-4783-8853-797ee159537e", "fitness": 0.4825080372730426, "name": "AdaptiveNichingDE", "description": "A differential evolution strategy that adapts both mutation and crossover rates based on the success of previous generations, combined with a niching strategy to maintain population diversity and prevent premature convergence.", "code": "import numpy as np\n\nclass AdaptiveNichingDE:\n    def __init__(self, budget=10000, dim=10, pop_multiplier=5, initial_F=0.5, initial_CR=0.9, niche_radius=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = dim * pop_multiplier\n        self.F = initial_F  # Initial mutation factor\n        self.CR = initial_CR # Crossover rate\n        self.niche_radius = niche_radius # Radius of the niche\n        self.lb = -5.0\n        self.ub = 5.0\n        self.x_opt = None\n        self.f_opt = np.inf\n        self.F_history = []\n        self.CR_history = []\n\n\n    def __call__(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                if len(self.F_history) > 0:\n                    self.F = np.mean(self.F_history)\n                if len(self.CR_history) > 0:\n                    self.CR = np.mean(self.CR_history)\n                \n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[idxs]\n                mutant = self.population[i] + self.F * (x_r2 - x_r3)\n                mutant = np.clip(mutant, self.lb, self.ub)\n                \n                # Crossover\n                crossover = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                #Niche Comparison: only accept if it is better AND far from existing solutions within the niche\n                distance_to_others = np.linalg.norm(self.population - trial, axis=1)\n                nearby_indices = np.where(distance_to_others < self.niche_radius)[0]\n                \n                \n                if f_trial < self.fitness[i]:\n                    is_better_than_nearby = True\n                    for idx in nearby_indices:\n                         if f_trial >= self.fitness[idx]:\n                            is_better_than_nearby = False\n                            break #Only is_better_than_nearby when strictly better\n                    \n                    if is_better_than_nearby:\n                        \n                        if self.budget > 0:\n                            self.F_history.append(self.F)\n                            self.CR_history.append(self.CR)\n                        self.fitness[i] = f_trial\n                        self.population[i] = trial\n\n                        if f_trial < self.f_opt:\n                            self.f_opt = f_trial\n                            self.x_opt = trial\n                \n\n                if self.budget <= 0:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveNichingDE scored 0.483 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["707be948-3733-45ca-bec4-f4a213d00e5f"], "operator": null, "metadata": {"aucs": [0.1629824147003064, 0.4004558518415531, 0.44411408413860376, 0.6770398434221868, 0.39832404171973457, 0.47245257469552915, 0.3232986397345108, 0.38661583519126386, 0.38523683525455377, 0.25998232697859935, 0.8173109505383043, 0.9989786536846582, 0.5042405139753487, 0.3586823573992758, 0.8687488195756877, 0.5036193552829384, 0.3920865198654807, 0.5839410290232278, 0.21874982418213296, 0.4933002742569562]}}
{"id": "779780c0-4b1f-4f06-88bf-bfdcd5612ff5", "fitness": 0.39552987189350874, "name": "AdaptiveDESACMA", "description": "An adaptive Differential Evolution strategy with a self-adaptive covariance matrix adaptation (SACMA) using a weighted recombination and a population reduction strategy to improve convergence.", "code": "import numpy as np\n\nclass AdaptiveDESACMA:\n    def __init__(self, budget=10000, dim=10, pop_multiplier=5, initial_F=0.5, initial_CR=0.9, reduction_factor=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = dim * pop_multiplier\n        self.F = initial_F  # Initial mutation factor\n        self.CR = initial_CR # Crossover rate\n        self.mean = np.zeros(dim) # Mean of the population\n        self.C = np.eye(dim) # Covariance matrix\n        self.c_learn = 0.1 # Learning rate for covariance matrix\n        self.mu = self.pop_size // 4 # Number of individuals for updating CMA\n        self.restart_trigger = 100 # Number of iterations without improvement before restart\n        self.no_improvement_counter = 0\n        self.best_fitness_ever = np.inf\n        self.lb = -5.0\n        self.ub = 5.0\n        self.x_opt = None\n        self.f_opt = np.inf\n        self.reduction_factor = reduction_factor # Factor for population reduction\n        self.min_pop_size = 10 # Minimum population size\n\n\n    def __call__(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        while self.budget > 0:\n            # Sort population by fitness\n            sorted_indices = np.argsort(self.fitness)\n            self.population = self.population[sorted_indices]\n            self.fitness = self.fitness[sorted_indices]\n\n            if self.fitness[0] < self.best_fitness_ever:\n                self.best_fitness_ever = self.fitness[0]\n                self.x_opt = self.population[0]\n                self.f_opt = self.fitness[0]\n                self.no_improvement_counter = 0\n            else:\n                self.no_improvement_counter += 1\n\n            if self.no_improvement_counter > self.restart_trigger:\n                # Restart strategy\n                self.mean = np.zeros(self.dim)\n                self.C = np.eye(self.dim)\n                self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.budget -= self.pop_size\n                self.no_improvement_counter = 0\n                continue\n\n            # Adaptive F and CR\n            adaptive_F = np.random.normal(self.F, 0.1)\n            adaptive_CR = np.random.normal(self.CR, 0.1)\n            adaptive_F = np.clip(adaptive_F, 0.1, 1.0)\n            adaptive_CR = np.clip(adaptive_CR, 0.1, 1.0)\n\n            for i in range(self.pop_size):\n                # Mutation using SACMA\n                z = np.random.multivariate_normal(np.zeros(self.dim), self.C)\n                mutant = self.population[i] + adaptive_F * z\n                mutant = np.clip(mutant, self.lb, self.ub)\n                \n                # Weighted Recombination\n                weights = np.random.rand(self.dim)\n                weights /= np.sum(weights)\n                recombined = np.zeros(self.dim)\n                for j in range(self.dim):\n                    recombined[j] = weights[j] * mutant[j] + (1 - weights[j]) * self.population[i][j]\n\n\n                # Crossover\n                crossover = np.random.rand(self.dim) < adaptive_CR\n                trial = np.where(crossover, recombined, self.population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                if self.budget <= 0:\n                    break\n\n            # Update CMA\n            self.mean = np.mean(self.population[:self.mu], axis=0)\n            diff = self.population[:self.mu] - self.mean\n            self.C = (1 - self.c_learn) * self.C + self.c_learn * (1/self.mu) * np.sum([np.outer(diff[i], diff[i]) for i in range(self.mu)], axis=0)\n            # Ensure C is positive definite\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)\n\n            # Population reduction\n            if self.pop_size > self.min_pop_size:\n                new_pop_size = int(self.pop_size * self.reduction_factor)\n                new_pop_size = max(new_pop_size, self.min_pop_size)\n                if new_pop_size < self.pop_size:\n                    self.pop_size = new_pop_size\n                    sorted_indices = np.argsort(self.fitness)\n                    self.population = self.population[sorted_indices[:self.pop_size]]\n                    self.fitness = self.fitness[sorted_indices[:self.pop_size]]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDESACMA scored 0.396 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["707be948-3733-45ca-bec4-f4a213d00e5f"], "operator": null, "metadata": {"aucs": [0.1103216271110079, 0.2406269593499507, 0.4465714141241257, 0.49722193012927896, 0.20130073894970402, 0.37512463317644873, 0.26698742598269054, 0.3737058777320975, 0.24601751552160211, 0.17073040665595507, 0.6883866094130175, 0.9216267926846107, 0.2958697831565704, 0.22830046496727596, 0.7083131481051206, 0.6001001321763852, 0.2868737059366985, 0.6193672372341605, 0.17204725921340636, 0.46110377625006727]}}
{"id": "c73bd602-9e66-43d1-a10e-144de6209ef0", "fitness": 0.4944978671538302, "name": "SOMAdaptiveDE", "description": "An adaptive differential evolution algorithm that uses a self-organizing map (SOM) to map solutions onto a grid and adjusts mutation based on neighborhood diversity.", "code": "import numpy as np\n\nclass SOMAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, som_grid_size=10, learning_rate=0.1, sigma=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.som_grid_size = som_grid_size  # Size of the SOM grid (som_grid_size x som_grid_size)\n        self.learning_rate = learning_rate  # SOM learning rate\n        self.sigma = sigma  # SOM neighborhood radius\n        self.som = np.random.rand(som_grid_size, som_grid_size, dim)  # Initialize SOM weights randomly\n        self.F = 0.5\n        self.CR = 0.9\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        for i in range(self.pop_size):\n            if fitness[i] < self.f_opt:\n                self.f_opt = fitness[i]\n                self.x_opt = self.population[i]\n\n        while self.budget > 0:\n            # SOM Training Step\n            for x in self.population:\n                self.train_som(x)\n\n            for i in range(self.pop_size):\n                # Find Best Matching Unit (BMU) in SOM for current individual\n                bmu_row, bmu_col = self.find_bmu(self.population[i])\n\n                # Calculate neighborhood diversity around the BMU\n                diversity = self.calculate_neighborhood_diversity(bmu_row, bmu_col)\n\n                # Adjust F and CR based on neighborhood diversity\n                self.F = 0.1 + 0.8 * diversity  # F ranges from 0.1 to 0.9\n                self.CR = 0.1 + 0.8 * (1 - diversity) # CR ranges from 0.1 to 0.9 (inverse relation)\n\n\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                mutant = x_1 + self.F * (x_2 - x_3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                crossover = np.random.uniform(size=self.dim) < self.CR\n                trial = np.where(crossover, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    self.population[i] = trial\n\n        return self.f_opt, self.x_opt\n\n    def train_som(self, x):\n        # Find Best Matching Unit (BMU)\n        bmu_row, bmu_col = self.find_bmu(x)\n\n        # Update SOM weights in the neighborhood of the BMU\n        for row in range(self.som_grid_size):\n            for col in range(self.som_grid_size):\n                distance = np.sqrt((row - bmu_row)**2 + (col - bmu_col)**2)\n                if distance <= self.sigma:\n                    influence = np.exp(-distance**2 / (2 * self.sigma**2))\n                    self.som[row, col] += self.learning_rate * influence * (x - self.som[row, col])\n\n    def find_bmu(self, x):\n        # Find the Best Matching Unit (BMU) for a given input vector x\n        min_dist = np.inf\n        bmu_row, bmu_col = -1, -1\n        for row in range(self.som_grid_size):\n            for col in range(self.som_grid_size):\n                dist = np.linalg.norm(x - self.som[row, col])\n                if dist < min_dist:\n                    min_dist = dist\n                    bmu_row, bmu_col = row, col\n        return bmu_row, bmu_col\n\n    def calculate_neighborhood_diversity(self, row, col):\n        # Calculate the average Euclidean distance between SOM weights in the neighborhood\n        distances = []\n        for i in range(max(0, row - 1), min(self.som_grid_size, row + 2)):\n            for j in range(max(0, col - 1), min(self.som_grid_size, col + 2)):\n                if i != row or j != col:\n                    distances.append(np.linalg.norm(self.som[row, col] - self.som[i, j]))\n        if distances:\n            return np.mean(distances) / np.linalg.norm(self.som[row, col])  # Normalize by magnitude of BMU vector\n        else:\n            return 0.0  # If no neighbors, diversity is 0", "configspace": "", "generation": 2, "feedback": "The algorithm SOMAdaptiveDE scored 0.494 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d7579762-3ff0-46ce-b447-539068fbbfd5"], "operator": null, "metadata": {"aucs": [0.21271450718723683, 0.24528569668324507, 0.4172387102430285, 0.8142895591707409, 0.4474306191028803, 0.6982417131503004, 0.4068084816971884, 0.394377761393383, 0.4831565325421149, 0.31792962762821153, 0.47831425112225145, 0.986763291582685, 0.26263572664671087, 0.5389280857642266, 0.7071829627975352, 0.5534352610087278, 0.48865658650161037, 0.7067337317625046, 0.20054748806582068, 0.5292867490262018]}}
{"id": "6990cd9e-078c-4214-b3a1-fd55a531452e", "fitness": -Infinity, "name": "MirroredAdaptiveDE", "description": "A differential evolution strategy employing a mirrored sampling technique to enhance exploration and avoid boundary violations, combined with adaptive parameter control based on the success history of previous generations.", "code": "import numpy as np\n\nclass MirroredAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, memory_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.memory_size = memory_size\n        self.F = 0.5\n        self.CR = 0.9\n        self.F_memory = np.ones(memory_size) * 0.5\n        self.CR_memory = np.ones(memory_size) * 0.9\n        self.memory_idx = 0\n        self.success_F = []\n        self.success_CR = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        for i in range(self.pop_size):\n            if fitness[i] < self.f_opt:\n                self.f_opt = fitness[i]\n                self.x_opt = self.population[i]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Parameter adaptation\n                self.F = self.F_memory[self.memory_idx]\n                self.CR = self.CR_memory[self.memory_idx]\n\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                mutant = x_1 + self.F * (x_2 - x_3)\n\n                # Mirrored sampling to handle boundary violations\n                for j in range(self.dim):\n                    if mutant[j] < lb:\n                        mutant[j] = lb + np.abs(mutant[j] - lb)\n                    if mutant[j] > ub:\n                        mutant[j] = ub - np.abs(mutant[j] - ub)\n                        \n                # Crossover\n                crossover = np.random.uniform(size=self.dim) < self.CR\n                trial = np.where(crossover, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                if f_trial < fitness[i]:\n                    self.success_F.append(self.F)\n                    self.success_CR.append(self.CR)\n                    fitness[i] = f_trial\n                    self.population[i] = trial\n            \n            # Update memory\n            if self.success_F:\n                self.F_memory[self.memory_idx] = np.mean(self.success_F)\n                self.CR_memory[self.memory_idx] = np.mean(self.success_CR)\n            self.success_F = []\n            self.success_CR = []\n            self.memory_idx = (self.memory_idx + 1) % self.memory_size\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 44, in __call__\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n.", "error": "", "parent_ids": ["c73bd602-9e66-43d1-a10e-144de6209ef0"], "operator": null, "metadata": {}}
{"id": "cc4edc86-6b0e-4c3f-af48-b5364230e421", "fitness": -Infinity, "name": "GPSurrogateDE", "description": "A hybrid algorithm combining differential evolution with a Gaussian process surrogate model for efficient exploration and exploitation.", "code": "import numpy as np\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n\nclass GPSurrogateDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, n_initial_samples=10, exploration_weight=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.n_initial_samples = n_initial_samples\n        self.exploration_weight = exploration_weight\n        self.X = None\n        self.y = None\n        self.gpr = GaussianProcessRegressor(kernel=C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2)),\n                                             n_restarts_optimizer=10, alpha=1e-5)\n        self.F = 0.5\n        self.CR = 0.7\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n        # Initial sampling\n        X_initial = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.n_initial_samples, self.dim))\n        y_initial = np.array([func(x) for x in X_initial])\n        self.budget -= self.n_initial_samples\n        self.X = X_initial\n        self.y = y_initial\n\n        best_index = np.argmin(self.y)\n        self.f_opt = self.y[best_index]\n        self.x_opt = self.X[best_index]\n\n\n        # DE population initialization\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        for i in range(self.pop_size):\n            if fitness[i] < self.f_opt:\n                self.f_opt = fitness[i]\n                self.x_opt = population[i]\n\n        while self.budget > 0:\n            # Train GP model\n            self.gpr.fit(self.X, self.y)\n\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = population[idxs]\n                mutant = x_1 + self.F * (x_2 - x_3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                crossover = np.random.uniform(size=self.dim) < self.CR\n                trial = np.where(crossover, mutant, population[i])\n\n                # Evaluate trial point using GP surrogate and exploration bonus\n                f_trial_pred, sigma = self.gpr.predict(trial.reshape(1, -1), return_std=True)\n                f_trial_gp = f_trial_pred[0] - self.exploration_weight * sigma[0]  # Exploration bonus\n\n                # Evaluate trial point with the actual function with a small probability\n                if np.random.rand() < 0.1 or self.budget < 50: # Explore more in the end\n                    f_trial = func(trial)\n                    self.budget -= 1\n\n                    self.X = np.vstack((self.X, trial))\n                    self.y = np.append(self.y, f_trial)\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                    \n                    if f_trial < fitness[i]:\n                        fitness[i] = f_trial\n                        population[i] = trial\n                else:\n                     f_trial = f_trial_gp\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n\n            # Update best solution based on GP predictions (exploitation)\n            fitness_pred, _ = self.gpr.predict(population, return_std=True)\n            best_index = np.argmin(fitness_pred)\n            if fitness_pred[best_index] < self.f_opt:\n                self.x_opt = population[best_index]\n                \n                #Evaluate the real function for the predicted optimum\n                f_opt_real = func(self.x_opt)\n                self.budget -= 1\n\n                self.X = np.vstack((self.X, self.x_opt))\n                self.y = np.append(self.y, f_opt_real)\n\n                self.f_opt = f_opt_real\n                \n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 120, in evaluate\n    algorithm = local_env[algorithm_name](budget=100, dim=2)\n  File \"<string>\", line 14, in __init__\nNameError: name 'GaussianProcessRegressor' is not defined\n.", "error": "", "parent_ids": ["c73bd602-9e66-43d1-a10e-144de6209ef0"], "operator": null, "metadata": {}}
{"id": "0b3f60e3-b2c2-4f2a-9213-3df29f34a801", "fitness": 0.0, "name": "SOMAdaptiveDELocalSearch", "description": "An adaptive differential evolution algorithm that uses a simplified self-organizing map (SOM) to adjust mutation parameters based on the distance to the closest SOM node and introduces a local search step around the best solution found so far.", "code": "import numpy as np\n\nclass SOMAdaptiveDELocalSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=50, som_size=10, learning_rate=0.1, local_search_radius=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.som_size = som_size\n        self.learning_rate = learning_rate\n        self.local_search_radius = local_search_radius\n        self.som = np.random.uniform(-5.0, 5.0, size=(self.som_size, self.dim))  # Simplified SOM initialization\n        self.F = 0.5\n        self.CR = 0.9\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        best_idx = np.argmin(fitness)\n        if fitness[best_idx] < self.f_opt:\n            self.f_opt = fitness[best_idx]\n            self.x_opt = self.population[best_idx]\n\n        while self.budget > 0:\n            # SOM Training\n            for x in self.population:\n                closest_node_idx = np.argmin(np.linalg.norm(self.som - x, axis=1))\n                self.som[closest_node_idx] += self.learning_rate * (x - self.som[closest_node_idx])\n\n            for i in range(self.pop_size):\n                # Parameter Adaptation based on SOM distance\n                closest_node_idx = np.argmin(np.linalg.norm(self.som - self.population[i], axis=1))\n                distance = np.linalg.norm(self.som[closest_node_idx] - self.population[i])\n                self.F = 0.1 + 0.8 * np.exp(-distance)  # F decreases with distance\n                self.CR = 0.1 + 0.8 * (1 - np.exp(-distance)) # CR increases with distance\n\n\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                mutant = x_1 + self.F * (x_2 - x_3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                crossover = np.random.uniform(size=self.dim) < self.CR\n                trial = np.where(crossover, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    self.population[i] = trial\n            \n            # Local Search around best solution\n            if self.budget > 0:\n                x_local = self.x_opt + np.random.uniform(-self.local_search_radius, self.local_search_radius, size=self.dim)\n                x_local = np.clip(x_local, func.bounds.lb, func.bounds.ub)\n                f_local = func(x_local)\n                self.budget -= 1\n                if f_local < self.f_opt:\n                    self.f_opt = f_local\n                    self.x_opt = x_local\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm SOMAdaptiveDELocalSearch scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c73bd602-9e66-43d1-a10e-144de6209ef0"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "7663fca4-a3df-4480-81e8-0e8e600c905a", "fitness": -Infinity, "name": "MultiStrategyAdaptiveDE", "description": "A differential evolution algorithm that utilizes a pool of mutation strategies, adaptively selecting them based on their historical success using a credit assignment mechanism inspired by reinforcement learning.", "code": "import numpy as np\n\nclass MultiStrategyAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10,\n                 mutation_strategies=[\n                     lambda F, x1, x2, x3: x1 + F * (x2 - x3),  # DE/rand/1\n                     lambda F, x1, x2, x3: x1 + F * (x2 - x3) + F * (np.random.rand(dim) - np.random.rand(dim)),  # DE/rand/1 with exploration\n                     lambda F, x1, x2, x3, x_best: x_best + F * (x1 - x2),  # DE/best/1\n                     lambda F, x1, x2, x3, x_best: x_best + F * (x1 - x2) + F * (np.random.rand(dim) - np.random.rand(dim)),  # DE/best/1 with exploration\n                 ],\n                 F_values=[0.1, 0.5, 0.9], CR_values=[0.1, 0.5, 0.9]):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.mutation_strategies = mutation_strategies\n        self.F_values = F_values\n        self.CR_values = CR_values\n        self.strategy_rewards = [0.0] * len(mutation_strategies)\n        self.strategy_counts = [0] * len(mutation_strategies)\n        self.archive = []\n        self.p = 0.1\n        self.epsilon = 0.1  # Exploration rate for strategy selection\n\n    def select_strategy(self):\n        if np.random.rand() < self.epsilon:\n            return np.random.randint(len(self.mutation_strategies))\n        else:\n            avg_rewards = [self.strategy_rewards[i] / (self.strategy_counts[i] + 1e-6) for i in range(len(self.mutation_strategies))]\n            return np.argmax(avg_rewards)\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        for i in range(self.pop_size):\n            if fitness[i] < self.f_opt:\n                self.f_opt = fitness[i]\n                self.x_opt = self.population[i]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Strategy Selection\n                strategy_index = self.select_strategy()\n                mutation_strategy = self.mutation_strategies[strategy_index]\n\n                # Parameter Selection\n                F = np.random.choice(self.F_values)\n                CR = np.random.choice(self.CR_values)\n\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                x_best = self.population[np.argmin(fitness)] if len(self.population) > 0 else x_1 #fallback\n\n                if len(self.mutation_strategies[strategy_index].__code__.co_varnames) > 3:\n                    mutant = mutation_strategy(F, x_1, x_2, x_3, x_best)\n                else:\n                    mutant = mutation_strategy(F, x_1, x_2, x_3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                crossover = np.random.uniform(size=self.dim) < CR\n                trial = np.where(crossover, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                if (fitness[i] < 0 and f_trial < 0) or np.random.rand() < self.p:\n                    if f_trial < fitness[i]:\n                        reward = fitness[i] - f_trial\n\n                        # Update Strategy Rewards\n                        self.strategy_rewards[strategy_index] += reward\n\n                        # Update Strategy Counts\n                        self.strategy_counts[strategy_index] += 1\n\n                        fitness[i] = f_trial\n                        self.population[i] = trial\n\n                        if len(self.archive) < self.archive_size:\n                            self.archive.append(self.population[i].copy())\n                        else:\n                            idx_to_replace = np.random.randint(0, self.archive_size)\n                            self.archive[idx_to_replace] = self.population[i].copy()\n                else:\n                     if f_trial < fitness[i]:\n                        fitness[i] = f_trial\n                        self.population[i] = trial\n\n                        if len(self.archive) < self.archive_size:\n                            self.archive.append(self.population[i].copy())\n                        else:\n                            idx_to_replace = np.random.randint(0, self.archive_size)\n                            self.archive[idx_to_replace] = self.population[i].copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 60, in __call__\nTypeError: MultiStrategyAdaptiveDE.<lambda>() takes 4 positional arguments but 5 were given\n.", "error": "", "parent_ids": ["c0662e45-9654-48b9-a311-f6140bdf8e50"], "operator": null, "metadata": {}}
{"id": "2d644cb7-76ad-49e1-acda-7e3e439e8869", "fitness": -Infinity, "name": "DECCMA", "description": "Population-based algorithm using a combination of differential evolution, covariance matrix adaptation, and a restart mechanism to escape local optima.", "code": "import numpy as np\n\nclass DECCMA:\n    def __init__(self, budget=10000, dim=10, pop_size=None, restart_trigger=1e-8):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.restart_trigger = restart_trigger\n        self.mu = self.pop_size // 2  # Number of parents\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1/(4*self.dim) + 1/(21*self.dim**2))\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        self.sigma = 0.3\n        self.mean = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.archive = []\n        self.archive_size = 10\n\n    def __call__(self, func):\n        self.mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        \n        while self.budget > 0:\n            # Generate population\n            z = np.random.randn(self.dim, self.pop_size)\n            y = self.sigma * np.dot(np.linalg.cholesky(self.C), z)\n            x = self.mean[:, np.newaxis] + y\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            \n            fitness = np.array([func(xi) for xi in x.T])\n            self.budget -= self.pop_size\n\n            # Sort by fitness\n            idx = np.argsort(fitness)\n            fitness = fitness[idx]\n            x = x[:, idx]\n\n            # Update optimal solution\n            if fitness[0] < self.f_opt:\n                self.f_opt = fitness[0]\n                self.x_opt = x[:, 0].copy()\n\n            # Update mean\n            y_mean = np.sum(self.weights * y[:, :self.mu], axis=1)\n            self.mean += self.cs * self.ps\n            self.mean = np.clip(self.mean, func.bounds.lb, func.bounds.ub)\n\n            # Update evolution path\n            ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * y_mean / self.sigma\n            hsig = np.sum(ps**2) / (1 - (1 - self.cs)**(2 * self.budget / self.pop_size)) / self.dim < 2 + 4 / (self.dim + 1)\n            self.ps = ps\n            \n            # Update covariance matrix\n            pc = (1 - 1) * self.pc + hsig * np.sqrt(1 * (2 - 1) * self.mueff) * (self.mean - self.mean) / self.sigma\n            self.pc = pc\n\n            delta = (1 - self.c1 - self.cmu)\n            C1 = (self.c1 * (np.outer(ps, ps) + delta * self.C))\n            C2 = (self.cmu * np.sum(self.weights * (y[:, :self.mu] * y[:, :self.mu].T), axis=1))\n            self.C = delta * self.C + self.c1 * (np.outer(ps, ps) + delta * self.C) + self.cmu * np.sum([w * np.outer(y[:, i], y[:, i]) for i, w in enumerate(self.weights)], axis=0)\n            \n            # Update step size\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            self.sigma = np.clip(self.sigma, 1e-10, 10)\n\n            # Restart mechanism\n            if np.max(np.diag(self.C)) > 1e10 or self.sigma < self.restart_trigger:\n                self.C = np.eye(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.sigma = 0.3\n                self.mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 36, in __call__\n  File \"<__array_function__ internals>\", line 200, in clip\n  File \"/home/neocortex/.cache/pypoetry/virtualenvs/blade-xXF9vABH-py3.10/lib/python3.10/site-packages/numpy/core/fromnumeric.py\", line 2180, in clip\n    return _wrapfunc(a, 'clip', a_min, a_max, out=out, **kwargs)\n  File \"/home/neocortex/.cache/pypoetry/virtualenvs/blade-xXF9vABH-py3.10/lib/python3.10/site-packages/numpy/core/fromnumeric.py\", line 57, in _wrapfunc\n    return bound(*args, **kwds)\n  File \"/home/neocortex/.cache/pypoetry/virtualenvs/blade-xXF9vABH-py3.10/lib/python3.10/site-packages/numpy/core/_methods.py\", line 161, in _clip\n    return _clip_dep_invoke_with_casting(\n  File \"/home/neocortex/.cache/pypoetry/virtualenvs/blade-xXF9vABH-py3.10/lib/python3.10/site-packages/numpy/core/_methods.py\", line 115, in _clip_dep_invoke_with_casting\n    return ufunc(*args, out=out, **kwargs)\nValueError: operands could not be broadcast together with shapes (2,6) (2,) (2,) \n.", "error": "", "parent_ids": ["ab5619df-0f27-436b-a61e-757c243f1076"], "operator": null, "metadata": {}}
{"id": "b42a8b77-fd0e-4953-8071-744c4430d15d", "fitness": 0.0, "name": "MultiPopAdaptiveDE", "description": "A differential evolution strategy with multiple interacting populations, migration between them, and adaptive parameter control based on population performance.", "code": "import numpy as np\n\nclass MultiPopAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, num_pops=3, pop_size=30, migration_interval=50, migration_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.num_pops = num_pops\n        self.pop_size = pop_size\n        self.migration_interval = migration_interval\n        self.migration_rate = migration_rate\n        self.populations = [np.random.uniform(-5, 5, size=(pop_size, dim)) for _ in range(num_pops)]\n        self.fitness = [np.zeros(pop_size) for _ in range(num_pops)]\n        self.F = [0.5] * num_pops\n        self.CR = [0.9] * num_pops\n        self.best_fitness = [np.inf] * num_pops\n        self.best_solutions = [None] * num_pops\n        self.overall_best_f = np.inf\n        self.overall_best_x = None\n        self.eval_counts = [0] * num_pops\n\n    def evaluate_population(self, func, pop_idx):\n        for i in range(self.pop_size):\n            if self.eval_counts[pop_idx] < self.budget:\n                self.fitness[pop_idx][i] = func(self.populations[pop_idx][i])\n                self.eval_counts[pop_idx] += 1\n                if self.fitness[pop_idx][i] < self.best_fitness[pop_idx]:\n                    self.best_fitness[pop_idx] = self.fitness[pop_idx][i]\n                    self.best_solutions[pop_idx] = self.populations[pop_idx][i].copy()\n                    if self.fitness[pop_idx][i] < self.overall_best_f:\n                        self.overall_best_f = self.fitness[pop_idx][i]\n                        self.overall_best_x = self.populations[pop_idx][i].copy()\n            else:\n                return False\n        return True\n\n    def evolve_population(self, func, pop_idx):\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.populations[pop_idx][idxs]\n            mutant = x_r1 + self.F[pop_idx] * (x_r2 - x_r3)\n            mutant = np.clip(mutant, -5, 5)\n            \n            crossover = np.random.uniform(size=self.dim) < self.CR[pop_idx]\n            trial = np.where(crossover, mutant, self.populations[pop_idx][i])\n\n            if self.eval_counts[pop_idx] < self.budget:\n                f_trial = func(trial)\n                self.eval_counts[pop_idx] += 1\n\n                if f_trial < self.overall_best_f:\n                    self.overall_best_f = f_trial\n                    self.overall_best_x = trial.copy()\n                    \n                if f_trial < self.fitness[pop_idx][i]:\n                    self.fitness[pop_idx][i] = f_trial\n                    self.populations[pop_idx][i] = trial.copy()\n                    if f_trial < self.best_fitness[pop_idx]:\n                        self.best_fitness[pop_idx] = f_trial\n                        self.best_solutions[pop_idx] = trial.copy()\n            else:\n                return False\n        return True\n\n    def migrate(self):\n        # Select two random populations\n        pop_indices = np.random.choice(self.num_pops, 2, replace=False)\n        pop_idx1, pop_idx2 = pop_indices[0], pop_indices[1]\n\n        # Select individuals to migrate\n        num_migrants = int(self.migration_rate * self.pop_size)\n        \n        # Replace worst individuals in pop_idx2 with best from pop_idx1\n        indices_to_replace = np.argsort(self.fitness[pop_idx2])[-num_migrants:]\n        sorted_indices_pop1 = np.argsort(self.fitness[pop_idx1])[:num_migrants]\n\n        for i in range(num_migrants):\n            self.populations[pop_idx2][indices_to_replace[i]] = self.populations[pop_idx1][sorted_indices_pop1[i]].copy()\n            self.fitness[pop_idx2][indices_to_replace[i]] = self.fitness[pop_idx1][sorted_indices_pop1[i]]\n        \n        # Update best fitness and solutions after migration\n        if np.min(self.fitness[pop_idx2]) < self.best_fitness[pop_idx2]:\n            best_idx = np.argmin(self.fitness[pop_idx2])\n            self.best_fitness[pop_idx2] = self.fitness[pop_idx2][best_idx]\n            self.best_solutions[pop_idx2] = self.populations[pop_idx2][best_idx].copy()\n\n\n    def adapt_parameters(self):\n        for i in range(self.num_pops):\n            # Simplified adaptation: adjust F and CR based on fitness improvement\n            improvement = self.best_fitness[i] - np.min(self.fitness[i])\n            if improvement < 0:\n                self.F[i] *= 0.9\n                self.CR[i] *= 1.1\n            else:\n                self.F[i] *= 1.1\n                self.CR[i] *= 0.9\n            self.F[i] = np.clip(self.F[i], 0.1, 1.0)\n            self.CR[i] = np.clip(self.CR[i], 0.1, 1.0)\n\n    def __call__(self, func):\n        # Initialize and evaluate populations\n        for i in range(self.num_pops):\n            if not self.evaluate_population(func, i):\n                return self.overall_best_f, self.overall_best_x\n\n        generation = 0\n        while min(self.eval_counts) < self.budget:\n            for i in range(self.num_pops):\n                if not self.evolve_population(func, i):\n                    return self.overall_best_f, self.overall_best_x\n\n            if generation % self.migration_interval == 0:\n                self.migrate()\n\n            self.adapt_parameters()\n            generation += 1\n\n        return self.overall_best_f, self.overall_best_x", "configspace": "", "generation": 3, "feedback": "The algorithm MultiPopAdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ab5619df-0f27-436b-a61e-757c243f1076"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "f0e55a70-292c-4a08-9a4f-8e6cd533fba8", "fitness": 0.16741515973021748, "name": "OrthogonalRestartDE", "description": "An adaptive differential evolution algorithm that employs a diversity-guided mutation strategy using orthogonal learning to enhance exploration in promising regions and uses a restart mechanism to escape local optima.", "code": "import numpy as np\n\nclass OrthogonalRestartDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, restart_trigger=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.restart_trigger = restart_trigger\n        self.F = 0.5\n        self.CR = 0.9\n        self.best_fitness_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        \n        for i in range(self.pop_size):\n            if fitness[i] < self.f_opt:\n                self.f_opt = fitness[i]\n                self.x_opt = self.population[i]\n        \n        self.best_fitness_history.append(self.f_opt)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Diversity-Guided Mutation\n                if np.std(fitness) > self.restart_trigger:  # High diversity, encourage exploration\n                    idxs = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[idxs]\n                    mutant = x_r1 + self.F * (x_r2 - x_r3)  # Classical DE mutation\n                else:  # Low diversity, orthogonal learning\n                    num_samples = min(self.dim, self.budget)  # Sample size for orthogonal design\n                    if num_samples <= 0:\n                       break\n                    orthogonal_basis = self.generate_orthogonal_array(num_samples)\n                    mutant = self.population[i].copy()\n                    for j in range(num_samples):\n                        pertubation = orthogonal_basis[j] * np.random.uniform(-self.F, self.F)\n                        mutant[j % self.dim] += pertubation\n                    mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n\n                # Crossover\n                crossover = np.random.uniform(size=self.dim) < self.CR\n                trial = np.where(crossover, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    self.population[i] = trial\n\n            # Restart Mechanism\n            if len(self.best_fitness_history) > 10 and np.std(self.best_fitness_history[-10:]) < self.restart_trigger:\n                # Stagnation detected, restart a portion of the population\n                num_to_restart = int(self.pop_size * 0.2)\n                idxs_to_restart = np.random.choice(self.pop_size, num_to_restart, replace=False)\n                self.population[idxs_to_restart] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(num_to_restart, self.dim))\n                new_fitness = np.array([func(x) for x in self.population[idxs_to_restart]])\n                self.budget -= num_to_restart\n                fitness[idxs_to_restart] = new_fitness\n                \n                for i in idxs_to_restart:\n                    if fitness[i] < self.f_opt:\n                        self.f_opt = fitness[i]\n                        self.x_opt = self.population[i]\n\n            self.best_fitness_history.append(self.f_opt)\n\n        return self.f_opt, self.x_opt\n\n    def generate_orthogonal_array(self, n):\n        # Simplified orthogonal array generation (Hadamard matrix based)\n        # Not a complete implementation for all n, but works for powers of 2\n        n = int(2**np.ceil(np.log2(n)))  # Pad to next power of 2\n        if n == 1:\n            return np.array([[1]])\n        H = np.array([[1]])\n        while H.shape[0] < n:\n            H = np.vstack((np.hstack((H, H)), np.hstack((H, -H))))\n        return H[:n]", "configspace": "", "generation": 3, "feedback": "The algorithm OrthogonalRestartDE scored 0.167 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ab5619df-0f27-436b-a61e-757c243f1076"], "operator": null, "metadata": {"aucs": [0.17712430572803928, 0.32512117346261316, 0]}}
{"id": "2bdaf5be-656a-411c-8b77-4e075d1ee9a7", "fitness": -Infinity, "name": "ReinforcedAdaptiveDE", "description": "An adaptive differential evolution algorithm using a pool of mutation strategies and a reinforcement learning agent to select the best strategy based on its historical performance.", "code": "import numpy as np\n\nclass ReinforcedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, mutation_strategies=['DE/rand/1', 'DE/best/1', 'DE/current-to-rand/1'], learning_rate=0.1, exploration_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.mutation_strategies = mutation_strategies\n        self.learning_rate = learning_rate\n        self.exploration_rate = exploration_rate\n        self.strategy_rewards = {strategy: 0.0 for strategy in mutation_strategies}\n        self.strategy_counts = {strategy: 0 for strategy in mutation_strategies}\n        self.F = 0.5\n        self.CR = 0.7\n\n    def apply_mutation(self, strategy, population, i, func_bounds):\n        if strategy == 'DE/rand/1':\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_1, x_2, x_3 = population[idxs]\n            mutant = x_1 + self.F * (x_2 - x_3)\n        elif strategy == 'DE/best/1':\n            best_idx = np.argmin([func(x) for x in population])\n            x_best = population[best_idx]\n            idxs = np.random.choice(self.pop_size, 2, replace=False)\n            x_1, x_2 = population[idxs]\n            mutant = x_best + self.F * (x_1 - x_2)\n        elif strategy == 'DE/current-to-rand/1':\n            x_current = population[i]\n            idxs = np.random.choice(self.pop_size, 2, replace=False)\n            x_1, x_2 = population[idxs]\n            mutant = x_current + self.F * (x_1 - x_2)\n        else:\n            raise ValueError(f\"Unknown mutation strategy: {strategy}\")\n\n        mutant = np.clip(mutant, func_bounds.lb, func_bounds.ub)\n        return mutant\n\n    def choose_strategy(self):\n        if np.random.rand() < self.exploration_rate:\n            return np.random.choice(self.mutation_strategies)\n        else:\n            avg_rewards = {s: self.strategy_rewards[s] / (self.strategy_counts[s] + 1e-6) for s in self.mutation_strategies}\n            return max(avg_rewards, key=avg_rewards.get)\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        for i in range(self.pop_size):\n            if fitness[i] < self.f_opt:\n                self.f_opt = fitness[i]\n                self.x_opt = self.population[i]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Strategy Selection\n                strategy = self.choose_strategy()\n\n                # Mutation\n                mutant = self.apply_mutation(strategy, self.population, i, func.bounds)\n\n                # Crossover\n                crossover = np.random.uniform(size=self.dim) < self.CR\n                trial = np.where(crossover, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                # Reinforcement Learning Update\n                reward = fitness[i] - f_trial  # Immediate reward\n                self.strategy_rewards[strategy] += self.learning_rate * reward\n                self.strategy_counts[strategy] += 1\n                \n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    self.population[i] = trial\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 63, in __call__\n  File \"<string>\", line 22, in apply_mutation\n  File \"<string>\", line 22, in <listcomp>\nNameError: name 'func' is not defined\n.", "error": "", "parent_ids": ["c0662e45-9654-48b9-a311-f6140bdf8e50"], "operator": null, "metadata": {}}
{"id": "2fd16c53-65f6-4f1d-8062-f7ee83d1b397", "fitness": 0.6562163894725861, "name": "DynamicPopulationDE", "description": "Implements a differential evolution strategy with a dynamically adjusted population size and a restart mechanism based on stagnation detection.", "code": "import numpy as np\n\nclass DynamicPopulationDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, min_pop_size=10, max_pop_size=100, stagnation_threshold=1000):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.min_pop_size = min_pop_size\n        self.max_pop_size = max_pop_size\n        self.F = 0.5\n        self.CR = 0.9\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        for i in range(self.pop_size):\n            if fitness[i] < self.f_opt:\n                self.f_opt = fitness[i]\n                self.x_opt = self.population[i]\n\n        self.best_fitness_history.append(self.f_opt)\n\n        while self.budget > 0:\n            # Dynamic Population Adjustment (simple heuristic)\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                 if np.std(self.best_fitness_history[-self.stagnation_threshold:]) < 1e-6:  # Check for stagnation\n                    self.pop_size = max(self.min_pop_size, int(self.pop_size * 0.75)) #Reduce population size\n                    self.stagnation_counter += 1\n                    if self.stagnation_counter > 3:\n                        self.restart_population(func) #Restart if still stagnating\n\n                 else:\n                    self.pop_size = min(self.max_pop_size, int(self.pop_size * 1.1)) #Increase Population size\n                    self.stagnation_counter = 0 #reset\n\n                 self.pop_size = np.clip(self.pop_size, self.min_pop_size, self.max_pop_size)\n\n            #DE Step\n            new_population = np.zeros((self.pop_size, self.dim))\n            new_fitness = np.zeros(self.pop_size)\n\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                mutant = x_1 + self.F * (x_2 - x_3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                crossover = np.random.uniform(size=self.dim) < self.CR\n                trial = np.where(crossover, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if self.budget <= 0:\n                  break\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                if f_trial < fitness[i]:\n                    new_fitness[i] = f_trial\n                    new_population[i] = trial\n                else:\n                    new_fitness[i] = fitness[i]\n                    new_population[i] = self.population[i]\n\n            if self.budget <= 0:\n                break\n\n            self.population = new_population\n            fitness = new_fitness\n\n            self.best_fitness_history.append(self.f_opt)\n\n\n        return self.f_opt, self.x_opt\n\n    def restart_population(self, func):\n        # Restart the population with new random individuals\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size  #Important: Update budget when restart population\n\n        for i in range(self.pop_size):\n            if fitness[i] < self.f_opt:\n                self.f_opt = fitness[i]\n                self.x_opt = self.population[i]", "configspace": "", "generation": 3, "feedback": "The algorithm DynamicPopulationDE scored 0.656 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c73bd602-9e66-43d1-a10e-144de6209ef0"], "operator": null, "metadata": {"aucs": [0.2545130268848277, 0.5808368361237034, 0.6214061408923148, 0.8371815003015828, 0.6999801403750684, 0.7555019641257269, 0.5607523900895321, 0.5801822487103037, 0.6655747280069968, 0.6380773369881294, 0.8198830778372064, 0.9964194271522315, 0.5828392140694573, 0.6965580343840088, 0.9024531860798394, 0.7453013812162893, 0.5196485798619312, 0.8032228746509105, 0.3397997782104164, 0.5241959234912474]}}
{"id": "e561c742-5957-43af-b3f4-3f1b51278912", "fitness": -Infinity, "name": "CMAES_Restart", "description": "Implements a Co-variance Matrix Adaptation Evolution Strategy (CMA-ES) with restart capabilities and adaptive population sizing based on landscape ruggedness detection.", "code": "import numpy as np\n\nclass CMAES_Restart:\n    def __init__(self, budget=10000, dim=10, pop_size=None, initial_sigma=0.5, restart_trigger=1e-12, adaptation_rate = 0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_sigma = initial_sigma\n        self.restart_trigger = restart_trigger\n        self.adaptation_rate = adaptation_rate\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(self.dim))  # Default population size\n\n        self.mean = None\n        self.sigma = None\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.chiN = None\n        self.eigen_updated = False\n        self.B = None\n        self.D = None\n\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.generation = 0\n\n    def initialize(self, func):\n        self.mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.sigma = self.initial_sigma\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - 1/(4*self.dim) + 1/(21*self.dim**2))\n        self.eigen_updated = False\n\n    def sample_population(self):\n        z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n        y = np.dot(z, np.transpose(self.B * self.D)) if self.eigen_updated else z * np.sqrt(np.diag(self.C))\n        x = self.mean + self.sigma * y\n        return x, z\n\n    def update_distribution(self, x, z, fitness):\n        idx = np.argsort(fitness)\n        x_sorted = x[idx]\n        z_sorted = z[idx]\n\n        weights = np.log(self.pop_size + 1) - np.log(np.arange(1, self.pop_size + 1))\n        weights = weights / np.sum(weights)\n\n        # Update mean\n        mean_old = self.mean.copy()\n        self.mean = np.sum(x_sorted[:self.pop_size] * weights[:, np.newaxis], axis=0)\n\n        # Update evolution path for covariance matrix\n        y = self.mean - mean_old\n        self.ps = (1 - self.adaptation_rate) * self.ps + np.sqrt(self.adaptation_rate * (2 - self.adaptation_rate)) * (np.dot(y, np.linalg.inv(self.C)) / self.sigma if self.eigen_updated else y / self.sigma) #np.linalg.solve\n        hsig = np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.adaptation_rate)**(2*(self.eval_count//self.pop_size))) < (1.4 + 2/(self.dim+1))*self.chiN\n        self.pc = (1 - self.adaptation_rate) * self.pc + hsig * np.sqrt(self.adaptation_rate * (2 - self.adaptation_rate)) * y / self.sigma\n\n        # Update covariance matrix\n        delta = (1-hsig) * self.adaptation_rate * (2-self.adaptation_rate)\n        self.C = (1 - self.adaptation_rate) * self.C + self.adaptation_rate * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :]) + delta * self.adaptation_rate * (2-self.adaptation_rate) * self.C\n        self.C = (1 - self.adaptation_rate) * self.C + self.adaptation_rate * np.sum(weights[:, np.newaxis, np.newaxis] * (y[:, np.newaxis] @ y[np.newaxis, :]), axis=0)\n\n        # Update step size\n        self.sigma *= np.exp((self.adaptation_rate/0.8) * (np.linalg.norm(self.ps)/self.chiN - 1))\n\n        # Eigen decomposition update\n        if self.eval_count // self.pop_size % (1 + int(30*self.dim/self.pop_size)) == 0: # Adaptive frequency\n             try:\n                  self.D, self.B = np.linalg.eig(self.C)\n                  self.D = np.sqrt(np.abs(self.D))\n                  self.eigen_updated = True\n             except np.linalg.LinAlgError:\n                  self.C = np.eye(self.dim)\n                  self.eigen_updated = False\n\n    def detect_stagnation(self):\n          return self.sigma < self.restart_trigger\n\n    def __call__(self, func):\n        self.initialize(func)\n        while self.budget > 0:\n            x, z = self.sample_population()\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            fitness = np.array([func(xi) for xi in x])\n            self.budget -= self.pop_size\n            self.eval_count += self.pop_size\n\n            if np.min(fitness) < self.f_opt:\n                self.f_opt = np.min(fitness)\n                self.x_opt = x[np.argmin(fitness)]\n\n            self.update_distribution(x, z, fitness)\n            self.generation += 1\n\n            if self.detect_stagnation():\n                self.initialize(func) # Restart\n                self.pop_size = min(self.pop_size + 10, 200)\n                print(f\"Restarting CMA-ES, new pop_size = {self.pop_size}\")\n\n            if self.budget <= 0:\n                  break\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 144, in evaluate\n    algorithm(f_new)\n  File \"<string>\", line 94, in __call__\n  File \"<string>\", line 56, in update_distribution\n  File \"<__array_function__ internals>\", line 200, in inv\n  File \"/home/neocortex/.cache/pypoetry/virtualenvs/blade-xXF9vABH-py3.10/lib/python3.10/site-packages/numpy/linalg/linalg.py\", line 538, in inv\n    ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj)\n  File \"/home/neocortex/.cache/pypoetry/virtualenvs/blade-xXF9vABH-py3.10/lib/python3.10/site-packages/numpy/linalg/linalg.py\", line 89, in _raise_linalgerror_singular\n    raise LinAlgError(\"Singular matrix\")\nnumpy.linalg.LinAlgError: Singular matrix\n.", "error": "", "parent_ids": ["c73bd602-9e66-43d1-a10e-144de6209ef0"], "operator": null, "metadata": {}}
{"id": "493bc13e-2c3f-4bd5-a5bf-98aea86c4591", "fitness": 0.5266735370085508, "name": "ArchivedDifferentialEvolution", "description": "A differential evolution algorithm that uses a separate archive to store promising solutions found during the search, enhancing exploration and exploitation.", "code": "import numpy as np\n\nclass ArchivedDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=25, F=0.5, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.CR = CR\n        self.archive = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        for i in range(self.pop_size):\n            if fitness[i] < self.f_opt:\n                self.f_opt = fitness[i]\n                self.x_opt = self.population[i]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n\n                # Add a random solution from the archive if the archive is not empty\n                if self.archive:\n                    x_4 = self.archive[np.random.choice(len(self.archive))]\n                    mutant = x_1 + self.F * (x_2 - x_3) + self.F * (x_4 - self.population[i]) # Adding a component from the archive\n                else:\n                    mutant = x_1 + self.F * (x_2 - x_3)\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                crossover = np.random.uniform(size=self.dim) < self.CR\n                trial = np.where(crossover, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                if f_trial < fitness[i]:\n                    # Replace individual in population\n                    fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    # Update archive: Add trial to archive if it's not already present\n                    if not any((trial == x).all() for x in self.archive):\n                        self.archive.append(trial)\n                        if len(self.archive) > self.archive_size:\n                            self.archive.pop(np.random.randint(0, len(self.archive)))  # Remove a random element if archive is full\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm ArchivedDifferentialEvolution scored 0.527 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c73bd602-9e66-43d1-a10e-144de6209ef0"], "operator": null, "metadata": {"aucs": [0.17713008767053562, 0.31307623724120937, 0.4858444226914159, 0.7739305200114074, 0.48905397429429365, 0.6433464828882213, 0.3499339488542641, 0.4648714685337736, 0.5251115576500924, 0.4111930797596077, 0.7391322831550761, 0.9948937644140818, 0.41787243600854185, 0.46678632713298096, 0.8715626378036085, 0.5762907604636385, 0.4118200672384382, 0.7287261973510893, 0.19068575989779424, 0.5022087271109452]}}
{"id": "ae9311ce-199b-4f66-a93d-1b3e918b660d", "fitness": 0.48272408105571507, "name": "ToroidalAdaptiveDE", "description": "A self-adaptive Differential Evolution algorithm that employs a toroidal population topology and dynamically adjusts mutation and crossover rates based on local neighborhood performance.", "code": "import numpy as np\n\nclass ToroidalAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, neighborhood_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.neighborhood_size = neighborhood_size\n        self.F = 0.5 * np.ones(pop_size)\n        self.CR = 0.9 * np.ones(pop_size)\n        self.population = None\n        self.fitness = None\n        self.best_fitness = np.inf\n        self.best_solution = None\n        self.success_F = [[] for _ in range(pop_size)]\n        self.success_CR = [[] for _ in range(pop_size)]\n        self.epsilon = 1e-6\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(self.fitness)\n        self.best_fitness = self.fitness[best_index]\n        self.best_solution = self.population[best_index].copy()\n\n    def toroidal_neighbors(self, index):\n        neighbors = []\n        for i in range(-self.neighborhood_size // 2, self.neighborhood_size // 2 + 1):\n            neighbor_index = (index + i) % self.pop_size\n            neighbors.append(neighbor_index)\n        return neighbors\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            neighbors = self.toroidal_neighbors(i)\n            neighbor_population = self.population[neighbors]\n            neighbor_fitness = self.fitness[neighbors]\n\n            # Mutation\n            idxs = np.random.choice(len(neighbors), 3, replace=False)\n            x_r1, x_r2, x_r3 = neighbor_population[idxs]\n            mutant = self.population[i] + self.F[i] * (x_r1 - x_r2)\n\n            mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            crossover_mask = np.random.rand(self.dim) < self.CR[i]\n            trial = np.where(crossover_mask, mutant, self.population[i])\n\n            # Evaluation\n            trial_fitness = func(trial)\n            self.budget -= 1\n\n            if trial_fitness < self.best_fitness:\n                self.best_fitness = trial_fitness\n                self.best_solution = trial.copy()\n            \n            # Selection and Parameter Adaptation\n            if trial_fitness < self.fitness[i]:\n                self.success_F[i].append(self.F[i])\n                self.success_CR[i].append(self.CR[i])\n                \n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n            \n            # Update F and CR based on neighborhood performance\n            if self.success_F[i]:\n                self.F[i] = np.mean(self.success_F[i])\n                self.CR[i] = np.mean(self.success_CR[i])\n            \n            self.F[i] = np.clip(self.F[i], 0.1, 1.0)\n            self.CR[i] = np.clip(self.CR[i], 0.1, 1.0)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            self.evolve(func)\n\n        return self.best_fitness, self.best_solution", "configspace": "", "generation": 3, "feedback": "The algorithm ToroidalAdaptiveDE scored 0.483 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ab5619df-0f27-436b-a61e-757c243f1076"], "operator": null, "metadata": {"aucs": [0.16383702610487105, 0.3898172513175412, 0.45584666279004804, 0.7330731124463392, 0.3548730974240072, 0.5852762312950842, 0.29545733966594734, 0.44419184011962565, 0.48811746596771954, 0.2188295100598706, 0.7129256079114058, 0.9903132666310539, 0.4411842055724202, 0.3534445580634432, 0.8094155187273997, 0.46717033472426484, 0.3818485174213113, 0.6711166901763571, 0.20945221501253786, 0.48829116968305286]}}
{"id": "c3c81fb9-d67c-4b5e-9f5c-899a69d88e60", "fitness": -Infinity, "name": "SelfOrganizingScoutBee", "description": "Implements a self-organizing scout bee algorithm with dynamic step size and neighborhood adaptation for exploration and exploitation.", "code": "import numpy as np\n\nclass SelfOrganizingScoutBee:\n    def __init__(self, budget=10000, dim=10, n_scouts=10, initial_step_size=0.5, min_step_size=0.01, neighborhood_size=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.n_scouts = n_scouts\n        self.step_size = initial_step_size\n        self.min_step_size = min_step_size\n        self.neighborhood_size = neighborhood_size\n        self.x_opt = None\n        self.f_opt = np.Inf\n\n    def __call__(self, func):\n        # Initialize scout bees randomly\n        scout_positions = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.n_scouts, self.dim))\n        scout_fitnesses = np.array([func(pos) for pos in scout_positions])\n        self.budget -= self.n_scouts\n\n        # Update global best\n        best_index = np.argmin(scout_fitnesses)\n        if scout_fitnesses[best_index] < self.f_opt:\n            self.f_opt = scout_fitnesses[best_index]\n            self.x_opt = scout_positions[best_index]\n\n        while self.budget > 0:\n            # Exploration phase: Each scout searches its neighborhood\n            for i in range(self.n_scouts):\n                # Generate a new position within the neighborhood\n                new_position = scout_positions[i] + np.random.uniform(-self.step_size, self.step_size, size=self.dim)\n                new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n\n                # Evaluate the new position\n                new_fitness = func(new_position)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                # Update scout position if better\n                if new_fitness < scout_fitnesses[i]:\n                    scout_fitnesses[i] = new_fitness\n                    scout_positions[i] = new_position\n\n                    # Update global best if necessary\n                    if new_fitness < self.f_opt:\n                        self.f_opt = new_fitness\n                        self.x_opt = new_position\n            if self.budget <= 0:\n                break\n\n            # Self-Organization: Adjust step size and neighborhood based on performance\n            fitness_improvement = self.f_opt - np.min(scout_fitnesses)\n\n            if fitness_improvement > 0:\n                # Reduce step size if progress is being made\n                self.step_size *= 0.9\n                self.step_size = max(self.step_size, self.min_step_size)\n            else:\n                # Increase step size if no progress\n                self.step_size *= 1.1\n                # Ensure step size doesn't become too large (avoid unbounded expansion)\n                self.step_size = min(self.step_size, (func.bounds.ub - func.bounds.lb) / 20.0) #max step size 0.5\n\n            #Adaptive Neighborhood size\n            self.neighborhood_size = 0.1 + 0.9 * (self.step_size / ((func.bounds.ub - func.bounds.lb)/2))  #maps step size to neighborhood\n\n            # Scout bee relocation: Replace worst performing scouts with new random scouts\n            worst_index = np.argmax(scout_fitnesses)\n            if np.random.rand() < 0.1: #10% probability\n                scout_positions[worst_index] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                scout_fitnesses[worst_index] = func(scout_positions[worst_index])\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                if scout_fitnesses[worst_index] < self.f_opt:\n                    self.f_opt = scout_fitnesses[worst_index]\n                    self.x_opt = scout_positions[worst_index]\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 62, in __call__\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n.", "error": "", "parent_ids": ["2fd16c53-65f6-4f1d-8062-f7ee83d1b397"], "operator": null, "metadata": {}}
{"id": "c3c81fb9-d67c-4b5e-9f5c-899a69d88e60", "fitness": -Infinity, "name": "SelfOrganizingScoutBee", "description": "Implements a self-organizing scout bee algorithm with dynamic step size and neighborhood adaptation for exploration and exploitation.", "code": "import numpy as np\n\nclass SelfOrganizingScoutBee:\n    def __init__(self, budget=10000, dim=10, n_scouts=10, initial_step_size=0.5, min_step_size=0.01, neighborhood_size=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.n_scouts = n_scouts\n        self.step_size = initial_step_size\n        self.min_step_size = min_step_size\n        self.neighborhood_size = neighborhood_size\n        self.x_opt = None\n        self.f_opt = np.Inf\n\n    def __call__(self, func):\n        # Initialize scout bees randomly\n        scout_positions = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.n_scouts, self.dim))\n        scout_fitnesses = np.array([func(pos) for pos in scout_positions])\n        self.budget -= self.n_scouts\n\n        # Update global best\n        best_index = np.argmin(scout_fitnesses)\n        if scout_fitnesses[best_index] < self.f_opt:\n            self.f_opt = scout_fitnesses[best_index]\n            self.x_opt = scout_positions[best_index]\n\n        while self.budget > 0:\n            # Exploration phase: Each scout searches its neighborhood\n            for i in range(self.n_scouts):\n                # Generate a new position within the neighborhood\n                new_position = scout_positions[i] + np.random.uniform(-self.step_size, self.step_size, size=self.dim)\n                new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n\n                # Evaluate the new position\n                new_fitness = func(new_position)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                # Update scout position if better\n                if new_fitness < scout_fitnesses[i]:\n                    scout_fitnesses[i] = new_fitness\n                    scout_positions[i] = new_position\n\n                    # Update global best if necessary\n                    if new_fitness < self.f_opt:\n                        self.f_opt = new_fitness\n                        self.x_opt = new_position\n            if self.budget <= 0:\n                break\n\n            # Self-Organization: Adjust step size and neighborhood based on performance\n            fitness_improvement = self.f_opt - np.min(scout_fitnesses)\n\n            if fitness_improvement > 0:\n                # Reduce step size if progress is being made\n                self.step_size *= 0.9\n                self.step_size = max(self.step_size, self.min_step_size)\n            else:\n                # Increase step size if no progress\n                self.step_size *= 1.1\n                # Ensure step size doesn't become too large (avoid unbounded expansion)\n                self.step_size = min(self.step_size, (func.bounds.ub - func.bounds.lb) / 20.0) #max step size 0.5\n\n            #Adaptive Neighborhood size\n            self.neighborhood_size = 0.1 + 0.9 * (self.step_size / ((func.bounds.ub - func.bounds.lb)/2))  #maps step size to neighborhood\n\n            # Scout bee relocation: Replace worst performing scouts with new random scouts\n            worst_index = np.argmax(scout_fitnesses)\n            if np.random.rand() < 0.1: #10% probability\n                scout_positions[worst_index] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                scout_fitnesses[worst_index] = func(scout_positions[worst_index])\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                if scout_fitnesses[worst_index] < self.f_opt:\n                    self.f_opt = scout_fitnesses[worst_index]\n                    self.x_opt = scout_positions[worst_index]\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 62, in __call__\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n.", "error": "", "parent_ids": ["2fd16c53-65f6-4f1d-8062-f7ee83d1b397"], "operator": null, "metadata": {}}
{"id": "c3c81fb9-d67c-4b5e-9f5c-899a69d88e60", "fitness": -Infinity, "name": "SelfOrganizingScoutBee", "description": "Implements a self-organizing scout bee algorithm with dynamic step size and neighborhood adaptation for exploration and exploitation.", "code": "import numpy as np\n\nclass SelfOrganizingScoutBee:\n    def __init__(self, budget=10000, dim=10, n_scouts=10, initial_step_size=0.5, min_step_size=0.01, neighborhood_size=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.n_scouts = n_scouts\n        self.step_size = initial_step_size\n        self.min_step_size = min_step_size\n        self.neighborhood_size = neighborhood_size\n        self.x_opt = None\n        self.f_opt = np.Inf\n\n    def __call__(self, func):\n        # Initialize scout bees randomly\n        scout_positions = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.n_scouts, self.dim))\n        scout_fitnesses = np.array([func(pos) for pos in scout_positions])\n        self.budget -= self.n_scouts\n\n        # Update global best\n        best_index = np.argmin(scout_fitnesses)\n        if scout_fitnesses[best_index] < self.f_opt:\n            self.f_opt = scout_fitnesses[best_index]\n            self.x_opt = scout_positions[best_index]\n\n        while self.budget > 0:\n            # Exploration phase: Each scout searches its neighborhood\n            for i in range(self.n_scouts):\n                # Generate a new position within the neighborhood\n                new_position = scout_positions[i] + np.random.uniform(-self.step_size, self.step_size, size=self.dim)\n                new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n\n                # Evaluate the new position\n                new_fitness = func(new_position)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                # Update scout position if better\n                if new_fitness < scout_fitnesses[i]:\n                    scout_fitnesses[i] = new_fitness\n                    scout_positions[i] = new_position\n\n                    # Update global best if necessary\n                    if new_fitness < self.f_opt:\n                        self.f_opt = new_fitness\n                        self.x_opt = new_position\n            if self.budget <= 0:\n                break\n\n            # Self-Organization: Adjust step size and neighborhood based on performance\n            fitness_improvement = self.f_opt - np.min(scout_fitnesses)\n\n            if fitness_improvement > 0:\n                # Reduce step size if progress is being made\n                self.step_size *= 0.9\n                self.step_size = max(self.step_size, self.min_step_size)\n            else:\n                # Increase step size if no progress\n                self.step_size *= 1.1\n                # Ensure step size doesn't become too large (avoid unbounded expansion)\n                self.step_size = min(self.step_size, (func.bounds.ub - func.bounds.lb) / 20.0) #max step size 0.5\n\n            #Adaptive Neighborhood size\n            self.neighborhood_size = 0.1 + 0.9 * (self.step_size / ((func.bounds.ub - func.bounds.lb)/2))  #maps step size to neighborhood\n\n            # Scout bee relocation: Replace worst performing scouts with new random scouts\n            worst_index = np.argmax(scout_fitnesses)\n            if np.random.rand() < 0.1: #10% probability\n                scout_positions[worst_index] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                scout_fitnesses[worst_index] = func(scout_positions[worst_index])\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                if scout_fitnesses[worst_index] < self.f_opt:\n                    self.f_opt = scout_fitnesses[worst_index]\n                    self.x_opt = scout_positions[worst_index]\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 62, in __call__\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n.", "error": "", "parent_ids": ["2fd16c53-65f6-4f1d-8062-f7ee83d1b397"], "operator": null, "metadata": {}}
{"id": "c3c81fb9-d67c-4b5e-9f5c-899a69d88e60", "fitness": -Infinity, "name": "SelfOrganizingScoutBee", "description": "Implements a self-organizing scout bee algorithm with dynamic step size and neighborhood adaptation for exploration and exploitation.", "code": "import numpy as np\n\nclass SelfOrganizingScoutBee:\n    def __init__(self, budget=10000, dim=10, n_scouts=10, initial_step_size=0.5, min_step_size=0.01, neighborhood_size=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.n_scouts = n_scouts\n        self.step_size = initial_step_size\n        self.min_step_size = min_step_size\n        self.neighborhood_size = neighborhood_size\n        self.x_opt = None\n        self.f_opt = np.Inf\n\n    def __call__(self, func):\n        # Initialize scout bees randomly\n        scout_positions = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.n_scouts, self.dim))\n        scout_fitnesses = np.array([func(pos) for pos in scout_positions])\n        self.budget -= self.n_scouts\n\n        # Update global best\n        best_index = np.argmin(scout_fitnesses)\n        if scout_fitnesses[best_index] < self.f_opt:\n            self.f_opt = scout_fitnesses[best_index]\n            self.x_opt = scout_positions[best_index]\n\n        while self.budget > 0:\n            # Exploration phase: Each scout searches its neighborhood\n            for i in range(self.n_scouts):\n                # Generate a new position within the neighborhood\n                new_position = scout_positions[i] + np.random.uniform(-self.step_size, self.step_size, size=self.dim)\n                new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n\n                # Evaluate the new position\n                new_fitness = func(new_position)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                # Update scout position if better\n                if new_fitness < scout_fitnesses[i]:\n                    scout_fitnesses[i] = new_fitness\n                    scout_positions[i] = new_position\n\n                    # Update global best if necessary\n                    if new_fitness < self.f_opt:\n                        self.f_opt = new_fitness\n                        self.x_opt = new_position\n            if self.budget <= 0:\n                break\n\n            # Self-Organization: Adjust step size and neighborhood based on performance\n            fitness_improvement = self.f_opt - np.min(scout_fitnesses)\n\n            if fitness_improvement > 0:\n                # Reduce step size if progress is being made\n                self.step_size *= 0.9\n                self.step_size = max(self.step_size, self.min_step_size)\n            else:\n                # Increase step size if no progress\n                self.step_size *= 1.1\n                # Ensure step size doesn't become too large (avoid unbounded expansion)\n                self.step_size = min(self.step_size, (func.bounds.ub - func.bounds.lb) / 20.0) #max step size 0.5\n\n            #Adaptive Neighborhood size\n            self.neighborhood_size = 0.1 + 0.9 * (self.step_size / ((func.bounds.ub - func.bounds.lb)/2))  #maps step size to neighborhood\n\n            # Scout bee relocation: Replace worst performing scouts with new random scouts\n            worst_index = np.argmax(scout_fitnesses)\n            if np.random.rand() < 0.1: #10% probability\n                scout_positions[worst_index] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                scout_fitnesses[worst_index] = func(scout_positions[worst_index])\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                if scout_fitnesses[worst_index] < self.f_opt:\n                    self.f_opt = scout_fitnesses[worst_index]\n                    self.x_opt = scout_positions[worst_index]\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 62, in __call__\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n.", "error": "", "parent_ids": ["2fd16c53-65f6-4f1d-8062-f7ee83d1b397"], "operator": null, "metadata": {}}
{"id": "c3c81fb9-d67c-4b5e-9f5c-899a69d88e60", "fitness": -Infinity, "name": "SelfOrganizingScoutBee", "description": "Implements a self-organizing scout bee algorithm with dynamic step size and neighborhood adaptation for exploration and exploitation.", "code": "import numpy as np\n\nclass SelfOrganizingScoutBee:\n    def __init__(self, budget=10000, dim=10, n_scouts=10, initial_step_size=0.5, min_step_size=0.01, neighborhood_size=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.n_scouts = n_scouts\n        self.step_size = initial_step_size\n        self.min_step_size = min_step_size\n        self.neighborhood_size = neighborhood_size\n        self.x_opt = None\n        self.f_opt = np.Inf\n\n    def __call__(self, func):\n        # Initialize scout bees randomly\n        scout_positions = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.n_scouts, self.dim))\n        scout_fitnesses = np.array([func(pos) for pos in scout_positions])\n        self.budget -= self.n_scouts\n\n        # Update global best\n        best_index = np.argmin(scout_fitnesses)\n        if scout_fitnesses[best_index] < self.f_opt:\n            self.f_opt = scout_fitnesses[best_index]\n            self.x_opt = scout_positions[best_index]\n\n        while self.budget > 0:\n            # Exploration phase: Each scout searches its neighborhood\n            for i in range(self.n_scouts):\n                # Generate a new position within the neighborhood\n                new_position = scout_positions[i] + np.random.uniform(-self.step_size, self.step_size, size=self.dim)\n                new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n\n                # Evaluate the new position\n                new_fitness = func(new_position)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                # Update scout position if better\n                if new_fitness < scout_fitnesses[i]:\n                    scout_fitnesses[i] = new_fitness\n                    scout_positions[i] = new_position\n\n                    # Update global best if necessary\n                    if new_fitness < self.f_opt:\n                        self.f_opt = new_fitness\n                        self.x_opt = new_position\n            if self.budget <= 0:\n                break\n\n            # Self-Organization: Adjust step size and neighborhood based on performance\n            fitness_improvement = self.f_opt - np.min(scout_fitnesses)\n\n            if fitness_improvement > 0:\n                # Reduce step size if progress is being made\n                self.step_size *= 0.9\n                self.step_size = max(self.step_size, self.min_step_size)\n            else:\n                # Increase step size if no progress\n                self.step_size *= 1.1\n                # Ensure step size doesn't become too large (avoid unbounded expansion)\n                self.step_size = min(self.step_size, (func.bounds.ub - func.bounds.lb) / 20.0) #max step size 0.5\n\n            #Adaptive Neighborhood size\n            self.neighborhood_size = 0.1 + 0.9 * (self.step_size / ((func.bounds.ub - func.bounds.lb)/2))  #maps step size to neighborhood\n\n            # Scout bee relocation: Replace worst performing scouts with new random scouts\n            worst_index = np.argmax(scout_fitnesses)\n            if np.random.rand() < 0.1: #10% probability\n                scout_positions[worst_index] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                scout_fitnesses[worst_index] = func(scout_positions[worst_index])\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                if scout_fitnesses[worst_index] < self.f_opt:\n                    self.f_opt = scout_fitnesses[worst_index]\n                    self.x_opt = scout_positions[worst_index]\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 62, in __call__\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n.", "error": "", "parent_ids": ["2fd16c53-65f6-4f1d-8062-f7ee83d1b397"], "operator": null, "metadata": {}}
{"id": "c3c81fb9-d67c-4b5e-9f5c-899a69d88e60", "fitness": -Infinity, "name": "SelfOrganizingScoutBee", "description": "Implements a self-organizing scout bee algorithm with dynamic step size and neighborhood adaptation for exploration and exploitation.", "code": "import numpy as np\n\nclass SelfOrganizingScoutBee:\n    def __init__(self, budget=10000, dim=10, n_scouts=10, initial_step_size=0.5, min_step_size=0.01, neighborhood_size=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.n_scouts = n_scouts\n        self.step_size = initial_step_size\n        self.min_step_size = min_step_size\n        self.neighborhood_size = neighborhood_size\n        self.x_opt = None\n        self.f_opt = np.Inf\n\n    def __call__(self, func):\n        # Initialize scout bees randomly\n        scout_positions = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.n_scouts, self.dim))\n        scout_fitnesses = np.array([func(pos) for pos in scout_positions])\n        self.budget -= self.n_scouts\n\n        # Update global best\n        best_index = np.argmin(scout_fitnesses)\n        if scout_fitnesses[best_index] < self.f_opt:\n            self.f_opt = scout_fitnesses[best_index]\n            self.x_opt = scout_positions[best_index]\n\n        while self.budget > 0:\n            # Exploration phase: Each scout searches its neighborhood\n            for i in range(self.n_scouts):\n                # Generate a new position within the neighborhood\n                new_position = scout_positions[i] + np.random.uniform(-self.step_size, self.step_size, size=self.dim)\n                new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n\n                # Evaluate the new position\n                new_fitness = func(new_position)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                # Update scout position if better\n                if new_fitness < scout_fitnesses[i]:\n                    scout_fitnesses[i] = new_fitness\n                    scout_positions[i] = new_position\n\n                    # Update global best if necessary\n                    if new_fitness < self.f_opt:\n                        self.f_opt = new_fitness\n                        self.x_opt = new_position\n            if self.budget <= 0:\n                break\n\n            # Self-Organization: Adjust step size and neighborhood based on performance\n            fitness_improvement = self.f_opt - np.min(scout_fitnesses)\n\n            if fitness_improvement > 0:\n                # Reduce step size if progress is being made\n                self.step_size *= 0.9\n                self.step_size = max(self.step_size, self.min_step_size)\n            else:\n                # Increase step size if no progress\n                self.step_size *= 1.1\n                # Ensure step size doesn't become too large (avoid unbounded expansion)\n                self.step_size = min(self.step_size, (func.bounds.ub - func.bounds.lb) / 20.0) #max step size 0.5\n\n            #Adaptive Neighborhood size\n            self.neighborhood_size = 0.1 + 0.9 * (self.step_size / ((func.bounds.ub - func.bounds.lb)/2))  #maps step size to neighborhood\n\n            # Scout bee relocation: Replace worst performing scouts with new random scouts\n            worst_index = np.argmax(scout_fitnesses)\n            if np.random.rand() < 0.1: #10% probability\n                scout_positions[worst_index] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                scout_fitnesses[worst_index] = func(scout_positions[worst_index])\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                if scout_fitnesses[worst_index] < self.f_opt:\n                    self.f_opt = scout_fitnesses[worst_index]\n                    self.x_opt = scout_positions[worst_index]\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 62, in __call__\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n.", "error": "", "parent_ids": ["2fd16c53-65f6-4f1d-8062-f7ee83d1b397"], "operator": null, "metadata": {}}
{"id": "c3c81fb9-d67c-4b5e-9f5c-899a69d88e60", "fitness": -Infinity, "name": "SelfOrganizingScoutBee", "description": "Implements a self-organizing scout bee algorithm with dynamic step size and neighborhood adaptation for exploration and exploitation.", "code": "import numpy as np\n\nclass SelfOrganizingScoutBee:\n    def __init__(self, budget=10000, dim=10, n_scouts=10, initial_step_size=0.5, min_step_size=0.01, neighborhood_size=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.n_scouts = n_scouts\n        self.step_size = initial_step_size\n        self.min_step_size = min_step_size\n        self.neighborhood_size = neighborhood_size\n        self.x_opt = None\n        self.f_opt = np.Inf\n\n    def __call__(self, func):\n        # Initialize scout bees randomly\n        scout_positions = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.n_scouts, self.dim))\n        scout_fitnesses = np.array([func(pos) for pos in scout_positions])\n        self.budget -= self.n_scouts\n\n        # Update global best\n        best_index = np.argmin(scout_fitnesses)\n        if scout_fitnesses[best_index] < self.f_opt:\n            self.f_opt = scout_fitnesses[best_index]\n            self.x_opt = scout_positions[best_index]\n\n        while self.budget > 0:\n            # Exploration phase: Each scout searches its neighborhood\n            for i in range(self.n_scouts):\n                # Generate a new position within the neighborhood\n                new_position = scout_positions[i] + np.random.uniform(-self.step_size, self.step_size, size=self.dim)\n                new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n\n                # Evaluate the new position\n                new_fitness = func(new_position)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                # Update scout position if better\n                if new_fitness < scout_fitnesses[i]:\n                    scout_fitnesses[i] = new_fitness\n                    scout_positions[i] = new_position\n\n                    # Update global best if necessary\n                    if new_fitness < self.f_opt:\n                        self.f_opt = new_fitness\n                        self.x_opt = new_position\n            if self.budget <= 0:\n                break\n\n            # Self-Organization: Adjust step size and neighborhood based on performance\n            fitness_improvement = self.f_opt - np.min(scout_fitnesses)\n\n            if fitness_improvement > 0:\n                # Reduce step size if progress is being made\n                self.step_size *= 0.9\n                self.step_size = max(self.step_size, self.min_step_size)\n            else:\n                # Increase step size if no progress\n                self.step_size *= 1.1\n                # Ensure step size doesn't become too large (avoid unbounded expansion)\n                self.step_size = min(self.step_size, (func.bounds.ub - func.bounds.lb) / 20.0) #max step size 0.5\n\n            #Adaptive Neighborhood size\n            self.neighborhood_size = 0.1 + 0.9 * (self.step_size / ((func.bounds.ub - func.bounds.lb)/2))  #maps step size to neighborhood\n\n            # Scout bee relocation: Replace worst performing scouts with new random scouts\n            worst_index = np.argmax(scout_fitnesses)\n            if np.random.rand() < 0.1: #10% probability\n                scout_positions[worst_index] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                scout_fitnesses[worst_index] = func(scout_positions[worst_index])\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                if scout_fitnesses[worst_index] < self.f_opt:\n                    self.f_opt = scout_fitnesses[worst_index]\n                    self.x_opt = scout_positions[worst_index]\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 62, in __call__\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n.", "error": "", "parent_ids": ["2fd16c53-65f6-4f1d-8062-f7ee83d1b397"], "operator": null, "metadata": {}}
{"id": "c3c81fb9-d67c-4b5e-9f5c-899a69d88e60", "fitness": -Infinity, "name": "SelfOrganizingScoutBee", "description": "Implements a self-organizing scout bee algorithm with dynamic step size and neighborhood adaptation for exploration and exploitation.", "code": "import numpy as np\n\nclass SelfOrganizingScoutBee:\n    def __init__(self, budget=10000, dim=10, n_scouts=10, initial_step_size=0.5, min_step_size=0.01, neighborhood_size=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.n_scouts = n_scouts\n        self.step_size = initial_step_size\n        self.min_step_size = min_step_size\n        self.neighborhood_size = neighborhood_size\n        self.x_opt = None\n        self.f_opt = np.Inf\n\n    def __call__(self, func):\n        # Initialize scout bees randomly\n        scout_positions = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.n_scouts, self.dim))\n        scout_fitnesses = np.array([func(pos) for pos in scout_positions])\n        self.budget -= self.n_scouts\n\n        # Update global best\n        best_index = np.argmin(scout_fitnesses)\n        if scout_fitnesses[best_index] < self.f_opt:\n            self.f_opt = scout_fitnesses[best_index]\n            self.x_opt = scout_positions[best_index]\n\n        while self.budget > 0:\n            # Exploration phase: Each scout searches its neighborhood\n            for i in range(self.n_scouts):\n                # Generate a new position within the neighborhood\n                new_position = scout_positions[i] + np.random.uniform(-self.step_size, self.step_size, size=self.dim)\n                new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n\n                # Evaluate the new position\n                new_fitness = func(new_position)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                # Update scout position if better\n                if new_fitness < scout_fitnesses[i]:\n                    scout_fitnesses[i] = new_fitness\n                    scout_positions[i] = new_position\n\n                    # Update global best if necessary\n                    if new_fitness < self.f_opt:\n                        self.f_opt = new_fitness\n                        self.x_opt = new_position\n            if self.budget <= 0:\n                break\n\n            # Self-Organization: Adjust step size and neighborhood based on performance\n            fitness_improvement = self.f_opt - np.min(scout_fitnesses)\n\n            if fitness_improvement > 0:\n                # Reduce step size if progress is being made\n                self.step_size *= 0.9\n                self.step_size = max(self.step_size, self.min_step_size)\n            else:\n                # Increase step size if no progress\n                self.step_size *= 1.1\n                # Ensure step size doesn't become too large (avoid unbounded expansion)\n                self.step_size = min(self.step_size, (func.bounds.ub - func.bounds.lb) / 20.0) #max step size 0.5\n\n            #Adaptive Neighborhood size\n            self.neighborhood_size = 0.1 + 0.9 * (self.step_size / ((func.bounds.ub - func.bounds.lb)/2))  #maps step size to neighborhood\n\n            # Scout bee relocation: Replace worst performing scouts with new random scouts\n            worst_index = np.argmax(scout_fitnesses)\n            if np.random.rand() < 0.1: #10% probability\n                scout_positions[worst_index] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                scout_fitnesses[worst_index] = func(scout_positions[worst_index])\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                if scout_fitnesses[worst_index] < self.f_opt:\n                    self.f_opt = scout_fitnesses[worst_index]\n                    self.x_opt = scout_positions[worst_index]\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 62, in __call__\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n.", "error": "", "parent_ids": ["2fd16c53-65f6-4f1d-8062-f7ee83d1b397"], "operator": null, "metadata": {}}
{"id": "c3c81fb9-d67c-4b5e-9f5c-899a69d88e60", "fitness": -Infinity, "name": "SelfOrganizingScoutBee", "description": "Implements a self-organizing scout bee algorithm with dynamic step size and neighborhood adaptation for exploration and exploitation.", "code": "import numpy as np\n\nclass SelfOrganizingScoutBee:\n    def __init__(self, budget=10000, dim=10, n_scouts=10, initial_step_size=0.5, min_step_size=0.01, neighborhood_size=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.n_scouts = n_scouts\n        self.step_size = initial_step_size\n        self.min_step_size = min_step_size\n        self.neighborhood_size = neighborhood_size\n        self.x_opt = None\n        self.f_opt = np.Inf\n\n    def __call__(self, func):\n        # Initialize scout bees randomly\n        scout_positions = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.n_scouts, self.dim))\n        scout_fitnesses = np.array([func(pos) for pos in scout_positions])\n        self.budget -= self.n_scouts\n\n        # Update global best\n        best_index = np.argmin(scout_fitnesses)\n        if scout_fitnesses[best_index] < self.f_opt:\n            self.f_opt = scout_fitnesses[best_index]\n            self.x_opt = scout_positions[best_index]\n\n        while self.budget > 0:\n            # Exploration phase: Each scout searches its neighborhood\n            for i in range(self.n_scouts):\n                # Generate a new position within the neighborhood\n                new_position = scout_positions[i] + np.random.uniform(-self.step_size, self.step_size, size=self.dim)\n                new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n\n                # Evaluate the new position\n                new_fitness = func(new_position)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                # Update scout position if better\n                if new_fitness < scout_fitnesses[i]:\n                    scout_fitnesses[i] = new_fitness\n                    scout_positions[i] = new_position\n\n                    # Update global best if necessary\n                    if new_fitness < self.f_opt:\n                        self.f_opt = new_fitness\n                        self.x_opt = new_position\n            if self.budget <= 0:\n                break\n\n            # Self-Organization: Adjust step size and neighborhood based on performance\n            fitness_improvement = self.f_opt - np.min(scout_fitnesses)\n\n            if fitness_improvement > 0:\n                # Reduce step size if progress is being made\n                self.step_size *= 0.9\n                self.step_size = max(self.step_size, self.min_step_size)\n            else:\n                # Increase step size if no progress\n                self.step_size *= 1.1\n                # Ensure step size doesn't become too large (avoid unbounded expansion)\n                self.step_size = min(self.step_size, (func.bounds.ub - func.bounds.lb) / 20.0) #max step size 0.5\n\n            #Adaptive Neighborhood size\n            self.neighborhood_size = 0.1 + 0.9 * (self.step_size / ((func.bounds.ub - func.bounds.lb)/2))  #maps step size to neighborhood\n\n            # Scout bee relocation: Replace worst performing scouts with new random scouts\n            worst_index = np.argmax(scout_fitnesses)\n            if np.random.rand() < 0.1: #10% probability\n                scout_positions[worst_index] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                scout_fitnesses[worst_index] = func(scout_positions[worst_index])\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                if scout_fitnesses[worst_index] < self.f_opt:\n                    self.f_opt = scout_fitnesses[worst_index]\n                    self.x_opt = scout_positions[worst_index]\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 62, in __call__\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n.", "error": "", "parent_ids": ["2fd16c53-65f6-4f1d-8062-f7ee83d1b397"], "operator": null, "metadata": {}}
{"id": "0238dcfe-fe50-463f-b41a-355bfa872f0f", "fitness": -Infinity, "name": "FuzzyAdaptiveDE", "description": "Implements a differential evolution strategy with a fuzzy adaptive parameter control and a local search refinement step, using a fuzzy logic controller to adjust F and CR based on the success rate and diversity of the population, followed by a Nelder-Mead local search to refine the best solution.", "code": "import numpy as np\nimport skfuzzy as fuzz\nfrom skfuzzy import control as ctrl\nfrom scipy.optimize import minimize\n\nclass FuzzyAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, local_search_iterations=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.local_search_iterations = local_search_iterations\n        self.population = None\n        self.fitness = None\n        self.best_fitness = np.inf\n        self.best_solution = None\n\n        # Fuzzy Logic Controller Setup\n        self.success_rate = ctrl.Antecedent(np.arange(0, 1.01, 0.01), 'success_rate')\n        self.diversity = ctrl.Antecedent(np.arange(0, 1.01, 0.01), 'diversity')\n        self.F = ctrl.Consequent(np.arange(0.1, 1.01, 0.01), 'F')\n        self.CR = ctrl.Consequent(np.arange(0.1, 1.01, 0.01), 'CR')\n\n        # Membership functions (example - can be tuned)\n        self.success_rate['low'] = fuzz.trimf(self.success_rate.universe, [0, 0, 0.5])\n        self.success_rate['medium'] = fuzz.trimf(self.success_rate.universe, [0.25, 0.5, 0.75])\n        self.success_rate['high'] = fuzz.trimf(self.success_rate.universe, [0.5, 1, 1])\n\n        self.diversity['low'] = fuzz.trimf(self.diversity.universe, [0, 0, 0.5])\n        self.diversity['medium'] = fuzz.trimf(self.diversity.universe, [0.25, 0.5, 0.75])\n        self.diversity['high'] = fuzz.trimf(self.diversity.universe, [0.5, 1, 1])\n\n        self.F['small'] = fuzz.trimf(self.F.universe, [0.1, 0.1, 0.6])\n        self.F['medium'] = fuzz.trimf(self.F.universe, [0.3, 0.6, 0.9])\n        self.F['large'] = fuzz.trimf(self.F.universe, [0.6, 1, 1])\n\n        self.CR['small'] = fuzz.trimf(self.CR.universe, [0.1, 0.1, 0.6])\n        self.CR['medium'] = fuzz.trimf(self.CR.universe, [0.3, 0.6, 0.9])\n        self.CR['large'] = fuzz.trimf(self.CR.universe, [0.6, 1, 1])\n\n        # Rules (example - can be tuned)\n        rule1 = ctrl.Rule(self.success_rate['low'] & self.diversity['low'], (self.F['large'], self.CR['small']))\n        rule2 = ctrl.Rule(self.success_rate['low'] & self.diversity['medium'], (self.F['medium'], self.CR['small']))\n        rule3 = ctrl.Rule(self.success_rate['low'] & self.diversity['high'], (self.F['small'], self.CR['medium']))\n        rule4 = ctrl.Rule(self.success_rate['medium'] & self.diversity['low'], (self.F['large'], self.CR['medium']))\n        rule5 = ctrl.Rule(self.success_rate['medium'] & self.diversity['medium'], (self.F['medium'], self.CR['medium']))\n        rule6 = ctrl.Rule(self.success_rate['medium'] & self.diversity['high'], (self.F['small'], self.CR['large']))\n        rule7 = ctrl.Rule(self.success_rate['high'] & self.diversity['low'], (self.F['medium'], self.CR['large']))\n        rule8 = ctrl.Rule(self.success_rate['high'] & self.diversity['medium'], (self.F['small'], self.CR['large']))\n        rule9 = ctrl.Rule(self.success_rate['high'] & self.diversity['high'], (self.F['small'], self.CR['large']))\n\n        self.parameter_control = ctrl.ControlSystem([rule1, rule2, rule3, rule4, rule5, rule6, rule7, rule8, rule9])\n        self.parameter_simulation = ctrl.ControlSystemSimulation(self.parameter_control)\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(self.fitness)\n        self.best_fitness = self.fitness[best_index]\n        self.best_solution = self.population[best_index].copy()\n\n    def calculate_diversity(self):\n        # Simple diversity measure: average distance from centroid\n        centroid = np.mean(self.population, axis=0)\n        distances = np.linalg.norm(self.population - centroid, axis=1)\n        diversity = np.mean(distances)\n        # Normalize diversity to [0, 1]\n        diversity = diversity / (func.bounds.ub[0] - func.bounds.lb[0])  # Assuming bounds are the same for all dimensions\n        return np.clip(diversity, 0, 1)\n\n\n    def evolve(self, func):\n        F = 0.5 * np.ones(self.pop_size)  # Initialize F and CR\n        CR = 0.9 * np.ones(self.pop_size)\n\n        success_count = 0\n        for i in range(self.pop_size):\n            # Mutation\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[idxs]\n            mutant = self.population[i] + F[i] * (x_r1 - x_r2)\n            mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            crossover_mask = np.random.rand(self.dim) < CR[i]\n            trial = np.where(crossover_mask, mutant, self.population[i])\n\n            # Evaluation\n            trial_fitness = func(trial)\n            self.budget -= 1\n\n            if trial_fitness < self.best_fitness:\n                self.best_fitness = trial_fitness\n                self.best_solution = trial.copy()\n                success_count += 1\n            \n            # Selection\n            if trial_fitness < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = trial_fitness\n            \n        # Fuzzy Parameter Adaptation\n        success_rate = success_count / self.pop_size if self.pop_size > 0 else 0\n        diversity = self.calculate_diversity()\n\n        self.parameter_simulation.input['success_rate'] = success_rate\n        self.parameter_simulation.input['diversity'] = diversity\n        self.parameter_simulation.compute()\n\n        F = self.parameter_simulation.output['F']\n        CR = self.parameter_simulation.output['CR']\n\n        return F, CR\n    \n    def local_search(self, func):\n        bounds = [(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)]\n        result = minimize(func, self.best_solution, method='Nelder-Mead', bounds=bounds, options={'maxiter': self.local_search_iterations})\n        if result.fun < self.best_fitness:\n            self.best_fitness = result.fun\n            self.best_solution = result.x\n\n        self.budget -= result.nit # Account for function evaluations by Nelder-Mead\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > self.local_search_iterations:\n            F, CR = self.evolve(func)\n        \n        self.local_search(func)\n\n        return self.best_fitness, self.best_solution", "configspace": "", "generation": 4, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 111, in evaluate\n    exec(code, safe_globals, local_env)\n  File \"<string>\", line 2, in <module>\nModuleNotFoundError: No module named 'skfuzzy'\n.", "error": "", "parent_ids": ["ae9311ce-199b-4f66-a93d-1b3e918b660d"], "operator": null, "metadata": {}}
{"id": "cda08a6d-adea-4430-b67f-76352ae7ae81", "fitness": 0.7194469803443957, "name": "MirroredSamplingDE", "description": "A differential evolution strategy that uses a mirrored sampling technique to enhance exploration and exploitation around the best-so-far solution.", "code": "import numpy as np\n\nclass MirroredSamplingDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, mirror_rate=0.2, F=0.5, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.mirror_rate = mirror_rate\n        self.F = F\n        self.CR = CR\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        for i in range(self.pop_size):\n            if fitness[i] < self.f_opt:\n                self.f_opt = fitness[i]\n                self.x_opt = self.population[i]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[idxs]\n                mutant = x_r1 + self.F * (x_r2 - x_r3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Mirrored Sampling\n                if np.random.rand() < self.mirror_rate:\n                    mirror_point = self.x_opt + np.random.normal(0, 0.1, self.dim)  # Sample around best solution\n                    mirror_point = np.clip(mirror_point, func.bounds.lb, func.bounds.ub)\n                    mutant = 0.5 * (mutant + mirror_point) # Combine with the mutant\n\n                # Crossover\n                crossover = np.random.uniform(size=self.dim) < self.CR\n                trial = np.where(crossover, mutant, self.population[i])\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    self.population[i] = trial\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm MirroredSamplingDE scored 0.719 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f0e55a70-292c-4a08-9a4f-8e6cd533fba8"], "operator": null, "metadata": {"aucs": [0.3244130087154825, 0.7479067383661102, 0.6933989186812615, 0.938253691664686, 0.7604096405475839, 0.8084466221868382, 0.656963863161068, 0.6375883607643273, 0.7785356773632128, 0.6480371994559223, 0.9125399198849633, 0.9923678946821141, 0.5941806014173163, 0.7591061065855604, 0.9426984561770838, 0.7840647178553957, 0.5710006828689762, 0.8848397638514371, 0.2529712330778745, 0.7012165095807006]}}
{"id": "eb06b43e-cb7d-4d4a-9fc2-5b62f5a7c13c", "fitness": 0.42120493910569134, "name": "SelfOrganizingSpeciationDE", "description": "Implements a self-organizing speciation-based differential evolution with adaptive parameter control and local search enhancement, promoting diversity and fine-tuning.", "code": "import numpy as np\n\nclass SelfOrganizingSpeciationDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, num_species=5, lr_F=0.1, lr_CR=0.1, local_search_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.num_species = num_species\n        self.lr_F = lr_F\n        self.lr_CR = lr_CR\n        self.local_search_prob = local_search_prob\n        self.F = np.full(num_species, 0.5)\n        self.CR = np.full(num_species, 0.9)\n        self.species = None\n        self.centroids = None\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        \n        for i in range(self.pop_size):\n            if fitness[i] < self.f_opt:\n                self.f_opt = fitness[i]\n                self.x_opt = self.population[i]\n        \n        # Initialize Species\n        self.centroids = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.num_species, self.dim))\n        self.assign_species()\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                species_id = self.species[i]\n\n                # Differential Evolution\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[idxs]\n                mutant = x_r1 + self.F[species_id] * (x_r2 - x_r3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                \n                crossover = np.random.uniform(size=self.dim) < self.CR[species_id]\n                trial = np.where(crossover, mutant, self.population[i])\n                \n                # Local Search\n                if np.random.rand() < self.local_search_prob:\n                    trial = self.local_search(trial, func.bounds.lb, func.bounds.ub)\n                \n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                if f_trial < fitness[i]:\n                    # Adaptive Parameter Control\n                    delta_f = fitness[i] - f_trial\n                    self.F[species_id] = max(0, min(1, self.F[species_id] + self.lr_F * delta_f))\n                    self.CR[species_id] = max(0, min(1, self.CR[species_id] + self.lr_CR * delta_f))\n                    \n                    fitness[i] = f_trial\n                    self.population[i] = trial\n            \n            # Re-assign Species periodically\n            if self.budget % (self.pop_size) == 0:\n                self.assign_species()\n\n        return self.f_opt, self.x_opt\n\n    def assign_species(self):\n        self.species = np.zeros(self.pop_size, dtype=int)\n        for i in range(self.pop_size):\n            distances = np.linalg.norm(self.population[i] - self.centroids, axis=1)\n            self.species[i] = np.argmin(distances)\n\n    def local_search(self, x, lb, ub, step_size=0.1):\n        # Simple local search around x\n        x_new = x + np.random.uniform(-step_size, step_size, size=self.dim)\n        return np.clip(x_new, lb, ub)", "configspace": "", "generation": 4, "feedback": "The algorithm SelfOrganizingSpeciationDE scored 0.421 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f0e55a70-292c-4a08-9a4f-8e6cd533fba8"], "operator": null, "metadata": {"aucs": [0.16238061582859686, 0.29566169689330846, 0.36181151767665787, 0.6345902376241752, 0.3039616755415032, 0.4281954456773227, 0.2934941227366956, 0.3392434221190107, 0.31314908471588065, 0.23137848590676735, 0.4978457637545599, 0.9935110068892545, 0.3891077210740955, 0.32231030185952114, 0.845302082137116, 0.4278151443180668, 0.3249717994829182, 0.5002797275161148, 0.26673815618629815, 0.4923507741759642]}}
{"id": "0bb927fc-2190-4caf-b918-8f7c01c88c04", "fitness": -Infinity, "name": "AdaptiveLandscapeExplorer", "description": "A population-based algorithm that dynamically adjusts its search behavior based on the observed fitness landscape, using a combination of global exploration and local refinement strategies.", "code": "import numpy as np\n\nclass AdaptiveLandscapeExplorer:\n    def __init__(self, budget=10000, dim=10, pop_size=50, exploration_prob=0.3, local_search_intensity=0.1, step_size_init=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.exploration_prob = exploration_prob\n        self.local_search_intensity = local_search_intensity\n        self.step_size = step_size_init\n        self.population = None\n        self.fitness = None\n        self.best_x = None\n        self.best_f = np.inf\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n\n        # Initialize population\n        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        # Find initial best solution\n        best_idx = np.argmin(self.fitness)\n        self.best_x = self.population[best_idx]\n        self.best_f = self.fitness[best_idx]\n\n        while self.budget > 0:\n            new_population = np.copy(self.population)\n            new_fitness = np.copy(self.fitness)\n\n            for i in range(self.pop_size):\n                if np.random.rand() < self.exploration_prob:\n                    # Global exploration: Randomly sample a new solution\n                    new_x = np.random.uniform(lb, ub, size=self.dim)\n                else:\n                    # Local refinement: Perturb the current solution\n                    new_x = self.population[i] + np.random.uniform(-self.step_size, self.step_size, size=self.dim) * self.local_search_intensity\n                    new_x = np.clip(new_x, lb, ub)\n\n                new_f = func(new_x)\n                self.budget -= 1\n                \n                if new_f < self.best_f:\n                    self.best_f = new_f\n                    self.best_x = new_x\n\n                if new_f < self.fitness[i]:\n                    new_population[i] = new_x\n                    new_fitness[i] = new_f\n            \n            # Update population and fitness\n            self.population = new_population\n            self.fitness = new_fitness\n\n            # Dynamically adjust step size (exploration vs. exploitation)\n            improvement_ratio = np.sum(self.fitness - self.population) / self.pop_size\n            if improvement_ratio > 0:\n              self.step_size *= 0.95 # Reduce step size\n            else:\n              self.step_size *= 1.05 # Increase step size\n            self.step_size = np.clip(self.step_size, 0.01, 0.5)\n\n        return self.best_f, self.best_x", "configspace": "", "generation": 5, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 59, in __call__\nValueError: operands could not be broadcast together with shapes (50,) (50,2) \n.", "error": "", "parent_ids": ["eb06b43e-cb7d-4d4a-9fc2-5b62f5a7c13c"], "operator": null, "metadata": {}}
{"id": "5cf887fe-d2e3-4e04-9b82-4636fbc517ef", "fitness": 0.0, "name": "ShrinkingHyperrectangleDE", "description": "Population-based algorithm with a shrinking hyperrectangle search space and Cauchy mutation.", "code": "import numpy as np\n\nclass ShrinkingHyperrectangleDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, shrinkage_rate=0.99, F=0.7, CR=0.8, cauchy_scale=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.shrinkage_rate = shrinkage_rate\n        self.F = F\n        self.CR = CR\n        self.cauchy_scale = cauchy_scale\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = self.population[best_index]\n        \n        current_lb = np.full(self.dim, self.lb)\n        current_ub = np.full(self.dim, self.ub)\n\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation (Cauchy)\n                cauchy_noise = np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n                mutant = self.population[i] + cauchy_noise\n                mutant = np.clip(mutant, current_lb, current_ub)\n\n                # Crossover\n                crossover = np.random.uniform(size=self.dim) < self.CR\n                trial = np.where(crossover, mutant, self.population[i])\n                trial = np.clip(trial, current_lb, current_ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    self.population[i] = trial\n                    \n            # Shrink the hyperrectangle\n            best_index = np.argmin(fitness)\n            best_solution = self.population[best_index]\n            \n            range_width = (current_ub - current_lb) * self.shrinkage_rate\n            current_lb = best_solution - range_width / 2\n            current_ub = best_solution + range_width / 2\n            \n            current_lb = np.maximum(current_lb, np.full(self.dim, self.lb))\n            current_ub = np.minimum(current_ub, np.full(self.dim, self.ub))\n\n            # Repopulate the solutions within the shrinked hyperrectangle\n            self.population = np.random.uniform(current_lb, current_ub, size=(self.pop_size, self.dim))\n            fitness = np.array([func(x) for x in self.population])\n            self.budget -= self.pop_size\n            \n            best_index_local = np.argmin(fitness)\n            if fitness[best_index_local] < self.f_opt:\n                 self.f_opt = fitness[best_index_local]\n                 self.x_opt = self.population[best_index_local]\n        \n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm ShrinkingHyperrectangleDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["cda08a6d-adea-4430-b67f-76352ae7ae81"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "9ccfdec3-c426-40f6-a075-770e0ed0e416", "fitness": 0.0, "name": "AdaptiveLocalSearchDE", "description": "Combines differential evolution with a local search strategy that adapts its intensity based on the diversity of the population.", "code": "import numpy as np\n\nclass AdaptiveLocalSearchDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, local_search_prob=0.1, local_search_radius=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.local_search_prob = local_search_prob\n        self.local_search_radius = local_search_radius\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        for i in range(self.pop_size):\n            if fitness[i] < self.f_opt:\n                self.f_opt = fitness[i]\n                self.x_opt = self.population[i]\n\n        while self.budget > 0:\n            # Calculate population diversity (standard deviation along each dimension)\n            diversity = np.std(self.population, axis=0)\n            # Adapt local search radius based on population diversity\n            adaptive_radius = self.local_search_radius * np.mean(diversity)\n\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                mutant = x_1 + self.F * (x_2 - x_3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                crossover = np.random.uniform(size=self.dim) < self.CR\n                trial = np.where(crossover, mutant, self.population[i])\n\n                # Local Search\n                if np.random.rand() < self.local_search_prob:\n                    trial_local = trial + np.random.uniform(-adaptive_radius, adaptive_radius, size=self.dim)\n                    trial_local = np.clip(trial_local, func.bounds.lb, func.bounds.ub)\n\n                    f_trial_local = func(trial_local)\n                    self.budget -= 1\n\n                    if f_trial_local < self.f_opt:\n                        self.f_opt = f_trial_local\n                        self.x_opt = trial_local\n\n                    if f_trial_local < fitness[i]:\n                        fitness[i] = f_trial_local\n                        self.population[i] = trial_local\n                        trial = trial_local\n                    else:\n                        f_trial = func(trial)\n                        self.budget -= 1\n                        if f_trial < self.f_opt:\n                            self.f_opt = f_trial\n                            self.x_opt = trial\n\n                        if f_trial < fitness[i]:\n                            fitness[i] = f_trial\n                            self.population[i] = trial\n\n\n                else:\n\n                    f_trial = func(trial)\n                    self.budget -= 1\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                    if f_trial < fitness[i]:\n                        fitness[i] = f_trial\n                        self.population[i] = trial\n\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveLocalSearchDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2a597676-bc60-41b7-9453-830956bb9327"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "84aafec0-9ec0-489d-80d7-3461846ebebc", "fitness": 0.0, "name": "AdaptiveLandscapeDE", "description": "A differential evolution strategy that dynamically adjusts its parameters based on the problem's ruggedness detected through fitness landscape analysis, incorporating a local search step to refine solutions.", "code": "import numpy as np\n\nclass AdaptiveLandscapeDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, local_search_iterations=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.local_search_iterations = local_search_iterations\n        self.F = 0.5\n        self.CR = 0.9\n        self.smoothness = 0.5  # Initial guess; updated dynamically\n        self.min_F = 0.1\n        self.max_F = 0.9\n        self.min_CR = 0.1\n        self.max_CR = 0.9\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        \n        for i in range(self.pop_size):\n            if fitness[i] < self.f_opt:\n                self.f_opt = fitness[i]\n                self.x_opt = self.population[i]\n\n        while self.budget > 0:\n            # Landscape Analysis (Simple estimation of ruggedness)\n            fitness_std = np.std(fitness)\n            # Update smoothness parameter based on the fitness landscape. Higher std means more rugged.\n            self.smoothness = 1.0 / (1.0 + fitness_std) # smoothness is between 0 and 1. close to 1 is smooth\n\n            # Adaptive Parameter Control\n            self.F = self.min_F + (self.max_F - self.min_F) * self.smoothness\n            self.CR = self.min_CR + (self.max_CR - self.min_CR) * self.smoothness\n\n\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[idxs]\n                mutant = x_r1 + self.F * (x_r2 - x_r3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                crossover = np.random.uniform(size=self.dim) < self.CR\n                trial = np.where(crossover, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n                \n                # Local Search\n                if f_trial < fitness[i]:\n                    # Perform local search around the new solution\n                    x_local = self.local_search(trial, func, self.local_search_iterations)\n                    f_local = func(x_local)\n                    self.budget -= 1  # Local search counts as function evaluations\n\n                    if f_local < self.f_opt:\n                        self.f_opt = f_local\n                        self.x_opt = x_local\n\n                    if f_local < f_trial:\n                         f_trial = f_local\n                         trial = x_local #replace trial\n\n                    if f_trial < fitness[i]:\n                        fitness[i] = f_trial\n                        self.population[i] = trial\n        return self.f_opt, self.x_opt\n\n    def local_search(self, x, func, iterations):\n        x_best = x.copy()\n        f_best = func(x)\n\n        for _ in range(iterations):\n            # Generate a neighbor by adding a small random perturbation\n            x_neighbor = x_best + np.random.normal(0, 0.1, size=self.dim)  # Adjust step size (0.1) as needed\n            x_neighbor = np.clip(x_neighbor, func.bounds.lb, func.bounds.ub)\n            f_neighbor = func(x_neighbor)\n            \n            if f_neighbor < f_best:\n                f_best = f_neighbor\n                x_best = x_neighbor.copy()\n        return x_best", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveLandscapeDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["68f818e5-f119-42a5-8d7f-63a919a76e68"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "32511415-66fa-46a7-ae19-b9523a0b4daf", "fitness": 0.0, "name": "DynamicArchiveLocalSearchDE", "description": "A differential evolution strategy with a dynamic archive size and a local search phase for intensification.", "code": "import numpy as np\n\nclass DynamicArchiveLocalSearchDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size_init=10, F=0.5, CR=0.9, local_search_prob=0.1, local_search_radius=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size_init = archive_size_init\n        self.archive_size = archive_size_init\n        self.F = F\n        self.CR = CR\n        self.archive = []\n        self.local_search_prob = local_search_prob\n        self.local_search_radius = local_search_radius\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        for i in range(self.pop_size):\n            if fitness[i] < self.f_opt:\n                self.f_opt = fitness[i]\n                self.x_opt = self.population[i]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n\n                # Add a random solution from the archive if the archive is not empty\n                if self.archive:\n                    x_4 = self.archive[np.random.choice(len(self.archive))]\n                    mutant = x_1 + self.F * (x_2 - x_3) + self.F * (x_4 - self.population[i]) # Adding a component from the archive\n                else:\n                    mutant = x_1 + self.F * (x_2 - x_3)\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                crossover = np.random.uniform(size=self.dim) < self.CR\n                trial = np.where(crossover, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                if f_trial < fitness[i]:\n                    # Replace individual in population\n                    fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    # Update archive: Add trial to archive if it's not already present\n                    if not any((trial == x).all() for x in self.archive):\n                        self.archive.append(trial)\n                        if len(self.archive) > self.archive_size:\n                            self.archive.pop(np.random.randint(0, len(self.archive)))  # Remove a random element if archive is full\n            \n            # Local Search: Apply local search to the best solution with a certain probability\n            if np.random.rand() < self.local_search_prob:\n                x_local_search = self.x_opt + np.random.uniform(-self.local_search_radius, self.local_search_radius, size=self.dim)\n                x_local_search = np.clip(x_local_search, func.bounds.lb, func.bounds.ub)\n                f_local_search = func(x_local_search)\n                self.budget -= 1\n\n                if f_local_search < self.f_opt:\n                    self.f_opt = f_local_search\n                    self.x_opt = x_local_search\n\n            # Adjust archive size dynamically\n            if generation % 10 == 0:\n                if self.f_opt == np.min(fitness):\n                     self.archive_size = min(self.archive_size + 1, self.pop_size)\n                else:\n                    self.archive_size = max(self.archive_size - 1, self.archive_size_init) #ensure archive size doesn't become zero.\n                    \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm DynamicArchiveLocalSearchDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2a597676-bc60-41b7-9453-830956bb9327"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "a5e2c252-ed4a-4415-89c5-d456a96cc9a4", "fitness": 0.0, "name": "DynamicDESLS", "description": "A differential evolution strategy with dynamic population sizing and a local search operator triggered by stagnation detection to balance exploration and exploitation.", "code": "import numpy as np\n\nclass DynamicDESLS:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, stagnation_threshold=0.001, ls_probability=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.initial_pop_size = initial_pop_size\n        self.stagnation_threshold = stagnation_threshold\n        self.ls_probability = ls_probability\n        self.F = 0.5\n        self.CR = 0.9\n        self.best_fitness_history = []\n        self.shrink_factor = 0.9\n        self.grow_factor = 1.1\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        \n        for i in range(self.pop_size):\n            if fitness[i] < self.f_opt:\n                self.f_opt = fitness[i]\n                self.x_opt = self.population[i]\n        \n        self.best_fitness_history.append(self.f_opt)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[idxs]\n                mutant = x_r1 + self.F * (x_r2 - x_r3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                crossover = np.random.uniform(size=self.dim) < self.CR\n                trial = np.where(crossover, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    self.population[i] = trial\n                \n                # Local Search\n                if np.random.rand() < self.ls_probability and self.budget > 0:\n                    x_ls = self.local_search(func, trial, func.bounds.lb, func.bounds.ub)\n                    f_ls = func(x_ls)\n                    self.budget -= 1\n                    if f_ls < self.f_opt:\n                        self.f_opt = f_ls\n                        self.x_opt = x_ls\n                    if f_ls < fitness[i]:\n                        fitness[i] = f_ls\n                        self.population[i] = x_ls\n\n            # Dynamic Population Sizing\n            if len(self.best_fitness_history) > 10:\n                if np.std(self.best_fitness_history[-10:]) < self.stagnation_threshold:\n                    # Stagnation detected, shrink population\n                    self.pop_size = max(int(self.pop_size * self.shrink_factor), 10)  # Ensure minimum size\n                    self.population = self.population[:self.pop_size]\n                    fitness = fitness[:self.pop_size]\n                    #Increase local search probability when stagnated\n                    self.ls_probability = min(self.ls_probability * 1.2, 0.5)\n                else:\n                    # Increase Population Size\n                     self.pop_size = min(int(self.pop_size * self.grow_factor), self.initial_pop_size * 2)\n                     if self.pop_size > len(self.population):\n                        num_to_add = self.pop_size - len(self.population)\n                        new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(num_to_add, self.dim))\n                        new_fitness = np.array([func(x) for x in new_individuals])\n                        self.budget -= num_to_add\n                        self.population = np.vstack((self.population, new_individuals))\n                        fitness = np.concatenate((fitness, new_fitness))\n                        for i in range(len(fitness)-num_to_add, len(fitness)):\n                            if fitness[i] < self.f_opt:\n                                self.f_opt = fitness[i]\n                                self.x_opt = self.population[i]\n                     #Decrease local search probability when not stagnated\n                     self.ls_probability = max(self.ls_probability / 1.2, 0.01)\n\n            self.best_fitness_history.append(self.f_opt)\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt\n\n    def local_search(self, func, x, lb, ub, radius=0.1, iterations=5):\n        x_best = x.copy()\n        f_best = func(x_best)\n        for _ in range(iterations):\n            x_new = x_best + np.random.uniform(-radius, radius, size=self.dim)\n            x_new = np.clip(x_new, lb, ub)\n            f_new = func(x_new)\n            if f_new < f_best:\n                f_best = f_new\n                x_best = x_new\n        return x_best", "configspace": "", "generation": 5, "feedback": "The algorithm DynamicDESLS scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["68f818e5-f119-42a5-8d7f-63a919a76e68"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "a5d02b26-ccb3-4bd1-8fab-45d58138ef62", "fitness": 0.452734012314931, "name": "AdaptiveRadiusMarginSampling", "description": "Adaptive Radius Margin Sampling, which adjusts the sampling radius based on the success rate of finding better solutions, focusing exploration in promising areas and exploitation when near optima.", "code": "import numpy as np\n\nclass AdaptiveRadiusMarginSampling:\n    def __init__(self, budget=10000, dim=10, initial_radius=0.5, success_rate_threshold=0.2, radius_decay=0.95, radius_growth=1.05):\n        self.budget = budget\n        self.dim = dim\n        self.radius = initial_radius\n        self.success_rate_threshold = success_rate_threshold\n        self.radius_decay = radius_decay\n        self.radius_growth = radius_growth\n        self.x_best = None\n        self.f_best = np.inf\n        self.archive = []\n        self.archive_size = 50  # Limit archive size to prevent memory issues\n\n    def __call__(self, func):\n        # Initial random solution\n        x = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        f = func(x)\n        self.budget -= 1\n\n        self.x_best = x\n        self.f_best = f\n        self.archive.append((x, f))  # Store the initial solution\n\n        successes = 0\n        iterations = 0\n\n        while self.budget > 0:\n            iterations += 1\n            # Sample around the best solution within a radius\n            x_new = self.x_best + np.random.uniform(-self.radius, self.radius, size=self.dim)\n            x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n\n            f_new = func(x_new)\n            self.budget -= 1\n\n            if f_new < self.f_best:\n                self.f_best = f_new\n                self.x_best = x_new\n                successes += 1\n\n                #Archive Management\n                self.archive.append((x_new, f_new))\n                self.archive = sorted(self.archive, key=lambda item: item[1]) #Sort by fitness\n                if len(self.archive) > self.archive_size:\n                  self.archive.pop()  #Remove the worst from archive to maintain size limit\n\n            # Adjust radius based on success rate\n            if iterations % 10 == 0:\n                success_rate = successes / 10\n                if success_rate > self.success_rate_threshold:\n                    self.radius *= self.radius_growth # Expand exploration when successful\n                else:\n                    self.radius *= self.radius_decay # Reduce exploration when unsuccessful\n                self.radius = np.clip(self.radius, 1e-6, 1) #Limit radius to avoid divergence\n                successes = 0  # Reset success counter\n\n        return self.f_best, self.x_best", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveRadiusMarginSampling scored 0.453 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2a597676-bc60-41b7-9453-830956bb9327"], "operator": null, "metadata": {"aucs": [0.04023759020104023, 0.22083613072921893, 0.9089827230027481, 0.2105134609170224, 0.19926518517890124, 0.9238869283886462, 0.2787202917176501, 0.8388098038491385, 0.9097782658780798, 0.11663202896498404, 0.9621712184331923, 0.9987887374072512, 0.24368530713268843, 0.23651582166906604, 0.8821722814681923, 0.3345233268786787, 0.2505858358049894, 0.19360452502801972, 0.11756497670811206, 0.18740580694100162]}}
{"id": "9e5611cf-797c-4810-b292-caedfb43542e", "fitness": 0.5295115827449769, "name": "RingTopologyAdaptiveDE", "description": "A population-based algorithm using a ring topology for information sharing and a self-adaptive strategy based on fitness improvement to adjust mutation parameters.", "code": "import numpy as np\n\nclass RingTopologyAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, ring_neighbors=3, lr_F=0.1, lr_CR=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.ring_neighbors = ring_neighbors\n        self.lr_F = lr_F\n        self.lr_CR = lr_CR\n        self.F = np.full(pop_size, 0.5)\n        self.CR = np.full(pop_size, 0.9)\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        \n        for i in range(self.pop_size):\n            if fitness[i] < self.f_opt:\n                self.f_opt = fitness[i]\n                self.x_opt = self.population[i]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Ring Topology Selection\n                neighbors = [(i + j) % self.pop_size for j in range(1, self.ring_neighbors + 1)]\n                \n                # Select best neighbor for mutation\n                best_neighbor = i\n                for neighbor in neighbors:\n                    if fitness[neighbor] < fitness[best_neighbor]:\n                        best_neighbor = neighbor\n                \n                # Differential Evolution\n                idxs = np.random.choice(self.pop_size, 2, replace=False)\n                x_r1, x_r2 = self.population[idxs]\n                mutant = self.population[i] + self.F[i] * (self.population[best_neighbor] - self.population[i]) + self.F[i] * (x_r1 - x_r2)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                \n                crossover = np.random.uniform(size=self.dim) < self.CR[i]\n                trial = np.where(crossover, mutant, self.population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                if f_trial < fitness[i]:\n                    # Adaptive Parameter Control\n                    delta_f = fitness[i] - f_trial\n                    self.F[i] = max(0, min(1, self.F[i] + self.lr_F * delta_f, 1.0))\n                    self.CR[i] = max(0, min(1, self.CR[i] + self.lr_CR * delta_f, 1.0))\n                    \n                    fitness[i] = f_trial\n                    self.population[i] = trial\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm RingTopologyAdaptiveDE scored 0.530 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["eb06b43e-cb7d-4d4a-9fc2-5b62f5a7c13c"], "operator": null, "metadata": {"aucs": [0.14362574258525806, 0.42489608352172414, 0.6981698444520117, 0.7075065244678564, 0.4591119561293462, 0.5825146518258133, 0.3145687865734378, 0.38435629896832246, 0.47516789968734663, 0.39646350324837676, 0.6669337669697539, 0.991891581263147, 0.47582663241732714, 0.4373248902710779, 0.8971844944154479, 0.7296725334095302, 0.4044794220506094, 0.6639949158430056, 0.21000057146415374, 0.5265415553359921]}}
{"id": "9f400c11-b5df-4a6b-bb11-5cc695c64d2d", "fitness": 0.33110868210730526, "name": "AdaptiveRadiusMarginSampling", "description": "Adaptive Radius Margin Sampling (ARMS) adjusts the sampling radius around the best solution based on the success rate of finding better solutions, balancing exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveRadiusMarginSampling:\n    def __init__(self, budget=10000, dim=10, initial_radius=0.5, radius_decay=0.99, success_threshold=0.2, radius_increase=1.1):\n        self.budget = budget\n        self.dim = dim\n        self.radius = initial_radius\n        self.radius_decay = radius_decay\n        self.success_threshold = success_threshold\n        self.radius_increase = radius_increase\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.success_count = 0\n        self.iteration = 0\n\n    def __call__(self, func):\n        # Initial sample\n        x = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        f = func(x)\n        self.budget -= 1\n        self.f_opt = f\n        self.x_opt = x\n        \n        while self.budget > 0:\n            self.iteration += 1\n            # Sample within a radius of the best solution\n            lb = np.maximum(self.x_opt - self.radius, func.bounds.lb)\n            ub = np.minimum(self.x_opt + self.radius, func.bounds.ub)\n            x_new = np.random.uniform(lb, ub, size=self.dim)\n\n            f_new = func(x_new)\n            self.budget -= 1\n\n            if f_new < self.f_opt:\n                self.f_opt = f_new\n                self.x_opt = x_new\n                self.success_count += 1\n            \n            # Adjust the radius based on success rate\n            if self.iteration % 10 == 0:  # Adjust every 10 iterations\n                success_rate = self.success_count / 10\n                if success_rate < self.success_threshold:\n                    self.radius *= self.radius_increase # Increase radius if success rate is low to explore more\n                else:\n                    self.radius *= self.radius_decay # Decrease radius if success rate is high to exploit more\n                \n                self.radius = np.clip(self.radius, 1e-6, 5)  # Keep radius within reasonable bounds\n                self.success_count = 0  # Reset success count\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveRadiusMarginSampling scored 0.331 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2a597676-bc60-41b7-9453-830956bb9327"], "operator": null, "metadata": {"aucs": [0.12217140926511716, 0.18715623739271126, 0.41692772542909207, 0.2567583846689093, 0.24491152088754697, 0.3014502527005246, 0.24696607207334775, 0.2656463583907698, 0.2265083090030161, 0.16530286434740415, 0.3830015377089929, 0.9851003389108998, 0.2900677495934195, 0.24433668455783752, 0.6908045045555319, 0.27876881148131194, 0.2696810201094566, 0.3841721826649013, 0.17091604250075854, 0.49152563590455634]}}
{"id": "b309fec0-05d0-46a6-9ef4-f35ddad8dcad", "fitness": 0.7020441767523764, "name": "AdaptiveDifferentialEvolution", "description": "An adaptive differential evolution algorithm that dynamically adjusts its parameters based on the success rate of previous generations, encouraging exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, adaptation_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.adaptation_rate = adaptation_rate\n        self.success_history_F = []\n        self.success_history_CR = []\n        self.memory_size = 10 \n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        for i in range(self.pop_size):\n            if fitness[i] < self.f_opt:\n                self.f_opt = fitness[i]\n                self.x_opt = self.population[i]\n\n        while self.budget > 0:\n            successful_F = []\n            successful_CR = []\n\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[idxs]\n                mutant = x_r1 + self.F * (x_r2 - x_r3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                crossover = np.random.uniform(size=self.dim) < self.CR\n                trial = np.where(crossover, mutant, self.population[i])\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                if f_trial < fitness[i]:\n                    successful_F.append(self.F)\n                    successful_CR.append(self.CR)\n                    fitness[i] = f_trial\n                    self.population[i] = trial\n            \n            # Adapt parameters\n            if successful_F:\n                self.F = np.mean(successful_F)\n            else:\n                self.F = 0.5  # Reset if no success\n\n            if successful_CR:\n                self.CR = np.mean(successful_CR)\n            else:\n                self.CR = 0.9 # Reset if no success\n\n            # Apply Adaptation Rate\n            self.F = self.F * (1 - self.adaptation_rate) + np.random.normal(0.5, 0.1) * self.adaptation_rate\n            self.CR = self.CR * (1 - self.adaptation_rate) + np.random.normal(0.9, 0.1) * self.adaptation_rate\n            self.F = np.clip(self.F, 0.1, 0.9)\n            self.CR = np.clip(self.CR, 0.1, 1.0)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.702 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["cda08a6d-adea-4430-b67f-76352ae7ae81"], "operator": null, "metadata": {"aucs": [0.3532499989251663, 0.6638709182567037, 0.6821291909810323, 0.8444645663901678, 0.7367566147771991, 0.7912287326273815, 0.6058855515841602, 0.6278585263008452, 0.7282627764955549, 0.6628039275342295, 0.8461100097792151, 0.9998220955275449, 0.6526956798113972, 0.6936406363888117, 0.9182804221984001, 0.7854378932309071, 0.6194263696632893, 0.8157337387577481, 0.4960811565282567, 0.5171447292895173]}}
{"id": "e4f5bad7-58fb-4cd4-8db2-9b8f0c67541e", "fitness": 0.5515174134825431, "name": "AdaptiveDE", "description": "An adaptive differential evolution with a combination of global and local search strategies, adjusting search behavior based on population diversity.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_init=0.5, CR_init=0.9, local_search_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.CR = CR_init\n        self.local_search_rate = local_search_rate\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        for i in range(self.pop_size):\n            if fitness[i] < self.f_opt:\n                self.f_opt = fitness[i]\n                self.x_opt = self.population[i]\n\n        while self.budget > 0:\n            # Calculate population diversity (standard deviation of each dimension)\n            diversity = np.std(self.population, axis=0)\n            diversity_threshold = np.mean(diversity) #Adaptive threshold\n\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[idxs]\n                mutant = x_r1 + self.F * (x_r2 - x_r3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                crossover = np.random.uniform(size=self.dim) < self.CR\n                trial = np.where(crossover, mutant, self.population[i])\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Local Search (only if diversity is low)\n                if np.random.rand() < self.local_search_rate and np.all(diversity < diversity_threshold):\n                   trial = self.x_opt + np.random.normal(0, 0.05, self.dim)  # Perturb around best\n                   trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    self.population[i] = trial\n\n            # Adaptive F and CR (adjust based on diversity)\n            if np.all(diversity < diversity_threshold):  # Low diversity\n                self.F = min(self.F + 0.01, 0.9)  # Increase F to explore more\n                self.CR = max(self.CR - 0.01, 0.1)  # Reduce CR to allow more mutation\n            else:  # High diversity\n                self.F = max(self.F - 0.01, 0.1)  # Reduce F to exploit more\n                self.CR = min(self.CR + 0.01, 0.9)  # Increase CR to use more of the trial vector\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDE scored 0.552 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["cda08a6d-adea-4430-b67f-76352ae7ae81"], "operator": null, "metadata": {"aucs": [0.3099074096511494, 0.36951407329428443, 0.3693445807532012, 0.8983116736516512, 0.5674154481831319, 0.4965850932402399, 0.36708950062291434, 0.3631998262773306, 0.5174069758731923, 0.4694308410948457, 0.8824445214346861, 0.9994021869271114, 0.410564566057843, 0.6393084017477044, 0.6934872417769071, 0.5873652416575805, 0.4593790013742559, 0.6917479315188231, 0.4357253269155196, 0.502718427598493]}}
{"id": "2d877ddc-d399-45c0-869a-e409d9b7000f", "fitness": 0.5202342463996786, "name": "EvolvingSearchStrategies", "description": "A population-based algorithm that dynamically adjusts its search behavior by evolving a population of search strategies, each with unique parameter settings, and then probabilistically selecting and applying the best performing ones.", "code": "import numpy as np\n\nclass EvolvingSearchStrategies:\n    def __init__(self, budget=10000, dim=10, pop_size=50, num_strategies=5, strategy_mutation_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.num_strategies = num_strategies\n        self.strategy_mutation_rate = strategy_mutation_rate\n        self.strategies = self.initialize_strategies()\n        self.strategy_performance = np.zeros(num_strategies)\n        self.population = None\n        self.fitness = None\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def initialize_strategies(self):\n        strategies = []\n        for _ in range(self.num_strategies):\n            strategy = {\n                'F': np.random.uniform(0.1, 1.0),\n                'CR': np.random.uniform(0.1, 1.0),\n                'local_search_prob': np.random.uniform(0.0, 0.2),\n                'local_search_step_size': np.random.uniform(0.01, 0.1)\n            }\n            strategies.append(strategy)\n        return strategies\n\n    def __call__(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(self.fitness)\n        self.x_opt = self.population[best_index]\n        self.f_opt = self.fitness[best_index]\n        \n        while self.budget > 0:\n            new_population = np.copy(self.population)\n            new_fitness = np.copy(self.fitness)\n\n            # Select and apply strategies\n            for i in range(self.pop_size):\n                strategy_id = self.select_strategy()\n                strategy = self.strategies[strategy_id]\n\n                mutant = self.create_mutant(i, strategy, func.bounds.lb, func.bounds.ub)\n                trial = self.crossover(self.population[i], mutant, strategy['CR'])\n                trial = self.local_search(trial, func.bounds.lb, func.bounds.ub, strategy['local_search_prob'], strategy['local_search_step_size'])\n                \n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n                \n                if f_trial < self.fitness[i]:\n                    new_fitness[i] = f_trial\n                    new_population[i] = trial\n                    self.strategy_performance[strategy_id] += (self.fitness[i] - f_trial) # Reward the strategy\n\n            self.population = new_population\n            self.fitness = new_fitness\n\n            # Update strategies\n            self.evolve_strategies()\n            \n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index]\n\n\n        return self.f_opt, self.x_opt\n\n    def select_strategy(self):\n        probabilities = np.exp(self.strategy_performance - np.max(self.strategy_performance))\n        probabilities /= np.sum(probabilities)\n        return np.random.choice(range(self.num_strategies), p=probabilities)\n\n    def create_mutant(self, i, strategy, lb, ub):\n        idxs = np.random.choice(self.pop_size, 3, replace=False)\n        x_r1, x_r2, x_r3 = self.population[idxs]\n        mutant = x_r1 + strategy['F'] * (x_r2 - x_r3)\n        return np.clip(mutant, lb, ub)\n\n    def crossover(self, individual, mutant, CR):\n        crossover = np.random.uniform(size=self.dim) < CR\n        return np.where(crossover, mutant, individual)\n\n    def local_search(self, x, lb, ub, prob, step_size):\n        if np.random.rand() < prob:\n            x_new = x + np.random.uniform(-step_size, step_size, size=self.dim)\n            return np.clip(x_new, lb, ub)\n        return x\n\n    def evolve_strategies(self):\n        for i in range(self.num_strategies):\n            if np.random.rand() < self.strategy_mutation_rate:\n                param_to_mutate = np.random.choice(['F', 'CR', 'local_search_prob', 'local_search_step_size'])\n                if param_to_mutate == 'F':\n                    self.strategies[i]['F'] = np.clip(self.strategies[i]['F'] + np.random.normal(0, 0.1), 0.1, 1.0)\n                elif param_to_mutate == 'CR':\n                    self.strategies[i]['CR'] = np.clip(self.strategies[i]['CR'] + np.random.normal(0, 0.1), 0.1, 1.0)\n                elif param_to_mutate == 'local_search_prob':\n                    self.strategies[i]['local_search_prob'] = np.clip(self.strategies[i]['local_search_prob'] + np.random.normal(0, 0.02), 0.0, 0.2)\n                elif param_to_mutate == 'local_search_step_size':\n                    self.strategies[i]['local_search_step_size'] = np.clip(self.strategies[i]['local_search_step_size'] + np.random.normal(0, 0.005), 0.01, 0.1)\n\n            self.strategy_performance[i] *= 0.9  # Decay performance to favor recent strategies", "configspace": "", "generation": 5, "feedback": "The algorithm EvolvingSearchStrategies scored 0.520 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["eb06b43e-cb7d-4d4a-9fc2-5b62f5a7c13c"], "operator": null, "metadata": {"aucs": [0.16235920936085457, 0.22596864329199828, 0.5474514794199061, 0.8967313808152841, 0.6529084764054404, 0.5359463076763715, 0.3399888805064929, 0.3963992353348963, 0.6393354953459458, 0.23996008941251534, 0.6019862979149113, 0.992453919969435, 0.29414805923116916, 0.7194228154897713, 0.7835066302492226, 0.5630506632033818, 0.367110835114156, 0.7351369425077338, 0.19319306454082474, 0.5176265022032637]}}
{"id": "2e64ed39-dabe-4206-8d0a-1dba67a269ef", "fitness": -Infinity, "name": "ClusteringAdaptiveDE", "description": "A differential evolution algorithm that utilizes a clustering approach to dynamically adapt the mutation strategy based on population diversity and cluster performance.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass ClusteringAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, num_clusters=5, lr_F=0.1, lr_CR=0.1, F_base=0.5, CR_base=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.num_clusters = num_clusters\n        self.lr_F = lr_F\n        self.lr_CR = lr_CR\n        self.F = np.full(pop_size, F_base)\n        self.CR = np.full(pop_size, CR_base)\n        self.F_base = F_base\n        self.CR_base = CR_base\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        \n        for i in range(self.pop_size):\n            if fitness[i] < self.f_opt:\n                self.f_opt = fitness[i]\n                self.x_opt = self.population[i]\n\n        while self.budget > 0:\n            # Clustering\n            kmeans = KMeans(n_clusters=self.num_clusters, random_state=0, n_init=\"auto\").fit(self.population)\n            clusters = [[] for _ in range(self.num_clusters)]\n            for i in range(self.pop_size):\n                clusters[kmeans.labels_[i]].append(i)\n\n            for cluster_id in range(self.num_clusters):\n                cluster_indices = clusters[cluster_id]\n                if not cluster_indices:\n                    continue\n\n                # Differential Evolution within cluster\n                for i in cluster_indices:\n                    # Mutation\n                    idxs = np.random.choice(cluster_indices, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[idxs]\n                    mutant = self.population[i] + self.F[i] * (x_r1 - x_r2) + self.F[i] * (x_r3 - self.population[i])\n                    mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                \n                    # Crossover\n                    crossover = np.random.uniform(size=self.dim) < self.CR[i]\n                    trial = np.where(crossover, mutant, self.population[i])\n                \n                    # Selection\n                    f_trial = func(trial)\n                    self.budget -= 1\n                \n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                    if f_trial < fitness[i]:\n                        # Adaptive Parameter Control\n                        delta_f = fitness[i] - f_trial\n                        self.F[i] = max(0, min(1, self.F[i] + self.lr_F * delta_f, 1.0))\n                        self.CR[i] = max(0, min(1, self.CR[i] + self.lr_CR * delta_f, 1.0))\n                    \n                        fitness[i] = f_trial\n                        self.population[i] = trial\n                    else:\n                        #If no improvement, revert to base values, encouraging exploration\n                        self.F[i] = self.F_base\n                        self.CR[i] = self.CR_base\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 31, in __call__\nNameError: name 'KMeans' is not defined\n.", "error": "", "parent_ids": ["9e5611cf-797c-4810-b292-caedfb43542e"], "operator": null, "metadata": {}}
{"id": "6f537959-23b8-40a3-a0c4-ac4f752bb9ba", "fitness": 0.0, "name": "SelfOrganizingSearch", "description": "A self-organizing algorithm that adaptively adjusts search behavior based on local neighborhood fitness comparisons and dynamically allocates budget to promising regions.", "code": "import numpy as np\n\nclass SelfOrganizingSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=40, neighborhood_size=5, learning_rate=0.1, exploration_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.neighborhood_size = neighborhood_size\n        self.learning_rate = learning_rate\n        self.exploration_prob = exploration_prob\n        self.population = None\n        self.fitness = None\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def __call__(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(self.fitness)\n        self.x_opt = self.population[best_index]\n        self.f_opt = self.fitness[best_index]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Select a random neighbor\n                neighbors = np.random.choice(self.pop_size, self.neighborhood_size, replace=False)\n                best_neighbor_index = neighbors[np.argmin(self.fitness[neighbors])]\n\n                # Adaptation based on neighbor comparison\n                if self.fitness[i] > self.fitness[best_neighbor_index]:\n                    # Move towards the better neighbor with a learning rate\n                    direction = self.population[best_neighbor_index] - self.population[i]\n                    new_position = self.population[i] + self.learning_rate * direction\n                    new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)  # Keep within bounds\n\n                    # Explore with a small probability\n                    if np.random.rand() < self.exploration_prob:\n                        new_position = np.random.uniform(func.bounds.lb, func.bounds.ub)\n\n                    f_new = func(new_position)\n                    self.budget -= 1\n\n                    if f_new < self.f_opt:\n                        self.f_opt = f_new\n                        self.x_opt = new_position\n\n                    if f_new < self.fitness[i]:\n                        self.fitness[i] = f_new\n                        self.population[i] = new_position\n\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm SelfOrganizingSearch scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2d877ddc-d399-45c0-869a-e409d9b7000f"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "fa595cc2-1b78-482e-a37b-72906c644d2e", "fitness": 0.0, "name": "DynamicPopulationDE", "description": "A differential evolution algorithm that uses a dynamically adjusted population size and a probabilistic local search strategy to enhance exploration and exploitation.", "code": "import numpy as np\n\nclass DynamicPopulationDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, F=0.5, CR=0.9, local_search_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.initial_pop_size = initial_pop_size\n        self.F = F\n        self.CR = CR\n        self.local_search_prob = local_search_prob\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        \n        for i in range(self.pop_size):\n            if fitness[i] < self.f_opt:\n                self.f_opt = fitness[i]\n                self.x_opt = self.population[i]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(self.population)\n            new_fitness = np.copy(fitness)\n\n            for i in range(self.pop_size):\n                # Differential Evolution\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[idxs]\n                mutant = x_r1 + self.F * (x_r2 - x_r3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                \n                crossover = np.random.uniform(size=self.dim) < self.CR\n                trial = np.where(crossover, mutant, self.population[i])\n                \n                # Local Search with Probability\n                if np.random.rand() < self.local_search_prob:\n                    trial = trial + np.random.normal(0, 0.05, size=self.dim)\n                    trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n                \n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n                    \n                if f_trial < fitness[i]:\n                    new_fitness[i] = f_trial\n                    new_population[i] = trial\n                else:\n                    new_fitness[i] = fitness[i]\n                    new_population[i] = self.population[i]\n\n            self.population = new_population\n            fitness = new_fitness\n            \n            # Adjust population size dynamically\n            if generation % 10 == 0:\n                if np.mean(fitness) < np.mean([func(np.random.uniform(func.bounds.lb, func.bounds.ub)) for _ in range(100)]):\n                    self.pop_size = min(2 * self.pop_size, self.initial_pop_size * 3) # Increase pop size if doing well\n                    self.population = np.concatenate((self.population, np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size - len(self.population), self.dim))))\n                    fitness = np.concatenate((fitness, np.array([func(x) for x in self.population[len(fitness):]])))\n                    self.budget -= (self.pop_size - len(fitness))\n                else:\n                    self.pop_size = max(self.initial_pop_size, self.pop_size // 2) # Decrease pop size if not improving\n                    self.population = self.population[:self.pop_size]\n                    fitness = fitness[:self.pop_size]\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm DynamicPopulationDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9e5611cf-797c-4810-b292-caedfb43542e"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "fe5daf2f-646d-457f-9330-6bb87193b4c1", "fitness": 0.46048054718574266, "name": "SOM_DE", "description": "A metaheuristic algorithm combining a self-organizing map (SOM) for population diversification with differential evolution (DE) for exploitation, adapting DE parameters based on SOM performance.", "code": "import numpy as np\n\nclass SOM_DE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, som_grid_size=10, de_mutation_factor=0.5, de_crossover_rate=0.7, som_learning_rate=0.1, som_sigma=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.som_grid_size = som_grid_size\n        self.de_mutation_factor = de_mutation_factor\n        self.de_crossover_rate = de_crossover_rate\n        self.som_learning_rate = som_learning_rate\n        self.som_sigma = som_sigma\n        self.som = None\n        self.population = None\n        self.fitness = None\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def initialize_som(self, lb, ub):\n        # Initialize SOM weights randomly within bounds\n        self.som = np.random.uniform(lb, ub, size=(self.som_grid_size, self.som_grid_size, self.dim))\n\n    def find_closest_node(self, x):\n        # Find the best matching unit (BMU) for input x\n        distances = np.sum((self.som - x)**2, axis=2)  # Euclidean distance along the feature dimension\n        bmu_index = np.unravel_index(np.argmin(distances), distances.shape)\n        return bmu_index\n\n    def update_som(self, x, bmu_index):\n        # Update SOM weights based on input x and BMU\n        for i in range(self.som_grid_size):\n            for j in range(self.som_grid_size):\n                distance = np.sqrt((i - bmu_index[0])**2 + (j - bmu_index[1])**2)\n                influence = np.exp(-distance**2 / (2 * self.som_sigma**2))\n                self.som[i, j] += self.som_learning_rate * influence * (x - self.som[i, j])\n\n    def initialize_population(self, lb, ub):\n        # Initialize population using SOM as a distribution\n        self.population = np.zeros((self.pop_size, self.dim))\n        for i in range(self.pop_size):\n            node_index = (np.random.randint(0, self.som_grid_size), np.random.randint(0, self.som_grid_size))\n            self.population[i] = self.som[node_index] + np.random.normal(0, 0.1*(ub-lb), size=self.dim) #Add a bit of noise\n            self.population[i] = np.clip(self.population[i], lb, ub) # Keep inside bounds\n\n    def differential_evolution(self, func, lb, ub):\n        new_population = np.copy(self.population)\n        new_fitness = np.copy(self.fitness)\n\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[idxs]\n            mutant = x_r1 + self.de_mutation_factor * (x_r2 - x_r3)\n            mutant = np.clip(mutant, lb, ub)\n            \n            crossover = np.random.uniform(size=self.dim) < self.de_crossover_rate\n            trial = np.where(crossover, mutant, self.population[i])\n\n            f_trial = func(trial)\n            self.budget -= 1\n            \n            if f_trial < self.f_opt:\n                self.f_opt = f_trial\n                self.x_opt = trial\n            \n            if f_trial < self.fitness[i]:\n                new_fitness[i] = f_trial\n                new_population[i] = trial\n\n        self.population = new_population\n        self.fitness = new_fitness\n            \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_som(lb, ub)\n\n        # Initial SOM training with random samples\n        for _ in range(self.pop_size):\n          x = np.random.uniform(lb, ub, size=self.dim)\n          bmu_index = self.find_closest_node(x)\n          self.update_som(x, bmu_index)\n\n        self.initialize_population(lb, ub) # SOM-informed initialization\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(self.fitness)\n        self.x_opt = self.population[best_index]\n        self.f_opt = self.fitness[best_index]\n        \n        while self.budget > 0:\n            self.differential_evolution(func, lb, ub)\n\n            # Update SOM with the improved population\n            for x in self.population:\n                bmu_index = self.find_closest_node(x)\n                self.update_som(x, bmu_index)\n\n            # Adjust DE parameters based on the SOM quality\n            # Heuristic: If SOM is highly structured (low variance in node distances), reduce mutation\n            node_distances = []\n            for i in range(self.som_grid_size):\n              for j in range(self.som_grid_size):\n                for k in range(i, self.som_grid_size):\n                  for l in range(j+1, self.som_grid_size):\n                    node_distances.append(np.linalg.norm(self.som[i,j]-self.som[k,l])) #All distances\n\n            if len(node_distances) > 0:\n              som_distance_std = np.std(node_distances)\n            else:\n              som_distance_std = 0.1  #small default\n            \n            self.de_mutation_factor = np.clip(self.de_mutation_factor * (1 + np.random.normal(0, 0.1 * som_distance_std)), 0.1, 1.0)\n            self.de_crossover_rate = np.clip(self.de_crossover_rate * (1 - np.random.normal(0, 0.1 * som_distance_std)), 0.1, 1.0)\n           \n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm SOM_DE scored 0.460 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2d877ddc-d399-45c0-869a-e409d9b7000f"], "operator": null, "metadata": {"aucs": [0.19082679361447086, 0.2517396484361031, 0.42883376674689466, 0.729410488005722, 0.4500853627295527, 0.5530510088893994, 0.4112895801960129, 0.48849412337697873, 0.37742219810572364, 0.2611892546475435, 0.33875544364641874, 0.9946395428060394, 0.3231845155131663, 0.35830297992305193, 0.6829342278409442, 0.4869118672120486, 0.3899169917645189, 0.7802940390316977, 0.19880983209823044, 0.5135192791303346]}}
{"id": "fe5daf2f-646d-457f-9330-6bb87193b4c1", "fitness": 0.46048054718574266, "name": "SOM_DE", "description": "A metaheuristic algorithm combining a self-organizing map (SOM) for population diversification with differential evolution (DE) for exploitation, adapting DE parameters based on SOM performance.", "code": "import numpy as np\n\nclass SOM_DE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, som_grid_size=10, de_mutation_factor=0.5, de_crossover_rate=0.7, som_learning_rate=0.1, som_sigma=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.som_grid_size = som_grid_size\n        self.de_mutation_factor = de_mutation_factor\n        self.de_crossover_rate = de_crossover_rate\n        self.som_learning_rate = som_learning_rate\n        self.som_sigma = som_sigma\n        self.som = None\n        self.population = None\n        self.fitness = None\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def initialize_som(self, lb, ub):\n        # Initialize SOM weights randomly within bounds\n        self.som = np.random.uniform(lb, ub, size=(self.som_grid_size, self.som_grid_size, self.dim))\n\n    def find_closest_node(self, x):\n        # Find the best matching unit (BMU) for input x\n        distances = np.sum((self.som - x)**2, axis=2)  # Euclidean distance along the feature dimension\n        bmu_index = np.unravel_index(np.argmin(distances), distances.shape)\n        return bmu_index\n\n    def update_som(self, x, bmu_index):\n        # Update SOM weights based on input x and BMU\n        for i in range(self.som_grid_size):\n            for j in range(self.som_grid_size):\n                distance = np.sqrt((i - bmu_index[0])**2 + (j - bmu_index[1])**2)\n                influence = np.exp(-distance**2 / (2 * self.som_sigma**2))\n                self.som[i, j] += self.som_learning_rate * influence * (x - self.som[i, j])\n\n    def initialize_population(self, lb, ub):\n        # Initialize population using SOM as a distribution\n        self.population = np.zeros((self.pop_size, self.dim))\n        for i in range(self.pop_size):\n            node_index = (np.random.randint(0, self.som_grid_size), np.random.randint(0, self.som_grid_size))\n            self.population[i] = self.som[node_index] + np.random.normal(0, 0.1*(ub-lb), size=self.dim) #Add a bit of noise\n            self.population[i] = np.clip(self.population[i], lb, ub) # Keep inside bounds\n\n    def differential_evolution(self, func, lb, ub):\n        new_population = np.copy(self.population)\n        new_fitness = np.copy(self.fitness)\n\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[idxs]\n            mutant = x_r1 + self.de_mutation_factor * (x_r2 - x_r3)\n            mutant = np.clip(mutant, lb, ub)\n            \n            crossover = np.random.uniform(size=self.dim) < self.de_crossover_rate\n            trial = np.where(crossover, mutant, self.population[i])\n\n            f_trial = func(trial)\n            self.budget -= 1\n            \n            if f_trial < self.f_opt:\n                self.f_opt = f_trial\n                self.x_opt = trial\n            \n            if f_trial < self.fitness[i]:\n                new_fitness[i] = f_trial\n                new_population[i] = trial\n\n        self.population = new_population\n        self.fitness = new_fitness\n            \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_som(lb, ub)\n\n        # Initial SOM training with random samples\n        for _ in range(self.pop_size):\n          x = np.random.uniform(lb, ub, size=self.dim)\n          bmu_index = self.find_closest_node(x)\n          self.update_som(x, bmu_index)\n\n        self.initialize_population(lb, ub) # SOM-informed initialization\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(self.fitness)\n        self.x_opt = self.population[best_index]\n        self.f_opt = self.fitness[best_index]\n        \n        while self.budget > 0:\n            self.differential_evolution(func, lb, ub)\n\n            # Update SOM with the improved population\n            for x in self.population:\n                bmu_index = self.find_closest_node(x)\n                self.update_som(x, bmu_index)\n\n            # Adjust DE parameters based on the SOM quality\n            # Heuristic: If SOM is highly structured (low variance in node distances), reduce mutation\n            node_distances = []\n            for i in range(self.som_grid_size):\n              for j in range(self.som_grid_size):\n                for k in range(i, self.som_grid_size):\n                  for l in range(j+1, self.som_grid_size):\n                    node_distances.append(np.linalg.norm(self.som[i,j]-self.som[k,l])) #All distances\n\n            if len(node_distances) > 0:\n              som_distance_std = np.std(node_distances)\n            else:\n              som_distance_std = 0.1  #small default\n            \n            self.de_mutation_factor = np.clip(self.de_mutation_factor * (1 + np.random.normal(0, 0.1 * som_distance_std)), 0.1, 1.0)\n            self.de_crossover_rate = np.clip(self.de_crossover_rate * (1 - np.random.normal(0, 0.1 * som_distance_std)), 0.1, 1.0)\n           \n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm SOM_DE scored 0.460 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2d877ddc-d399-45c0-869a-e409d9b7000f"], "operator": null, "metadata": {"aucs": [0.19082679361447086, 0.2517396484361031, 0.42883376674689466, 0.729410488005722, 0.4500853627295527, 0.5530510088893994, 0.4112895801960129, 0.48849412337697873, 0.37742219810572364, 0.2611892546475435, 0.33875544364641874, 0.9946395428060394, 0.3231845155131663, 0.35830297992305193, 0.6829342278409442, 0.4869118672120486, 0.3899169917645189, 0.7802940390316977, 0.19880983209823044, 0.5135192791303346]}}
{"id": "fe5daf2f-646d-457f-9330-6bb87193b4c1", "fitness": 0.46048054718574266, "name": "SOM_DE", "description": "A metaheuristic algorithm combining a self-organizing map (SOM) for population diversification with differential evolution (DE) for exploitation, adapting DE parameters based on SOM performance.", "code": "import numpy as np\n\nclass SOM_DE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, som_grid_size=10, de_mutation_factor=0.5, de_crossover_rate=0.7, som_learning_rate=0.1, som_sigma=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.som_grid_size = som_grid_size\n        self.de_mutation_factor = de_mutation_factor\n        self.de_crossover_rate = de_crossover_rate\n        self.som_learning_rate = som_learning_rate\n        self.som_sigma = som_sigma\n        self.som = None\n        self.population = None\n        self.fitness = None\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def initialize_som(self, lb, ub):\n        # Initialize SOM weights randomly within bounds\n        self.som = np.random.uniform(lb, ub, size=(self.som_grid_size, self.som_grid_size, self.dim))\n\n    def find_closest_node(self, x):\n        # Find the best matching unit (BMU) for input x\n        distances = np.sum((self.som - x)**2, axis=2)  # Euclidean distance along the feature dimension\n        bmu_index = np.unravel_index(np.argmin(distances), distances.shape)\n        return bmu_index\n\n    def update_som(self, x, bmu_index):\n        # Update SOM weights based on input x and BMU\n        for i in range(self.som_grid_size):\n            for j in range(self.som_grid_size):\n                distance = np.sqrt((i - bmu_index[0])**2 + (j - bmu_index[1])**2)\n                influence = np.exp(-distance**2 / (2 * self.som_sigma**2))\n                self.som[i, j] += self.som_learning_rate * influence * (x - self.som[i, j])\n\n    def initialize_population(self, lb, ub):\n        # Initialize population using SOM as a distribution\n        self.population = np.zeros((self.pop_size, self.dim))\n        for i in range(self.pop_size):\n            node_index = (np.random.randint(0, self.som_grid_size), np.random.randint(0, self.som_grid_size))\n            self.population[i] = self.som[node_index] + np.random.normal(0, 0.1*(ub-lb), size=self.dim) #Add a bit of noise\n            self.population[i] = np.clip(self.population[i], lb, ub) # Keep inside bounds\n\n    def differential_evolution(self, func, lb, ub):\n        new_population = np.copy(self.population)\n        new_fitness = np.copy(self.fitness)\n\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[idxs]\n            mutant = x_r1 + self.de_mutation_factor * (x_r2 - x_r3)\n            mutant = np.clip(mutant, lb, ub)\n            \n            crossover = np.random.uniform(size=self.dim) < self.de_crossover_rate\n            trial = np.where(crossover, mutant, self.population[i])\n\n            f_trial = func(trial)\n            self.budget -= 1\n            \n            if f_trial < self.f_opt:\n                self.f_opt = f_trial\n                self.x_opt = trial\n            \n            if f_trial < self.fitness[i]:\n                new_fitness[i] = f_trial\n                new_population[i] = trial\n\n        self.population = new_population\n        self.fitness = new_fitness\n            \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_som(lb, ub)\n\n        # Initial SOM training with random samples\n        for _ in range(self.pop_size):\n          x = np.random.uniform(lb, ub, size=self.dim)\n          bmu_index = self.find_closest_node(x)\n          self.update_som(x, bmu_index)\n\n        self.initialize_population(lb, ub) # SOM-informed initialization\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(self.fitness)\n        self.x_opt = self.population[best_index]\n        self.f_opt = self.fitness[best_index]\n        \n        while self.budget > 0:\n            self.differential_evolution(func, lb, ub)\n\n            # Update SOM with the improved population\n            for x in self.population:\n                bmu_index = self.find_closest_node(x)\n                self.update_som(x, bmu_index)\n\n            # Adjust DE parameters based on the SOM quality\n            # Heuristic: If SOM is highly structured (low variance in node distances), reduce mutation\n            node_distances = []\n            for i in range(self.som_grid_size):\n              for j in range(self.som_grid_size):\n                for k in range(i, self.som_grid_size):\n                  for l in range(j+1, self.som_grid_size):\n                    node_distances.append(np.linalg.norm(self.som[i,j]-self.som[k,l])) #All distances\n\n            if len(node_distances) > 0:\n              som_distance_std = np.std(node_distances)\n            else:\n              som_distance_std = 0.1  #small default\n            \n            self.de_mutation_factor = np.clip(self.de_mutation_factor * (1 + np.random.normal(0, 0.1 * som_distance_std)), 0.1, 1.0)\n            self.de_crossover_rate = np.clip(self.de_crossover_rate * (1 - np.random.normal(0, 0.1 * som_distance_std)), 0.1, 1.0)\n           \n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm SOM_DE scored 0.460 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2d877ddc-d399-45c0-869a-e409d9b7000f"], "operator": null, "metadata": {"aucs": [0.19082679361447086, 0.2517396484361031, 0.42883376674689466, 0.729410488005722, 0.4500853627295527, 0.5530510088893994, 0.4112895801960129, 0.48849412337697873, 0.37742219810572364, 0.2611892546475435, 0.33875544364641874, 0.9946395428060394, 0.3231845155131663, 0.35830297992305193, 0.6829342278409442, 0.4869118672120486, 0.3899169917645189, 0.7802940390316977, 0.19880983209823044, 0.5135192791303346]}}
{"id": "fe5daf2f-646d-457f-9330-6bb87193b4c1", "fitness": 0.46048054718574266, "name": "SOM_DE", "description": "A metaheuristic algorithm combining a self-organizing map (SOM) for population diversification with differential evolution (DE) for exploitation, adapting DE parameters based on SOM performance.", "code": "import numpy as np\n\nclass SOM_DE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, som_grid_size=10, de_mutation_factor=0.5, de_crossover_rate=0.7, som_learning_rate=0.1, som_sigma=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.som_grid_size = som_grid_size\n        self.de_mutation_factor = de_mutation_factor\n        self.de_crossover_rate = de_crossover_rate\n        self.som_learning_rate = som_learning_rate\n        self.som_sigma = som_sigma\n        self.som = None\n        self.population = None\n        self.fitness = None\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def initialize_som(self, lb, ub):\n        # Initialize SOM weights randomly within bounds\n        self.som = np.random.uniform(lb, ub, size=(self.som_grid_size, self.som_grid_size, self.dim))\n\n    def find_closest_node(self, x):\n        # Find the best matching unit (BMU) for input x\n        distances = np.sum((self.som - x)**2, axis=2)  # Euclidean distance along the feature dimension\n        bmu_index = np.unravel_index(np.argmin(distances), distances.shape)\n        return bmu_index\n\n    def update_som(self, x, bmu_index):\n        # Update SOM weights based on input x and BMU\n        for i in range(self.som_grid_size):\n            for j in range(self.som_grid_size):\n                distance = np.sqrt((i - bmu_index[0])**2 + (j - bmu_index[1])**2)\n                influence = np.exp(-distance**2 / (2 * self.som_sigma**2))\n                self.som[i, j] += self.som_learning_rate * influence * (x - self.som[i, j])\n\n    def initialize_population(self, lb, ub):\n        # Initialize population using SOM as a distribution\n        self.population = np.zeros((self.pop_size, self.dim))\n        for i in range(self.pop_size):\n            node_index = (np.random.randint(0, self.som_grid_size), np.random.randint(0, self.som_grid_size))\n            self.population[i] = self.som[node_index] + np.random.normal(0, 0.1*(ub-lb), size=self.dim) #Add a bit of noise\n            self.population[i] = np.clip(self.population[i], lb, ub) # Keep inside bounds\n\n    def differential_evolution(self, func, lb, ub):\n        new_population = np.copy(self.population)\n        new_fitness = np.copy(self.fitness)\n\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[idxs]\n            mutant = x_r1 + self.de_mutation_factor * (x_r2 - x_r3)\n            mutant = np.clip(mutant, lb, ub)\n            \n            crossover = np.random.uniform(size=self.dim) < self.de_crossover_rate\n            trial = np.where(crossover, mutant, self.population[i])\n\n            f_trial = func(trial)\n            self.budget -= 1\n            \n            if f_trial < self.f_opt:\n                self.f_opt = f_trial\n                self.x_opt = trial\n            \n            if f_trial < self.fitness[i]:\n                new_fitness[i] = f_trial\n                new_population[i] = trial\n\n        self.population = new_population\n        self.fitness = new_fitness\n            \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_som(lb, ub)\n\n        # Initial SOM training with random samples\n        for _ in range(self.pop_size):\n          x = np.random.uniform(lb, ub, size=self.dim)\n          bmu_index = self.find_closest_node(x)\n          self.update_som(x, bmu_index)\n\n        self.initialize_population(lb, ub) # SOM-informed initialization\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(self.fitness)\n        self.x_opt = self.population[best_index]\n        self.f_opt = self.fitness[best_index]\n        \n        while self.budget > 0:\n            self.differential_evolution(func, lb, ub)\n\n            # Update SOM with the improved population\n            for x in self.population:\n                bmu_index = self.find_closest_node(x)\n                self.update_som(x, bmu_index)\n\n            # Adjust DE parameters based on the SOM quality\n            # Heuristic: If SOM is highly structured (low variance in node distances), reduce mutation\n            node_distances = []\n            for i in range(self.som_grid_size):\n              for j in range(self.som_grid_size):\n                for k in range(i, self.som_grid_size):\n                  for l in range(j+1, self.som_grid_size):\n                    node_distances.append(np.linalg.norm(self.som[i,j]-self.som[k,l])) #All distances\n\n            if len(node_distances) > 0:\n              som_distance_std = np.std(node_distances)\n            else:\n              som_distance_std = 0.1  #small default\n            \n            self.de_mutation_factor = np.clip(self.de_mutation_factor * (1 + np.random.normal(0, 0.1 * som_distance_std)), 0.1, 1.0)\n            self.de_crossover_rate = np.clip(self.de_crossover_rate * (1 - np.random.normal(0, 0.1 * som_distance_std)), 0.1, 1.0)\n           \n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm SOM_DE scored 0.460 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2d877ddc-d399-45c0-869a-e409d9b7000f"], "operator": null, "metadata": {"aucs": [0.19082679361447086, 0.2517396484361031, 0.42883376674689466, 0.729410488005722, 0.4500853627295527, 0.5530510088893994, 0.4112895801960129, 0.48849412337697873, 0.37742219810572364, 0.2611892546475435, 0.33875544364641874, 0.9946395428060394, 0.3231845155131663, 0.35830297992305193, 0.6829342278409442, 0.4869118672120486, 0.3899169917645189, 0.7802940390316977, 0.19880983209823044, 0.5135192791303346]}}
{"id": "fe5daf2f-646d-457f-9330-6bb87193b4c1", "fitness": 0.46048054718574266, "name": "SOM_DE", "description": "A metaheuristic algorithm combining a self-organizing map (SOM) for population diversification with differential evolution (DE) for exploitation, adapting DE parameters based on SOM performance.", "code": "import numpy as np\n\nclass SOM_DE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, som_grid_size=10, de_mutation_factor=0.5, de_crossover_rate=0.7, som_learning_rate=0.1, som_sigma=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.som_grid_size = som_grid_size\n        self.de_mutation_factor = de_mutation_factor\n        self.de_crossover_rate = de_crossover_rate\n        self.som_learning_rate = som_learning_rate\n        self.som_sigma = som_sigma\n        self.som = None\n        self.population = None\n        self.fitness = None\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def initialize_som(self, lb, ub):\n        # Initialize SOM weights randomly within bounds\n        self.som = np.random.uniform(lb, ub, size=(self.som_grid_size, self.som_grid_size, self.dim))\n\n    def find_closest_node(self, x):\n        # Find the best matching unit (BMU) for input x\n        distances = np.sum((self.som - x)**2, axis=2)  # Euclidean distance along the feature dimension\n        bmu_index = np.unravel_index(np.argmin(distances), distances.shape)\n        return bmu_index\n\n    def update_som(self, x, bmu_index):\n        # Update SOM weights based on input x and BMU\n        for i in range(self.som_grid_size):\n            for j in range(self.som_grid_size):\n                distance = np.sqrt((i - bmu_index[0])**2 + (j - bmu_index[1])**2)\n                influence = np.exp(-distance**2 / (2 * self.som_sigma**2))\n                self.som[i, j] += self.som_learning_rate * influence * (x - self.som[i, j])\n\n    def initialize_population(self, lb, ub):\n        # Initialize population using SOM as a distribution\n        self.population = np.zeros((self.pop_size, self.dim))\n        for i in range(self.pop_size):\n            node_index = (np.random.randint(0, self.som_grid_size), np.random.randint(0, self.som_grid_size))\n            self.population[i] = self.som[node_index] + np.random.normal(0, 0.1*(ub-lb), size=self.dim) #Add a bit of noise\n            self.population[i] = np.clip(self.population[i], lb, ub) # Keep inside bounds\n\n    def differential_evolution(self, func, lb, ub):\n        new_population = np.copy(self.population)\n        new_fitness = np.copy(self.fitness)\n\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[idxs]\n            mutant = x_r1 + self.de_mutation_factor * (x_r2 - x_r3)\n            mutant = np.clip(mutant, lb, ub)\n            \n            crossover = np.random.uniform(size=self.dim) < self.de_crossover_rate\n            trial = np.where(crossover, mutant, self.population[i])\n\n            f_trial = func(trial)\n            self.budget -= 1\n            \n            if f_trial < self.f_opt:\n                self.f_opt = f_trial\n                self.x_opt = trial\n            \n            if f_trial < self.fitness[i]:\n                new_fitness[i] = f_trial\n                new_population[i] = trial\n\n        self.population = new_population\n        self.fitness = new_fitness\n            \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_som(lb, ub)\n\n        # Initial SOM training with random samples\n        for _ in range(self.pop_size):\n          x = np.random.uniform(lb, ub, size=self.dim)\n          bmu_index = self.find_closest_node(x)\n          self.update_som(x, bmu_index)\n\n        self.initialize_population(lb, ub) # SOM-informed initialization\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(self.fitness)\n        self.x_opt = self.population[best_index]\n        self.f_opt = self.fitness[best_index]\n        \n        while self.budget > 0:\n            self.differential_evolution(func, lb, ub)\n\n            # Update SOM with the improved population\n            for x in self.population:\n                bmu_index = self.find_closest_node(x)\n                self.update_som(x, bmu_index)\n\n            # Adjust DE parameters based on the SOM quality\n            # Heuristic: If SOM is highly structured (low variance in node distances), reduce mutation\n            node_distances = []\n            for i in range(self.som_grid_size):\n              for j in range(self.som_grid_size):\n                for k in range(i, self.som_grid_size):\n                  for l in range(j+1, self.som_grid_size):\n                    node_distances.append(np.linalg.norm(self.som[i,j]-self.som[k,l])) #All distances\n\n            if len(node_distances) > 0:\n              som_distance_std = np.std(node_distances)\n            else:\n              som_distance_std = 0.1  #small default\n            \n            self.de_mutation_factor = np.clip(self.de_mutation_factor * (1 + np.random.normal(0, 0.1 * som_distance_std)), 0.1, 1.0)\n            self.de_crossover_rate = np.clip(self.de_crossover_rate * (1 - np.random.normal(0, 0.1 * som_distance_std)), 0.1, 1.0)\n           \n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm SOM_DE scored 0.460 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2d877ddc-d399-45c0-869a-e409d9b7000f"], "operator": null, "metadata": {"aucs": [0.19082679361447086, 0.2517396484361031, 0.42883376674689466, 0.729410488005722, 0.4500853627295527, 0.5530510088893994, 0.4112895801960129, 0.48849412337697873, 0.37742219810572364, 0.2611892546475435, 0.33875544364641874, 0.9946395428060394, 0.3231845155131663, 0.35830297992305193, 0.6829342278409442, 0.4869118672120486, 0.3899169917645189, 0.7802940390316977, 0.19880983209823044, 0.5135192791303346]}}
{"id": "fe5daf2f-646d-457f-9330-6bb87193b4c1", "fitness": 0.46048054718574266, "name": "SOM_DE", "description": "A metaheuristic algorithm combining a self-organizing map (SOM) for population diversification with differential evolution (DE) for exploitation, adapting DE parameters based on SOM performance.", "code": "import numpy as np\n\nclass SOM_DE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, som_grid_size=10, de_mutation_factor=0.5, de_crossover_rate=0.7, som_learning_rate=0.1, som_sigma=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.som_grid_size = som_grid_size\n        self.de_mutation_factor = de_mutation_factor\n        self.de_crossover_rate = de_crossover_rate\n        self.som_learning_rate = som_learning_rate\n        self.som_sigma = som_sigma\n        self.som = None\n        self.population = None\n        self.fitness = None\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def initialize_som(self, lb, ub):\n        # Initialize SOM weights randomly within bounds\n        self.som = np.random.uniform(lb, ub, size=(self.som_grid_size, self.som_grid_size, self.dim))\n\n    def find_closest_node(self, x):\n        # Find the best matching unit (BMU) for input x\n        distances = np.sum((self.som - x)**2, axis=2)  # Euclidean distance along the feature dimension\n        bmu_index = np.unravel_index(np.argmin(distances), distances.shape)\n        return bmu_index\n\n    def update_som(self, x, bmu_index):\n        # Update SOM weights based on input x and BMU\n        for i in range(self.som_grid_size):\n            for j in range(self.som_grid_size):\n                distance = np.sqrt((i - bmu_index[0])**2 + (j - bmu_index[1])**2)\n                influence = np.exp(-distance**2 / (2 * self.som_sigma**2))\n                self.som[i, j] += self.som_learning_rate * influence * (x - self.som[i, j])\n\n    def initialize_population(self, lb, ub):\n        # Initialize population using SOM as a distribution\n        self.population = np.zeros((self.pop_size, self.dim))\n        for i in range(self.pop_size):\n            node_index = (np.random.randint(0, self.som_grid_size), np.random.randint(0, self.som_grid_size))\n            self.population[i] = self.som[node_index] + np.random.normal(0, 0.1*(ub-lb), size=self.dim) #Add a bit of noise\n            self.population[i] = np.clip(self.population[i], lb, ub) # Keep inside bounds\n\n    def differential_evolution(self, func, lb, ub):\n        new_population = np.copy(self.population)\n        new_fitness = np.copy(self.fitness)\n\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[idxs]\n            mutant = x_r1 + self.de_mutation_factor * (x_r2 - x_r3)\n            mutant = np.clip(mutant, lb, ub)\n            \n            crossover = np.random.uniform(size=self.dim) < self.de_crossover_rate\n            trial = np.where(crossover, mutant, self.population[i])\n\n            f_trial = func(trial)\n            self.budget -= 1\n            \n            if f_trial < self.f_opt:\n                self.f_opt = f_trial\n                self.x_opt = trial\n            \n            if f_trial < self.fitness[i]:\n                new_fitness[i] = f_trial\n                new_population[i] = trial\n\n        self.population = new_population\n        self.fitness = new_fitness\n            \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_som(lb, ub)\n\n        # Initial SOM training with random samples\n        for _ in range(self.pop_size):\n          x = np.random.uniform(lb, ub, size=self.dim)\n          bmu_index = self.find_closest_node(x)\n          self.update_som(x, bmu_index)\n\n        self.initialize_population(lb, ub) # SOM-informed initialization\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(self.fitness)\n        self.x_opt = self.population[best_index]\n        self.f_opt = self.fitness[best_index]\n        \n        while self.budget > 0:\n            self.differential_evolution(func, lb, ub)\n\n            # Update SOM with the improved population\n            for x in self.population:\n                bmu_index = self.find_closest_node(x)\n                self.update_som(x, bmu_index)\n\n            # Adjust DE parameters based on the SOM quality\n            # Heuristic: If SOM is highly structured (low variance in node distances), reduce mutation\n            node_distances = []\n            for i in range(self.som_grid_size):\n              for j in range(self.som_grid_size):\n                for k in range(i, self.som_grid_size):\n                  for l in range(j+1, self.som_grid_size):\n                    node_distances.append(np.linalg.norm(self.som[i,j]-self.som[k,l])) #All distances\n\n            if len(node_distances) > 0:\n              som_distance_std = np.std(node_distances)\n            else:\n              som_distance_std = 0.1  #small default\n            \n            self.de_mutation_factor = np.clip(self.de_mutation_factor * (1 + np.random.normal(0, 0.1 * som_distance_std)), 0.1, 1.0)\n            self.de_crossover_rate = np.clip(self.de_crossover_rate * (1 - np.random.normal(0, 0.1 * som_distance_std)), 0.1, 1.0)\n           \n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm SOM_DE scored 0.460 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2d877ddc-d399-45c0-869a-e409d9b7000f"], "operator": null, "metadata": {"aucs": [0.19082679361447086, 0.2517396484361031, 0.42883376674689466, 0.729410488005722, 0.4500853627295527, 0.5530510088893994, 0.4112895801960129, 0.48849412337697873, 0.37742219810572364, 0.2611892546475435, 0.33875544364641874, 0.9946395428060394, 0.3231845155131663, 0.35830297992305193, 0.6829342278409442, 0.4869118672120486, 0.3899169917645189, 0.7802940390316977, 0.19880983209823044, 0.5135192791303346]}}
{"id": "fe5daf2f-646d-457f-9330-6bb87193b4c1", "fitness": 0.46048054718574266, "name": "SOM_DE", "description": "A metaheuristic algorithm combining a self-organizing map (SOM) for population diversification with differential evolution (DE) for exploitation, adapting DE parameters based on SOM performance.", "code": "import numpy as np\n\nclass SOM_DE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, som_grid_size=10, de_mutation_factor=0.5, de_crossover_rate=0.7, som_learning_rate=0.1, som_sigma=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.som_grid_size = som_grid_size\n        self.de_mutation_factor = de_mutation_factor\n        self.de_crossover_rate = de_crossover_rate\n        self.som_learning_rate = som_learning_rate\n        self.som_sigma = som_sigma\n        self.som = None\n        self.population = None\n        self.fitness = None\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def initialize_som(self, lb, ub):\n        # Initialize SOM weights randomly within bounds\n        self.som = np.random.uniform(lb, ub, size=(self.som_grid_size, self.som_grid_size, self.dim))\n\n    def find_closest_node(self, x):\n        # Find the best matching unit (BMU) for input x\n        distances = np.sum((self.som - x)**2, axis=2)  # Euclidean distance along the feature dimension\n        bmu_index = np.unravel_index(np.argmin(distances), distances.shape)\n        return bmu_index\n\n    def update_som(self, x, bmu_index):\n        # Update SOM weights based on input x and BMU\n        for i in range(self.som_grid_size):\n            for j in range(self.som_grid_size):\n                distance = np.sqrt((i - bmu_index[0])**2 + (j - bmu_index[1])**2)\n                influence = np.exp(-distance**2 / (2 * self.som_sigma**2))\n                self.som[i, j] += self.som_learning_rate * influence * (x - self.som[i, j])\n\n    def initialize_population(self, lb, ub):\n        # Initialize population using SOM as a distribution\n        self.population = np.zeros((self.pop_size, self.dim))\n        for i in range(self.pop_size):\n            node_index = (np.random.randint(0, self.som_grid_size), np.random.randint(0, self.som_grid_size))\n            self.population[i] = self.som[node_index] + np.random.normal(0, 0.1*(ub-lb), size=self.dim) #Add a bit of noise\n            self.population[i] = np.clip(self.population[i], lb, ub) # Keep inside bounds\n\n    def differential_evolution(self, func, lb, ub):\n        new_population = np.copy(self.population)\n        new_fitness = np.copy(self.fitness)\n\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[idxs]\n            mutant = x_r1 + self.de_mutation_factor * (x_r2 - x_r3)\n            mutant = np.clip(mutant, lb, ub)\n            \n            crossover = np.random.uniform(size=self.dim) < self.de_crossover_rate\n            trial = np.where(crossover, mutant, self.population[i])\n\n            f_trial = func(trial)\n            self.budget -= 1\n            \n            if f_trial < self.f_opt:\n                self.f_opt = f_trial\n                self.x_opt = trial\n            \n            if f_trial < self.fitness[i]:\n                new_fitness[i] = f_trial\n                new_population[i] = trial\n\n        self.population = new_population\n        self.fitness = new_fitness\n            \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_som(lb, ub)\n\n        # Initial SOM training with random samples\n        for _ in range(self.pop_size):\n          x = np.random.uniform(lb, ub, size=self.dim)\n          bmu_index = self.find_closest_node(x)\n          self.update_som(x, bmu_index)\n\n        self.initialize_population(lb, ub) # SOM-informed initialization\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(self.fitness)\n        self.x_opt = self.population[best_index]\n        self.f_opt = self.fitness[best_index]\n        \n        while self.budget > 0:\n            self.differential_evolution(func, lb, ub)\n\n            # Update SOM with the improved population\n            for x in self.population:\n                bmu_index = self.find_closest_node(x)\n                self.update_som(x, bmu_index)\n\n            # Adjust DE parameters based on the SOM quality\n            # Heuristic: If SOM is highly structured (low variance in node distances), reduce mutation\n            node_distances = []\n            for i in range(self.som_grid_size):\n              for j in range(self.som_grid_size):\n                for k in range(i, self.som_grid_size):\n                  for l in range(j+1, self.som_grid_size):\n                    node_distances.append(np.linalg.norm(self.som[i,j]-self.som[k,l])) #All distances\n\n            if len(node_distances) > 0:\n              som_distance_std = np.std(node_distances)\n            else:\n              som_distance_std = 0.1  #small default\n            \n            self.de_mutation_factor = np.clip(self.de_mutation_factor * (1 + np.random.normal(0, 0.1 * som_distance_std)), 0.1, 1.0)\n            self.de_crossover_rate = np.clip(self.de_crossover_rate * (1 - np.random.normal(0, 0.1 * som_distance_std)), 0.1, 1.0)\n           \n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm SOM_DE scored 0.460 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2d877ddc-d399-45c0-869a-e409d9b7000f"], "operator": null, "metadata": {"aucs": [0.19082679361447086, 0.2517396484361031, 0.42883376674689466, 0.729410488005722, 0.4500853627295527, 0.5530510088893994, 0.4112895801960129, 0.48849412337697873, 0.37742219810572364, 0.2611892546475435, 0.33875544364641874, 0.9946395428060394, 0.3231845155131663, 0.35830297992305193, 0.6829342278409442, 0.4869118672120486, 0.3899169917645189, 0.7802940390316977, 0.19880983209823044, 0.5135192791303346]}}
{"id": "fe5daf2f-646d-457f-9330-6bb87193b4c1", "fitness": 0.46048054718574266, "name": "SOM_DE", "description": "A metaheuristic algorithm combining a self-organizing map (SOM) for population diversification with differential evolution (DE) for exploitation, adapting DE parameters based on SOM performance.", "code": "import numpy as np\n\nclass SOM_DE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, som_grid_size=10, de_mutation_factor=0.5, de_crossover_rate=0.7, som_learning_rate=0.1, som_sigma=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.som_grid_size = som_grid_size\n        self.de_mutation_factor = de_mutation_factor\n        self.de_crossover_rate = de_crossover_rate\n        self.som_learning_rate = som_learning_rate\n        self.som_sigma = som_sigma\n        self.som = None\n        self.population = None\n        self.fitness = None\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def initialize_som(self, lb, ub):\n        # Initialize SOM weights randomly within bounds\n        self.som = np.random.uniform(lb, ub, size=(self.som_grid_size, self.som_grid_size, self.dim))\n\n    def find_closest_node(self, x):\n        # Find the best matching unit (BMU) for input x\n        distances = np.sum((self.som - x)**2, axis=2)  # Euclidean distance along the feature dimension\n        bmu_index = np.unravel_index(np.argmin(distances), distances.shape)\n        return bmu_index\n\n    def update_som(self, x, bmu_index):\n        # Update SOM weights based on input x and BMU\n        for i in range(self.som_grid_size):\n            for j in range(self.som_grid_size):\n                distance = np.sqrt((i - bmu_index[0])**2 + (j - bmu_index[1])**2)\n                influence = np.exp(-distance**2 / (2 * self.som_sigma**2))\n                self.som[i, j] += self.som_learning_rate * influence * (x - self.som[i, j])\n\n    def initialize_population(self, lb, ub):\n        # Initialize population using SOM as a distribution\n        self.population = np.zeros((self.pop_size, self.dim))\n        for i in range(self.pop_size):\n            node_index = (np.random.randint(0, self.som_grid_size), np.random.randint(0, self.som_grid_size))\n            self.population[i] = self.som[node_index] + np.random.normal(0, 0.1*(ub-lb), size=self.dim) #Add a bit of noise\n            self.population[i] = np.clip(self.population[i], lb, ub) # Keep inside bounds\n\n    def differential_evolution(self, func, lb, ub):\n        new_population = np.copy(self.population)\n        new_fitness = np.copy(self.fitness)\n\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[idxs]\n            mutant = x_r1 + self.de_mutation_factor * (x_r2 - x_r3)\n            mutant = np.clip(mutant, lb, ub)\n            \n            crossover = np.random.uniform(size=self.dim) < self.de_crossover_rate\n            trial = np.where(crossover, mutant, self.population[i])\n\n            f_trial = func(trial)\n            self.budget -= 1\n            \n            if f_trial < self.f_opt:\n                self.f_opt = f_trial\n                self.x_opt = trial\n            \n            if f_trial < self.fitness[i]:\n                new_fitness[i] = f_trial\n                new_population[i] = trial\n\n        self.population = new_population\n        self.fitness = new_fitness\n            \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_som(lb, ub)\n\n        # Initial SOM training with random samples\n        for _ in range(self.pop_size):\n          x = np.random.uniform(lb, ub, size=self.dim)\n          bmu_index = self.find_closest_node(x)\n          self.update_som(x, bmu_index)\n\n        self.initialize_population(lb, ub) # SOM-informed initialization\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(self.fitness)\n        self.x_opt = self.population[best_index]\n        self.f_opt = self.fitness[best_index]\n        \n        while self.budget > 0:\n            self.differential_evolution(func, lb, ub)\n\n            # Update SOM with the improved population\n            for x in self.population:\n                bmu_index = self.find_closest_node(x)\n                self.update_som(x, bmu_index)\n\n            # Adjust DE parameters based on the SOM quality\n            # Heuristic: If SOM is highly structured (low variance in node distances), reduce mutation\n            node_distances = []\n            for i in range(self.som_grid_size):\n              for j in range(self.som_grid_size):\n                for k in range(i, self.som_grid_size):\n                  for l in range(j+1, self.som_grid_size):\n                    node_distances.append(np.linalg.norm(self.som[i,j]-self.som[k,l])) #All distances\n\n            if len(node_distances) > 0:\n              som_distance_std = np.std(node_distances)\n            else:\n              som_distance_std = 0.1  #small default\n            \n            self.de_mutation_factor = np.clip(self.de_mutation_factor * (1 + np.random.normal(0, 0.1 * som_distance_std)), 0.1, 1.0)\n            self.de_crossover_rate = np.clip(self.de_crossover_rate * (1 - np.random.normal(0, 0.1 * som_distance_std)), 0.1, 1.0)\n           \n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm SOM_DE scored 0.460 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2d877ddc-d399-45c0-869a-e409d9b7000f"], "operator": null, "metadata": {"aucs": [0.19082679361447086, 0.2517396484361031, 0.42883376674689466, 0.729410488005722, 0.4500853627295527, 0.5530510088893994, 0.4112895801960129, 0.48849412337697873, 0.37742219810572364, 0.2611892546475435, 0.33875544364641874, 0.9946395428060394, 0.3231845155131663, 0.35830297992305193, 0.6829342278409442, 0.4869118672120486, 0.3899169917645189, 0.7802940390316977, 0.19880983209823044, 0.5135192791303346]}}
{"id": "fe5daf2f-646d-457f-9330-6bb87193b4c1", "fitness": 0.46048054718574266, "name": "SOM_DE", "description": "A metaheuristic algorithm combining a self-organizing map (SOM) for population diversification with differential evolution (DE) for exploitation, adapting DE parameters based on SOM performance.", "code": "import numpy as np\n\nclass SOM_DE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, som_grid_size=10, de_mutation_factor=0.5, de_crossover_rate=0.7, som_learning_rate=0.1, som_sigma=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.som_grid_size = som_grid_size\n        self.de_mutation_factor = de_mutation_factor\n        self.de_crossover_rate = de_crossover_rate\n        self.som_learning_rate = som_learning_rate\n        self.som_sigma = som_sigma\n        self.som = None\n        self.population = None\n        self.fitness = None\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def initialize_som(self, lb, ub):\n        # Initialize SOM weights randomly within bounds\n        self.som = np.random.uniform(lb, ub, size=(self.som_grid_size, self.som_grid_size, self.dim))\n\n    def find_closest_node(self, x):\n        # Find the best matching unit (BMU) for input x\n        distances = np.sum((self.som - x)**2, axis=2)  # Euclidean distance along the feature dimension\n        bmu_index = np.unravel_index(np.argmin(distances), distances.shape)\n        return bmu_index\n\n    def update_som(self, x, bmu_index):\n        # Update SOM weights based on input x and BMU\n        for i in range(self.som_grid_size):\n            for j in range(self.som_grid_size):\n                distance = np.sqrt((i - bmu_index[0])**2 + (j - bmu_index[1])**2)\n                influence = np.exp(-distance**2 / (2 * self.som_sigma**2))\n                self.som[i, j] += self.som_learning_rate * influence * (x - self.som[i, j])\n\n    def initialize_population(self, lb, ub):\n        # Initialize population using SOM as a distribution\n        self.population = np.zeros((self.pop_size, self.dim))\n        for i in range(self.pop_size):\n            node_index = (np.random.randint(0, self.som_grid_size), np.random.randint(0, self.som_grid_size))\n            self.population[i] = self.som[node_index] + np.random.normal(0, 0.1*(ub-lb), size=self.dim) #Add a bit of noise\n            self.population[i] = np.clip(self.population[i], lb, ub) # Keep inside bounds\n\n    def differential_evolution(self, func, lb, ub):\n        new_population = np.copy(self.population)\n        new_fitness = np.copy(self.fitness)\n\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[idxs]\n            mutant = x_r1 + self.de_mutation_factor * (x_r2 - x_r3)\n            mutant = np.clip(mutant, lb, ub)\n            \n            crossover = np.random.uniform(size=self.dim) < self.de_crossover_rate\n            trial = np.where(crossover, mutant, self.population[i])\n\n            f_trial = func(trial)\n            self.budget -= 1\n            \n            if f_trial < self.f_opt:\n                self.f_opt = f_trial\n                self.x_opt = trial\n            \n            if f_trial < self.fitness[i]:\n                new_fitness[i] = f_trial\n                new_population[i] = trial\n\n        self.population = new_population\n        self.fitness = new_fitness\n            \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_som(lb, ub)\n\n        # Initial SOM training with random samples\n        for _ in range(self.pop_size):\n          x = np.random.uniform(lb, ub, size=self.dim)\n          bmu_index = self.find_closest_node(x)\n          self.update_som(x, bmu_index)\n\n        self.initialize_population(lb, ub) # SOM-informed initialization\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(self.fitness)\n        self.x_opt = self.population[best_index]\n        self.f_opt = self.fitness[best_index]\n        \n        while self.budget > 0:\n            self.differential_evolution(func, lb, ub)\n\n            # Update SOM with the improved population\n            for x in self.population:\n                bmu_index = self.find_closest_node(x)\n                self.update_som(x, bmu_index)\n\n            # Adjust DE parameters based on the SOM quality\n            # Heuristic: If SOM is highly structured (low variance in node distances), reduce mutation\n            node_distances = []\n            for i in range(self.som_grid_size):\n              for j in range(self.som_grid_size):\n                for k in range(i, self.som_grid_size):\n                  for l in range(j+1, self.som_grid_size):\n                    node_distances.append(np.linalg.norm(self.som[i,j]-self.som[k,l])) #All distances\n\n            if len(node_distances) > 0:\n              som_distance_std = np.std(node_distances)\n            else:\n              som_distance_std = 0.1  #small default\n            \n            self.de_mutation_factor = np.clip(self.de_mutation_factor * (1 + np.random.normal(0, 0.1 * som_distance_std)), 0.1, 1.0)\n            self.de_crossover_rate = np.clip(self.de_crossover_rate * (1 - np.random.normal(0, 0.1 * som_distance_std)), 0.1, 1.0)\n           \n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm SOM_DE scored 0.460 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2d877ddc-d399-45c0-869a-e409d9b7000f"], "operator": null, "metadata": {"aucs": [0.19082679361447086, 0.2517396484361031, 0.42883376674689466, 0.729410488005722, 0.4500853627295527, 0.5530510088893994, 0.4112895801960129, 0.48849412337697873, 0.37742219810572364, 0.2611892546475435, 0.33875544364641874, 0.9946395428060394, 0.3231845155131663, 0.35830297992305193, 0.6829342278409442, 0.4869118672120486, 0.3899169917645189, 0.7802940390316977, 0.19880983209823044, 0.5135192791303346]}}
{"id": "fbec53ec-4f2e-415c-b375-733b2c6aeb40", "fitness": -Infinity, "name": "CMAES_NelderMead", "description": "A hybrid algorithm combining the strengths of Covariance Matrix Adaptation Evolution Strategy (CMA-ES) for global search with a Nelder-Mead simplex algorithm for local refinement.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass CMAES_NelderMead:\n    def __init__(self, budget=10000, dim=10, cmaes_popsize=None, nelder_mead_iters=5):\n        self.budget = budget\n        self.dim = dim\n        self.cmaes_popsize = cmaes_popsize if cmaes_popsize is not None else 4 + int(3 * np.log(dim))\n        self.nelder_mead_iters = nelder_mead_iters\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def __call__(self, func):\n        # Initial CMA-ES population\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = 0.5  # Initial step size\n        population = np.random.normal(mean, sigma, size=(self.cmaes_popsize, self.dim))\n        population = np.clip(population, func.bounds.lb, func.bounds.ub)\n\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.cmaes_popsize\n\n        best_index = np.argmin(fitness)\n        self.x_opt = population[best_index]\n        self.f_opt = fitness[best_index]\n\n        # CMA-ES loop\n        while self.budget > 0:\n            # Sample new population\n            population = np.random.normal(mean, sigma, size=(self.cmaes_popsize, self.dim))\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n\n            fitness = np.array([func(x) for x in population])\n            self.budget -= self.cmaes_popsize\n\n            # Update best solution\n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n\n            # Update mean and sigma (simplified CMA-ES update)\n            mean = np.mean(population, axis=0)\n            sigma *= np.exp(0.1 * (np.std(fitness) / np.mean(fitness) - 1))\n\n            # Local search with Nelder-Mead every few iterations (e.g., every 10 CMA-ES steps)\n            if self.budget > 0 and self.budget % (10 * self.cmaes_popsize) <= self.cmaes_popsize and self.nelder_mead_iters > 0:\n                # Use Nelder-Mead starting from the best CMA-ES point\n                result = minimize(func, self.x_opt, method='Nelder-Mead', \n                                  options={'maxiter': self.nelder_mead_iters})  # Limit iterations to save budget\n\n                if result.fun < self.f_opt:\n                    self.f_opt = result.fun\n                    self.x_opt = result.x\n                self.budget -= result.nit # Number of iterations that Nelder-Mead performed.  \n                if self.budget < 0:\n                   break;       \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 49, in __call__\nNameError: name 'minimize' is not defined\n.", "error": "", "parent_ids": ["0ea44e30-0fd8-4c8f-82f1-a9fbec38ee92"], "operator": null, "metadata": {}}
{"id": "edc475de-e0c5-4b90-bc5b-0a1e12597a4a", "fitness": 0.0, "name": "RepulsiveShrinkingSearch", "description": "A population-based algorithm that uses a repulsive force between individuals combined with a shrinking search radius to encourage exploration and exploitation within a confined space.", "code": "import numpy as np\n\nclass RepulsiveShrinkingSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=40, initial_radius=2.5, repulsion_factor=0.1, shrink_factor=0.995):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.initial_radius = initial_radius\n        self.repulsion_factor = repulsion_factor\n        self.shrink_factor = shrink_factor\n        self.population = None\n        self.fitness = None\n        self.x_opt = None\n        self.f_opt = np.inf\n        self.current_radius = initial_radius\n\n    def __call__(self, func):\n        # Initialize population within the radius of the center\n        center = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.population = np.random.uniform(-self.current_radius, self.current_radius, size=(self.pop_size, self.dim)) + center\n        self.population = np.clip(self.population, func.bounds.lb, func.bounds.ub)\n\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(self.fitness)\n        self.x_opt = self.population[best_index]\n        self.f_opt = self.fitness[best_index]\n\n        while self.budget > 0:\n            # Repulsion Step\n            for i in range(self.pop_size):\n                repulsion_force = np.zeros(self.dim)\n                for j in range(self.pop_size):\n                    if i != j:\n                        distance = np.linalg.norm(self.population[i] - self.population[j])\n                        if distance > 0:\n                            repulsion_direction = (self.population[i] - self.population[j]) / distance\n                            repulsion_force += repulsion_direction / distance  # Inverse distance repulsion\n\n                # Move the individual based on the repulsion force\n                new_position = self.population[i] + self.repulsion_factor * repulsion_force\n                new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n\n                f_new = func(new_position)\n                self.budget -= 1\n\n                if f_new < self.f_opt:\n                    self.f_opt = f_new\n                    self.x_opt = new_position\n\n                if f_new < self.fitness[i]:\n                    self.fitness[i] = f_new\n                    self.population[i] = new_position\n\n            # Shrink the search radius\n            self.current_radius *= self.shrink_factor\n            \n            # Move population to within the radius\n            center = self.population[np.argmin(self.fitness)]\n            self.population = np.random.uniform(-self.current_radius, self.current_radius, size=(self.pop_size, self.dim)) + center\n            self.population = np.clip(self.population, func.bounds.lb, func.bounds.ub)\n            self.fitness = np.array([func(x) for x in self.population])\n            self.budget -= self.pop_size\n            \n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm RepulsiveShrinkingSearch scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6f537959-23b8-40a3-a0c4-ac4f752bb9ba"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "13cdfdee-a9d0-448b-9a1d-0bfea1f1e1b5", "fitness": -Infinity, "name": "HybridCMAESPSO", "description": "A hybrid algorithm combining aspects of covariance matrix adaptation evolution strategy (CMA-ES) with a simplified particle swarm optimization (PSO) to balance exploration and exploitation.", "code": "import numpy as np\n\nclass HybridCMAESPSO:\n    def __init__(self, budget=10000, dim=10, pop_size=20, cma_sigma0=0.5, pso_inertia=0.7, pso_cognitive=1.4, pso_social=1.4):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.cma_sigma0 = cma_sigma0\n        self.pso_inertia = pso_inertia\n        self.pso_cognitive = pso_cognitive\n        self.pso_social = pso_social\n        self.mean = None\n        self.covariance = None\n        self.population = None\n        self.fitness = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = None\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n\n    def initialize(self, lb, ub):\n        self.mean = np.random.uniform(lb, ub, size=self.dim)\n        self.covariance = np.eye(self.dim) * self.cma_sigma0**2\n        self.population = np.random.multivariate_normal(self.mean, self.covariance, size=self.pop_size)\n        self.population = np.clip(self.population, lb, ub)\n        self.velocities = np.random.uniform(-0.1*(ub-lb), 0.1*(ub-lb), size=(self.pop_size, self.dim))\n        self.personal_best_positions = np.copy(self.population)\n        self.personal_best_fitness = np.full(self.pop_size, np.inf)\n        \n    def sample_population(self):\n        self.population = np.random.multivariate_normal(self.mean, self.covariance, size=self.pop_size)\n        self.population = np.clip(self.population, func.bounds.lb, func.bounds.ub)\n\n    def update_cma(self):\n        # Simple CMA-ES update rules (can be improved)\n        weights = np.sort(np.random.rand(self.pop_size))[::-1]\n        weights /= np.sum(weights)\n\n        delta_x = self.population - self.mean\n        self.mean = np.sum(weights[:, np.newaxis] * self.population, axis=0)\n\n        self.covariance = np.cov(delta_x.T, aweights=weights) + np.eye(self.dim) * self.cma_sigma0**2 #Regularization\n\n    def update_pso(self):\n        r1 = np.random.rand(self.pop_size, self.dim)\n        r2 = np.random.rand(self.pop_size, self.dim)\n\n        cognitive_component = self.pso_cognitive * r1 * (self.personal_best_positions - self.population)\n        social_component = self.pso_social * r2 * (self.global_best_position - self.population)\n\n        self.velocities = (self.pso_inertia * self.velocities + cognitive_component + social_component)\n        self.population += self.velocities\n        self.population = np.clip(self.population, func.bounds.lb, func.bounds.ub)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize(lb, ub)\n        \n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        \n        for i in range(self.pop_size):\n            if self.fitness[i] < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = self.fitness[i]\n                self.personal_best_positions[i] = self.population[i]\n            if self.fitness[i] < self.global_best_fitness:\n                self.global_best_fitness = self.fitness[i]\n                self.global_best_position = self.population[i]\n        \n        while self.budget > 0:\n            # Alternate between CMA-ES and PSO steps\n            if self.budget % 2 == 0:\n                self.update_cma()\n                self.sample_population()\n            else:\n                self.update_pso()\n\n            self.fitness = np.array([func(x) for x in self.population])\n            self.budget -= self.pop_size\n            \n            for i in range(self.pop_size):\n                if self.fitness[i] < self.personal_best_fitness[i]:\n                    self.personal_best_fitness[i] = self.fitness[i]\n                    self.personal_best_positions[i] = self.population[i]\n                if self.fitness[i] < self.global_best_fitness:\n                    self.global_best_fitness = self.fitness[i]\n                    self.global_best_position = self.population[i]\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 7, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 75, in __call__\n  File \"<string>\", line 33, in sample_population\nNameError: name 'func' is not defined\n.", "error": "", "parent_ids": ["fe5daf2f-646d-457f-9330-6bb87193b4c1"], "operator": null, "metadata": {}}
{"id": "a02cb1b8-a2b4-4564-a8b3-5b67d83182a9", "fitness": -Infinity, "name": "AdaptiveDE_Archive_BayesLS", "description": "Adaptive Differential Evolution with Archive and Bayesian Optimization assisted Local Search, using an archive to store promising solutions and employing Bayesian Optimization to intelligently guide local search in their vicinity.", "code": "import numpy as np\nfrom bayes_opt import BayesianOptimization\n\nclass AdaptiveDE_Archive_BayesLS:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, F=0.5, CR=0.9, archive_size=10, local_search_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.initial_pop_size = initial_pop_size\n        self.F = F\n        self.CR = CR\n        self.archive_size = archive_size\n        self.archive = []\n        self.local_search_prob = local_search_prob\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        \n        for i in range(self.pop_size):\n            if fitness[i] < self.f_opt:\n                self.f_opt = fitness[i]\n                self.x_opt = self.population[i]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(self.population)\n            new_fitness = np.copy(fitness)\n\n            # Update Archive\n            for i in range(self.pop_size):\n                if len(self.archive) < self.archive_size:\n                    self.archive.append((self.population[i], fitness[i]))\n                else:\n                    worst_arch_idx = np.argmax([item[1] for item in self.archive]) #find worst fitness in archive\n                    if fitness[i] < self.archive[worst_arch_idx][1]:\n                        self.archive[worst_arch_idx] = (self.population[i], fitness[i])\n\n            for i in range(self.pop_size):\n                # Differential Evolution\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[idxs]\n                \n                #Use archive with a small probability\n                if np.random.rand() < 0.1 and len(self.archive) > 0:\n                    arch_idx = np.random.randint(0, len(self.archive))\n                    x_r1 = self.archive[arch_idx][0]\n                    \n                mutant = x_r1 + self.F * (x_r2 - x_r3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                \n                crossover = np.random.uniform(size=self.dim) < self.CR\n                trial = np.where(crossover, mutant, self.population[i])\n                \n                # Local Search with Probability using Bayesian Optimization\n                if np.random.rand() < self.local_search_prob:\n                    # Define the bounds for the Bayesian Optimization\n                    pbounds = {f'x{k}': (max(func.bounds.lb[k], trial[k] - 0.5), min(func.bounds.ub[k], trial[k] + 0.5)) for k in range(self.dim)}\n\n                    def local_function( **kwargs):\n                        x = np.array([kwargs[f'x{k}'] for k in range(self.dim)])\n                        f = func(x)\n                        return -f  #BO maximizes, so negate for minimization\n\n                    optimizer = BayesianOptimization(\n                        f=local_function,\n                        pbounds=pbounds,\n                        random_state=1,\n                    )\n                    \n                    init_points = min(5, self.budget)  # Reduce if budget is low\n                    n_iter = min(10, self.budget-init_points)  # Reduce if budget is low\n\n                    optimizer.maximize(\n                        init_points=init_points,\n                        n_iter=n_iter,\n                    )\n                    \n                    self.budget -= (init_points + n_iter)\n                    \n                    best_params = optimizer.max['params']\n                    trial = np.array([best_params[f'x{k}'] for k in range(self.dim)])\n                    \n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n                    \n                if f_trial < fitness[i]:\n                    new_fitness[i] = f_trial\n                    new_population[i] = trial\n                else:\n                    new_fitness[i] = fitness[i]\n                    new_population[i] = self.population[i]\n\n            self.population = new_population\n            fitness = new_fitness\n            \n            # Adjust population size dynamically\n            if generation % 10 == 0:\n                if np.mean(fitness) < np.mean([func(np.random.uniform(func.bounds.lb, func.bounds.ub)) for _ in range(100)]):\n                    self.pop_size = min(2 * self.pop_size, self.initial_pop_size * 3) # Increase pop size if doing well\n                    self.population = np.concatenate((self.population, np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size - len(self.population), self.dim))))\n                    fitness = np.concatenate((fitness, np.array([func(x) for x in self.population[len(fitness):]])))\n                    self.budget -= (self.pop_size - len(fitness))\n                else:\n                    self.pop_size = max(self.initial_pop_size, self.pop_size // 2) # Decrease pop size if not improving\n                    self.population = self.population[:self.pop_size]\n                    fitness = fitness[:self.pop_size]\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 111, in evaluate\n    exec(code, safe_globals, local_env)\n  File \"<string>\", line 2, in <module>\nModuleNotFoundError: No module named 'bayes_opt'\n.", "error": "", "parent_ids": ["fa595cc2-1b78-482e-a37b-72906c644d2e"], "operator": null, "metadata": {}}
{"id": "24692914-034d-423b-923c-e1ff031870cb", "fitness": -Infinity, "name": "AdaptiveCMAES", "description": "An adaptive covariance matrix adaptation evolution strategy (CMA-ES) that dynamically adjusts its parameters based on function landscape features detected through fitness variance analysis.", "code": "import numpy as np\n\nclass AdaptiveCMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, initial_sigma=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.initial_sigma = initial_sigma\n        self.mean = None\n        self.covariance = None\n        self.sigma = None\n        self.pc = None\n        self.ps = None\n        self.C = None\n        self.eigenvalues = None\n        self.eigenbasis = None\n        self.mu = int(self.pop_size / 2)\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cc = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def initialize(self, lb, ub):\n        self.mean = np.random.uniform(lb, ub, size=self.dim)\n        self.sigma = self.initial_sigma\n        self.covariance = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.C = self.covariance\n        self.eigenvalues, self.eigenbasis = np.linalg.eigh(self.C)\n\n    def sample_population(self):\n        z = np.random.normal(size=(self.pop_size, self.dim))\n        samples = self.mean + self.sigma * self.eigenbasis @ (np.sqrt(self.eigenvalues) * z.T).T\n        return samples\n\n    def update_distribution(self, samples, fitness_values):\n        sorted_indices = np.argsort(fitness_values)\n        best_samples = samples[sorted_indices[:self.mu]]\n\n        # Calculate weighted mean of best samples\n        delta_mean = np.sum((best_samples - self.mean) * self.weights[:, np.newaxis], axis=0)\n        \n        # Update evolution path\n        self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * (self.eigenbasis @ (delta_mean / self.sigma))\n        \n        # Length control\n        sigma_norm = np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.budget / self.pop_size)) / 1.414 / ((np.sqrt(self.dim) * (1 - 1/(4*self.dim) + 1/(21 * self.dim**2))))\n        if sigma_norm < 0.2:\n            self.sigma *= 0.8\n        if sigma_norm > 5:\n            self.sigma *= 1.2\n\n        hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.budget / self.pop_size)) < (1.4 + 2/(self.dim+1)))\n\n        self.pc = (1 - self.cc) * self.pc + hsig * np.sqrt(self.cc * (2 - self.cc) * self.mueff) * delta_mean / self.sigma\n\n        # Adapt covariance matrix\n        artmp = (best_samples - self.mean) / self.sigma\n        self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :]) + self.cmu * np.sum(self.weights[:, np.newaxis, np.newaxis] * artmp[:, :, np.newaxis] @ artmp[:, np.newaxis, :], axis=0)\n\n        self.mean += delta_mean\n        self.C = np.triu(self.C) + np.triu(self.C, 1).T  # enforce symmetry\n        self.eigenvalues, self.eigenbasis = np.linalg.eigh(self.C)\n        self.eigenvalues = np.maximum(self.eigenvalues, 1e-10)  # Avoid zero eigenvalues\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize(lb, ub)\n        \n        while self.budget > 0:\n            samples = self.sample_population()\n            samples = np.clip(samples, lb, ub)\n            fitness_values = np.array([func(x) for x in samples])\n            self.budget -= self.pop_size\n            \n            best_index = np.argmin(fitness_values)\n            if fitness_values[best_index] < self.f_opt:\n                self.f_opt = fitness_values[best_index]\n                self.x_opt = samples[best_index]\n\n            self.update_distribution(samples, fitness_values)\n            \n            # Adaptive Sigma Scaling based on Fitness Variance\n            fitness_std = np.std(fitness_values)\n            if fitness_std < 1e-3:  # Low variance, converge faster\n               self.sigma *= 0.95\n            elif fitness_std > 1.0: # High variance, explore more\n                self.sigma *= 1.05\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 78, in __call__\n  File \"<string>\", line 40, in sample_population\nValueError: operands could not be broadcast together with shapes (2,) (2,6) \n.", "error": "", "parent_ids": ["fe5daf2f-646d-457f-9330-6bb87193b4c1"], "operator": null, "metadata": {}}
{"id": "19d476ed-1a56-4a8f-9f03-ff57c33ae5c5", "fitness": 0.34405820574989077, "name": "AdaptiveDEAgingRestart", "description": "Adaptive Differential Evolution with Aging and Restart, adjusting DE parameters and population diversity based on stagnation and age, with a periodic restart mechanism.", "code": "import numpy as np\n\nclass AdaptiveDEAgingRestart:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, F=0.5, CR=0.9, local_search_prob=0.1, stagnation_threshold=100, restart_frequency=500):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.initial_pop_size = initial_pop_size\n        self.F = F\n        self.CR = CR\n        self.local_search_prob = local_search_prob\n        self.stagnation_threshold = stagnation_threshold\n        self.restart_frequency = restart_frequency\n        self.ages = np.zeros(self.pop_size)  # Initialize ages for each individual\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        \n        for i in range(self.pop_size):\n            if fitness[i] < self.f_opt:\n                self.f_opt = fitness[i]\n                self.x_opt = self.population[i]\n\n        generation = 0\n        stagnation_counter = 0\n\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(self.population)\n            new_fitness = np.copy(fitness)\n            \n            # Increase ages of all individuals\n            self.ages += 1\n\n            for i in range(self.pop_size):\n                # Differential Evolution\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[idxs]\n                mutant = x_r1 + self.F * (x_r2 - x_r3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                \n                crossover = np.random.uniform(size=self.dim) < self.CR\n                trial = np.where(crossover, mutant, self.population[i])\n                \n                # Local Search with Probability\n                if np.random.rand() < self.local_search_prob:\n                    trial = trial + np.random.normal(0, 0.05, size=self.dim)\n                    trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n                \n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n                    stagnation_counter = 0  # Reset stagnation counter\n\n                if f_trial < fitness[i]:\n                    new_fitness[i] = f_trial\n                    new_population[i] = trial\n                    self.ages[i] = 0 # Reset age if individual improves\n                else:\n                    new_fitness[i] = fitness[i]\n                    new_population[i] = self.population[i]\n                    \n            self.population = new_population\n            fitness = new_fitness\n\n            # Adaptive F and CR\n            self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 0.9)\n            self.CR = np.clip(np.random.normal(0.9, 0.1), 0.1, 0.9)\n\n            # Stagnation Check and Restart\n            stagnation_counter += 1\n            if stagnation_counter > self.stagnation_threshold or generation % self.restart_frequency == 0:\n                # Introduce diversity by re-initializing a portion of the population\n                num_to_reinitialize = int(0.3 * self.pop_size)\n                idxs_to_reinitialize = np.argsort(self.ages)[-num_to_reinitialize:]  # Reinitialize oldest individuals\n                self.population[idxs_to_reinitialize] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(num_to_reinitialize, self.dim))\n                fitness[idxs_to_reinitialize] = np.array([func(x) for x in self.population[idxs_to_reinitialize]])\n                self.budget -= num_to_reinitialize\n                self.ages[idxs_to_reinitialize] = 0 # Reset age of reinitialized individuals\n                stagnation_counter = 0  # Reset stagnation counter\n\n                for i in idxs_to_reinitialize:\n                    if fitness[i] < self.f_opt:\n                        self.f_opt = fitness[i]\n                        self.x_opt = self.population[i]\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveDEAgingRestart scored 0.344 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["fa595cc2-1b78-482e-a37b-72906c644d2e"], "operator": null, "metadata": {"aucs": [0.26498413292002165, 0.5371135810024172, 0.5741351090771241, 0]}}
{"id": "311f8899-fd33-4e22-916b-2389f126cf8e", "fitness": -Infinity, "name": "AdaptiveCMAES_DE", "description": "An adaptive search algorithm that combines aspects of covariance matrix adaptation evolution strategy (CMA-ES) and differential evolution (DE), using CMA-ES for global exploration and DE for local exploitation, adaptively switching between them based on performance.", "code": "import numpy as np\n\nclass AdaptiveCMAES_DE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, de_mutation=0.5, de_crossover=0.7, cmaes_sigma=0.5, adaptation_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.de_mutation = de_mutation\n        self.de_crossover = de_crossover\n        self.cmaes_sigma = cmaes_sigma\n        self.adaptation_rate = adaptation_rate\n        self.population = None\n        self.fitness = None\n        self.x_opt = None\n        self.f_opt = np.inf\n        self.use_cmaes = True  # Start with CMA-ES\n\n        # CMA-ES related variables\n        self.mean = None\n        self.covariance = None\n        self.pc = None\n        self.ps = None\n        self.chiN = None\n        self.C = None\n        self.eigenspace = None\n        self.eigenvalues = None\n        self.mu = self.pop_size // 2  # Number of individuals for recombination\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.cs = (self.budget/self.dim + 4) / (self.budget/self.dim + self.mu + 4)\n        self.damps = 1 + 2*max(0, np.sqrt((self.mu-1)/(self.dim+1)) - 1) + self.cs\n        self.cc = (4 + self.mu / self.dim) / (self.dim + 4 + 2 * self.mu / self.dim)\n        self.mucov = self.mu/(self.dim*self.dim)\n        self.c1 = self.adaptation_rate / ((self.dim + 1.3)**2 + self.mucov)\n        self.cmu = min(1 - self.c1, self.adaptation_rate * self.mu**2 / ((self.dim + 2)**2 + self.mucov))\n\n    def initialize_cmaes(self, func):\n        self.mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.covariance = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1/(4*self.dim) + 1/(21*self.dim*self.dim))\n        self.C = self.covariance\n        self.eigenspace = np.eye(self.dim)\n        self.eigenvalues = np.ones(self.dim)\n\n    def sample_population_cmaes(self, func):\n        z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n        y = self.eigenspace @ (self.eigenvalues**0.5 * z.T)\n        x = self.mean + self.cmaes_sigma * y.T\n        x = np.clip(x, func.bounds.lb, func.bounds.ub)\n        return x\n\n    def update_cmaes(self, population, fitness):\n        sorted_indices = np.argsort(fitness)\n        best_indices = sorted_indices[:self.mu]\n\n        y = (population[best_indices] - self.mean) / self.cmaes_sigma\n        self.mean = np.sum(self.weights[:, None] * population[best_indices], axis=0)\n\n        ps_temp = (1-self.cs)*self.ps + (self.cs*(2-self.cs))**0.5 * self.eigenspace @ (y.T @ self.weights)\n        norm_ps = np.linalg.norm(ps_temp)\n        self.ps = ps_temp\n        hsig = norm_ps / (1 - (1 - self.cs)**(self.budget/self.pop_size)) / self.chiN < 1 + 2/(self.dim+1)\n        self.pc = (1-self.cc)*self.pc + hsig * (self.cc*(2-self.cc))**0.5 * (population[best_indices[0]] - self.mean) / self.cmaes_sigma\n\n        dC = np.diag(self.weights @ (y**2))\n        self.C = (1-self.c1-self.cmu) * self.C + self.c1 * (self.pc[:, None] * self.pc) + self.cmu * (y.T @ np.diag(self.weights) @ y)\n\n        self.covariance = np.triu(self.C) + np.triu(self.C, 1).T\n        self.eigenvalues, self.eigenspace = np.linalg.eig(self.covariance)\n        self.eigenvalues = np.real(self.eigenvalues)\n        self.eigenspace = np.real(self.eigenspace)\n\n\n    def differential_evolution(self, func):\n        for i in range(self.pop_size):\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = a + self.de_mutation * (b - c)\n            mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n            cross_points = np.random.rand(self.dim) < self.de_crossover\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n\n            trial = np.where(cross_points, mutant, self.population[i])\n            f_trial = func(trial)\n            self.budget -= 1\n\n            if f_trial < self.fitness[i]:\n                self.fitness[i] = f_trial\n                self.population[i] = trial\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n\n    def __call__(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(self.fitness)\n        self.x_opt = self.population[best_index]\n        self.f_opt = self.fitness[best_index]\n\n        self.initialize_cmaes(func)\n\n        cmaes_success = 0\n        de_success = 0\n\n        while self.budget > 0:\n            if self.use_cmaes:\n                # CMA-ES Step\n                new_population = self.sample_population_cmaes(func)\n                new_fitness = np.array([func(x) for x in new_population])\n                self.budget -= self.pop_size\n\n                best_index_cmaes = np.argmin(new_fitness)\n                if new_fitness[best_index_cmaes] < np.min(self.fitness):\n                    cmaes_success += 1\n                else:\n                    cmaes_success = max(0, cmaes_success -1)\n\n\n                self.update_cmaes(new_population, new_fitness)\n                combined_population = np.concatenate((self.population, new_population), axis=0)\n                combined_fitness = np.concatenate((self.fitness, new_fitness))\n\n                sorted_indices = np.argsort(combined_fitness)[:self.pop_size]\n                self.population = combined_population[sorted_indices]\n                self.fitness = combined_fitness[sorted_indices]\n                \n                best_index = np.argmin(self.fitness)\n                if self.fitness[best_index] < self.f_opt:\n                    self.f_opt = self.fitness[best_index]\n                    self.x_opt = self.population[best_index]\n\n\n\n            else:\n                # Differential Evolution Step\n                self.differential_evolution(func)\n\n                best_index_de = np.argmin(self.fitness)\n                if self.fitness[best_index_de] < self.f_opt:\n                    de_success += 1\n                else:\n                     de_success = max(0, de_success -1)\n\n\n\n            # Adapt strategy\n            if cmaes_success > 5 and de_success < -5:\n                self.use_cmaes = True\n                cmaes_success = 0\n                de_success = 0\n            elif de_success > 5 and cmaes_success < -5:\n                self.use_cmaes = False\n                cmaes_success = 0\n                de_success = 0\n\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 117, in __call__\n  File \"<string>\", line 49, in sample_population_cmaes\nValueError: operands could not be broadcast together with shapes (2,) (2,20) \n.", "error": "", "parent_ids": ["6f537959-23b8-40a3-a0c4-ac4f752bb9ba"], "operator": null, "metadata": {}}
{"id": "44bb1463-3a83-44cd-bf51-20fe8ce52866", "fitness": -Infinity, "name": "PSO_NM", "description": "A hybrid algorithm combining the strengths of Particle Swarm Optimization (PSO) for exploration and Nelder-Mead Simplex for exploitation, adaptively switching between them based on population diversity.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass PSO_NM:\n    def __init__(self, budget=10000, dim=10, pop_size=30, inertia_weight=0.7, cognitive_coeff=1.4, social_coeff=1.4, nm_max_iter=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.inertia_weight = inertia_weight\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.nm_max_iter = nm_max_iter\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = None\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.lb = None\n        self.ub = None\n\n    def initialize_particles(self):\n        self.particles = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.random.uniform(-0.1 * (self.ub - self.lb), 0.1 * (self.ub - self.lb), size=(self.pop_size, self.dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_fitness = np.full(self.pop_size, np.inf)\n\n    def pso_step(self, func):\n        for i in range(self.pop_size):\n            inertia = self.inertia_weight * self.velocities[i]\n            cognitive = self.cognitive_coeff * np.random.rand(self.dim) * (self.personal_best_positions[i] - self.particles[i])\n            social = self.social_coeff * np.random.rand(self.dim) * (self.global_best_position - self.particles[i])\n\n            self.velocities[i] = inertia + cognitive + social\n            self.particles[i] += self.velocities[i]\n            self.particles[i] = np.clip(self.particles[i], self.lb, self.ub)\n\n            fitness = func(self.particles[i])\n            self.budget -= 1\n            \n            if fitness < self.global_best_fitness:\n                self.global_best_fitness = fitness\n                self.global_best_position = np.copy(self.particles[i])\n\n            if fitness < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = fitness\n                self.personal_best_positions[i] = np.copy(self.particles[i])\n    \n    def nelder_mead_optimization(self, func, x0):\n        bounds = [(self.lb, self.ub)] * self.dim\n        result = minimize(func, x0, method='Nelder-Mead', options={'maxiter': self.nm_max_iter}, bounds=bounds)\n        self.budget -= result.nit # Account function evaluations\n        return result.fun, result.x\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self.initialize_particles()\n\n        # Initial evaluation\n        for i in range(self.pop_size):\n            fitness = func(self.particles[i])\n            self.budget -= 1\n            if fitness < self.global_best_fitness:\n                self.global_best_fitness = fitness\n                self.global_best_position = np.copy(self.particles[i])\n            self.personal_best_fitness[i] = fitness\n            self.personal_best_positions[i] = np.copy(self.particles[i])\n            \n        while self.budget > 0:\n            # PSO Step\n            self.pso_step(func)\n\n            # Adaptive switching based on population diversity\n            diversity = np.std(self.particles)\n\n            if diversity < 0.1 * (self.ub - self.lb):  # Low diversity: switch to Nelder-Mead\n                # Apply Nelder-Mead around the global best\n                nm_fitness, nm_position = self.nelder_mead_optimization(func, self.global_best_position)\n                if nm_fitness < self.global_best_fitness:\n                    self.global_best_fitness = nm_fitness\n                    self.global_best_position = nm_position\n                \n                # Perturb particles to increase diversity\n                self.particles = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n                for i in range(self.pop_size):\n                   self.personal_best_positions[i] = np.copy(self.particles[i])\n                   self.personal_best_fitness[i] = func(self.particles[i])\n                   self.budget -= 1\n                   if self.personal_best_fitness[i] < self.global_best_fitness:\n                       self.global_best_fitness = self.personal_best_fitness[i]\n                       self.global_best_position = np.copy(self.particles[i])\n\n            \n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 7, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 76, in __call__\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n.", "error": "", "parent_ids": ["fe5daf2f-646d-457f-9330-6bb87193b4c1"], "operator": null, "metadata": {}}
{"id": "777cf317-dcd1-40bf-b084-5ad475bf651a", "fitness": 0.6489976529199647, "name": "RingTopologyDE", "description": "A population-based algorithm employing a ring topology for information sharing and a decaying exploration rate to balance exploration and exploitation.", "code": "import numpy as np\n\nclass RingTopologyDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, exploration_decay=0.999):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.exploration_decay = exploration_decay\n        self.exploration_rate = 1.0\n        self.population = None\n        self.fitness = None\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def __call__(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(self.fitness)\n        self.x_opt = self.population[best_index]\n        self.f_opt = self.fitness[best_index]\n\n        while self.budget > 0:\n            new_population = np.copy(self.population)\n            new_fitness = np.copy(self.fitness)\n\n            for i in range(self.pop_size):\n                # Ring Topology: Select neighbors\n                left = (i - 1) % self.pop_size\n                right = (i + 1) % self.pop_size\n\n                # Create mutant vector\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[idxs]\n\n                if np.random.rand() < self.exploration_rate:\n                    mutant = x_r1 + self.F * (x_r2 - x_r3)  # Global exploration\n                else:\n                    mutant = self.population[i] + self.F * (self.population[left] - self.population[right]) # Local exploitation via ring\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                crossover = np.random.uniform(size=self.dim) < self.CR\n                trial = np.where(crossover, mutant, self.population[i])\n\n                # Evaluation\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                # Selection\n                if f_trial < self.fitness[i]:\n                    new_fitness[i] = f_trial\n                    new_population[i] = trial\n\n            self.population = new_population\n            self.fitness = new_fitness\n\n            # Update best\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index]\n\n            self.exploration_rate *= self.exploration_decay # Decay exploration rate\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm RingTopologyDE scored 0.649 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0ea44e30-0fd8-4c8f-82f1-a9fbec38ee92"], "operator": null, "metadata": {"aucs": [0.2710417803618794, 0.5671812018285356, 0.6241441388618019, 0.8668215148724231, 0.6685454275358531, 0.7419757060252573, 0.574166591239184, 0.5866630029279082, 0.6974592411352858, 0.617771796572091, 0.8402386972713969, 0.9956589394489002, 0.5744709409798658, 0.6532569298103836, 0.9073802223348169, 0.7159587309579842, 0.5379236220036079, 0.7838508532949002, 0.23469144984204848, 0.5207522710951693]}}
{"id": "bf60bcf7-f45f-430e-a559-29f2036d8b47", "fitness": 0.6475158769611874, "name": "AdaptiveDE", "description": "An adaptive Differential Evolution algorithm that dynamically adjusts its mutation factor (F) and crossover rate (CR) based on the population diversity and improvement rate.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, initial_F=0.5, initial_CR=0.9, diversity_threshold=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = initial_F\n        self.CR = initial_CR\n        self.initial_F = initial_F\n        self.initial_CR = initial_CR\n        self.diversity_threshold = diversity_threshold\n        self.archive_size = int(pop_size * 0.2)  # Archive size is 20% of population size\n        self.archive = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        \n        for i in range(self.pop_size):\n            if fitness[i] < self.f_opt:\n                self.f_opt = fitness[i]\n                self.x_opt = self.population[i]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.copy(self.population)\n            new_fitness = np.copy(fitness)\n\n            # Calculate population diversity\n            diversity = np.std(self.population)\n\n            # Adjust F and CR based on diversity and improvement rate\n            if diversity < self.diversity_threshold:\n                self.F = self.initial_F + 0.2 * np.random.randn()  # Increase F for exploration\n                self.CR = self.initial_CR - 0.1 * np.random.rand() # Decrease CR for exploration\n            else:\n                self.F = self.initial_F - 0.1 * np.random.randn()  # Decrease F for exploitation\n                self.CR = self.initial_CR + 0.2 * np.random.rand() # Increase CR for exploitation\n\n            self.F = np.clip(self.F, 0.1, 0.9)\n            self.CR = np.clip(self.CR, 0.1, 0.9)\n\n            for i in range(self.pop_size):\n                # Differential Evolution\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[idxs]\n                mutant = x_r1 + self.F * (x_r2 - x_r3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                \n                crossover = np.random.uniform(size=self.dim) < self.CR\n                trial = np.where(crossover, mutant, self.population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n                    \n                if f_trial < fitness[i]:\n                    new_fitness[i] = f_trial\n                    new_population[i] = trial\n\n                    # Update archive with successful trials\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                    else:\n                        # Replace a random element in the archive\n                        idx_to_replace = np.random.randint(0, self.archive_size)\n                        self.archive[idx_to_replace] = trial\n                else:\n                    new_fitness[i] = fitness[i]\n                    new_population[i] = self.population[i]\n\n            self.population = new_population\n            fitness = new_fitness\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveDE scored 0.648 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["fa595cc2-1b78-482e-a37b-72906c644d2e"], "operator": null, "metadata": {"aucs": [0.2975541017900517, 0.5867091913860423, 0.5876564979324645, 0.8375586934224762, 0.6831123209730057, 0.7318962081387526, 0.5613514595428712, 0.5845232653805417, 0.6609504501397712, 0.6228917674076856, 0.817109422506706, 0.9892782229086424, 0.5648460777441193, 0.6600674345868479, 0.8970312307328262, 0.7352537555665347, 0.5574288176012587, 0.7963167194872318, 0.22886256146009387, 0.5499193405158249]}}
{"id": "db520efd-12a4-462a-a7ac-b5bfb3f34b51", "fitness": 0.5063298631110683, "name": "AdaptiveArchiveDE", "description": "An adaptive Differential Evolution algorithm that leverages a historical archive of successful solutions to guide mutation and dynamically adjusts its parameters based on success rates.", "code": "import numpy as np\n\nclass AdaptiveArchiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=100, initial_mutation_factor=0.5, initial_crossover_rate=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.mutation_factor = initial_mutation_factor\n        self.crossover_rate = initial_crossover_rate\n        self.population = None\n        self.fitness = None\n        self.archive = []\n        self.archive_fitness = []\n        self.x_opt = None\n        self.f_opt = np.inf\n        self.success_mutation_factors = []\n        self.success_crossover_rates = []\n\n    def initialize_population(self, lb, ub):\n        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n\n    def update_archive(self, x, f):\n        if len(self.archive) < self.archive_size:\n            self.archive.append(x)\n            self.archive_fitness.append(f)\n        else:\n            # Replace worst element in archive if new solution is better\n            worst_index = np.argmax(self.archive_fitness)\n            if f < self.archive_fitness[worst_index]:\n                self.archive[worst_index] = x\n                self.archive_fitness[worst_index] = f\n\n    def differential_evolution(self, func, lb, ub):\n        new_population = np.copy(self.population)\n        new_fitness = np.copy(self.fitness)\n\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[idxs]\n\n            # Incorporate archive member into mutation\n            if len(self.archive) > 0:\n                x_archive = self.archive[np.random.randint(0, len(self.archive))]\n                mutant = x_r1 + self.mutation_factor * (x_r2 - x_r3) + self.mutation_factor * (x_archive - self.population[i]) # ADDED ARCHIVE\n            else:\n                mutant = x_r1 + self.mutation_factor * (x_r2 - x_r3)\n\n            mutant = np.clip(mutant, lb, ub)\n\n            crossover = np.random.uniform(size=self.dim) < self.crossover_rate\n            trial = np.where(crossover, mutant, self.population[i])\n\n            f_trial = func(trial)\n            self.budget -= 1\n\n            if f_trial < self.f_opt:\n                self.f_opt = f_trial\n                self.x_opt = trial\n\n            if f_trial < self.fitness[i]:\n                new_fitness[i] = f_trial\n                new_population[i] = trial\n                self.update_archive(trial, f_trial)\n\n                # Store successful parameters\n                self.success_mutation_factors.append(self.mutation_factor)\n                self.success_crossover_rates.append(self.crossover_rate)\n\n        self.population = new_population\n        self.fitness = new_fitness\n\n    def adapt_parameters(self):\n        # Adapt mutation factor and crossover rate based on success history\n        if len(self.success_mutation_factors) > 0:\n            self.mutation_factor = np.mean(self.success_mutation_factors)\n            self.crossover_rate = np.mean(self.success_crossover_rates)\n\n            # Add some noise to prevent stagnation\n            self.mutation_factor = np.clip(self.mutation_factor + np.random.normal(0, 0.1), 0.1, 1.0)\n            self.crossover_rate = np.clip(self.crossover_rate + np.random.normal(0, 0.1), 0.1, 1.0)\n\n            self.success_mutation_factors = [] #reset\n            self.success_crossover_rates = [] #reset\n        else:\n            # If no success, increase exploration\n            self.mutation_factor = np.clip(self.mutation_factor + 0.1, 0.1, 1.0)\n            self.crossover_rate = np.clip(self.crossover_rate - 0.1, 0.1, 1.0) # Reduce to exploit less.\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(self.fitness)\n        self.x_opt = self.population[best_index]\n        self.f_opt = self.fitness[best_index]\n        self.update_archive(self.x_opt, self.f_opt)\n\n        while self.budget > 0:\n            self.differential_evolution(func, lb, ub)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveArchiveDE scored 0.506 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["fe5daf2f-646d-457f-9330-6bb87193b4c1"], "operator": null, "metadata": {"aucs": [0.14318874250869562, 0.23850908679334482, 0.35926475568803484, 0.7819961680740796, 0.7021142029156926, 0.6891907142164344, 0.33670556747804525, 0.37148359119737917, 0.6049169211375205, 0.21040699979566824, 0.7194956600402395, 0.9993181425109411, 0.2854452058315001, 0.5755347980378019, 0.6769725919766683, 0.6936028708614076, 0.4725863548176812, 0.5860488995023422, 0.1877892534743263, 0.492026735363564]}}
{"id": "c9ffb827-25e5-48f1-8132-026f534359a1", "fitness": 0.44393705711693954, "name": "AdaptiveNeighborhoodSearch", "description": "An adaptive search algorithm that utilizes a decaying exploration rate and a neighborhood-based mutation strategy, dynamically focusing search efforts based on fitness landscape characteristics.", "code": "import numpy as np\n\nclass AdaptiveNeighborhoodSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=40, neighborhood_size=5, initial_exploration_rate=0.5, exploration_decay=0.995):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.neighborhood_size = neighborhood_size\n        self.initial_exploration_rate = initial_exploration_rate\n        self.exploration_rate = initial_exploration_rate\n        self.exploration_decay = exploration_decay\n        self.population = None\n        self.fitness = None\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def __call__(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(self.fitness)\n        self.x_opt = self.population[best_index]\n        self.f_opt = self.fitness[best_index]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Select a random neighbor\n                neighbors = np.random.choice(self.pop_size, self.neighborhood_size, replace=False)\n                best_neighbor_index = neighbors[np.argmin(self.fitness[neighbors])]\n\n                # Adaptive mutation based on neighborhood\n                mutation_scale = np.abs(self.population[best_neighbor_index] - self.population[i])\n                mutation = np.random.normal(0, self.exploration_rate * mutation_scale, size=self.dim)\n                new_position = self.population[i] + mutation\n                new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n\n                # Explore with decaying probability\n                if np.random.rand() < self.exploration_rate:\n                    new_position = np.random.uniform(func.bounds.lb, func.bounds.ub)\n\n                f_new = func(new_position)\n                self.budget -= 1\n\n                if f_new < self.f_opt:\n                    self.f_opt = f_new\n                    self.x_opt = new_position\n\n                if f_new < self.fitness[i]:\n                    self.fitness[i] = f_new\n                    self.population[i] = new_position\n            \n            # Decay the exploration rate\n            self.exploration_rate *= self.exploration_decay\n\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveNeighborhoodSearch scored 0.444 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6f537959-23b8-40a3-a0c4-ac4f752bb9ba"], "operator": null, "metadata": {"aucs": [0.14545886551247222, 0.3050361407547212, 0.4026910690148283, 0.6913772216081657, 0.40479180018055216, 0.48076697969674775, 0.312511882133497, 0.43021920583621, 0.47898926042075884, 0.2543500079653225, 0.6415113618994603, 0.9805866848805723, 0.26950999972251977, 0.33395544815145894, 0.7282681257240714, 0.3689977910648585, 0.3609028916546455, 0.583115067873458, 0.2158864550353684, 0.48981488320910094]}}
{"id": "73718266-664e-4c31-be8e-6f3b2ebfbd2c", "fitness": 0.0, "name": "SOMGuidedSearch", "description": "An algorithm that combines a self-organizing map (SOM) for landscape approximation with a local search strategy guided by the SOM.", "code": "import numpy as np\n\nclass SOMGuidedSearch:\n    def __init__(self, budget=10000, dim=10, som_size=10, learning_rate=0.1, neighborhood_radius=None, local_search_iterations=5):\n        self.budget = budget\n        self.dim = dim\n        self.som_size = som_size\n        self.learning_rate = learning_rate\n        self.neighborhood_radius = neighborhood_radius if neighborhood_radius is not None else som_size // 3\n        self.local_search_iterations = local_search_iterations\n\n        self.som = np.random.uniform(-1, 1, size=(som_size, som_size, dim)) # Initialize SOM weights\n        self.fitness_map = np.full((som_size, som_size), np.inf)\n\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def find_best_matching_unit(self, x):\n        \"\"\"Find the best matching unit (BMU) in the SOM for a given input vector.\"\"\"\n        distances = np.sum((self.som - x)**2, axis=2)\n        return np.unravel_index(np.argmin(distances), distances.shape)\n\n    def update_som(self, x, bmu, func):\n         \"\"\"Update the SOM weights based on the input vector and BMU.\"\"\"\n         for i in range(self.som_size):\n            for j in range(self.som_size):\n                distance = np.sqrt((i - bmu[0])**2 + (j - bmu[1])**2)\n                if distance <= self.neighborhood_radius:\n                    influence = np.exp(-distance**2 / (2 * self.neighborhood_radius**2))\n                    self.som[i, j] += self.learning_rate * influence * (x - self.som[i, j])\n                    f = func(self.som[i,j])\n                    if f < self.fitness_map[i,j]:\n                      self.fitness_map[i,j] = f\n\n\n    def local_search(self, x, func):\n        \"\"\"Perform a local search around a given point.\"\"\"\n        best_x = x.copy()\n        best_f = func(x)\n        for _ in range(self.local_search_iterations):\n            # Generate a random perturbation\n            perturbation = np.random.normal(0, 0.1, size=self.dim)\n            new_x = x + perturbation\n            new_x = np.clip(new_x, func.bounds.lb, func.bounds.ub)\n            new_f = func(new_x)\n\n            if new_f < best_f:\n                best_f = new_f\n                best_x = new_x\n\n        return best_f, best_x\n\n\n    def __call__(self, func):\n        # Initial exploration\n        initial_samples = min(self.budget, self.som_size * self.som_size) # limit initial sampling to budget\n        for _ in range(initial_samples):\n            x = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n            f = func(x)\n            self.budget -= 1\n\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x\n\n            bmu = self.find_best_matching_unit(x)\n            self.update_som(x, bmu, func)\n\n        # Main optimization loop\n        while self.budget > 0:\n            # Select a random unit from the SOM\n            i, j = np.random.randint(0, self.som_size, size=2)\n            x = self.som[i, j].copy()\n\n            # Perform local search\n            f_local, x_local = self.local_search(x, func)\n            self.budget -= self.local_search_iterations # Account for local search evals.\n\n            if f_local < self.f_opt:\n                self.f_opt = f_local\n                self.x_opt = x_local\n\n            # Update the SOM based on the local search result.\n            bmu = self.find_best_matching_unit(x_local)\n            self.update_som(x_local, bmu, func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm SOMGuidedSearch scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c9ffb827-25e5-48f1-8132-026f534359a1"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "a9811b25-16cc-4b7d-95f9-575c9eb31a25", "fitness": 0.1382066096052716, "name": "SelfAdaptiveDE", "description": "A self-adaptive Differential Evolution algorithm with a dynamic population size and an aging mechanism for particles, promoting exploration and exploitation.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, min_pop_size=10, max_pop_size=100, initial_F=0.5, initial_CR=0.9, age_limit=50):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = initial_pop_size\n        self.min_pop_size = min_pop_size\n        self.max_pop_size = max_pop_size\n        self.pop_size = initial_pop_size\n        self.F = initial_F\n        self.CR = initial_CR\n        self.age_limit = age_limit\n        self.population = None\n        self.fitness = None\n        self.ages = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.ages = np.zeros(self.pop_size, dtype=int)\n        self.budget -= self.pop_size\n\n        for i in range(self.pop_size):\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.population[i]\n\n    def adjust_population_size(self):\n        # Dynamically adjust population size based on stagnation\n        if np.std(self.fitness) < 1e-6 and self.pop_size < self.max_pop_size:  #Stagnation detected.\n            self.pop_size = min(self.pop_size + 5, self.max_pop_size) #Increase the population size.\n        elif self.pop_size > self.initial_pop_size and np.random.rand() < 0.05: # Reduce population if conditions are favorable\n            self.pop_size = max(self.pop_size - 2, self.min_pop_size)\n\n    def age_population(self):\n        # Increment age for each individual\n        self.ages += 1\n\n        # Identify and remove old individuals (replace with new ones)\n        old_indices = np.where(self.ages >= self.age_limit)[0]\n        num_old = len(old_indices)\n\n        if num_old > 0:\n            self.population[old_indices] = np.random.uniform(self.population.min(), self.population.max(), size=(num_old, self.dim)) #func.bounds.lb, func.bounds.ub, size=(num_old, self.dim)\n            self.fitness[old_indices] = np.array([np.inf] * num_old) # To force re-evaluation\n            self.ages[old_indices] = 0 #Reset age.\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n\n            self.adjust_population_size()\n            if self.pop_size != len(self.population): # population size was changed.\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.ages = np.zeros(self.pop_size, dtype=int)\n                self.budget -= self.pop_size\n                for i in range(self.pop_size):\n                    if self.fitness[i] < self.f_opt:\n                        self.f_opt = self.fitness[i]\n                        self.x_opt = self.population[i]\n\n            new_population = np.copy(self.population)\n            new_fitness = np.copy(self.fitness)\n\n            for i in range(self.pop_size):\n                # Differential Evolution\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[idxs]\n                mutant = x_r1 + self.F * (x_r2 - x_r3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                \n                crossover = np.random.uniform(size=self.dim) < self.CR\n                trial = np.where(crossover, mutant, self.population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n                    \n                if f_trial < self.fitness[i]:\n                    new_fitness[i] = f_trial\n                    new_population[i] = trial\n                    self.ages[i] = 0 # Reset age if improved.\n                else:\n                    new_fitness[i] = self.fitness[i]\n                    new_population[i] = self.population[i]\n\n            self.population = new_population\n            self.fitness = new_fitness\n            self.age_population()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm SelfAdaptiveDE scored 0.138 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["bf60bcf7-f45f-430e-a559-29f2036d8b47"], "operator": null, "metadata": {"aucs": [0.2764132192105432, 0]}}
{"id": "8ebd8fdc-e69a-4e54-bef4-0c424c8f3b30", "fitness": -Infinity, "name": "AdaptivePopulationDE", "description": "An adaptive Differential Evolution algorithm with a self-adaptive population size and a diversity-guided mutation strategy.", "code": "import numpy as np\n\nclass AdaptivePopulationDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, min_pop_size=10, max_pop_size=100, initial_mutation_factor=0.5, initial_crossover_rate=0.7, diversity_threshold=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.min_pop_size = min_pop_size\n        self.max_pop_size = max_pop_size\n        self.mutation_factor = initial_mutation_factor\n        self.crossover_rate = initial_crossover_rate\n        self.population = None\n        self.fitness = None\n        self.x_opt = None\n        self.f_opt = np.inf\n        self.success_mutation_factors = []\n        self.success_crossover_rates = []\n        self.diversity_threshold = diversity_threshold\n\n    def initialize_population(self, lb, ub):\n        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n\n    def calculate_diversity(self):\n        # Calculate the average pairwise distance between individuals\n        if self.pop_size <= 1:\n            return 1.0  # Maximum diversity if only one individual\n\n        distances = np.sum((self.population[:, None, :] - self.population[None, :, :]) ** 2, axis=2)\n        distances = np.triu(distances, k=1)  # Upper triangle to avoid duplicates\n        mean_distance = np.sum(distances) / (self.pop_size * (self.pop_size - 1) / 2)\n\n        # Normalize diversity to be between 0 and 1\n        diversity = np.clip(mean_distance / (10 * self.dim), 0, 1)  # scaling factor adjusted\n\n        return diversity\n\n\n    def adjust_population_size(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold and self.pop_size < self.max_pop_size:\n            # Low diversity, increase population size\n            increase_amount = min(int(self.pop_size * 0.1), self.max_pop_size - self.pop_size)\n            if increase_amount > 0:\n                new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(increase_amount, self.dim))\n                self.population = np.vstack((self.population, new_individuals))\n                new_fitness = np.array([func(x) for x in new_individuals])\n                self.fitness = np.concatenate((self.fitness, new_fitness))\n                self.pop_size += increase_amount\n                self.budget -= increase_amount\n\n        elif diversity > (1-self.diversity_threshold) and self.pop_size > self.min_pop_size:\n            # High diversity, decrease population size\n            decrease_amount = min(int(self.pop_size * 0.1), self.pop_size - self.min_pop_size)\n            if decrease_amount > 0:\n                #Remove worst performing individuals\n                worst_indices = np.argsort(self.fitness)[-decrease_amount:]\n                keep_indices = np.setdiff1d(np.arange(self.pop_size), worst_indices)\n                self.population = self.population[keep_indices]\n                self.fitness = self.fitness[keep_indices]\n                self.pop_size -= decrease_amount\n\n\n    def differential_evolution(self, func, lb, ub):\n        new_population = np.copy(self.population)\n        new_fitness = np.copy(self.fitness)\n\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[idxs]\n\n            # Diversity-guided mutation\n            diversity = self.calculate_diversity()\n            if diversity < self.diversity_threshold:\n                # If diversity is low, explore more\n                mutant = x_r1 + self.mutation_factor * (x_r2 - x_r3) + np.random.normal(0, 0.1, size=self.dim) #Increased Exploration\n            else:\n                # If diversity is high, exploit more\n                mutant = x_r1 + self.mutation_factor * (x_r2 - x_r3)\n\n            mutant = np.clip(mutant, lb, ub)\n\n            crossover = np.random.uniform(size=self.dim) < self.crossover_rate\n            trial = np.where(crossover, mutant, self.population[i])\n\n            f_trial = func(trial)\n            self.budget -= 1\n\n            if f_trial < self.f_opt:\n                self.f_opt = f_trial\n                self.x_opt = trial\n\n            if f_trial < self.fitness[i]:\n                new_fitness[i] = f_trial\n                new_population[i] = trial\n\n                # Store successful parameters\n                self.success_mutation_factors.append(self.mutation_factor)\n                self.success_crossover_rates.append(self.crossover_rate)\n\n        self.population = new_population\n        self.fitness = new_fitness\n\n    def adapt_parameters(self):\n        # Adapt mutation factor and crossover rate based on success history\n        if len(self.success_mutation_factors) > 0:\n            self.mutation_factor = np.mean(self.success_mutation_factors)\n            self.crossover_rate = np.mean(self.success_crossover_rates)\n\n            # Add some noise to prevent stagnation\n            self.mutation_factor = np.clip(self.mutation_factor + np.random.normal(0, 0.05), 0.1, 1.0) #Reduced Noise\n            self.crossover_rate = np.clip(self.crossover_rate + np.random.normal(0, 0.05), 0.1, 1.0) #Reduced Noise\n\n            self.success_mutation_factors = []\n            self.success_crossover_rates = []\n        else:\n            # If no success, increase exploration\n            self.mutation_factor = np.clip(self.mutation_factor + 0.1, 0.1, 1.0)\n            self.crossover_rate = np.clip(self.crossover_rate - 0.1, 0.1, 1.0)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(self.fitness)\n        self.x_opt = self.population[best_index]\n        self.f_opt = self.fitness[best_index]\n\n        while self.budget > 0:\n            self.adjust_population_size()\n            self.differential_evolution(func, lb, ub)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 144, in evaluate\n    algorithm(f_new)\n  File \"<string>\", line 131, in __call__\n  File \"<string>\", line 44, in adjust_population_size\nNameError: name 'func' is not defined\n.", "error": "", "parent_ids": ["db520efd-12a4-462a-a7ac-b5bfb3f34b51"], "operator": null, "metadata": {}}
{"id": "294cdded-9324-4238-9714-671b05a0e6b4", "fitness": 0.0, "name": "AdaptivePopulationSizeArchiveDE", "description": "An adaptive Differential Evolution algorithm that uses a dynamically updated external archive and adjusts its population size based on performance.", "code": "import numpy as np\n\nclass AdaptivePopulationSizeArchiveDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, archive_size=100, initial_mutation_factor=0.5, initial_crossover_rate=0.7, pop_size_adapt_freq=20):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.archive_size = archive_size\n        self.mutation_factor = initial_mutation_factor\n        self.crossover_rate = initial_crossover_rate\n        self.population = None\n        self.fitness = None\n        self.archive = []\n        self.archive_fitness = []\n        self.x_opt = None\n        self.f_opt = np.inf\n        self.success_mutation_factors = []\n        self.success_crossover_rates = []\n        self.pop_size_adapt_freq = pop_size_adapt_freq\n        self.generation = 0\n        self.min_pop_size = 10\n        self.max_pop_size = 100\n\n    def initialize_population(self, lb, ub):\n        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n\n    def update_archive(self, x, f):\n        if len(self.archive) < self.archive_size:\n            self.archive.append(x)\n            self.archive_fitness.append(f)\n        else:\n            worst_index = np.argmax(self.archive_fitness)\n            if f < self.archive_fitness[worst_index]:\n                self.archive[worst_index] = x\n                self.archive_fitness[worst_index] = f\n\n    def differential_evolution(self, func, lb, ub):\n        new_population = np.copy(self.population)\n        new_fitness = np.copy(self.fitness)\n\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[idxs]\n\n            # Incorporate archive member into mutation\n            if len(self.archive) > 0:\n                x_archive = self.archive[np.random.randint(0, len(self.archive))]\n                mutant = x_r1 + self.mutation_factor * (x_r2 - x_r3) + self.mutation_factor * (x_archive - self.population[i])\n            else:\n                mutant = x_r1 + self.mutation_factor * (x_r2 - x_r3)\n\n            mutant = np.clip(mutant, lb, ub)\n\n            crossover = np.random.uniform(size=self.dim) < self.crossover_rate\n            trial = np.where(crossover, mutant, self.population[i])\n\n            f_trial = func(trial)\n            self.budget -= 1\n\n            if f_trial < self.f_opt:\n                self.f_opt = f_trial\n                self.x_opt = trial\n\n            if f_trial < self.fitness[i]:\n                new_fitness[i] = f_trial\n                new_population[i] = trial\n                self.update_archive(trial, f_trial)\n\n                self.success_mutation_factors.append(self.mutation_factor)\n                self.success_crossover_rates.append(self.crossover_rate)\n\n        self.population = new_population\n        self.fitness = new_fitness\n\n    def adapt_parameters(self):\n        if len(self.success_mutation_factors) > 0:\n            self.mutation_factor = np.mean(self.success_mutation_factors)\n            self.crossover_rate = np.mean(self.success_crossover_rates)\n\n            self.mutation_factor = np.clip(self.mutation_factor + np.random.normal(0, 0.1), 0.1, 1.0)\n            self.crossover_rate = np.clip(self.crossover_rate + np.random.normal(0, 0.1), 0.1, 1.0)\n\n            self.success_mutation_factors = []\n            self.success_crossover_rates = []\n        else:\n            self.mutation_factor = np.clip(self.mutation_factor + 0.1, 0.1, 1.0)\n            self.crossover_rate = np.clip(self.crossover_rate - 0.1, 0.1, 1.0)\n\n    def adapt_population_size(self):\n        # Adjust population size based on archive performance\n        if len(self.archive) > 0:\n            archive_fitness_std = np.std(self.archive_fitness)\n            if archive_fitness_std < 1e-6:  # Archive is converging\n                self.pop_size = max(self.min_pop_size, int(self.pop_size * 0.8))  # Reduce pop size\n            else:\n                self.pop_size = min(self.max_pop_size, int(self.pop_size * 1.2))  # Increase pop size\n            self.pop_size = int(self.pop_size) # enforce that it is an integer\n        else:\n            self.pop_size = min(self.max_pop_size, int(self.pop_size * 1.1))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(self.fitness)\n        self.x_opt = self.population[best_index]\n        self.f_opt = self.fitness[best_index]\n        self.update_archive(self.x_opt, self.f_opt)\n\n        while self.budget > 0:\n            self.differential_evolution(func, lb, ub)\n            self.adapt_parameters()\n\n            self.generation += 1\n            if self.generation % self.pop_size_adapt_freq == 0:\n                self.adapt_population_size()\n                # Re-initialize population with new size\n                if self.budget > 0:\n                    old_pop_size = self.population.shape[0]\n                    new_population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n                    new_fitness = np.array([func(x) for x in new_population])\n                    self.budget -= self.pop_size\n                    \n                    # Carry over best solutions from old population if possible\n                    num_carry_over = min(old_pop_size, self.pop_size)\n                    if num_carry_over > 0 and old_pop_size>0:\n\n                        indices_to_carry = np.argsort(self.fitness)[:num_carry_over]\n                        new_population[:num_carry_over] = self.population[indices_to_carry]\n                        new_fitness[:num_carry_over] = self.fitness[indices_to_carry]\n                        \n                    self.population = new_population\n                    self.fitness = new_fitness\n                    \n                    best_index = np.argmin(self.fitness)\n                    if self.fitness[best_index] < self.f_opt:\n                        self.x_opt = self.population[best_index]\n                        self.f_opt = self.fitness[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptivePopulationSizeArchiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["db520efd-12a4-462a-a7ac-b5bfb3f34b51"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "d315e177-2d4e-4efb-863e-431b456fed75", "fitness": -Infinity, "name": "SOM_DE", "description": "Population-based algorithm that uses a self-organizing map (SOM) to learn the fitness landscape and guide the search, coupled with differential evolution for local refinement.", "code": "import numpy as np\n\nclass SOM_DE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, som_grid_size=10, learning_rate=0.1, mutation_factor=0.5, crossover_rate=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.som_grid_size = som_grid_size\n        self.learning_rate = learning_rate\n        self.mutation_factor = mutation_factor\n        self.crossover_rate = crossover_rate\n        self.som = None\n        self.population = None\n        self.fitness = None\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def initialize_som(self):\n        self.som = np.random.uniform(-1, 1, size=(self.som_grid_size, self.som_grid_size, self.dim))\n\n    def find_best_matching_unit(self, x):\n        distances = np.sum((self.som - x)**2, axis=2)\n        bmu_index = np.unravel_index(np.argmin(distances), distances.shape)\n        return bmu_index\n\n    def update_som(self, x, bmu_index):\n        distance = np.sqrt((np.arange(self.som_grid_size) - bmu_index[0])**2[:, None] + (np.arange(self.som_grid_size) - bmu_index[1])**2[None, :])\n        influence = np.exp(-(distance**2) / (2 * (self.som_grid_size/4)**2))  # Gaussian neighborhood\n        \n        for i in range(self.som_grid_size):\n            for j in range(self.som_grid_size):\n                self.som[i, j] += self.learning_rate * influence[i, j] * (x - self.som[i, j])\n\n\n    def differential_evolution(self, func):\n        for i in range(self.pop_size):\n            # Mutation\n            indices = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[indices]\n            x_mutated = x_r1 + self.mutation_factor * (x_r2 - x_r3)\n            x_mutated = np.clip(x_mutated, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            x_trial = np.copy(self.population[i])\n            for j in range(self.dim):\n                if np.random.rand() < self.crossover_rate:\n                    x_trial[j] = x_mutated[j]\n\n            f_trial = func(x_trial)\n            self.budget -= 1\n\n            if f_trial < self.fitness[i]:\n                self.fitness[i] = f_trial\n                self.population[i] = x_trial\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = x_trial\n            \n            if self.budget <= 0:\n                break\n\n\n    def __call__(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(self.fitness)\n        self.x_opt = self.population[best_index]\n        self.f_opt = self.fitness[best_index]\n\n        self.initialize_som()\n\n        while self.budget > 0:\n            # SOM Training\n            for x in self.population:\n                bmu_index = self.find_best_matching_unit(x)\n                self.update_som(x, bmu_index)\n\n            # Differential Evolution\n            self.differential_evolution(func)\n\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index]\n            \n            self.learning_rate = 0.1 * (self.budget / 10000)\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 79, in __call__\n  File \"<string>\", line 27, in update_som\nTypeError: 'int' object is not subscriptable\n.", "error": "", "parent_ids": ["c9ffb827-25e5-48f1-8132-026f534359a1"], "operator": null, "metadata": {}}
{"id": "f24a157e-a479-4a08-8452-18ffef8b2883", "fitness": 0.3394175722456424, "name": "CooperativeAdaptiveNeighborhoodSearch", "description": "Cooperative Adaptive Neighborhood Search: Employs multiple interacting neighborhoods with adaptive radii and migration of promising solutions between them to enhance exploration and exploitation.", "code": "import numpy as np\n\nclass CooperativeAdaptiveNeighborhoodSearch:\n    def __init__(self, budget=10000, dim=10, num_neighborhoods=5, pop_size=20, initial_radius=0.5, radius_decay=0.995, migration_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.num_neighborhoods = num_neighborhoods\n        self.pop_size = pop_size\n        self.initial_radius = initial_radius\n        self.radius = [initial_radius] * num_neighborhoods  # radius for each neighborhood\n        self.radius_decay = radius_decay\n        self.migration_rate = migration_rate\n        self.neighborhoods = []\n        self.fitness = []\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def initialize_neighborhood(self, func):\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        return population, fitness\n\n    def __call__(self, func):\n        # Initialize neighborhoods\n        for _ in range(self.num_neighborhoods):\n            population, fitness = self.initialize_neighborhood(func)\n            self.neighborhoods.append(population)\n            self.fitness.append(fitness)\n            self.budget -= self.pop_size\n\n        # Initial best solution\n        for i in range(self.num_neighborhoods):\n            best_index = np.argmin(self.fitness[i])\n            if self.fitness[i][best_index] < self.f_opt:\n                self.f_opt = self.fitness[i][best_index]\n                self.x_opt = self.neighborhoods[i][best_index]\n\n        while self.budget > 0:\n            for i in range(self.num_neighborhoods):\n                for j in range(self.pop_size):\n                    # Sample within the neighborhood\n                    mutation = np.random.normal(0, self.radius[i], size=self.dim)\n                    new_position = self.neighborhoods[i][j] + mutation\n                    new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n\n                    f_new = func(new_position)\n                    self.budget -= 1\n\n                    if f_new < self.f_opt:\n                        self.f_opt = f_new\n                        self.x_opt = new_position\n\n                    if f_new < self.fitness[i][j]:\n                        self.fitness[i][j] = f_new\n                        self.neighborhoods[i][j] = new_position\n\n                # Decay neighborhood radius\n                self.radius[i] *= self.radius_decay\n\n                # Migration of solutions between neighborhoods\n                if np.random.rand() < self.migration_rate:\n                    # Select a random neighborhood to migrate to\n                    target_neighborhood = np.random.choice(self.num_neighborhoods)\n                    if target_neighborhood != i:\n                        # Replace a random individual in the target neighborhood\n                        replace_index = np.random.randint(self.pop_size)\n                        best_index = np.argmin(self.fitness[i])\n                        self.neighborhoods[target_neighborhood][replace_index] = self.neighborhoods[i][best_index].copy()\n                        self.fitness[target_neighborhood][replace_index] = self.fitness[i][best_index]\n\n            # Update global best solution\n            for i in range(self.num_neighborhoods):\n                best_index = np.argmin(self.fitness[i])\n                if self.fitness[i][best_index] < self.f_opt:\n                    self.f_opt = self.fitness[i][best_index]\n                    self.x_opt = self.neighborhoods[i][best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm CooperativeAdaptiveNeighborhoodSearch scored 0.339 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c9ffb827-25e5-48f1-8132-026f534359a1"], "operator": null, "metadata": {"aucs": [0.14772742532011163, 0.24312660017140963, 0.35239755589223154, 0.31364159031988825, 0.25611371052409215, 0.31861837277624094, 0.2838306052891031, 0.27492746586310346, 0.25440011880507574, 0.16484775712082522, 0.30907208694809774, 0.9983055339008325, 0.2726310617883778, 0.2553581724771299, 0.6976085275445694, 0.3285831687695133, 0.277733914030708, 0.36070220289658395, 0.18975267181107502, 0.48897290266387905]}}
{"id": "0a22bd23-3c3c-4e7e-9ff2-7288fc328409", "fitness": 0.0, "name": "DynamicPopulationDE", "description": "Implements a Differential Evolution strategy with a dynamic population size and a self-adaptive mutation factor based on fitness improvement.", "code": "import numpy as np\n\nclass DynamicPopulationDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, F=0.5, CR=0.9, pop_growth_rate=0.05, pop_shrink_rate=0.02, F_adapt_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.initial_pop_size = initial_pop_size\n        self.F = F\n        self.CR = CR\n        self.pop_growth_rate = pop_growth_rate\n        self.pop_shrink_rate = pop_shrink_rate\n        self.F_adapt_rate = F_adapt_rate\n        self.population = None\n        self.fitness = None\n        self.x_opt = None\n        self.f_opt = np.inf\n        self.success_history = []\n\n\n    def __call__(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(self.fitness)\n        self.x_opt = self.population[best_index]\n        self.f_opt = self.fitness[best_index]\n\n        while self.budget > 0:\n            new_population = np.copy(self.population)\n            new_fitness = np.copy(self.fitness)\n            improvements = 0  # Track number of improvements in this generation\n\n\n            for i in range(self.pop_size):\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[idxs]\n\n                mutant = x_r1 + self.F * (x_r2 - x_r3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover = np.random.uniform(size=self.dim) < self.CR\n                trial = np.where(crossover, mutant, self.population[i])\n\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n                    improvements += 1\n\n                if f_trial < self.fitness[i]:\n                    new_fitness[i] = f_trial\n                    new_population[i] = trial\n\n            self.population = new_population\n            self.fitness = new_fitness\n\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index]\n\n            # Adapt population size based on improvement rate\n            improvement_rate = improvements / self.pop_size\n\n            if improvement_rate > self.pop_growth_rate and self.budget > self.dim:\n                # Increase population size\n                num_new = int(self.pop_size * self.pop_growth_rate)\n                new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(num_new, self.dim))\n                new_fitnesses = np.array([func(x) for x in new_individuals])\n                self.budget -= num_new\n\n                self.population = np.vstack((self.population, new_individuals))\n                self.fitness = np.concatenate((self.fitness, new_fitnesses))\n                self.pop_size += num_new\n\n            elif improvement_rate < self.pop_shrink_rate and self.pop_size > self.initial_pop_size:\n                # Decrease population size\n                num_remove = int(self.pop_size * self.pop_shrink_rate)\n                indices_to_remove = np.argsort(self.fitness)[-num_remove:]  # Remove worst individuals\n                self.population = np.delete(self.population, indices_to_remove, axis=0)\n                self.fitness = np.delete(self.fitness, indices_to_remove)\n                self.pop_size -= num_remove\n\n\n            # Adapt F based on improvement rate\n            if improvement_rate > 0.1:\n                self.F *= (1 + self.F_adapt_rate)\n            else:\n                self.F *= (1 - self.F_adapt_rate)\n            self.F = np.clip(self.F, 0.1, 1.0)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm DynamicPopulationDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["777cf317-dcd1-40bf-b084-5ad475bf651a"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "1a114249-0ac0-44cf-9c23-d9c5c409018e", "fitness": 0.5872398336319044, "name": "BanditDE", "description": "A self-adjusting Differential Evolution algorithm that dynamically adapts its parameters based on the success history of different parameter settings, using a bandit-based approach.", "code": "import numpy as np\n\nclass BanditDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, num_arms=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.num_arms = num_arms\n        self.arms = []\n        for _ in range(num_arms):\n            self.arms.append({\n                'F': np.random.uniform(0.3, 0.8),\n                'CR': np.random.uniform(0.3, 0.8),\n                'wins': 0,\n                'trials': 0\n            })\n        self.epsilon = 0.1  # Exploration rate\n        self.archive = []\n        self.archive_size = int(pop_size * 0.2)\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        for i in range(self.pop_size):\n            if fitness[i] < self.f_opt:\n                self.f_opt = fitness[i]\n                self.x_opt = self.population[i]\n\n        while self.budget > 0:\n            new_population = np.copy(self.population)\n            new_fitness = np.copy(fitness)\n\n            for i in range(self.pop_size):\n                # Bandit selection\n                if np.random.rand() < self.epsilon:\n                    arm_index = np.random.randint(self.num_arms)  # Explore\n                else:\n                    # Exploit: choose the arm with highest win rate\n                    win_rates = [arm['wins'] / (arm['trials'] + 1e-6) for arm in self.arms]\n                    arm_index = np.argmax(win_rates)\n\n                arm = self.arms[arm_index]\n                arm['trials'] += 1\n\n                # Differential Evolution with selected arm parameters\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[idxs]\n                mutant = x_r1 + arm['F'] * (x_r2 - x_r3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                crossover = np.random.uniform(size=self.dim) < arm['CR']\n                trial = np.where(crossover, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                if f_trial < fitness[i]:\n                    new_fitness[i] = f_trial\n                    new_population[i] = trial\n                    arm['wins'] += 1\n\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                    else:\n                        idx_to_replace = np.random.randint(0, self.archive_size)\n                        self.archive[idx_to_replace] = trial\n\n                else:\n                    new_fitness[i] = fitness[i]\n                    new_population[i] = self.population[i]\n                \n            self.population = new_population\n            fitness = new_fitness\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm BanditDE scored 0.587 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["bf60bcf7-f45f-430e-a559-29f2036d8b47"], "operator": null, "metadata": {"aucs": [0.2331876310166795, 0.5880723534386285, 0.5856614901608536, 0.7884288180988411, 0.4148491073000866, 0.6467788708316687, 0.43625836405094354, 0.5834234386904872, 0.6868894708939388, 0.557857400692129, 0.7849423940044421, 1.0, 0.36623570929286087, 0.6209345525658132, 0.7769425042239595, 0.7212099297547496, 0.4678880282722826, 0.773738769569405, 0.1918524150569142, 0.5196454247234026]}}
{"id": "60ed801a-34c5-445d-8beb-dac2159a0813", "fitness": 0.5499429730081312, "name": "AdaptiveDEGaussianLocalSearch", "description": "An adaptive Differential Evolution algorithm using a local search operator based on Gaussian perturbation around the best solution found so far, enhancing exploitation.", "code": "import numpy as np\n\nclass AdaptiveDEGaussianLocalSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, CR=0.7, local_search_prob=0.1, local_search_sigma=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.local_search_prob = local_search_prob\n        self.local_search_sigma = local_search_sigma\n        self.population = None\n        self.fitness = None\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def __call__(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(self.fitness)\n        self.x_opt = self.population[best_index]\n        self.f_opt = self.fitness[best_index]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Differential Evolution mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[idxs]\n                \n                # Mutation\n                v_trial = self.population[i] + self.F * (x_r2 - x_r3)\n                \n                # Crossover\n                u_trial = np.zeros(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == np.random.randint(self.dim):\n                        u_trial[j] = v_trial[j]\n                    else:\n                        u_trial[j] = self.population[i][j]\n                \n                u_trial = np.clip(u_trial, func.bounds.lb, func.bounds.ub)\n\n                # Local Search around best solution\n                if np.random.rand() < self.local_search_prob:\n                    mutation = np.random.normal(0, self.local_search_sigma, size=self.dim)\n                    u_trial = self.x_opt + mutation\n                    u_trial = np.clip(u_trial, func.bounds.lb, func.bounds.ub)\n\n                f_new = func(u_trial)\n                self.budget -= 1\n                \n                if f_new < self.f_opt:\n                    self.f_opt = f_new\n                    self.x_opt = u_trial\n\n                if f_new < self.fitness[i]:\n                    self.fitness[i] = f_new\n                    self.population[i] = u_trial\n\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveDEGaussianLocalSearch scored 0.550 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c9ffb827-25e5-48f1-8132-026f534359a1"], "operator": null, "metadata": {"aucs": [0.21433788922432373, 0.4791686480746957, 0.5256221200286826, 0.9263919937696321, 0.5215471442882509, 0.6579317741908475, 0.3205363860606023, 0.45928617440887065, 0.5921883591390815, 0.20811678951509316, 0.8667870002193883, 0.98710617872965, 0.40106124058880877, 0.42756851549017016, 0.9113207839096914, 0.613744955669599, 0.4026647780124295, 0.7935770636522232, 0.18653277639472876, 0.5033688887958554]}}
{"id": "9d8a4d57-cf96-40c7-9a7c-7b416cc2cbe6", "fitness": 0.5258720526290993, "name": "SelfAdaptiveDE_GLS", "description": "Implements a Self-Adaptive Differential Evolution with a Gaussian Local Search to refine solutions around promising areas.", "code": "import numpy as np\n\nclass SelfAdaptiveDE_GLS:\n    def __init__(self, budget=10000, dim=10, pop_size=50, initial_F=0.5, initial_CR=0.9, local_search_prob=0.1, local_search_sigma=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = initial_F * np.ones(pop_size)\n        self.CR = initial_CR * np.ones(pop_size)\n        self.local_search_prob = local_search_prob\n        self.local_search_sigma = local_search_sigma\n        self.lb = None\n        self.ub = None\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.lb = func.bounds.lb\n        self.ub = func.bounds.ub\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        \n        for i in range(self.pop_size):\n            if fitness[i] < self.f_opt:\n                self.f_opt = fitness[i]\n                self.x_opt = self.population[i]\n\n        while self.budget > 0:\n            new_population = np.copy(self.population)\n            new_fitness = np.copy(fitness)\n\n            for i in range(self.pop_size):\n                # Self-adaptive parameters\n                self.F[i] = np.clip(self.F[i] + 0.1 * np.random.randn(), 0.1, 0.9)\n                self.CR[i] = np.clip(self.CR[i] + 0.1 * np.random.randn(), 0.1, 0.9)\n\n                # Differential Evolution\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[idxs]\n                mutant = x_r1 + self.F[i] * (x_r2 - x_r3)\n                mutant = np.clip(mutant, self.lb, self.ub)\n                \n                crossover = np.random.uniform(size=self.dim) < self.CR[i]\n                trial = np.where(crossover, mutant, self.population[i])\n                \n                # Local Search\n                if np.random.rand() < self.local_search_prob:\n                    trial = trial + self.local_search_sigma * np.random.randn(self.dim)\n                    trial = np.clip(trial, self.lb, self.ub)\n                \n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n                    \n                if f_trial < fitness[i]:\n                    new_fitness[i] = f_trial\n                    new_population[i] = trial\n\n            self.population = new_population\n            fitness = new_fitness\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm SelfAdaptiveDE_GLS scored 0.526 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["bf60bcf7-f45f-430e-a559-29f2036d8b47"], "operator": null, "metadata": {"aucs": [0.19724861244116432, 0.31655341153356586, 0.4598414949183076, 0.7818958604944586, 0.5718034593703197, 0.6372414960744581, 0.3881853192584569, 0.4541428551819825, 0.5553705034399297, 0.4178311600491855, 0.7213166324834117, 0.9986959298280549, 0.30272362498213934, 0.47374677192806147, 0.7930387099675362, 0.6177363563968516, 0.40588611721591605, 0.7012265469308139, 0.21304814074485634, 0.5099080493425161]}}
{"id": "6eb459da-93e3-4559-82d8-34f6d7d625d7", "fitness": 0.4952314206494909, "name": "SOM_AdaptiveDE", "description": "An adaptive Differential Evolution algorithm employing a self-organizing map (SOM) to cluster solutions and guide the search based on cluster performance, dynamically adjusting mutation and crossover parameters.", "code": "import numpy as np\n\nclass SOM_AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, som_grid_size=10, initial_mutation_factor=0.5, initial_crossover_rate=0.7, learning_rate=0.1, sigma=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.som_grid_size = som_grid_size\n        self.mutation_factor = initial_mutation_factor\n        self.crossover_rate = initial_crossover_rate\n        self.learning_rate = learning_rate\n        self.sigma = sigma  # Neighborhood radius for SOM update\n        self.population = None\n        self.fitness = None\n        self.som = np.random.rand(som_grid_size, som_grid_size, dim)  # Initialize SOM weights\n        self.som_fitness = np.zeros((som_grid_size, som_grid_size))  # Fitness associated with each SOM node\n        self.x_opt = None\n        self.f_opt = np.inf\n        self.success_mutation_factors = []\n        self.success_crossover_rates = []\n\n    def initialize_population(self, lb, ub):\n        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n\n    def find_best_matching_unit(self, x):\n        distances = np.sum((self.som - x)**2, axis=2)\n        return np.unravel_index(np.argmin(distances), distances.shape)\n\n    def update_som(self, x, fitness, bmu):\n        # Gaussian neighborhood function\n        for i in range(self.som_grid_size):\n            for j in range(self.som_grid_size):\n                distance = np.sqrt((i - bmu[0])**2 + (j - bmu[1])**2)\n                influence = np.exp(-distance**2 / (2 * self.sigma**2))\n                self.som[i, j] += self.learning_rate * influence * (x - self.som[i, j])\n                self.som_fitness[i, j] = 0.9 * self.som_fitness[i,j] + 0.1 * fitness # EWMA fitness update for the node\n\n    def differential_evolution(self, func, lb, ub):\n        new_population = np.copy(self.population)\n        new_fitness = np.copy(self.fitness)\n\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[idxs]\n\n            bmu = self.find_best_matching_unit(self.population[i])\n            x_som = self.som[bmu] # Use the best matching unit from SOM.\n            \n            mutant = x_r1 + self.mutation_factor * (x_r2 - x_r3) + self.mutation_factor * (x_som - self.population[i]) # Use SOM for guiding mutation\n\n            mutant = np.clip(mutant, lb, ub)\n\n            crossover = np.random.uniform(size=self.dim) < self.crossover_rate\n            trial = np.where(crossover, mutant, self.population[i])\n\n            f_trial = func(trial)\n            self.budget -= 1\n\n            if f_trial < self.f_opt:\n                self.f_opt = f_trial\n                self.x_opt = trial\n\n            if f_trial < self.fitness[i]:\n                new_fitness[i] = f_trial\n                new_population[i] = trial\n                self.update_som(trial, f_trial, bmu)\n\n                # Store successful parameters\n                self.success_mutation_factors.append(self.mutation_factor)\n                self.success_crossover_rates.append(self.crossover_rate)\n\n        self.population = new_population\n        self.fitness = new_fitness\n\n    def adapt_parameters(self):\n        # Adapt mutation factor and crossover rate based on success history\n        if len(self.success_mutation_factors) > 0:\n            self.mutation_factor = np.mean(self.success_mutation_factors)\n            self.crossover_rate = np.mean(self.success_crossover_rates)\n\n            # Add some noise to prevent stagnation\n            self.mutation_factor = np.clip(self.mutation_factor + np.random.normal(0, 0.1), 0.1, 1.0)\n            self.crossover_rate = np.clip(self.crossover_rate + np.random.normal(0, 0.1), 0.1, 1.0)\n\n            self.success_mutation_factors = [] #reset\n            self.success_crossover_rates = [] #reset\n        else:\n            # If no success, increase exploration\n            self.mutation_factor = np.clip(self.mutation_factor + 0.1, 0.1, 1.0)\n            self.crossover_rate = np.clip(self.crossover_rate - 0.1, 0.1, 1.0) # Reduce to exploit less.\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.initialize_population(lb, ub)\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(self.fitness)\n        self.x_opt = self.population[best_index]\n        self.f_opt = self.fitness[best_index]\n\n        # Initial SOM training with initial population\n        for x, fitness in zip(self.population, self.fitness):\n             bmu = self.find_best_matching_unit(x)\n             self.update_som(x, fitness, bmu)\n\n\n        while self.budget > 0:\n            self.differential_evolution(func, lb, ub)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm SOM_AdaptiveDE scored 0.495 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["db520efd-12a4-462a-a7ac-b5bfb3f34b51"], "operator": null, "metadata": {"aucs": [0.14661627678224054, 0.22382541822421476, 0.483170791291506, 0.8865448086549155, 0.5628772742985866, 0.5386216371649264, 0.3306278114719492, 0.5643073312404204, 0.5086542949279781, 0.19836768454194786, 0.5788031358585187, 0.9978639872145287, 0.2946433878897523, 0.47299111816278006, 0.7692095257987743, 0.5691335664373974, 0.3899125223581478, 0.7009825692717179, 0.19528789171583727, 0.49218737968367543]}}
{"id": "7027dbbc-d36f-491c-8cc3-1b4835c16a6c", "fitness": 0.24787936479326061, "name": "SelfOrganizingSearch", "description": "A self-organizing search algorithm that adjusts step sizes based on local fitness landscape curvature and individual success rates, while also maintaining population diversity through a repulsion mechanism.", "code": "import numpy as np\n\nclass SelfOrganizingSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=40, initial_step_size=0.1, step_size_decay=0.99, success_memory=10, repulsion_factor=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.initial_step_size = initial_step_size\n        self.step_size = np.full(pop_size, initial_step_size)\n        self.step_size_decay = step_size_decay\n        self.success_memory = success_memory\n        self.success_history = np.zeros((pop_size, success_memory))\n        self.repulsion_factor = repulsion_factor\n        self.population = None\n        self.fitness = None\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def __call__(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(self.fitness)\n        self.x_opt = self.population[best_index]\n        self.f_opt = self.fitness[best_index]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Adaptive step size adjustment\n                success_rate = np.mean(self.success_history[i])\n                self.step_size[i] = self.initial_step_size * (self.step_size_decay ** (1 - success_rate))\n\n                # Generate a candidate solution\n                mutation = np.random.normal(0, self.step_size[i], size=self.dim)\n                new_position = self.population[i] + mutation\n                new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n\n                # Repulsion from other individuals to maintain diversity\n                for j in range(self.pop_size):\n                    if i != j:\n                        direction = self.population[i] - self.population[j]\n                        distance = np.linalg.norm(direction)\n                        if distance > 0:\n                            new_position += self.repulsion_factor * direction / (distance + 1e-8)\n                            new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n\n                f_new = func(new_position)\n                self.budget -= 1\n\n                # Update individual's position and success history\n                if f_new < self.f_opt:\n                    self.f_opt = f_new\n                    self.x_opt = new_position\n\n                if f_new < self.fitness[i]:\n                    self.fitness[i] = f_new\n                    self.population[i] = new_position\n                    self.success_history[i] = np.roll(self.success_history[i], 1)\n                    self.success_history[i][0] = 1\n                else:\n                    self.success_history[i] = np.roll(self.success_history[i], 1)\n                    self.success_history[i][0] = 0\n\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm SelfOrganizingSearch scored 0.248 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c9ffb827-25e5-48f1-8132-026f534359a1"], "operator": null, "metadata": {"aucs": [0.0761930931000534, 0.17733844369186025, 0.22540206872428692, 0.15757286443749763, 0.13454828781905126, 0.19510772502749762, 0.19773097767066228, 0.18323597997755714, 0.1598014242678355, 0.13810112463979762, 0.17102858130956333, 0.9981748549314929, 0.27208238357998604, 0.15486419663965334, 0.5097451114705677, 0.23469410534797275, 0.19363534467516474, 0.20577194497538287, 0.127282898004333, 0.4452758855749961]}}
