{"id": "c540ae0e-7e96-41d3-9ec5-5333e5d68138", "fitness": -Infinity, "name": "CMAES_R1", "description": "Covariance Matrix Adaptation Evolution Strategy with restarts and a simplified rank-one update to reduce computational complexity.", "code": "import numpy as np\n\nclass CMAES_R1:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damp=None, c1=None, restarts=3):\n        self.budget = budget\n        self.dim = dim\n        self.restarts = restarts\n        if popsize is None:\n            self.popsize = 4 + int(3 * np.log(self.dim))\n        else:\n            self.popsize = popsize\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        \n        self.cs = cs\n        if damp is None:\n            self.damp = 1 + 2 * np.max([0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1]) + self.cs\n        else:\n            self.damp = damp\n\n        if c1 is None:\n            self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        else:\n            self.c1 = c1\n\n    def __call__(self, func):\n        f_opt = np.Inf\n        x_opt = None\n        evals = 0\n        \n        for r in range(self.restarts):\n            # Initialization\n            mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n            sigma = 0.5  # Adjust initial sigma\n            p_sigma = np.zeros(self.dim)\n            C = np.eye(self.dim)\n            \n            while evals < self.budget:\n                # Sampling\n                z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n                x = mean[:, np.newaxis] + sigma * np.dot(np.linalg.cholesky(C), z)\n                x = np.clip(x, func.bounds.lb, func.bounds.ub)\n                \n                f = np.array([func(x[:, i]) for i in range(self.popsize)])\n                evals += self.popsize\n                \n                if np.any(f < f_opt):\n                  best_idx = np.argmin(f)\n                  if f[best_idx] < f_opt:\n                    f_opt = f[best_idx]\n                    x_opt = x[:, best_idx].copy()\n\n                # Selection and Recombination\n                idx = np.argsort(f)\n                x_mu = x[:, idx[:self.mu]]\n                mean_new = np.sum(x_mu * self.weights[np.newaxis, :], axis=1)\n\n                # Rank-one update\n                y = (mean_new - mean) / sigma\n                p_sigma = (1 - self.cs) * p_sigma + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * y\n                C = (1 - self.c1) * C + self.c1 * np.outer(p_sigma, p_sigma)\n                \n                # Adjust step-size\n                sigma *= np.exp((self.cs / self.damp) * (np.linalg.norm(p_sigma) / np.sqrt(self.dim) - 1))\n                \n                # Update mean\n                mean = mean_new\n                \n                if evals >= self.budget:\n                    break\n        \n        return f_opt, x_opt", "configspace": "", "generation": 0, "feedback": "An exception occurred: operands could not be broadcast together with shapes (2,6) (2,) (2,) .", "error": "", "parent_ids": [], "operator": null, "metadata": {}}
{"id": "2fab30d6-f6fe-4811-91a5-aabffc5345c0", "fitness": -Infinity, "name": "CMAES", "description": "Covariance Matrix Adaptation Evolution Strategy (CMA-ES) with restarts and improved step size adaptation.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damps=None, ccov1=None, ccovmu=None, restarts=3):\n        self.budget = budget\n        self.dim = dim\n        self.restarts = restarts\n\n        if popsize is None:\n            self.popsize = 4 + int(3 * np.log(self.dim))\n        else:\n            self.popsize = popsize\n\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        \n        self.cs = cs\n        if damps is None:\n            self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        else:\n            self.damps = damps\n        \n        self.ccov1 = (1 / self.mueff) * min(1, (self.budget / self.popsize) / (self.dim**2)) if ccov1 is None else ccov1\n        self.ccovmu = min(1 - self.ccov1, (2 * (self.mueff - 2 + 1/self.mueff)) / ((self.dim + 2)**2 + self.mueff)) if ccovmu is None else ccovmu\n        self.chiN = self.dim**0.5 * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n    def __call__(self, func):\n        f_opt = np.Inf\n        x_opt = None\n        evals = 0\n        \n        for r in range(self.restarts):\n            # Initialization\n            mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n            sigma = 0.5 \n            C = np.eye(self.dim)\n            P_c = np.zeros(self.dim)\n            P_sigma = np.zeros(self.dim)\n\n            B = np.eye(self.dim)\n            D = np.ones(self.dim)\n            C_eig = 0\n            \n            while evals < self.budget:\n                # Sampling\n                Z = np.random.normal(size=(self.dim, self.popsize))\n                y = B @ (D[:, None] * Z)\n                x = mean[:, None] + sigma * y\n                x = np.clip(x, func.bounds.lb, func.bounds.ub)\n                \n                # Evaluation\n                fitness = np.array([func(xi) for xi in x.T])\n                evals += self.popsize\n                \n                # Sort by fitness\n                idx = np.argsort(fitness)\n                fitness = fitness[idx]\n                x = x[:, idx]\n                \n                # Update optimal solution\n                if fitness[0] < f_opt:\n                    f_opt = fitness[0]\n                    x_opt = x[:, 0].copy()\n\n                # Update mean\n                mean_old = mean.copy()\n                mean = np.sum(self.weights * x[:, :self.mu], axis=1)\n                \n                # Cumulation\n                y_mean = (mean - mean_old) / sigma\n                P_c = (1 - self.cs) * P_c + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * y_mean\n                hsigma = np.linalg.norm(P_c) / np.sqrt(1 - (1 - self.cs)**(2 * (evals / self.popsize))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n                P_sigma = (1 - self.damps) * P_sigma + hsigma * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (B @ Z[:, :self.mu] @ self.weights)\n\n                # Covariance matrix adaptation\n                C_old = C.copy()\n                C = (1 - self.ccov1 - self.ccovmu + self.ccov1 * (1 - hsigma) * self.ccov1) * C + self.ccov1 * P_c[:, None] @ P_c[None, :] + self.ccovmu * B @ ((D[:, None] * Z[:, :self.mu]) @ np.diag(self.weights) @ (D[:, None] * Z[:, :self.mu]).T) @ B.T\n                \n                # Rank-one update of covariance matrix\n                # Eigen decomposition\n                C_eig += 1\n                if C_eig > self.popsize / (self.ccov1 + self.ccovmu) / self.dim / 10:  # limit the update frequency\n                   C_eig = 0\n                   C = np.triu(C) + np.triu(C, 1).T # enforce symmetry\n                   try:\n                      D, B = np.linalg.eigh(C)           # eigen decomposition, B==normalized eigenvectors\n                   except np.linalg.LinAlgError as e:\n                       C = C_old # Revert to old C\n                       D, B = np.linalg.eigh(C) # Repeat decomposition\n                       print(\"Eigenvalue decomposition failed:\", e)\n\n                   D = np.sqrt(D.real)                # enforce eigenvalues are real\n\n                # Step size control\n                sigma = sigma * np.exp((self.cs / self.damps) * (np.linalg.norm(P_sigma) / self.chiN - 1))\n                \n                if sigma < 1e-10:\n                    sigma = 0.5\n                    C = np.eye(self.dim)\n                    P_c = np.zeros(self.dim)\n                    P_sigma = np.zeros(self.dim)\n                    B = np.eye(self.dim)\n                    D = np.ones(self.dim)\n                    C_eig = 0\n\n        return f_opt, x_opt", "configspace": "", "generation": 0, "feedback": "An exception occurred: operands could not be broadcast together with shapes (2,6) (2,) (2,) .", "error": "", "parent_ids": [], "operator": null, "metadata": {}}
{"id": "15ffe61e-b8b5-425d-a73c-1372bbda485c", "fitness": 0.6626544514321424, "name": "CMAES", "description": "Covariance Matrix Adaptation Evolution Strategy with resampling and budget-aware step-size adaptation.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damp=None, c_cov=None):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        \n        self.cs = cs\n        self.damps = damp if damp is not None else 1 + 2 * np.max([0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1]) + self.cs\n        self.c_cov_mu = self.mueff / (self.dim + 2)**2\n        self.c_cov_one = 0.3 / ((self.dim + 1.3)**2 + self.mueff)\n        if c_cov is not None:\n            self.c_cov = c_cov\n        else:\n            self.c_cov = (1 / self.mueff) * self.c_cov_mu + (1 - 1 / self.mueff) * self.c_cov_one\n\n        self.chiN = self.dim**0.5 * (1 - 1 / (4 * self.dim) + 1 / (21 * self.dim**2))\n\n        self.xmean = None\n        self.sigma = 0.5\n        self.C = None\n        self.B = None\n        self.D = None\n        self.pc = None\n        self.ps = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n\n    def initialize(self):\n        self.xmean = np.random.uniform(-5, 5, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.B = np.eye(self.dim)\n        self.D = np.ones(self.dim)\n\n    def sample_population(self):\n        z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n        y = self.B @ np.diag(self.D) @ z\n        x = self.xmean[:, np.newaxis] + self.sigma * y\n        return x.T\n\n    def resample_individual(self, func, x, lb, ub):\n        while True:\n            if np.all(x >= lb) and np.all(x <= ub):\n                return x\n            x = self.xmean + self.sigma * self.B @ (self.D * np.random.randn(self.dim))\n            \n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.initialize()\n\n        while self.evals < self.budget:\n            x = self.sample_population()\n            \n            # Resample out-of-bounds individuals\n            for i in range(self.popsize):\n                if not np.all(x[i] >= lb) or not np.all(x[i] <= ub):\n                    x[i] = self.resample_individual(func, x[i], lb, ub)\n            \n            f = np.array([func(xi) for xi in x])\n            self.evals += self.popsize\n\n            if np.any(f < self.f_opt):\n                best_idx = np.argmin(f)\n                if f[best_idx] < self.f_opt:\n                    self.f_opt = f[best_idx]\n                    self.x_opt = x[best_idx]\n\n            idx = np.argsort(f)\n            x_sorted = x[idx[:self.mu]]\n            y = (x_sorted - self.xmean) / self.sigma\n            \n            self.xmean = np.sum(self.weights[:, np.newaxis] * x_sorted, axis=0)\n\n            y_w = np.sum(self.weights[:, np.newaxis] * y, axis=0)\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * self.B @ (self.D * y_w)\n            \n            norm_ps = np.linalg.norm(self.ps)\n            \n            self.sigma *= np.exp((self.cs / self.damps) * (norm_ps / self.chiN - 1))\n            self.sigma = min(self.sigma, 5.0) # Budget aware step size.\n\n            self.pc = (1 - self.c_cov) * self.pc + np.sqrt(self.c_cov * (2 - self.c_cov) * self.mueff) * y_w\n\n            C_temp = self.c_cov_one * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :])\n            C_temp += self.c_cov_mu * y.T @ np.diag(self.weights) @ y\n\n            self.C = (1 - self.c_cov_one - self.c_cov_mu) * self.C + C_temp\n\n            try:\n                self.D, self.B = np.linalg.eigh(self.C)\n                self.D = np.sqrt(np.maximum(self.D, 1e-16))\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)\n                self.B = np.eye(self.dim)\n                self.D = np.ones(self.dim)\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm CMAES scored 0.663 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.2441350828256218, 0.17703334254550407, 0.49831204943072893, 0.9681406114844946, 0.9594794654170654, 0.9717167432005878, 0.3605974380286924, 0.859684342679199, 0.9675498716113454, 0.19658816441447147, 0.9303069035901093, 0.9952509654017353, 0.7651518792819814, 0.7253058031125297, 0.9406818518616276, 0.7091249149081071, 0.4299943017749901, 0.9720769427687485, 0.12820638163154474, 0.4537519726737639]}}
{"id": "04e7bbcc-f9e3-4b75-bdaf-82c893b55f19", "fitness": 0.6458530583380087, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with Archive and Restart: Adapts DE parameters based on success, uses an archive to maintain diversity, and restarts search when stagnation is detected.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, f=0.5, cr=0.9, restart_trigger=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.f = f  # Mutation factor\n        self.cr = cr # Crossover rate\n        self.restart_trigger = restart_trigger # Percentage of budget before restart\n        self.archive = []\n        self.archive_fitness = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        \n        for i in range(self.pop_size):\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.population[i]\n        \n        while self.evals < self.budget:\n            \n            # Restart strategy\n            if self.evals > self.restart_trigger * self.budget:\n                if np.std(self.fitness) < 1e-6:  # Stagnation check\n                    self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                    self.fitness = np.array([func(x) for x in self.population])\n                    self.evals += self.pop_size\n                    for j in range(self.pop_size):\n                        if self.fitness[j] < self.f_opt:\n                            self.f_opt = self.fitness[j]\n                            self.x_opt = self.population[j]\n                    continue\n            \n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(np.arange(self.pop_size), 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                \n                # Add archive to selection pool with some probability\n                if len(self.archive) > 0 and np.random.rand() < 0.1:  # Adjust probability as needed\n                    idx_archive = np.random.randint(len(self.archive))\n                    x4 = self.archive[idx_archive]\n                    idxs = np.random.choice(np.arange(self.pop_size), 2, replace=False)\n                    x1, x2 = self.population[idxs]\n                    v = x1 + self.f * (x2 - x3)\n                    \n                else:\n                    v = x1 + self.f * (x2 - x3)\n\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = np.random.rand(self.dim) < self.cr\n                u = v * u + self.population[i] * (~u)\n                \n                # Evaluation\n                f = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f < self.fitness[i]:\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i])\n                        self.archive_fitness.append(self.fitness[i])\n                    else:\n                        # Replace random element in archive\n                        idx_replace = np.random.randint(self.archive_size)\n                        self.archive[idx_replace] = self.population[i]\n                        self.archive_fitness[idx_replace] = self.fitness[i]\n                    \n                    self.fitness[i] = f\n                    self.population[i] = u\n                    \n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n\n                # Adaptive Parameter Control (simple adaptation)\n                if np.random.rand() < 0.1:  # Adaptation probability\n                    self.f = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n                    self.cr = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0)\n\n                if self.evals >= self.budget:\n                    break\n                    \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDE scored 0.646 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.30702698686685925, 0.5871799766847996, 0.5982261055358415, 0.8705924268808614, 0.6813545285996451, 0.6899149593349903, 0.5815152146784881, 0.5964700660424842, 0.6400964635250376, 0.5328297672752712, 0.8935117394589946, 0.9938483511141806, 0.5919036790830517, 0.6313211455690358, 0.8293605025501318, 0.6735761863276766, 0.5773224479627821, 0.7531342835538243, 0.31071677803958986, 0.5771595576766284]}}
{"id": "4a472858-6a83-45ef-a44e-eecf4cc42da8", "fitness": -Infinity, "name": "CMAES_R1_Coord", "description": "A simplified CMA-ES with coordinate-wise adaptation of the step size and a rank-one covariance update for faster computation.", "code": "import numpy as np\n\nclass CMAES_R1_Coord:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damp=None, c1=None, restarts=3):\n        self.budget = budget\n        self.dim = dim\n        self.restarts = restarts\n        if popsize is None:\n            self.popsize = 4 + int(3 * np.log(self.dim))\n        else:\n            self.popsize = popsize\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        \n        self.cs = cs\n        if damp is None:\n            self.damp = 1 + 2 * np.max([0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1]) + self.cs\n        else:\n            self.damp = damp\n\n        if c1 is None:\n            self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        else:\n            self.c1 = c1\n\n    def __call__(self, func):\n        f_opt = np.Inf\n        x_opt = None\n        evals = 0\n        \n        for r in range(self.restarts):\n            # Initialization\n            mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n            sigma = np.full(self.dim, 0.5)  # Coordinate-wise step size\n            p_sigma = np.zeros(self.dim)\n            C = np.eye(self.dim)\n            \n            while evals < self.budget:\n                # Sampling\n                z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n                x = mean[:, np.newaxis] + sigma[:, np.newaxis] * z\n                x = np.clip(x, func.bounds.lb, func.bounds.ub)\n                \n                f = np.array([func(x[:, i]) for i in range(self.popsize)])\n                evals += self.popsize\n                \n                if np.any(f < f_opt):\n                  best_idx = np.argmin(f)\n                  if f[best_idx] < f_opt:\n                    f_opt = f[best_idx]\n                    x_opt = x[:, best_idx].copy()\n\n                # Selection and Recombination\n                idx = np.argsort(f)\n                x_mu = x[:, idx[:self.mu]]\n                mean_new = np.sum(x_mu * self.weights[np.newaxis, :], axis=1)\n\n                # Rank-one update\n                y = (mean_new - mean) / sigma\n                p_sigma = (1 - self.cs) * p_sigma + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * y\n                C = (1 - self.c1) * C + self.c1 * np.outer(p_sigma, p_sigma)\n                \n                # Adjust step-size\n                sigma *= np.exp((self.cs / self.damp) * (p_sigma / np.sqrt(self.dim) - 1/self.dim)) # Coordinate-wise adaptation\n                \n                # Update mean\n                mean = mean_new\n                \n                if evals >= self.budget:\n                    break\n        \n        return f_opt, x_opt", "configspace": "", "generation": 1, "feedback": "An exception occurred: operands could not be broadcast together with shapes (2,6) (2,) (2,) .", "error": "", "parent_ids": ["c540ae0e-7e96-41d3-9ec5-5333e5d68138"], "operator": null, "metadata": {}}
{"id": "17d56162-a608-4821-8fad-51ba998b53c4", "fitness": -Infinity, "name": "HybridDE", "description": "Population-based algorithm that combines Differential Evolution's mutation and crossover with a gradient-based local search to refine promising solutions.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, f=0.5, cr=0.9, local_search_freq=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.f = f  # Mutation factor\n        self.cr = cr # Crossover rate\n        self.local_search_freq = local_search_freq # Frequency of local search application\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        \n        for i in range(self.pop_size):\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.population[i]\n        \n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(np.arange(self.pop_size), 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                v = x1 + self.f * (x2 - x3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = np.random.rand(self.dim) < self.cr\n                u = v * u + self.population[i] * (~u)\n                \n                # Evaluation\n                f = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.population[i] = u\n                    \n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n                        \n                # Local Search\n                if np.random.rand() < self.local_search_freq:\n                    \n                    # Define the objective function for local search (closure)\n                    def objective(x):\n                        return func(x)\n\n                    # Perform local search using L-BFGS-B\n                    bounds = [(func.bounds.lb[j], func.bounds.ub[j]) for j in range(self.dim)]\n                    res = minimize(objective, self.population[i], method='L-BFGS-B', bounds=bounds, options={'maxiter': 5}) # Reduced maxiter to control budget\n                    \n                    if res.success:  # Only update if the optimization was successful\n                        f_local = func(res.x)\n                        self.evals += 1  # Increment evaluation count\n                        \n                        if f_local < self.fitness[i]:\n                            self.fitness[i] = f_local\n                            self.population[i] = res.x\n\n                            if f_local < self.f_opt:\n                                self.f_opt = f_local\n                                self.x_opt = res.x\n                \n\n                if self.evals >= self.budget:\n                    break\n                    \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "An exception occurred: name 'minimize' is not defined.", "error": "", "parent_ids": ["04e7bbcc-f9e3-4b75-bdaf-82c893b55f19"], "operator": null, "metadata": {}}
{"id": "3189432e-b829-43c1-a7d3-6bb2553b7062", "fitness": 0.0, "name": "PopulationBasedAdam", "description": "Adaptive Moment Estimation (Adam) optimizer with a population-based approach, using multiple search agents that share information and adapt their step sizes based on historical gradient information.", "code": "import numpy as np\n\nclass PopulationBasedAdam:\n    def __init__(self, budget=10000, dim=10, popsize=20, lr=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize\n        self.lr = lr\n        self.beta1 = beta1\n        self.beta2 = beta2\n        self.epsilon = epsilon\n\n        self.x = np.random.uniform(-5, 5, size=(self.popsize, self.dim))\n        self.m = np.zeros((self.popsize, self.dim))\n        self.v = np.zeros((self.popsize, self.dim))\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n        self.t = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n\n        while self.evals < self.budget:\n            self.t += 1\n            f = np.array([func(xi) for xi in self.x])\n            self.evals += self.popsize\n\n            if np.any(f < self.f_opt):\n                best_idx = np.argmin(f)\n                if f[best_idx] < self.f_opt:\n                    self.f_opt = f[best_idx]\n                    self.x_opt = self.x[best_idx]\n\n            # Calculate gradients (finite difference approximation)\n            grad = np.zeros((self.popsize, self.dim))\n            delta = 1e-4  # Step size for finite difference\n            for i in range(self.popsize):\n                for j in range(self.dim):\n                    x_plus = self.x[i].copy()\n                    x_minus = self.x[i].copy()\n                    x_plus[j] += delta\n                    x_minus[j] -= delta\n\n                    # Clip to bounds\n                    x_plus[j] = np.clip(x_plus[j], lb[j], ub[j])\n                    x_minus[j] = np.clip(x_minus[j], lb[j], ub[j])\n\n                    f_plus = func(x_plus)\n                    f_minus = func(x_minus)\n                    self.evals += 2  # Account for extra function evaluations\n\n                    grad[i, j] = (f_plus - f_minus) / (2 * delta)\n\n            # Adam update\n            self.m = self.beta1 * self.m + (1 - self.beta1) * grad\n            self.v = self.beta2 * self.v + (1 - self.beta2) * (grad ** 2)\n\n            m_hat = self.m / (1 - self.beta1 ** self.t)\n            v_hat = self.v / (1 - self.beta2 ** self.t)\n\n            self.x = self.x - self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)\n\n            # Clip to bounds\n            self.x = np.clip(self.x, lb, ub)\n\n            # Sharing information: Move worse solutions towards the best\n            worst_idx = np.argsort(f)[-self.popsize // 4:] # consider the 25% worst solutions\n            for i in worst_idx:\n                self.x[i] = 0.5 * self.x[i] + 0.5 * self.x_opt  # Move towards best solution\n\n\n            if self.evals >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm PopulationBasedAdam scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["15ffe61e-b8b5-425d-a73c-1372bbda485c"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "ba8bcf78-1269-42c7-b418-5537d2fe4518", "fitness": -Infinity, "name": "CMAES", "description": "CMA-ES with simplified covariance update, step-size adaptation, and a saturation mechanism to prevent premature convergence.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damps=None, ccov1=None, ccovmu=None, restarts=3):\n        self.budget = budget\n        self.dim = dim\n        self.restarts = restarts\n\n        if popsize is None:\n            self.popsize = 4 + int(3 * np.log(self.dim))\n        else:\n            self.popsize = popsize\n\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        \n        self.cs = cs\n        if damps is None:\n            self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        else:\n            self.damps = damps\n        \n        self.ccov1 = (1 / self.mueff) * min(1, (self.budget / self.popsize) / (self.dim**2)) if ccov1 is None else ccov1\n        self.ccovmu = min(1 - self.ccov1, (2 * (self.mueff - 2 + 1/self.mueff)) / ((self.dim + 2)**2 + self.mueff)) if ccovmu is None else ccovmu\n        self.chiN = self.dim**0.5 * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n    def __call__(self, func):\n        f_opt = np.Inf\n        x_opt = None\n        evals = 0\n        \n        for r in range(self.restarts):\n            # Initialization\n            mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n            sigma = 0.5\n            C = np.eye(self.dim)\n            P_c = np.zeros(self.dim)\n            P_sigma = np.zeros(self.dim)\n\n            while evals < self.budget:\n                # Sampling\n                Z = np.random.normal(size=(self.dim, self.popsize))\n                y = np.linalg.cholesky(C) @ Z  # Use Cholesky decomposition\n                x = mean[:, None] + sigma * y\n                x = np.clip(x, func.bounds.lb, func.bounds.ub)\n                \n                # Evaluation\n                fitness = np.array([func(xi) for xi in x.T])\n                evals += self.popsize\n                \n                # Sort by fitness\n                idx = np.argsort(fitness)\n                fitness = fitness[idx]\n                x = x[:, idx]\n                \n                # Update optimal solution\n                if fitness[0] < f_opt:\n                    f_opt = fitness[0]\n                    x_opt = x[:, 0].copy()\n\n                # Update mean\n                mean_old = mean.copy()\n                mean = np.sum(self.weights * x[:, :self.mu], axis=1)\n\n                # Cumulation\n                y_mean = (mean - mean_old) / sigma\n                P_c = (1 - self.cs) * P_c + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * y_mean\n                P_sigma = (1 - self.damps) * P_sigma + np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (Z[:, :self.mu] @ self.weights)\n\n                # Covariance matrix adaptation\n                C = (1 - self.ccov1 - self.ccovmu) * C + self.ccov1 * np.outer(P_c, P_c) + self.ccovmu * (Z[:, :self.mu] @ np.diag(self.weights) @ Z[:, :self.mu].T)\n\n                # Step size control\n                sigma = sigma * np.exp((self.cs / self.damps) * (np.linalg.norm(P_sigma) / self.chiN - 1))\n                sigma = min(sigma, 2) # Saturation, prevent too large steps\n                sigma = max(sigma, 1e-10) # Prevent premature convergence\n\n        return f_opt, x_opt", "configspace": "", "generation": 1, "feedback": "An exception occurred: operands could not be broadcast together with shapes (2,6) (2,) (2,) .", "error": "", "parent_ids": ["2fab30d6-f6fe-4811-91a5-aabffc5345c0"], "operator": null, "metadata": {}}
{"id": "a4a72d06-a975-4906-83d0-f6934cceb6cc", "fitness": -Infinity, "name": "CMAES", "description": "CMA-ES with improved error handling, eigenvalue decomposition frequency and simplified step size adaptation and restart mechanism.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damps=None, ccov1=None, ccovmu=None, restarts=3):\n        self.budget = budget\n        self.dim = dim\n        self.restarts = restarts\n\n        if popsize is None:\n            self.popsize = 4 + int(3 * np.log(self.dim))\n        else:\n            self.popsize = popsize\n\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        \n        self.cs = cs\n        if damps is None:\n            self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        else:\n            self.damps = damps\n        \n        self.ccov1 = (1 / self.mueff) * min(1, (self.budget / self.popsize) / (self.dim**2)) if ccov1 is None else ccov1\n        self.ccovmu = min(1 - self.ccov1, (2 * (self.mueff - 2 + 1/self.mueff)) / ((self.dim + 2)**2 + self.mueff)) if ccovmu is None else ccovmu\n        self.chiN = self.dim**0.5 * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n    def __call__(self, func):\n        f_opt = np.Inf\n        x_opt = None\n        evals = 0\n        \n        for r in range(self.restarts):\n            # Initialization\n            mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n            sigma = 0.5 \n            C = np.eye(self.dim)\n            P_c = np.zeros(self.dim)\n            P_sigma = np.zeros(self.dim)\n\n            B = np.eye(self.dim)\n            D = np.ones(self.dim)\n            C_eig = 0\n            \n            while evals < self.budget:\n                # Sampling\n                Z = np.random.normal(size=(self.dim, self.popsize))\n                y = B @ (D[:, None] * Z)\n                x = mean[:, None] + sigma * y\n                x = np.clip(x, func.bounds.lb, func.bounds.ub)\n                \n                # Evaluation\n                fitness = np.array([func(xi) for xi in x.T])\n                evals += self.popsize\n                \n                # Sort by fitness\n                idx = np.argsort(fitness)\n                fitness = fitness[idx]\n                x = x[:, idx]\n                \n                # Update optimal solution\n                if fitness[0] < f_opt:\n                    f_opt = fitness[0]\n                    x_opt = x[:, 0].copy()\n\n                # Update mean\n                mean_old = mean.copy()\n                mean = np.sum(self.weights * x[:, :self.mu], axis=1)\n                \n                # Cumulation\n                y_mean = (mean - mean_old) / sigma\n                P_c = (1 - self.cs) * P_c + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * y_mean\n                hsigma = np.linalg.norm(P_c) / np.sqrt(1 - (1 - self.cs)**(2 * (evals / self.popsize))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n                P_sigma = (1 - self.damps) * P_sigma + hsigma * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (B @ Z[:, :self.mu] @ self.weights)\n\n                # Covariance matrix adaptation\n                C_old = C.copy()\n                C = (1 - self.ccov1 - self.ccovmu + self.ccov1 * (1 - hsigma) * self.ccov1) * C + self.ccov1 * P_c[:, None] @ P_c[None, :] + self.ccovmu * B @ ((D[:, None] * Z[:, :self.mu]) @ np.diag(self.weights) @ (D[:, None] * Z[:, :self.mu]).T) @ B.T\n                \n                # Eigen decomposition\n                C_eig += 1\n                if C_eig > self.popsize / (self.ccov1 + self.ccovmu) / self.dim / 10:  # limit the update frequency\n                   C_eig = 0\n                   C = np.triu(C) + np.triu(C, 1).T # enforce symmetry\n                   try:\n                      D, B = np.linalg.eigh(C)           # eigen decomposition, B==normalized eigenvectors\n                   except np.linalg.LinAlgError as e:\n                       C = C_old # Revert to old C\n                       D, B = np.linalg.eigh(C) # Repeat decomposition\n\n                   D = np.sqrt(D.real)                # enforce eigenvalues are real\n\n                # Step size control\n                sigma = sigma * np.exp((self.cs / self.damps) * (np.linalg.norm(P_sigma) / self.chiN - 1))\n                \n                if sigma < 1e-10 or not np.all(np.isfinite(mean)) or not np.all(np.isfinite(sigma)):\n                    # Restart strategy\n                    mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                    sigma = 0.5\n                    C = np.eye(self.dim)\n                    P_c = np.zeros(self.dim)\n                    P_sigma = np.zeros(self.dim)\n                    B = np.eye(self.dim)\n                    D = np.ones(self.dim)\n                    C_eig = 0\n\n        return f_opt, x_opt", "configspace": "", "generation": 1, "feedback": "An exception occurred: operands could not be broadcast together with shapes (2,6) (2,) (2,) .", "error": "", "parent_ids": ["2fab30d6-f6fe-4811-91a5-aabffc5345c0"], "operator": null, "metadata": {}}
{"id": "9767cfbf-4577-4dc3-a30a-5a33de0dfdfc", "fitness": 0.2950110171626906, "name": "AdaptiveGradientDescent", "description": "Gradient Estimation using Finite Differences with adaptive step size and momentum.", "code": "import numpy as np\n\nclass AdaptiveGradientDescent:\n    def __init__(self, budget=10000, dim=10, learning_rate=0.1, momentum=0.9, initial_step_size=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.learning_rate = learning_rate\n        self.momentum = momentum\n        self.initial_step_size = initial_step_size\n        self.velocity = np.zeros(dim)\n        self.step_size = np.full(dim, initial_step_size)\n\n    def __call__(self, func):\n        x = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        f_opt = func(x)\n        x_opt = x.copy()\n        evals = 1\n\n        while evals < self.budget:\n            # Estimate gradient using finite differences\n            gradient = np.zeros(self.dim)\n            for i in range(self.dim):\n                x_plus = x.copy()\n                x_minus = x.copy()\n\n                x_plus[i] += self.step_size[i]\n                x_minus[i] -= self.step_size[i]\n\n                x_plus = np.clip(x_plus, func.bounds.lb, func.bounds.ub)\n                x_minus = np.clip(x_minus, func.bounds.lb, func.bounds.ub)\n\n                f_plus = func(x_plus)\n                f_minus = func(x_minus)\n                evals += 2\n\n                gradient[i] = (f_plus - f_minus) / (2 * self.step_size[i])\n\n                # Adjust step size based on the function values\n                if f_plus < f_minus:\n                    self.step_size[i] *= 1.1  # Increase step if moving downhill\n                else:\n                    self.step_size[i] *= 0.5  # Decrease step if moving uphill\n\n                self.step_size[i] = np.clip(self.step_size[i], 1e-6, 1.0)\n\n                if evals >= self.budget:\n                    break\n\n            # Update position using momentum\n            self.velocity = self.momentum * self.velocity - self.learning_rate * gradient\n            x = x + self.velocity\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n\n            f = func(x)\n            evals += 1\n\n            if f < f_opt:\n                f_opt = f\n                x_opt = x.copy()\n            \n            if evals >= self.budget:\n                break\n\n        return f_opt, x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveGradientDescent scored 0.295 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c540ae0e-7e96-41d3-9ec5-5333e5d68138"], "operator": null, "metadata": {"aucs": [0.04678090091256948, 0.1328991237673146, 0.197618792678927, 0.8775032238100032, 0.09813282411046798, 0.16865882982257951, 0.22004116807541096, 0.2476263622874315, 0.17352591959598662, 0.12350276213892597, 0.8396931112588291, 0.19457265874918206, 0.17232035720678374, 0.1491312847158679, 0.5511081366843623, 0.32706592182741023, 0.21014727005921763, 0.9253347266839268, 0.0738013480817915, 0.1707556207868245]}}
{"id": "2771f839-c30b-4b4e-ae0e-a440daeab0df", "fitness": 0.7066674833515456, "name": "AdaptiveDE", "description": "Simplified Adaptive Differential Evolution with parameter adaptation based on population diversity and reduced complexity.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, f=0.5, cr=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.f = f  # Mutation factor\n        self.cr = cr # Crossover rate\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        \n        for i in range(self.pop_size):\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.population[i]\n        \n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(np.arange(self.pop_size), 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                v = x1 + self.f * (x2 - x3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = np.random.rand(self.dim) < self.cr\n                u = v * u + self.population[i] * (~u)\n                \n                # Evaluation\n                f = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.population[i] = u\n                    \n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n\n            # Adaptive Parameter Control (adaptation based on population diversity)\n            if np.std(self.fitness) > 1e-3:\n                self.f = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n                self.cr = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0)\n\n            if self.evals >= self.budget:\n                break\n                    \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDE scored 0.707 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["04e7bbcc-f9e3-4b75-bdaf-82c893b55f19"], "operator": null, "metadata": {"aucs": [0.35773684175424425, 0.6800596890299567, 0.6806387833856209, 0.8907072729216436, 0.7298054725319074, 0.8089389836118888, 0.6229114074264923, 0.6786559640379506, 0.7106644529668968, 0.6781433547035228, 0.8384043208447369, 0.9821784097201952, 0.7309950695602996, 0.7170267614598704, 0.9031449572588663, 0.7675407228771941, 0.6308963483751968, 0.8515988987089733, 0.3612313462238087, 0.5120706096316443]}}
{"id": "a24ca9fd-e3ca-44e7-9b63-20b9e2b314bb", "fitness": 0.6567757314223278, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with orthogonal crossover and scaling factor adaptation based on fitness improvement.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, f=0.5, cr=0.9, restart_trigger=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.f = f  # Mutation factor\n        self.cr = cr # Crossover rate\n        self.restart_trigger = restart_trigger # Percentage of budget before restart\n        self.archive = []\n        self.archive_fitness = []\n        self.sf_history = []  # History of successful scaling factors\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        \n        for i in range(self.pop_size):\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.population[i]\n        \n        while self.evals < self.budget:\n            \n            # Restart strategy\n            if self.evals > self.restart_trigger * self.budget:\n                if np.std(self.fitness) < 1e-6:  # Stagnation check\n                    self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                    self.fitness = np.array([func(x) for x in self.population])\n                    self.evals += self.pop_size\n                    for j in range(self.pop_size):\n                        if self.fitness[j] < self.f_opt:\n                            self.f_opt = self.fitness[j]\n                            self.x_opt = self.population[j]\n                    continue\n            \n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(np.arange(self.pop_size), 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                \n                # Add archive to selection pool with some probability\n                if len(self.archive) > 0 and np.random.rand() < 0.1:  # Adjust probability as needed\n                    idx_archive = np.random.randint(len(self.archive))\n                    x4 = self.archive[idx_archive]\n                    idxs = np.random.choice(np.arange(self.pop_size), 2, replace=False)\n                    x1, x2 = self.population[idxs]\n                    v = x1 + self.f * (x2 - x3)\n                    \n                else:\n                    v = x1 + self.f * (x2 - x3)\n\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Orthogonal Crossover\n                u = self.orthogonal_crossover(self.population[i], v)\n                \n                # Evaluation\n                f = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f < self.fitness[i]:\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i])\n                        self.archive_fitness.append(self.fitness[i])\n                    else:\n                        # Replace random element in archive\n                        idx_replace = np.random.randint(self.archive_size)\n                        self.archive[idx_replace] = self.population[i]\n                        self.archive_fitness[idx_replace] = self.fitness[i]\n                    \n                    # Adaptive Scaling Factor\n                    delta_f = abs(self.fitness[i] - f)\n                    if delta_f > 0.0:\n                        self.sf_history.append(delta_f)  # Store improvement\n                        if len(self.sf_history) > 50:  # Keep a rolling window of recent improvements\n                            self.sf_history.pop(0)\n                        \n                    self.fitness[i] = f\n                    self.population[i] = u\n                    \n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n\n                # Adaptive Parameter Control (improved adaptation)\n                if np.random.rand() < 0.1:  # Adaptation probability\n                    if len(self.sf_history) > 0:\n                        self.f = np.clip(np.random.normal(0.5, np.std(self.sf_history)), 0.1, 1.0)\n                    else:\n                         self.f = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0) # Fallback strategy\n                    self.cr = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0)\n\n                if self.evals >= self.budget:\n                    break\n                    \n        return self.f_opt, self.x_opt\n        \n    def orthogonal_crossover(self, x, v):\n        u = x.copy()\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.cr or j == j_rand:\n                u[j] = v[j]\n        return u", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDE scored 0.657 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["04e7bbcc-f9e3-4b75-bdaf-82c893b55f19"], "operator": null, "metadata": {"aucs": [0.3357727951389323, 0.6675511916388079, 0.6446951149751311, 0.8766325385989845, 0.657426337636901, 0.6738986804152542, 0.6101427608044082, 0.6043566456035887, 0.6655325362581677, 0.6204591097510954, 0.8375299098825563, 0.995669704297494, 0.6281784370514678, 0.6615741193100778, 0.7656170945294185, 0.6827495793561827, 0.5927185275909399, 0.7342348295081478, 0.357714437434837, 0.523060278664164]}}
{"id": "5ccb4803-b485-48f7-85ef-dc4ff615335d", "fitness": 0.6965096211066708, "name": "SelfAdaptiveDE_Neighborhood", "description": "Self-Adaptive Differential Evolution with Neighborhood Search that dynamically adjusts mutation and crossover rates based on individual success and incorporates a local search strategy around the best solution to refine convergence.", "code": "import numpy as np\n\nclass SelfAdaptiveDE_Neighborhood:\n    def __init__(self, budget=10000, dim=10, pop_size=50, f_init=0.5, cr_init=0.9, neighborhood_size=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.f = np.full(pop_size, f_init)  # Individual mutation factors\n        self.cr = np.full(pop_size, cr_init) # Individual crossover rates\n        self.neighborhood_size = neighborhood_size\n        self.success_f = np.zeros(pop_size)\n        self.success_cr = np.zeros(pop_size)\n        self.success_count = np.zeros(pop_size)\n        self.learning_rate = 0.1 # Adjusts the adaptation rate\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        \n        for i in range(self.pop_size):\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.population[i]\n        \n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(np.arange(self.pop_size), 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                v = x1 + self.f[i] * (x2 - x3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = np.random.rand(self.dim) < self.cr[i]\n                u = v * u + self.population[i] * (~u)\n                \n                # Evaluation\n                f = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f < self.fitness[i]:\n                    self.success_count[i] += 1\n                    self.success_f[i] = self.f[i]\n                    self.success_cr[i] = self.cr[i]\n                    \n                    self.fitness[i] = f\n                    self.population[i] = u\n                    \n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n                        \n                # Adaptation of F and CR\n                self.f[i] = self.f[i] * np.exp(self.learning_rate * (self.success_f[i] - self.f[i])) if self.success_count[i] > 0 else np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n                self.cr[i] = self.cr[i] * np.exp(self.learning_rate * (self.success_cr[i] - self.cr[i])) if self.success_count[i] > 0 else np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0)\n                self.f[i] = np.clip(self.f[i], 0.1, 1.0)\n                self.cr[i] = np.clip(self.cr[i], 0.1, 1.0)\n                \n                # Reset success counters periodically or when stagnation is detected\n                if self.evals % (self.budget // 10) == 0:\n                    self.success_f[:] = 0\n                    self.success_cr[:] = 0\n                    self.success_count[:] = 0\n\n                if self.evals >= self.budget:\n                    break\n            \n            # Neighborhood Search around best solution\n            if self.evals < self.budget and np.random.rand() < 0.1: # Probability of performing local search\n                x_neighbor = self.x_opt + np.random.uniform(-self.neighborhood_size, self.neighborhood_size, size=self.dim)\n                x_neighbor = np.clip(x_neighbor, func.bounds.lb, func.bounds.ub)\n                f_neighbor = func(x_neighbor)\n                self.evals += 1\n\n                if f_neighbor < self.f_opt:\n                    self.f_opt = f_neighbor\n                    self.x_opt = x_neighbor\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm SelfAdaptiveDE_Neighborhood scored 0.697 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["04e7bbcc-f9e3-4b75-bdaf-82c893b55f19"], "operator": null, "metadata": {"aucs": [0.3811185075743345, 0.6648618727843851, 0.6302735833744175, 0.8533208281009916, 0.7574553269405727, 0.7846197285338152, 0.6453860122038937, 0.6255673883528408, 0.7186009094243742, 0.6881729836771219, 0.846978676921468, 0.9973101927482586, 0.6548948118704112, 0.732811246953197, 0.9160326502461245, 0.7781174205355673, 0.6206355987333855, 0.822239645308772, 0.2876535230013392, 0.5241415148481466]}}
{"id": "a733ebb1-7e9b-4a2b-9fac-37d1e0f1a61b", "fitness": 0.631108576202777, "name": "AdaptiveDE", "description": "Self-adaptive Differential Evolution with improved parameter adaptation, archive management, and a Cauchy mutation operator for increased exploration.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, f_init=0.5, cr_init=0.9, restart_trigger=0.1, p_archive=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.f = np.full(pop_size, f_init)  # Mutation factor for each individual\n        self.cr = np.full(pop_size, cr_init) # Crossover rate for each individual\n        self.restart_trigger = restart_trigger # Percentage of budget before restart\n        self.archive = []\n        self.archive_fitness = []\n        self.p_archive = p_archive # Probability of using archive individual\n\n        self.sf_memory = [] # Memory of successful F values\n        self.scr_memory = [] # Memory of successful CR values\n        self.memory_size = 10\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        \n        for i in range(self.pop_size):\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.population[i]\n        \n        while self.evals < self.budget:\n            \n            # Restart strategy\n            if self.evals > self.restart_trigger * self.budget:\n                if np.std(self.fitness) < 1e-6:  # Stagnation check\n                    self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                    self.fitness = np.array([func(x) for x in self.population])\n                    self.f = np.full(self.pop_size, 0.5)\n                    self.cr = np.full(self.pop_size, 0.9)\n                    self.evals += self.pop_size\n                    for j in range(self.pop_size):\n                        if self.fitness[j] < self.f_opt:\n                            self.f_opt = self.fitness[j]\n                            self.x_opt = self.population[j]\n                    continue\n            \n            for i in range(self.pop_size):\n                # Mutation - Using Cauchy distribution\n                idxs = np.random.choice(np.arange(self.pop_size), 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                \n                if len(self.archive) > 0 and np.random.rand() < self.p_archive:\n                    idx_archive = np.random.randint(len(self.archive))\n                    x4 = self.archive[idx_archive]\n                    v = x1 + self.f[i] * (x2 - x3) # original\n                else:\n                    v = x1 + self.f[i] * (x2 - x3)\n\n                # Cauchy mutation\n                #v = x1 + np.random.standard_cauchy(size=self.dim) * self.f[i] * (x2 - x3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = np.random.rand(self.dim) < self.cr[i]\n                u = v * u + self.population[i] * (~u)\n                u = np.clip(u, func.bounds.lb, func.bounds.ub)\n                \n                # Evaluation\n                f = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f < self.fitness[i]:\n                    # Update archive - adaptive archive update\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                        self.archive_fitness.append(self.fitness[i])\n                    else:\n                        if self.fitness[i] > np.min(self.archive_fitness):\n                            idx_replace = np.argmin(self.archive_fitness)\n                            self.archive[idx_replace] = self.population[i].copy()\n                            self.archive_fitness[idx_replace] = self.fitness[i]\n                    \n                    # Save successful F and CR\n                    if len(self.sf_memory) < self.memory_size:\n                      self.sf_memory.append(self.f[i])\n                      self.scr_memory.append(self.cr[i])\n                    else:\n                      self.sf_memory = self.sf_memory[1:] + [self.f[i]]\n                      self.scr_memory = self.scr_memory[1:] + [self.cr[i]]\n\n                    self.fitness[i] = f\n                    self.population[i] = u\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n\n                # Adaptive Parameter Control (using memory)\n                if self.sf_memory:\n                  mf = np.mean(self.sf_memory)\n                  mcr = np.mean(self.scr_memory)\n\n                  self.f[i] = np.clip(np.random.normal(mf, 0.1), 0.1, 1.0)\n                  self.cr[i] = np.clip(np.random.normal(mcr, 0.1), 0.1, 1.0)\n                else:\n                  self.f[i] = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n                  self.cr[i] = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0)\n\n                if self.evals >= self.budget:\n                    break\n                    \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDE scored 0.631 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["04e7bbcc-f9e3-4b75-bdaf-82c893b55f19"], "operator": null, "metadata": {"aucs": [0.2736094128752379, 0.659387844269365, 0.415226545694525, 0.8637256370161015, 0.6594125994435343, 0.7005501334762168, 0.6215625256060371, 0.6445782538333868, 0.6382044768277938, 0.628919708523092, 0.8734584703996993, 0.9997577553087864, 0.3732087304042552, 0.6574398959242848, 0.7640595064196396, 0.6875183734557269, 0.6364257421997825, 0.7497738381117175, 0.25564270196042393, 0.5197093723059318]}}
{"id": "613a205f-605a-412f-bb76-563beaefed91", "fitness": 0.23820252389538651, "name": "AdaptiveNeighborhoodSearch", "description": "Neighborhood search with adaptive radius, dynamically adjusting based on success rate and function landscape curvature.", "code": "import numpy as np\n\nclass AdaptiveNeighborhoodSearch:\n    def __init__(self, budget=10000, dim=10, initial_radius=0.1, success_threshold=0.2, radius_decay=0.95, radius_increase=1.1, curvature_learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.radius = initial_radius\n        self.success_threshold = success_threshold\n        self.radius_decay = radius_decay\n        self.radius_increase = radius_increase\n        self.curvature_learning_rate = curvature_learning_rate\n        self.x_opt = None\n        self.f_opt = np.Inf\n        self.evals = 0\n        self.successes = 0\n        self.curvature = 0.0  # Estimate of local curvature\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n\n        self.x_opt = np.random.uniform(lb, ub, size=self.dim)\n        self.f_opt = func(self.x_opt)\n        self.evals += 1\n\n        while self.evals < self.budget:\n            # Sample a neighbor\n            x_new = self.x_opt + np.random.normal(0, self.radius, size=self.dim)\n\n            # Clip to bounds\n            x_new = np.clip(x_new, lb, ub)\n\n            f_new = func(x_new)\n            self.evals += 1\n\n            if f_new < self.f_opt:\n                # Update optimal solution\n                self.x_opt = x_new\n                self.f_opt = f_new\n                self.successes += 1\n\n                # Adaptive radius increase based on curvature\n                self.radius *= self.radius_increase\n                self.radius = min(self.radius, 5.0)\n            else:\n                # Adaptive radius decrease\n                self.radius *= self.radius_decay\n\n                #Curvature update\n                df = f_new - self.f_opt\n                self.curvature = (1 - self.curvature_learning_rate) * self.curvature + self.curvature_learning_rate * df / (self.radius**2 + 1e-8)\n                \n            self.radius = max(self.radius, 1e-6)  # Ensure radius doesn't vanish\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveNeighborhoodSearch scored 0.238 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["15ffe61e-b8b5-425d-a73c-1372bbda485c"], "operator": null, "metadata": {"aucs": [0.1105578284994303, 0.08987716887808384, 0.18703465050243084, 0.1298612403597844, 0.06780009148348287, 0.09454301397707021, 0.2291521627100429, 0.39281967378003524, 0.13038524896298676, 0.09023510585227634, 0.2131412145422118, 0.11947458858792726, 0.2313625297168268, 0.21413722132610014, 0.7618599961572002, 0.255442016517877, 0.2027825427915162, 0.9817891428238222, 0.10360655127363061, 0.1581884891649945]}}
{"id": "fe8cb003-fa7b-4206-8745-e992563617dc", "fitness": 0.5507667181106174, "name": "AdaptiveDE", "description": "Enhanced Adaptive Differential Evolution with a distance-based mutation strategy, self-adaptive parameters, and a better archive update method for maintaining diversity and accelerating convergence.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, f_init=0.5, cr_init=0.9, restart_trigger=0.1, adapt_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.f = f_init  # Mutation factor\n        self.cr = cr_init # Crossover rate\n        self.restart_trigger = restart_trigger # Percentage of budget before restart\n        self.archive = []\n        self.archive_fitness = []\n        self.adapt_prob = adapt_prob # Probability of adapting F and CR\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_fitness_history = []\n\n        for i in range(self.pop_size):\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.population[i]\n        self.best_fitness_history.append(self.f_opt)\n        \n        while self.evals < self.budget:\n            \n            # Restart strategy\n            if self.evals > self.restart_trigger * self.budget:\n                if np.std(self.fitness) < 1e-6:  # Stagnation check\n                    self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                    self.fitness = np.array([func(x) for x in self.population])\n                    self.evals += self.pop_size\n                    for j in range(self.pop_size):\n                        if self.fitness[j] < self.f_opt:\n                            self.f_opt = self.fitness[j]\n                            self.x_opt = self.population[j]\n                    self.best_fitness_history.append(self.f_opt)\n                    continue\n            \n            for i in range(self.pop_size):\n                # Mutation using distance information\n                idxs = np.random.choice(np.arange(self.pop_size), 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n\n                # Calculate distance to the best solution\n                distance = np.linalg.norm(self.population[i] - self.x_opt)\n                \n                # Adjust mutation factor based on distance\n                f_adaptive = self.f * (1 + 0.1 * distance)  # Increase F when far from best\n\n                # Add archive to selection pool with some probability\n                if len(self.archive) > 0 and np.random.rand() < 0.1:\n                    idx_archive = np.random.randint(len(self.archive))\n                    x4 = self.archive[idx_archive]\n                    idxs = np.random.choice(np.arange(self.pop_size), 2, replace=False)\n                    x1, x2 = self.population[idxs]\n                    v = x1 + f_adaptive * (x2 - x3)\n                else:\n                     v = x1 + f_adaptive * (x2 - x3)\n                    \n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = np.random.rand(self.dim) < self.cr\n                u = v * u + self.population[i] * (~u)\n                \n                # Evaluation\n                f = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f < self.fitness[i]:\n                    # Update archive using improved replacement strategy\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i])\n                        self.archive_fitness.append(self.fitness[i])\n                    else:\n                        # Replace the worst element in the archive\n                        idx_replace = np.argmax(self.archive_fitness)\n                        self.archive[idx_replace] = self.population[i]\n                        self.archive_fitness[idx_replace] = self.fitness[i]\n                    \n                    self.fitness[i] = f\n                    self.population[i] = u\n                    \n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n                        \n                # Adaptive Parameter Control (more robust adaptation)\n                if np.random.rand() < self.adapt_prob:\n                    self.f = np.clip(np.random.normal(self.f, 0.1), 0.1, 1.0)\n                    self.cr = np.clip(np.random.normal(self.cr, 0.1), 0.1, 1.0)\n\n                if self.evals >= self.budget:\n                    break\n            self.best_fitness_history.append(self.f_opt)\n                    \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDE scored 0.551 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["04e7bbcc-f9e3-4b75-bdaf-82c893b55f19"], "operator": null, "metadata": {"aucs": [0.2686116015095351, 0.34704040863839325, 0.5221440886449296, 0.6062866610985999, 0.6794133693735767, 0.5829587216963816, 0.5173113418598004, 0.4990023670663374, 0.4594953763825167, 0.5050812109563975, 0.8333479741724216, 0.9981269801291175, 0.4210572120220196, 0.6041894442821594, 0.7698661970007371, 0.5984216861314764, 0.4158824884054737, 0.6563320153498615, 0.21611878054117273, 0.5146464369514412]}}
{"id": "a276bc8a-9c89-4078-8d9a-cab59d8f4ade", "fitness": 0.0, "name": "AdaptiveDE_DynamicPop", "description": "Differential Evolution with a dynamically adjusted population size and a restart mechanism triggered by stagnation detection.", "code": "import numpy as np\n\nclass AdaptiveDE_DynamicPop:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, f=0.5, cr=0.9, stagnation_threshold=500):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.initial_pop_size = initial_pop_size\n        self.f = f  # Mutation factor\n        self.cr = cr # Crossover rate\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        \n        for i in range(self.pop_size):\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.population[i]\n        \n        self.best_fitness_history.append(self.f_opt)\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(np.arange(self.pop_size), 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                v = x1 + self.f * (x2 - x3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = np.random.rand(self.dim) < self.cr\n                u = v * u + self.population[i] * (~u)\n                \n                # Evaluation\n                f = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.population[i] = u\n                    \n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n                        self.stagnation_counter = 0 # Reset stagnation counter when finding better solution\n                else:\n                    self.stagnation_counter += 1\n\n            self.best_fitness_history.append(self.f_opt)\n\n            # Dynamic Population Size Adjustment\n            if self.stagnation_counter > self.stagnation_threshold:\n                # Restart with a larger population if stagnating\n                self.pop_size = int(self.pop_size * 1.2)  # Increase population size by 20%\n                self.pop_size = min(self.pop_size, 200)  # Cap population size\n                \n                # Repopulate with new random individuals\n                new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size - len(self.population), self.dim))\n                self.population = np.vstack((self.population, new_individuals))\n                new_fitness = np.array([func(x) for x in new_individuals])\n                self.fitness = np.concatenate((self.fitness, new_fitness))\n                self.evals += len(new_individuals)\n                \n                # Resetting stagnation counter\n                self.stagnation_counter = 0\n                \n                # Update best fitness and solution after restart\n                for i in range(self.pop_size):\n                    if self.fitness[i] < self.f_opt:\n                        self.f_opt = self.fitness[i]\n                        self.x_opt = self.population[i]\n            # Adaptive Parameter Control (adaptation based on population diversity)\n            if np.std(self.fitness) > 1e-3:\n                self.f = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n                self.cr = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0)\n            \n            if self.evals >= self.budget:\n                break\n                    \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDE_DynamicPop scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2771f839-c30b-4b4e-ae0e-a440daeab0df"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "e74587d2-2504-45d4-a7c2-dcf9d72f344b", "fitness": 0.0, "name": "DynamicAdaptiveDE", "description": "Differential Evolution with a dynamically adjusted population size and a local search component triggered based on stagnation detection.", "code": "import numpy as np\n\nclass DynamicAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, f=0.5, cr=0.9, local_search_iterations=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.initial_pop_size = initial_pop_size\n        self.f = f  # Mutation factor\n        self.cr = cr # Crossover rate\n        self.local_search_iterations = local_search_iterations\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 20\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        \n        for i in range(self.pop_size):\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.population[i]\n        \n        best_fitness_history = []\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(np.arange(self.pop_size), 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                v = x1 + self.f * (x2 - x3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = np.random.rand(self.dim) < self.cr\n                u = v * u + self.population[i] * (~u)\n                \n                # Evaluation\n                f = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.population[i] = u\n                    \n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n            \n            # Stagnation Detection\n            best_fitness_history.append(self.f_opt)\n            if len(best_fitness_history) > self.stagnation_threshold:\n                best_fitness_history.pop(0)\n                if np.std(best_fitness_history) < 1e-5:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0\n\n            # Local Search Trigger\n            if self.stagnation_counter >= 3:\n                self.local_search(func)\n                self.stagnation_counter = 0\n                \n            # Dynamic Population Size Adjustment\n            if self.evals < self.budget // 2 and self.pop_size < 2 * self.initial_pop_size:\n                self.pop_size = min(2 * self.initial_pop_size, self.budget - self.evals)\n                new_population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size - len(self.population), self.dim))\n                self.population = np.vstack((self.population, new_population))\n                new_fitness = np.array([func(x) for x in new_population])\n                self.fitness = np.concatenate((self.fitness, new_fitness))\n                self.evals += len(new_fitness)\n                for i in range(len(self.fitness)):\n                    if self.fitness[i] < self.f_opt:\n                        self.f_opt = self.fitness[i]\n                        self.x_opt = self.population[i]\n\n            # Adaptive Parameter Control (adaptation based on population diversity)\n            if np.std(self.fitness) > 1e-3:\n                self.f = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n                self.cr = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0)\n                \n            if self.evals >= self.budget:\n                break\n                    \n        return self.f_opt, self.x_opt\n    \n    def local_search(self, func):\n        # Perform local search around the best solution\n        for _ in range(self.local_search_iterations):\n            perturbation = np.random.normal(0, 0.05, size=self.dim)  # Small perturbation\n            x_new = self.x_opt + perturbation\n            x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n            f_new = func(x_new)\n            self.evals += 1\n\n            if f_new < self.f_opt:\n                self.f_opt = f_new\n                self.x_opt = x_new\n                \n            if self.evals >= self.budget:\n                break", "configspace": "", "generation": 2, "feedback": "The algorithm DynamicAdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2771f839-c30b-4b4e-ae0e-a440daeab0df"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "65c7636f-9a53-454c-a6c2-624e7495fddc", "fitness": 0.7081473118309991, "name": "AdaptiveDE", "description": "Simplified Adaptive Differential Evolution with reduced parameter adaptation frequency and simplified mutation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, f=0.5, cr=0.9, adapt_freq=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.f = f\n        self.cr = cr\n        self.adapt_freq = adapt_freq\n        self.best_fitness_history = []\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.population[best_idx]\n        self.best_fitness_history.append(self.f_opt)\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(np.arange(self.pop_size), 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                v = x1 + self.f * (x2 - x3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = np.random.rand(self.dim) < self.cr\n                u = v * u + self.population[i] * (~u)\n                \n                # Evaluation\n                f = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.population[i] = u\n                    \n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n                        self.best_fitness_history.append(self.f_opt)\n\n            # Adaptive Parameter Control (less frequent adaptation)\n            if self.evals % self.adapt_freq == 0:\n                if np.std(self.fitness) > 1e-3:\n                    self.f = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n                    self.cr = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0)\n\n            if self.evals >= self.budget:\n                break\n                    \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDE scored 0.708 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2771f839-c30b-4b4e-ae0e-a440daeab0df"], "operator": null, "metadata": {"aucs": [0.3042285895075576, 0.6636738816390028, 0.6726576100269301, 0.8605493827996392, 0.7435074111876678, 0.8112181860442367, 0.6776865929225804, 0.6571183939686569, 0.7275896179390904, 0.6827438426449967, 0.8624133856980576, 0.9911540112014754, 0.7175092278332729, 0.7347036723797886, 0.8751828295783722, 0.7923265941039558, 0.6002986728967248, 0.85102756310127, 0.4065054763678503, 0.5308512947788538]}}
{"id": "169278f3-7c23-4ffc-9b45-20c73278580c", "fitness": 0.5717437105298729, "name": "AdaptiveDE", "description": "Simplified Adaptive Differential Evolution with dynamic parameter adjustment based on success history and a reduced archive strategy for enhanced exploration-exploitation balance.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=5, f=0.5, cr=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.f = f\n        self.cr = cr\n        self.archive = []\n        self.sf_history = []\n        self.cr_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.population[best_idx]\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(np.arange(self.pop_size), 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                \n                # Add archive to selection pool\n                if len(self.archive) > 0 and np.random.rand() < 0.1:\n                    idx_archive = np.random.randint(len(self.archive))\n                    v = x1 + self.f * (self.archive[idx_archive] - x2)\n                else:\n                    v = x1 + self.f * (x2 - x3)\n                \n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = self.crossover(self.population[i], v)\n                \n                # Evaluation\n                f = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f < self.fitness[i]:\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        idx_replace = np.random.randint(self.archive_size)\n                        self.archive[idx_replace] = self.population[i].copy()\n                    \n                    # Adaptive Parameter Control (success history)\n                    delta_f = self.fitness[i] - f\n                    if delta_f > 0:\n                        self.sf_history.append(self.f)\n                        self.cr_history.append(self.cr)\n                        if len(self.sf_history) > 20:\n                            self.sf_history.pop(0)\n                            self.cr_history.pop(0)\n\n                    self.fitness[i] = f\n                    self.population[i] = u\n                    \n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u.copy()\n            \n            # Global Parameter Adaptation\n            if len(self.sf_history) > 0:\n                self.f = np.clip(np.random.normal(np.mean(self.sf_history), 0.1), 0.1, 1.0)\n                self.cr = np.clip(np.random.normal(np.mean(self.cr_history), 0.1), 0.1, 1.0)\n\n            if self.evals >= self.budget:\n                break\n                    \n        return self.f_opt, self.x_opt\n        \n    def crossover(self, x, v):\n        u = x.copy()\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.cr or j == j_rand:\n                u[j] = v[j]\n        return u", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDE scored 0.572 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a24ca9fd-e3ca-44e7-9b63-20b9e2b314bb"], "operator": null, "metadata": {"aucs": [0.2553199765977886, 0.617895196709606, 0.5324915512562747, 0.6945340939321739, 0.7203283451924463, 0.6814762318920422, 0.5286339683419077, 0.4555188716844343, 0.6378716877654573, 0.20778854409704806, 0.7701264516048991, 0.9983418388066095, 0.3386647803020141, 0.6084820096494795, 0.8967787156464498, 0.4370183910758595, 0.5767439654139981, 0.7321119889396315, 0.24784780758851543, 0.4968997941008212]}}
{"id": "a2b576fa-126e-4997-80e9-b3cd2657356d", "fitness": 0.659954372370215, "name": "DynamicDE", "description": "Differential Evolution with a dynamically adjusted population size based on optimization progress and a modified mutation strategy incorporating Levy flights for enhanced exploration.", "code": "import numpy as np\n\nclass DynamicDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, f=0.5, cr=0.9, levy_exponent=1.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.initial_pop_size = initial_pop_size\n        self.f = f  # Mutation factor\n        self.cr = cr # Crossover rate\n        self.levy_exponent = levy_exponent # Levy flight exponent\n        self.min_pop_size = 10 # Minimal population size\n\n    def levy_flight(self, size):\n        \"\"\"\n        Generate Levy distribution samples.\n        \"\"\"\n        num = np.random.normal(0, 1, size)\n        den = np.power(np.abs(np.random.normal(0, 1, size)), (1/self.levy_exponent))\n        sigma = (np.math.gamma(1 + self.levy_exponent) * np.sin(np.pi * self.levy_exponent / 2) / (np.math.gamma((1 + self.levy_exponent) / 2) * self.levy_exponent * np.power(2, (self.levy_exponent - 1) / 2)))**(1/self.levy_exponent)\n        return sigma * (num / den)\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        \n        for i in range(self.pop_size):\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.population[i]\n        \n        generation = 0\n        while self.evals < self.budget:\n            generation += 1\n            for i in range(self.pop_size):\n                # Mutation with Levy Flight\n                idxs = np.random.choice(np.arange(self.pop_size), 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                levy_step = self.levy_flight(self.dim)\n                v = x1 + self.f * (x2 - x3) + 0.01 * levy_step * (self.x_opt - self.population[i]) # Adding levy flight component\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = np.random.rand(self.dim) < self.cr\n                u = v * u + self.population[i] * (~u)\n                \n                # Evaluation\n                f = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.population[i] = u\n                    \n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n\n            # Dynamic Population Size Adjustment (reduce if stagnating, increase if promising)\n            if generation % 10 == 0:\n                fitness_std = np.std(self.fitness)\n                if fitness_std < 1e-4 and self.pop_size > self.min_pop_size:\n                    self.pop_size = max(self.min_pop_size, int(self.pop_size * 0.9))  # Reduce population\n                    self.population = self.population[:self.pop_size]\n                    self.fitness = self.fitness[:self.pop_size]\n                elif fitness_std > 1e-2 and self.pop_size < self.initial_pop_size * 2:\n                    new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.initial_pop_size - self.pop_size, self.dim))\n                    new_fitness = np.array([func(x) for x in new_individuals])\n                    self.evals += len(new_individuals)\n                    self.population = np.vstack((self.population, new_individuals))\n                    self.fitness = np.concatenate((self.fitness, new_fitness))\n                    self.pop_size = self.initial_pop_size\n                    for i in range(self.pop_size, len(self.population)):\n                        if self.fitness[i] < self.f_opt:\n                            self.f_opt = self.fitness[i]\n                            self.x_opt = self.population[i]\n\n            if self.evals >= self.budget:\n                break\n                    \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm DynamicDE scored 0.660 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2771f839-c30b-4b4e-ae0e-a440daeab0df"], "operator": null, "metadata": {"aucs": [0.28741933029841493, 0.674199885046271, 0.6860930625749293, 0.8855794181025194, 0.7378072309122484, 0.8013287605484474, 0.6236049865245146, 0.6656808738977985, 0.7450189551734956, 0.6681613558796402, 0.8747975796480562, 1.0, 0.35960263223170563, 0.6956875030393506, 0.9228439353220663, 0.8093859564284464, 0.6265212535067362, 0.8332113210498738, 0.3021434072197877, 0]}}
{"id": "b75a0ce2-7ce8-4a15-b9dd-abc2011f696f", "fitness": 0.6695883616035524, "name": "AdaptiveDE", "description": "Simplified Adaptive Differential Evolution with self-adjusting parameters based on success history and reduced complexity by removing orthogonal crossover.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, f=0.5, cr=0.9, restart_trigger=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.f = f  # Mutation factor\n        self.cr = cr # Crossover rate\n        self.restart_trigger = restart_trigger # Percentage of budget before restart\n        self.archive = []\n        self.archive_fitness = []\n        self.sf_history = []  # History of successful scaling factors\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        \n        for i in range(self.pop_size):\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.population[i]\n        \n        while self.evals < self.budget:\n            \n            # Restart strategy\n            if self.evals > self.restart_trigger * self.budget:\n                if np.std(self.fitness) < 1e-6:  # Stagnation check\n                    self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                    self.fitness = np.array([func(x) for x in self.population])\n                    self.evals += self.pop_size\n                    for j in range(self.pop_size):\n                        if self.fitness[j] < self.f_opt:\n                            self.f_opt = self.fitness[j]\n                            self.x_opt = self.population[j]\n                    continue\n            \n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(np.arange(self.pop_size), 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                \n                # Add archive to selection pool with some probability\n                if len(self.archive) > 0 and np.random.rand() < 0.1:  # Adjust probability as needed\n                    idx_archive = np.random.randint(len(self.archive))\n                    x4 = self.archive[idx_archive]\n                    idxs = np.random.choice(np.arange(self.pop_size), 2, replace=False)\n                    x1, x2 = self.population[idxs]\n                    v = x1 + self.f * (x2 - x3)\n                    \n                else:\n                    v = x1 + self.f * (x2 - x3)\n\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = self.crossover(self.population[i], v)\n                \n                # Evaluation\n                f = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f < self.fitness[i]:\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i])\n                        self.archive_fitness.append(self.fitness[i])\n                    else:\n                        # Replace random element in archive\n                        idx_replace = np.random.randint(self.archive_size)\n                        self.archive[idx_replace] = self.population[i]\n                        self.archive_fitness[idx_replace] = self.fitness[i]\n                    \n                    # Adaptive Scaling Factor\n                    delta_f = abs(self.fitness[i] - f)\n                    if delta_f > 0.0:\n                        self.sf_history.append(delta_f)  # Store improvement\n                        if len(self.sf_history) > 50:  # Keep a rolling window of recent improvements\n                            self.sf_history.pop(0)\n                        \n                    self.fitness[i] = f\n                    self.population[i] = u\n                    \n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n\n                # Adaptive Parameter Control (improved adaptation)\n                if np.random.rand() < 0.1:  # Adaptation probability\n                    if len(self.sf_history) > 0:\n                        self.f = np.clip(np.random.normal(0.5, np.std(self.sf_history)), 0.1, 1.0)\n                    else:\n                         self.f = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0) # Fallback strategy\n                    self.cr = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0)\n\n                if self.evals >= self.budget:\n                    break\n                    \n        return self.f_opt, self.x_opt\n        \n    def crossover(self, x, v):\n        u = x.copy()\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.cr or j == j_rand:\n                u[j] = v[j]\n        return u", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDE scored 0.670 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a24ca9fd-e3ca-44e7-9b63-20b9e2b314bb"], "operator": null, "metadata": {"aucs": [0.44574458240935966, 0.54314060078206, 0.6009724380753743, 0.8647154384702441, 0.6866217345023504, 0.6943390775593988, 0.5747475878876632, 0.5790369778186948, 0.6539712996994667, 0.6381229189554102, 0.8731826260760661, 0.9976727833140387, 0.6432917243305032, 0.656850149222502, 0.7618557242726901, 0.6862644719656203, 0.6030653018315486, 0.7501302424409135, 0.6328655007460389, 0.5051760517111057]}}
{"id": "a917b064-0553-4347-99c5-48f8eb0b49d8", "fitness": 0.3885102629472287, "name": "AdaptiveDETwinPopulation", "description": "Adaptive Differential Evolution with a twin population, momentum-based mutation, and an aging mechanism to diversify the search.", "code": "import numpy as np\n\nclass AdaptiveDETwinPopulation:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, f=0.5, cr=0.9, restart_trigger=0.1, aging_rate=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.f = f  # Mutation factor\n        self.cr = cr # Crossover rate\n        self.restart_trigger = restart_trigger # Percentage of budget before restart\n        self.aging_rate = aging_rate # Rate at which fitness degrades for aging individuals\n        self.archive = []\n        self.archive_fitness = []\n        self.sf_history = []  # History of successful scaling factors\n        self.momentum = np.zeros((pop_size, dim))  # Momentum for mutation\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population1 = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.population2 = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim)) # Twin population\n        self.fitness1 = np.array([func(x) for x in self.population1])\n        self.fitness2 = np.array([func(x) for x in self.population2])\n        self.evals = 2 * self.pop_size\n        self.age = np.zeros(self.pop_size) # Individual age\n\n        for i in range(self.pop_size):\n            if self.fitness1[i] < self.f_opt:\n                self.f_opt = self.fitness1[i]\n                self.x_opt = self.population1[i]\n            if self.fitness2[i] < self.f_opt:\n                self.f_opt = self.fitness2[i]\n                self.x_opt = self.population2[i]\n        \n        while self.evals < self.budget:\n            # Restart strategy\n            if self.evals > self.restart_trigger * self.budget:\n                if np.std(np.concatenate([self.fitness1, self.fitness2])) < 1e-6:  # Stagnation check\n                    self.population1 = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                    self.population2 = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                    self.fitness1 = np.array([func(x) for x in self.population1])\n                    self.fitness2 = np.array([func(x) for x in self.population2])\n                    self.evals += 2*self.pop_size\n                    self.age = np.zeros(self.pop_size)\n\n                    for j in range(self.pop_size):\n                        if self.fitness1[j] < self.f_opt:\n                            self.f_opt = self.fitness1[j]\n                            self.x_opt = self.population1[j]\n                        if self.fitness2[j] < self.f_opt:\n                            self.f_opt = self.fitness2[j]\n                            self.x_opt = self.population2[j]\n                    continue\n\n            # Iterate through both populations\n            for pop_idx, (population, fitness) in enumerate([(self.population1, self.fitness1), (self.population2, self.fitness2)]):\n                for i in range(self.pop_size):\n                    # Mutation with momentum\n                    idxs = np.random.choice(np.arange(self.pop_size), 3, replace=False)\n                    x1, x2, x3 = population[idxs]\n\n                    # Momentum update\n                    self.momentum[i] = 0.9 * self.momentum[i] + 0.1 * (x1 - population[i])\n\n                    v = x1 + self.f * (x2 - x3) + self.momentum[i] # Apply momentum\n                    v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                    \n                    # Orthogonal Crossover\n                    u = self.orthogonal_crossover(population[i], v)\n                    \n                    # Evaluation\n                    f = func(u)\n                    self.evals += 1\n                    \n                    # Selection\n                    if f < fitness[i]:\n                        # Update archive (simplified)\n                        if len(self.archive) < self.archive_size:\n                            self.archive.append(population[i])\n                            self.archive_fitness.append(fitness[i])\n                        else:\n                            idx_replace = np.random.randint(self.archive_size)\n                            self.archive[idx_replace] = population[i]\n                            self.archive_fitness[idx_replace] = fitness[i]\n                            \n                        # Adaptive Scaling Factor (simplified)\n                        delta_f = abs(fitness[i] - f)\n                        if delta_f > 0.0:\n                            self.sf_history.append(delta_f)\n                            if len(self.sf_history) > 50:\n                                self.sf_history.pop(0)\n                        \n                        fitness[i] = f\n                        population[i] = u\n                        self.age[i] = 0  # Reset age upon improvement\n                        \n                        if f < self.f_opt:\n                            self.f_opt = f\n                            self.x_opt = u\n                    else:\n                        self.age[i] += 1 # Increase age\n\n                    # Aging mechanism: degrade fitness based on age\n                    fitness[i] += self.aging_rate * self.age[i]\n\n                    # Adaptive Parameter Control (simplified)\n                    if np.random.rand() < 0.1:\n                        if len(self.sf_history) > 0:\n                            self.f = np.clip(np.random.normal(0.5, np.std(self.sf_history)), 0.1, 1.0)\n                        else:\n                            self.f = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n                        self.cr = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0)\n\n                    if self.evals >= self.budget:\n                        break\n                if self.evals >= self.budget:\n                    break\n                    \n        return self.f_opt, self.x_opt\n        \n    def orthogonal_crossover(self, x, v):\n        u = x.copy()\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.cr or j == j_rand:\n                u[j] = v[j]\n        return u", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDETwinPopulation scored 0.389 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a24ca9fd-e3ca-44e7-9b63-20b9e2b314bb"], "operator": null, "metadata": {"aucs": [0.16660326266318093, 0.2564079177623999, 0.34976979923234663, 0.4620813655765712, 0.350068967371744, 0.4008946978777457, 0.30439906401068284, 0.31604735492540836, 0.3242230593652211, 0.2654929163607622, 0.4225578067071194, 0.9995729143598151, 0.3390168816864285, 0.34368217560760284, 0.6556075582900995, 0.3719685964035704, 0.31922617281590937, 0.45422302198472386, 0.2023380690753488, 0.4660236568678914]}}
{"id": "ad497881-ce68-49d6-9a2d-4724f27e8390", "fitness": 0.6490311384233789, "name": "AdaptiveDE", "description": "Enhanced Adaptive Differential Evolution with a modified mutation strategy using the best solution and a learning mechanism for parameter adaptation based on success rate.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, f=0.5, cr=0.9, restart_trigger=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.f = f  # Mutation factor\n        self.cr = cr # Crossover rate\n        self.restart_trigger = restart_trigger # Percentage of budget before restart\n        self.archive = []\n        self.archive_fitness = []\n        self.sf_history = []  # History of successful scaling factors\n        self.cr_history = []  # History of successful crossover rates\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.best_idx = np.argmin(self.fitness)\n        self.x_opt = self.population[self.best_idx]\n        self.f_opt = self.fitness[self.best_idx]\n        \n        \n        while self.evals < self.budget:\n            \n            # Restart strategy\n            if self.evals > self.restart_trigger * self.budget:\n                if np.std(self.fitness) < 1e-6:  # Stagnation check\n                    self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                    self.fitness = np.array([func(x) for x in self.population])\n                    self.evals += self.pop_size\n                    self.best_idx = np.argmin(self.fitness)\n                    self.x_opt = self.population[self.best_idx]\n                    self.f_opt = self.fitness[self.best_idx]\n                    continue\n            \n            for i in range(self.pop_size):\n                # Mutation using best solution\n                idxs = np.random.choice(np.arange(self.pop_size), 2, replace=False)\n                x1, x2 = self.population[idxs]\n                \n                # Mutate based on current best\n                v = self.population[i] + self.f * (self.x_opt - self.population[i]) + self.f * (x1 - x2)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Orthogonal Crossover\n                u = self.orthogonal_crossover(self.population[i], v)\n                \n                # Evaluation\n                f = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f < self.fitness[i]:\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i])\n                        self.archive_fitness.append(self.fitness[i])\n                    else:\n                        # Replace random element in archive\n                        idx_replace = np.random.randint(self.archive_size)\n                        self.archive[idx_replace] = self.population[i]\n                        self.archive_fitness[idx_replace] = self.fitness[i]\n                    \n                    # Adaptive Scaling Factor and Crossover Rate\n                    delta_f = abs(self.fitness[i] - f)\n                    if delta_f > 0.0:\n                        self.sf_history.append(self.f)\n                        self.cr_history.append(self.cr)  # Store successful CR\n\n                    self.fitness[i] = f\n                    self.population[i] = u\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n                        self.best_idx = i\n                        \n                # Adaptive Parameter Control (success-based adaptation)\n                if len(self.sf_history) > 10:\n                        self.f = np.clip(np.mean(self.sf_history[-10:]), 0.1, 1.0)\n                else:\n                    self.f = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n                if len(self.cr_history) > 10:\n                    self.cr = np.clip(np.mean(self.cr_history[-10:]), 0.1, 1.0)\n                else:\n                    self.cr = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0)\n\n                if self.evals >= self.budget:\n                    break\n                    \n        return self.f_opt, self.x_opt\n        \n    def orthogonal_crossover(self, x, v):\n        u = x.copy()\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.cr or j == j_rand:\n                u[j] = v[j]\n        return u", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDE scored 0.649 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a24ca9fd-e3ca-44e7-9b63-20b9e2b314bb"], "operator": null, "metadata": {"aucs": [0.23765478243662996, 0.7463667198546078, 0.7404847184204226, 0.9226169374173675, 0.8784221641107786, 0.7640177399394295, 0.4588535512715698, 0.7281594478599158, 0.7944556828355708, 0.46772897882265063, 0.9440917794206202, 0.995082990192137, 0.33398438004867625, 0.3913570675250808, 0.9492829507098786, 0.7514247234298852, 0.32464910439622563, 0.8538977922543327, 0.22178937350445194, 0.47630188401734597]}}
{"id": "e2d5689c-6ea5-4289-a346-818243966656", "fitness": 0.6958947705019223, "name": "HybridDECMASobol", "description": "Hybrid optimization using self-adaptive differential evolution with a Sobol sequence initialization, covariance matrix adaptation evolution strategy (CMA-ES) component, and a local search around the best solution, dynamically balancing exploration and exploitation.", "code": "import numpy as np\nfrom scipy.linalg import sqrtm\n\nclass HybridDECMASobol:\n    def __init__(self, budget=10000, dim=10, pop_size=50, f_init=0.5, cr_init=0.9, neighborhood_size=0.1, cma_generations=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.f = np.full(pop_size, f_init)  # Individual mutation factors\n        self.cr = np.full(pop_size, cr_init) # Individual crossover rates\n        self.neighborhood_size = neighborhood_size\n        self.cma_generations = cma_generations\n        self.learning_rate = 0.1 # Adjusts the adaptation rate\n        self.cma_evals = 0\n\n    def sobol_sequence(self, n_points, dim):\n        \"\"\"Generate n_points of Sobol sequence in dim dimensions.\"\"\"\n        try:\n            from sobol_seq import i4_sobol_generate\n            points = i4_sobol_generate(dim, n_points)\n            return points\n        except ImportError:\n            print(\"Sobol sequence library not found.  Falling back to uniform random numbers\")\n            return np.random.rand(n_points, dim)\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        # Sobol sequence initialization\n        sobol_points = self.sobol_sequence(self.pop_size, self.dim)\n        self.population = func.bounds.lb + sobol_points * (func.bounds.ub - func.bounds.lb)\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        \n        for i in range(self.pop_size):\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.population[i]\n        \n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(np.arange(self.pop_size), 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                v = x1 + self.f[i] * (x2 - x3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = np.random.rand(self.dim) < self.cr[i]\n                u = v * u + self.population[i] * (~u)\n                \n                # Evaluation\n                f = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.population[i] = u\n                    \n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n                        \n                # Adaptation of F and CR\n                success = f < self.fitness[i]\n                if success:\n                    self.f[i] = 0.9 * self.f[i] + 0.1 * np.random.uniform(0.1, 0.9)\n                    self.cr[i] = 0.9 * self.cr[i] + 0.1 * np.random.uniform(0.1, 0.9)\n                else:\n                    self.f[i] = np.clip(np.random.normal(0.5, 0.3), 0.1, 1.0)\n                    self.cr[i] = np.clip(np.random.normal(0.9, 0.2), 0.1, 1.0)\n                \n                self.f[i] = np.clip(self.f[i], 0.1, 1.0)\n                self.cr[i] = np.clip(self.cr[i], 0.1, 1.0)\n\n                if self.evals >= self.budget:\n                    break\n            \n            # CMA-ES component, triggered periodically\n            if self.evals < self.budget and self.evals % (self.budget // 5) == 0:\n                self.cma_es(func)\n\n            # Neighborhood Search around best solution, triggered periodically\n            if self.evals < self.budget and np.random.rand() < 0.1: # Probability of performing local search\n                x_neighbor = self.x_opt + np.random.uniform(-self.neighborhood_size, self.neighborhood_size, size=self.dim)\n                x_neighbor = np.clip(x_neighbor, func.bounds.lb, func.bounds.ub)\n                f_neighbor = func(x_neighbor)\n                self.evals += 1\n\n                if f_neighbor < self.f_opt:\n                    self.f_opt = f_neighbor\n                    self.x_opt = x_neighbor\n        \n        return self.f_opt, self.x_opt\n\n    def cma_es(self, func):\n        \"\"\"Covariance Matrix Adaptation Evolution Strategy.\"\"\"\n        mean = self.x_opt.copy()\n        sigma = 0.1 * (func.bounds.ub - func.bounds.lb)  # Initial step size\n        C = np.eye(self.dim)  # Initial covariance matrix\n        \n        for _ in range(min(self.cma_generations, (self.budget - self.evals) // self.pop_size)):\n            if self.evals >= self.budget:\n                break\n                \n            # Generate samples from multivariate normal distribution\n            z = np.random.randn(self.pop_size, self.dim)\n            samples = mean + sigma * np.dot(z, sqrtm(C).T)\n            samples = np.clip(samples, func.bounds.lb, func.bounds.ub)  # Clip to bounds\n            \n            # Evaluate samples\n            fitness_values = np.array([func(x) for x in samples])\n            self.evals += self.pop_size\n            \n            # Sort by fitness\n            idx = np.argsort(fitness_values)\n            best_samples = samples[idx]\n            best_fitness = fitness_values[idx]\n            \n            # Update best solution\n            if best_fitness[0] < self.f_opt:\n                self.f_opt = best_fitness[0]\n                self.x_opt = best_samples[0]\n            \n            # Update mean\n            mean = np.mean(best_samples[:self.pop_size//2], axis=0) # Use top 50%\n            \n            # Update covariance matrix (simplified)\n            C = np.cov(best_samples[:self.pop_size//2].T)  # Use top 50%\n            \n            # Adjust sigma\n            sigma *= np.exp(0.5 * (np.mean(best_fitness) - self.f_opt) / np.std(fitness_values)) # Damping", "configspace": "", "generation": 2, "feedback": "The algorithm HybridDECMASobol scored 0.696 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5ccb4803-b485-48f7-85ef-dc4ff615335d"], "operator": null, "metadata": {"aucs": [0.3586765535454619, 0.6616662567199392, 0.6583992908264724, 0.8631357532374462, 0.7314659770871511, 0.795892808302114, 0.685921441107163, 0.6628604848660641, 0.7249339248903753, 0.6839098721174799, 0.839394876819723, 0.9937884847596125, 0.5950839763686617, 0.7435951151895622, 0.9253617964207939, 0.7881601569794603, 0.6088552214002553, 0.8467994933739899, 0.22646245462583436, 0.5235314714008867]}}
{"id": "727bb8b0-5a52-4e82-80d6-83f122049b37", "fitness": 0.6419341365579634, "name": "AdaptiveDE", "description": "Simplified Adaptive Differential Evolution with memory-based parameter adaptation, archive, and restart mechanisms for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, f_init=0.5, cr_init=0.9, restart_trigger=0.1, p_archive=0.1, memory_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.f = np.full(pop_size, f_init)\n        self.cr = np.full(pop_size, cr_init)\n        self.restart_trigger = restart_trigger\n        self.archive = []\n        self.archive_fitness = []\n        self.p_archive = p_archive\n        self.memory_size = memory_size\n        self.sf_memory = []\n        self.scr_memory = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.population[best_idx]\n        \n        while self.evals < self.budget:\n            if self.evals > self.restart_trigger * self.budget:\n                if np.std(self.fitness) < 1e-6:\n                    self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                    self.fitness = np.array([func(x) for x in self.population])\n                    self.f = np.full(self.pop_size, 0.5)\n                    self.cr = np.full(self.pop_size, 0.9)\n                    self.evals += self.pop_size\n                    best_idx = np.argmin(self.fitness)\n                    if self.fitness[best_idx] < self.f_opt:\n                        self.f_opt = self.fitness[best_idx]\n                        self.x_opt = self.population[best_idx]\n                    continue\n            \n            for i in range(self.pop_size):\n                idxs = np.random.choice(np.arange(self.pop_size), 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                \n                if len(self.archive) > 0 and np.random.rand() < self.p_archive:\n                    idx_archive = np.random.randint(len(self.archive))\n                    x4 = self.archive[idx_archive]\n                    v = x1 + self.f[i] * (x2 - x3) # original\n                else:\n                    v = x1 + self.f[i] * (x2 - x3)\n                \n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                u = np.random.rand(self.dim) < self.cr[i]\n                u = v * u + self.population[i] * (~u)\n                u = np.clip(u, func.bounds.lb, func.bounds.ub)\n                \n                f = func(u)\n                self.evals += 1\n                \n                if f < self.fitness[i]:\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                        self.archive_fitness.append(self.fitness[i])\n                    else:\n                        if self.fitness[i] > np.min(self.archive_fitness):\n                            idx_replace = np.argmin(self.archive_fitness)\n                            self.archive[idx_replace] = self.population[i].copy()\n                            self.archive_fitness[idx_replace] = self.fitness[i]\n                    \n                    if len(self.sf_memory) < self.memory_size:\n                      self.sf_memory.append(self.f[i])\n                      self.scr_memory.append(self.cr[i])\n                    else:\n                      self.sf_memory = self.sf_memory[1:] + [self.f[i]]\n                      self.scr_memory = self.scr_memory[1:] + [self.cr[i]]\n\n                    self.fitness[i] = f\n                    self.population[i] = u\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n\n                if self.sf_memory:\n                  mf = np.mean(self.sf_memory)\n                  mcr = np.mean(self.scr_memory)\n\n                  self.f[i] = np.clip(np.random.normal(mf, 0.1), 0.1, 1.0)\n                  self.cr[i] = np.clip(np.random.normal(mcr, 0.1), 0.1, 1.0)\n                else:\n                  self.f[i] = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n                  self.cr[i] = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0)\n\n                if self.evals >= self.budget:\n                    break\n                    \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDE scored 0.642 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a733ebb1-7e9b-4a2b-9fac-37d1e0f1a61b"], "operator": null, "metadata": {"aucs": [0.24264510942545459, 0.5539655117486166, 0.5887115608921285, 0.9033015769655801, 0.5573537998890274, 0.7436526955890765, 0.5204921108855471, 0.6273041324388601, 0.6848011769813733, 0.6339647275471512, 0.8646763482031748, 0.995766163860085, 0.4869843076049464, 0.5503348451585959, 0.7921084416579242, 0.6979037312653933, 0.5358217681237715, 0.8014465512573253, 0.5439995953144297, 0.5134485763508048]}}
{"id": "c698389a-153f-47b6-9da1-fb7bb07e4542", "fitness": 0.481881445242258, "name": "AdaptiveDE_Levy", "description": "Adaptive Differential Evolution with a simplified adaptation of F and CR based on the population's current best and worst fitness, combined with a Lvy flight mutation for enhanced exploration.", "code": "import numpy as np\n\nclass AdaptiveDE_Levy:\n    def __init__(self, budget=10000, dim=10, pop_size=50, f_init=0.5, cr_init=0.9, levy_exponent=1.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.f_init = f_init\n        self.cr_init = cr_init\n        self.levy_exponent = levy_exponent  # Parameter for Levy flight\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n\n    def levy_flight(self, size, exponent):\n        \"\"\"Generates Levy flight steps.\"\"\"\n        num = np.random.normal(0, scale=1, size=size)\n        den = np.power(np.abs(np.random.normal(0, scale=1, size=size)), (1/exponent))\n        sigma = (np.math.gamma(1 + exponent) * np.sin(np.pi * exponent / 2) / (np.math.gamma((1 + exponent) / 2) * exponent * np.power(2, (exponent - 1) / 2)))**(1/exponent)\n        step = sigma * (num / den)\n        return step\n\n    def __call__(self, func):\n        # Initialize population\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n\n        # Initial best fitness and solution\n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.population[best_idx]\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(np.arange(self.pop_size), 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n\n                # Adaptive F and CR based on best and worst fitness\n                f = self.f_init * (1 + (self.fitness[np.argmax(self.fitness)] - self.fitness[i]) / (self.fitness[np.argmax(self.fitness)] - self.fitness[np.argmin(self.fitness)] + 1e-8))\n                cr = self.cr_init * (1 - (self.fitness[np.argmax(self.fitness)] - self.fitness[i]) / (self.fitness[np.argmax(self.fitness)] - self.fitness[np.argmin(self.fitness)] + 1e-8))\n\n                # Levy flight mutation\n                levy_steps = self.levy_flight(self.dim, self.levy_exponent)\n                v = x1 + f * (x2 - x3) + 0.01 * levy_steps  # Added Levy flight\n\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                u = np.random.rand(self.dim) < cr\n                u = v * u + self.population[i] * (~u)\n                u = np.clip(u, func.bounds.lb, func.bounds.ub)\n\n                # Evaluation\n                f_new = func(u)\n                self.evals += 1\n\n                # Selection\n                if f_new < self.fitness[i]:\n                    self.fitness[i] = f_new\n                    self.population[i] = u\n\n                    if f_new < self.f_opt:\n                        self.f_opt = f_new\n                        self.x_opt = u\n\n                if self.evals >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDE_Levy scored 0.482 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a733ebb1-7e9b-4a2b-9fac-37d1e0f1a61b"], "operator": null, "metadata": {"aucs": [0.172057494061143, 0.2841247552349849, 0.4588826096340466, 0.7455172931918597, 0.44924019742380983, 0.539974532931235, 0.33948094237255333, 0.4289391912497448, 0.46472289647963305, 0.35074212640458535, 0.6400884014501675, 0.9999137351313342, 0.26801018094008866, 0.42604687096015315, 0.827280856092769, 0.555948115701955, 0.3709856982875964, 0.6221415493526947, 0.19394774466674014, 0.49958371327806594]}}
{"id": "19c96dd5-bb9f-40dd-90b5-ef2f451288f5", "fitness": 0.6332964831287409, "name": "AdaptiveDE", "description": "Simplified Adaptive Differential Evolution with memory-based parameter adaptation and a stagnation-aware restart mechanism.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, f_init=0.5, cr_init=0.9, restart_trigger=0.1, p_archive=0.1, memory_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.f = np.full(pop_size, f_init)\n        self.cr = np.full(pop_size, cr_init)\n        self.restart_trigger = restart_trigger\n        self.archive = []\n        self.p_archive = p_archive\n        self.memory_size = memory_size\n        self.sf_memory = []\n        self.scr_memory = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.population[best_idx]\n        \n        while self.evals < self.budget:\n            # Restart strategy\n            if self.evals > self.restart_trigger * self.budget:\n                if np.std(self.fitness) < 1e-6:  # Stagnation check\n                    self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                    self.fitness = np.array([func(x) for x in self.population])\n                    self.f = np.full(self.pop_size, 0.5)\n                    self.cr = np.full(self.pop_size, 0.9)\n                    self.evals += self.pop_size\n                    \n                    best_idx = np.argmin(self.fitness)\n                    if self.fitness[best_idx] < self.f_opt:\n                        self.f_opt = self.fitness[best_idx]\n                        self.x_opt = self.population[best_idx]\n                    continue\n            \n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(np.arange(self.pop_size), 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                \n                if len(self.archive) > 0 and np.random.rand() < self.p_archive:\n                    idx_archive = np.random.randint(len(self.archive))\n                    v = x1 + self.f[i] * (x2 - self.archive[idx_archive])\n                else:\n                    v = x1 + self.f[i] * (x2 - x3)\n                \n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = np.random.rand(self.dim) < self.cr[i]\n                u = v * u + self.population[i] * (~u)\n                u = np.clip(u, func.bounds.lb, func.bounds.ub)\n                \n                # Evaluation\n                f = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f < self.fitness[i]:\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        self.archive[np.random.randint(self.archive_size)] = self.population[i].copy()\n                    \n                    # Save successful F and CR\n                    self.sf_memory.append(self.f[i])\n                    self.scr_memory.append(self.cr[i])\n                    if len(self.sf_memory) > self.memory_size:\n                        self.sf_memory.pop(0)\n                        self.scr_memory.pop(0)\n\n                    self.fitness[i] = f\n                    self.population[i] = u\n                    \n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n\n                # Adaptive Parameter Control\n                if self.sf_memory:\n                    mf = np.mean(self.sf_memory)\n                    mcr = np.mean(self.scr_memory)\n                    self.f[i] = np.clip(np.random.normal(mf, 0.1), 0.1, 1.0)\n                    self.cr[i] = np.clip(np.random.normal(mcr, 0.1), 0.1, 1.0)\n                else:\n                    self.f[i] = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n                    self.cr[i] = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0)\n\n                if self.evals >= self.budget:\n                    break\n                    \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDE scored 0.633 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a733ebb1-7e9b-4a2b-9fac-37d1e0f1a61b"], "operator": null, "metadata": {"aucs": [0.33617893737374327, 0.7043810283829525, 0.5299553360772481, 0.871509345632147, 0.49016551242440565, 0.7003675779742127, 0.6158338920140696, 0.5235828912990645, 0.6793754679812638, 0.6957244660933386, 0.8675627005912416, 0.9915376290803536, 0.3931355657515303, 0.6829470808921575, 0.762611368687001, 0.7134529948188286, 0.6072607889415418, 0.7419139684538805, 0.24417160689326167, 0.5142615032125729]}}
{"id": "694497a4-a34e-4089-a7b7-f1e1290f0977", "fitness": 0.11846961041815562, "name": "DynamicDE", "description": "Simplified Dynamic Differential Evolution with adaptive mutation factor and reduced population adjustments.", "code": "import numpy as np\n\nclass DynamicDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, cr=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.initial_pop_size = initial_pop_size\n        self.cr = cr  # Crossover rate\n        self.min_pop_size = 10  # Minimal population size\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n\n        for i in range(self.pop_size):\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.population[i]\n\n        generation = 0\n        while self.evals < self.budget:\n            generation += 1\n            adaptive_f = 0.5 + 0.5 * np.exp(-generation / 50) # Adaptive F\n\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(np.arange(self.pop_size), 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                v = x1 + adaptive_f * (x2 - x3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                u = np.random.rand(self.dim) < self.cr\n                u = v * u + self.population[i] * (~u)\n\n                # Evaluation\n                f = func(u)\n                self.evals += 1\n\n                # Selection\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.population[i] = u\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n\n            # Dynamic Population Size Adjustment (simplified)\n            if generation % 50 == 0:\n                if np.std(self.fitness) < 1e-4 and self.pop_size > self.min_pop_size:\n                    self.pop_size = max(self.min_pop_size, int(self.pop_size * 0.8))\n                    self.population = self.population[:self.pop_size]\n                    self.fitness = self.fitness[:self.pop_size]\n\n\n            if self.evals >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm DynamicDE scored 0.118 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a2b576fa-126e-4997-80e9-b3cd2657356d"], "operator": null, "metadata": {"aucs": [0.23693922083631125, 0]}}
{"id": "073d648c-5bee-42f0-be57-1f27f7910be6", "fitness": 0.0, "name": "AdaptiveDE", "description": "Simplified Differential Evolution with reduced parameter adaptation and population diversity maintenance through random re-initialization of stagnant individuals.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, f=0.5, cr=0.9, adapt_freq=10, stagnation_threshold=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.f = f\n        self.cr = cr\n        self.adapt_freq = adapt_freq\n        self.stagnation_threshold = stagnation_threshold\n        self.best_fitness_history = []\n        self.stagnation_counter = 0\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.population[best_idx]\n        self.best_fitness_history.append(self.f_opt)\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(np.arange(self.pop_size), 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                v = x1 + self.f * (x2 - x3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = np.random.rand(self.dim) < self.cr\n                u = v * u + self.population[i] * (~u)\n                \n                # Evaluation\n                f = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.population[i] = u\n                    \n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n                        self.best_fitness_history.append(self.f_opt)\n                        self.stagnation_counter = 0 # Reset stagnation counter\n                else:\n                    self.stagnation_counter += 1  # Increment if no improvement\n\n            # Adaptive Parameter Control (less frequent adaptation)\n            if self.evals % self.adapt_freq == 0:\n                if np.std(self.fitness) > 1e-3:\n                    self.f = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n                    self.cr = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0)\n\n            # Stagnation Check and Re-initialization\n            if self.stagnation_counter > self.stagnation_threshold * self.pop_size:\n                worst_idx = np.argmax(self.fitness)\n                self.population[worst_idx] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                self.fitness[worst_idx] = func(self.population[worst_idx])\n                self.evals += 1\n                self.stagnation_counter = 0  # Reset stagnation counter\n                best_idx = np.argmin(self.fitness)\n                if self.fitness[best_idx] < self.f_opt:\n                    self.f_opt = self.fitness[best_idx]\n                    self.x_opt = self.population[best_idx]\n                    self.best_fitness_history.append(self.f_opt)\n\n\n            if self.evals >= self.budget:\n                break\n                    \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["65c7636f-9a53-454c-a6c2-624e7495fddc"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "2d39e557-6a00-4161-8474-0c995dddd6ad", "fitness": 0.0, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with velocity clamping, archive for stagnation avoidance, and simplified adaptation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, f=0.5, cr=0.9, adapt_freq=10, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.f = f\n        self.cr = cr\n        self.adapt_freq = adapt_freq\n        self.archive_size = archive_size\n        self.archive = []\n        self.best_fitness_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.population[best_idx]\n        self.best_fitness_history.append(self.f_opt)\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(np.arange(self.pop_size), 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                \n                # Velocity Clamping: Limit the step size\n                velocity = self.f * (x2 - x3)\n                velocity_norm = np.linalg.norm(velocity)\n                if velocity_norm > 1.0:  # Example threshold\n                    velocity = velocity / velocity_norm  # Normalize to unit length\n\n                v = x1 + velocity\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = np.random.rand(self.dim) < self.cr\n                u = v * u + self.population[i] * (~u)\n                \n                # Evaluation\n                f = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f < self.fitness[i]:\n                    # Archive the replaced individual (stagnation avoidance)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        idx_to_replace = np.random.randint(0, self.archive_size)\n                        self.archive[idx_to_replace] = self.population[i].copy()\n                    \n                    self.fitness[i] = f\n                    self.population[i] = u\n                    \n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n                        self.best_fitness_history.append(self.f_opt)\n                else:\n                    # If the trial vector is not better, try perturbing from the archive\n                    if len(self.archive) > 0 and np.random.rand() < 0.1: #Small probability\n                        arch_idx = np.random.randint(0, len(self.archive))\n                        v = self.archive[arch_idx] + self.f * (x1 - x2) #perturb\n                        v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                        \n                        u = np.random.rand(self.dim) < self.cr\n                        u = v * u + self.population[i] * (~u)\n                        \n                        f = func(u)\n                        self.evals += 1\n                        \n                        if f < self.fitness[i]:\n                            self.fitness[i] = f\n                            self.population[i] = u\n                            \n                            if f < self.f_opt:\n                                self.f_opt = f\n                                self.x_opt = u\n                                self.best_fitness_history.append(self.f_opt)\n\n            # Adaptive Parameter Control (less frequent adaptation)\n            if self.evals % self.adapt_freq == 0:\n                if np.std(self.fitness) > 1e-3:\n                    self.f = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n                    self.cr = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0)\n\n            if self.evals >= self.budget:\n                break\n                    \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["65c7636f-9a53-454c-a6c2-624e7495fddc"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "acc0744f-81f3-4407-ac39-4377b4bac1de", "fitness": 0.05458692521511166, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with success-history based parameter adaptation and a restart mechanism to escape local optima.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, f=0.5, cr=0.9, adapt_freq=10, restart_trigger=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.f = f\n        self.cr = cr\n        self.adapt_freq = adapt_freq\n        self.memory_f = []\n        self.memory_cr = []\n        self.best_fitness_history = []\n        self.restart_trigger = restart_trigger  # Number of iterations without improvement to trigger restart\n        self.no_improvement_count = 0\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.population[best_idx]\n        self.best_fitness_history.append(self.f_opt)\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(np.arange(self.pop_size), 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                v = x1 + self.f * (x2 - x3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = np.random.rand(self.dim) < self.cr\n                u = v * u + self.population[i] * (~u)\n                \n                # Evaluation\n                f = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.population[i] = u\n                    self.memory_f.append(self.f)\n                    self.memory_cr.append(self.cr)\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n                        self.best_fitness_history.append(self.f_opt)\n                        self.no_improvement_count = 0  # Reset counter\n                else:\n                    self.no_improvement_count += 1  # Increment counter\n\n            # Adaptive Parameter Control (success-history based adaptation)\n            if self.evals % self.adapt_freq == 0 and len(self.memory_f) > 0:\n                self.f = np.clip(np.mean(self.memory_f), 0.1, 1.0) if len(self.memory_f) > 0 else 0.5\n                self.cr = np.clip(np.mean(self.memory_cr), 0.1, 1.0) if len(self.memory_cr) > 0 else 0.9\n                self.memory_f = []\n                self.memory_cr = []\n\n\n            # Restart mechanism\n            if self.no_improvement_count >= self.restart_trigger:\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.evals += self.pop_size\n\n                best_idx = np.argmin(self.fitness)\n                if self.fitness[best_idx] < self.f_opt:\n                     self.f_opt = self.fitness[best_idx]\n                     self.x_opt = self.population[best_idx]\n                self.no_improvement_count = 0  # Reset counter\n                \n\n            if self.evals >= self.budget:\n                break\n                    \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDE scored 0.055 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["65c7636f-9a53-454c-a6c2-624e7495fddc"], "operator": null, "metadata": {"aucs": [0.10917385043022332, 0]}}
{"id": "03b33ecd-12d8-499a-816c-3b2e9ab981d8", "fitness": 0.0, "name": "EnhancedDynamicDE", "description": "Enhanced Dynamic Differential Evolution with toroidal boundary handling, adaptive mutation factor, and orthogonal learning for improved exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedDynamicDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, f_initial=0.5, cr=0.9, levy_exponent=1.5, f_decay=0.995):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.initial_pop_size = initial_pop_size\n        self.f = f_initial  # Mutation factor (initial value)\n        self.f_initial = f_initial\n        self.cr = cr # Crossover rate\n        self.levy_exponent = levy_exponent # Levy flight exponent\n        self.min_pop_size = 10 # Minimal population size\n        self.f_decay = f_decay\n        self.archive_factor = 2.0  # Archive size relative to population size\n\n    def levy_flight(self, size):\n        \"\"\"\n        Generate Levy distribution samples.\n        \"\"\"\n        num = np.random.normal(0, 1, size)\n        den = np.power(np.abs(np.random.normal(0, 1, size)), (1/self.levy_exponent))\n        sigma = (np.math.gamma(1 + self.levy_exponent) * np.sin(np.pi * self.levy_exponent / 2) / (np.math.gamma((1 + self.levy_exponent) / 2) * self.levy_exponent * np.power(2, (self.levy_exponent - 1) / 2)))**(1/self.levy_exponent)\n        return sigma * (num / den)\n\n    def toroidal_boundary_handling(self, x, lb, ub):\n        \"\"\"\n        Wraps the solution within the boundaries.\n        \"\"\"\n        width = ub - lb\n        x_wrapped = lb + (x - lb) % width\n        return x_wrapped\n    \n    def orthogonal_learning(self, population, fitness, lb, ub, num_samples=5):\n        \"\"\"\n        Orthogonal learning strategy to explore promising regions.\n        \"\"\"\n        best_idx = np.argmin(fitness)\n        best_individual = population[best_idx]\n        dim = population.shape[1]\n        new_samples = np.zeros((num_samples, dim))\n        \n        for i in range(num_samples):\n            # Generate a new sample by perturbing the best individual\n            perturbation = np.random.uniform(-0.1, 0.1, dim) * (ub - lb)  # Smaller perturbation\n            new_sample = best_individual + perturbation\n            new_sample = self.toroidal_boundary_handling(new_sample, lb, ub)\n            new_samples[i] = new_sample\n        \n        return new_samples\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.archive = []\n\n        for i in range(self.pop_size):\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.population[i]\n        \n        generation = 0\n        while self.evals < self.budget:\n            generation += 1\n            \n            # Adaptive Mutation Factor\n            self.f = self.f * self.f_decay\n\n            for i in range(self.pop_size):\n                # Mutation with Levy Flight\n                idxs = np.random.choice(np.arange(self.pop_size), 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                levy_step = self.levy_flight(self.dim)\n                v = x1 + self.f * (x2 - x3) + 0.01 * levy_step * (self.x_opt - self.population[i]) # Adding levy flight component\n                v = self.toroidal_boundary_handling(v, lb, ub)\n                \n                # Crossover\n                u = np.random.rand(self.dim) < self.cr\n                u = v * u + self.population[i] * (~u)\n                \n                # Evaluation\n                f = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f < self.fitness[i]:\n                    self.archive.append(self.population[i].copy())\n                    self.fitness[i] = f\n                    self.population[i] = u\n                    \n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n                else:\n                    self.archive.append(u.copy())\n\n            # Dynamic Population Size Adjustment (reduce if stagnating, increase if promising)\n            if generation % 10 == 0:\n                fitness_std = np.std(self.fitness)\n                if fitness_std < 1e-4 and self.pop_size > self.min_pop_size:\n                    self.pop_size = max(self.min_pop_size, int(self.pop_size * 0.9))  # Reduce population\n                    self.population = self.population[:self.pop_size]\n                    self.fitness = self.fitness[:self.pop_size]\n                elif fitness_std > 1e-2 and self.pop_size < self.initial_pop_size * 2:\n                    new_individuals = np.random.uniform(lb, ub, size=(self.initial_pop_size - self.pop_size, self.dim))\n                    new_fitness = np.array([func(x) for x in new_individuals])\n                    self.evals += len(new_individuals)\n                    self.population = np.vstack((self.population, new_individuals))\n                    self.fitness = np.concatenate((self.fitness, new_fitness))\n                    self.pop_size = self.initial_pop_size\n                    for i in range(self.pop_size, len(self.population)):\n                        if self.fitness[i] < self.f_opt:\n                            self.f_opt = self.fitness[i]\n                            self.x_opt = self.population[i]\n            \n            # Orthogonal learning\n            new_samples = self.orthogonal_learning(self.population, self.fitness, lb, ub)\n            new_fitness = [func(x) for x in new_samples]\n            self.evals += len(new_samples)\n            \n            for i, f in enumerate(new_fitness):\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = new_samples[i]\n\n            # Archive management (optional)\n            if len(self.archive) > self.archive_factor * self.pop_size:\n                self.archive = self.archive[-int(self.archive_factor * self.pop_size):]\n\n            if self.evals >= self.budget:\n                break\n                    \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm EnhancedDynamicDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a2b576fa-126e-4997-80e9-b3cd2657356d"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "1a2de72e-d459-44f2-bdf5-61040dc3d271", "fitness": 0.0, "name": "EnhancedDynamicDE", "description": "Enhanced Dynamic Differential Evolution with adaptive parameter control, orthogonal learning, and a restart mechanism to escape local optima.", "code": "import numpy as np\n\nclass EnhancedDynamicDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, f=0.5, cr=0.9, levy_exponent=1.5, archive_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.initial_pop_size = initial_pop_size\n        self.f = f  # Mutation factor\n        self.cr = cr # Crossover rate\n        self.levy_exponent = levy_exponent # Levy flight exponent\n        self.min_pop_size = 10 # Minimal population size\n        self.archive_size = archive_size\n        self.archive = []\n        self.archive_fitness = []\n\n    def levy_flight(self, size):\n        \"\"\"\n        Generate Levy distribution samples.\n        \"\"\"\n        num = np.random.normal(0, 1, size)\n        den = np.power(np.abs(np.random.normal(0, 1, size)), (1/self.levy_exponent))\n        sigma = (np.math.gamma(1 + self.levy_exponent) * np.sin(np.pi * self.levy_exponent / 2) / (np.math.gamma((1 + self.levy_exponent) / 2) * self.levy_exponent * np.power(2, (self.levy_exponent - 1) / 2)))**(1/self.levy_exponent)\n        return sigma * (num / den)\n\n    def orthogonal_learning(self, x):\n        \"\"\"\n        Generate an orthogonal vector based on the current solution.\n        \"\"\"\n        orthogonal_vector = np.random.normal(0, 0.1, self.dim)  # Small perturbation\n        return x + orthogonal_vector\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        \n        for i in range(self.pop_size):\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.population[i]\n        \n        generation = 0\n        stagnation_counter = 0\n        last_improvement = 0\n\n        while self.evals < self.budget:\n            generation += 1\n            \n            # Adaptive F and CR\n            self.f = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n            self.cr = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0)\n\n            for i in range(self.pop_size):\n                # Mutation with Levy Flight\n                idxs = np.random.choice(np.arange(self.pop_size), 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                levy_step = self.levy_flight(self.dim)\n                v = x1 + self.f * (x2 - x3) + 0.01 * levy_step * (self.x_opt - self.population[i]) # Adding levy flight component\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = np.random.rand(self.dim) < self.cr\n                u = v * u + self.population[i] * (~u)\n                \n                # Orthogonal learning\n                u_orth = self.orthogonal_learning(u)\n                u_orth = np.clip(u_orth, func.bounds.lb, func.bounds.ub)\n                \n                # Evaluation\n                f = func(u)\n                f_orth = func(u_orth)\n                self.evals += 2\n                \n                # Selection\n                if f < self.fitness[i] and f <= f_orth:\n                    self.fitness[i] = f\n                    self.population[i] = u\n                    \n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n                        last_improvement = generation\n                elif f_orth < self.fitness[i]:\n                    self.fitness[i] = f_orth\n                    self.population[i] = u_orth\n\n                    if f_orth < self.f_opt:\n                        self.f_opt = f_orth\n                        self.x_opt = u_orth\n                        last_improvement = generation\n            \n            # Archive best solutions\n            if len(self.archive) < self.archive_size:\n                self.archive.append(self.x_opt.copy())\n                self.archive_fitness.append(self.f_opt)\n            else:\n                if self.f_opt < max(self.archive_fitness):\n                    max_index = np.argmax(self.archive_fitness)\n                    self.archive[max_index] = self.x_opt.copy()\n                    self.archive_fitness[max_index] = self.f_opt\n\n            # Dynamic Population Size Adjustment (reduce if stagnating, increase if promising)\n            if generation % 10 == 0:\n                fitness_std = np.std(self.fitness)\n                if fitness_std < 1e-4 and self.pop_size > self.min_pop_size:\n                    self.pop_size = max(self.min_pop_size, int(self.pop_size * 0.9))  # Reduce population\n                    self.population = self.population[:self.pop_size]\n                    self.fitness = self.fitness[:self.pop_size]\n                elif fitness_std > 1e-2 and self.pop_size < self.initial_pop_size * 2:\n                    new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.initial_pop_size - self.pop_size, self.dim))\n                    new_fitness = np.array([func(x) for x in new_individuals])\n                    self.evals += len(new_individuals)\n                    self.population = np.vstack((self.population, new_individuals))\n                    self.fitness = np.concatenate((self.fitness, new_fitness))\n                    self.pop_size = self.initial_pop_size\n                    for i in range(self.pop_size, len(self.population)):\n                        if self.fitness[i] < self.f_opt:\n                            self.f_opt = self.fitness[i]\n                            self.x_opt = self.population[i]\n            \n            # Stagnation check and restart\n            if generation - last_improvement > 50:\n                stagnation_counter += 1\n                if stagnation_counter > 2:\n                    # Restart: Reinitialize population around best archive member\n                    best_archive_index = np.argmin(self.archive_fitness)\n                    best_archive_x = self.archive[best_archive_index]\n                    self.population = np.random.normal(loc=best_archive_x, scale=0.1, size=(self.pop_size, self.dim))\n                    self.population = np.clip(self.population, func.bounds.lb, func.bounds.ub)\n                    self.fitness = np.array([func(x) for x in self.population])\n                    self.evals += self.pop_size\n                    self.f_opt = np.min(self.fitness)\n                    self.x_opt = self.population[np.argmin(self.fitness)]\n                    stagnation_counter = 0\n                    last_improvement = generation\n\n\n            if self.evals >= self.budget:\n                break\n                    \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm EnhancedDynamicDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a2b576fa-126e-4997-80e9-b3cd2657356d"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "7d1cb9c7-5556-4d74-a487-2741662fab69", "fitness": 0.0, "name": "EnhancedDynamicDE", "description": "Enhanced Dynamic Differential Evolution with adaptive mutation factor, crossover rate, and population size based on success history, combined with a simplified local search.", "code": "import numpy as np\n\nclass EnhancedDynamicDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, f_initial=0.5, cr_initial=0.9, levy_exponent=1.5, local_search_freq=20):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.initial_pop_size = initial_pop_size\n        self.f = f_initial  # Mutation factor\n        self.cr = cr_initial # Crossover rate\n        self.levy_exponent = levy_exponent # Levy flight exponent\n        self.min_pop_size = 10 # Minimal population size\n        self.memory_f = []\n        self.memory_cr = []\n        self.local_search_freq = local_search_freq\n        self.success_prob = 0.0\n        self.success_history_f = []\n        self.success_history_cr = []\n        self.archive = []\n        self.archive_f = []\n\n    def levy_flight(self, size):\n        \"\"\"\n        Generate Levy distribution samples.\n        \"\"\"\n        num = np.random.normal(0, 1, size)\n        den = np.power(np.abs(np.random.normal(0, 1, size)), (1/self.levy_exponent))\n        sigma = (np.math.gamma(1 + self.levy_exponent) * np.sin(np.pi * self.levy_exponent / 2) / (np.math.gamma((1 + self.levy_exponent) / 2) * self.levy_exponent * np.power(2, (self.levy_exponent - 1) / 2)))**(1/self.levy_exponent)\n        return sigma * (num / den)\n\n    def local_search(self, func, x, f_x, step_size=0.1):\n        \"\"\"\n        Perform a simplified local search around the best solution.\n        \"\"\"\n        x_new = x + np.random.uniform(-step_size, step_size, size=self.dim)\n        x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n        f_new = func(x_new)\n        \n        return x_new, f_new\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        \n        for i in range(self.pop_size):\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.population[i]\n        \n        generation = 0\n        while self.evals < self.budget:\n            generation += 1\n            new_population = np.copy(self.population)\n            new_fitness = np.copy(self.fitness)\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                if self.success_history_f:\n                    self.f = np.random.choice(self.success_history_f)\n                if self.success_history_cr:\n                    self.cr = np.random.choice(self.success_history_cr)\n                \n                # Mutation with Levy Flight\n                idxs = np.random.choice(np.arange(self.pop_size), 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                levy_step = self.levy_flight(self.dim)\n                v = x1 + self.f * (x2 - x3) + 0.01 * levy_step * (self.x_opt - self.population[i]) # Adding levy flight component\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = np.random.rand(self.dim) < self.cr\n                u = v * u + self.population[i] * (~u)\n                \n                # Evaluation\n                f = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f < self.fitness[i]:\n                    new_fitness[i] = f\n                    new_population[i] = u\n                    self.success_history_f.append(self.f)\n                    self.success_history_cr.append(self.cr)\n                    if len(self.success_history_f) > 10:\n                        self.success_history_f.pop(0)\n                    if len(self.success_history_cr) > 10:\n                        self.success_history_cr.pop(0)\n                    \n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n                else:\n                    self.archive.append(self.population[i].copy())\n                    self.archive_f.append(self.fitness[i])\n            \n            self.population = new_population\n            self.fitness = new_fitness\n\n            # Dynamic Population Size Adjustment (reduce if stagnating, increase if promising)\n            if generation % 10 == 0:\n                fitness_std = np.std(self.fitness)\n                if fitness_std < 1e-4 and self.pop_size > self.min_pop_size:\n                    self.pop_size = max(self.min_pop_size, int(self.pop_size * 0.9))  # Reduce population\n                    self.population = self.population[:self.pop_size]\n                    self.fitness = self.fitness[:self.pop_size]\n                elif fitness_std > 1e-2 and self.pop_size < self.initial_pop_size * 2:\n                    new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.initial_pop_size - self.pop_size, self.dim))\n                    new_fitness = np.array([func(x) for x in new_individuals])\n                    self.evals += len(new_individuals)\n                    self.population = np.vstack((self.population, new_individuals))\n                    self.fitness = np.concatenate((self.fitness, new_fitness))\n                    self.pop_size = self.initial_pop_size\n                    for i in range(self.pop_size, len(self.population)):\n                        if self.fitness[i] < self.f_opt:\n                            self.f_opt = self.fitness[i]\n                            self.x_opt = self.population[i]\n            \n            # Local Search\n            if generation % self.local_search_freq == 0:\n                x_new, f_new = self.local_search(func, self.x_opt, self.f_opt)\n                self.evals += 1\n                if f_new < self.f_opt:\n                    self.f_opt = f_new\n                    self.x_opt = x_new\n\n            if self.evals >= self.budget:\n                break\n                    \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm EnhancedDynamicDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a2b576fa-126e-4997-80e9-b3cd2657356d"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "24da9b3b-348f-4ec1-b7f6-55bc26a87d9c", "fitness": 0.6424880504597842, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with simplified parameter adaptation based on success rate and reduced complexity by removing the archive.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, f=0.5, cr=0.9, restart_trigger=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.f = f  # Mutation factor\n        self.cr = cr # Crossover rate\n        self.restart_trigger = restart_trigger # Percentage of budget before restart\n        self.success_history_f = []\n        self.success_history_cr = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        \n        for i in range(self.pop_size):\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.population[i]\n        \n        while self.evals < self.budget:\n            \n            # Restart strategy\n            if self.evals > self.restart_trigger * self.budget:\n                if np.std(self.fitness) < 1e-6:  # Stagnation check\n                    self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                    self.fitness = np.array([func(x) for x in self.population])\n                    self.evals += self.pop_size\n                    for j in range(self.pop_size):\n                        if self.fitness[j] < self.f_opt:\n                            self.f_opt = self.fitness[j]\n                            self.x_opt = self.population[j]\n                    continue\n            \n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(np.arange(self.pop_size), 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                v = x1 + self.f * (x2 - x3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = self.crossover(self.population[i], v)\n                \n                # Evaluation\n                f = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f < self.fitness[i]:\n                    \n                    # Adaptive Parameter Control\n                    self.success_history_f.append(self.f)\n                    self.success_history_cr.append(self.cr)\n                    \n                    self.fitness[i] = f\n                    self.population[i] = u\n                    \n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n\n                    #Update F and CR\n                    self.f = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n                    self.cr = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0)\n\n                if self.evals >= self.budget:\n                    break\n                    \n        return self.f_opt, self.x_opt\n        \n    def crossover(self, x, v):\n        u = x.copy()\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.cr or j == j_rand:\n                u[j] = v[j]\n        return u", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDE scored 0.642 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b75a0ce2-7ce8-4a15-b9dd-abc2011f696f"], "operator": null, "metadata": {"aucs": [0.356160708724633, 0.6178332169418104, 0.6321645721536521, 0.874008812854659, 0.6716037890580709, 0.6848343056104929, 0.5688134335587136, 0.5939639123983825, 0.6633196472459209, 0.5693991875400942, 0.8553967787515349, 0.9988878483352869, 0.5623951580414512, 0.6559262806377533, 0.7828850840770814, 0.6761263002334823, 0.581303689167138, 0.7560902545658724, 0.23783039892177493, 0.5108176303778798]}}
{"id": "fcd8940d-a40e-4be3-977d-2248d7105651", "fitness": 0.5004052997603645, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with simplified parameter adaptation and reduced complexity using a combined archive and rank-based mutation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, f=0.5, cr=0.9, restart_trigger=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.f = f\n        self.cr = cr\n        self.restart_trigger = restart_trigger\n        self.archive = []\n        self.archive_fitness = []\n        self.sf_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.population[best_idx]\n        \n        while self.evals < self.budget:\n            # Restart strategy\n            if self.evals > self.restart_trigger * self.budget:\n                if np.std(self.fitness) < 1e-6:\n                    self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                    self.fitness = np.array([func(x) for x in self.population])\n                    self.evals += self.pop_size\n                    best_idx = np.argmin(self.fitness)\n                    if self.fitness[best_idx] < self.f_opt:\n                        self.f_opt = self.fitness[best_idx]\n                        self.x_opt = self.population[best_idx]\n                    continue\n            \n            ranked_indices = np.argsort(self.fitness) # Rank individuals\n            \n            for i in range(self.pop_size):\n                # Mutation: Rank-based selection\n                best_idx = ranked_indices[0]  # Best individual\n                worst_idx = ranked_indices[-1] # Worst individual\n\n                idxs = np.random.choice(np.arange(self.pop_size), 2, replace=False)\n                x1, x2 = self.population[idxs]\n                \n                v = self.population[best_idx] + self.f * (x1 - x2)\n\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = self.crossover(self.population[i], v)\n                \n                # Evaluation\n                f = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f < self.fitness[i]:\n                    # Combine Archive and Population for replacement\n                    combined_population = np.vstack((self.population, np.array(self.archive))) if self.archive else self.population\n                    combined_fitness = np.concatenate((self.fitness, np.array(self.archive_fitness))) if self.archive else self.fitness\n\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i])\n                        self.archive_fitness.append(self.fitness[i])\n                    else:\n                        worst_archive_idx = np.argmax(self.archive_fitness)\n                        if self.fitness[i] < self.archive_fitness[worst_archive_idx]:\n                            self.archive[worst_archive_idx] = self.population[i]\n                            self.archive_fitness[worst_archive_idx] = self.fitness[i]\n\n                    #Adaptive F update based on success\n                    if f < self.fitness[i]:\n                        self.sf_history.append(abs(self.fitness[i] - f))\n                        if len(self.sf_history) > 50:\n                            self.sf_history.pop(0)\n\n                    self.fitness[i] = f\n                    self.population[i] = u\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n                \n                # Adaptive Parameter Control (simplified)\n                if np.random.rand() < 0.1:\n                    if self.sf_history:\n                        self.f = np.clip(np.random.normal(0.5, np.std(self.sf_history)), 0.1, 1.0)\n                    else:\n                        self.f = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0) \n                    self.cr = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0)\n\n                if self.evals >= self.budget:\n                    break\n        return self.f_opt, self.x_opt\n        \n    def crossover(self, x, v):\n        u = x.copy()\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.cr or j == j_rand:\n                u[j] = v[j]\n        return u", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDE scored 0.500 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b75a0ce2-7ce8-4a15-b9dd-abc2011f696f"], "operator": null, "metadata": {"aucs": [0.15382342677537308, 0.2613035377424563, 0.6248729622246809, 0.3378505056062939, 0.8499426481550475, 0.27561948856995366, 0.37773669384365904, 0.27888999600616815, 0.37946194585634807, 0.17664867610822566, 0.9041969022475207, 0.9959023136609447, 0.2271944143255924, 0.6397894415799232, 0.8346297541858823, 0.7648413443873452, 0.3329198468893162, 0.9037515975538124, 0.20373342900790414, 0.48499707048084373]}}
{"id": "00f88f83-119a-4cb1-8eec-4370c9b14fb6", "fitness": 0.6677738067066276, "name": "EnsembleAdaptiveDE", "description": "Adaptive Differential Evolution with ensemble-based parameter control and a restart mechanism triggered by stagnation detection.", "code": "import numpy as np\n\nclass EnsembleAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, num_ensembles=3, adapt_freq=50, stagnation_threshold=1000):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.num_ensembles = num_ensembles\n        self.adapt_freq = adapt_freq\n        self.stagnation_threshold = stagnation_threshold\n        self.ensemble_fs = np.random.uniform(0.3, 0.7, size=num_ensembles)\n        self.ensemble_crs = np.random.uniform(0.7, 0.95, size=num_ensembles)\n        self.ensemble_success = np.zeros(num_ensembles)\n        self.ensemble_counts = np.ones(num_ensembles)\n        self.best_fitness_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.last_improvement = 0\n\n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.population[best_idx]\n        self.best_fitness_history.append(self.f_opt)\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Choose an ensemble member\n                ensemble_idx = np.random.choice(np.arange(self.num_ensembles), p=self.ensemble_success / self.ensemble_success.sum()) if self.ensemble_success.sum() > 0 else np.random.randint(0, self.num_ensembles)\n\n                f = self.ensemble_fs[ensemble_idx]\n                cr = self.ensemble_crs[ensemble_idx]\n\n                # Mutation\n                idxs = np.random.choice(np.arange(self.pop_size), 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                v = x1 + f * (x2 - x3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = np.random.rand(self.dim) < cr\n                u = v * u + self.population[i] * (~u)\n                \n                # Evaluation\n                f_new = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f_new < self.fitness[i]:\n                    self.fitness[i] = f_new\n                    self.population[i] = u\n                    self.ensemble_success[ensemble_idx] += 1\n\n                    if f_new < self.f_opt:\n                        self.f_opt = f_new\n                        self.x_opt = u\n                        self.best_fitness_history.append(self.f_opt)\n                        self.last_improvement = self.evals\n                \n                self.ensemble_counts[ensemble_idx] += 1\n\n            # Adaptive Parameter Control\n            if self.evals % self.adapt_freq == 0:\n                self.ensemble_fs = np.clip(np.random.normal(self.ensemble_fs, 0.05), 0.1, 0.9)\n                self.ensemble_crs = np.clip(np.random.normal(self.ensemble_crs, 0.05), 0.5, 1.0)\n                self.ensemble_success[:] = 0\n                self.ensemble_counts[:] = 1\n            \n            # Stagnation Check and Restart\n            if self.evals - self.last_improvement > self.stagnation_threshold:\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.evals += self.pop_size\n                best_idx = np.argmin(self.fitness)\n                self.f_opt = self.fitness[best_idx]\n                self.x_opt = self.population[best_idx]\n                self.last_improvement = self.evals\n                self.best_fitness_history.append(self.f_opt)\n\n\n            if self.evals >= self.budget:\n                break\n                    \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm EnsembleAdaptiveDE scored 0.668 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["65c7636f-9a53-454c-a6c2-624e7495fddc"], "operator": null, "metadata": {"aucs": [0.2650277344810096, 0.49462785239155527, 0.4953232133894381, 0.8900469298177608, 0.6354489124560312, 0.7512011567430522, 0.7160442012239407, 0.6109289216674648, 0.697830732467819, 0.6669704257667217, 0.8624151990732445, 0.9910010826062896, 0.33847479240362033, 0.7940747331068264, 0.9096483880734835, 0.74819061814889, 0.5278721721304275, 0.827264057145854, 0.6152420035587566, 0.5178430074803658]}}
{"id": "4f41b7f1-3026-4f4d-acd2-1b2451a03723", "fitness": 0.0, "name": "SelfOrganizingPSO", "description": "A self-organizing particle swarm optimizer with velocity clamping and adaptive inertia, coupled with a mutation operator triggered by stagnation.", "code": "import numpy as np\n\nclass SelfOrganizingPSO:\n    def __init__(self, budget=10000, dim=10, pop_size=50, inertia=0.7, c1=1.5, c2=1.5, stagnation_threshold=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.inertia = inertia\n        self.c1 = c1\n        self.c2 = c2\n        self.stagnation_threshold = stagnation_threshold\n        self.velocity_clamp = 1.0  # Clamp velocities to +/- this value\n        self.mutation_rate = 0.05\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n        # Initialize particles and velocities\n        self.particles = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.random.uniform(-0.1 * (func.bounds.ub - func.bounds.lb), 0.1 * (func.bounds.ub - func.bounds.lb), size=(self.pop_size, self.dim))  # Initialize velocities\n\n        # Initialize personal best positions and fitnesses\n        self.personal_best_positions = self.particles.copy()\n        self.personal_best_fitnesses = np.array([func(x) for x in self.particles])\n        self.evals = self.pop_size\n\n        # Initialize global best position and fitness\n        best_idx = np.argmin(self.personal_best_fitnesses)\n        self.global_best_position = self.personal_best_positions[best_idx].copy()\n        self.global_best_fitness = self.personal_best_fitnesses[best_idx]\n        self.x_opt = self.global_best_position\n        self.f_opt = self.global_best_fitness\n        \n        stagnation_counter = 0\n\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Update velocity\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                \n                cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.particles[i])\n                social_component = self.c2 * r2 * (self.global_best_position - self.particles[i])\n                \n                self.velocities[i] = self.inertia * self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp, self.velocity_clamp)  # Clamp velocity\n\n                # Update position\n                self.particles[i] = self.particles[i] + self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], func.bounds.lb, func.bounds.ub)\n\n                # Evaluate fitness\n                fitness = func(self.particles[i])\n                self.evals += 1\n\n                # Update personal best\n                if fitness < self.personal_best_fitnesses[i]:\n                    self.personal_best_fitnesses[i] = fitness\n                    self.personal_best_positions[i] = self.particles[i].copy()\n\n                    # Update global best\n                    if fitness < self.global_best_fitness:\n                        self.global_best_fitness = fitness\n                        self.global_best_position = self.particles[i].copy()\n                        self.x_opt = self.global_best_position\n                        self.f_opt = self.global_best_fitness\n                        stagnation_counter = 0 # Reset stagnation counter\n            \n            # Stagnation Check & Mutation\n            stagnation_counter += 1\n            if stagnation_counter > self.stagnation_threshold:\n                # Apply mutation to a fraction of the population\n                num_mutants = int(self.mutation_rate * self.pop_size)\n                mutant_indices = np.random.choice(self.pop_size, num_mutants, replace=False)\n                for idx in mutant_indices:\n                    self.particles[idx] = np.random.uniform(func.bounds.lb, func.bounds.ub) #Reinitialize position\n                    self.velocities[idx] = np.random.uniform(-0.1 * (func.bounds.ub - func.bounds.lb), 0.1 * (func.bounds.ub - func.bounds.lb))  # Reinitialize velocity\n                    fitness = func(self.particles[idx])\n                    self.evals += 1\n                    self.personal_best_positions[idx] = self.particles[idx].copy()\n                    self.personal_best_fitnesses[idx] = fitness\n                    if fitness < self.global_best_fitness:\n                        self.global_best_fitness = fitness\n                        self.global_best_position = self.particles[idx].copy()\n                        self.x_opt = self.global_best_position\n                        self.f_opt = self.global_best_fitness\n                stagnation_counter = 0  # Reset stagnation counter\n                \n            if self.evals >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm SelfOrganizingPSO scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["65c7636f-9a53-454c-a6c2-624e7495fddc"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "77b25529-87c5-43b2-9417-0cde1ade20a1", "fitness": 0.6972181283860382, "name": "HybridDECMASobolV2", "description": "Enhanced hybrid DE with CMA-ES and local search, featuring adaptive parameter control based on success rates and incorporating a restart mechanism to escape local optima.", "code": "import numpy as np\nfrom scipy.linalg import sqrtm\n\nclass HybridDECMASobolV2:\n    def __init__(self, budget=10000, dim=10, pop_size=50, f_init=0.5, cr_init=0.9, neighborhood_size=0.1, cma_generations=5, restart_trigger=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.f = np.full(pop_size, f_init)  # Individual mutation factors\n        self.cr = np.full(pop_size, cr_init) # Individual crossover rates\n        self.neighborhood_size = neighborhood_size\n        self.cma_generations = cma_generations\n        self.learning_rate = 0.1  # Adjusts the adaptation rate\n        self.cma_evals = 0\n        self.success_f = np.zeros(pop_size)\n        self.success_cr = np.zeros(pop_size)\n        self.success_count = np.zeros(pop_size)\n        self.restart_trigger = restart_trigger\n\n    def sobol_sequence(self, n_points, dim):\n        \"\"\"Generate n_points of Sobol sequence in dim dimensions.\"\"\"\n        try:\n            from sobol_seq import i4_sobol_generate\n            points = i4_sobol_generate(dim, n_points)\n            return points\n        except ImportError:\n            print(\"Sobol sequence library not found.  Falling back to uniform random numbers\")\n            return np.random.rand(n_points, dim)\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.best_history = []\n        self.stagnation_counter = 0\n        \n        # Sobol sequence initialization\n        sobol_points = self.sobol_sequence(self.pop_size, self.dim)\n        self.population = func.bounds.lb + sobol_points * (func.bounds.ub - func.bounds.lb)\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n\n        for i in range(self.pop_size):\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.population[i]\n        \n        self.best_history.append(self.f_opt)\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(np.arange(self.pop_size), 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                v = x1 + self.f[i] * (x2 - x3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = np.random.rand(self.dim) < self.cr[i]\n                u = v * u + self.population[i] * (~u)\n                u = np.clip(u, func.bounds.lb, func.bounds.ub)  # Clip after crossover\n\n                # Evaluation\n                f = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.population[i] = u\n                    \n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n                        self.stagnation_counter = 0  # Reset stagnation counter\n                    \n                    # Update success history\n                    self.success_f[i] = self.f[i]\n                    self.success_cr[i] = self.cr[i]\n                    self.success_count[i] += 1\n\n                # Adaptation of F and CR based on success history\n                if self.success_count[i] > 0:\n                    self.f[i] = 0.9 * self.f[i] + 0.1 * np.mean(self.success_f[self.success_count > 0]) if np.any(self.success_count > 0) else np.random.uniform(0.1, 0.9)\n                    self.cr[i] = 0.9 * self.cr[i] + 0.1 * np.mean(self.success_cr[self.success_count > 0]) if np.any(self.success_count > 0) else np.random.uniform(0.1, 0.9)\n                else:\n                    self.f[i] = np.clip(np.random.normal(0.5, 0.3), 0.1, 1.0)\n                    self.cr[i] = np.clip(np.random.normal(0.9, 0.2), 0.1, 1.0)\n                \n                self.f[i] = np.clip(self.f[i], 0.1, 1.0)\n                self.cr[i] = np.clip(self.cr[i], 0.1, 1.0)\n                \n                if self.evals >= self.budget:\n                    break\n            \n            # CMA-ES component, triggered periodically\n            if self.evals < self.budget and self.evals % (self.budget // 5) == 0:\n                self.cma_es(func)\n\n            # Neighborhood Search around best solution, triggered periodically\n            if self.evals < self.budget and np.random.rand() < 0.1: # Probability of performing local search\n                x_neighbor = self.x_opt + np.random.uniform(-self.neighborhood_size, self.neighborhood_size, size=self.dim)\n                x_neighbor = np.clip(x_neighbor, func.bounds.lb, func.bounds.ub)\n                f_neighbor = func(x_neighbor)\n                self.evals += 1\n\n                if f_neighbor < self.f_opt:\n                    self.f_opt = f_neighbor\n                    self.x_opt = x_neighbor\n                    self.stagnation_counter = 0 # Reset stagnation counter\n            \n            self.best_history.append(self.f_opt)\n            if len(self.best_history) > 1:\n                if self.best_history[-1] >= self.best_history[-2]:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0\n\n            # Restart mechanism\n            if self.stagnation_counter > self.budget * self.restart_trigger:\n                self.population = func.bounds.lb + np.random.rand(self.pop_size, self.dim) * (func.bounds.ub - func.bounds.lb)\n                self.fitness = np.array([func(x) for x in self.population])\n                self.evals += self.pop_size\n                self.f_opt = np.min(self.fitness)\n                self.x_opt = self.population[np.argmin(self.fitness)]\n                self.stagnation_counter = 0\n                self.success_f = np.zeros(self.pop_size)\n                self.success_cr = np.zeros(self.pop_size)\n                self.success_count = np.zeros(self.pop_size)\n                print(\"Restarting population...\")\n\n        \n        return self.f_opt, self.x_opt\n\n    def cma_es(self, func):\n        \"\"\"Covariance Matrix Adaptation Evolution Strategy.\"\"\"\n        mean = self.x_opt.copy()\n        sigma = 0.1 * (func.bounds.ub - func.bounds.lb)  # Initial step size\n        C = np.eye(self.dim)  # Initial covariance matrix\n        \n        for _ in range(min(self.cma_generations, (self.budget - self.evals) // self.pop_size)):\n            if self.evals >= self.budget:\n                break\n                \n            # Generate samples from multivariate normal distribution\n            z = np.random.randn(self.pop_size, self.dim)\n            samples = mean + sigma * np.dot(z, sqrtm(C).T)\n            samples = np.clip(samples, func.bounds.lb, func.bounds.ub)  # Clip to bounds\n            \n            # Evaluate samples\n            fitness_values = np.array([func(x) for x in samples])\n            self.evals += self.pop_size\n            \n            # Sort by fitness\n            idx = np.argsort(fitness_values)\n            best_samples = samples[idx]\n            best_fitness = fitness_values[idx]\n            \n            # Update best solution\n            if best_fitness[0] < self.f_opt:\n                self.f_opt = best_fitness[0]\n                self.x_opt = best_samples[0]\n            \n            # Update mean\n            mean = np.mean(best_samples[:self.pop_size//2], axis=0) # Use top 50%\n            \n            # Update covariance matrix (simplified)\n            C = np.cov(best_samples[:self.pop_size//2].T)  # Use top 50%\n\n            # Regularize covariance matrix\n            C = C + 1e-6 * np.eye(self.dim)\n            \n            # Adjust sigma\n            sigma *= np.exp(0.5 * (np.mean(best_fitness) - self.f_opt) / np.std(fitness_values)) # Damping", "configspace": "", "generation": 3, "feedback": "The algorithm HybridDECMASobolV2 scored 0.697 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e2d5689c-6ea5-4289-a346-818243966656"], "operator": null, "metadata": {"aucs": [0.3357023654097736, 0.6077544034596273, 0.700364012263746, 0.8559348917854449, 0.7663154385118085, 0.8039447671925526, 0.564459838637827, 0.645069800925139, 0.7528099943993853, 0.6761526185837765, 0.8670615421450177, 0.9931870198326394, 0.6218368752239434, 0.7298468870534961, 0.9120593068363159, 0.7870280469973041, 0.6337407727104539, 0.8325088335895722, 0.3377183998186283, 0.5208667523443121]}}
{"id": "6cbb1828-c5ca-4b9f-a9e2-98f6cdf65ce1", "fitness": 0.0, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with Simplified Parameter Adaptation and Population Diversity Enhancement via periodic random restarts.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, f=0.5, cr=0.9, restart_interval=500):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.f = f\n        self.cr = cr\n        self.restart_interval = restart_interval\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n\n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.population[best_idx]\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(np.arange(self.pop_size), 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                v = x1 + self.f * (x2 - x3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                u = self.crossover(self.population[i], v)\n\n                # Evaluation\n                f = func(u)\n                self.evals += 1\n\n                # Selection\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.population[i] = u\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n\n                if self.evals >= self.budget:\n                    break\n\n            # Periodic restart\n            if self.evals % self.restart_interval == 0:\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.evals += self.pop_size\n                best_idx = np.argmin(self.fitness)\n                if self.fitness[best_idx] < self.f_opt:\n                    self.f_opt = self.fitness[best_idx]\n                    self.x_opt = self.population[best_idx]\n\n        return self.f_opt, self.x_opt\n\n    def crossover(self, x, v):\n        u = x.copy()\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.cr or j == j_rand:\n                u[j] = v[j]\n        return u", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["fcd8940d-a40e-4be3-977d-2248d7105651"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "eb0ef5e2-3d66-4ba4-bedb-a2806da7c7b9", "fitness": 0.0, "name": "SOSDE", "description": "A self-organizing speciation-based differential evolution algorithm that dynamically adjusts population diversity and speciation radius to enhance exploration and exploitation.", "code": "import numpy as np\n\nclass SOSDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, num_species=5, initial_speciation_radius=0.5, radius_decay=0.99, stagnation_threshold=500):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.num_species = num_species\n        self.speciation_radius = initial_speciation_radius\n        self.radius_decay = radius_decay\n        self.stagnation_threshold = stagnation_threshold\n        self.population = None\n        self.fitness = None\n        self.species = None\n        self.evals = 0\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.last_improvement = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals += self.pop_size\n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.population[best_idx]\n        self.last_improvement = self.evals\n        self.assign_species()\n    \n\n    def assign_species(self):\n        centroids = self.population[np.random.choice(self.pop_size, self.num_species, replace=False)]\n        self.species = np.zeros(self.pop_size, dtype=int)\n        for i in range(self.pop_size):\n            distances = np.linalg.norm(self.population[i] - centroids, axis=1)\n            species_id = np.argmin(distances)\n            self.species[i] = species_id\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            # DE mutation and crossover\n            idxs = np.random.choice(np.arange(self.pop_size), 3, replace=False)\n            x1, x2, x3 = self.population[idxs]\n            F = np.random.uniform(0.3, 0.8)\n            v = x1 + F * (x2 - x3)\n            v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n            CR = np.random.uniform(0.7, 1.0)\n            u = np.random.rand(self.dim) < CR\n            u = v * u + self.population[i] * (~u)\n\n            # Evaluate offspring\n            f_new = func(u)\n            self.evals += 1\n\n            # Selection\n            if f_new < self.fitness[i]:\n                self.fitness[i] = f_new\n                self.population[i] = u\n                \n                if f_new < self.f_opt:\n                    self.f_opt = f_new\n                    self.x_opt = u\n                    self.last_improvement = self.evals\n\n    def adjust_speciation_radius(self):\n         self.speciation_radius *= self.radius_decay\n\n    def redistribute_species(self, func):\n        # Identify underperforming species\n        species_fitness = np.array([np.mean(self.fitness[self.species == i]) for i in range(self.num_species)])\n        worst_species = np.argmax(species_fitness)\n\n        # Redistribute individuals from worst species\n        worst_species_indices = np.where(self.species == worst_species)[0]\n        num_redistribute = len(worst_species_indices) // 2 # Redistribute half of the individuals\n\n        for i in range(num_redistribute):\n            idx = worst_species_indices[i]\n            self.population[idx] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n            self.fitness[idx] = func(self.population[idx])\n            self.evals += 1\n\n        self.assign_species()\n\n    def check_stagnation(self, func):\n        if self.evals - self.last_improvement > self.stagnation_threshold:\n            self.redistribute_species(func)\n            self.assign_species()\n            self.adjust_speciation_radius()\n            return True\n        return False\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.evals < self.budget:\n            self.evolve(func)\n            \n            if self.check_stagnation(func):\n                continue\n\n            if self.evals >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm SOSDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["00f88f83-119a-4cb1-8eec-4370c9b14fb6"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "13fb0e4c-8c61-4e41-b03a-20e47fd0332f", "fitness": 0.0, "name": "AdaptiveDE_OL_HoF", "description": "Adaptive Differential Evolution with orthogonal learning and a diversity maintenance strategy using a Hall of Fame.", "code": "import numpy as np\n\nclass AdaptiveDE_OL_HoF:\n    def __init__(self, budget=10000, dim=10, pop_size=50, f=0.5, cr=0.9, hof_size=5, restart_trigger=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.f = f\n        self.cr = cr\n        self.hof_size = hof_size\n        self.restart_trigger = restart_trigger\n        self.hof = []  # Hall of Fame to store diverse and good solutions\n        self.hof_fitness = []\n        self.success_history_f = []\n        self.success_history_cr = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n\n        for i in range(self.pop_size):\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.population[i]\n\n        # Initialize Hall of Fame\n        idxs = np.argsort(self.fitness)[:self.hof_size]\n        self.hof = self.population[idxs].copy()\n        self.hof_fitness = self.fitness[idxs].copy()\n        \n        while self.evals < self.budget:\n            # Restart strategy\n            if self.evals > self.restart_trigger * self.budget:\n                if np.std(self.fitness) < 1e-6:  # Stagnation check\n                    self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                    self.fitness = np.array([func(x) for x in self.population])\n                    self.evals += self.pop_size\n                    for j in range(self.pop_size):\n                        if self.fitness[j] < self.f_opt:\n                            self.f_opt = self.fitness[j]\n                            self.x_opt = self.population[j]\n                    \n                    # Re-initialize Hall of Fame\n                    idxs = np.argsort(self.fitness)[:self.hof_size]\n                    self.hof = self.population[idxs].copy()\n                    self.hof_fitness = self.fitness[idxs].copy()\n                    \n                    continue\n\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(np.arange(self.pop_size), 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                \n                # Incorporate Hall of Fame influence\n                hof_idx = np.random.randint(self.hof_size)\n                v = x1 + self.f * (x2 - x3) + self.f * (self.hof[hof_idx] - self.population[i]) # Hall of Fame\n                \n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = self.crossover(self.population[i], v)\n\n                # Orthogonal Learning\n                u = self.orthogonal_learning(u, func)\n                \n                # Evaluation\n                f = func(u)\n                self.evals += 1\n\n                # Selection\n                if f < self.fitness[i]:\n                    # Adaptive Parameter Control\n                    self.success_history_f.append(self.f)\n                    self.success_history_cr.append(self.cr)\n                    \n                    self.fitness[i] = f\n                    self.population[i] = u\n                    \n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n                        \n                    #Update F and CR\n                    self.f = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n                    self.cr = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0)\n                    \n                    # Update Hall of Fame\n                    if f < np.max(self.hof_fitness):\n                        worst_idx = np.argmax(self.hof_fitness)\n                        self.hof[worst_idx] = u\n                        self.hof_fitness[worst_idx] = f\n                        \n                if self.evals >= self.budget:\n                    break\n        return self.f_opt, self.x_opt\n\n    def crossover(self, x, v):\n        u = x.copy()\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.cr or j == j_rand:\n                u[j] = v[j]\n        return u\n    \n    def orthogonal_learning(self, x, func, ol_samples=5):\n        # Orthogonal learning to refine the solution\n        best_f = func(x)\n        best_x = x.copy()\n        \n        for _ in range(ol_samples):\n            # Generate a random perturbation around x\n            perturbation = np.random.normal(0, 0.05, self.dim)  # Smaller perturbation\n            x_new = x + perturbation\n            x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n            \n            f_new = func(x_new)\n            self.evals += 1\n            \n            if f_new < best_f:\n                best_f = f_new\n                best_x = x_new.copy()\n                \n            if self.evals >= self.budget:\n                break\n        \n        return best_x", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDE_OL_HoF scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["24da9b3b-348f-4ec1-b7f6-55bc26a87d9c"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "83f6eaf3-8568-4a21-9c23-fac144dda869", "fitness": 0.0, "name": "PopCMAES", "description": "Covariance matrix adaptation evolution strategy with a population-based approach, incorporating a dynamic population size adjustment and a restart strategy based on stagnation detection.", "code": "import numpy as np\n\nclass PopCMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, init_sigma=0.5, stagnation_threshold=1000):\n        self.budget = budget\n        self.dim = dim\n        self.init_pop_size = 4 + int(3 * np.log(dim)) if pop_size is None else pop_size  # Initial population size\n        self.pop_size = self.init_pop_size\n        self.init_sigma = init_sigma\n        self.stagnation_threshold = stagnation_threshold\n        self.mu = self.pop_size // 2  # Number of parents\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.m = None  # Mean\n        self.C = None  # Covariance matrix\n        self.sigma = init_sigma  # Step size\n        self.pc = None  # Evolution path for C\n        self.ps = None  # Evolution path for sigma\n        self.chiN = dim**0.5 * (1 - 1/(4*dim) + 1/(21*dim**2))  # Expectation of ||N(0,I)||\n        self.c_sigma = (self.mu / (dim + self.mu))**0.5\n        self.c_c = (4 + self.mu/dim) / (dim + 4 + 2*self.mu/dim)\n        self.d_sigma = 1 + 2*max(0, ((self.mu-1)/(dim+1) - 1)) + self.c_sigma\n        self.c_1 = 2 / ((dim + 1.3)**2 + self.mu)\n        self.c_mu = min(1 - self.c_1, 2 * (self.mu - 2 + 1/self.mu) / ((dim + 2)**2 + self.mu))\n        self.D = None\n        self.B = None\n        self.evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.last_improvement = 0\n        self.best_fitness_history = []\n\n    def initialize(self, func):\n        self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.D = np.ones(self.dim)\n        self.B = np.eye(self.dim)\n\n    def sample_population(self):\n        z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n        y = np.dot(z, np.diag(self.D).dot(self.B.T))\n        x = self.m + self.sigma * y\n        return x, z\n\n    def __call__(self, func):\n        self.initialize(func)\n\n        while self.evals < self.budget:\n            x, z = self.sample_population()\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            fitness = np.array([func(xi) for xi in x])\n            self.evals += self.pop_size\n\n            idx = np.argsort(fitness)\n            fitness = fitness[idx]\n            x = x[idx]\n            z = z[idx]\n\n            if fitness[0] < self.f_opt:\n                self.f_opt = fitness[0]\n                self.x_opt = x[0]\n                self.last_improvement = self.evals\n                self.best_fitness_history.append(self.f_opt)\n                \n            z_mean = np.sum(z[:self.mu].T * self.weights, axis=1)\n            y_mean = np.dot(z_mean, np.diag(self.D).dot(self.B.T))\n\n            self.ps = (1 - self.c_sigma) * self.ps + self.c_sigma**0.5 * (self.B @ self.D @ z_mean)\n            self.sigma *= np.exp((self.c_sigma / self.d_sigma) * (np.linalg.norm(self.ps)/self.chiN - 1))\n            self.pc = (1 - self.c_c) * self.pc + self.c_c**0.5 * y_mean\n            \n            delta = x[:self.mu] - self.m\n            self.m += np.sum((delta.T * self.weights).T, axis=0)\n\n            C_temp = self.c_1 * (self.pc[:, None] @ self.pc[None, :])\n            for i in range(self.mu):\n                C_temp += self.c_mu * self.weights[i] * (delta[i, :, None] @ delta[i, None, :]) / (self.sigma**2)\n            self.C = (1 - self.c_1 - self.c_mu) * self.C + C_temp\n\n            try:\n                self.D, self.B = np.linalg.eigh(self.C)\n                self.D = np.sqrt(np.abs(self.D))\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)\n                self.D = np.ones(self.dim)\n                self.B = np.eye(self.dim)\n\n            if self.evals - self.last_improvement > self.stagnation_threshold:\n                 # Dynamic Population Size Adjustment\n                if self.pop_size > self.init_pop_size / 2:\n                   self.pop_size = max(int(self.pop_size * 0.8), self.init_pop_size // 2) # Reduce population size\n                else:\n                   self.pop_size = self.init_pop_size  # Reset population size\n                   self.sigma = self.init_sigma\n\n                self.initialize(func)\n                self.last_improvement = self.evals\n                \n            if self.evals >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm PopCMAES scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["00f88f83-119a-4cb1-8eec-4370c9b14fb6"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "645a5c45-b868-4268-bdab-fa8df8154ec7", "fitness": -Infinity, "name": "HybridDECMASobolV3", "description": "Simplified hybrid DE with CMA-ES and adaptive parameter control based on success rates, focusing on efficient adaptation and reduced complexity by removing the local search and Sobol initialization.", "code": "import numpy as np\nfrom scipy.linalg import sqrtm\n\nclass HybridDECMASobolV3:\n    def __init__(self, budget=10000, dim=10, pop_size=50, f_init=0.5, cr_init=0.9, cma_generations=5, restart_trigger=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.f = np.full(pop_size, f_init)  # Individual mutation factors\n        self.cr = np.full(pop_size, cr_init) # Individual crossover rates\n        self.cma_generations = cma_generations\n        self.learning_rate = 0.1  # Adjusts the adaptation rate\n        self.cma_evals = 0\n        self.success_f = np.zeros(pop_size)\n        self.success_cr = np.zeros(pop_size)\n        self.success_count = np.zeros(pop_size)\n        self.restart_trigger = restart_trigger\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.best_history = []\n        self.stagnation_counter = 0\n        \n        # Random initialization\n        self.population = func.bounds.lb + np.random.rand(self.pop_size, self.dim) * (func.bounds.ub - func.bounds.lb)\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n\n        for i in range(self.pop_size):\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.population[i]\n        \n        self.best_history.append(self.f_opt)\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(np.arange(self.pop_size), 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                v = x1 + self.f[i] * (x2 - x3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = np.random.rand(self.dim) < self.cr[i]\n                u = v * u + self.population[i] * (~u)\n                u = np.clip(u, func.bounds.lb, func.bounds.ub)  # Clip after crossover\n\n                # Evaluation\n                f = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.population[i] = u\n                    \n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n                        self.stagnation_counter = 0  # Reset stagnation counter\n                    \n                    # Update success history\n                    self.success_f[i] = self.f[i]\n                    self.success_cr[i] = self.cr[i]\n                    self.success_count[i] += 1\n\n                # Adaptation of F and CR based on success history (simplified)\n                if self.success_count[i] > 0:\n                    self.f[i] = 0.8 * self.f[i] + 0.2 * np.mean(self.success_f[self.success_count > 0]) if np.any(self.success_count > 0) else np.random.uniform(0.1, 0.9)\n                    self.cr[i] = 0.8 * self.cr[i] + 0.2 * np.mean(self.success_cr[self.success_count > 0]) if np.any(self.success_count > 0) else np.random.uniform(0.1, 0.9)\n                else:\n                    self.f[i] = np.clip(np.random.normal(0.5, 0.3), 0.1, 1.0)\n                    self.cr[i] = np.clip(np.random.normal(0.9, 0.2), 0.1, 1.0)\n                \n                self.f[i] = np.clip(self.f[i], 0.1, 1.0)\n                self.cr[i] = np.clip(self.cr[i], 0.1, 1.0)\n                \n                if self.evals >= self.budget:\n                    break\n            \n            # CMA-ES component, triggered periodically\n            if self.evals < self.budget and self.evals % (self.budget // 5) == 0:\n                self.cma_es(func)\n            \n            self.best_history.append(self.f_opt)\n            if len(self.best_history) > 1:\n                if self.best_history[-1] >= self.best_history[-2]:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0\n\n            # Restart mechanism\n            if self.stagnation_counter > self.budget * self.restart_trigger:\n                self.population = func.bounds.lb + np.random.rand(self.pop_size, self.dim) * (func.bounds.ub - func.bounds.lb)\n                self.fitness = np.array([func(x) for x in self.population])\n                self.evals += self.pop_size\n                self.f_opt = np.min(self.fitness)\n                self.x_opt = self.population[np.argmin(self.fitness)]\n                self.stagnation_counter = 0\n                self.success_f = np.zeros(self.pop_size)\n                self.success_cr = np.zeros(self.pop_size)\n                self.success_count = np.zeros(self.pop_size)\n                #print(\"Restarting population...\")\n\n        \n        return self.f_opt, self.x_opt\n\n    def cma_es(self, func):\n        \"\"\"Covariance Matrix Adaptation Evolution Strategy.\"\"\"\n        mean = self.x_opt.copy()\n        sigma = 0.1 * (func.bounds.ub - func.bounds.lb)  # Initial step size\n        C = np.eye(self.dim)  # Initial covariance matrix\n        \n        for _ in range(min(self.cma_generations, (self.budget - self.evals) // self.pop_size)):\n            if self.evals >= self.budget:\n                break\n                \n            # Generate samples from multivariate normal distribution\n            z = np.random.randn(self.pop_size, self.dim)\n            samples = mean + sigma * np.dot(z, sqrtm(C).T)\n            samples = np.clip(samples, func.bounds.lb, func.bounds.ub)  # Clip to bounds\n            \n            # Evaluate samples\n            fitness_values = np.array([func(x) for x in self.population]) # BUG, was samples. Changed back to the overall population\n            self.evals += self.pop_size\n            \n            # Sort by fitness\n            idx = np.argsort(fitness_values)\n            best_samples = samples[idx]\n            best_fitness = fitness_values[idx]\n            \n            # Update best solution\n            if best_fitness[0] < self.f_opt:\n                self.f_opt = best_fitness[0]\n                self.x_opt = best_samples[0]\n            \n            # Update mean\n            mean = np.mean(best_samples[:self.pop_size//2], axis=0) # Use top 50%\n            \n            # Update covariance matrix (simplified)\n            C = np.cov(best_samples[:self.pop_size//2].T)  # Use top 50%\n\n            # Regularize covariance matrix\n            C = C + 1e-6 * np.eye(self.dim)\n            \n            # Adjust sigma\n            sigma *= np.exp(0.5 * (np.mean(best_fitness) - self.f_opt) / np.std(fitness_values)) # Damping", "configspace": "", "generation": 4, "feedback": "An exception occurred: name 'sqrtm' is not defined.", "error": "", "parent_ids": ["77b25529-87c5-43b2-9417-0cde1ade20a1"], "operator": null, "metadata": {}}
{"id": "6441c85e-1240-4823-b812-0d638e5f439d", "fitness": -Infinity, "name": "HybridDECMASobolV3", "description": "Enhanced Hybrid DE with CMA-ES and Sobol initialization, featuring adaptive parameter control, local search, and a CMA-ES restart mechanism to improve convergence and exploration, utilizing orthogonal sampling for better population diversity.", "code": "import numpy as np\nfrom scipy.linalg import sqrtm\n\nclass HybridDECMASobolV3:\n    def __init__(self, budget=10000, dim=10, pop_size=50, f_init=0.5, cr_init=0.9, neighborhood_size=0.1, cma_generations=5, restart_trigger=0.2, orthogonal_levels=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.f = np.full(pop_size, f_init)  # Individual mutation factors\n        self.cr = np.full(pop_size, cr_init) # Individual crossover rates\n        self.neighborhood_size = neighborhood_size\n        self.cma_generations = cma_generations\n        self.learning_rate = 0.1  # Adjusts the adaptation rate\n        self.cma_evals = 0\n        self.success_f = np.zeros(pop_size)\n        self.success_cr = np.zeros(pop_size)\n        self.success_count = np.zeros(pop_size)\n        self.restart_trigger = restart_trigger\n        self.orthogonal_levels = orthogonal_levels\n\n    def sobol_sequence(self, n_points, dim):\n        \"\"\"Generate n_points of Sobol sequence in dim dimensions.\"\"\"\n        try:\n            from sobol_seq import i4_sobol_generate\n            points = i4_sobol_generate(dim, n_points)\n            return points\n        except ImportError:\n            print(\"Sobol sequence library not found.  Falling back to uniform random numbers\")\n            return np.random.rand(n_points, dim)\n\n    def orthogonal_design(self, levels, dim, n_samples):\n        \"\"\"Generate an orthogonal design using Latin Hypercube Sampling.\"\"\"\n        from smt.sampling_methods import LHS\n        sampling = LHS(xlimits=np.array([[0, 1]] * dim), criterion='maximin')\n        return sampling(n_samples)\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.best_history = []\n        self.stagnation_counter = 0\n        \n        # Orthogonal design initialization\n        orthogonal_points = self.orthogonal_design(self.orthogonal_levels, self.dim, self.pop_size)\n        self.population = func.bounds.lb + orthogonal_points * (func.bounds.ub - func.bounds.lb)\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n\n        for i in range(self.pop_size):\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.population[i]\n        \n        self.best_history.append(self.f_opt)\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(np.arange(self.pop_size), 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                v = x1 + self.f[i] * (x2 - x3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = np.random.rand(self.dim) < self.cr[i]\n                u = v * u + self.population[i] * (~u)\n                u = np.clip(u, func.bounds.lb, func.bounds.ub)  # Clip after crossover\n\n                # Evaluation\n                f = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.population[i] = u\n                    \n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n                        self.stagnation_counter = 0  # Reset stagnation counter\n                    \n                    # Update success history\n                    self.success_f[i] = self.f[i]\n                    self.success_cr[i] = self.cr[i]\n                    self.success_count[i] += 1\n\n                # Adaptation of F and CR based on success history\n                if self.success_count[i] > 0:\n                    self.f[i] = 0.9 * self.f[i] + 0.1 * np.mean(self.success_f[self.success_count > 0]) if np.any(self.success_count > 0) else np.random.uniform(0.1, 0.9)\n                    self.cr[i] = 0.9 * self.cr[i] + 0.1 * np.mean(self.success_cr[self.success_count > 0]) if np.any(self.success_count > 0) else np.random.uniform(0.1, 0.9)\n                else:\n                    self.f[i] = np.clip(np.random.normal(0.5, 0.3), 0.1, 1.0)\n                    self.cr[i] = np.clip(np.random.normal(0.9, 0.2), 0.1, 1.0)\n                \n                self.f[i] = np.clip(self.f[i], 0.1, 1.0)\n                self.cr[i] = np.clip(self.cr[i], 0.1, 1.0)\n                \n                if self.evals >= self.budget:\n                    break\n            \n            # CMA-ES component, triggered periodically and adapt sigma\n            if self.evals < self.budget and self.evals % (self.budget // 5) == 0:\n                sigma_scale = np.std(self.fitness) / (func.bounds.ub - func.bounds.lb)\n                self.cma_es(func, sigma_scale)\n\n            # Neighborhood Search around best solution, triggered periodically\n            if self.evals < self.budget and np.random.rand() < 0.1: # Probability of performing local search\n                x_neighbor = self.x_opt + np.random.uniform(-self.neighborhood_size, self.neighborhood_size, size=self.dim)\n                x_neighbor = np.clip(x_neighbor, func.bounds.lb, func.bounds.ub)\n                f_neighbor = func(x_neighbor)\n                self.evals += 1\n\n                if f_neighbor < self.f_opt:\n                    self.f_opt = f_neighbor\n                    self.x_opt = x_neighbor\n                    self.stagnation_counter = 0 # Reset stagnation counter\n            \n            self.best_history.append(self.f_opt)\n            if len(self.best_history) > 1:\n                if self.best_history[-1] >= self.best_history[-2]:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0\n\n            # Restart mechanism\n            if self.stagnation_counter > self.budget * self.restart_trigger:\n                orthogonal_points = self.orthogonal_design(self.orthogonal_levels, self.dim, self.pop_size)\n                self.population = func.bounds.lb + orthogonal_points * (func.bounds.ub - func.bounds.lb)\n                self.fitness = np.array([func(x) for x in self.population])\n                self.evals += self.pop_size\n                self.f_opt = np.min(self.fitness)\n                self.x_opt = self.population[np.argmin(self.fitness)]\n                self.stagnation_counter = 0\n                self.success_f = np.zeros(self.pop_size)\n                self.success_cr = np.zeros(self.pop_size)\n                self.success_count = np.zeros(self.pop_size)\n                print(\"Restarting population...\")\n\n        \n        return self.f_opt, self.x_opt\n\n    def cma_es(self, func, sigma_scale):\n        \"\"\"Covariance Matrix Adaptation Evolution Strategy.\"\"\"\n        mean = self.x_opt.copy()\n        sigma = sigma_scale * (func.bounds.ub - func.bounds.lb)  # Scaled Initial step size\n        C = np.eye(self.dim)  # Initial covariance matrix\n        \n        for _ in range(min(self.cma_generations, (self.budget - self.evals) // self.pop_size)):\n            if self.evals >= self.budget:\n                break\n                \n            # Generate samples from multivariate normal distribution\n            z = np.random.randn(self.pop_size, self.dim)\n            samples = mean + sigma * np.dot(z, sqrtm(C).T)\n            samples = np.clip(samples, func.bounds.lb, func.bounds.ub)  # Clip to bounds\n            \n            # Evaluate samples\n            fitness_values = np.array([func(x) for x in samples])\n            self.evals += self.pop_size\n            \n            # Sort by fitness\n            idx = np.argsort(fitness_values)\n            best_samples = samples[idx]\n            best_fitness = fitness_values[idx]\n            \n            # Update best solution\n            if best_fitness[0] < self.f_opt:\n                self.f_opt = best_fitness[0]\n                self.x_opt = best_samples[0]\n            \n            # Update mean\n            mean = np.mean(best_samples[:self.pop_size//2], axis=0) # Use top 50%\n            \n            # Update covariance matrix (simplified)\n            C = np.cov(best_samples[:self.pop_size//2].T)  # Use top 50%\n\n            # Regularize covariance matrix\n            C = C + 1e-6 * np.eye(self.dim)\n            \n            # Adjust sigma\n            sigma *= np.exp(0.5 * (np.mean(best_fitness) - self.f_opt) / np.std(fitness_values)) # Damping\n            sigma = np.clip(sigma, 1e-6, (func.bounds.ub-func.bounds.lb))\n\n            if np.isnan(sigma):\n                sigma = sigma_scale * (func.bounds.ub - func.bounds.lb)\n", "configspace": "", "generation": 4, "feedback": "An exception occurred: No module named 'smt'.", "error": "", "parent_ids": ["77b25529-87c5-43b2-9417-0cde1ade20a1"], "operator": null, "metadata": {}}
{"id": "243a9555-d632-46d6-9b11-b51bfcbbbd53", "fitness": 0.0, "name": "HybridDECMASobolV3", "description": "Enhanced Hybrid DE with CMA-ES and adaptive local search, incorporating a dynamic restart mechanism and jitter-based diversity injection.", "code": "import numpy as np\nfrom scipy.linalg import sqrtm\n\nclass HybridDECMASobolV3:\n    def __init__(self, budget=10000, dim=10, pop_size=50, f_init=0.5, cr_init=0.9, neighborhood_size=0.1, cma_generations=5, restart_trigger=0.2, jitter_prob=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.f = np.full(pop_size, f_init)  # Individual mutation factors\n        self.cr = np.full(pop_size, cr_init) # Individual crossover rates\n        self.neighborhood_size = neighborhood_size\n        self.cma_generations = cma_generations\n        self.learning_rate = 0.1  # Adjusts the adaptation rate\n        self.cma_evals = 0\n        self.success_f = np.zeros(pop_size)\n        self.success_cr = np.zeros(pop_size)\n        self.success_count = np.zeros(pop_size)\n        self.restart_trigger = restart_trigger\n        self.jitter_prob = jitter_prob\n\n    def sobol_sequence(self, n_points, dim):\n        \"\"\"Generate n_points of Sobol sequence in dim dimensions.\"\"\"\n        try:\n            from sobol_seq import i4_sobol_generate\n            points = i4_sobol_generate(dim, n_points)\n            return points\n        except ImportError:\n            print(\"Sobol sequence library not found.  Falling back to uniform random numbers\")\n            return np.random.rand(n_points, dim)\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.best_history = []\n        self.stagnation_counter = 0\n        \n        # Sobol sequence initialization\n        sobol_points = self.sobol_sequence(self.pop_size, self.dim)\n        self.population = func.bounds.lb + sobol_points * (func.bounds.ub - func.bounds.lb)\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n\n        for i in range(self.pop_size):\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.population[i]\n        \n        self.best_history.append(self.f_opt)\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(np.arange(self.pop_size), 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                v = x1 + self.f[i] * (x2 - x3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = np.random.rand(self.dim) < self.cr[i]\n                u = v * u + self.population[i] * (~u)\n                u = np.clip(u, func.bounds.lb, func.bounds.ub)  # Clip after crossover\n\n                # Evaluation\n                f = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.population[i] = u\n                    \n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n                        self.stagnation_counter = 0  # Reset stagnation counter\n                    \n                    # Update success history\n                    self.success_f[i] = self.f[i]\n                    self.success_cr[i] = self.cr[i]\n                    self.success_count[i] += 1\n\n                # Adaptation of F and CR based on success history\n                if self.success_count[i] > 0:\n                    self.f[i] = 0.9 * self.f[i] + 0.1 * np.mean(self.success_f[self.success_count > 0]) if np.any(self.success_count > 0) else np.random.uniform(0.1, 0.9)\n                    self.cr[i] = 0.9 * self.cr[i] + 0.1 * np.mean(self.success_cr[self.success_count > 0]) if np.any(self.success_count > 0) else np.random.uniform(0.1, 0.9)\n                else:\n                    self.f[i] = np.clip(np.random.normal(0.5, 0.3), 0.1, 1.0)\n                    self.cr[i] = np.clip(np.random.normal(0.9, 0.2), 0.1, 1.0)\n                \n                self.f[i] = np.clip(self.f[i], 0.1, 1.0)\n                self.cr[i] = np.clip(self.cr[i], 0.1, 1.0)\n                \n                if self.evals >= self.budget:\n                    break\n            \n            # CMA-ES component, triggered periodically\n            if self.evals < self.budget and self.evals % (self.budget // 5) == 0:\n                self.cma_es(func)\n\n            # Adaptive Neighborhood Search around best solution\n            if self.evals < self.budget:\n                # Adjust neighborhood size based on stagnation\n                adaptive_neighborhood_size = self.neighborhood_size * (1 + self.stagnation_counter / (self.budget * self.restart_trigger))\n                x_neighbor = self.x_opt + np.random.uniform(-adaptive_neighborhood_size, adaptive_neighborhood_size, size=self.dim)\n                x_neighbor = np.clip(x_neighbor, func.bounds.lb, func.bounds.ub)\n                f_neighbor = func(x_neighbor)\n                self.evals += 1\n\n                if f_neighbor < self.f_opt:\n                    self.f_opt = f_neighbor\n                    self.x_opt = x_neighbor\n                    self.stagnation_counter = 0 # Reset stagnation counter\n            \n            # Jittering: Introduce small random changes to diversify the population\n            for i in range(self.pop_size):\n                if np.random.rand() < self.jitter_prob:\n                    jitter = np.random.uniform(-0.05, 0.05, self.dim) * (func.bounds.ub - func.bounds.lb) # Small percentage of the range\n                    self.population[i] = np.clip(self.population[i] + jitter, func.bounds.lb, func.bounds.ub)\n                    self.fitness[i] = func(self.population[i])\n                    self.evals += 1\n                    if self.fitness[i] < self.f_opt:\n                        self.f_opt = self.fitness[i]\n                        self.x_opt = self.population[i]\n\n            self.best_history.append(self.f_opt)\n            if len(self.best_history) > 1:\n                if self.best_history[-1] >= self.best_history[-2]:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0\n\n            # Dynamic Restart mechanism\n            if self.stagnation_counter > self.budget * self.restart_trigger:\n                self.population = func.bounds.lb + np.random.rand(self.pop_size, self.dim) * (func.bounds.ub - func.bounds.lb)\n                self.fitness = np.array([func(x) for x in self.population])\n                self.evals += self.pop_size\n                self.f_opt = np.min(self.fitness)\n                self.x_opt = self.population[np.argmin(self.fitness)]\n                self.stagnation_counter = 0\n                self.success_f = np.zeros(self.pop_size)\n                self.success_cr = np.zeros(self.pop_size)\n                self.success_count = np.zeros(self.pop_size)\n                print(\"Restarting population...\")\n\n        \n        return self.f_opt, self.x_opt\n\n    def cma_es(self, func):\n        \"\"\"Covariance Matrix Adaptation Evolution Strategy.\"\"\"\n        mean = self.x_opt.copy()\n        sigma = 0.1 * (func.bounds.ub - func.bounds.lb)  # Initial step size\n        C = np.eye(self.dim)  # Initial covariance matrix\n        \n        for _ in range(min(self.cma_generations, (self.budget - self.evals) // self.pop_size)):\n            if self.evals >= self.budget:\n                break\n                \n            # Generate samples from multivariate normal distribution\n            z = np.random.randn(self.pop_size, self.dim)\n            samples = mean + sigma * np.dot(z, sqrtm(C).T)\n            samples = np.clip(samples, func.bounds.lb, func.bounds.ub)  # Clip to bounds\n            \n            # Evaluate samples\n            fitness_values = np.array([func(x) for x in samples])\n            self.evals += self.pop_size\n            \n            # Sort by fitness\n            idx = np.argsort(fitness_values)\n            best_samples = samples[idx]\n            best_fitness = fitness_values[idx]\n            \n            # Update best solution\n            if best_fitness[0] < self.f_opt:\n                self.f_opt = best_fitness[0]\n                self.x_opt = best_samples[0]\n            \n            # Update mean\n            mean = np.mean(best_samples[:self.pop_size//2], axis=0) # Use top 50%\n            \n            # Update covariance matrix (simplified)\n            C = np.cov(best_samples[:self.pop_size//2].T)  # Use top 50%\n\n            # Regularize covariance matrix\n            C = C + 1e-6 * np.eye(self.dim)\n            \n            # Adjust sigma\n            sigma *= np.exp(0.5 * (np.mean(best_fitness) - self.f_opt) / np.std(fitness_values)) # Damping", "configspace": "", "generation": 4, "feedback": "The algorithm HybridDECMASobolV3 scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["77b25529-87c5-43b2-9417-0cde1ade20a1"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "e80c9b07-9e4a-47ea-b07f-4479fdb1d3e3", "fitness": 0.6721632016257091, "name": "SimplifiedAdaptiveDE", "description": "Simplified Adaptive DE with a single F/CR pair, adaptive learning rate, and stagnation-based restart.", "code": "import numpy as np\n\nclass SimplifiedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, adapt_freq=50, stagnation_threshold=1000, learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.adapt_freq = adapt_freq\n        self.stagnation_threshold = stagnation_threshold\n        self.learning_rate = learning_rate\n        self.f = 0.5\n        self.cr = 0.9\n        self.success_f = []\n        self.success_cr = []\n        self.best_fitness_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.last_improvement = 0\n\n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.population[best_idx]\n        self.best_fitness_history.append(self.f_opt)\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(np.arange(self.pop_size), 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                v = x1 + self.f * (x2 - x3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = np.random.rand(self.dim) < self.cr\n                u = v * u + self.population[i] * (~u)\n                \n                # Evaluation\n                f_new = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f_new < self.fitness[i]:\n                    self.success_f.append(self.f)\n                    self.success_cr.append(self.cr)\n                    self.fitness[i] = f_new\n                    self.population[i] = u\n\n                    if f_new < self.f_opt:\n                        self.f_opt = f_new\n                        self.x_opt = u\n                        self.best_fitness_history.append(self.f_opt)\n                        self.last_improvement = self.evals\n\n            # Adaptive Parameter Control\n            if self.evals % self.adapt_freq == 0:\n                if self.success_f:\n                    self.f = (1 - self.learning_rate) * self.f + self.learning_rate * np.mean(self.success_f)\n                    self.cr = (1 - self.learning_rate) * self.cr + self.learning_rate * np.mean(self.success_cr)\n                self.f = np.clip(self.f, 0.1, 0.9)\n                self.cr = np.clip(self.cr, 0.1, 1.0)\n                self.success_f = []\n                self.success_cr = []\n            \n            # Stagnation Check and Restart\n            if self.evals - self.last_improvement > self.stagnation_threshold:\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.evals += self.pop_size\n                best_idx = np.argmin(self.fitness)\n                self.f_opt = self.fitness[best_idx]\n                self.x_opt = self.population[best_idx]\n                self.last_improvement = self.evals\n                self.best_fitness_history.append(self.f_opt)\n\n\n            if self.evals >= self.budget:\n                break\n                    \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm SimplifiedAdaptiveDE scored 0.672 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["00f88f83-119a-4cb1-8eec-4370c9b14fb6"], "operator": null, "metadata": {"aucs": [0.23375424415634594, 0.6877387864241408, 0.6755126554063282, 0.8746300904883083, 0.7147553252857783, 0.7646812522590309, 0.6244522389859128, 0.6566242801563789, 0.7323106631049023, 0.2693694435579772, 0.8758839939685477, 0.9960915617657737, 0.6507112677406552, 0.7379643799909754, 0.9209069227273202, 0.7474524039627006, 0.6200151528855671, 0.8490718484017187, 0.2901014861906921, 0.521236035055127]}}
{"id": "227caa0a-bdd6-4fd7-aa48-e3783b0c999d", "fitness": 0.46659713470957376, "name": "AdaptiveDE", "description": "Simplified Adaptive DE with combined archive and rank-based mutation, focusing on more frequent parameter adaptation and archive updates.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, f=0.5, cr=0.9, restart_trigger=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.f = f\n        self.cr = cr\n        self.restart_trigger = restart_trigger\n        self.archive = []\n        self.archive_fitness = []\n        self.sf_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.population[best_idx]\n        \n        while self.evals < self.budget:\n            # Restart strategy\n            if self.evals > self.restart_trigger * self.budget:\n                if np.std(self.fitness) < 1e-6:\n                    self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                    self.fitness = np.array([func(x) for x in self.population])\n                    self.evals += self.pop_size\n                    best_idx = np.argmin(self.fitness)\n                    if self.fitness[best_idx] < self.f_opt:\n                        self.f_opt = self.fitness[best_idx]\n                        self.x_opt = self.population[best_idx]\n                    continue\n            \n            ranked_indices = np.argsort(self.fitness) # Rank individuals\n            \n            for i in range(self.pop_size):\n                # Mutation: Rank-based selection\n                best_idx = ranked_indices[0]  # Best individual\n                worst_idx = ranked_indices[-1] # Worst individual\n\n                idxs = np.random.choice(np.arange(self.pop_size), 2, replace=False)\n                x1, x2 = self.population[idxs]\n                \n                v = self.population[best_idx] + self.f * (x1 - x2)\n\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = self.crossover(self.population[i], v)\n                \n                # Evaluation\n                f = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f < self.fitness[i]:\n                    # Archive update\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i])\n                        self.archive_fitness.append(self.fitness[i])\n                    else:\n                        worst_archive_idx = np.argmax(self.archive_fitness)\n                        if self.fitness[i] < self.archive_fitness[worst_archive_idx]:\n                            self.archive[worst_archive_idx] = self.population[i]\n                            self.archive_fitness[worst_archive_idx] = self.fitness[i]\n\n                    #Adaptive F update based on success - simplified update\n                    if f < self.fitness[i]:\n                        self.sf_history.append(abs(self.fitness[i] - f))\n                        if len(self.sf_history) > 50:\n                            self.sf_history.pop(0)\n\n                    self.fitness[i] = f\n                    self.population[i] = u\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n                \n                # Adaptive Parameter Control (simplified and more frequent)\n                if np.random.rand() < 0.3:  # Increased frequency\n                    if self.sf_history:\n                        self.f = np.clip(np.random.normal(0.5, np.std(self.sf_history)), 0.1, 1.0)\n                    else:\n                        self.f = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0) \n                    self.cr = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0)\n\n                if self.evals >= self.budget:\n                    break\n        return self.f_opt, self.x_opt\n        \n    def crossover(self, x, v):\n        u = x.copy()\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.cr or j == j_rand:\n                u[j] = v[j]\n        return u", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDE scored 0.467 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["fcd8940d-a40e-4be3-977d-2248d7105651"], "operator": null, "metadata": {"aucs": [0.1968634051400283, 0.24707027207484267, 0.7829012712752997, 0.34282736905696165, 0.2788565922110471, 0.6051267024867585, 0.2779536930966403, 0.45334339563040593, 0.7775834831691816, 0.16113527686712836, 0.8364162092610526, 0.9988175502591826, 0.299257116148847, 0.26629989410775456, 0.8716260348322398, 0.38378808866873426, 0.27560395632329626, 0.5391222642360789, 0.24902954438866365, 0.4883205749573316]}}
{"id": "c34149bf-cb50-43d4-a989-f869a67149fd", "fitness": 0.6931410602700907, "name": "HybridDECMASobolV3", "description": "Improved Hybrid DE with CMA-ES and Sobol initialization, featuring adaptive parameter control with a weighted average, dynamic neighborhood size adjustment, and enhanced CMA-ES integration with covariance matrix regularization.", "code": "import numpy as np\nfrom scipy.linalg import sqrtm\n\nclass HybridDECMASobolV3:\n    def __init__(self, budget=10000, dim=10, pop_size=50, f_init=0.5, cr_init=0.9, neighborhood_size=0.1, cma_generations=5, restart_trigger=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.f = np.full(pop_size, f_init)  # Individual mutation factors\n        self.cr = np.full(pop_size, cr_init) # Individual crossover rates\n        self.neighborhood_size = neighborhood_size\n        self.cma_generations = cma_generations\n        self.learning_rate = 0.1  # Adjusts the adaptation rate\n        self.cma_evals = 0\n        self.success_f = np.zeros(pop_size)\n        self.success_cr = np.zeros(pop_size)\n        self.success_count = np.zeros(pop_size)\n        self.restart_trigger = restart_trigger\n        self.archive_f = np.full(pop_size, f_init)\n        self.archive_cr = np.full(pop_size, cr_init)\n\n    def sobol_sequence(self, n_points, dim):\n        \"\"\"Generate n_points of Sobol sequence in dim dimensions.\"\"\"\n        try:\n            from sobol_seq import i4_sobol_generate\n            points = i4_sobol_generate(dim, n_points)\n            return points\n        except ImportError:\n            print(\"Sobol sequence library not found.  Falling back to uniform random numbers\")\n            return np.random.rand(n_points, dim)\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.best_history = []\n        self.stagnation_counter = 0\n        \n        # Sobol sequence initialization\n        sobol_points = self.sobol_sequence(self.pop_size, self.dim)\n        self.population = func.bounds.lb + sobol_points * (func.bounds.ub - func.bounds.lb)\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n\n        for i in range(self.pop_size):\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.population[i]\n        \n        self.best_history.append(self.f_opt)\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(np.arange(self.pop_size), 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                v = x1 + self.f[i] * (x2 - x3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = np.random.rand(self.dim) < self.cr[i]\n                u = v * u + self.population[i] * (~u)\n                u = np.clip(u, func.bounds.lb, func.bounds.ub)  # Clip after crossover\n\n                # Evaluation\n                f = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f < self.fitness[i]:\n                    # Update success history\n                    self.archive_f[i] = self.f[i]\n                    self.archive_cr[i] = self.cr[i]\n                    self.success_count[i] += 1\n\n                    self.fitness[i] = f\n                    self.population[i] = u\n                    \n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n                        self.stagnation_counter = 0  # Reset stagnation counter\n\n\n                # Adaptation of F and CR based on success history\n                if self.success_count[i] > 0:\n                    successful_f = self.archive_f[self.success_count > 0]\n                    successful_cr = self.archive_cr[self.success_count > 0]\n                    \n                    # Weighted average with more weight on recent values\n                    self.f[i] = 0.8 * self.f[i] + 0.2 * np.mean(successful_f) if successful_f.size > 0 else np.random.uniform(0.1, 0.9)\n                    self.cr[i] = 0.8 * self.cr[i] + 0.2 * np.mean(successful_cr) if successful_cr.size > 0 else np.random.uniform(0.1, 0.9)\n                else:\n                    self.f[i] = np.clip(np.random.normal(0.5, 0.3), 0.1, 1.0)\n                    self.cr[i] = np.clip(np.random.normal(0.9, 0.2), 0.1, 1.0)\n                \n                self.f[i] = np.clip(self.f[i], 0.1, 1.0)\n                self.cr[i] = np.clip(self.cr[i], 0.1, 1.0)\n                \n                if self.evals >= self.budget:\n                    break\n            \n            # CMA-ES component, triggered periodically\n            if self.evals < self.budget and self.evals % (self.budget // 5) == 0:\n                self.cma_es(func)\n\n            # Dynamic Neighborhood Search around best solution\n            if self.evals < self.budget:\n                # Adjust neighborhood size based on stagnation\n                if self.stagnation_counter > self.budget * 0.05:  # More stagnation, smaller neighborhood\n                    self.neighborhood_size = max(0.01, self.neighborhood_size * 0.9)\n                else:  # Less stagnation, larger neighborhood\n                    self.neighborhood_size = min(0.5, self.neighborhood_size * 1.1)\n\n                x_neighbor = self.x_opt + np.random.uniform(-self.neighborhood_size, self.neighborhood_size, size=self.dim)\n                x_neighbor = np.clip(x_neighbor, func.bounds.lb, func.bounds.ub)\n                f_neighbor = func(x_neighbor)\n                self.evals += 1\n\n                if f_neighbor < self.f_opt:\n                    self.f_opt = f_neighbor\n                    self.x_opt = x_neighbor\n                    self.stagnation_counter = 0 # Reset stagnation counter\n            \n            self.best_history.append(self.f_opt)\n            if len(self.best_history) > 1:\n                if self.best_history[-1] >= self.best_history[-2]:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0\n\n            # Restart mechanism\n            if self.stagnation_counter > self.budget * self.restart_trigger:\n                self.population = func.bounds.lb + np.random.rand(self.pop_size, self.dim) * (func.bounds.ub - func.bounds.lb)\n                self.fitness = np.array([func(x) for x in self.population])\n                self.evals += self.pop_size\n                self.f_opt = np.min(self.fitness)\n                self.x_opt = self.population[np.argmin(self.fitness)]\n                self.stagnation_counter = 0\n                self.success_f = np.zeros(self.pop_size)\n                self.success_cr = np.zeros(self.pop_size)\n                self.success_count = np.zeros(self.pop_size)\n                self.archive_f = np.full(self.pop_size, 0.5)\n                self.archive_cr = np.full(self.pop_size, 0.9)\n                print(\"Restarting population...\")\n\n        \n        return self.f_opt, self.x_opt\n\n    def cma_es(self, func):\n        \"\"\"Covariance Matrix Adaptation Evolution Strategy.\"\"\"\n        mean = self.x_opt.copy()\n        sigma = 0.1 * (func.bounds.ub - func.bounds.lb)  # Initial step size\n        C = np.eye(self.dim)  # Initial covariance matrix\n        \n        # Adapt the number of generations based on remaining budget\n        generations = min(self.cma_generations, (self.budget - self.evals) // self.pop_size)\n\n        if generations <= 0:\n            return\n\n        for _ in range(generations):\n            if self.evals >= self.budget:\n                break\n                \n            # Generate samples from multivariate normal distribution\n            z = np.random.randn(self.pop_size, self.dim)\n            samples = mean + sigma * np.dot(z, sqrtm(C).T)\n            samples = np.clip(samples, func.bounds.lb, func.bounds.ub)  # Clip to bounds\n            \n            # Evaluate samples\n            fitness_values = np.array([func(x) for x in samples])\n            self.evals += self.pop_size\n            \n            # Sort by fitness\n            idx = np.argsort(fitness_values)\n            best_samples = samples[idx]\n            best_fitness = fitness_values[idx]\n            \n            # Update best solution\n            if best_fitness[0] < self.f_opt:\n                self.f_opt = best_fitness[0]\n                self.x_opt = best_samples[0]\n            \n            # Update mean\n            mean = np.mean(best_samples[:self.pop_size//2], axis=0) # Use top 50%\n            \n            # Update covariance matrix (simplified)\n            C = np.cov(best_samples[:self.pop_size//2].T)  # Use top 50%\n\n            # Regularize covariance matrix\n            C = C + 1e-6 * np.eye(self.dim)\n\n            # Ensure that C is positive definite\n            try:\n                sqrtm(C)\n            except np.linalg.LinAlgError:\n                C = C + 1e-5 * np.eye(self.dim)  # Add a small diagonal to make it positive definite\n            \n            # Adjust sigma\n            sigma *= np.exp(0.5 * (np.mean(best_fitness) - self.f_opt) / np.std(fitness_values)) # Damping", "configspace": "", "generation": 4, "feedback": "The algorithm HybridDECMASobolV3 scored 0.693 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["77b25529-87c5-43b2-9417-0cde1ade20a1"], "operator": null, "metadata": {"aucs": [0.3428516174357168, 0.6774093838626751, 0.6443641123254208, 0.8590280095318379, 0.7239605907731954, 0.796977176206196, 0.6194663820559914, 0.6932905892667378, 0.731604876080995, 0.7027222087905002, 0.8591854490840982, 0.993747733497477, 0.6449093381442752, 0.7060683245821794, 0.889652787683831, 0.7753707754961732, 0.6180804535997204, 0.8339220437784257, 0.24396659381483543, 0.506242759391534]}}
{"id": "83884d24-e2e4-4655-a76a-c7b74da89d8d", "fitness": -Infinity, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with a pool of mutation strategies and adaptive parameter control based on exponential averaging of success rates.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, f=0.5, cr=0.9, restart_trigger=0.1, num_strategies=3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.f = f  # Mutation factor\n        self.cr = cr # Crossover rate\n        self.restart_trigger = restart_trigger # Percentage of budget before restart\n        self.success_history_f = np.zeros(num_strategies)\n        self.success_history_cr = np.zeros(num_strategies)\n        self.num_strategies = num_strategies\n        self.strategy_probs = np.ones(num_strategies) / num_strategies\n        self.learning_rate = 0.1\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        \n        for i in range(self.pop_size):\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.population[i]\n        \n        while self.evals < self.budget:\n            \n            # Restart strategy\n            if self.evals > self.restart_trigger * self.budget:\n                if np.std(self.fitness) < 1e-6:  # Stagnation check\n                    self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                    self.fitness = np.array([func(x) for x in self.population])\n                    self.evals += self.pop_size\n                    for j in range(self.pop_size):\n                        if self.fitness[j] < self.f_opt:\n                            self.f_opt = self.fitness[j]\n                            self.x_opt = self.population[j]\n                    continue\n            \n            for i in range(self.pop_size):\n                # Strategy Selection\n                strategy_idx = np.random.choice(self.num_strategies, p=self.strategy_probs)\n                \n                # Parameter Adaptation\n                f = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n                cr = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0)\n                \n                # Mutation\n                idxs = np.random.choice(np.arange(self.pop_size), 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                \n                # Different mutation strategies\n                if strategy_idx == 0:\n                    v = x1 + f * (x2 - x3)\n                elif strategy_idx == 1:\n                    v = self.population[i] + f * (x1 - self.population[i]) + f * (x2 - x3)\n                else:\n                    best_idx = np.argmin(self.fitness)\n                    v = self.population[i] + f * (self.population[best_idx] - self.population[i]) + f * (x2 - x3)\n                \n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = self.crossover(self.population[i], v, cr)\n                \n                # Evaluation\n                f_trial = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f_trial < self.fitness[i]:\n                    delta_fitness = self.fitness[i] - f_trial\n                    # Adaptive Parameter Control - Exponential Averaging\n                    self.success_history_f[strategy_idx] = (1 - self.learning_rate) * self.success_history_f[strategy_idx] + self.learning_rate * f\n                    self.success_history_cr[strategy_idx] = (1 - self.learning_rate) * self.success_history_cr[strategy_idx] + self.learning_rate * cr\n                    \n                    self.fitness[i] = f_trial\n                    self.population[i] = u\n                    \n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = u\n                    \n                    # Update Strategy Probabilities\n                    normalized_fitness_improvement = delta_fitness / (np.abs(self.f_opt) + 1e-8)\n                    self.strategy_probs[strategy_idx] *= np.exp(self.learning_rate * normalized_fitness_improvement)\n                    self.strategy_probs /= np.sum(self.strategy_probs)\n\n                if self.evals >= self.budget:\n                    break\n                    \n        return self.f_opt, self.x_opt\n        \n    def crossover(self, x, v, cr):\n        u = x.copy()\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < cr or j == j_rand:\n                u[j] = v[j]\n        return u", "configspace": "", "generation": 4, "feedback": "An exception occurred: probabilities contain NaN.", "error": "", "parent_ids": ["24da9b3b-348f-4ec1-b7f6-55bc26a87d9c"], "operator": null, "metadata": {}}
{"id": "299c4538-7283-46e5-a347-3b7552f9fb30", "fitness": 0.651215143207062, "name": "AdaptiveDE", "description": "Simplified Adaptive Differential Evolution with enhanced parameter adaptation and a stagnation-based restart mechanism.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, f=0.5, cr=0.9, restart_trigger=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.f = f  # Mutation factor\n        self.cr = cr # Crossover rate\n        self.restart_trigger = restart_trigger # Percentage of budget before restart\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        \n        for i in range(self.pop_size):\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.population[i]\n        \n        while self.evals < self.budget:\n            \n            # Restart strategy: Simplified stagnation check\n            if self.evals > self.restart_trigger * self.budget and np.std(self.fitness) < 1e-6:\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.evals += self.pop_size\n                for j in range(self.pop_size):\n                    if self.fitness[j] < self.f_opt:\n                        self.f_opt = self.fitness[j]\n                        self.x_opt = self.population[j]\n                continue\n            \n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(np.arange(self.pop_size), 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                v = x1 + self.f * (x2 - x3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = self.crossover(self.population[i], v)\n                \n                # Evaluation\n                f = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.population[i] = u\n                    \n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n\n                    # Adaptive Parameter Control: Simplified update\n                    self.f = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n                    self.cr = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0)\n\n                if self.evals >= self.budget:\n                    break\n                    \n        return self.f_opt, self.x_opt\n        \n    def crossover(self, x, v):\n        u = x.copy()\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.cr or j == j_rand:\n                u[j] = v[j]\n        return u", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDE scored 0.651 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["24da9b3b-348f-4ec1-b7f6-55bc26a87d9c"], "operator": null, "metadata": {"aucs": [0.33686525263285516, 0.6002676892905365, 0.6295836484820176, 0.8483641144739323, 0.6514719450858852, 0.6889240810541296, 0.5684923023521711, 0.5967187231573563, 0.6414318525058087, 0.6354757779963266, 0.8762311565030416, 0.9961444665262892, 0.5915684149082039, 0.6267686822369631, 0.8009003589321017, 0.6702781491649386, 0.5948008884695623, 0.7659623206426203, 0.38681644812591964, 0.5172365916005801]}}
{"id": "35108c5b-ff48-486d-8b50-af0d87a77e99", "fitness": -Infinity, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with momentum-based mutation, spectral restart, and improved parameter adaptation using both success history and population diversity.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, f=0.5, cr=0.9, restart_trigger=0.1, momentum=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.f = f\n        self.cr = cr\n        self.restart_trigger = restart_trigger\n        self.archive = []\n        self.archive_fitness = []\n        self.sf_history = []\n        self.momentum = momentum\n        self.velocity = np.zeros((pop_size, dim)) # Initialize velocity\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.population[best_idx]\n        \n        while self.evals < self.budget:\n            # Restart strategy: Spectral Restart\n            if self.evals > self.restart_trigger * self.budget:\n                if np.std(self.fitness) < 1e-6:\n                    # Spectral restart: Move population towards the mean\n                    mean_pos = np.mean(self.population, axis=0)\n                    self.population = np.random.uniform(mean_pos - 0.5, mean_pos + 0.5, size=(self.pop_size, self.dim)) # Smaller range around the mean\n                    self.population = np.clip(self.population, func.bounds.lb, func.bounds.ub) # Ensure bounds are respected\n                    self.fitness = np.array([func(x) for x in self.population])\n                    self.evals += self.pop_size\n                    best_idx = np.argmin(self.fitness)\n                    if self.fitness[best_idx] < self.f_opt:\n                        self.f_opt = self.fitness[best_idx]\n                        self.x_opt = self.population[best_idx]\n                    self.velocity = np.zeros((self.pop_size, self.dim)) #reset velocity\n                    continue\n            \n            ranked_indices = np.argsort(self.fitness) # Rank individuals\n            \n            for i in range(self.pop_size):\n                # Mutation: Rank-based selection with momentum\n                best_idx = ranked_indices[0]  # Best individual\n                worst_idx = ranked_indices[-1] # Worst individual\n\n                idxs = np.random.choice(np.arange(self.pop_size), 2, replace=False)\n                x1, x2 = self.population[idxs]\n                \n                # Momentum-based velocity update\n                self.velocity[i] = self.momentum * self.velocity[i] + self.f * (x1 - x2)\n                \n                v = self.population[best_idx] + self.velocity[i]\n\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = self.crossover(self.population[i], v)\n                \n                # Evaluation\n                f = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f < self.fitness[i]:\n                    # Archive update\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i])\n                        self.archive_fitness.append(self.fitness[i])\n                    else:\n                        worst_archive_idx = np.argmax(self.archive_fitness)\n                        if self.fitness[i] < self.archive_fitness[worst_archive_idx]:\n                            self.archive[worst_archive_idx] = self.population[i]\n                            self.archive_fitness[worst_archive_idx] = self.fitness[i]\n\n                    #Adaptive F update based on success - simplified update\n                    if f < self.fitness[i]:\n                        self.sf_history.append(abs(self.fitness[i] - f))\n                        if len(self.sf_history) > 50:\n                            self.sf_history.pop(0)\n\n                    self.fitness[i] = f\n                    self.population[i] = u\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n                \n                # Adaptive Parameter Control (simplified and more frequent), incorporating population diversity\n                if np.random.rand() < 0.3:  # Increased frequency\n                    if self.sf_history:\n                        self.f = np.clip(np.random.normal(0.5, np.std(self.sf_history)), 0.1, 1.0)\n                    else:\n                        self.f = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n\n                    # Adjust CR based on population diversity\n                    diversity = np.std(self.fitness) # Using std dev as a simple measure of diversity\n                    self.cr = np.clip(np.random.normal(0.9, 0.1 * (1.0 - diversity)), 0.1, 1.0)  # CR decreases if population is too uniform\n\n                if self.evals >= self.budget:\n                    break\n        return self.f_opt, self.x_opt\n        \n    def crossover(self, x, v):\n        u = x.copy()\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.cr or j == j_rand:\n                u[j] = v[j]\n        return u", "configspace": "", "generation": 5, "feedback": "An exception occurred: scale < 0.", "error": "", "parent_ids": ["227caa0a-bdd6-4fd7-aa48-e3783b0c999d"], "operator": null, "metadata": {}}
{"id": "ebae4b8b-f63a-4bbb-a1f9-ff650d883d2d", "fitness": -Infinity, "name": "AdaptiveDECauchyOrthogonal", "description": "An adaptive Differential Evolution strategy employing a novel combination of Cauchy mutation, orthogonal learning, and a self-adaptive population size.", "code": "import numpy as np\n\nclass AdaptiveDECauchyOrthogonal:\n    def __init__(self, budget=10000, dim=10, pop_size_init=50, cauchy_scale=0.1, orthogonal_learning_rate=0.1, pop_size_adapt_freq=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_init = pop_size_init\n        self.pop_size = pop_size_init\n        self.cauchy_scale = cauchy_scale\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.pop_size_adapt_freq = pop_size_adapt_freq\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n        self.stagnation_counter = 0\n        self.archive = []\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Cauchy Mutation\n                idxs = np.random.choice(np.arange(self.pop_size), 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                v = x1 + np.random.standard_cauchy(size=self.dim) * self.cauchy_scale * (x2 - x3)  # Cauchy mutation\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cr = np.random.rand()\n                u = np.random.rand(self.dim) < cr\n                u = v * u + self.population[i] * (~u)\n                u = np.clip(u, func.bounds.lb, func.bounds.ub)\n                \n                # Evaluation\n                f = func(u)\n                self.evals += 1\n\n                # Selection\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.population[i] = u\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n                        self.stagnation_counter = 0\n                        self.archive.append((f, u))\n                else:\n                    self.stagnation_counter += 1\n\n                if self.evals >= self.budget:\n                    break\n\n            # Orthogonal Learning\n            self.orthogonal_learning(func)\n\n            # Population size adaptation\n            if self.evals % self.pop_size_adapt_freq == 0:\n                self.adapt_population_size()\n\n            # Restart mechanism\n            if self.stagnation_counter > self.budget * 0.1:\n                self.initialize_population(func)\n                self.stagnation_counter = 0\n                print(\"Restarting population...\")\n\n        return self.f_opt, self.x_opt\n\n    def initialize_population(self, func):\n        self.population = func.bounds.lb + np.random.rand(self.pop_size, self.dim) * (func.bounds.ub - func.bounds.lb)\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals += self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.archive = [(self.f_opt, self.x_opt)]\n\n    def orthogonal_learning(self, func):\n        \"\"\"Improve solutions using orthogonal design.\"\"\"\n        if not self.archive:\n            return\n\n        best_fitness, best_solution = self.archive[-1]  # Use the best from archive\n\n        # Generate orthogonal array (simplified - two levels)\n        orthogonal_array = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n\n        for row in orthogonal_array:\n            neighbor = best_solution.copy()\n            for i in range(self.dim):\n                if row[i % 2] == 0: # Simplified for 2 levels\n                    neighbor[i] = best_solution[i] + self.orthogonal_learning_rate * (func.bounds.ub - func.bounds.lb) * np.random.rand()\n                else:\n                    neighbor[i] = best_solution[i] - self.orthogonal_learning_rate * (func.bounds.ub - func.bounds.lb) * np.random.rand()\n                neighbor[i] = np.clip(neighbor[i], func.bounds.lb, func.bounds.ub)\n\n            f_neighbor = func(neighbor)\n            self.evals += 1\n\n            if f_neighbor < self.f_opt:\n                self.f_opt = f_neighbor\n                self.x_opt = neighbor\n                self.archive.append((f_neighbor, neighbor))\n\n            if self.evals >= self.budget:\n                break\n\n\n    def adapt_population_size(self):\n        \"\"\"Adapt population size based on stagnation.\"\"\"\n        if self.stagnation_counter > self.budget * 0.05:\n            self.pop_size = max(10, int(self.pop_size * 0.9))  # Reduce population size\n            print(f\"Reducing population size to {self.pop_size}\")\n        else:\n            self.pop_size = min(self.pop_size_init, int(self.pop_size * 1.1))  # Increase population size\n            print(f\"Increasing population size to {self.pop_size}\")\n\n        # Reinitialize population with new size\n        # This is important to avoid issues with array sizes\n        # Reinitialize population if population size changed significantly\n        if abs(self.pop_size - len(self.population)) > self.pop_size * 0.1:\n              #Keep the current best solution in the population\n              self.initialize_population(func)", "configspace": "", "generation": 5, "feedback": "An exception occurred: setting an array element with a sequence..", "error": "", "parent_ids": ["c34149bf-cb50-43d4-a989-f869a67149fd"], "operator": null, "metadata": {}}
{"id": "eed6a60b-3127-493e-bf85-bf9e38bb5997", "fitness": 0.3791095533170047, "name": "EnhancedAdaptiveDE", "description": "Enhanced Adaptive DE with per-parameter adaptation of F and CR, orthogonal crossover, and a more robust stagnation restart.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, adapt_freq=50, stagnation_threshold=1000, learning_rate=0.1, orthogonal_crossover_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.adapt_freq = adapt_freq\n        self.stagnation_threshold = stagnation_threshold\n        self.learning_rate = learning_rate\n        self.orthogonal_crossover_rate = orthogonal_crossover_rate\n        self.f = np.full(dim, 0.5)  # F for each dimension\n        self.cr = np.full(dim, 0.9)  # CR for each dimension\n        self.success_f = [[] for _ in range(dim)]\n        self.success_cr = [[] for _ in range(dim)]\n        self.best_fitness_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.last_improvement = 0\n        self.restart_count = 0\n\n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.population[best_idx]\n        self.best_fitness_history.append(self.f_opt)\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(np.arange(self.pop_size), 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                v = x1 + self.f * (x2 - x3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = np.copy(self.population[i])  # Start with the current individual\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr[j] or np.random.rand() < self.orthogonal_crossover_rate:\n                        u[j] = v[j]\n\n                # Evaluation\n                f_new = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f_new < self.fitness[i]:\n                    for j in range(self.dim):\n                        if u[j] != self.population[i][j]:\n                            self.success_f[j].append(self.f[j])\n                            self.success_cr[j].append(self.cr[j])\n                    self.fitness[i] = f_new\n                    self.population[i] = u\n\n                    if f_new < self.f_opt:\n                        self.f_opt = f_new\n                        self.x_opt = u\n                        self.best_fitness_history.append(self.f_opt)\n                        self.last_improvement = self.evals\n\n            # Adaptive Parameter Control\n            if self.evals % self.adapt_freq == 0:\n                for j in range(self.dim):\n                    if self.success_f[j]:\n                        self.f[j] = (1 - self.learning_rate) * self.f[j] + self.learning_rate * np.mean(self.success_f[j])\n                        self.cr[j] = (1 - self.learning_rate) * self.cr[j] + self.learning_rate * np.mean(self.success_cr[j])\n                    self.f[j] = np.clip(self.f[j], 0.1, 0.9)\n                    self.cr[j] = np.clip(self.cr[j], 0.1, 1.0)\n                    self.success_f[j] = []\n                    self.success_cr[j] = []\n            \n            # Stagnation Check and Restart - Adjusted restart strategy\n            if self.evals - self.last_improvement > self.stagnation_threshold:\n                self.restart_count += 1\n                # Option 1: Re-initialize only a portion of the population around the best solution.\n                num_to_restart = int(self.pop_size * 0.5)  # Restart half the population\n                indices_to_restart = np.random.choice(self.pop_size, num_to_restart, replace=False)\n                for idx in indices_to_restart:\n                    self.population[idx] = self.x_opt + np.random.uniform(-0.5, 0.5, size=self.dim)  # Initialize around best\n                    self.population[idx] = np.clip(self.population[idx], func.bounds.lb, func.bounds.ub)\n                    self.fitness[idx] = func(self.population[idx])\n                    self.evals += 1\n\n                best_idx = np.argmin(self.fitness)\n                if self.fitness[best_idx] < self.f_opt:\n                        self.f_opt = self.fitness[best_idx]\n                        self.x_opt = self.population[best_idx]\n                        self.best_fitness_history.append(self.f_opt)\n                        self.last_improvement = self.evals\n\n\n            if self.evals >= self.budget:\n                break\n                    \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm EnhancedAdaptiveDE scored 0.379 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e80c9b07-9e4a-47ea-b07f-4479fdb1d3e3"], "operator": null, "metadata": {"aucs": [0.21519535828181324, 0.6359995200061367, 0.6652433349800688, 0]}}
{"id": "2c0cbf82-8136-473e-9c12-ac4e0bf60dd4", "fitness": 0.0, "name": "AdaptiveDECauchy", "description": "An adaptive differential evolution strategy with a Cauchy mutation operator and a self-adaptive population size adjustment mechanism.", "code": "import numpy as np\n\nclass AdaptiveDECauchy:\n    def __init__(self, budget=10000, dim=10, pop_size_init=50, f_init=0.5, cr_init=0.9, pop_size_adapt_freq=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size_init\n        self.pop_size_min = 10\n        self.pop_size_max = 100\n        self.f = np.full(self.pop_size, f_init)\n        self.cr = np.full(self.pop_size, cr_init)\n        self.pop_size_adapt_freq = pop_size_adapt_freq\n        self.learning_rate = 0.1\n        self.archive_f = np.full(self.pop_size, f_init)\n        self.archive_cr = np.full(self.pop_size, cr_init)\n        self.success_count = np.zeros(self.pop_size)\n        self.stagnation_counter = 0\n        self.restart_trigger = 0.2\n        self.pop_size_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n        self.best_history = []\n\n        # Initialize population\n        self.population = func.bounds.lb + np.random.rand(self.pop_size, self.dim) * (func.bounds.ub - func.bounds.lb)\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals += self.pop_size\n\n        for i in range(self.pop_size):\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.population[i]\n\n        self.best_history.append(self.f_opt)\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation (Cauchy)\n                idxs = np.random.choice(np.arange(self.pop_size), 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                # Cauchy mutation\n                v = x1 + self.f[i] * np.random.standard_cauchy(size=self.dim) * (x2 - x3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                u = np.random.rand(self.dim) < self.cr[i]\n                u = v * u + self.population[i] * (~u)\n                u = np.clip(u, func.bounds.lb, func.bounds.ub)\n\n                # Evaluation\n                f = func(u)\n                self.evals += 1\n\n                # Selection\n                if f < self.fitness[i]:\n                    self.archive_f[i] = self.f[i]\n                    self.archive_cr[i] = self.cr[i]\n                    self.success_count[i] += 1\n                    self.fitness[i] = f\n                    self.population[i] = u\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n                        self.stagnation_counter = 0  # Reset stagnation\n\n                # Adaptation of F and CR\n                if self.success_count[i] > 0:\n                    successful_f = self.archive_f[self.success_count > 0]\n                    successful_cr = self.archive_cr[self.success_count > 0]\n\n                    self.f[i] = 0.8 * self.f[i] + 0.2 * np.mean(successful_f) if successful_f.size > 0 else np.random.uniform(0.1, 0.9)\n                    self.cr[i] = 0.8 * self.cr[i] + 0.2 * np.mean(successful_cr) if successful_cr.size > 0 else np.random.uniform(0.1, 0.9)\n                else:\n                    self.f[i] = np.clip(np.random.normal(0.5, 0.3), 0.1, 1.0)\n                    self.cr[i] = np.clip(np.random.normal(0.9, 0.2), 0.1, 1.0)\n\n                self.f[i] = np.clip(self.f[i], 0.1, 1.0)\n                self.cr[i] = np.clip(self.cr[i], 0.1, 1.0)\n\n                if self.evals >= self.budget:\n                    break\n\n            # Population size adaptation\n            if self.evals // self.pop_size_adapt_freq > len(self.pop_size_history):\n                self.pop_size_history.append(self.pop_size)\n                # Adjust population size based on stagnation\n                if self.stagnation_counter > self.budget * 0.05:\n                    self.pop_size = max(self.pop_size_min, int(self.pop_size * 0.9))  # Reduce pop size\n                else:\n                    self.pop_size = min(self.pop_size_max, int(self.pop_size * 1.1))  # Increase pop size\n\n                # Resize population and parameters if needed\n                if self.pop_size != self.population.shape[0]:\n                    old_pop = self.population.copy()\n                    old_fitness = self.fitness.copy()\n\n                    self.population = func.bounds.lb + np.random.rand(self.pop_size, self.dim) * (func.bounds.ub - func.bounds.lb)\n                    self.fitness = np.array([func(x) for x in self.population])\n                    self.evals += abs(self.pop_size - old_pop.shape[0])\n                    \n                    # Carry over best individuals\n                    num_carry = min(self.pop_size, old_pop.shape[0])\n                    idx = np.argsort(old_fitness)\n                    self.population[:num_carry] = old_pop[idx[:num_carry]]\n                    self.fitness[:num_carry] = old_fitness[idx[:num_carry]]\n\n                    self.f = np.full(self.pop_size, np.mean(self.archive_f))\n                    self.cr = np.full(self.pop_size, np.mean(self.archive_cr))\n                    self.archive_f = np.full(self.pop_size, np.mean(self.archive_f))\n                    self.archive_cr = np.full(self.pop_size, np.mean(self.archive_cr))\n                    self.success_count = np.zeros(self.pop_size)\n\n            self.best_history.append(self.f_opt)\n\n            if len(self.best_history) > 1:\n                if self.best_history[-1] >= self.best_history[-2]:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0\n\n            # Restart mechanism\n            if self.stagnation_counter > self.budget * self.restart_trigger:\n                self.population = func.bounds.lb + np.random.rand(self.pop_size, self.dim) * (func.bounds.ub - func.bounds.lb)\n                self.fitness = np.array([func(x) for x in self.population])\n                self.evals += self.pop_size\n                self.f_opt = np.min(self.fitness)\n                self.x_opt = self.population[np.argmin(self.fitness)]\n                self.stagnation_counter = 0\n                self.success_count = np.zeros(self.pop_size)\n                self.archive_f = np.full(self.pop_size, 0.5)\n                self.archive_cr = np.full(self.pop_size, 0.9)\n                print(\"Restarting population...\")\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDECauchy scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c34149bf-cb50-43d4-a989-f869a67149fd"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "287faa6b-3e5c-44db-a18e-0af232051e12", "fitness": -Infinity, "name": "HybridDE", "description": "Hybrid DE with simplified adaptive parameter control, dynamic population size, and occasional local search to escape local optima.", "code": "import numpy as np\n\nclass HybridDE:\n    def __init__(self, budget=10000, dim=10, pop_size_init=50, f_init=0.5, cr_init=0.9, local_search_freq=0.1, restart_trigger=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_init = pop_size_init\n        self.pop_size = pop_size_init\n        self.f = np.full(pop_size_init, f_init)\n        self.cr = np.full(pop_size_init, cr_init)\n        self.local_search_freq = local_search_freq\n        self.learning_rate = 0.1\n        self.restart_trigger = restart_trigger\n        self.archive_f = np.full(pop_size_init, f_init)\n        self.archive_cr = np.full(pop_size_init, cr_init)\n        self.success_count = np.zeros(pop_size_init)\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n        self.stagnation_counter = 0\n        self.best_history = []\n        \n        # Initialize population\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals += self.pop_size\n\n        for i in range(self.pop_size):\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.population[i]\n        self.best_history.append(self.f_opt)\n\n\n        while self.evals < self.budget:\n            # Dynamic population size adjustment\n            if len(self.best_history) > 50:\n                if np.std(self.best_history[-50:]) < 1e-6:\n                    self.pop_size = min(self.pop_size + 10, 2 * self.pop_size_init)\n                else:\n                    self.pop_size = max(self.pop_size - 5, self.pop_size_init // 2)\n                self.pop_size = int(self.pop_size)\n                if self.population.shape[0] != self.pop_size:\n                  self.population = self.population[:self.pop_size]\n                  self.fitness = self.fitness[:self.pop_size]\n                  self.f = self.f[:self.pop_size]\n                  self.cr = self.cr[:self.pop_size]\n                  self.archive_f = self.archive_f[:self.pop_size]\n                  self.archive_cr = self.archive_cr[:self.pop_size]\n                  self.success_count = self.success_count[:self.pop_size]\n            \n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(np.arange(self.pop_size), 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                v = x1 + self.f[i] * (x2 - x3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = np.random.rand(self.dim) < self.cr[i]\n                u = v * u + self.population[i] * (~u)\n                u = np.clip(u, func.bounds.lb, func.bounds.ub)\n\n                # Evaluation\n                f = func(u)\n                self.evals += 1\n\n                # Selection\n                if f < self.fitness[i]:\n                    self.archive_f[i] = self.f[i]\n                    self.archive_cr[i] = self.cr[i]\n                    self.success_count[i] += 1\n\n                    self.fitness[i] = f\n                    self.population[i] = u\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n                        self.stagnation_counter = 0\n                \n                # Simplified Adaptation of F and CR\n                if self.success_count[i] > 0:\n                    self.f[i] = 0.8 * self.f[i] + 0.2 * np.mean(self.archive_f[self.success_count > 0]) if np.any(self.success_count > 0) else 0.5\n                    self.cr[i] = 0.8 * self.cr[i] + 0.2 * np.mean(self.archive_cr[self.success_count > 0]) if np.any(self.success_count > 0) else 0.9\n                else:\n                    self.f[i] = np.random.uniform(0.1, 0.9)\n                    self.cr[i] = np.random.uniform(0.1, 0.9)\n                \n                self.f[i] = np.clip(self.f[i], 0.1, 1.0)\n                self.cr[i] = np.clip(self.cr[i], 0.1, 1.0)\n                \n\n                if self.evals >= self.budget:\n                    break\n            \n            # Local search around best solution\n            if np.random.rand() < self.local_search_freq and self.evals < self.budget:\n                x_local = self.x_opt + np.random.normal(0, 0.05 * (func.bounds.ub - func.bounds.lb), size=self.dim)\n                x_local = np.clip(x_local, func.bounds.lb, func.bounds.ub)\n                f_local = func(x_local)\n                self.evals += 1\n\n                if f_local < self.f_opt:\n                    self.f_opt = f_local\n                    self.x_opt = x_local\n                    self.stagnation_counter = 0\n\n            self.best_history.append(self.f_opt)\n            if len(self.best_history) > 1:\n                if self.best_history[-1] >= self.best_history[-2]:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0\n\n            # Restart mechanism\n            if self.stagnation_counter > self.budget * self.restart_trigger:\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.evals += self.pop_size\n                self.f_opt = np.min(self.fitness)\n                self.x_opt = self.population[np.argmin(self.fitness)]\n                self.stagnation_counter = 0\n                self.archive_f = np.full(self.pop_size, 0.5)\n                self.archive_cr = np.full(self.pop_size, 0.9)\n                self.success_count = np.zeros(self.pop_size)\n                print(\"Restarting population...\")\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "An exception occurred: index 26 is out of bounds for axis 0 with size 25.", "error": "", "parent_ids": ["c34149bf-cb50-43d4-a989-f869a67149fd"], "operator": null, "metadata": {}}
{"id": "bbf7a9c1-fa2d-4c78-9cd9-0bcc260ef43a", "fitness": 0.3584700597428059, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with orthogonal design-based crossover and dynamic parameter adaptation based on success rate and population diversity.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, f=0.5, cr=0.9, restart_trigger=0.1, od_samples=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.f = f\n        self.cr = cr\n        self.restart_trigger = restart_trigger\n        self.archive = []\n        self.archive_fitness = []\n        self.sf_history = []\n        self.cr_history = []\n        self.od_samples = od_samples\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.population[best_idx]\n        \n        while self.evals < self.budget:\n            # Restart strategy\n            if self.evals > self.restart_trigger * self.budget:\n                if np.std(self.fitness) < 1e-6:\n                    self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                    self.fitness = np.array([func(x) for x in self.population])\n                    self.evals += self.pop_size\n                    best_idx = np.argmin(self.fitness)\n                    if self.fitness[best_idx] < self.f_opt:\n                        self.f_opt = self.fitness[best_idx]\n                        self.x_opt = self.population[best_idx]\n                    continue\n            \n            ranked_indices = np.argsort(self.fitness) # Rank individuals\n            \n            for i in range(self.pop_size):\n                # Mutation: Rank-based selection\n                best_idx = ranked_indices[0]  # Best individual\n                worst_idx = ranked_indices[-1] # Worst individual\n\n                idxs = np.random.choice(np.arange(self.pop_size), 2, replace=False)\n                x1, x2 = self.population[idxs]\n                \n                v = self.population[best_idx] + self.f * (x1 - x2)\n\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover: Orthogonal Design-based Crossover\n                u_list = self.orthogonal_crossover(self.population[i], v)\n                f_list = []\n                for u in u_list:\n                    f = func(u)\n                    self.evals += 1\n                    f_list.append(f)\n\n                best_u_idx = np.argmin(f_list)\n                u = u_list[best_u_idx]\n                f = f_list[best_u_idx]\n                \n                # Selection\n                if f < self.fitness[i]:\n                    # Archive update\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i])\n                        self.archive_fitness.append(self.fitness[i])\n                    else:\n                        worst_archive_idx = np.argmax(self.archive_fitness)\n                        if self.fitness[i] < self.archive_fitness[worst_archive_idx]:\n                            self.archive[worst_archive_idx] = self.population[i]\n                            self.archive_fitness[worst_archive_idx] = self.fitness[i]\n\n                    #Adaptive F/CR update based on success - simplified update\n                    if f < self.fitness[i]:\n                        self.sf_history.append(abs(self.fitness[i] - f))\n                        self.cr_history.append(1.0) #Mark success\n                        if len(self.sf_history) > 50:\n                            self.sf_history.pop(0)\n                        if len(self.cr_history) > 50:\n                            self.cr_history.pop(0)\n\n                    self.fitness[i] = f\n                    self.population[i] = u\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n                else:\n                    self.cr_history.append(0.0) #Mark failure\n                    if len(self.cr_history) > 50:\n                        self.cr_history.pop(0)\n                \n                # Adaptive Parameter Control (simplified and more frequent)\n                if np.random.rand() < 0.3:  # Increased frequency\n                    if self.sf_history:\n                        self.f = np.clip(np.random.normal(0.5, np.std(self.sf_history)), 0.1, 1.0)\n                    else:\n                        self.f = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n                    \n                    # Dynamic CR adaptation based on success rate\n                    if self.cr_history:\n                        success_rate = np.mean(self.cr_history)\n                        self.cr = np.clip(np.random.normal(success_rate, 0.1), 0.1, 1.0)\n                    else:\n                        self.cr = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0)\n\n                if self.evals >= self.budget:\n                    break\n        return self.f_opt, self.x_opt\n\n    def orthogonal_crossover(self, x, v):\n        # Generate an orthogonal array (OA) using Plackett-Burman design\n        # For simplicity, we will assume dim <= 12 for PB design.\n        dim = self.dim\n        n_samples = self.od_samples\n\n        # Use a fixed OA, or adapt it based on the dimension\n        if dim <= 4:\n          oa = np.array([[1, 1, 1],\n                         [1, 2, 2],\n                         [2, 1, 2],\n                         [2, 2, 1]]) - 1\n        elif dim <= 8:\n          oa = np.array([[1, 1, 1, 1, 1, 1, 1],\n                         [1, 1, 1, 2, 2, 2, 2],\n                         [1, 1, 2, 1, 1, 2, 2],\n                         [1, 1, 2, 2, 2, 1, 1],\n                         [1, 2, 1, 1, 2, 1, 2],\n                         [1, 2, 1, 2, 1, 2, 1],\n                         [1, 2, 2, 1, 2, 2, 1],\n                         [1, 2, 2, 2, 1, 1, 2]]) - 1\n        elif dim <= 12:\n          oa = np.array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n                         [1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2],\n                         [1, 1, 1, 2, 2, 1, 1, 1, 2, 2, 2],\n                         [1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1],\n                         [1, 2, 2, 1, 1, 1, 1, 2, 1, 2, 2],\n                         [1, 2, 2, 1, 1, 2, 2, 1, 2, 1, 1],\n                         [1, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2],\n                         [1, 2, 2, 2, 2, 2, 2, 1, 2, 1, 1],\n                         [2, 1, 2, 1, 2, 1, 2, 1, 1, 1, 2],\n                         [2, 1, 2, 1, 2, 2, 1, 2, 2, 2, 1],\n                         [2, 1, 2, 2, 1, 1, 2, 1, 2, 2, 1],\n                         [2, 1, 2, 2, 1, 2, 1, 2, 1, 1, 2]]) - 1\n        else:\n            oa = np.random.randint(0, 2, size=(n_samples, dim))\n\n        n_rows = oa.shape[0]\n        u_list = []\n        for i in range(min(n_rows, n_samples)):  # Use min to avoid errors\n            u = x.copy()\n            for j in range(dim):\n                if oa[i, j] == 0:  # If OA entry is 0, take from x\n                    u[j] = x[j]\n                else:  # If OA entry is 1, take from v\n                    u[j] = v[j]\n            u_list.append(u)\n\n        return u_list", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDE scored 0.358 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["227caa0a-bdd6-4fd7-aa48-e3783b0c999d"], "operator": null, "metadata": {"aucs": [0.15682904191473046, 0.2121647986359957, 0.3525000488119837, 0.31578665961865493, 0.28803212301223224, 0.3429945460192837, 0.28519078489844374, 0.2837216423118524, 0.2995912567291068, 0.18821504642468867, 0.28084940760768806, 0.9955750533122865, 0.25858652611016764, 0.3125763699507522, 0.723761536051812, 0.39604020894417447, 0.2949590335750031, 0.44059058029365517, 0.24432605796499973, 0.49711047266860664]}}
{"id": "81618a0e-283c-4b95-a231-637a46509895", "fitness": 0.6619421903032572, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with simplified adaptive F and CR, restart mechanism, and a tournament-based survivor selection.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, f=0.5, cr=0.9, restart_trigger=0.1, tournament_size=3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.f = f  # Mutation factor\n        self.cr = cr # Crossover rate\n        self.restart_trigger = restart_trigger # Percentage of budget before restart\n        self.tournament_size = tournament_size\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        \n        for i in range(self.pop_size):\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.population[i]\n        \n        while self.evals < self.budget:\n            \n            # Restart strategy: Simplified stagnation check\n            if self.evals > self.restart_trigger * self.budget and np.std(self.fitness) < 1e-6:\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.evals += self.pop_size\n                for j in range(self.pop_size):\n                    if self.fitness[j] < self.f_opt:\n                        self.f_opt = self.fitness[j]\n                        self.x_opt = self.population[j]\n                continue\n            \n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(np.arange(self.pop_size), 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                v = x1 + self.f * (x2 - x3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = self.crossover(self.population[i], v)\n                \n                # Evaluation\n                f = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.population[i] = u\n                    \n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n\n                # Adaptive Parameter Control: Simplified update, only after successful update\n                if np.random.rand() < 0.1: # Reduced frequency of adaptation\n                    self.f = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n                    self.cr = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0)\n\n                if self.evals >= self.budget:\n                    break\n            \n            # Tournament Selection: Simplified survivor selection.  Replaces the worst with a tournament winner.\n            worst_idx = np.argmax(self.fitness)\n\n            competitors_idx = np.random.choice(np.arange(self.pop_size), self.tournament_size, replace=False)\n            tournament_fitness = self.fitness[competitors_idx]\n            winner_idx = competitors_idx[np.argmin(tournament_fitness)]\n\n            if self.fitness[winner_idx] < self.fitness[worst_idx]:\n                self.population[worst_idx] = self.population[winner_idx]\n                self.fitness[worst_idx] = self.fitness[winner_idx]\n\n\n                    \n        return self.f_opt, self.x_opt\n        \n    def crossover(self, x, v):\n        u = x.copy()\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.cr or j == j_rand:\n                u[j] = v[j]\n        return u", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDE scored 0.662 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["299c4538-7283-46e5-a347-3b7552f9fb30"], "operator": null, "metadata": {"aucs": [0.3732249704051741, 0.691723858692107, 0.6021991701518977, 0.8693132912025487, 0.6695252035730815, 0.6902608430939865, 0.5939112265845019, 0.6158040991206499, 0.6620189860318907, 0.663458584129195, 0.8873778153595273, 1.0, 0.6190654882753543, 0.6852593591685446, 0.782968883779871, 0.6913721835796685, 0.5866297320346682, 0.8103285966721154, 0.23615737453016727, 0.5082441396801953]}}
{"id": "388eef40-3c24-449e-8db5-9bc282696841", "fitness": 0.6398990869274084, "name": "AdaptiveDE", "description": "Adaptive DE with simplified parameter adaptation, rank-based mutation, and archive, focusing on faster convergence through aggressive parameter updates and adaptive archive management.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, f=0.5, cr=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.f = f\n        self.cr = cr\n        self.archive = []\n        self.archive_fitness = []\n        self.sf_history = []\n        self.success_count = 0 # Track success for parameter adaptation\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.population[best_idx]\n        \n        while self.evals < self.budget:\n            ranked_indices = np.argsort(self.fitness) # Rank individuals\n            \n            for i in range(self.pop_size):\n                # Mutation: Rank-based selection\n                best_idx = ranked_indices[0]  # Best individual\n\n                idxs = np.random.choice(np.arange(self.pop_size), 2, replace=False)\n                x1, x2 = self.population[idxs]\n                \n                v = self.population[best_idx] + self.f * (x1 - x2)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = self.crossover(self.population[i], v)\n                \n                # Evaluation\n                f = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f < self.fitness[i]:\n                    # Archive update\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i])\n                        self.archive_fitness.append(self.fitness[i])\n                    else:\n                        worst_archive_idx = np.argmax(self.archive_fitness)\n                        if f < self.archive_fitness[worst_archive_idx]:\n                            self.archive[worst_archive_idx] = self.population[i]\n                            self.archive_fitness[worst_archive_idx] = self.fitness[i]\n\n                    self.fitness[i] = f\n                    self.population[i] = u\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n                    self.success_count += 1\n                else:\n                    self.success_count = 0\n                \n                #Adaptive F update based on success\n                if self.success_count > 5:\n                    self.f = np.clip(self.f * 0.95, 0.1, 0.9)\n                    self.success_count = 0\n                else:\n                    self.f = np.clip(self.f * 1.05, 0.1, 0.9)\n\n                if np.random.rand() < 0.1: # Adaptive CR\n                    self.cr = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0)\n\n                if self.evals >= self.budget:\n                    break\n        return self.f_opt, self.x_opt\n        \n    def crossover(self, x, v):\n        u = x.copy()\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.cr or j == j_rand:\n                u[j] = v[j]\n        return u", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDE scored 0.640 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["227caa0a-bdd6-4fd7-aa48-e3783b0c999d"], "operator": null, "metadata": {"aucs": [0.23105264796874714, 0.5788438540457482, 0.5755311114762478, 0.8565691844438824, 0.29370274298325083, 0.73665487437141, 0.4891007831453744, 0.5368437416522844, 0.6693856302599821, 0.651521653989239, 0.8606034323201376, 0.9956114343744128, 0.631773853381975, 0.6876532028500506, 0.9232147591403058, 0.7279690568992397, 0.554880921839808, 0.8197950924015485, 0.3242444593324073, 0.6530293016721147]}}
{"id": "2090f1a6-3df7-41f3-b337-22436fd042a2", "fitness": 0.861289619460457, "name": "NeighborhoodAdaptiveDE", "description": "Adaptive Differential Evolution with a neighborhood-based mutation and orthogonal crossover.", "code": "import numpy as np\n\nclass NeighborhoodAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, adapt_freq=50, stagnation_threshold=1000, learning_rate=0.1, neighborhood_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.adapt_freq = adapt_freq\n        self.stagnation_threshold = stagnation_threshold\n        self.learning_rate = learning_rate\n        self.neighborhood_size = neighborhood_size\n        self.f = 0.5\n        self.cr = 0.9\n        self.success_f = []\n        self.success_cr = []\n        self.best_fitness_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.last_improvement = 0\n\n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.population[best_idx]\n        self.best_fitness_history.append(self.f_opt)\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Neighborhood-based Mutation\n                neighbors = np.random.choice(np.arange(self.pop_size), self.neighborhood_size, replace=False)\n                best_neighbor_idx = neighbors[np.argmin(self.fitness[neighbors])]\n                \n                idxs = np.random.choice(np.arange(self.pop_size), 2, replace=False)\n                x1, x2 = self.population[idxs]\n\n                v = self.population[best_neighbor_idx] + self.f * (x1 - x2)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Orthogonal Crossover\n                u = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        u[j] = v[j]\n                \n                # Evaluation\n                f_new = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f_new < self.fitness[i]:\n                    self.success_f.append(self.f)\n                    self.success_cr.append(self.cr)\n                    self.fitness[i] = f_new\n                    self.population[i] = u\n\n                    if f_new < self.f_opt:\n                        self.f_opt = f_new\n                        self.x_opt = u\n                        self.best_fitness_history.append(self.f_opt)\n                        self.last_improvement = self.evals\n\n            # Adaptive Parameter Control\n            if self.evals % self.adapt_freq == 0:\n                if self.success_f:\n                    self.f = (1 - self.learning_rate) * self.f + self.learning_rate * np.mean(self.success_f)\n                    self.cr = (1 - self.learning_rate) * self.cr + self.learning_rate * np.mean(self.success_cr)\n                self.f = np.clip(self.f, 0.1, 0.9)\n                self.cr = np.clip(self.cr, 0.1, 1.0)\n                self.success_f = []\n                self.success_cr = []\n            \n            # Stagnation Check and Restart\n            if self.evals - self.last_improvement > self.stagnation_threshold:\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.evals += self.pop_size\n                best_idx = np.argmin(self.fitness)\n                self.f_opt = self.fitness[best_idx]\n                self.x_opt = self.population[best_idx]\n                self.last_improvement = self.evals\n                self.best_fitness_history.append(self.f_opt)\n\n\n            if self.evals >= self.budget:\n                break\n                    \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm NeighborhoodAdaptiveDE scored 0.861 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e80c9b07-9e4a-47ea-b07f-4479fdb1d3e3"], "operator": null, "metadata": {"aucs": [0.6767843852158779, 0.8223678378167558, 0.865327560127738, 0.9527354756820765, 0.8823093556509845, 0.9036146726216274, 0.8532564712350131, 0.8612545530146006, 0.8937926821278295, 0.8544885369849906, 0.9414145770921373, 0.9956578664302538, 0.8629289357446537, 0.8911559092474174, 0.9662509858344767, 0.9050021526585408, 0.8342250038767512, 0.9297679784913813, 0.8112421338398238, 0.52221531551621]}}
{"id": "a5d60a56-8206-443e-973b-2f55515bfa16", "fitness": 0.4045575219817151, "name": "AdaptiveDEOrthogonalCMA", "description": "Adaptive Differential Evolution with orthogonal learning and covariance matrix adaptation for parameter control.", "code": "import numpy as np\n\nclass AdaptiveDEOrthogonalCMA:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, f=0.5, cr=0.9, restart_trigger=0.1, cma_learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.f = f\n        self.cr = cr\n        self.restart_trigger = restart_trigger\n        self.archive = []\n        self.archive_fitness = []\n        self.sf_history = []\n        self.cma_learning_rate = cma_learning_rate\n        self.covariance_matrix = np.eye(dim)  # Initialize covariance matrix for CMA\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.population[best_idx]\n        \n        while self.evals < self.budget:\n            # Restart strategy\n            if self.evals > self.restart_trigger * self.budget:\n                if np.std(self.fitness) < 1e-6:\n                    self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                    self.fitness = np.array([func(x) for x in self.population])\n                    self.evals += self.pop_size\n                    best_idx = np.argmin(self.fitness)\n                    if self.fitness[best_idx] < self.f_opt:\n                        self.f_opt = self.fitness[best_idx]\n                        self.x_opt = self.population[best_idx]\n                    continue\n            \n            ranked_indices = np.argsort(self.fitness) # Rank individuals\n            \n            for i in range(self.pop_size):\n                # Mutation: Rank-based selection\n                best_idx = ranked_indices[0]  # Best individual\n                \n                idxs = np.random.choice(np.arange(self.pop_size), 2, replace=False)\n                x1, x2 = self.population[idxs]\n\n                # Orthogonal learning: generate orthogonal vector\n                orthogonal_vector = self.generate_orthogonal_vector(self.population[i])\n\n                v = self.population[best_idx] + self.f * (x1 - x2) + self.f * orthogonal_vector # Add orthogonal component\n\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = self.crossover(self.population[i], v)\n                \n                # Evaluation\n                f = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f < self.fitness[i]:\n                    # Archive update\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i])\n                        self.archive_fitness.append(self.fitness[i])\n                    else:\n                        worst_archive_idx = np.argmax(self.archive_fitness)\n                        if self.fitness[i] < self.archive_fitness[worst_archive_idx]:\n                            self.archive[worst_archive_idx] = self.population[i]\n                            self.archive_fitness[worst_archive_idx] = self.fitness[i]\n\n                    #Adaptive F update based on success - simplified update\n                    if f < self.fitness[i]:\n                        self.sf_history.append(abs(self.fitness[i] - f))\n                        if len(self.sf_history) > 50:\n                            self.sf_history.pop(0)\n\n                    self.fitness[i] = f\n                    self.population[i] = u\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n                    \n                    # Update covariance matrix (CMA-like adaptation)\n                    diff = u - self.population[i]\n                    self.covariance_matrix = (1 - self.cma_learning_rate) * self.covariance_matrix + self.cma_learning_rate * np.outer(diff, diff)\n                \n                # Adaptive Parameter Control (simplified and more frequent) based on covariance\n                if np.random.rand() < 0.3:  # Increased frequency\n                    eigenvalues = np.linalg.eigvalsh(self.covariance_matrix)\n                    self.f = np.clip(np.random.normal(0.5, np.std(eigenvalues)), 0.1, 1.0)\n                    self.cr = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0)\n\n\n                if self.evals >= self.budget:\n                    break\n        return self.f_opt, self.x_opt\n        \n    def crossover(self, x, v):\n        u = x.copy()\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.cr or j == j_rand:\n                u[j] = v[j]\n        return u\n    \n    def generate_orthogonal_vector(self, x):\n        # Generate a random orthogonal vector with respect to x\n        v = np.random.randn(self.dim)\n        v -= v.dot(x) * x / np.linalg.norm(x)**2\n        return v", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDEOrthogonalCMA scored 0.405 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["227caa0a-bdd6-4fd7-aa48-e3783b0c999d"], "operator": null, "metadata": {"aucs": [0.20474271499126728, 0.3340598242334044, 0.36878090228264515, 0.4690443218082332, 0.2960166066303386, 0.378313016466183, 0.3016571985668022, 0.3250429140565535, 0.32236918632950395, 0.19140134842595813, 0.43076264885371207, 0.9958439316579244, 0.38451151737024125, 0.3165980660407721, 0.7734198811786138, 0.39529498734269064, 0.333655342749, 0.5563198375225473, 0.2224988394658136, 0.49081735366209733]}}
{"id": "7c9b79fc-7665-4188-bc17-6b7f3309e190", "fitness": -Infinity, "name": "HybridDECMASobolV4", "description": "Hybrid DE with CMA-ES and Sobol initialization, adaptively adjusting F/CR, integrating CMA-ES, and using a simplified restart mechanism based on fitness improvement rate.", "code": "import numpy as np\nfrom scipy.linalg import sqrtm\n\nclass HybridDECMASobolV4:\n    def __init__(self, budget=10000, dim=10, pop_size=50, f_init=0.5, cr_init=0.9, cma_generations=3, restart_trigger=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.f = np.full(pop_size, f_init)\n        self.cr = np.full(pop_size, cr_init)\n        self.cma_generations = cma_generations\n        self.restart_trigger = restart_trigger\n        self.archive_f = np.full(pop_size, f_init)\n        self.archive_cr = np.full(pop_size, cr_init)\n        self.success_count = np.zeros(pop_size)\n        self.stagnation_counter = 0\n        self.best_history = []\n\n    def sobol_sequence(self, n_points, dim):\n        \"\"\"Generate n_points of Sobol sequence in dim dimensions.\"\"\"\n        try:\n            from sobol_seq import i4_sobol_generate\n            points = i4_sobol_generate(dim, n_points)\n            return points\n        except ImportError:\n            print(\"Sobol sequence library not found.  Falling back to uniform random numbers\")\n            return np.random.rand(n_points, dim)\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n        # Sobol sequence initialization\n        sobol_points = self.sobol_sequence(self.pop_size, self.dim)\n        self.population = func.bounds.lb + sobol_points * (func.bounds.ub - func.bounds.lb)\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n\n        for i in range(self.pop_size):\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.population[i]\n        \n        self.best_history.append(self.f_opt)\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(np.arange(self.pop_size), 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                v = x1 + self.f[i] * (x2 - x3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = np.random.rand(self.dim) < self.cr[i]\n                u = v * u + self.population[i] * (~u)\n                u = np.clip(u, func.bounds.lb, func.bounds.ub)\n\n                # Evaluation\n                f = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f < self.fitness[i]:\n                    # Update success history\n                    self.archive_f[i] = self.f[i]\n                    self.archive_cr[i] = self.cr[i]\n                    self.success_count[i] += 1\n\n                    self.fitness[i] = f\n                    self.population[i] = u\n                    \n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n                        self.stagnation_counter = 0\n\n\n                # Adaptation of F and CR\n                if self.success_count[i] > 0:\n                    self.f[i] = 0.8 * self.f[i] + 0.2 * np.mean(self.archive_f[self.success_count > 0]) if np.any(self.success_count > 0) else np.random.uniform(0.1, 0.9)\n                    self.cr[i] = 0.8 * self.cr[i] + 0.2 * np.mean(self.archive_cr[self.success_count > 0]) if np.any(self.success_count > 0) else np.random.uniform(0.1, 0.9)\n                else:\n                    self.f[i] = np.clip(np.random.normal(0.5, 0.3), 0.1, 1.0)\n                    self.cr[i] = np.clip(np.random.normal(0.9, 0.2), 0.1, 1.0)\n                \n                self.f[i] = np.clip(self.f[i], 0.1, 1.0)\n                self.cr[i] = np.clip(self.cr[i], 0.1, 1.0)\n\n                if self.evals >= self.budget:\n                    break\n\n            # CMA-ES component\n            if self.evals < self.budget and self.evals % (self.budget // 5) == 0:\n                self.cma_es(func)\n            \n            self.best_history.append(self.f_opt)\n\n            # Stagnation Check\n            if len(self.best_history) > 2:\n                improvement = (self.best_history[-2] - self.best_history[-1]) / abs(self.best_history[-2]) if self.best_history[-2] != 0 else 0\n                if improvement < 1e-6:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0\n\n            # Restart mechanism\n            if self.stagnation_counter > self.budget * self.restart_trigger:\n                self.population = func.bounds.lb + np.random.rand(self.pop_size, self.dim) * (func.bounds.ub - func.bounds.lb)\n                self.fitness = np.array([func(x) for x in self.population])\n                self.evals += self.pop_size\n                best_index = np.argmin(self.fitness)\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index]\n                self.best_history.append(self.f_opt)\n                self.stagnation_counter = 0\n                self.success_count = np.zeros(self.pop_size)\n                self.archive_f = np.full(self.pop_size, 0.5)\n                self.archive_cr = np.full(self.pop_size, 0.9)\n\n\n        return self.f_opt, self.x_opt\n\n    def cma_es(self, func):\n        \"\"\"Covariance Matrix Adaptation Evolution Strategy.\"\"\"\n        mean = self.x_opt.copy()\n        sigma = 0.1 * (func.bounds.ub - func.bounds.lb)\n        C = np.eye(self.dim)\n\n        generations = min(self.cma_generations, (self.budget - self.evals) // self.pop_size)\n\n        if generations <= 0:\n            return\n\n        for _ in range(generations):\n            if self.evals >= self.budget:\n                break\n\n            z = np.random.randn(self.pop_size, self.dim)\n            samples = mean + sigma * np.dot(z, sqrtm(C).T)\n            samples = np.clip(samples, func.bounds.lb, func.bounds.ub)\n            \n            fitness_values = np.array([func(x) for x in samples])\n            self.evals += self.pop_size\n            \n            idx = np.argsort(fitness_values)\n            best_samples = samples[idx]\n            best_fitness = fitness_values[idx]\n            \n            if best_fitness[0] < self.f_opt:\n                self.f_opt = best_fitness[0]\n                self.x_opt = best_samples[0]\n            \n            mean = np.mean(best_samples[:self.pop_size//2], axis=0)\n            C = np.cov(best_samples[:self.pop_size//2].T)\n            C = C + 1e-6 * np.eye(self.dim)\n\n            try:\n                sqrtm(C)\n            except np.linalg.LinAlgError:\n                C = C + 1e-5 * np.eye(self.dim)\n            \n            sigma *= np.exp(0.5 * (np.mean(best_fitness) - self.f_opt) / np.std(fitness_values))", "configspace": "", "generation": 5, "feedback": "An exception occurred: name 'sqrtm' is not defined.", "error": "", "parent_ids": ["c34149bf-cb50-43d4-a989-f869a67149fd"], "operator": null, "metadata": {}}
{"id": "04245570-5fa3-4746-8314-a017bc61bbd6", "fitness": 0.6844585571145932, "name": "HybridDECMASobolV4", "description": "Integrates a repair mechanism using the mean of successful solutions, adjusts F/CR adaptation based on a decay factor, and incorporates a mirrored boundary handling strategy to enhance boundary exploitation.", "code": "import numpy as np\nfrom scipy.linalg import sqrtm\n\nclass HybridDECMASobolV4:\n    def __init__(self, budget=10000, dim=10, pop_size=50, f_init=0.5, cr_init=0.9, neighborhood_size=0.1, cma_generations=5, restart_trigger=0.2, decay_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.f = np.full(pop_size, f_init)  # Individual mutation factors\n        self.cr = np.full(pop_size, cr_init) # Individual crossover rates\n        self.neighborhood_size = neighborhood_size\n        self.cma_generations = cma_generations\n        self.learning_rate = 0.1  # Adjusts the adaptation rate\n        self.cma_evals = 0\n        self.success_f = np.zeros(pop_size)\n        self.success_cr = np.zeros(pop_size)\n        self.success_count = np.zeros(pop_size)\n        self.restart_trigger = restart_trigger\n        self.archive_f = np.full(pop_size, f_init)\n        self.archive_cr = np.full(pop_size, cr_init)\n        self.decay_rate = decay_rate  # Decay rate for F/CR adaptation\n\n    def sobol_sequence(self, n_points, dim):\n        \"\"\"Generate n_points of Sobol sequence in dim dimensions.\"\"\"\n        try:\n            from sobol_seq import i4_sobol_generate\n            points = i4_sobol_generate(dim, n_points)\n            return points\n        except ImportError:\n            print(\"Sobol sequence library not found.  Falling back to uniform random numbers\")\n            return np.random.rand(n_points, dim)\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.best_history = []\n        self.stagnation_counter = 0\n        \n        # Sobol sequence initialization\n        sobol_points = self.sobol_sequence(self.pop_size, self.dim)\n        self.population = func.bounds.lb + sobol_points * (func.bounds.ub - func.bounds.lb)\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n\n        for i in range(self.pop_size):\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.population[i]\n        \n        self.best_history.append(self.f_opt)\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(np.arange(self.pop_size), 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                v = x1 + self.f[i] * (x2 - x3)\n\n                # Mirrored boundary handling\n                v = np.where(v < func.bounds.lb, 2 * func.bounds.lb - v, v)\n                v = np.where(v > func.bounds.ub, 2 * func.bounds.ub - v, v)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = np.random.rand(self.dim) < self.cr[i]\n                u = v * u + self.population[i] * (~u)\n                \n                # Mirrored boundary handling\n                u = np.where(u < func.bounds.lb, 2 * func.bounds.lb - u, u)\n                u = np.where(u > func.bounds.ub, 2 * func.bounds.ub - u, u)\n                u = np.clip(u, func.bounds.lb, func.bounds.ub)\n\n                # Evaluation\n                f = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f < self.fitness[i]:\n                    # Update success history\n                    self.archive_f[i] = self.f[i]\n                    self.archive_cr[i] = self.cr[i]\n                    self.success_count[i] += 1\n\n                    self.fitness[i] = f\n                    self.population[i] = u\n                    \n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n                        self.stagnation_counter = 0  # Reset stagnation counter\n                \n                # Adaptation of F and CR based on success history\n                if self.success_count[i] > 0:\n                    successful_f = self.archive_f[self.success_count > 0]\n                    successful_cr = self.archive_cr[self.success_count > 0]\n                    \n                    # Weighted average with more weight on recent values\n                    self.f[i] = self.decay_rate * self.f[i] + (1 - self.decay_rate) * np.mean(successful_f) if successful_f.size > 0 else np.random.uniform(0.1, 0.9)\n                    self.cr[i] = self.decay_rate * self.cr[i] + (1 - self.decay_rate) * np.mean(successful_cr) if successful_cr.size > 0 else np.random.uniform(0.1, 0.9)\n                else:\n                    self.f[i] = np.clip(np.random.normal(0.5, 0.3), 0.1, 1.0)\n                    self.cr[i] = np.clip(np.random.normal(0.9, 0.2), 0.1, 1.0)\n                \n                self.f[i] = np.clip(self.f[i], 0.1, 1.0)\n                self.cr[i] = np.clip(self.cr[i], 0.1, 1.0)\n                \n                if self.evals >= self.budget:\n                    break\n            \n            # CMA-ES component, triggered periodically\n            if self.evals < self.budget and self.evals % (self.budget // 5) == 0:\n                self.cma_es(func)\n\n            # Dynamic Neighborhood Search around best solution\n            if self.evals < self.budget:\n                # Adjust neighborhood size based on stagnation\n                if self.stagnation_counter > self.budget * 0.05:  # More stagnation, smaller neighborhood\n                    self.neighborhood_size = max(0.01, self.neighborhood_size * 0.9)\n                else:  # Less stagnation, larger neighborhood\n                    self.neighborhood_size = min(0.5, self.neighborhood_size * 1.1)\n\n                x_neighbor = self.x_opt + np.random.uniform(-self.neighborhood_size, self.neighborhood_size, size=self.dim)\n                \n                # Mirrored boundary handling\n                x_neighbor = np.where(x_neighbor < func.bounds.lb, 2 * func.bounds.lb - x_neighbor, x_neighbor)\n                x_neighbor = np.where(x_neighbor > func.bounds.ub, 2 * func.bounds.ub - x_neighbor, x_neighbor)\n                x_neighbor = np.clip(x_neighbor, func.bounds.lb, func.bounds.ub)\n\n                f_neighbor = func(x_neighbor)\n                self.evals += 1\n\n                if f_neighbor < self.f_opt:\n                    self.f_opt = f_neighbor\n                    self.x_opt = x_neighbor\n                    self.stagnation_counter = 0 # Reset stagnation counter\n            \n            self.best_history.append(self.f_opt)\n            if len(self.best_history) > 1:\n                if self.best_history[-1] >= self.best_history[-2]:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0\n\n            # Restart mechanism\n            if self.stagnation_counter > self.budget * self.restart_trigger:\n                self.population = func.bounds.lb + np.random.rand(self.pop_size, self.dim) * (func.bounds.ub - func.bounds.lb)\n                self.fitness = np.array([func(x) for x in self.population])\n                self.evals += self.pop_size\n                self.f_opt = np.min(self.fitness)\n                self.x_opt = self.population[np.argmin(self.fitness)]\n                self.stagnation_counter = 0\n                self.success_f = np.zeros(self.pop_size)\n                self.success_cr = np.zeros(self.pop_size)\n                self.success_count = np.zeros(self.pop_size)\n                self.archive_f = np.full(self.pop_size, 0.5)\n                self.archive_cr = np.full(self.pop_size, 0.9)\n                print(\"Restarting population...\")\n\n        \n        return self.f_opt, self.x_opt\n\n    def cma_es(self, func):\n        \"\"\"Covariance Matrix Adaptation Evolution Strategy.\"\"\"\n        mean = self.x_opt.copy()\n        sigma = 0.1 * (func.bounds.ub - func.bounds.lb)  # Initial step size\n        C = np.eye(self.dim)  # Initial covariance matrix\n        \n        # Adapt the number of generations based on remaining budget\n        generations = min(self.cma_generations, (self.budget - self.evals) // self.pop_size)\n\n        if generations <= 0:\n            return\n\n        for _ in range(generations):\n            if self.evals >= self.budget:\n                break\n                \n            # Generate samples from multivariate normal distribution\n            z = np.random.randn(self.pop_size, self.dim)\n            samples = mean + sigma * np.dot(z, sqrtm(C).T)\n            \n            # Mirrored boundary handling\n            samples = np.where(samples < func.bounds.lb, 2 * func.bounds.lb - samples, samples)\n            samples = np.where(samples > func.bounds.ub, 2 * func.bounds.ub - samples, samples)\n            samples = np.clip(samples, func.bounds.lb, func.bounds.ub)\n            \n            # Evaluate samples\n            fitness_values = np.array([func(x) for x in samples])\n            self.evals += self.pop_size\n            \n            # Sort by fitness\n            idx = np.argsort(fitness_values)\n            best_samples = samples[idx]\n            best_fitness = fitness_values[idx]\n            \n            # Update best solution\n            if best_fitness[0] < self.f_opt:\n                self.f_opt = best_fitness[0]\n                self.x_opt = best_samples[0]\n            \n            # Update mean\n            mean = np.mean(best_samples[:self.pop_size//2], axis=0) # Use top 50%\n            \n            # Update covariance matrix (simplified)\n            C = np.cov(best_samples[:self.pop_size//2].T)  # Use top 50%\n\n            # Regularize covariance matrix\n            C = C + 1e-6 * np.eye(self.dim)\n\n            # Ensure that C is positive definite\n            try:\n                sqrtm(C)\n            except np.linalg.LinAlgError:\n                C = C + 1e-5 * np.eye(self.dim)  # Add a small diagonal to make it positive definite\n            \n            # Adjust sigma\n            sigma *= np.exp(0.5 * (np.mean(best_fitness) - self.f_opt) / np.std(fitness_values)) # Damping", "configspace": "", "generation": 5, "feedback": "The algorithm HybridDECMASobolV4 scored 0.684 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c34149bf-cb50-43d4-a989-f869a67149fd"], "operator": null, "metadata": {"aucs": [0.3809745733636938, 0.605591464266162, 0.658801829716104, 0.8688124761995453, 0.7034203223189915, 0.7776396090357243, 0.5610561515012933, 0.6863810931613208, 0.7278244480104057, 0.6498890011793635, 0.8544548951874935, 0.9943766376146002, 0.6013318373186605, 0.7339869792557185, 0.9059741733556129, 0.7564551635138758, 0.5932018886168531, 0.8315226445227308, 0.2176395157894606, 0.5798364383642536]}}
{"id": "ebcc4ce3-5865-4121-a571-e2c2b015c7e6", "fitness": -Infinity, "name": "DEQuadInterpolation", "description": "Implements a population-based algorithm that combines Differential Evolution with a Quadratic Interpolation based local search and dynamically adjusts search parameters based on performance feedback and population diversity.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DEQuadInterpolation:\n    def __init__(self, budget=10000, dim=10, pop_size=50, f_init=0.5, cr_init=0.9, local_search_prob=0.1, diversity_threshold=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.f = np.full(pop_size, f_init)\n        self.cr = np.full(pop_size, cr_init)\n        self.local_search_prob = local_search_prob\n        self.diversity_threshold = diversity_threshold\n        self.population = None\n        self.fitness = None\n        self.best_fitness_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n        \n        # Initialize population\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals += self.pop_size\n        \n        # Update optimal fitness\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.best_fitness_history.append(self.f_opt)\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Differential Evolution Mutation\n                idxs = np.random.choice(np.arange(self.pop_size), 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                v = self.population[i] + self.f[i] * (x2 - x3) # Use current individual as base\n\n                # Boundary Handling\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                u = np.random.rand(self.dim) < self.cr[i]\n                u = v * u + self.population[i] * (~u)\n                u = np.clip(u, func.bounds.lb, func.bounds.ub)\n\n                # Evaluate\n                f = func(u)\n                self.evals += 1\n\n                # Selection\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.population[i] = u\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n                \n                # Local Search (Quadratic Interpolation)\n                if np.random.rand() < self.local_search_prob and self.evals < self.budget:\n                    \n                    def obj_func(x):\n                        return func(x)\n                    \n                    bounds = [(func.bounds.lb, func.bounds.ub) for _ in range(self.dim)]\n                    \n                    res = minimize(obj_func, self.population[i], method='L-BFGS-B', bounds=bounds)\n                    \n                    if res.fun < self.fitness[i]:\n                        self.fitness[i] = res.fun\n                        self.population[i] = res.x\n                        self.evals += res.nfev\n\n                        if res.fun < self.f_opt:\n                            self.f_opt = res.fun\n                            self.x_opt = res.x\n                \n                if self.evals >= self.budget:\n                    break\n            \n            # Adaptive Parameter Control (F and CR)\n            if len(self.best_fitness_history) > 5:\n                if self.best_fitness_history[-1] >= self.best_fitness_history[-2]: # Stagnation\n                   self.f *= 1.1 # Increase exploration \n                   self.cr *= 0.9 # Reduce exploitation\n                else:\n                   self.f *= 0.9 # Reduce exploration\n                   self.cr *= 1.1 # Increase exploitation\n                   \n                self.f = np.clip(self.f, 0.1, 1.0)\n                self.cr = np.clip(self.cr, 0.1, 1.0)\n\n            # Diversity Control (Restart if necessary)\n            if self.population_diversity() < self.diversity_threshold:\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.evals += self.pop_size\n                self.f_opt = np.min(self.fitness)\n                self.x_opt = self.population[np.argmin(self.fitness)]\n            \n            self.best_fitness_history.append(self.f_opt)\n            \n        return self.f_opt, self.x_opt\n\n    def population_diversity(self):\n        \"\"\"Calculates the diversity of the population based on the average distance from the centroid.\"\"\"\n        centroid = np.mean(self.population, axis=0)\n        distances = np.linalg.norm(self.population - centroid, axis=1)\n        return np.mean(distances)", "configspace": "", "generation": 6, "feedback": "An exception occurred: name 'minimize' is not defined.", "error": "", "parent_ids": ["04245570-5fa3-4746-8314-a017bc61bbd6"], "operator": null, "metadata": {}}
{"id": "c557fa64-e2b2-4c2b-b334-56e9d6e2393a", "fitness": 0.0, "name": "AdaptivePopulationDE", "description": "Adaptively adjusts both the population size and mutation strategy of Differential Evolution based on the observed fitness landscape and stagnation detection.", "code": "import numpy as np\n\nclass AdaptivePopulationDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, min_pop_size=10, max_pop_size=100, adapt_freq=50, stagnation_threshold=500, learning_rate=0.1, expansion_rate=1.2, contraction_rate=0.8, mutation_strategy=\"rand1bin\"):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = initial_pop_size\n        self.pop_size = initial_pop_size\n        self.min_pop_size = min_pop_size\n        self.max_pop_size = max_pop_size\n        self.adapt_freq = adapt_freq\n        self.stagnation_threshold = stagnation_threshold\n        self.learning_rate = learning_rate\n        self.expansion_rate = expansion_rate\n        self.contraction_rate = contraction_rate\n        self.f = 0.5\n        self.cr = 0.9\n        self.success_f = []\n        self.success_cr = []\n        self.best_fitness_history = []\n        self.mutation_strategy = mutation_strategy\n        self.last_improvement = 0\n        self.evals = 0\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals += self.pop_size\n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.population[best_idx]\n        self.best_fitness_history.append(self.f_opt)\n        self.last_improvement = 0\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                if self.mutation_strategy == \"rand1bin\":\n                    idxs = np.random.choice(np.arange(self.pop_size), 3, replace=False)\n                    x1, x2, x3 = self.population[idxs]\n                    v = x1 + self.f * (x2 - x3)\n                elif self.mutation_strategy == \"best1bin\":\n                    best_idx = np.argmin(self.fitness)\n                    idxs = np.random.choice(np.arange(self.pop_size), 2, replace=False)\n                    x1, x2 = self.population[idxs]\n                    v = self.population[best_idx] + self.f * (x1 - x2)\n                else:  # \"current_to_rand1\"\n                    idxs = np.random.choice(np.arange(self.pop_size), 2, replace=False)\n                    x1, x2 = self.population[idxs]\n                    v = self.population[i] + self.f * (x1 - x2) + self.f * (np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim) - self.population[i])\n\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                u = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        u[j] = v[j]\n\n                # Evaluation\n                f_new = func(u)\n                self.evals += 1\n\n                # Selection\n                if f_new < self.fitness[i]:\n                    self.success_f.append(self.f)\n                    self.success_cr.append(self.cr)\n                    self.fitness[i] = f_new\n                    self.population[i] = u\n\n                    if f_new < self.f_opt:\n                        self.f_opt = f_new\n                        self.x_opt = u\n                        self.best_fitness_history.append(self.f_opt)\n                        self.last_improvement = self.evals\n            \n            #Adaptive Population Size\n            if self.evals % self.adapt_freq == 0:\n                if self.evals - self.last_improvement > self.stagnation_threshold:\n                    self.pop_size = int(max(self.min_pop_size, self.pop_size * self.expansion_rate))\n                    if self.pop_size > self.max_pop_size:\n                        self.pop_size = self.max_pop_size\n\n                    new_population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size - self.population.shape[0], self.dim))\n                    self.population = np.vstack((self.population, new_population))\n                    new_fitness = np.array([func(x) for x in new_population])\n                    self.fitness = np.concatenate((self.fitness, new_fitness))\n                    self.evals += new_population.shape[0]\n                else:\n                     self.pop_size = int(self.pop_size * self.contraction_rate)\n                     if self.pop_size < self.min_pop_size:\n                         self.pop_size = self.min_pop_size\n\n                     best_indices = np.argsort(self.fitness)[:self.pop_size]\n                     self.population = self.population[best_indices]\n                     self.fitness = self.fitness[best_indices]\n\n            if self.evals % self.adapt_freq == 0:\n                if self.success_f:\n                    self.f = (1 - self.learning_rate) * self.f + self.learning_rate * np.mean(self.success_f)\n                    self.cr = (1 - self.learning_rate) * self.cr + self.learning_rate * np.mean(self.success_cr)\n                self.f = np.clip(self.f, 0.1, 0.9)\n                self.cr = np.clip(self.cr, 0.1, 1.0)\n                self.success_f = []\n                self.success_cr = []\n            \n            if self.evals >= self.budget:\n                break\n                    \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptivePopulationDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2090f1a6-3df7-41f3-b337-22436fd042a2"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "818628bf-865c-4a42-86bc-b285893c361c", "fitness": 0.0, "name": "AdaptiveDE", "description": "Adaptive DE with a mirrored archive, periodic population rejuvenation, and covariance matrix adaptation for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, f=0.5, cr=0.9, rejuvenation_rate=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.f = f\n        self.cr = cr\n        self.archive = []\n        self.archive_fitness = []\n        self.rejuvenation_rate = rejuvenation_rate\n        self.sf_history = []\n        self.success_count = 0\n        self.population = None\n        self.fitness = None\n        self.evals = 0\n        self.covariance_matrix = np.eye(dim) * 0.1  # Initialize covariance matrix\n        self.learning_rate = 0.1 #for covariance matrix update\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.population[best_idx]\n        \n        while self.evals < self.budget:\n            ranked_indices = np.argsort(self.fitness) # Rank individuals\n            \n            for i in range(self.pop_size):\n                # Mutation: Rank-based selection\n                best_idx = ranked_indices[0]  # Best individual\n\n                idxs = np.random.choice(np.arange(self.pop_size), 2, replace=False)\n                x1, x2 = self.population[idxs]\n\n                # Sample mutation vector from a multivariate normal distribution\n                mutation_vector = np.random.multivariate_normal(np.zeros(self.dim), self.covariance_matrix)\n                v = self.population[best_idx] + self.f * (x1 - x2) + mutation_vector\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = self.crossover(self.population[i], v)\n                \n                # Evaluation\n                f = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f < self.fitness[i]:\n                    delta = u - self.population[i]\n                    self.covariance_matrix = (1 - self.learning_rate) * self.covariance_matrix + self.learning_rate * np.outer(delta, delta)\n                    # Archive update\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i])\n                        self.archive_fitness.append(self.fitness[i])\n                    else:\n                        worst_archive_idx = np.argmax(self.archive_fitness)\n                        if f < self.archive_fitness[worst_archive_idx]:\n                            #Mirrored Archive Update\n                            mirrored_x = 2 * np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim) - self.population[i] #simple mirroring\n                            mirrored_x = np.clip(mirrored_x, func.bounds.lb, func.bounds.ub)\n                            mirrored_f = func(mirrored_x)\n                            if mirrored_f < self.archive_fitness[worst_archive_idx]:\n                                self.archive[worst_archive_idx] = mirrored_x\n                                self.archive_fitness[worst_archive_idx] = mirrored_f\n\n                            self.archive[worst_archive_idx] = self.population[i]\n                            self.archive_fitness[worst_archive_idx] = self.fitness[i]\n\n                    self.fitness[i] = f\n                    self.population[i] = u\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n                    self.success_count += 1\n                else:\n                    self.success_count = 0\n                \n                #Adaptive F update based on success\n                if self.success_count > 5:\n                    self.f = np.clip(self.f * 0.95, 0.1, 0.9)\n                    self.success_count = 0\n                else:\n                    self.f = np.clip(self.f * 1.05, 0.1, 0.9)\n\n                if np.random.rand() < 0.1: # Adaptive CR\n                    self.cr = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0)\n\n                if self.evals >= self.budget:\n                    break\n            # Population Rejuvenation\n            if np.random.rand() < self.rejuvenation_rate:\n                num_rejuvenate = int(self.pop_size * 0.1)\n                worst_indices = np.argsort(self.fitness)[-num_rejuvenate:]\n                self.population[worst_indices] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(num_rejuvenate, self.dim))\n                self.fitness[worst_indices] = [func(x) for x in self.population[worst_indices]]\n                self.evals += num_rejuvenate\n\n        return self.f_opt, self.x_opt\n        \n    def crossover(self, x, v):\n        u = x.copy()\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.cr or j == j_rand:\n                u[j] = v[j]\n        return u", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["388eef40-3c24-449e-8db5-9bc282696841"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "9b8e587d-666c-4c22-ad65-f0292fa8479f", "fitness": -Infinity, "name": "HybridDECMASobolV5", "description": "Integrates a success-history-based parameter adaptation in DE with a CMA-ES component, Sobol initialization, mirrored boundary handling, and population diversity maintenance through a Cauchy mutation operator for enhanced exploration.", "code": "import numpy as np\nfrom scipy.linalg import sqrtm\nfrom scipy.stats import cauchy\n\nclass HybridDECMASobolV5:\n    def __init__(self, budget=10000, dim=10, pop_size=50, f_init=0.5, cr_init=0.9, neighborhood_size=0.1, cma_generations=5, restart_trigger=0.2, decay_rate=0.95, cauchy_scale=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.f = np.full(pop_size, f_init)  # Individual mutation factors\n        self.cr = np.full(pop_size, cr_init) # Individual crossover rates\n        self.neighborhood_size = neighborhood_size\n        self.cma_generations = cma_generations\n        self.learning_rate = 0.1  # Adjusts the adaptation rate\n        self.cma_evals = 0\n        self.success_f = np.zeros(pop_size)\n        self.success_cr = np.zeros(pop_size)\n        self.success_count = np.zeros(pop_size)\n        self.restart_trigger = restart_trigger\n        self.archive_f = np.full(pop_size, f_init)\n        self.archive_cr = np.full(pop_size, cr_init)\n        self.decay_rate = decay_rate  # Decay rate for F/CR adaptation\n        self.cauchy_scale = cauchy_scale # Scale parameter for Cauchy mutation\n\n    def sobol_sequence(self, n_points, dim):\n        \"\"\"Generate n_points of Sobol sequence in dim dimensions.\"\"\"\n        try:\n            from sobol_seq import i4_sobol_generate\n            points = i4_sobol_generate(dim, n_points)\n            return points\n        except ImportError:\n            print(\"Sobol sequence library not found.  Falling back to uniform random numbers\")\n            return np.random.rand(n_points, dim)\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.best_history = []\n        self.stagnation_counter = 0\n        \n        # Sobol sequence initialization\n        sobol_points = self.sobol_sequence(self.pop_size, self.dim)\n        self.population = func.bounds.lb + sobol_points * (func.bounds.ub - func.bounds.lb)\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n\n        for i in range(self.pop_size):\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.population[i]\n        \n        self.best_history.append(self.f_opt)\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(np.arange(self.pop_size), 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                \n                # Cauchy mutation for diversity\n                cauchy_mutation = self.cauchy_scale * cauchy.rvs(loc=0, scale=1, size=self.dim)\n                v = x1 + self.f[i] * (x2 - x3) + cauchy_mutation\n                \n\n                # Mirrored boundary handling\n                v = np.where(v < func.bounds.lb, 2 * func.bounds.lb - v, v)\n                v = np.where(v > func.bounds.ub, 2 * func.bounds.ub - v, v)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = np.random.rand(self.dim) < self.cr[i]\n                u = v * u + self.population[i] * (~u)\n                \n                # Mirrored boundary handling\n                u = np.where(u < func.bounds.lb, 2 * func.bounds.lb - u, u)\n                u = np.where(u > func.bounds.ub, 2 * func.bounds.ub - u, u)\n                u = np.clip(u, func.bounds.lb, func.bounds.ub)\n\n                # Evaluation\n                f = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f < self.fitness[i]:\n                    # Update success history\n                    self.archive_f[i] = self.f[i]\n                    self.archive_cr[i] = self.cr[i]\n                    self.success_count[i] += 1\n\n                    self.fitness[i] = f\n                    self.population[i] = u\n                    \n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n                        self.stagnation_counter = 0  # Reset stagnation counter\n                \n                # Adaptation of F and CR based on success history\n                if self.success_count[i] > 0:\n                    successful_f = self.archive_f[self.success_count > 0]\n                    successful_cr = self.archive_cr[self.success_count > 0]\n                    \n                    # Weighted average with more weight on recent values\n                    self.f[i] = self.decay_rate * self.f[i] + (1 - self.decay_rate) * np.mean(successful_f) if successful_f.size > 0 else np.random.uniform(0.1, 0.9)\n                    self.cr[i] = self.decay_rate * self.cr[i] + (1 - self.decay_rate) * np.mean(successful_cr) if successful_cr.size > 0 else np.random.uniform(0.1, 0.9)\n                else:\n                    self.f[i] = np.clip(np.random.normal(0.5, 0.3), 0.1, 1.0)\n                    self.cr[i] = np.clip(np.random.normal(0.9, 0.2), 0.1, 1.0)\n                \n                self.f[i] = np.clip(self.f[i], 0.1, 1.0)\n                self.cr[i] = np.clip(self.cr[i], 0.1, 1.0)\n                \n                if self.evals >= self.budget:\n                    break\n            \n            # CMA-ES component, triggered periodically\n            if self.evals < self.budget and self.evals % (self.budget // 5) == 0:\n                self.cma_es(func)\n\n            # Dynamic Neighborhood Search around best solution\n            if self.evals < self.budget:\n                # Adjust neighborhood size based on stagnation\n                if self.stagnation_counter > self.budget * 0.05:  # More stagnation, smaller neighborhood\n                    self.neighborhood_size = max(0.01, self.neighborhood_size * 0.9)\n                else:  # Less stagnation, larger neighborhood\n                    self.neighborhood_size = min(0.5, self.neighborhood_size * 1.1)\n\n                x_neighbor = self.x_opt + np.random.uniform(-self.neighborhood_size, self.neighborhood_size, size=self.dim)\n                \n                # Mirrored boundary handling\n                x_neighbor = np.where(x_neighbor < func.bounds.lb, 2 * func.bounds.lb - x_neighbor, x_neighbor)\n                x_neighbor = np.where(x_neighbor > func.bounds.ub, 2 * func.bounds.ub - x_neighbor, x_neighbor)\n                x_neighbor = np.clip(x_neighbor, func.bounds.lb, func.bounds.ub)\n\n                f_neighbor = func(x_neighbor)\n                self.evals += 1\n\n                if f_neighbor < self.f_opt:\n                    self.f_opt = f_neighbor\n                    self.x_opt = x_neighbor\n                    self.stagnation_counter = 0 # Reset stagnation counter\n            \n            self.best_history.append(self.f_opt)\n            if len(self.best_history) > 1:\n                if self.best_history[-1] >= self.best_history[-2]:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0\n\n            # Restart mechanism\n            if self.stagnation_counter > self.budget * self.restart_trigger:\n                self.population = func.bounds.lb + np.random.rand(self.pop_size, self.dim) * (func.bounds.ub - func.bounds.lb)\n                self.fitness = np.array([func(x) for x in self.population])\n                self.evals += self.pop_size\n                self.f_opt = np.min(self.fitness)\n                self.x_opt = self.population[np.argmin(self.fitness)]\n                self.stagnation_counter = 0\n                self.success_f = np.zeros(self.pop_size)\n                self.success_cr = np.zeros(self.pop_size)\n                self.success_count = np.zeros(self.pop_size)\n                self.archive_f = np.full(self.pop_size, 0.5)\n                self.archive_cr = np.full(self.pop_size, 0.9)\n                print(\"Restarting population...\")\n\n        \n        return self.f_opt, self.x_opt\n\n    def cma_es(self, func):\n        \"\"\"Covariance Matrix Adaptation Evolution Strategy.\"\"\"\n        mean = self.x_opt.copy()\n        sigma = 0.1 * (func.bounds.ub - func.bounds.lb)  # Initial step size\n        C = np.eye(self.dim)  # Initial covariance matrix\n        \n        # Adapt the number of generations based on remaining budget\n        generations = min(self.cma_generations, (self.budget - self.evals) // self.pop_size)\n\n        if generations <= 0:\n            return\n\n        for _ in range(generations):\n            if self.evals >= self.budget:\n                break\n                \n            # Generate samples from multivariate normal distribution\n            z = np.random.randn(self.pop_size, self.dim)\n            samples = mean + sigma * np.dot(z, sqrtm(C).T)\n            \n            # Mirrored boundary handling\n            samples = np.where(samples < func.bounds.lb, 2 * func.bounds.lb - samples, samples)\n            samples = np.where(samples > func.bounds.ub, 2 * func.bounds.ub - samples, samples)\n            samples = np.clip(samples, func.bounds.lb, func.bounds.ub)\n            \n            # Evaluate samples\n            fitness_values = np.array([func(x) for x in samples])\n            self.evals += self.pop_size\n            \n            # Sort by fitness\n            idx = np.argsort(fitness_values)\n            best_samples = samples[idx]\n            best_fitness = fitness_values[idx]\n            \n            # Update best solution\n            if best_fitness[0] < self.f_opt:\n                self.f_opt = best_fitness[0]\n                self.x_opt = best_samples[0]\n            \n            # Update mean\n            mean = np.mean(best_samples[:self.pop_size//2], axis=0) # Use top 50%\n            \n            # Update covariance matrix (simplified)\n            C = np.cov(best_samples[:self.pop_size//2].T)  # Use top 50%\n\n            # Regularize covariance matrix\n            C = C + 1e-6 * np.eye(self.dim)\n\n            # Ensure that C is positive definite\n            try:\n                sqrtm(C)\n            except np.linalg.LinAlgError:\n                C = C + 1e-5 * np.eye(self.dim)  # Add a small diagonal to make it positive definite\n            \n            # Adjust sigma\n            sigma *= np.exp(0.5 * (np.mean(best_fitness) - self.f_opt) / np.std(fitness_values)) # Damping", "configspace": "", "generation": 6, "feedback": "An exception occurred: name 'cauchy' is not defined.", "error": "", "parent_ids": ["04245570-5fa3-4746-8314-a017bc61bbd6"], "operator": null, "metadata": {}}
{"id": "93dcfaed-83d2-4ecc-b6c4-93fcfb395a6c", "fitness": 0.0, "name": "SOMAdaptiveDE", "description": "Adaptive Differential Evolution with a self-organizing map (SOM) for population diversity and dynamic parameter control based on SOM neuron fitness.", "code": "import numpy as np\n\nclass SOMAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, adapt_freq=50, som_grid_size=10, learning_rate=0.1, sigma=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.adapt_freq = adapt_freq\n        self.som_grid_size = som_grid_size\n        self.learning_rate = learning_rate\n        self.sigma = sigma  # Initial SOM neighborhood radius\n        self.f = 0.5\n        self.cr = 0.9\n        self.success_f = []\n        self.success_cr = []\n        self.som = np.random.rand(som_grid_size, som_grid_size, dim)  # SOM neurons\n        self.neuron_fitness = np.zeros((som_grid_size, som_grid_size))\n        self.evals = 0\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n\n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.population[best_idx]\n\n        # Initialize SOM with population data\n        for i in range(self.pop_size):\n            self.update_som(self.population[i], func)\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # DE Mutation\n                idxs = np.random.choice(np.arange(self.pop_size), 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n\n                # Parameter adaptation based on SOM neuron fitness\n                bmu_row, bmu_col = self.find_bmu(self.population[i])\n                f_neuron = self.neuron_fitness[bmu_row, bmu_col]\n                cr_neuron = 1.0 - f_neuron  # Example: Higher fitness -> lower CR\n                \n                # Dynamic parameter adaptation based on neuron fitness\n                adapted_f = self.f * (0.5 + f_neuron)\n                adapted_cr = self.cr * (0.5 + cr_neuron)\n                adapted_f = np.clip(adapted_f, 0.1, 0.9)\n                adapted_cr = np.clip(adapted_cr, 0.1, 1.0)\n                \n                v = x1 + adapted_f * (x2 - x3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                u = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < adapted_cr or j == j_rand:\n                        u[j] = v[j]\n\n                # Evaluation\n                f_new = func(u)\n                self.evals += 1\n\n                # Selection\n                if f_new < self.fitness[i]:\n                    self.success_f.append(adapted_f)\n                    self.success_cr.append(adapted_cr)\n                    self.fitness[i] = f_new\n                    self.population[i] = u\n                    self.update_som(u, func) # Update SOM with successful solution\n\n                    if f_new < self.f_opt:\n                        self.f_opt = f_new\n                        self.x_opt = u\n\n            # Adaptive Parameter Control (global)\n            if self.evals % self.adapt_freq == 0:\n                if self.success_f:\n                    self.f = (1 - self.learning_rate) * self.f + self.learning_rate * np.mean(self.success_f)\n                    self.cr = (1 - self.learning_rate) * self.cr + self.learning_rate * np.mean(self.success_cr)\n                self.f = np.clip(self.f, 0.1, 0.9)\n                self.cr = np.clip(self.cr, 0.1, 1.0)\n                self.success_f = []\n                self.success_cr = []\n\n            # Decrease SOM neighborhood radius (linearly)\n            self.sigma = max(0.1, 1.0 - (self.evals / self.budget))  \n\n            if self.evals >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt\n    \n    def find_bmu(self, x):\n        \"\"\"Find the Best Matching Unit (BMU) in the SOM for a given vector x.\"\"\"\n        min_dist = np.Inf\n        bmu_row = -1\n        bmu_col = -1\n        for i in range(self.som_grid_size):\n            for j in range(self.som_grid_size):\n                dist = np.linalg.norm(x - self.som[i, j])\n                if dist < min_dist:\n                    min_dist = dist\n                    bmu_row, bmu_col = i, j\n        return bmu_row, bmu_col\n\n    def update_som(self, x, func):\n        \"\"\"Update the SOM based on the input vector x.\"\"\"\n        bmu_row, bmu_col = self.find_bmu(x)\n\n        # Update the neighborhood of the BMU\n        for i in range(self.som_grid_size):\n            for j in range(self.som_grid_size):\n                dist = np.sqrt((i - bmu_row)**2 + (j - bmu_col)**2)\n                if dist <= self.sigma:\n                    influence = np.exp(-dist**2 / (2 * self.sigma**2))\n                    self.som[i, j] += self.learning_rate * influence * (x - self.som[i, j])\n\n        # Update Neuron Fitness (Inverse of the evaluation of the SOM)\n        self.neuron_fitness[bmu_row, bmu_col] = 1.0 / (1.0 + func(self.som[bmu_row, bmu_col]))", "configspace": "", "generation": 6, "feedback": "The algorithm SOMAdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2090f1a6-3df7-41f3-b337-22436fd042a2"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "034ab5a9-0ff1-4538-a3df-4cae2effa984", "fitness": 0.5592800501611878, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with simplified adaptation and enhanced exploration by incorporating a jitter-based mutation and a binomial crossover.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.f = 0.5  # Mutation factor\n        self.cr = 0.9 # Crossover rate\n        self.jitter = 0.01\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        \n        for i in range(self.pop_size):\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.population[i]\n        \n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation with jitter\n                idxs = np.random.choice(np.arange(self.pop_size), 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                v = x1 + self.f * (x2 - x3) + self.jitter * np.random.randn(self.dim)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Binomial Crossover\n                u = self.crossover(self.population[i], v)\n                \n                # Evaluation\n                f = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.population[i] = u\n                    \n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n                \n                # Adaptive Parameter Control: Simplified update, only after successful update\n                if np.random.rand() < 0.05: # Reduced frequency of adaptation\n                    self.f = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n                    self.cr = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0)\n                if self.evals >= self.budget:\n                    break\n                    \n        return self.f_opt, self.x_opt\n        \n    def crossover(self, x, v):\n        u = x.copy()\n        for j in range(self.dim):\n            if np.random.rand() < self.cr:\n                u[j] = v[j]\n        return u", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveDE scored 0.559 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["81618a0e-283c-4b95-a231-637a46509895"], "operator": null, "metadata": {"aucs": [0.24773743441917995, 0.5280690027615214, 0.48559681038614155, 0.8443333927813226, 0.5037706359108396, 0.5783601925687685, 0.39070695836619995, 0.4358545539812617, 0.509482778437137, 0.4389819987353034, 0.8481771440445159, 0.9980997536864665, 0.3875685063003892, 0.5228914094570407, 0.9231867545939543, 0.5792152703395854, 0.43853704099756974, 0.7219818797853961, 0.27849504925608204, 0.5245544364150796]}}
{"id": "7d8f7f64-708a-463c-95d0-2b80c3c9212c", "fitness": 0.46836839570292754, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with simplified parameter adaptation and rank-based mutation, using a reduced archive and simplified crossover.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=5, f=0.5, cr=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.f = f\n        self.cr = cr\n        self.archive = []\n        self.archive_fitness = []\n        self.sf_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.population[best_idx]\n        \n        while self.evals < self.budget:\n            ranked_indices = np.argsort(self.fitness) # Rank individuals\n            \n            for i in range(self.pop_size):\n                # Mutation: Rank-based selection\n                best_idx = ranked_indices[0]  # Best individual\n\n                idxs = np.random.choice(np.arange(self.pop_size), 2, replace=False)\n                x1, x2 = self.population[idxs]\n                \n                v = self.population[best_idx] + self.f * (x1 - x2)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = self.crossover(self.population[i], v)\n                \n                # Evaluation\n                f = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f < self.fitness[i]:\n                    # Archive update\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i])\n                        self.archive_fitness.append(self.fitness[i])\n                    else:\n                        worst_archive_idx = np.argmax(self.archive_fitness)\n                        if f < self.archive_fitness[worst_archive_idx]:\n                            self.archive[worst_archive_idx] = self.population[i]\n                            self.archive_fitness[worst_archive_idx] = self.fitness[i]\n\n                    self.fitness[i] = f\n                    self.population[i] = u\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n                \n                #Adaptive F update based on success\n                if np.random.rand() < 0.1: # Adaptive F\n                    self.f = np.clip(np.random.normal(0.5, 0.1), 0.1, 0.9)\n\n                if np.random.rand() < 0.1: # Adaptive CR\n                    self.cr = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0)\n\n                if self.evals >= self.budget:\n                    break\n        return self.f_opt, self.x_opt\n        \n    def crossover(self, x, v):\n        u = x.copy()\n        for j in range(self.dim):\n            if np.random.rand() < self.cr:\n                u[j] = v[j]\n        return u", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveDE scored 0.468 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["388eef40-3c24-449e-8db5-9bc282696841"], "operator": null, "metadata": {"aucs": [0.1459489065357319, 0.29123794229822764, 0.3976271932438329, 0.9493721052898221, 0.28330471959353776, 0.23158590488947872, 0.34774208091781356, 0.5135424319603501, 0.30694655246265623, 0.20435796971114528, 0.9491077863124695, 0.9952620889823176, 0.27787948729580514, 0.33487818686062476, 0.9741167996909099, 0.33410569495314135, 0.4561417801062465, 0.7242200929305919, 0.15537934599142122, 0.4946108440324265]}}
{"id": "19b1ee1e-10c5-4ebf-8ee9-02ed76f853ce", "fitness": 0.2594504976680386, "name": "AdaptiveDE", "description": "Simplified Adaptive Differential Evolution with adaptive F and CR, and rank-based mutation, focusing on efficient exploration and exploitation with a reduced population size.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=25, f=0.5, cr=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.f = f\n        self.cr = cr\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n\n        for i in range(self.pop_size):\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.population[i]\n\n        while self.evals < self.budget:\n            ranked_indices = np.argsort(self.fitness)\n\n            for i in range(self.pop_size):\n                # Rank-based Mutation\n                best_idx = ranked_indices[0]\n                random_idx = np.random.choice(np.arange(self.pop_size))\n                x_best = self.population[best_idx]\n                x_rand = self.population[random_idx]\n\n                v = self.population[i] + self.f * (x_best - self.population[i]) + self.f * (x_rand - self.population[i]) # Simplified mutation\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                u = self.crossover(self.population[i], v)\n\n                # Evaluation\n                f = func(u)\n                self.evals += 1\n\n                # Selection\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.population[i] = u\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n                \n                # Adaptive F/CR\n                self.f = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n                self.cr = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0)\n\n                if self.evals >= self.budget:\n                    break\n                    \n        return self.f_opt, self.x_opt\n\n    def crossover(self, x, v):\n        u = x.copy()\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.cr or j == j_rand:\n                u[j] = v[j]\n        return u", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveDE scored 0.259 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["81618a0e-283c-4b95-a231-637a46509895"], "operator": null, "metadata": {"aucs": [0.09528919023413451, 0.17080462400351681, 0.2781153730027721, 0.23598730994825634, 0.19192924827654434, 0.22644651323084464, 0.2490669551921647, 0.2501739527381043, 0.20082504155798053, 0.18060367566325186, 0.16532734000326987, 0.9979182526271521, 0.2161648781694172, 0.23645724473989116, 0.2063884545451341, 0.2643380521117924, 0.21497751448910507, 0.1933891332948, 0.15617911372312498, 0.4586280858095162]}}
{"id": "19e663bb-d548-4797-8314-07ab811a5bbd", "fitness": 0.6064327705512833, "name": "AdaptiveDE", "description": "Rank-based Adaptive DE with simplified parameter adaptation and enhanced archive replacement strategy.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, f=0.5, cr=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.f = f\n        self.cr = cr\n        self.archive = []\n        self.archive_fitness = []\n        self.sf_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.population[best_idx]\n        \n        while self.evals < self.budget:\n            ranked_indices = np.argsort(self.fitness) # Rank individuals\n            \n            for i in range(self.pop_size):\n                # Mutation: Rank-based selection\n                best_idx = ranked_indices[0]  # Best individual\n\n                idxs = np.random.choice(np.arange(self.pop_size), 2, replace=False)\n                x1, x2 = self.population[idxs]\n                \n                v = self.population[best_idx] + self.f * (x1 - x2)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = self.crossover(self.population[i], v)\n                \n                # Evaluation\n                f = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f < self.fitness[i]:\n                    # Archive update with replacement strategy based on fitness difference\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i])\n                        self.archive_fitness.append(self.fitness[i])\n                    else:\n                        fitness_diffs = np.abs(np.array(self.archive_fitness) - self.f_opt)\n                        worst_archive_idx = np.argmax(fitness_diffs) #Replace the archive member farthest from current best\n                        if f < self.archive_fitness[worst_archive_idx]:\n                            self.archive[worst_archive_idx] = self.population[i]\n                            self.archive_fitness[worst_archive_idx] = self.fitness[i]\n\n                    self.fitness[i] = f\n                    self.population[i] = u\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n                \n                # Adaptive F and CR update\n                self.f = np.clip(np.random.normal(0.5, 0.1), 0.1, 0.9)\n                if np.random.rand() < 0.1:\n                    self.cr = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0)\n\n                if self.evals >= self.budget:\n                    break\n        return self.f_opt, self.x_opt\n        \n    def crossover(self, x, v):\n        u = x.copy()\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.cr or j == j_rand:\n                u[j] = v[j]\n        return u", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveDE scored 0.606 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["388eef40-3c24-449e-8db5-9bc282696841"], "operator": null, "metadata": {"aucs": [0.13368937538463865, 0.28265940627916275, 0.7191458527150982, 0.9617495221724414, 0.9248552933585266, 0.8974098288753438, 0.3325023630252918, 0.4868397127987666, 0.9357517815659631, 0.2535156623755169, 0.9035537604098839, 0.9986793883597761, 0.36651389183468863, 0.27632832539832175, 0.7884462721402816, 0.8545883436042447, 0.3428436401463265, 0.9527893587159294, 0.24102655850560928, 0.4757670733598548]}}
{"id": "9c9801be-4a80-417a-b22d-7175bb13814c", "fitness": 0.8360611236377713, "name": "NeighborhoodAdaptiveDE", "description": "Dynamically adjusts the neighborhood size and F/CR parameters based on the success rate and population diversity to balance exploration and exploitation.", "code": "import numpy as np\n\nclass NeighborhoodAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, adapt_freq=50, stagnation_threshold=1000, learning_rate=0.1, initial_neighborhood_size=5, neighborhood_adapt_rate=0.1, diversity_threshold=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.adapt_freq = adapt_freq\n        self.stagnation_threshold = stagnation_threshold\n        self.learning_rate = learning_rate\n        self.initial_neighborhood_size = initial_neighborhood_size\n        self.neighborhood_size = initial_neighborhood_size\n        self.neighborhood_adapt_rate = neighborhood_adapt_rate\n        self.diversity_threshold = diversity_threshold\n        self.f = 0.5\n        self.cr = 0.9\n        self.success_f = []\n        self.success_cr = []\n        self.best_fitness_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.last_improvement = 0\n\n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.population[best_idx]\n        self.best_fitness_history.append(self.f_opt)\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Neighborhood-based Mutation\n                neighbors = np.random.choice(np.arange(self.pop_size), min(self.neighborhood_size, self.pop_size), replace=False)\n                best_neighbor_idx = neighbors[np.argmin(self.fitness[neighbors])]\n                \n                idxs = np.random.choice(np.arange(self.pop_size), 2, replace=False)\n                x1, x2 = self.population[idxs]\n\n                v = self.population[best_neighbor_idx] + self.f * (x1 - x2)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Orthogonal Crossover\n                u = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        u[j] = v[j]\n                \n                # Evaluation\n                f_new = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f_new < self.fitness[i]:\n                    self.success_f.append(self.f)\n                    self.success_cr.append(self.cr)\n                    self.fitness[i] = f_new\n                    self.population[i] = u\n\n                    if f_new < self.f_opt:\n                        self.f_opt = f_new\n                        self.x_opt = u\n                        self.best_fitness_history.append(self.f_opt)\n                        self.last_improvement = self.evals\n\n            # Adaptive Parameter Control\n            if self.evals % self.adapt_freq == 0:\n                # Adjust F/CR\n                if self.success_f:\n                    self.f = (1 - self.learning_rate) * self.f + self.learning_rate * np.mean(self.success_f)\n                    self.cr = (1 - self.learning_rate) * self.cr + self.learning_rate * np.mean(self.success_cr)\n                self.f = np.clip(self.f, 0.1, 0.9)\n                self.cr = np.clip(self.cr, 0.1, 1.0)\n                self.success_f = []\n                self.success_cr = []\n                \n                # Adjust neighborhood size based on success and diversity\n                if len(self.best_fitness_history) > 1 and self.best_fitness_history[-1] == self.best_fitness_history[-2]:\n                    self.neighborhood_size = int(max(1, self.neighborhood_size * (1 - self.neighborhood_adapt_rate)))  # Reduce neighborhood size if stagnant\n                else:\n                    self.neighborhood_size = int(min(self.pop_size, self.neighborhood_size * (1 + self.neighborhood_adapt_rate)))  # Increase if improving\n                    \n                # Calculate population diversity\n                diversity = np.std(self.population)\n                if diversity < self.diversity_threshold:\n                     self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                     self.fitness = np.array([func(x) for x in self.population])\n                     self.evals += self.pop_size\n                     best_idx = np.argmin(self.fitness)\n                     self.f_opt = self.fitness[best_idx]\n                     self.x_opt = self.population[best_idx]\n                     self.last_improvement = self.evals\n                     self.best_fitness_history.append(self.f_opt)\n                     \n            # Stagnation Check and Restart\n            if self.evals - self.last_improvement > self.stagnation_threshold:\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.evals += self.pop_size\n                best_idx = np.argmin(self.fitness)\n                self.f_opt = self.fitness[best_idx]\n                self.x_opt = self.population[best_idx]\n                self.last_improvement = self.evals\n                self.best_fitness_history.append(self.f_opt)\n                # Reset neighborhood size after restart\n                self.neighborhood_size = self.initial_neighborhood_size\n\n            if self.evals >= self.budget:\n                break\n                    \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm NeighborhoodAdaptiveDE scored 0.836 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2090f1a6-3df7-41f3-b337-22436fd042a2"], "operator": null, "metadata": {"aucs": [0.7282877469527334, 0.8367656578562793, 0.8645153995367075, 0.9397510268907869, 0.8928175933549746, 0.9130090801161692, 0.8396850895684762, 0.863576663857103, 0.8905360177461401, 0.8537456986379712, 0.9439778971078264, 0.9899697841827969, 0.8569330261648267, 0.8833544511360226, 0.95870086107786, 0.9019586982200747, 0.8561246964844296, 0.9318007393736518, 0.23654020802671905, 0.5391721364638771]}}
{"id": "3f21fb14-d71b-4e3d-9ecb-3636fc72c865", "fitness": 0.336124327321213, "name": "EnhancedAdaptiveDE", "description": "Enhanced Adaptive Differential Evolution with Aging, Dynamic Neighborhood Size, and a Focused Restart Strategy.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, adapt_freq=50, stagnation_threshold=1000, learning_rate=0.1, neighborhood_size=5, age_threshold=500):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.adapt_freq = adapt_freq\n        self.stagnation_threshold = stagnation_threshold\n        self.learning_rate = learning_rate\n        self.neighborhood_size = neighborhood_size\n        self.age_threshold = age_threshold\n        self.f = 0.5\n        self.cr = 0.9\n        self.success_f = []\n        self.success_cr = []\n        self.best_fitness_history = []\n        self.ages = np.zeros(pop_size) # Individual ages\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.last_improvement = 0\n\n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.population[best_idx]\n        self.best_fitness_history.append(self.f_opt)\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Neighborhood-based Mutation (Dynamic Neighborhood)\n                current_neighborhood_size = max(2, int(self.neighborhood_size * (1 - self.ages[i] / self.age_threshold))) # Smaller for older individuals\n                neighbors = np.random.choice(np.arange(self.pop_size), min(current_neighborhood_size, self.pop_size), replace=False)\n                best_neighbor_idx = neighbors[np.argmin(self.fitness[neighbors])]\n                \n                idxs = np.random.choice(np.arange(self.pop_size), 2, replace=False)\n                x1, x2 = self.population[idxs]\n\n                v = self.population[best_neighbor_idx] + self.f * (x1 - x2)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Orthogonal Crossover\n                u = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        u[j] = v[j]\n                \n                # Evaluation\n                f_new = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f_new < self.fitness[i]:\n                    self.success_f.append(self.f)\n                    self.success_cr.append(self.cr)\n                    self.fitness[i] = f_new\n                    self.population[i] = u\n                    self.ages[i] = 0  # Reset age\n                    \n                    if f_new < self.f_opt:\n                        self.f_opt = f_new\n                        self.x_opt = u\n                        self.best_fitness_history.append(self.f_opt)\n                        self.last_improvement = self.evals\n                else:\n                    self.ages[i] += 1  # Increment age\n\n            # Adaptive Parameter Control\n            if self.evals % self.adapt_freq == 0:\n                if self.success_f:\n                    self.f = (1 - self.learning_rate) * self.f + self.learning_rate * np.mean(self.success_f)\n                    self.cr = (1 - self.learning_rate) * self.cr + self.learning_rate * np.mean(self.success_cr)\n                self.f = np.clip(self.f, 0.1, 0.9)\n                self.cr = np.clip(self.cr, 0.1, 1.0)\n                self.success_f = []\n                self.success_cr = []\n            \n            # Stagnation Check and Restart (Focused Restart)\n            if self.evals - self.last_improvement > self.stagnation_threshold:\n                # Restart only the oldest individuals\n                num_to_restart = int(0.2 * self.pop_size) # Restart 20% of population\n                oldest_indices = np.argsort(self.ages)[-num_to_restart:]\n                \n                for idx in oldest_indices:\n                    self.population[idx] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                    self.fitness[idx] = func(self.population[idx])\n                    self.evals += 1\n                    self.ages[idx] = 0 # Reset age\n\n                best_idx = np.argmin(self.fitness)\n                if self.fitness[best_idx] < self.f_opt: #Update f_opt if better solution is found\n                  self.f_opt = self.fitness[best_idx]\n                  self.x_opt = self.population[best_idx]\n                  self.last_improvement = self.evals\n                  self.best_fitness_history.append(self.f_opt)\n\n\n            if self.evals >= self.budget:\n                break\n                    \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm EnhancedAdaptiveDE scored 0.336 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2090f1a6-3df7-41f3-b337-22436fd042a2"], "operator": null, "metadata": {"aucs": [0.672248654642426, 0]}}
{"id": "cf79fc16-e39b-4d4b-84af-313dc499cbe3", "fitness": 0.4489585191931512, "name": "AdaptiveDE", "description": "Enhanced Adaptive DE with dynamic population size, combined mutation strategies (rank-based and current-to-rand), adaptive CR with memory, and archive-based exploitation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, f=0.5, cr=0.9, pop_factor=2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.f = f\n        self.cr = cr\n        self.archive = []\n        self.archive_fitness = []\n        self.sf_history = []\n        self.success_count = 0  # Track success for parameter adaptation\n        self.cr_memory = []      # Memory for successful CR values\n        self.pop_factor = pop_factor # Factor to adjust pop size\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.population[best_idx]\n        \n        while self.evals < self.budget:\n            ranked_indices = np.argsort(self.fitness) # Rank individuals\n            \n            #Dynamic Pop size\n            dynamic_pop_size = max(10, int(self.pop_size * (1 - self.evals/self.budget)))\n            \n            for i in range(self.pop_size):\n                # Mutation: Rank-based selection\n                best_idx = ranked_indices[0]  # Best individual\n\n                idxs = np.random.choice(np.arange(self.pop_size), 2, replace=False)\n                x1, x2 = self.population[idxs]\n                \n                # Combined mutation: Rank-based and current-to-rand\n                if np.random.rand() < 0.5:\n                    v = self.population[best_idx] + self.f * (x1 - x2)\n                else:\n                     x_rand = self.population[np.random.randint(self.pop_size)]\n                     v = self.population[i] + self.f * (x_rand - self.population[i]) + self.f * (x1-x2) #current-to-rand\n                \n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = self.crossover(self.population[i], v)\n                \n                # Evaluation\n                f = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f < self.fitness[i]:\n                    # Archive update\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i])\n                        self.archive_fitness.append(self.fitness[i])\n                    else:\n                        worst_archive_idx = np.argmax(self.archive_fitness)\n                        if f < self.archive_fitness[worst_archive_idx]:\n                            self.archive[worst_archive_idx] = self.population[i]\n                            self.archive_fitness[worst_archive_idx] = self.fitness[i]\n\n                    self.fitness[i] = f\n                    self.population[i] = u\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n                    self.success_count += 1\n                    if len(self.cr_memory) < 10:\n                         self.cr_memory.append(self.cr)\n                    else:\n                         self.cr_memory.pop(0)\n                         self.cr_memory.append(self.cr)\n\n\n                else:\n                    self.success_count = 0\n                \n                #Adaptive F update based on success\n                if self.success_count > 5:\n                    self.f = np.clip(self.f * 0.95, 0.1, 0.9)\n                    self.success_count = 0\n                else:\n                    self.f = np.clip(self.f * 1.05, 0.1, 0.9)\n\n                # Adaptive CR with memory\n                if len(self.cr_memory) > 0:\n                    self.cr = np.clip(np.random.normal(np.mean(self.cr_memory), 0.1), 0.1, 1.0)\n                else:\n                     self.cr = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0)\n\n\n                if self.evals >= self.budget:\n                    break\n\n            # Archive-based exploitation (after each generation)\n            if len(self.archive) > 0 and np.random.rand() < 0.1:\n                archive_idx = np.random.randint(len(self.archive))\n                x_archive = self.archive[archive_idx]\n                \n                idx_to_replace = np.random.randint(self.pop_size)\n                self.population[idx_to_replace] = x_archive.copy()\n                self.fitness[idx_to_replace] = func(x_archive)\n                self.evals += 1\n                \n                if self.fitness[idx_to_replace] < self.f_opt:\n                    self.f_opt = self.fitness[idx_to_replace]\n                    self.x_opt = x_archive.copy()\n\n        return self.f_opt, self.x_opt\n        \n    def crossover(self, x, v):\n        u = x.copy()\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.cr or j == j_rand:\n                u[j] = v[j]\n        return u", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveDE scored 0.449 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["388eef40-3c24-449e-8db5-9bc282696841"], "operator": null, "metadata": {"aucs": [0.19040920714519138, 0.4917986588912737, 0.4711394113805032, 0.7918618829638059, 0.5349756359033393, 0.6798653655560765, 0.38605740758062645, 0.4320823134666769, 0.511395309044019, 0]}}
{"id": "3f58d670-7dd6-4a1e-8358-2dc66827e4a1", "fitness": -Infinity, "name": "AdaptiveDESelfAdaptiveMutation", "description": "An adaptive differential evolution algorithm with self-adaptive mutation strategies and population diversity control.", "code": "import numpy as np\n\nclass AdaptiveDESelfAdaptiveMutation:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=5, f=0.5, cr=0.9, diversity_threshold=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.f = f\n        self.cr = cr\n        self.archive = []\n        self.archive_fitness = []\n        self.sf_history = []\n        self.diversity_threshold = diversity_threshold\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.population[best_idx]\n        \n        while self.evals < self.budget:\n            ranked_indices = np.argsort(self.fitness) # Rank individuals\n            \n            for i in range(self.pop_size):\n                # Mutation: Self-adaptive mutation strategy\n                if np.random.rand() < 0.5:\n                    # Strategy 1: Rank-based selection\n                    best_idx = ranked_indices[0]  # Best individual\n                    idxs = np.random.choice(np.arange(self.pop_size), 2, replace=False)\n                    x1, x2 = self.population[idxs]\n                    v = self.population[best_idx] + self.f * (x1 - x2)\n                else:\n                    # Strategy 2: Random selection with archive\n                    idxs = np.random.choice(np.arange(self.pop_size), 3, replace=False)\n                    x1, x2, x3 = self.population[idxs]\n                    if len(self.archive) > 0 and np.random.rand() < 0.5:\n                        arch_idx = np.random.randint(len(self.archive))\n                        v = self.archive[arch_idx] + self.f * (x1 - x2)\n                    else:\n                        v = x1 + self.f * (x2 - x3)\n                        \n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = self.crossover(self.population[i], v)\n                \n                # Evaluation\n                f = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f < self.fitness[i]:\n                    # Archive update\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i])\n                        self.archive_fitness.append(self.fitness[i])\n                    else:\n                        worst_archive_idx = np.argmax(self.archive_fitness)\n                        if f < self.archive_fitness[worst_archive_idx]:\n                            self.archive[worst_archive_idx] = self.population[i]\n                            self.archive_fitness[worst_archive_idx] = self.fitness[i]\n\n                    self.fitness[i] = f\n                    self.population[i] = u\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n                \n                #Adaptive F update based on success\n                if np.random.rand() < 0.1: # Adaptive F\n                    self.f = np.clip(np.random.normal(0.5, 0.1), 0.1, 0.9)\n\n                if np.random.rand() < 0.1: # Adaptive CR\n                    self.cr = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0)\n\n                if self.evals >= self.budget:\n                    break\n                    \n            # Population diversity control\n            if self.population_diversity() < self.diversity_threshold:\n                # Re-initialize a fraction of the population\n                num_reinit = int(self.pop_size * 0.1)\n                reinit_indices = np.random.choice(np.arange(self.pop_size), num_reinit, replace=False)\n                self.population[reinit_indices] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(num_reinit, self.dim))\n                self.fitness[reinit_indices] = np.array([func(x) for x in self.population[reinit_indices]])\n                self.evals += num_reinit\n\n        return self.f_opt, self.x_opt\n        \n    def crossover(self, x, v):\n        u = x.copy()\n        for j in range(self.dim):\n            if np.random.rand() < self.cr:\n                u[j] = v[j]\n        return u\n\n    def population_diversity(self):\n        # Calculate the average distance of individuals from the population mean\n        mean_position = np.mean(self.population, axis=0)\n        distances = np.linalg.norm(self.population - mean_position, axis=1)\n        avg_distance = np.mean(distances)\n        return avg_distance / (func.bounds.ub[0] - func.bounds.lb[0]) # Normalize", "configspace": "", "generation": 7, "feedback": "An exception occurred: name 'func' is not defined.", "error": "", "parent_ids": ["7d8f7f64-708a-463c-95d0-2b80c3c9212c"], "operator": null, "metadata": {}}
{"id": "a5797d13-598f-49e6-b84b-060778c7647b", "fitness": 0.711752389859777, "name": "AdaptiveDE", "description": "Simplified Adaptive Differential Evolution with focus on population diversity using a distance-based mutation and self-adaptive parameters.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, f=0.5, cr=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.f = f\n        self.cr = cr\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.population[best_idx]\n        \n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation: Distance-based selection\n                idxs = np.random.choice(np.arange(self.pop_size), 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                \n                #Distance based mutation - encourages diversity\n                v = x1 + self.f * (x2 - x3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = self.crossover(self.population[i], v)\n                \n                # Evaluation\n                f = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.population[i] = u\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n                \n                #Adaptive F/CR update\n                if np.random.rand() < 0.1: # Adaptive F\n                    self.f = np.clip(np.random.normal(0.5, 0.3), 0.1, 0.9)\n\n                if np.random.rand() < 0.1: # Adaptive CR\n                    self.cr = np.clip(np.random.normal(0.9, 0.3), 0.1, 1.0)\n\n                if self.evals >= self.budget:\n                    break\n        return self.f_opt, self.x_opt\n        \n    def crossover(self, x, v):\n        u = x.copy()\n        for j in range(self.dim):\n            if np.random.rand() < self.cr:\n                u[j] = v[j]\n        return u", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveDE scored 0.712 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["7d8f7f64-708a-463c-95d0-2b80c3c9212c"], "operator": null, "metadata": {"aucs": [0.40177352089695295, 0.6616868673464208, 0.6346282723785133, 0.844479176064739, 0.7240473498942244, 0.7786553135969497, 0.6293507134882446, 0.6767409420008675, 0.7474272774711691, 0.6957594617689669, 0.8480590681104513, 0.9988161998234271, 0.6156087463064079, 0.7275799537702874, 0.8992698016005576, 0.7693187793214846, 0.6227901084391798, 0.8310634641186468, 0.45725148895071366, 0.6707412918473368]}}
{"id": "c1922c6a-5c2d-4fb4-a574-de75778ae362", "fitness": 0.46409576334155195, "name": "AdaptiveDE", "description": "Simplified Adaptive DE with rank-based mutation and self-adaptive CR, removing the archive and further simplifying parameter updates.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, f=0.5, cr=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.f = f\n        self.cr = cr\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.population[best_idx]\n        \n        while self.evals < self.budget:\n            ranked_indices = np.argsort(self.fitness)\n            \n            for i in range(self.pop_size):\n                # Mutation: Rank-based selection\n                best_idx = ranked_indices[0]\n\n                idxs = np.random.choice(np.arange(self.pop_size), 2, replace=False)\n                x1, x2 = self.population[idxs]\n                \n                v = self.population[best_idx] + self.f * (x1 - x2)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover with self-adaptive CR\n                u = self.crossover(self.population[i], v, self.cr)\n                \n                # Evaluation\n                f = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.population[i] = u\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n                \n                # Adaptive F and CR update\n                self.f = np.clip(np.random.normal(0.5, 0.1), 0.1, 0.9)\n                \n                if self.evals >= self.budget:\n                    break\n                    \n            # Self-adaptive CR using a normal distribution centered around 0.9\n            self.cr = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0)\n\n        return self.f_opt, self.x_opt\n        \n    def crossover(self, x, v, cr):\n        u = x.copy()\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < cr or j == j_rand:\n                u[j] = v[j]\n        return u", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveDE scored 0.464 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["19e663bb-d548-4797-8314-07ab811a5bbd"], "operator": null, "metadata": {"aucs": [0.2200472752574696, 0.22616578008752164, 0.9048751072168524, 0.2603252355674436, 0.2804878697088662, 0.9312966348283703, 0.3121481421886185, 0.4555840969584075, 0.3395596664608814, 0.21684620099736818, 0.5780218556270535, 0.9967541175139567, 0.24220390122588564, 0.3144479894704604, 0.6053920807596984, 0.33614482926016576, 0.41243535932639797, 0.9503465119361834, 0.23300711726965206, 0.46582549516978566]}}
{"id": "23c287e7-19fb-4ac2-8fa4-6e999d533a54", "fitness": 0.6101358757314671, "name": "SimplifiedAdaptiveDE", "description": "Simplified adaptive DE with dynamic F/CR adaptation based on recent success and a global best-guided mutation.", "code": "import numpy as np\n\nclass SimplifiedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, adapt_freq=50, learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.adapt_freq = adapt_freq\n        self.learning_rate = learning_rate\n        self.f = 0.5\n        self.cr = 0.9\n        self.success_f = []\n        self.success_cr = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n\n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.population[best_idx]\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Global Best-Guided Mutation\n                idxs = np.random.choice(np.arange(self.pop_size), 2, replace=False)\n                x1, x2 = self.population[idxs]\n\n                v = self.x_opt + self.f * (x1 - x2)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Binomial Crossover\n                u = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        u[j] = v[j]\n                \n                # Evaluation\n                f_new = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f_new < self.fitness[i]:\n                    self.success_f.append(self.f)\n                    self.success_cr.append(self.cr)\n                    self.fitness[i] = f_new\n                    self.population[i] = u\n\n                    if f_new < self.f_opt:\n                        self.f_opt = f_new\n                        self.x_opt = u\n\n            # Adaptive Parameter Control\n            if self.evals % self.adapt_freq == 0:\n                if self.success_f:\n                    self.f = (1 - self.learning_rate) * self.f + self.learning_rate * np.mean(self.success_f)\n                    self.cr = (1 - self.learning_rate) * self.cr + self.learning_rate * np.mean(self.success_cr)\n                self.f = np.clip(self.f, 0.1, 0.9)\n                self.cr = np.clip(self.cr, 0.1, 1.0)\n                self.success_f = []\n                self.success_cr = []\n\n            if self.evals >= self.budget:\n                break\n                    \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm SimplifiedAdaptiveDE scored 0.610 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9c9801be-4a80-417a-b22d-7175bb13814c"], "operator": null, "metadata": {"aucs": [0.6051221061964743, 0.23709641763520406, 0.9265665186441212, 0.1993493187226415, 0.25240071848733814, 0.9427934862779288, 0.35247563108356295, 0.9198939147959928, 0.9187427422749842, 0.20728548466831076, 0.9631327069151514, 0.9959187271817332, 0.6299561161187003, 0.31402734623570105, 0.7434520342130626, 0.9139416361263282, 0.3422116363163805, 0.9604491011888306, 0.23568196647599038, 0.5422199050709056]}}
{"id": "c97e2ca0-4d63-4810-a386-6a050bb19202", "fitness": 0.686138243889364, "name": "AdaptiveDE", "description": "Simplified Adaptive Differential Evolution with self-adaptive parameters and a focused archive update.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, f=0.5, cr=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.f = f\n        self.cr = cr\n        self.archive = []\n        self.archive_fitness = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.population[best_idx]\n        \n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation: Use best individual and two other random individuals\n                idxs = np.random.choice(np.arange(self.pop_size), 2, replace=False)\n                x1, x2 = self.population[idxs]\n                v = self.x_opt + self.f * (x1 - x2)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = self.crossover(self.population[i], v)\n                \n                # Evaluation\n                f = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f < self.fitness[i]:\n                    # Archive update: Replace worst archive member if new solution is better than worst archive and archive is full, else add to archive.\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i])\n                        self.archive_fitness.append(self.fitness[i])\n                    else:\n                        worst_archive_idx = np.argmax(self.archive_fitness)\n                        if f < self.archive_fitness[worst_archive_idx]:\n                            self.archive[worst_archive_idx] = self.population[i]\n                            self.archive_fitness[worst_archive_idx] = self.fitness[i]\n\n                    self.fitness[i] = f\n                    self.population[i] = u\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n                \n                # Adaptive F and CR update: Simplified self-adaptation\n                self.f = np.clip(np.random.normal(0.5, 0.1), 0.1, 0.9)\n                self.cr = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0)\n\n                if self.evals >= self.budget:\n                    break\n        return self.f_opt, self.x_opt\n        \n    def crossover(self, x, v):\n        u = x.copy()\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.cr or j == j_rand:\n                u[j] = v[j]\n        return u", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveDE scored 0.686 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["19e663bb-d548-4797-8314-07ab811a5bbd"], "operator": null, "metadata": {"aucs": [0.24742095856211, 0.9201303669521136, 0.9029605989123433, 0.9617088254713932, 0.2738040446593062, 0.9426944570350065, 0.34009438791510915, 0.454447014078709, 0.93485768139599, 0.21079655104645978, 0.9560073614760365, 0.9954741902748073, 0.8697334476878886, 0.3266598202428491, 0.9708064185954074, 0.9415406269539638, 0.8236978255836631, 0.9622353231762432, 0.18428165225405568, 0.5034133255138264]}}
{"id": "bac52812-f32f-418b-90f0-9e1c38f46fc5", "fitness": 0.8514849913057125, "name": "SimplifiedNeighborhoodAdaptiveDE", "description": "Simplified Adaptive DE with neighborhood-based mutation and periodic population rejuvenation to balance exploration and exploitation effectively.", "code": "import numpy as np\n\nclass SimplifiedNeighborhoodAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, adapt_freq=50, stagnation_threshold=1000, learning_rate=0.1, initial_neighborhood_size=5, diversity_threshold=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.adapt_freq = adapt_freq\n        self.stagnation_threshold = stagnation_threshold\n        self.learning_rate = learning_rate\n        self.neighborhood_size = initial_neighborhood_size\n        self.diversity_threshold = diversity_threshold\n        self.f = 0.5\n        self.cr = 0.9\n        self.success_f = []\n        self.success_cr = []\n        self.last_improvement = 0\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n\n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.population[best_idx]\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Neighborhood-based Mutation\n                neighbors = np.random.choice(np.arange(self.pop_size), min(self.neighborhood_size, self.pop_size - 1), replace=False)\n                neighbors = neighbors[neighbors != i] # Ensure i is not in neighbors\n                best_neighbor_idx = neighbors[np.argmin(self.fitness[neighbors])] if len(neighbors) > 0 else i # Handle edge case\n\n                idxs = np.random.choice(np.arange(self.pop_size), 2, replace=False)\n                x1, x2 = self.population[idxs]\n\n                v = self.population[best_neighbor_idx] + self.f * (x1 - x2)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        u[j] = v[j]\n                \n                # Evaluation\n                f_new = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f_new < self.fitness[i]:\n                    self.success_f.append(self.f)\n                    self.success_cr.append(self.cr)\n                    self.fitness[i] = f_new\n                    self.population[i] = u\n\n                    if f_new < self.f_opt:\n                        self.f_opt = f_new\n                        self.x_opt = u\n                        self.last_improvement = self.evals\n\n            # Adaptive Parameter Control\n            if self.evals % self.adapt_freq == 0:\n                if self.success_f:\n                    self.f = (1 - self.learning_rate) * self.f + self.learning_rate * np.mean(self.success_f)\n                    self.cr = (1 - self.learning_rate) * self.cr + self.learning_rate * np.mean(self.success_cr)\n                self.f = np.clip(self.f, 0.1, 0.9)\n                self.cr = np.clip(self.cr, 0.1, 1.0)\n                self.success_f = []\n                self.success_cr = []\n\n            # Population Rejuvenation\n            if self.evals - self.last_improvement > self.stagnation_threshold:\n                diversity = np.std(self.population)\n                if diversity < self.diversity_threshold:\n                    self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                    self.fitness = np.array([func(x) for x in self.population])\n                    self.evals += self.pop_size\n                    best_idx = np.argmin(self.fitness)\n                    self.f_opt = self.fitness[best_idx]\n                    self.x_opt = self.population[best_idx]\n                    self.last_improvement = self.evals\n\n            if self.evals >= self.budget:\n                break\n                    \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm SimplifiedNeighborhoodAdaptiveDE scored 0.851 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9c9801be-4a80-417a-b22d-7175bb13814c"], "operator": null, "metadata": {"aucs": [0.5739575468735838, 0.854166170793437, 0.8495650758245218, 0.9489321582106044, 0.887538775178174, 0.9193860093021736, 0.8454982412617118, 0.8510739490041488, 0.8950471305479227, 0.8562051501062282, 0.9495786575120281, 0.9952719686906759, 0.8496146606176909, 0.8793145628333287, 0.9452709603315872, 0.9032514185258183, 0.8385637752869914, 0.92863137125801, 0.71777582443406, 0.5410564195215575]}}
{"id": "29f29744-96f4-4b66-b541-080b91fde932", "fitness": 0.7774705478437519, "name": "AdaptiveDE", "description": "An adaptive Differential Evolution algorithm with a self-adaptive strategy for F and CR parameters using a success-history based adaptation, and includes a repair mechanism to improve solution quality.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=5, memory_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.memory_size = memory_size\n        self.memory_cr = np.ones(self.memory_size) * 0.5\n        self.memory_f = np.ones(self.memory_size) * 0.5\n        self.archive = []\n        self.archive_fitness = []\n        self.sf_history = []\n        self.scr_history = []\n        self.p_best = 0.05  # Probability of selecting p-best individual\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.population[best_idx]\n        \n        while self.evals < self.budget:\n            ranked_indices = np.argsort(self.fitness) # Rank individuals\n            \n            for i in range(self.pop_size):\n                # Parameter adaptation\n                memory_index = np.random.randint(self.memory_size)\n                cr = self.memory_cr[memory_index]\n                f = self.memory_f[memory_index]\n\n                # Mutation: p-best selection\n                p_best_count = max(1, int(self.pop_size * self.p_best))\n                p_best_indices = ranked_indices[:p_best_count]\n                best_idx = np.random.choice(p_best_indices)  # Select from top p%\n                \n                idxs = np.random.choice(np.arange(self.pop_size), 2, replace=False)\n                x1, x2 = self.population[idxs]\n                \n                v = self.population[best_idx] + f * (x1 - x2)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = self.crossover(self.population[i], v, cr)\n                \n                # Repair mechanism\n                u = self.repair(u, func.bounds.lb, func.bounds.ub)\n                \n                # Evaluation\n                f_new = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f_new < self.fitness[i]:\n                    # Archive update\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i])\n                        self.archive_fitness.append(self.fitness[i])\n                    else:\n                        worst_archive_idx = np.argmax(self.archive_fitness)\n                        if f_new < self.archive_fitness[worst_archive_idx]:\n                            self.archive[worst_archive_idx] = self.population[i]\n                            self.archive_fitness[worst_archive_idx] = self.fitness[i]\n\n                    self.fitness[i] = f_new\n                    self.population[i] = u\n\n                    if f_new < self.f_opt:\n                        self.f_opt = f_new\n                        self.x_opt = u\n\n                    # Success history update\n                    self.sf_history.append(f)\n                    self.scr_history.append(cr)\n\n                    self.memory_cr[memory_index] = cr\n                    self.memory_f[memory_index] = f\n                else:\n                    pass #No success\n                \n                if self.evals >= self.budget:\n                    break\n        return self.f_opt, self.x_opt\n        \n    def crossover(self, x, v, cr):\n        u = x.copy()\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < cr or j == j_rand:\n                u[j] = v[j]\n        return u\n\n    def repair(self, x, lb, ub):\n        # Simple bound repair\n        x = np.clip(x, lb, ub)\n        return x", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveDE scored 0.777 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["7d8f7f64-708a-463c-95d0-2b80c3c9212c"], "operator": null, "metadata": {"aucs": [0.29369736845950056, 0.7385570709910676, 0.8250313898360613, 0.9258992189033802, 0.8715412202794457, 0.8869166144686813, 0.8249220905182465, 0.8130069015699279, 0.8635590199885037, 0.8583156739600754, 0.9056967284865035, 0.9945631473296559, 0.4237450192309906, 0.8402865024157596, 0.9330681555604132, 0.896402259775031, 0.7477072685353154, 0.9165276052219121, 0.45183157464031554, 0.5381361267042497]}}
{"id": "326578cd-40f8-4e8d-ac8d-512444e54439", "fitness": 0.3226111930945273, "name": "AdaptiveDE", "description": "Simplifies Adaptive DE by removing the archive and using a dynamic F/CR adaptation based on population variance for better exploration/exploitation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, f=0.5, cr=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.f = f\n        self.cr = cr\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.population[best_idx]\n        \n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(np.arange(self.pop_size), 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                \n                v = x1 + self.f * (x2 - x3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = self.crossover(self.population[i], v)\n                \n                # Evaluation\n                f = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.population[i] = u\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n                \n                # Adaptive F/CR based on population variance\n                self.f = np.clip(0.5 * (1 + np.std(self.fitness)), 0.1, 0.9)\n                self.cr = np.clip(0.9 * (1 - np.std(self.population)), 0.1, 1.0)\n\n                if self.evals >= self.budget:\n                    break\n        return self.f_opt, self.x_opt\n        \n    def crossover(self, x, v):\n        u = x.copy()\n        for j in range(self.dim):\n            if np.random.rand() < self.cr:\n                u[j] = v[j]\n        return u", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveDE scored 0.323 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["7d8f7f64-708a-463c-95d0-2b80c3c9212c"], "operator": null, "metadata": {"aucs": [0.1263169957215703, 0.18924046301938635, 0.3159249051778128, 0.268011621325967, 0.25366634024711976, 0.32413610374881285, 0.2696124186670187, 0.27990831673813277, 0.2542170305373054, 0.1740997331841021, 0.23858524156439465, 0.997742150027707, 0.23333752806398922, 0.2591468586777551, 0.6446604220778509, 0.34834411706891444, 0.2696761666407419, 0.36093088176213917, 0.16205896120922625, 0.4826076064305991]}}
{"id": "598eecb4-e3f3-40e7-9888-1578932158b8", "fitness": 0.4639351942495557, "name": "NeighborhoodAdaptiveDECMA", "description": "Integrates a self-adaptive covariance matrix adaptation (CMA) strategy into the neighborhood-based differential evolution to enhance search direction adaptation.", "code": "import numpy as np\n\nclass NeighborhoodAdaptiveDECMA:\n    def __init__(self, budget=10000, dim=10, pop_size=50, adapt_freq=50, stagnation_threshold=1000, learning_rate=0.1, initial_neighborhood_size=5, neighborhood_adapt_rate=0.1, diversity_threshold=0.01, cma_learning_rate=0.1, cma_momentum=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.adapt_freq = adapt_freq\n        self.stagnation_threshold = stagnation_threshold\n        self.learning_rate = learning_rate\n        self.initial_neighborhood_size = initial_neighborhood_size\n        self.neighborhood_size = initial_neighborhood_size\n        self.neighborhood_adapt_rate = neighborhood_adapt_rate\n        self.diversity_threshold = diversity_threshold\n        self.f = 0.5\n        self.cr = 0.9\n        self.success_f = []\n        self.success_cr = []\n        self.best_fitness_history = []\n        self.cma_learning_rate = cma_learning_rate\n        self.cma_momentum = cma_momentum\n        self.mean = None\n        self.covariance = None\n        self.pc = None  # Evolution path for covariance matrix adaptation\n        self.ps = None  # Evolution path for step size adaptation\n        self.chiN = np.sqrt(dim) * (1 - (1 / (4 * dim)) + (1 / (21 * dim**2)))\n        self.c_sig = None\n        self.d_sig = None\n        self.c_cov = None\n        self.covariance_adaptation_rate = None\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.last_improvement = 0\n\n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.population[best_idx]\n        self.best_fitness_history.append(self.f_opt)\n\n        # Initialize CMA-ES parameters\n        self.mean = self.x_opt.copy()\n        self.covariance = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n\n        self.c_sig = np.sqrt(self.cma_learning_rate * (2 - self.cma_learning_rate))\n        self.d_sig = 1 + (2 * max(0, np.sqrt((self.cma_learning_rate * (2 - self.cma_learning_rate) * (np.sum(self.ps**2) / self.dim)) - 1)))\n        self.c_cov = (1/ self.pop_size) * (self.cma_learning_rate * (2 - self.cma_learning_rate))\n        self.covariance_adaptation_rate = self.cma_learning_rate\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Neighborhood-based Mutation with CMA sampling\n                neighbors = np.random.choice(np.arange(self.pop_size), min(self.neighborhood_size, self.pop_size), replace=False)\n                best_neighbor_idx = neighbors[np.argmin(self.fitness[neighbors])]\n\n                idxs = np.random.choice(np.arange(self.pop_size), 2, replace=False)\n                x1, x2 = self.population[idxs]\n\n                # Sample from multivariate normal distribution based on CMA\n                z = np.random.multivariate_normal(np.zeros(self.dim), self.covariance)\n                v = self.population[best_neighbor_idx] + self.f * (x1 - x2) + z * 0.1 # Added noise\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Orthogonal Crossover\n                u = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        u[j] = v[j]\n\n                # Evaluation\n                f_new = func(u)\n                self.evals += 1\n\n                # Selection\n                if f_new < self.fitness[i]:\n                    self.success_f.append(self.f)\n                    self.success_cr.append(self.cr)\n                    self.fitness[i] = f_new\n                    self.population[i] = u\n\n                    if f_new < self.f_opt:\n                        self.f_opt = f_new\n                        self.x_opt = u\n                        self.best_fitness_history.append(self.f_opt)\n                        self.last_improvement = self.evals\n\n            # Adaptive Parameter Control\n            if self.evals % self.adapt_freq == 0:\n                # Adjust F/CR\n                if self.success_f:\n                    self.f = (1 - self.learning_rate) * self.f + self.learning_rate * np.mean(self.success_f)\n                    self.cr = (1 - self.learning_rate) * self.cr + self.learning_rate * np.mean(self.success_cr)\n                self.f = np.clip(self.f, 0.1, 0.9)\n                self.cr = np.clip(self.cr, 0.1, 1.0)\n                self.success_f = []\n                self.success_cr = []\n\n                # Adjust neighborhood size based on success and diversity\n                if len(self.best_fitness_history) > 1 and self.best_fitness_history[-1] == self.best_fitness_history[-2]:\n                    self.neighborhood_size = int(max(1, self.neighborhood_size * (1 - self.neighborhood_adapt_rate)))  # Reduce neighborhood size if stagnant\n                else:\n                    self.neighborhood_size = int(min(self.pop_size, self.neighborhood_size * (1 + self.neighborhood_adapt_rate)))  # Increase if improving\n\n                # Calculate population diversity\n                diversity = np.std(self.population)\n                if diversity < self.diversity_threshold:\n                     self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                     self.fitness = np.array([func(x) for x in self.population])\n                     self.evals += self.pop_size\n                     best_idx = np.argmin(self.fitness)\n                     self.f_opt = self.fitness[best_idx]\n                     self.x_opt = self.population[best_idx]\n                     self.last_improvement = self.evals\n                     self.best_fitness_history.append(self.f_opt)\n\n                # Update CMA parameters\n                sorted_indices = np.argsort(self.fitness)\n                selected_individuals = self.population[sorted_indices[:self.pop_size // 2]]  # Use top 50% for CMA update\n                mean_diff = np.mean(selected_individuals, axis=0) - self.mean\n                self.mean = np.mean(selected_individuals, axis=0)\n\n                self.ps = (1 - self.cma_learning_rate) * self.ps + np.sqrt(self.cma_learning_rate * (2 - self.cma_learning_rate)) * (mean_diff / np.sqrt(np.diag(self.covariance)) if np.any(np.diag(self.covariance) > 0) else np.zeros(self.dim))\n\n                self.covariance = (1 - self.c_cov) * self.covariance + self.c_cov * (np.outer(self.ps, self.ps) + np.eye(self.dim))\n\n            # Stagnation Check and Restart\n            if self.evals - self.last_improvement > self.stagnation_threshold:\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.evals += self.pop_size\n                best_idx = np.argmin(self.fitness)\n                self.f_opt = self.fitness[best_idx]\n                self.x_opt = self.population[best_idx]\n                self.last_improvement = self.evals\n                self.best_fitness_history.append(self.f_opt)\n                # Reset neighborhood size after restart\n                self.neighborhood_size = self.initial_neighborhood_size\n                self.mean = self.x_opt.copy()\n                self.covariance = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n\n            if self.evals >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm NeighborhoodAdaptiveDECMA scored 0.464 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9c9801be-4a80-417a-b22d-7175bb13814c"], "operator": null, "metadata": {"aucs": [0.21431988464513996, 0.3917161545294413, 0.43553443848112283, 0.8976121305904519, 0.3525934907125886, 0.4594922901574783, 0.3507132546372278, 0.3767415081708302, 0.36745766609505404, 0.3632987674716158, 0.730708523589664, 0.998944567398852, 0.2944425579284029, 0.4053806747422989, 0.8556683095820119, 0.48512871142013625, 0.3710805663396862, 0]}}
{"id": "8e9d5dda-acac-4c03-bb37-2d5daf8b39ac", "fitness": -Infinity, "name": "AdaptiveDE_OL_PopSize", "description": "Adaptive Differential Evolution with orthogonal learning and a self-adaptive population size.", "code": "import numpy as np\n\nclass AdaptiveDE_OL_PopSize:\n    def __init__(self, budget=10000, dim=10, pop_size_init=50, archive_size=5, f=0.5, cr=0.9, pop_size_min=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_init = pop_size_init\n        self.pop_size_min = pop_size_min\n        self.archive_size = archive_size\n        self.f = f\n        self.cr = cr\n        self.archive = []\n        self.archive_fitness = []\n        self.sf_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.population[best_idx]\n        \n        generation = 0\n        while self.evals < self.budget:\n            generation += 1\n            ranked_indices = np.argsort(self.fitness) # Rank individuals\n            \n            for i in range(self.pop_size):\n                # Mutation: Rank-based selection\n                best_idx = ranked_indices[0]  # Best individual\n\n                idxs = np.random.choice(np.arange(self.pop_size), 2, replace=False)\n                x1, x2 = self.population[idxs]\n                \n                v = self.population[best_idx] + self.f * (x1 - x2)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = self.crossover(self.population[i], v)\n                \n                # Evaluation\n                f = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f < self.fitness[i]:\n                    # Archive update\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i])\n                        self.archive_fitness.append(self.fitness[i])\n                    else:\n                        worst_archive_idx = np.argmax(self.archive_fitness)\n                        if f < self.archive_fitness[worst_archive_idx]:\n                            self.archive[worst_archive_idx] = self.population[i]\n                            self.archive_fitness[worst_archive_idx] = self.fitness[i]\n\n                    self.fitness[i] = f\n                    self.population[i] = u\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n                else:\n                    # Orthogonal Learning: Generate a new solution based on current individual\n                    x_ortho = self.orthogonal_learning(self.population[i], func)\n                    f_ortho = func(x_ortho)\n                    self.evals += 1\n\n                    if f_ortho < self.fitness[i]:\n                        self.fitness[i] = f_ortho\n                        self.population[i] = x_ortho\n\n                        if f_ortho < self.f_opt:\n                            self.f_opt = f_ortho\n                            self.x_opt = x_ortho\n                            \n                #Adaptive F and CR update based on success\n                if np.random.rand() < 0.1:\n                    self.f = np.clip(np.random.normal(0.5, 0.1), 0.1, 0.9)\n\n                if np.random.rand() < 0.1:\n                    self.cr = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0)\n                    \n            # Population size adaptation\n            if generation % 10 == 0:\n                cv = np.std(self.fitness) / np.mean(self.fitness)\n                if cv < 0.01 and self.pop_size > self.pop_size_min:\n                    self.pop_size = max(self.pop_size_min, int(self.pop_size * 0.9))  # Reduce population size\n                    self.population = self.population[ranked_indices[:self.pop_size]]\n                    self.fitness = self.fitness[ranked_indices[:self.pop_size]]\n                    print(f\"Reducing population size to {self.pop_size}\")\n                elif cv > 0.1 and self.pop_size < 2 * self.pop_size_init:\n                    num_new = min(2 * self.pop_size_init - self.pop_size, 10)\n                    new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(num_new, self.dim))\n                    new_fitness = np.array([func(x) for x in new_individuals])\n                    self.evals += num_new\n                    self.population = np.vstack((self.population, new_individuals))\n                    self.fitness = np.concatenate((self.fitness, new_fitness))\n                    self.pop_size = self.population.shape[0]\n                    print(f\"Increasing population size to {self.pop_size}\")\n\n\n                best_idx = np.argmin(self.fitness)\n                self.f_opt = self.fitness[best_idx]\n                self.x_opt = self.population[best_idx]\n\n                if self.evals >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt\n        \n    def crossover(self, x, v):\n        u = x.copy()\n        for j in range(self.dim):\n            if np.random.rand() < self.cr:\n                u[j] = v[j]\n        return u\n    \n    def orthogonal_learning(self, x, func, sample_size=5):\n        # Generate a set of sample points around x\n        samples = np.random.normal(x, scale=0.1, size=(sample_size, self.dim))\n        samples = np.clip(samples, func.bounds.lb, func.bounds.ub)\n        \n        # Evaluate the sample points\n        fitness_values = [func(sample) for sample in samples]\n        self.evals += sample_size\n\n        # Select the best sample point\n        best_index = np.argmin(fitness_values)\n        \n        return samples[best_index]", "configspace": "", "generation": 7, "feedback": "An exception occurred: 'AdaptiveDE_OL_PopSize' object has no attribute 'pop_size_init'.", "error": "", "parent_ids": ["7d8f7f64-708a-463c-95d0-2b80c3c9212c"], "operator": null, "metadata": {}}
{"id": "0e02955e-e6f2-481b-a743-6a28f34b7859", "fitness": -Infinity, "name": "SOMAdaptiveDE", "description": "Adaptive Differential Evolution with a self-organizing map (SOM) to guide mutation and crossover parameter adaptation based on population diversity.", "code": "import numpy as np\n\nclass SOMAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, som_size=(5, 5), learning_rate=0.1, sigma=1.0, f=0.5, cr=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.som_size = som_size\n        self.learning_rate = learning_rate\n        self.sigma = sigma\n        self.f = f\n        self.cr = cr\n        self.som = np.random.rand(som_size[0], som_size[1], dim) # SOM weights\n        self.sf_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.population[best_idx]\n        \n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Find the best matching unit (BMU) in the SOM\n                bmu_row, bmu_col = self.find_bmu(self.population[i])\n\n                # Adaptive F and CR based on BMU location\n                self.f = np.clip(0.1 + bmu_row / self.som_size[0] * 0.8, 0.1, 0.9) # Linearly change F based on row\n                self.cr = np.clip(0.1 + bmu_col / self.som_size[1] * 0.9, 0.1, 1.0) # Linearly change CR based on column\n\n                # Mutation\n                idxs = np.random.choice(np.arange(self.pop_size), 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                v = self.population[i] + self.f * (x1 - x2)\n\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = self.crossover(self.population[i], v)\n                \n                # Evaluation\n                f = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.population[i] = u\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n                        \n                # Update SOM\n                self.update_som(self.population[i], bmu_row, bmu_col)\n                \n                if self.evals >= self.budget:\n                    break\n        return self.f_opt, self.x_opt\n        \n    def crossover(self, x, v):\n        u = x.copy()\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.cr or j == j_rand:\n                u[j] = v[j]\n        return u\n\n    def find_bmu(self, individual):\n        min_dist = np.Inf\n        bmu_row = -1\n        bmu_col = -1\n        for row in range(self.som_size[0]):\n            for col in range(self.som_size[1]):\n                dist = np.linalg.norm(individual - self.som[row, col])\n                if dist < min_dist:\n                    min_dist = dist\n                    bmu_row = row\n                    bmu_col = col\n        return bmu_row, bmu_col\n\n    def update_som(self, individual, bmu_row, bmu_col):\n        for row in range(self.som_size[0]):\n            for col in range(self.som_size[1]):\n                distance = np.sqrt((row - bmu_row)**2 + (col - bmu_col)**2)\n                influence = np.exp(-distance**2 / (2 * self.sigma**2))\n                self.som[row, col] += self.learning_rate * influence * (individual - self.som[row, col])", "configspace": "", "generation": 7, "feedback": "An exception occurred: Evaluation timed out after 60 seconds..", "error": "", "parent_ids": ["19e663bb-d548-4797-8314-07ab811a5bbd"], "operator": null, "metadata": {}}
{"id": "2d8aa020-3672-4507-b264-c8958f08b67f", "fitness": 0.6847379717589336, "name": "AdaptiveDE", "description": "Simplified Adaptive DE with a population-based archive and a more aggressive parameter adaptation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_rate=0.2, f=0.5, cr=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = int(pop_size * archive_rate)\n        self.f = f\n        self.cr = cr\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.population[best_idx]\n        \n        while self.evals < self.budget:\n            archive = self.population[np.random.choice(self.pop_size, self.archive_size, replace=False)] # Population-based archive\n            \n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                \n                if np.random.rand() < 0.5 and self.archive_size > 0:\n                    xa = archive[np.random.choice(self.archive_size)]\n                    v = self.population[i] + self.f * (xa - self.population[i]) + self.f * (x1 - x2)\n                else:\n                    v = x1 + self.f * (x2 - x3)\n                \n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = self.crossover(self.population[i], v)\n                \n                # Evaluation\n                f = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.population[i] = u\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n                        \n                # Parameter adaptation (more aggressive)\n                self.f = np.clip(np.random.normal(0.5, 0.3), 0.1, 0.9)\n                self.cr = np.clip(np.random.normal(0.9, 0.2), 0.1, 1.0)\n\n                if self.evals >= self.budget:\n                    break\n        return self.f_opt, self.x_opt\n        \n    def crossover(self, x, v):\n        u = x.copy()\n        for j in range(self.dim):\n            if np.random.rand() < self.cr:\n                u[j] = v[j]\n        return u", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveDE scored 0.685 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["7d8f7f64-708a-463c-95d0-2b80c3c9212c"], "operator": null, "metadata": {"aucs": [0.33440810416841127, 0.6546242047089721, 0.6254621473089701, 0.8688347913286768, 0.730903546236621, 0.7798269563087057, 0.6246658616723143, 0.6562488744629846, 0.7575416229175473, 0.6774872822072258, 0.8578519854318009, 0.9969578243773344, 0.6183391116337982, 0.7059667126832033, 0.8750836029213228, 0.7651364091441004, 0.5735684548477502, 0.8391034849361034, 0.25505454290230634, 0.49769391498052284]}}
{"id": "3e93143e-fcee-413b-a0c6-ea3a325c905a", "fitness": -Infinity, "name": "ClusteringDE", "description": "A Differential Evolution strategy that dynamically adjusts its mutation strategy based on the population's diversity and focuses the search towards promising regions identified through a clustering approach.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import pairwise_distances_argmin\n\nclass ClusteringDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, num_clusters=5, mutation_factor=0.5, crossover_rate=0.7, diversity_threshold=0.1, cluster_update_frequency=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.num_clusters = num_clusters\n        self.mutation_factor = mutation_factor\n        self.crossover_rate = crossover_rate\n        self.diversity_threshold = diversity_threshold\n        self.cluster_update_frequency = cluster_update_frequency\n        self.population = None\n        self.fitness = None\n        self.evals = 0\n        self.best_fitness = np.inf\n        self.best_solution = None\n        self.clusters = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals += self.pop_size\n        best_idx = np.argmin(self.fitness)\n        self.best_fitness = self.fitness[best_idx]\n        self.best_solution = self.population[best_idx]\n\n    def update_clusters(self):\n        kmeans = KMeans(n_clusters=self.num_clusters, random_state=0, n_init=10)\n        kmeans.fit(self.population)\n        self.clusters = kmeans.cluster_centers_\n\n    def diversity_metric(self):\n         return np.std(self.population)\n\n    def mutate(self, i):\n        if self.clusters is None:\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x1, x2, x3 = self.population[idxs]\n            v = self.population[i] + self.mutation_factor * (x2 - x3)\n        else:\n            # Find the closest cluster to the current individual\n            closest_cluster_idx = pairwise_distances_argmin(self.population[i].reshape(1, -1), self.clusters)[0]\n            cluster_center = self.clusters[closest_cluster_idx]\n\n            # Exploit the cluster center and two random individuals\n            idxs = np.random.choice(self.pop_size, 2, replace=False)\n            x1, x2 = self.population[idxs]\n            v = cluster_center + self.mutation_factor * (x1 - x2) # Move towards cluster center\n\n        v = np.clip(v, -5.0, 5.0) # Clip to the bounds\n\n        return v\n\n    def crossover(self, individual, mutant):\n        trial_vector = np.copy(individual)\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.crossover_rate or j == j_rand:\n                trial_vector[j] = mutant[j]\n        return trial_vector\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.evals < self.budget:\n            if self.evals % self.cluster_update_frequency == 0 or self.clusters is None:\n                 if self.diversity_metric() > self.diversity_threshold:\n                    self.update_clusters()\n                 else:\n                     self.clusters = None #Disable clustering if population collapsed\n\n            for i in range(self.pop_size):\n                mutant = self.mutate(i)\n                trial_vector = self.crossover(self.population[i], mutant)\n\n                f_trial = func(trial_vector)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial_vector\n\n                    if f_trial < self.best_fitness:\n                        self.best_fitness = f_trial\n                        self.best_solution = trial_vector\n\n                if self.evals >= self.budget:\n                    break\n\n        return self.best_fitness, self.best_solution", "configspace": "", "generation": 8, "feedback": "An exception occurred: name 'KMeans' is not defined.", "error": "", "parent_ids": ["bac52812-f32f-418b-90f0-9e1c38f46fc5"], "operator": null, "metadata": {}}
{"id": "3f42d631-962c-41ea-b790-1afdcb039bec", "fitness": 0.0, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with orthogonal crossover and enhanced parameter adaptation, focusing on exploration and exploitation balance.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.f = 0.5\n        self.cr = 0.9\n        self.archive_rate = 0.1 \n        self.archive = [] #To store inferior solutions that could be useful\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.population[best_idx]\n        \n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(np.arange(self.pop_size), 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                \n                # Archive interaction with probability\n                if np.random.rand() < self.archive_rate and len(self.archive) > 0:\n                    arc_idx = np.random.randint(len(self.archive))\n                    x4 = self.archive[arc_idx]\n                    v = x1 + self.f * (x2 - x3) + self.f * (x4 - self.population[i]) #Adding Archive influence\n                else:\n                    v = x1 + self.f * (x2 - x3)\n                \n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Orthogonal Crossover\n                u = self.orthogonal_crossover(self.population[i], v)\n                \n                # Evaluation\n                f = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f < self.fitness[i]:\n                    # Archive update: if parent is better, archive it. Helps exploration\n                    if self.fitness[i] < func(self.population[i]): # Original fitness of parent\n                         self.archive.append(self.population[i].copy())\n                         if len(self.archive) > self.pop_size: #Limit archive size\n                             self.archive.pop(0)\n                    \n                    self.fitness[i] = f\n                    self.population[i] = u\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n                \n                # Adaptive F/CR update - independent adaptation and simplified\n                self.f = np.clip(np.random.normal(0.5, 0.1), 0.1, 0.9)  # Smaller std for more fine-grained adaptation\n                self.cr = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0)  # Smaller std for more fine-grained adaptation\n\n                if self.evals >= self.budget:\n                    break\n        return self.f_opt, self.x_opt\n        \n    def orthogonal_crossover(self, x, v):\n        u = x.copy()\n        num_changed = 0\n        for j in range(self.dim):\n            if np.random.rand() < self.cr:\n                u[j] = v[j]\n                num_changed += 1\n        #Force at least one change\n        if num_changed == 0:\n          j = np.random.randint(self.dim)\n          u[j] = v[j]\n        return u", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a5797d13-598f-49e6-b84b-060778c7647b"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "1d732fb6-9ff6-4dd6-8f23-c3e788ec0b8f", "fitness": -Infinity, "name": "OrthogonalAdaptiveDE", "description": "Adaptive Differential Evolution with a diversity-preserving mechanism based on orthogonal design to enhance exploration and an adaptive local search.", "code": "import numpy as np\n\nclass OrthogonalAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, f=0.5, cr=0.9, ls_probability=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.f = f\n        self.cr = cr\n        self.archive = []\n        self.archive_fitness = []\n        self.ls_probability = ls_probability\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.population[best_idx]\n        \n        while self.evals < self.budget:\n            # Orthogonal Design for Diversity\n            orthogonal_matrix = self.generate_orthogonal_array(self.pop_size)\n\n            for i in range(self.pop_size):\n                # Mutation: Use best individual and two other random individuals\n                idxs = np.random.choice(np.arange(self.pop_size), 2, replace=False)\n                x1, x2 = self.population[idxs]\n                v = self.x_opt + self.f * (x1 - x2)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = self.crossover(self.population[i], v)\n                \n                # Evaluation\n                f = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f < self.fitness[i]:\n                    # Archive update: Replace worst archive member if new solution is better than worst archive and archive is full, else add to archive.\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i])\n                        self.archive_fitness.append(self.fitness[i])\n                    else:\n                        worst_archive_idx = np.argmax(self.archive_fitness)\n                        if f < self.archive_fitness[worst_archive_idx]:\n                            self.archive[worst_archive_idx] = self.population[i]\n                            self.archive_fitness[worst_archive_idx] = self.fitness[i]\n\n                    self.fitness[i] = f\n                    self.population[i] = u\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n                        \n                        # Local Search around best solution\n                        if np.random.rand() < self.ls_probability:\n                            self.x_opt, self.f_opt = self.local_search(self.x_opt, func)\n                \n                # Adaptive F and CR update: Simplified self-adaptation\n                self.f = np.clip(np.random.normal(0.5, 0.1), 0.1, 0.9)\n                self.cr = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0)\n                \n                # Diversity Preservation using Orthogonal Design\n                for j in range(self.dim):\n                    if orthogonal_matrix[i % orthogonal_matrix.shape[0], j % orthogonal_matrix.shape[1]] == 1:\n                        # Perturb dimensions of solutions based on orthogonal design.\n                        self.population[i, j] = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                self.fitness[i] = func(self.population[i])\n                self.evals +=1\n                \n                if self.evals >= self.budget:\n                    break\n        return self.f_opt, self.x_opt\n        \n    def crossover(self, x, v):\n        u = x.copy()\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.cr or j == j_rand:\n                u[j] = v[j]\n        return u\n    \n    def local_search(self, x, func, radius=0.1, iterations=5):\n        x_best = x.copy()\n        f_best = func(x)\n        self.evals += 1\n        \n        for _ in range(iterations):\n            x_new = x + np.random.uniform(-radius, radius, size=self.dim)\n            x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n            f_new = func(x_new)\n            self.evals += 1\n\n            if f_new < f_best:\n                f_best = f_new\n                x_best = x_new.copy()\n        \n        return x_best, f_best\n    \n    def generate_orthogonal_array(self, num_runs):\n        # This is a simplified example.  For larger designs, consider using specialized libraries\n        # or pre-computed orthogonal arrays.\n        if num_runs <= 2:\n            return np.array([[0, 1], [1, 0]])\n\n        k = int(np.ceil(np.log2(num_runs)))  # Ensure a power of 2\n        n = 2**k\n        \n        # Walsh Matrix (Hadamard Matrix for power of 2)\n        H = np.ones((1, 1))\n        while H.shape[0] < n:\n            H = np.vstack((np.hstack((H, H)), np.hstack((H, -H))))\n        \n        # Select a submatrix if num_runs is smaller than the full Hadamard matrix\n        orthogonal_array = (H[:num_runs, :]).astype(int)\n        orthogonal_array = (orthogonal_array + 1) // 2  # Convert -1/1 to 0/1\n        return orthogonal_array", "configspace": "", "generation": 8, "feedback": "An exception occurred: setting an array element with a sequence..", "error": "", "parent_ids": ["c97e2ca0-4d63-4810-a386-6a050bb19202"], "operator": null, "metadata": {}}
{"id": "742a7df5-9782-40f3-8ce8-fe8d1fc7e37f", "fitness": -Infinity, "name": "AdaptiveDEOrthogonal", "description": "Adaptive Differential Evolution with Orthogonal Learning, incorporating orthogonal experimental design to enhance the search efficiency and solution diversity.", "code": "import numpy as np\n\nclass AdaptiveDEOrthogonal:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, f=0.5, cr=0.9, orthogonal_levels=3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.f = f\n        self.cr = cr\n        self.archive = []\n        self.archive_fitness = []\n        self.orthogonal_levels = orthogonal_levels\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.population[best_idx]\n        \n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation: Use best individual and two other random individuals\n                idxs = np.random.choice(np.arange(self.pop_size), 2, replace=False)\n                x1, x2 = self.population[idxs]\n                v = self.x_opt + self.f * (x1 - x2)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = self.crossover(self.population[i], v)\n                \n                # Orthogonal Learning\n                u = self.orthogonal_learning(func, u)\n\n                # Evaluation\n                f = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f < self.fitness[i]:\n                    # Archive update: Replace worst archive member if new solution is better than worst archive and archive is full, else add to archive.\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i])\n                        self.archive_fitness.append(self.fitness[i])\n                    else:\n                        worst_archive_idx = np.argmax(self.archive_fitness)\n                        if f < self.archive_fitness[worst_archive_idx]:\n                            self.archive[worst_archive_idx] = self.population[i]\n                            self.archive_fitness[worst_archive_idx] = self.fitness[i]\n\n                    self.fitness[i] = f\n                    self.population[i] = u\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n                \n                # Adaptive F and CR update: Simplified self-adaptation\n                self.f = np.clip(np.random.normal(0.5, 0.1), 0.1, 0.9)\n                self.cr = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0)\n\n                if self.evals >= self.budget:\n                    break\n        return self.f_opt, self.x_opt\n        \n    def crossover(self, x, v):\n        u = x.copy()\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.cr or j == j_rand:\n                u[j] = v[j]\n        return u\n\n    def orthogonal_learning(self, func, x):\n        # Select a subset of dimensions for orthogonal experimental design\n        k = min(self.orthogonal_levels, self.dim)  # Number of dimensions to optimize in orthogonal design\n        dims = np.random.choice(np.arange(self.dim), k, replace=False)\n        \n        # Generate orthogonal array (L_k(orthogonal_levels^k))\n        orthogonal_array = self.generate_orthogonal_array(k, self.orthogonal_levels)\n        \n        # Create a matrix of solutions based on the orthogonal array\n        solutions = np.zeros((self.orthogonal_levels, self.dim))\n        for i in range(self.orthogonal_levels):\n            solutions[i, :] = x.copy()\n            for j, dim_idx in enumerate(dims):\n                # Define the levels for each selected dimension\n                level_values = np.linspace(func.bounds.lb, func.bounds.ub, self.orthogonal_levels)\n                solutions[i, dim_idx] = level_values[orthogonal_array[i, j]]\n        \n        # Evaluate the solutions\n        fitness_values = np.array([func(s) for s in solutions])\n        self.evals += self.orthogonal_levels\n        \n        # Select the best solution\n        best_idx = np.argmin(fitness_values)\n        best_solution = solutions[best_idx, :]\n        \n        return best_solution\n\n    def generate_orthogonal_array(self, k, levels):\n        # This is a simplified version. For a more robust implementation,\n        # you would typically use a pre-computed orthogonal array based on k and levels\n        # or use an orthogonal array generation library.\n\n        # This simple version only works for levels=3.  For a general version,\n        # a more complex construction or a lookup table would be needed.\n        if levels != 3:\n            raise ValueError(\"Only orthogonal arrays with 3 levels are currently supported.\")\n\n        if k == 2:\n            return np.array([[0, 0], [0, 1], [0, 2],\n                             [1, 0], [1, 1], [1, 2],\n                             [2, 0], [2, 1], [2, 2]])\n        elif k == 3:\n            return np.array([[0, 0, 0], [0, 1, 1], [0, 2, 2],\n                             [1, 0, 1], [1, 1, 2], [1, 2, 0],\n                             [2, 0, 2], [2, 1, 0], [2, 2, 1]])\n        elif k == 4:\n            return np.array([[0, 0, 0, 0], [0, 1, 1, 1], [0, 2, 2, 2],\n                             [1, 0, 1, 2], [1, 1, 2, 0], [1, 2, 0, 1],\n                             [2, 0, 2, 1], [2, 1, 0, 2], [2, 2, 1, 0]])\n\n        else:\n            # Return a random array for unsupported cases to keep the program running.\n            # Replace with a proper implementation for higher levels.\n            return np.random.randint(0, levels, size=(levels, k))", "configspace": "", "generation": 8, "feedback": "An exception occurred: setting an array element with a sequence..", "error": "", "parent_ids": ["c97e2ca0-4d63-4810-a386-6a050bb19202"], "operator": null, "metadata": {}}
{"id": "1ea5df31-83c1-4b63-9186-79f688435719", "fitness": 0.0, "name": "AdaptiveDEOrthogonalRestart", "description": "Adaptive Differential Evolution with orthogonal learning and a restart mechanism to enhance exploration.", "code": "import numpy as np\n\nclass AdaptiveDEOrthogonalRestart:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, f=0.5, cr=0.9, restart_trigger=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.f = f\n        self.cr = cr\n        self.archive = []\n        self.archive_fitness = []\n        self.restart_trigger = restart_trigger # Percentage of budget used before restart consideration\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.initial_fitness = self.fitness.copy()\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.population[best_idx]\n        \n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation: Use best individual and two other random individuals\n                idxs = np.random.choice(np.arange(self.pop_size), 2, replace=False)\n                x1, x2 = self.population[idxs]\n                v = self.x_opt + self.f * (x1 - x2)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = self.crossover(self.population[i], v)\n                \n                # Orthogonal Learning\n                u = self.orthogonal_learning(func, u)\n\n                # Evaluation\n                f = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f < self.fitness[i]:\n                    # Archive update: Replace worst archive member if new solution is better than worst archive and archive is full, else add to archive.\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i])\n                        self.archive_fitness.append(self.fitness[i])\n                    else:\n                        worst_archive_idx = np.argmax(self.archive_fitness)\n                        if f < self.archive_fitness[worst_archive_idx]:\n                            self.archive[worst_archive_idx] = self.population[i]\n                            self.archive_fitness[worst_archive_idx] = self.fitness[i]\n\n                    self.fitness[i] = f\n                    self.population[i] = u\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n                \n                # Adaptive F and CR update: Simplified self-adaptation\n                self.f = np.clip(np.random.normal(0.5, 0.1), 0.1, 0.9)\n                self.cr = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0)\n\n                # Restart Mechanism: If stuck, reinitialize population\n                if self.evals > self.restart_trigger * self.budget:\n                    fitness_change = np.abs(self.fitness - self.initial_fitness).mean()\n                    if fitness_change < 1e-6:  #Stagnation detected\n                        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                        self.fitness = np.array([func(x) for x in self.population])\n                        self.initial_fitness = self.fitness.copy()\n                        self.evals += self.pop_size\n                        best_idx = np.argmin(self.fitness)\n                        self.f_opt = self.fitness[best_idx]\n                        self.x_opt = self.population[best_idx]\n                        \n\n                if self.evals >= self.budget:\n                    break\n        return self.f_opt, self.x_opt\n        \n    def crossover(self, x, v):\n        u = x.copy()\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.cr or j == j_rand:\n                u[j] = v[j]\n        return u\n\n    def orthogonal_learning(self, func, x):\n        # Generate a set of candidate solutions around x using orthogonal design\n        levels = 3  # Number of levels for each dimension\n        candidates = []\n        \n        # Latin Hypercube Sampling (LHS) to generate design points\n        design = self.latin_hypercube_sampling(levels, self.dim)\n        \n        for i in range(levels**self.dim):\n            candidate = x.copy()\n            for j in range(self.dim):\n                # Map design points to the search space\n                level = design[i % levels, j] #Simplified - cyclical pattern\n                range_val = func.bounds.ub[j] - func.bounds.lb[j]\n                candidate[j] = x[j] + (level - 1) * (range_val / (levels - 1)) * 0.1 #Small perturbation\n                candidate[j] = np.clip(candidate[j], func.bounds.lb[j], func.bounds.ub[j])\n            candidates.append(candidate)\n\n        # Evaluate all candidate solutions\n        fitness_values = [func(c) for c in candidates]\n\n        # Select the best candidate\n        best_index = np.argmin(fitness_values)\n        best_candidate = candidates[best_index]\n\n        return best_candidate\n\n    def latin_hypercube_sampling(self, levels, dim):\n        # Generate a Latin Hypercube Sample design\n        design = np.zeros((levels, dim))\n        for j in range(dim):\n            design[:, j] = np.random.permutation(levels)\n        return design", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveDEOrthogonalRestart scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c97e2ca0-4d63-4810-a386-6a050bb19202"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "6edf69fc-a757-43d7-a5bd-edf0baae55a5", "fitness": 0.0, "name": "OrthogonalAdaptiveDE", "description": "Adaptive Differential Evolution with orthogonal learning to enhance search space exploration and exploitation.", "code": "import numpy as np\n\nclass OrthogonalAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, f=0.5, cr=0.9, orthogonal_dim=3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.f = f\n        self.cr = cr\n        self.archive = []\n        self.archive_fitness = []\n        self.orthogonal_dim = orthogonal_dim\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.population[best_idx]\n        \n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation: Use best individual and two other random individuals\n                idxs = np.random.choice(np.arange(self.pop_size), 2, replace=False)\n                x1, x2 = self.population[idxs]\n                v = self.x_opt + self.f * (x1 - x2)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = self.crossover(self.population[i], v)\n                \n                # Orthogonal learning\n                u = self.orthogonal_learning(func, u)\n\n                # Evaluation\n                f = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f < self.fitness[i]:\n                    # Archive update: Replace worst archive member if new solution is better than worst archive and archive is full, else add to archive.\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i])\n                        self.archive_fitness.append(self.fitness[i])\n                    else:\n                        worst_archive_idx = np.argmax(self.archive_fitness)\n                        if f < self.archive_fitness[worst_archive_idx]:\n                            self.archive[worst_archive_idx] = self.population[i]\n                            self.archive_fitness[worst_archive_idx] = self.fitness[i]\n\n                    self.fitness[i] = f\n                    self.population[i] = u\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n                \n                # Adaptive F and CR update: Simplified self-adaptation\n                self.f = np.clip(np.random.normal(0.5, 0.1), 0.1, 0.9)\n                self.cr = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0)\n\n                if self.evals >= self.budget:\n                    break\n        return self.f_opt, self.x_opt\n        \n    def crossover(self, x, v):\n        u = x.copy()\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.cr or j == j_rand:\n                u[j] = v[j]\n        return u\n\n    def orthogonal_learning(self, func, x):\n        # Orthogonal array design for parameter tuning.\n\n        levels = 3  # Number of levels for each factor\n        if self.dim < self.orthogonal_dim:\n          self.orthogonal_dim = self.dim\n        \n        # Generate an orthogonal array (OA) using Plackett-Burman design for small sample size\n        if self.orthogonal_dim==2:\n          oa = [[-1, -1],[-1,1],[1,-1],[1,1]]\n        elif self.orthogonal_dim==3:\n          oa = [[-1, -1, -1],[-1, 1, 1],[1, -1, 1],[1, 1, -1]]\n        elif self.orthogonal_dim==4:\n          oa = [[-1, -1, -1, -1], [-1, -1, 1, 1], [-1, 1, -1, 1], [-1, 1, 1, -1], [1, -1, -1, 1], [1, -1, 1, -1], [1, 1, -1, -1], [1, 1, 1, 1]]\n        else:\n          oa = [[-1, -1, -1, -1, -1, -1], [-1, -1, -1, 1, 1, 1], [-1, -1, 1, -1, 1, 1], [-1, -1, 1, 1, -1, -1], [-1, 1, -1, -1, 1, 1], [-1, 1, -1, 1, -1, -1], [-1, 1, 1, -1, -1, -1], [-1, 1, 1, 1, 1, 1], [1, -1, -1, -1, -1, 1], [1, -1, -1, 1, 1, -1], [1, -1, 1, -1, 1, -1], [1, -1, 1, 1, -1, 1], [1, 1, -1, -1, 1, -1], [1, 1, -1, 1, -1, 1], [1, 1, 1, -1, -1, 1], [1, 1, 1, 1, 1, -1]]\n          oa = oa[:2**self.orthogonal_dim]\n\n        \n        candidates = []\n        \n        # Scale parameters to [0, 1]\n        x_norm = (x - func.bounds.lb) / (func.bounds.ub - func.bounds.lb)\n        \n        # Ensure values are within [0, 1]\n        x_norm = np.clip(x_norm, 0, 1)\n\n        #Generate the different candidates based on the OA\n        for row in oa:\n            temp_x = x_norm.copy()\n            for j in range(self.orthogonal_dim):\n                # Map the OA values (-1, 1) to the parameter range\n                temp_x[j] = x_norm[j] + row[j] * 0.1  # Small perturbation\n                temp_x[j] = np.clip(temp_x[j], 0, 1) # Clip values to [0, 1]\n            \n            #Scale back to original range\n            temp_x = temp_x * (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n            candidates.append(temp_x)\n            \n        # Evaluate the candidates\n        fitness_values = [func(candidate) for candidate in candidates]\n        self.evals += len(candidates)\n\n        # Select the best candidate\n        best_index = np.argmin(fitness_values)\n        \n        if fitness_values[best_index] < func(x):\n           return candidates[best_index]\n        else:\n           return x", "configspace": "", "generation": 8, "feedback": "The algorithm OrthogonalAdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c97e2ca0-4d63-4810-a386-6a050bb19202"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "a0854125-0ccf-4665-a7af-bc51a19430e1", "fitness": 0.6340713764555634, "name": "AdaptiveDE", "description": "Streamlined Adaptive DE with simplified parameter adaptation and reduced archive usage for efficiency.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, f=0.5, cr=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.f = f\n        self.cr = cr\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.population[best_idx]\n        \n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation: Use best individual and two other random individuals\n                idxs = np.random.choice(np.arange(self.pop_size), 2, replace=False)\n                x1, x2 = self.population[idxs]\n                v = self.x_opt + self.f * (x1 - x2)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = self.crossover(self.population[i], v)\n                \n                # Evaluation\n                f = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.population[i] = u\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n                \n                # Adaptive F and CR update: Simplified self-adaptation\n                self.f = np.clip(np.random.normal(0.5, 0.1), 0.1, 0.9)\n                self.cr = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0)\n\n                if self.evals >= self.budget:\n                    break\n        return self.f_opt, self.x_opt\n        \n    def crossover(self, x, v):\n        u = x.copy()\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.cr or j == j_rand:\n                u[j] = v[j]\n        return u", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveDE scored 0.634 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c97e2ca0-4d63-4810-a386-6a050bb19202"], "operator": null, "metadata": {"aucs": [0.15306234485096304, 0.9090422526506097, 0.9322915392776812, 0.9699560882956135, 0.35493638875327627, 0.9421895963390653, 0.3719822750417082, 0.8331368768006772, 0.3886138024091671, 0.22901257045956303, 0.965738434563111, 0.9991281065986083, 0.4281305362790099, 0.2740870836921966, 0.9731884898009201, 0.9248044998106801, 0.34161945139336625, 0.9535051068221123, 0.22301671739692341, 0.5139853678760169]}}
{"id": "cbb646e9-3456-4ef6-b2e0-932a0315b5ad", "fitness": 0.25098566413565127, "name": "RingAdaptiveDE", "description": "Adaptive Differential Evolution with a ring topology-based mutation and a combined adaptation strategy for F and CR, incorporating both success-history and population-wide learning.", "code": "import numpy as np\n\nclass RingAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, memory_size=10, ring_neighbors=3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.memory_size = memory_size\n        self.ring_neighbors = ring_neighbors\n        self.memory_cr = np.ones(self.memory_size) * 0.5\n        self.memory_f = np.ones(self.memory_size) * 0.5\n        self.sf_history = []\n        self.scr_history = []\n        self.cr_learn_rate = 0.1\n        self.f_learn_rate = 0.1\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.population[best_idx]\n        \n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Parameter adaptation (success-history based)\n                memory_index = np.random.randint(self.memory_size)\n                cr = self.memory_cr[memory_index]\n                f = self.memory_f[memory_index]\n                \n                # Ring Topology-based Mutation\n                neighbors = [(i - j) % self.pop_size for j in range(1, self.ring_neighbors + 1)] + \\\n                            [(i + j) % self.pop_size for j in range(1, self.ring_neighbors + 1)]\n                \n                idxs = np.random.choice(neighbors, 2, replace=False)\n                x1, x2 = self.population[idxs]\n                \n                v = self.population[i] + f * (x1 - x2)  # Current individual as base\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = self.crossover(self.population[i], v, cr)\n                \n                # Evaluation\n                f_new = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f_new < self.fitness[i]:\n                    # Update memory based on success\n                    self.sf_history.append(f)\n                    self.scr_history.append(cr)\n\n                    # Update memory with success\n                    self.memory_cr[memory_index] = (1 - self.cr_learn_rate) * self.memory_cr[memory_index] + self.cr_learn_rate * cr\n                    self.memory_f[memory_index] = (1 - self.f_learn_rate) * self.memory_f[memory_index] + self.f_learn_rate * f\n\n                    self.fitness[i] = f_new\n                    self.population[i] = u\n\n                    if f_new < self.f_opt:\n                        self.f_opt = f_new\n                        self.x_opt = u\n                else:\n                     # Population-wide learning: adjust CR/F based on failed attempt\n                    self.memory_cr[memory_index] = (1 - self.cr_learn_rate) * self.memory_cr[memory_index]\n                    self.memory_f[memory_index] = (1 - self.f_learn_rate) * self.memory_f[memory_index]\n                \n                if self.evals >= self.budget:\n                    break\n        return self.f_opt, self.x_opt\n        \n    def crossover(self, x, v, cr):\n        u = x.copy()\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < cr or j == j_rand:\n                u[j] = v[j]\n        return u", "configspace": "", "generation": 8, "feedback": "The algorithm RingAdaptiveDE scored 0.251 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["29f29744-96f4-4b66-b541-080b91fde932"], "operator": null, "metadata": {"aucs": [0.1039335910251995, 0.21280856176179064, 0.2572594516979315, 0.1568486083329521, 0.17160816951502766, 0.1851063305297962, 0.21330826526706215, 0.20610514795562296, 0.16480899401199822, 0.1641377353491853, 0.17917760300072227, 0.9981027594714009, 0.2636302582571194, 0.17293934824440216, 0.2415428075753091, 0.28131770193411576, 0.20992258682292664, 0.21396158394026066, 0.1830377125237338, 0.4401560654964687]}}
{"id": "cce569a0-e014-4772-9609-01f033bde6f6", "fitness": 0.7603679130085187, "name": "AdaptiveDE", "description": "Simplified Adaptive Differential Evolution with improved parameter adaptation and archive handling for enhanced exploitation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=5, memory_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.memory_size = memory_size\n        self.memory_cr = np.ones(self.memory_size) * 0.5\n        self.memory_f = np.ones(self.memory_size) * 0.5\n        self.archive = []\n        self.sf_history = []\n        self.scr_history = []\n        self.p_best = 0.05  # Probability of selecting p-best individual\n        self.epsilon = 1e-6\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.population[best_idx]\n        \n        while self.evals < self.budget:\n            ranked_indices = np.argsort(self.fitness) # Rank individuals\n            \n            for i in range(self.pop_size):\n                # Parameter adaptation\n                memory_index = np.random.randint(self.memory_size)\n                cr = self.memory_cr[memory_index]\n                f = self.memory_f[memory_index]\n\n                # Mutation: p-best selection\n                p_best_count = max(1, int(self.pop_size * self.p_best))\n                p_best_indices = ranked_indices[:p_best_count]\n                best_idx = np.random.choice(p_best_indices)  # Select from top p%\n                \n                idxs = np.random.choice(np.arange(self.pop_size), 2, replace=False)\n                x1, x2 = self.population[idxs]\n                \n                v = self.population[best_idx] + f * (x1 - x2)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = self.crossover(self.population[i], v, cr)\n                \n                # Evaluation\n                f_new = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f_new < self.fitness[i]:\n                    # Archive update (simplified)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        self.archive[np.random.randint(self.archive_size)] = self.population[i].copy()\n\n                    self.fitness[i] = f_new\n                    self.population[i] = u\n\n                    if f_new < self.f_opt:\n                        self.f_opt = f_new\n                        self.x_opt = u\n\n                    # Success history update\n                    self.sf_history.append(f)\n                    self.scr_history.append(cr)\n                    self.update_memory(memory_index, f, cr)\n                \n                if self.evals >= self.budget:\n                    break\n        return self.f_opt, self.x_opt\n        \n    def crossover(self, x, v, cr):\n        u = x.copy()\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < cr or j == j_rand:\n                u[j] = v[j]\n        return u\n\n    def update_memory(self, i, f, cr):\n        self.memory_cr[i] = (0.9 * self.memory_cr[i] + 0.1 * cr) if cr > 0 else 0.5\n        self.memory_f[i] = (0.9 * self.memory_f[i] + 0.1 * f) if f > 0 else 0.5", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveDE scored 0.760 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["29f29744-96f4-4b66-b541-080b91fde932"], "operator": null, "metadata": {"aucs": [0.35113799114183575, 0.736114036506436, 0.8341782768162103, 0.917724976790613, 0.8519503716277426, 0.8754410493837197, 0.76583604473115, 0.7879430297158798, 0.866453591303937, 0.7955472433993878, 0.9012531997819966, 0.9976202327595585, 0.44332089646217676, 0.8349395719606496, 0.9386811972310214, 0.886441144311142, 0.7758308822850677, 0.9015722055386572, 0.22373354170903514, 0.5216387767141559]}}
{"id": "a8d8a418-c003-4a94-851e-c2f61b901ff6", "fitness": 0.5697097449474029, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with improved archive handling, orthogonal crossover, and dynamic p-best selection for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=5, memory_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.memory_size = memory_size\n        self.memory_cr = np.ones(self.memory_size) * 0.5\n        self.memory_f = np.ones(self.memory_size) * 0.5\n        self.archive = []\n        self.archive_fitness = []\n        self.sf_history = []\n        self.scr_history = []\n        self.p_best_initial = 0.1  # Initial probability of selecting p-best individual\n        self.p_best = self.p_best_initial\n        self.p_best_decay = 0.995 # Decay factor for p_best, gradually reducing over time\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.population[best_idx]\n        \n        while self.evals < self.budget:\n            ranked_indices = np.argsort(self.fitness) # Rank individuals\n            \n            for i in range(self.pop_size):\n                # Parameter adaptation\n                memory_index = np.random.randint(self.memory_size)\n                cr = self.memory_cr[memory_index]\n                f = self.memory_f[memory_index]\n\n                # Mutation: p-best selection\n                p_best_count = max(1, int(self.pop_size * self.p_best))\n                p_best_indices = ranked_indices[:p_best_count]\n                best_idx = np.random.choice(p_best_indices)  # Select from top p%\n                \n                idxs = np.random.choice(np.arange(self.pop_size), 2, replace=False)\n                x1, x2 = self.population[idxs]\n                \n                v = self.population[best_idx] + f * (x1 - x2)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = self.crossover(self.population[i], v, cr)\n                \n                # Repair mechanism\n                u = self.repair(u, func.bounds.lb, func.bounds.ub)\n                \n                # Evaluation\n                f_new = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f_new < self.fitness[i]:\n                    # Archive update - probabilistic replacement of worst\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                        self.archive_fitness.append(self.fitness[i])\n                    else:\n                        # Replace worst with probability proportional to fitness improvement\n                        worst_archive_idx = np.argmax(self.archive_fitness)\n                        improvement = self.archive_fitness[worst_archive_idx] - f_new\n                        probability = improvement / (self.archive_fitness[worst_archive_idx] + 1e-8) # Avoid division by zero\n\n                        if np.random.rand() < probability:\n                            self.archive[worst_archive_idx] = self.population[i].copy()\n                            self.archive_fitness[worst_archive_idx] = self.fitness[i]\n\n                    self.fitness[i] = f_new\n                    self.population[i] = u.copy()\n\n                    if f_new < self.f_opt:\n                        self.f_opt = f_new\n                        self.x_opt = u.copy()\n\n                    # Success history update\n                    self.sf_history.append(f)\n                    self.scr_history.append(cr)\n\n                    self.memory_cr[memory_index] = cr\n                    self.memory_f[memory_index] = f\n                else:\n                    pass #No success\n                \n                if self.evals >= self.budget:\n                    break\n            \n            # Decay p_best over time\n            self.p_best *= self.p_best_decay\n\n        return self.f_opt, self.x_opt\n        \n    def crossover(self, x, v, cr):\n        # Orthogonal Crossover\n        u = x.copy()\n        if np.random.rand() < 0.5: # 50% chance to perform crossover\n            for j in range(self.dim):\n                if np.random.rand() < cr:\n                    u[j] = v[j]\n        return u\n\n    def repair(self, x, lb, ub):\n        # Simple bound repair\n        x = np.clip(x, lb, ub)\n        return x", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveDE scored 0.570 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["29f29744-96f4-4b66-b541-080b91fde932"], "operator": null, "metadata": {"aucs": [0.19678485113747624, 0.3026023051868535, 0.5695349308522347, 0.8102587485450492, 0.6456117809222487, 0.7031285794963658, 0.3485887434074051, 0.5558940286572536, 0.6342550800338091, 0.5727020295216692, 0.7747203233823088, 0.9888120265206671, 0.3031562732577441, 0.56454552814114, 0.7244971427214704, 0.74848730380497, 0.47063095671885236, 0.7595111564702838, 0.20568699265450385, 0.5147861175157529]}}
{"id": "3cebd8ad-1b65-43ff-88d7-161eb649db64", "fitness": -Infinity, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with orthogonal learning to enhance population diversity and convergence speed.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=5, memory_size=10, orthogonal_learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.memory_size = memory_size\n        self.memory_cr = np.ones(self.memory_size) * 0.5\n        self.memory_f = np.ones(self.memory_size) * 0.5\n        self.archive = []\n        self.archive_fitness = []\n        self.sf_history = []\n        self.scr_history = []\n        self.p_best = 0.05  # Probability of selecting p-best individual\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.population[best_idx]\n        \n        while self.evals < self.budget:\n            ranked_indices = np.argsort(self.fitness) # Rank individuals\n            \n            for i in range(self.pop_size):\n                # Parameter adaptation\n                memory_index = np.random.randint(self.memory_size)\n                cr = self.memory_cr[memory_index]\n                f = self.memory_f[memory_index]\n\n                # Mutation: p-best selection\n                p_best_count = max(1, int(self.pop_size * self.p_best))\n                p_best_indices = ranked_indices[:p_best_count]\n                best_idx = np.random.choice(p_best_indices)  # Select from top p%\n                \n                idxs = np.random.choice(np.arange(self.pop_size), 2, replace=False)\n                x1, x2 = self.population[idxs]\n                \n                v = self.population[best_idx] + f * (x1 - x2)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = self.crossover(self.population[i], v, cr)\n                \n                # Repair mechanism\n                u = self.repair(u, func.bounds.lb, func.bounds.ub)\n\n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_learning_rate:\n                    u = self.orthogonal_learning(u, func, ranked_indices[:5]) # Top 5 solutions\n                \n                # Evaluation\n                f_new = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f_new < self.fitness[i]:\n                    # Archive update\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i])\n                        self.archive_fitness.append(self.fitness[i])\n                    else:\n                        worst_archive_idx = np.argmax(self.archive_fitness)\n                        if f_new < self.archive_fitness[worst_archive_idx]:\n                            self.archive[worst_archive_idx] = self.population[i]\n                            self.archive_fitness[worst_archive_idx] = self.fitness[i]\n\n                    self.fitness[i] = f_new\n                    self.population[i] = u\n\n                    if f_new < self.f_opt:\n                        self.f_opt = f_new\n                        self.x_opt = u\n\n                    # Success history update\n                    self.sf_history.append(f)\n                    self.scr_history.append(cr)\n\n                    self.memory_cr[memory_index] = cr\n                    self.memory_f[memory_index] = f\n                else:\n                    pass #No success\n                \n                if self.evals >= self.budget:\n                    break\n        return self.f_opt, self.x_opt\n        \n    def crossover(self, x, v, cr):\n        u = x.copy()\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < cr or j == j_rand:\n                u[j] = v[j]\n        return u\n\n    def repair(self, x, lb, ub):\n        # Simple bound repair\n        x = np.clip(x, lb, ub)\n        return x\n\n    def orthogonal_learning(self, x, func, best_indices, num_samples=5):\n        # Generate orthogonal array (OA)\n        levels = 3 # Three levels: low, mid, high\n        oa = self.generate_orthogonal_array(self.dim, levels)\n        \n        # Define factor levels based on current solution and best solutions\n        factor_levels = np.zeros((self.dim, levels))\n        for j in range(self.dim):\n            factor_levels[j, 0] = func.bounds.lb\n            factor_levels[j, 1] = x[j] # Current value\n            factor_levels[j, 2] = func.bounds.ub\n            \n        # Generate sample points based on OA and factor levels\n        samples = np.zeros((oa.shape[0], self.dim))\n        for i in range(oa.shape[0]):\n            for j in range(self.dim):\n                samples[i, j] = factor_levels[j, oa[i, j]]\n\n        # Evaluate sample points\n        fitness_values = np.array([func(sample) for sample in samples])\n        \n        # Find the best sample\n        best_sample_index = np.argmin(fitness_values)\n        best_sample = samples[best_sample_index]\n\n        return best_sample\n\n\n    def generate_orthogonal_array(self, n, k):\n        # A simple, fixed orthogonal array for demonstration (L9 array)\n        if n <= 2:\n            return np.array([[0, 0], [0, 1], [0, 2], [1, 0], [1, 1], [1, 2], [2, 0], [2, 1], [2, 2]])\n\n        oa = np.array([\n            [0, 0, 0, 0],\n            [0, 1, 1, 1],\n            [0, 2, 2, 2],\n            [1, 0, 1, 2],\n            [1, 1, 2, 0],\n            [1, 2, 0, 1],\n            [2, 0, 2, 1],\n            [2, 1, 0, 2],\n            [2, 2, 1, 0]\n        ])\n\n        if n <= 4:\n            return oa[:, :n]\n\n        oa = np.pad(oa, ((0,0),(0,n-4)), 'constant', constant_values=(0))\n\n        for i in range(oa.shape[1]):\n            for j in range(oa.shape[0]):\n                oa[j, i] = j % 3\n\n        return oa", "configspace": "", "generation": 8, "feedback": "An exception occurred: setting an array element with a sequence..", "error": "", "parent_ids": ["29f29744-96f4-4b66-b541-080b91fde932"], "operator": null, "metadata": {}}
{"id": "3778709d-2463-4e10-8d77-a9d25e483dbd", "fitness": 0.7810109261999348, "name": "EnhancedAdaptiveDE", "description": "Enhanced Adaptive DE with orthogonal design for crossover and improved archive handling for better exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=5, memory_size=10, orthogonal_design_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.memory_size = memory_size\n        self.orthogonal_design_size = orthogonal_design_size\n        self.memory_cr = np.ones(self.memory_size) * 0.5\n        self.memory_f = np.ones(self.memory_size) * 0.5\n        self.archive = []\n        self.archive_fitness = []\n        self.sf_history = []\n        self.scr_history = []\n        self.p_best = 0.05  # Probability of selecting p-best individual\n\n        # Initialize Orthogonal Design\n        self.orthogonal_design = self.create_orthogonal_design(self.orthogonal_design_size, self.dim)\n\n    def create_orthogonal_design(self, size, dim):\n        # A simple orthogonal design (can be replaced with a more sophisticated one)\n        design = np.random.choice([0, 1], size=(size, dim))\n        return design\n        \n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        \n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.population[best_idx]\n        \n        while self.evals < self.budget:\n            ranked_indices = np.argsort(self.fitness) # Rank individuals\n            \n            for i in range(self.pop_size):\n                # Parameter adaptation\n                memory_index = np.random.randint(self.memory_size)\n                cr = self.memory_cr[memory_index]\n                f = self.memory_f[memory_index]\n\n                # Mutation: p-best selection\n                p_best_count = max(1, int(self.pop_size * self.p_best))\n                p_best_indices = ranked_indices[:p_best_count]\n                best_idx = np.random.choice(p_best_indices)  # Select from top p%\n                \n                idxs = np.random.choice(np.arange(self.pop_size), 2, replace=False)\n                x1, x2 = self.population[idxs]\n                \n                v = self.population[best_idx] + f * (x1 - x2)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover using Orthogonal Design\n                u = self.crossover_orthogonal(self.population[i], v, cr)\n                \n                # Repair mechanism\n                u = self.repair(u, func.bounds.lb, func.bounds.ub)\n                \n                # Evaluation\n                f_new = func(u)\n                self.evals += 1\n                \n                # Selection\n                if f_new < self.fitness[i]:\n                    # Archive update with replacement strategy based on fitness difference\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i])\n                        self.archive_fitness.append(self.fitness[i])\n                    else:\n                        fitness_diff = np.abs(np.array(self.archive_fitness) - self.fitness[i])\n                        worst_archive_idx = np.argmax(fitness_diff)  # Replace archive member with smallest fitness difference\n                        if f_new < self.archive_fitness[worst_archive_idx]:\n                            self.archive[worst_archive_idx] = self.population[i]\n                            self.archive_fitness[worst_archive_idx] = self.fitness[i]\n\n                    self.fitness[i] = f_new\n                    self.population[i] = u\n\n                    if f_new < self.f_opt:\n                        self.f_opt = f_new\n                        self.x_opt = u\n\n                    # Success history update\n                    self.sf_history.append(f)\n                    self.scr_history.append(cr)\n\n                    self.memory_cr[memory_index] = cr\n                    self.memory_f[memory_index] = f\n                else:\n                    pass #No success\n                \n                if self.evals >= self.budget:\n                    break\n        return self.f_opt, self.x_opt\n\n    def crossover_orthogonal(self, x, v, cr):\n        u = x.copy()\n        design_vector = self.orthogonal_design[np.random.randint(self.orthogonal_design_size)]\n        for j in range(self.dim):\n            if np.random.rand() < cr or design_vector[j] == 1: #Apply cr or orthogonal design\n                u[j] = v[j]\n        return u\n        \n    def crossover(self, x, v, cr):\n        u = x.copy()\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < cr or j == j_rand:\n                u[j] = v[j]\n        return u\n\n    def repair(self, x, lb, ub):\n        # Simple bound repair\n        x = np.clip(x, lb, ub)\n        return x", "configspace": "", "generation": 8, "feedback": "The algorithm EnhancedAdaptiveDE scored 0.781 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["29f29744-96f4-4b66-b541-080b91fde932"], "operator": null, "metadata": {"aucs": [0.5299601965810206, 0.8432229150983317, 0.8588410536822138, 0.9260138182330223, 0.8993638072949811, 0.8984166391312562, 0.3599421587235728, 0.8560124357729113, 0.895017780582549, 0.2686634333864184, 0.936525084389003, 0.9944439707511562, 0.6137102178071425, 0.8834625495971367, 0.9639667948609351, 0.9022151909671399, 0.8108071268814494, 0.9336547297620164, 0.7342629121618267, 0.5117157083346122]}}
