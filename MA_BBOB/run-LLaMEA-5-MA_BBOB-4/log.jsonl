{"id": "793e646c-56c5-47f8-a505-da4bb9bb923d", "fitness": -Infinity, "name": "CMAES", "description": "Covariance matrix adaptation evolution strategy with resampling and budget-aware step size adaptation.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damps=1, c_mu=0.3, c_1=0.3):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.m = None\n        self.sigma = 0.5\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.chiN = None\n        self.cs = cs\n        self.damps = damps\n        self.c_mu = c_mu\n        self.c_1 = c_1\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def sample(self):\n        z = np.random.randn(self.dim, self.popsize)\n        x = self.m + self.sigma * np.dot(self.A, z)\n        return x\n\n    def check_bounds(self, x):\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        self.m = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.A = np.eye(self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1/(4*self.dim) + 1/(21*self.dim**2))\n        \n        while self.eval_count < self.budget:\n            x = self.sample()\n            x = self.check_bounds(x)\n            \n            f = np.array([func(x[:,i]) if self.eval_count < self.budget else np.inf for i in range(self.popsize)])\n            self.eval_count += self.popsize\n            \n            if np.min(f) < self.f_opt:\n                    self.f_opt = np.min(f)\n                    self.x_opt = x[:, np.argmin(f)]\n\n            idx = np.argsort(f)\n            x_mu = x[:, idx[:self.mu]]\n\n            m_old = self.m.copy()\n            self.m = np.sum(x_mu * self.weights[None, :], axis=1)\n\n            z_mu = np.linalg.solve(self.A, x_mu - m_old[:, None])\n            \n            self.ps = (1 - self.cs) * self.ps + (self.cs * (2 - self.cs))**0.5 * (self.m - m_old) / self.sigma\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.eval_count / self.popsize)) / self.chiN < 1.4 + 2/(self.dim+1))\n            self.pc = (1 - self.damps) * self.pc + hsig * (self.damps * (2 - self.damps))**0.5 * (self.m - m_old) / self.sigma\n\n            self.C = (1 - self.c_1 - self.c_mu) * self.C + self.c_1 * (self.pc[:, None] @ self.pc[None, :]) + self.c_mu * (z_mu * self.weights[None, :]) @ z_mu.T\n            \n            self.sigma *= np.exp((np.linalg.norm(self.ps) / self.chiN - 1) * self.cs / self.damps)\n\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            self.C = np.diag(np.diag(self.C)) + 0.9 * (self.C - np.diag(np.diag(self.C)))\n            try:\n                self.A = np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-10 * np.eye(self.dim)  # Add a small constant to the diagonal\n                self.A = np.linalg.cholesky(self.C)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "An exception occurred: operands could not be broadcast together with shapes (2,) (2,6) .", "error": "", "parent_ids": [], "operator": null, "metadata": {}}
{"id": "89524e9b-d516-4ec9-abbc-5a98ae425a7c", "fitness": 0.0, "name": "CMAES", "description": "Covariance Matrix Adaptation Evolution Strategy (CMA-ES) with restart mechanism for improved exploration.", "code": "import numpy as np\nfrom scipy.linalg import sqrtm\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, sigma0=0.5, cs=0.3, damps=1.0, ccov1=0.0, ccovmu=0.0, restarts=3):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.sigma = sigma0\n        self.cs = cs\n        self.damps = damps\n        self.ccov1 = ccov1\n        self.ccovmu = ccovmu\n        self.restarts = restarts\n\n        self.mu = None\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.eigen_updated = False\n        self.B = None\n        self.D = None\n        self.diagC = None\n        self.count_evals = 0\n\n    def initialize(self):\n        self.mu = np.random.uniform(-2.0, 2.0, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.eigen_updated = False\n        self.B = np.eye(self.dim)\n        self.D = np.ones(self.dim)\n        self.diagC = np.ones(self.dim)\n        self.count_evals = 0\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n        self.mueff = self.popsize / 2  # Simplified mueff calculation\n        self.cc = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n\n    def sample_population(self):\n        z = np.random.randn(self.dim, self.popsize)\n        x = self.mu[:, np.newaxis] + self.sigma * (self.B @ (self.D[:, np.newaxis] * z))\n        x = np.clip(x, -5.0, 5.0)  # Clip values to stay within bounds\n        return x\n\n    def update_distribution(self, x, fvals):\n        idx = np.argsort(fvals)\n        x_sorted = x[:, idx]\n\n        x_mu = x_sorted[:, :self.popsize // 2].mean(axis=1)\n        y = (x_mu - self.mu) / self.sigma\n        self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * (self.B @ y)\n        self.pc = (1 - self.cc) * self.pc + np.sqrt(self.cc * (2 - self.cc) * self.mueff) * y\n        self.mu = x_mu\n\n        hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**2 * self.count_evals / self.popsize) < 1.4 + 2/(self.dim+1))\n\n        delta_h = (1 - hsig) * self.cc * (2 - self.cc)\n        self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :] + delta_h * self.C) + \\\n                   self.cmu * (x_sorted[:, :self.popsize // 2] - self.mu[:, np.newaxis]) @ (x_sorted[:, :self.popsize // 2] - self.mu[:, np.newaxis]).T\n\n        self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / np.sqrt(self.dim) - 1))\n\n        self.diagC = np.diag(self.C)\n        self.eigen_updated = False\n\n    def update_eigen(self):\n        try:\n            self.D, self.B = np.linalg.eigh(self.C)\n        except np.linalg.LinAlgError:\n            self.C = self.C + 1e-8 * np.eye(self.dim)\n            self.D, self.B = np.linalg.eigh(self.C)\n\n        self.D = np.sqrt(np.maximum(self.D, 1e-20))\n        self.eigen_updated = True\n        self.diagC = np.diag(self.C)\n\n    def __call__(self, func):\n        \n        for r in range(self.restarts):\n            self.initialize()\n            while self.count_evals < self.budget:\n                \n                if not self.eigen_updated or self.count_evals == 0:\n                    self.update_eigen()\n\n                x = self.sample_population()\n                fvals = np.array([func(x[:, i]) for i in range(self.popsize)])\n                self.count_evals += self.popsize\n\n                if np.min(fvals) < self.f_opt:\n                  self.f_opt = np.min(fvals)\n                  self.x_opt = x[:, np.argmin(fvals)]\n\n                self.update_distribution(x, fvals)\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm CMAES scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0]}}
{"id": "17d8e933-3a7e-4688-ad32-28488a3fa466", "fitness": 0.3565600247860091, "name": "GaussianAdaptation", "description": "Population-based algorithm employing a Gaussian distribution to sample new solutions around the best performing individuals, while also incorporating a shrinking search space over time to promote convergence.", "code": "import numpy as np\n\nclass GaussianAdaptation:\n    def __init__(self, budget=10000, dim=10, pop_size=20, lr=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lr = lr  # Learning rate for adapting the search space\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        # Shrinking search space parameters\n        current_lb = self.lb\n        current_ub = self.ub\n\n        while self.budget > 0:\n            # Find best individual\n            best_idx = np.argmin(fitness)\n            best_x = population[best_idx]\n            best_f = fitness[best_idx]\n\n            # Generate new solutions around the best individual using Gaussian distribution\n            new_population = np.random.normal(loc=best_x, scale=0.1 * (current_ub - current_lb), size=(self.pop_size, self.dim))\n\n            # Clip new solutions to the current search space\n            new_population = np.clip(new_population, current_lb, current_ub)\n\n            # Evaluate new solutions\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n\n            # Update best solution\n            best_new_idx = np.argmin(new_fitness)\n            if new_fitness[best_new_idx] < self.f_opt:\n                self.f_opt = new_fitness[best_new_idx]\n                self.x_opt = new_population[best_new_idx]\n            \n            # Update population (replace worst with best from new population)\n            worst_idx = np.argmax(fitness)\n            best_new_idx = np.argmin(new_fitness)  # Ensure we use the index from the new_fitness\n            if new_fitness[best_new_idx] < fitness[worst_idx]:\n                population[worst_idx] = new_population[best_new_idx]\n                fitness[worst_idx] = new_fitness[best_new_idx]\n            \n            # Shrink search space\n            current_lb = max(self.lb, best_x.min() + self.lr * (self.lb - best_x.min())) #Adapt lb\n            current_ub = min(self.ub, best_x.max() + self.lr * (self.ub - best_x.max()))  #Adapt ub\n\n            # Ensure lb < ub\n            if np.any(current_lb >= current_ub):\n                current_lb = np.minimum(current_lb, current_ub - 1e-6)\n                current_ub = np.maximum(current_ub, current_lb + 1e-6)\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm GaussianAdaptation scored 0.357 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.13142228688143043, 0.2912700136132218, 0.38448082289836816, 0.4015713212417472, 0.23255543094824216, 0.3524121550522382, 0.28252890467514125, 0.33600042658290064, 0.2816963450559645, 0.198657022939, 0.42831081636606094, 0.9953260704992831, 0.2677390179973964, 0.22526983229512199, 0.5968155298707281, 0.3282780646020361, 0.3165069838905713, 0.42663749026355036, 0.1623208993550448, 0.4914010606921353]}}
{"id": "d114e1b9-425d-48a4-9fbb-0e7e1a8b5141", "fitness": 0.7025786626305874, "name": "HybridSwarmDE", "description": "A population-based algorithm that combines aspects of particle swarm optimization and differential evolution with a restart mechanism based on stagnation detection.", "code": "import numpy as np\n\nclass HybridSwarmDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, inertia=0.7, c1=1.5, c2=1.5, F=0.8, CR=0.9, stagnation_threshold=500):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.inertia = inertia\n        self.c1 = c1\n        self.c2 = c2\n        self.F = F\n        self.CR = CR\n        self.stagnation_threshold = stagnation_threshold\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.zeros((self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.pbest_fitness = np.copy(self.fitness)\n        self.pbest_position = np.copy(self.population)\n        self.gbest_index = np.argmin(self.fitness)\n        self.gbest_fitness = self.fitness[self.gbest_index]\n        self.gbest_position = self.population[self.gbest_index]\n        self.eval_count = self.pop_size\n        self.stagnation_counter = 0\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Particle Swarm Optimization Update\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] = self.inertia * self.velocities[i] + \\\n                                     self.c1 * r1 * (self.pbest_position[i] - self.population[i]) + \\\n                                     self.c2 * r2 * (self.gbest_position - self.population[i])\n                new_position = self.population[i] + self.velocities[i]\n\n                # Differential Evolution Mutation and Crossover\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n                mutant = a + self.F * (b - c)\n                \n                # Crossover\n                cross_mask = np.random.rand(self.dim) < self.CR\n                trial_vector = np.where(cross_mask, mutant, new_position)\n                trial_vector = np.clip(trial_vector, func.bounds.lb, func.bounds.ub)\n\n                new_fitness = func(trial_vector)\n                self.eval_count += 1\n\n                if new_fitness < self.fitness[i]:\n                    self.fitness[i] = new_fitness\n                    self.population[i] = trial_vector\n\n                    if new_fitness < self.pbest_fitness[i]:\n                        self.pbest_fitness[i] = new_fitness\n                        self.pbest_position[i] = trial_vector\n\n                        if new_fitness < self.gbest_fitness:\n                            self.gbest_fitness = new_fitness\n                            self.gbest_position = trial_vector\n                            self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n            \n            # Stagnation Check and Restart\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.pbest_fitness = np.copy(self.fitness)\n                self.pbest_position = np.copy(self.population)\n                self.gbest_index = np.argmin(self.fitness)\n                self.gbest_fitness = self.fitness[self.gbest_index]\n                self.gbest_position = self.population[self.gbest_index]\n                self.eval_count += self.pop_size \n                self.stagnation_counter = 0\n\n            if self.gbest_fitness < self.f_opt:\n                self.f_opt = self.gbest_fitness\n                self.x_opt = self.gbest_position\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridSwarmDE scored 0.703 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.18521746425084973, 0.7365023811532944, 0.7236676796560881, 0.8919842638645511, 0.762139008387522, 0.8128100423601072, 0.6750193867293457, 0.6908389330663307, 0.7285948213481297, 0.6962009973982448, 0.8936628188094299, 0.9953494857250502, 0.6644760843105653, 0.7723967008267989, 0.9449823227563455, 0.8163742377626262, 0.5183971668009408, 0.8427040889801547, 0.2027105480179946, 0.49754482040737846]}}
{"id": "7dac2bed-fe77-47bc-aa4b-65793942ec1e", "fitness": 0.0, "name": "AdaptiveCoordinateDescent", "description": "Adaptive Coordinate Descent with dynamic step size adjustment based on function value changes.", "code": "import numpy as np\n\nclass AdaptiveCoordinateDescent:\n    def __init__(self, budget=10000, dim=10, initial_step_size=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_step_size = initial_step_size\n        self.step_sizes = np.full(dim, initial_step_size)\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        x = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.x_opt = x.copy()\n        self.f_opt = func(x)\n        self.eval_count += 1\n\n        while self.eval_count < self.budget:\n            for i in range(self.dim):\n                # Explore positive direction\n                x_plus = x.copy()\n                x_plus[i] += self.step_sizes[i]\n                x_plus = np.clip(x_plus, self.lb, self.ub)\n                f_plus = func(x_plus)\n                self.eval_count += 1\n\n                # Explore negative direction\n                x_minus = x.copy()\n                x_minus[i] -= self.step_sizes[i]\n                x_minus = np.clip(x_minus, self.lb, self.ub)\n                f_minus = func(x_minus)\n                self.eval_count += 1\n                \n                if f_plus < self.f_opt and f_plus < f_minus:\n                    x = x_plus\n                    self.f_opt = f_plus\n                    self.x_opt = x.copy()\n                    self.step_sizes[i] *= 1.1  # Increase step size if successful\n                elif f_minus < self.f_opt and f_minus < f_plus:\n                    x = x_minus\n                    self.f_opt = f_minus\n                    self.x_opt = x.copy()\n                    self.step_sizes[i] *= 1.1  # Increase step size if successful\n                else:\n                    self.step_sizes[i] *= 0.5  # Decrease step size if unsuccessful\n                \n                if self.eval_count >= self.budget:\n                    break\n                    \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveCoordinateDescent scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["793e646c-56c5-47f8-a505-da4bb9bb923d"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "31552b81-bf2f-4383-880e-65217df23cf3", "fitness": -Infinity, "name": "GradientEnhancedOBLDE", "description": "Combines a gradient-based local search with a population-based sampling strategy inspired by opposition-based learning and differential evolution to balance exploration and exploitation.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass GradientEnhancedOBLDE:\n    def __init__(self, budget=10000, dim=10, pop_size=15, F=0.7, CR=0.8, local_search_iterations=5, obl_rate=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.local_search_iterations = local_search_iterations\n        self.obl_rate = obl_rate\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count = self.pop_size\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Differential Evolution Mutation and Crossover\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n                mutant = a + self.F * (b - c)\n                cross_mask = np.random.rand(self.dim) < self.CR\n                trial_vector = np.where(cross_mask, mutant, self.population[i])\n                trial_vector = np.clip(trial_vector, func.bounds.lb, func.bounds.ub)\n\n                # Opposition-Based Learning\n                if np.random.rand() < self.obl_rate:\n                    opposite_vector = func.bounds.lb + func.bounds.ub - trial_vector\n                    opposite_vector = np.clip(opposite_vector, func.bounds.lb, func.bounds.ub)\n                    trial_vector = opposite_vector\n\n                new_fitness = func(trial_vector)\n                self.eval_count += 1\n\n                # Gradient-Based Local Search\n                if new_fitness < self.fitness[i]:\n                    x0 = trial_vector\n                    bounds = [(func.bounds.lb, func.bounds.ub) for _ in range(self.dim)]\n                    local_search_result = minimize(func, x0, method='L-BFGS-B', bounds=bounds, options={'maxiter': self.local_search_iterations})\n\n                    if local_search_result.success:\n                        local_fitness = local_search_result.fun\n                        local_solution = local_search_result.x\n                        \n                        if local_fitness < new_fitness:\n                             new_fitness = local_fitness\n                             trial_vector = local_solution\n                        self.eval_count += local_search_result.nit #Number of iterations.\n\n                if new_fitness < self.fitness[i]:\n                    self.fitness[i] = new_fitness\n                    self.population[i] = trial_vector\n\n                    if new_fitness < self.f_opt:\n                        self.f_opt = new_fitness\n                        self.x_opt = trial_vector\n            \n            # Optional: Population Diversity Maintenance (e.g., using crowding distance) - omitted for brevity\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "An exception occurred: name 'minimize' is not defined.", "error": "", "parent_ids": ["d114e1b9-425d-48a4-9fbb-0e7e1a8b5141"], "operator": null, "metadata": {}}
{"id": "2b6fe1a3-b2cc-4067-bfc6-f79f9ee7295f", "fitness": -Infinity, "name": "GaussianAdaptationMomentum", "description": "Gaussian adaptation with momentum for search space shrinking and adaptive standard deviation.", "code": "import numpy as np\n\nclass GaussianAdaptationMomentum:\n    def __init__(self, budget=10000, dim=10, pop_size=20, lr=0.1, momentum=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lr = lr  # Learning rate for adapting the search space\n        self.momentum = momentum\n        self.lb = -5.0\n        self.ub = 5.0\n        self.std = 0.5  # Initial standard deviation\n        self.std_lr = 0.1 # Learning rate for standard deviation\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        # Shrinking search space parameters\n        current_lb = self.lb\n        current_ub = self.ub\n        \n        # Initialize momentum for search space boundaries\n        lb_velocity = np.zeros(self.dim)\n        ub_velocity = np.zeros(self.dim)\n\n        while self.budget > 0:\n            # Find best individual\n            best_idx = np.argmin(fitness)\n            best_x = population[best_idx]\n            best_f = fitness[best_idx]\n\n            # Generate new solutions around the best individual using Gaussian distribution\n            new_population = np.random.normal(loc=best_x, scale=self.std, size=(self.pop_size, self.dim))\n\n            # Clip new solutions to the current search space\n            new_population = np.clip(new_population, current_lb, current_ub)\n\n            # Evaluate new solutions\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n\n            # Update best solution\n            best_new_idx = np.argmin(new_fitness)\n            if new_fitness[best_new_idx] < self.f_opt:\n                self.f_opt = new_fitness[best_new_idx]\n                self.x_opt = new_population[best_new_idx]\n            \n            # Update population (replace worst with best from new population)\n            worst_idx = np.argmax(fitness)\n            best_new_idx = np.argmin(new_fitness)  # Ensure we use the index from the new_fitness\n            if new_fitness[best_new_idx] < fitness[worst_idx]:\n                population[worst_idx] = new_population[best_new_idx]\n                fitness[worst_idx] = new_fitness[best_new_idx]\n            \n            # Shrink search space with momentum\n            new_lb = max(self.lb, best_x + self.lr * (self.lb - best_x))\n            new_ub = min(self.ub, best_x + self.lr * (self.ub - best_x))\n            \n            lb_velocity = self.momentum * lb_velocity + (1 - self.momentum) * (new_lb - current_lb)\n            ub_velocity = self.momentum * ub_velocity + (1 - self.momentum) * (new_ub - current_ub)\n            \n            current_lb += lb_velocity\n            current_ub += ub_velocity\n\n            current_lb = np.clip(current_lb, self.lb, self.ub)\n            current_ub = np.clip(current_ub, self.lb, self.ub)\n            \n            # Ensure lb < ub\n            current_lb = np.minimum(current_lb, current_ub - 1e-6)\n            current_ub = np.maximum(current_ub, current_lb + 1e-6)\n            \n            # Adapt the standard deviation\n            if best_f < self.f_opt:\n                self.std *= (1 - self.std_lr) # Reduce std if improvement\n            else:\n                self.std *= (1 + self.std_lr) # Increase std if no improvement\n            \n            self.std = np.clip(self.std, 1e-6, (self.ub - self.lb) / 2)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "An exception occurred: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all().", "error": "", "parent_ids": ["17d8e933-3a7e-4688-ad32-28488a3fa466"], "operator": null, "metadata": {}}
{"id": "ea1f2798-03a3-434e-afb4-e80c5aa686d4", "fitness": 0.0, "name": "AdaptiveHybridPSO_SA", "description": "Adaptive Hybrid Particle Swarm Optimization with Simulated Annealing, adjusting PSO parameters and incorporating SA for refined local search around the best solution.", "code": "import numpy as np\n\nclass AdaptiveHybridPSO_SA:\n    def __init__(self, budget=10000, dim=10, pop_size=20, inertia_max=0.9, inertia_min=0.4, c1_max=2.5, c1_min=0.5, c2_max=2.5, c2_min=0.5, temp_init=100, temp_decay=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.inertia_max = inertia_max\n        self.inertia_min = inertia_min\n        self.c1_max = c1_max\n        self.c1_min = c1_min\n        self.c2_max = c2_max\n        self.c2_min = c2_min\n        self.temp_init = temp_init\n        self.temp_decay = temp_decay\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.zeros((self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.pbest_fitness = np.copy(self.fitness)\n        self.pbest_position = np.copy(self.population)\n        self.gbest_index = np.argmin(self.fitness)\n        self.gbest_fitness = self.fitness[self.gbest_index]\n        self.gbest_position = self.population[self.gbest_index]\n        self.eval_count = self.pop_size\n        self.temp = self.temp_init\n\n        while self.eval_count < self.budget:\n            # Adaptive PSO parameters\n            inertia = self.inertia_max - (self.inertia_max - self.inertia_min) * (self.eval_count / self.budget)\n            c1 = self.c1_max - (self.c1_max - self.c1_min) * (self.eval_count / self.budget)\n            c2 = self.c2_max - (self.c2_max - self.c2_min) * (self.eval_count / self.budget)\n\n            for i in range(self.pop_size):\n                # Particle Swarm Optimization Update\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] = inertia * self.velocities[i] + \\\n                                     c1 * r1 * (self.pbest_position[i] - self.population[i]) + \\\n                                     c2 * r2 * (self.gbest_position - self.population[i])\n                new_position = self.population[i] + self.velocities[i]\n                new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n\n                new_fitness = func(new_position)\n                self.eval_count += 1\n\n                if new_fitness < self.fitness[i]:\n                    self.fitness[i] = new_fitness\n                    self.population[i] = new_position\n\n                    if new_fitness < self.pbest_fitness[i]:\n                        self.pbest_fitness[i] = new_fitness\n                        self.pbest_position[i] = new_position\n\n                        if new_fitness < self.gbest_fitness:\n                            self.gbest_fitness = new_fitness\n                            self.gbest_position = new_position\n\n            # Simulated Annealing around gbest\n            neighbor = self.gbest_position + np.random.normal(0, 0.1, self.dim)  # Small perturbation\n            neighbor = np.clip(neighbor, func.bounds.lb, func.bounds.ub)\n            neighbor_fitness = func(neighbor)\n            self.eval_count += 1\n\n            delta_e = neighbor_fitness - self.gbest_fitness\n\n            if delta_e < 0:\n                self.gbest_fitness = neighbor_fitness\n                self.gbest_position = neighbor\n            else:\n                probability = np.exp(-delta_e / self.temp)\n                if np.random.rand() < probability:\n                    self.gbest_fitness = neighbor_fitness\n                    self.gbest_position = neighbor\n\n            self.temp *= self.temp_decay  #Cooling\n\n            if self.gbest_fitness < self.f_opt:\n                self.f_opt = self.gbest_fitness\n                self.x_opt = self.gbest_position\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveHybridPSO_SA scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d114e1b9-425d-48a4-9fbb-0e7e1a8b5141"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "866aae83-1638-4eb6-828b-87282db2c1e7", "fitness": 0.0, "name": "CMAES", "description": "Simplified CMA-ES with fewer parameters and eigenvalue updates only when necessary for better efficiency.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, sigma0=0.5, restarts=3):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.sigma = sigma0\n        self.restarts = restarts\n\n        self.mu = None\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.B = None\n        self.D = None\n        self.count_evals = 0\n        self.eigen_update_interval = 10 # Eigenvalue update interval\n        self.last_eigen_update = 0\n\n    def initialize(self):\n        self.mu = np.random.uniform(-2.0, 2.0, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.B = np.eye(self.dim)\n        self.D = np.ones(self.dim)\n        self.count_evals = 0\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n        self.mueff = self.popsize / 2\n        self.cc = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + 0.3 #Simplified dampening\n\n    def sample_population(self):\n        z = np.random.randn(self.dim, self.popsize)\n        x = self.mu[:, np.newaxis] + self.sigma * (self.B @ (self.D[:, np.newaxis] * z))\n        x = np.clip(x, -5.0, 5.0)\n        return x\n\n    def update_distribution(self, x, fvals):\n        idx = np.argsort(fvals)\n        x_sorted = x[:, idx]\n        x_mu = x_sorted[:, :self.popsize // 2].mean(axis=1)\n        y = (x_mu - self.mu) / self.sigma\n        \n        self.ps = (1 - self.cc) * self.ps + np.sqrt(self.cc * (2 - self.cc) * self.mueff) * y\n        hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cc)**2 * self.count_evals / self.popsize) < 1.4 + 2/(self.dim+1))\n        delta_h = (1 - hsig) * self.cc * (2 - self.cc)        \n        self.pc = (1 - self.cc) * self.pc + np.sqrt(self.cc * (2 - self.cc) * self.mueff) * y\n        self.mu = x_mu\n\n        self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :] + delta_h * self.C) + \\\n                   self.cmu * (x_sorted[:, :self.popsize // 2] - self.mu[:, np.newaxis]) @ (x_sorted[:, :self.popsize // 2] - self.mu[:, np.newaxis]).T / self.popsize\n        \n        self.sigma *= np.exp(0.5 * (np.linalg.norm(self.ps)**2 / self.dim - 1) * self.cc / self.damps)\n\n    def update_eigen(self):\n        try:\n            self.D, self.B = np.linalg.eigh(self.C)\n        except np.linalg.LinAlgError:\n            self.C = self.C + 1e-8 * np.eye(self.dim)\n            self.D, self.B = np.linalg.eigh(self.C)\n        self.D = np.sqrt(np.maximum(self.D, 1e-20))\n\n    def __call__(self, func):\n        for r in range(self.restarts):\n            self.initialize()\n            while self.count_evals < self.budget:\n                if self.count_evals - self.last_eigen_update >= self.eigen_update_interval or self.count_evals == 0:\n                    self.update_eigen()\n                    self.last_eigen_update = self.count_evals\n\n                x = self.sample_population()\n                fvals = np.array([func(x[:, i]) for i in range(self.popsize)])\n                self.count_evals += self.popsize\n\n                if np.min(fvals) < self.f_opt:\n                    self.f_opt = np.min(fvals)\n                    self.x_opt = x[:, np.argmin(fvals)]\n\n                self.update_distribution(x, fvals)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm CMAES scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["89524e9b-d516-4ec9-abbc-5a98ae425a7c"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "8662b29c-c614-49d5-b1a4-c0751ea72f61", "fitness": 0.0, "name": "CMAES", "description": "CMA-ES with improved parameter adaptation, stagnation detection, and dynamic population size adjustment.", "code": "import numpy as np\nfrom scipy.linalg import sqrtm\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, sigma0=0.5, cs=0.3, damps=1.0, ccov1=0.0, ccovmu=0.0, restarts=3, stagnation_threshold=100):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.sigma = sigma0\n        self.cs = cs\n        self.damps = damps\n        self.ccov1 = ccov1\n        self.ccovmu = ccovmu\n        self.restarts = restarts\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n\n        self.mu = None\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.eigen_updated = False\n        self.B = None\n        self.D = None\n        self.diagC = None\n        self.count_evals = 0\n        self.f_opt_history = []\n\n\n    def initialize(self):\n        self.mu = np.random.uniform(-2.0, 2.0, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.eigen_updated = False\n        self.B = np.eye(self.dim)\n        self.D = np.ones(self.dim)\n        self.diagC = np.ones(self.dim)\n        self.count_evals = 0\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.f_opt_history = []\n        self.stagnation_counter = 0\n\n        self.mueff = self.popsize / 2  # Simplified mueff calculation\n        self.cc = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n\n    def sample_population(self):\n        z = np.random.randn(self.dim, self.popsize)\n        x = self.mu[:, np.newaxis] + self.sigma * (self.B @ (self.D[:, np.newaxis] * z))\n        x = np.clip(x, -5.0, 5.0)  # Clip values to stay within bounds\n        return x\n\n    def update_distribution(self, x, fvals):\n        idx = np.argsort(fvals)\n        x_sorted = x[:, idx]\n\n        x_mu = x_sorted[:, :self.popsize // 2].mean(axis=1)\n        y = (x_mu - self.mu) / self.sigma\n        self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * (self.B @ y)\n        self.pc = (1 - self.cc) * self.pc + np.sqrt(self.cc * (2 - self.cc) * self.mueff) * y\n        self.mu = x_mu\n\n        hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**2 * self.count_evals / self.popsize) < 1.4 + 2/(self.dim+1))\n\n        delta_h = (1 - hsig) * self.cc * (2 - self.cc)\n        self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :] + delta_h * self.C) + \\\n                   self.cmu * (x_sorted[:, :self.popsize // 2] - self.mu[:, np.newaxis]) @ (x_sorted[:, :self.popsize // 2] - self.mu[:, np.newaxis]).T\n\n        self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / np.sqrt(self.dim) - 1))\n\n        self.diagC = np.diag(self.C)\n        self.eigen_updated = False\n        \n        # Dynamic Pop Size Adjustment\n        if len(self.f_opt_history) > 10 and np.std(self.f_opt_history[-10:]) < 1e-6:\n            self.popsize = min(self.popsize * 2, 100)  # Increase popsize if stagnating\n            self.mueff = self.popsize / 2\n        elif len(self.f_opt_history) > 10 and np.std(self.f_opt_history[-10:]) > 1e-3 and self.popsize > 4 + int(3 * np.log(self.dim)):\n            self.popsize = max(4 + int(3 * np.log(self.dim)), self.popsize // 2)\n            self.mueff = self.popsize / 2\n\n\n    def update_eigen(self):\n        try:\n            self.D, self.B = np.linalg.eigh(self.C)\n        except np.linalg.LinAlgError:\n            self.C = self.C + 1e-8 * np.eye(self.dim)\n            self.D, self.B = np.linalg.eigh(self.C)\n\n        self.D = np.sqrt(np.maximum(self.D, 1e-20))\n        self.eigen_updated = True\n        self.diagC = np.diag(self.C)\n\n    def __call__(self, func):\n        \n        for r in range(self.restarts):\n            self.initialize()\n            while self.count_evals < self.budget:\n                \n                if not self.eigen_updated or self.count_evals == 0:\n                    self.update_eigen()\n\n                x = self.sample_population()\n                fvals = np.array([func(x[:, i]) for i in range(self.popsize)])\n                self.count_evals += self.popsize\n\n                f_min = np.min(fvals)\n                if f_min < self.f_opt:\n                    self.f_opt = f_min\n                    self.x_opt = x[:, np.argmin(fvals)]\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += self.popsize\n                    if self.stagnation_counter > self.stagnation_threshold:\n                        self.sigma *= 0.9  # Reduce step size upon stagnation\n                        self.stagnation_counter = 0\n\n\n                self.f_opt_history.append(self.f_opt)\n                self.update_distribution(x, fvals)\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm CMAES scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["89524e9b-d516-4ec9-abbc-5a98ae425a7c"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "50a0c36a-2eb4-4383-bbca-3fc1f8119418", "fitness": 0.24423770902673017, "name": "AdaptiveGaussian", "description": "Adaptive Gaussian sampling around the best, combined with a dynamic population size and a more robust search space adaptation using quantiles.", "code": "import numpy as np\n\nclass AdaptiveGaussian:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=20, lr=0.1, quantile=0.25):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = initial_pop_size\n        self.lr = lr  # Learning rate for adapting the search space\n        self.quantile = quantile # Quantile for search space adaptation\n        self.lb = -5.0\n        self.ub = 5.0\n        self.pop_size = initial_pop_size # Dynamic pop size\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        # Shrinking search space parameters\n        current_lb = np.full(self.dim, self.lb)\n        current_ub = np.full(self.dim, self.ub)\n\n        while self.budget > self.initial_pop_size: # Ensure enough budget for at least an initial pop size\n            # Find best individual\n            best_idx = np.argmin(fitness)\n            best_x = population[best_idx]\n            best_f = fitness[best_idx]\n\n            # Generate new solutions around the best individual using Gaussian distribution\n            std = 0.1 * (current_ub - current_lb)\n            new_population = np.random.normal(loc=best_x, scale=std, size=(self.pop_size, self.dim))\n\n            # Clip new solutions to the current search space\n            new_population = np.clip(new_population, current_lb, current_ub)\n\n            # Evaluate new solutions\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n\n            # Update best solution\n            best_new_idx = np.argmin(new_fitness)\n            if new_fitness[best_new_idx] < self.f_opt:\n                self.f_opt = new_fitness[best_new_idx]\n                self.x_opt = new_population[best_new_idx]\n            \n            # Update population (replace worst with best from new population)\n            worst_idx = np.argmax(fitness)\n            if new_fitness[best_new_idx] < fitness[worst_idx]:\n                population[worst_idx] = new_population[best_new_idx]\n                fitness[worst_idx] = new_fitness[best_new_idx]\n            \n            # Shrink search space using quantiles\n            for i in range(self.dim):\n                current_lb[i] = max(self.lb, np.quantile(population[:, i], self.quantile))\n                current_ub[i] = min(self.ub, np.quantile(population[:, i], 1 - self.quantile))\n\n            # Ensure lb < ub\n            current_lb = np.minimum(current_lb, current_ub - 1e-6)\n            current_ub = np.maximum(current_ub, current_lb + 1e-6)\n\n            # Adapt population size\n            self.pop_size = min(self.initial_pop_size + self.budget // 100 , self.budget)\n            if self.pop_size != population.shape[0]:\n                population = np.random.uniform(current_lb, current_ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                best_idx = np.argmin(fitness)\n                if fitness[best_idx] < self.f_opt:\n                    self.f_opt = fitness[best_idx]\n                    self.x_opt = population[best_idx]\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveGaussian scored 0.244 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["17d8e933-3a7e-4688-ad32-28488a3fa466"], "operator": null, "metadata": {"aucs": [0.11229971659853022, 0.16391470760046922, 0.2878809839079599, 0.15604326669989355, 0.17150701903461663, 0.18497089994751514, 0.19674787256007575, 0.20280031210781302, 0.20598263274246686, 0.15433243904789906, 0.173917121254194, 0.9979012968292291, 0.24577915104453496, 0.1626564557531608, 0.21706300517721722, 0.2660315413781271, 0.22758592640300135, 0.18291971807089036, 0.14981346318953126, 0.424606651187478]}}
{"id": "6a048c90-9466-4f63-a43a-1c78e7eea32b", "fitness": 0.41594507701768657, "name": "AdaptiveGaussian", "description": "Adaptive Gaussian mutation with dynamic population size and covariance matrix adaptation for improved exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveGaussian:\n    def __init__(self, budget=10000, dim=10, pop_size_init=20, lr=0.1, sigma_init=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_init = pop_size_init\n        self.lr = lr  # Learning rate for adapting the search space\n        self.sigma_init = sigma_init\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        # Initialize population\n        pop_size = self.pop_size_init\n        population = np.random.uniform(self.lb, self.ub, size=(pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        # Shrinking search space parameters\n        current_lb = self.lb\n        current_ub = self.ub\n        sigma = self.sigma_init * (self.ub - self.lb) # Initial standard deviation\n\n        # Covariance matrix adaptation\n        C = np.eye(self.dim)  # Initialize covariance matrix\n        learning_rate_C = 0.1\n\n        while self.budget > 0:\n            # Find best individual\n            best_idx = np.argmin(fitness)\n            best_x = population[best_idx]\n            best_f = fitness[best_idx]\n\n            # Adaptive population size\n            pop_size = min(self.pop_size_init + int(self.budget / 1000), self.budget)\n            if pop_size <= 0:\n               pop_size = 1 # ensure pop size is at least 1\n\n            # Generate new solutions around the best individual using Gaussian distribution with covariance matrix\n            new_population = np.random.multivariate_normal(mean=best_x, cov=sigma**2 * C, size=pop_size)\n\n            # Clip new solutions to the current search space\n            new_population = np.clip(new_population, current_lb, current_ub)\n\n            # Evaluate new solutions\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= pop_size\n\n            # Update best solution\n            best_new_idx = np.argmin(new_fitness)\n            if new_fitness[best_new_idx] < self.f_opt:\n                self.f_opt = new_fitness[best_new_idx]\n                self.x_opt = new_population[best_new_idx]\n            \n            # Update population (replace worst with best from new population)\n            worst_idx = np.argmax(fitness)\n            best_new_idx = np.argmin(new_fitness)  # Ensure we use the index from the new_fitness\n            if new_fitness[best_new_idx] < fitness[worst_idx]:\n                population[worst_idx] = new_population[best_new_idx]\n                fitness[worst_idx] = new_fitness[best_new_idx]\n            \n            # Shrink search space\n            current_lb = max(self.lb, best_x.min() + self.lr * (self.lb - best_x.min())) #Adapt lb\n            current_ub = min(self.ub, best_x.max() + self.lr * (self.ub - best_x.max()))  #Adapt ub\n\n            # Ensure lb < ub\n            if np.any(current_lb >= current_ub):\n                current_lb = np.minimum(current_lb, current_ub - 1e-6)\n                current_ub = np.maximum(current_ub, current_lb + 1e-6)\n            \n            # Adapt step size (sigma)\n            if self.f_opt < best_f:\n              sigma *= 0.95\n            else:\n              sigma *= 1.05\n\n            # Update covariance matrix\n            diff = population[best_idx] - best_x\n            C = (1 - learning_rate_C) * C + learning_rate_C * np.outer(diff, diff)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveGaussian scored 0.416 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["17d8e933-3a7e-4688-ad32-28488a3fa466"], "operator": null, "metadata": {"aucs": [0.16633496960321736, 0.21642709761979695, 0.426627191015307, 0.18204597159452074, 0.27072671070708665, 0.5449113235655867, 0.33189581703360294, 0.4187180656269034, 0.3936120365405288, 0.1823142131975013, 0.7654193629507937, 0.9982418900476709, 0.25503253123919767, 0.2833797593218137, 0.8163659387932952, 0.4869497206988449, 0.3558527240534811, 0.519075741632409, 0.2009449559301958, 0.5040255191819769]}}
{"id": "d0d2c086-4c96-4e38-906e-48c9dda93c2a", "fitness": 0.5926534100787256, "name": "CMAES", "description": "Simplified CMA-ES with adaptive covariance updating and a single restart for efficiency.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, sigma0=0.5, cs=0.3, cc=None, c1=None, cmu=None):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.sigma = sigma0\n        self.cs = cs\n\n        self.mu = np.random.uniform(-2.0, 2.0, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.B = np.eye(self.dim)\n        self.D = np.ones(self.dim)\n        self.eigen_updated = False\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.count_evals = 0\n\n        self.mueff = self.popsize / 2\n        self.cc = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim) if cc is None else cc\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff) if c1 is None else c1\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2)**2 + self.mueff)) if cmu is None else cmu\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n\n    def sample_population(self):\n        z = np.random.randn(self.dim, self.popsize)\n        x = self.mu[:, np.newaxis] + self.sigma * (self.B @ (self.D[:, np.newaxis] * z))\n        x = np.clip(x, -5.0, 5.0)\n        return x\n\n    def update_distribution(self, x, fvals):\n        idx = np.argsort(fvals)\n        x_sorted = x[:, idx]\n        x_mu = x_sorted[:, :self.popsize // 2].mean(axis=1)\n        y = (x_mu - self.mu) / self.sigma\n\n        self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * (self.B @ y)\n        hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**2 * self.count_evals / self.budget) < 1.4 + 2 / (self.dim + 1))\n        delta_h = (1 - hsig) * self.cc * (2 - self.cc)\n        self.pc = (1 - self.cc) * self.pc + np.sqrt(self.cc * (2 - self.cc) * self.mueff) * y\n\n        self.mu = x_mu\n\n        self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :] + delta_h * self.C) + \\\n                   self.cmu * (x_sorted[:, :self.popsize // 2] - self.mu[:, np.newaxis]) @ (x_sorted[:, :self.popsize // 2] - self.mu[:, np.newaxis]).T / self.popsize\n\n        self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / np.sqrt(self.dim) - 1))\n\n    def update_eigen(self):\n        try:\n            self.D, self.B = np.linalg.eigh(self.C)\n        except np.linalg.LinAlgError:\n            self.C = self.C + 1e-8 * np.eye(self.dim)\n            self.D, self.B = np.linalg.eigh(self.C)\n        self.D = np.sqrt(np.maximum(self.D, 1e-20))\n        self.eigen_updated = True\n\n    def __call__(self, func):\n        while self.count_evals < self.budget:\n            if not self.eigen_updated:\n                self.update_eigen()\n\n            x = self.sample_population()\n            fvals = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.count_evals += self.popsize\n\n            if np.min(fvals) < self.f_opt:\n                self.f_opt = np.min(fvals)\n                self.x_opt = x[:, np.argmin(fvals)]\n\n            self.update_distribution(x, fvals)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm CMAES scored 0.593 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["89524e9b-d516-4ec9-abbc-5a98ae425a7c"], "operator": null, "metadata": {"aucs": [0.26069412998211494, 0.17279066535670895, 0.8545719265817747, 0.17381907291110132, 0.9028530943741289, 0.9338525286082087, 0.3085023952287975, 0.9015817192811734, 0.910672052287411, 0.19055041233376901, 0.9034876049485616, 0.9926344743674422, 0.2596739809730525, 0.9222718731086624, 0.8109767899739004, 0.3309754836428963, 0.4062804880147506, 0.9448927105792008, 0.17287739932196455, 0.49910939969889223]}}
{"id": "3a62fcfb-c4dc-4576-b61b-79889549b188", "fitness": 0.1379339162644559, "name": "CMAES", "description": "Enhanced CMA-ES with robust covariance matrix adaptation, bounds handling and budget-aware parameter updates.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damps=1, c_mu=0.3, c_1=0.3, lb=-5.0, ub=5.0):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.m = None\n        self.sigma = 0.5\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.chiN = None\n        self.cs = cs\n        self.damps = damps\n        self.c_mu = c_mu\n        self.c_1 = c_1\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.lb = lb\n        self.ub = ub\n        self.rng = np.random.default_rng()  # Use a random number generator\n\n    def sample(self):\n        z = self.rng.normal(size=(self.dim, self.popsize))\n        x = self.m[:, None] + self.sigma * (self.A @ z)\n        return x\n\n    def check_bounds(self, x):\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        self.m = self.rng.uniform(self.lb, self.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.A = np.eye(self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1/(4*self.dim) + 1/(21*self.dim**2))\n        \n        while self.eval_count < self.budget:\n            x = self.sample()\n            x = self.check_bounds(x)\n            \n            f = np.array([func(x[:,i]) if self.eval_count < self.budget else np.inf for i in range(self.popsize)])\n            self.eval_count += self.popsize\n            \n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)].copy()\n\n            idx = np.argsort(f)\n            x_mu = x[:, idx[:self.mu]]\n\n            m_old = self.m.copy()\n            self.m = np.sum(x_mu * self.weights[None, :], axis=1)\n\n            z_mu = np.linalg.solve(self.A, x_mu - m_old[:, None])\n            \n            self.ps = (1 - self.cs) * self.ps + (self.cs * (2 - self.cs))**0.5 * (self.m - m_old) / self.sigma\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.eval_count / self.popsize)) / self.chiN < 1.4 + 2/(self.dim+1))\n            self.pc = (1 - self.damps) * self.pc + hsig * (self.damps * (2 - self.damps))**0.5 * (self.m - m_old) / self.sigma\n\n            self.C = (1 - self.c_1 - self.c_mu) * self.C + self.c_1 * (self.pc[:, None] @ self.pc[None, :]) + self.c_mu * (z_mu * self.weights[None, :]) @ z_mu.T\n            \n            self.sigma *= np.exp((np.linalg.norm(self.ps) / self.chiN - 1) * self.cs / self.damps)\n\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            self.C = np.diag(np.diag(self.C)) + 0.9 * (self.C - np.diag(np.diag(self.C)))\n            try:\n                self.A = np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-10 * np.eye(self.dim)  # Add a small constant to the diagonal\n                self.A = np.linalg.cholesky(self.C)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm CMAES scored 0.138 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["793e646c-56c5-47f8-a505-da4bb9bb923d"], "operator": null, "metadata": {"aucs": [0.07809109716111762, 0.05413183874610494, 0.27269780203221805, 0.1631384504247606, 0.20306985234972275, 0.1496592265640696, 0.048010827372133136, 0.09892013697069835, 0.14598744299397726, 0.15773357572308433, 0.13311270209949622, 0.12222131470755793, 0.2053917054639477, 0.1653017660649746, 0.1095792644597785, 0.15677081040883856, 0.06722195068307102, 0.16738437161875086, 0.10408142984086166, 0.1561727596039545]}}
{"id": "61f0db2b-1df7-42f4-892e-cd2996cd77c2", "fitness": 0.34904745434473194, "name": "AdaptiveGaussianAdaptation", "description": "Adaptive Gaussian adaptation with momentum for search space shrinkage and dynamic step size.", "code": "import numpy as np\n\nclass AdaptiveGaussianAdaptation:\n    def __init__(self, budget=10000, dim=10, pop_size=20, lr=0.1, initial_scale=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lr = lr  # Learning rate for adapting the search space\n        self.lb = -5.0\n        self.ub = 5.0\n        self.scale = initial_scale # Initial scale for Gaussian distribution\n        self.scale_decay = 0.995 # Decay factor for the scale\n        self.min_scale = 0.01 #Minimum scale value\n        self.momentum_lb = 0.0\n        self.momentum_ub = 0.0\n        self.momentum_alpha = 0.9\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        # Shrinking search space parameters\n        current_lb = self.lb\n        current_ub = self.ub\n\n        while self.budget > 0:\n            # Find best individual\n            best_idx = np.argmin(fitness)\n            best_x = population[best_idx]\n            best_f = fitness[best_idx]\n\n            # Generate new solutions around the best individual using Gaussian distribution\n            new_population = np.random.normal(loc=best_x, scale=self.scale * (current_ub - current_lb), size=(self.pop_size, self.dim))\n\n            # Clip new solutions to the current search space\n            new_population = np.clip(new_population, current_lb, current_ub)\n\n            # Evaluate new solutions\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n\n            # Update best solution\n            best_new_idx = np.argmin(new_fitness)\n            if new_fitness[best_new_idx] < self.f_opt:\n                self.f_opt = new_fitness[best_new_idx]\n                self.x_opt = new_population[best_new_idx]\n            \n            # Update population (replace worst with best from new population)\n            worst_idx = np.argmax(fitness)\n            best_new_idx = np.argmin(new_fitness)  # Ensure we use the index from the new_fitness\n            if new_fitness[best_new_idx] < fitness[worst_idx]:\n                population[worst_idx] = new_population[best_new_idx]\n                fitness[worst_idx] = new_fitness[best_new_idx]\n            \n            # Shrink search space with momentum\n            lb_change = self.lr * (self.lb - best_x.min())\n            ub_change = self.lr * (self.ub - best_x.max())\n\n            self.momentum_lb = self.momentum_alpha * self.momentum_lb + (1 - self.momentum_alpha) * lb_change\n            self.momentum_ub = self.momentum_alpha * self.momentum_ub + (1 - self.momentum_alpha) * ub_change\n\n            current_lb = max(self.lb, best_x.min() + self.momentum_lb) #Adapt lb\n            current_ub = min(self.ub, best_x.max() + self.momentum_ub)  #Adapt ub\n            \n            # Ensure lb < ub\n            if np.any(current_lb >= current_ub):\n                current_lb = np.minimum(current_lb, current_ub - 1e-6)\n                current_ub = np.maximum(current_ub, current_lb + 1e-6)\n            \n            #Decay the scale\n            self.scale = max(self.scale * self.scale_decay, self.min_scale)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveGaussianAdaptation scored 0.349 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["17d8e933-3a7e-4688-ad32-28488a3fa466"], "operator": null, "metadata": {"aucs": [0.13201195288822676, 0.2668468248236855, 0.36695070759491444, 0.3645888075970819, 0.2052918652070571, 0.3416286548676183, 0.259931069211474, 0.29749656267512015, 0.2582331869847757, 0.17523957516347433, 0.38283998412495523, 0.9981554417102986, 0.27693725970078054, 0.264105374611991, 0.6755400464540806, 0.352037842335981, 0.29444436917324557, 0.3726822258842287, 0.21312466925571816, 0.4828626666299317]}}
{"id": "434d72f5-d481-420f-a3cc-17a7c96d8894", "fitness": 0.3537541092666296, "name": "CMAES", "description": "Fixes broadcasting issues and adds a stagnation restart mechanism to CMA-ES for more robust optimization.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damps=1, c_mu=0.3, c_1=0.3, stagnation_threshold=50):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.m = None\n        self.sigma = 0.5\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.chiN = None\n        self.cs = cs\n        self.damps = damps\n        self.c_mu = c_mu\n        self.c_1 = c_1\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.lb = -5.0\n        self.ub = 5.0\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_f_history = []\n\n    def sample(self):\n        z = np.random.randn(self.dim, self.popsize)\n        x = self.m[:, None] + self.sigma * np.dot(self.A, z)\n        return x\n\n    def check_bounds(self, x):\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        self.m = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.A = np.eye(self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1/(4*self.dim) + 1/(21*self.dim**2))\n        self.eval_count = 0\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.stagnation_counter = 0\n        self.best_f_history = []\n        \n        while self.eval_count < self.budget:\n            x = self.sample()\n            x = self.check_bounds(x)\n            \n            f = np.array([func(x[:,i]) if self.eval_count < self.budget else np.inf for i in range(self.popsize)])\n            self.eval_count += self.popsize\n            \n            if np.min(f) < self.f_opt:\n                    self.f_opt = np.min(f)\n                    self.x_opt = x[:, np.argmin(f)].copy()\n                    self.stagnation_counter = 0\n            else:\n                self.stagnation_counter += 1\n\n            self.best_f_history.append(self.f_opt)\n            if len(self.best_f_history) > self.stagnation_threshold:\n                self.best_f_history.pop(0)\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                # Restart mechanism\n                self.m = np.random.uniform(self.lb, self.ub, size=self.dim)\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.A = np.eye(self.dim)\n                self.sigma = 0.5\n                self.stagnation_counter = 0\n\n\n            idx = np.argsort(f)\n            x_mu = x[:, idx[:self.mu]]\n\n            m_old = self.m.copy()\n            self.m = np.sum(x_mu * self.weights[None, :], axis=1)\n\n            z_mu = np.linalg.solve(self.A, (x_mu - m_old[:, None]))\n            \n            self.ps = (1 - self.cs) * self.ps + (self.cs * (2 - self.cs))**0.5 * (self.m - m_old) / self.sigma\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.eval_count / self.popsize)) / self.chiN < 1.4 + 2/(self.dim+1))\n            self.pc = (1 - self.damps) * self.pc + hsig * (self.damps * (2 - self.damps))**0.5 * (self.m - m_old) / self.sigma\n\n            self.C = (1 - self.c_1 - self.c_mu) * self.C + self.c_1 * (self.pc[:, None] @ self.pc[None, :]) + self.c_mu * (z_mu * self.weights[None, :]) @ z_mu.T\n            \n            self.sigma *= np.exp((np.linalg.norm(self.ps) / self.chiN - 1) * self.cs / self.damps)\n\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            self.C = np.diag(np.diag(self.C)) + 0.9 * (self.C - np.diag(np.diag(self.C)))\n            try:\n                self.A = np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-10 * np.eye(self.dim)  # Add a small constant to the diagonal\n                self.A = np.linalg.cholesky(self.C)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm CMAES scored 0.354 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["793e646c-56c5-47f8-a505-da4bb9bb923d"], "operator": null, "metadata": {"aucs": [0.10154794423648983, 0.35074287216741085, 0.3291892247088378, 0.3432828034698353, 0.27348073572614906, 0.32894740140229195, 0.2855306970956928, 0.3605417291964208, 0.2876048579124687, 0.1787300923009325, 0.32207337070249564, 0.923740339830245, 0.24533086899803358, 0.281786440936761, 0.6272937135983689, 0.3390440928734173, 0.33033229563475286, 0.49817308075001754, 0.18904272292052116, 0.47866690087145025]}}
{"id": "bddba196-4e3b-455b-a793-082e05bb4096", "fitness": -Infinity, "name": "MirroredCMAES", "description": "Covariance matrix adaptation evolution strategy with a mirrored sampling scheme to enhance exploration and an adaptive population size based on the success rate.", "code": "import numpy as np\n\nclass MirroredCMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, sigma0=0.5, cs=0.3, cc=None, c1=None, cmu=None):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.sigma = sigma0\n        self.cs = cs\n\n        self.mu = np.random.uniform(-2.0, 2.0, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.B = np.eye(self.dim)\n        self.D = np.ones(self.dim)\n        self.eigen_updated = False\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.count_evals = 0\n\n        self.mueff = self.popsize / 2\n        self.cc = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim) if cc is None else cc\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff) if c1 is None else c1\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2)**2 + self.mueff)) if cmu is None else cmu\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.success_rate = 0.5\n        self.adaptation_rate = 0.25\n\n    def sample_population(self):\n        z = np.random.randn(self.dim, self.popsize // 2)\n        x = self.mu[:, np.newaxis] + self.sigma * (self.B @ (self.D[:, np.newaxis] * z))\n        x_mirrored = self.mu[:, np.newaxis] - self.sigma * (self.B @ (self.D[:, np.newaxis] * z))  # Mirrored samples\n        x = np.concatenate((x, x_mirrored), axis=1)\n        x = np.clip(x, -5.0, 5.0)\n        return x\n\n    def update_distribution(self, x, fvals):\n        idx = np.argsort(fvals)\n        x_sorted = x[:, idx]\n        x_mu = x_sorted[:, :self.popsize // 2].mean(axis=1)\n        y = (x_mu - self.mu) / self.sigma\n\n        self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * (self.B @ y)\n        hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**2 * self.count_evals / self.budget) < 1.4 + 2 / (self.dim + 1))\n        delta_h = (1 - hsig) * self.cc * (2 - self.cc)\n        self.pc = (1 - self.cc) * self.pc + np.sqrt(self.cc * (2 - self.cc) * self.mueff) * y\n\n        self.mu = x_mu\n\n        self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :] + delta_h * self.C) + \\\n                   self.cmu * (x_sorted[:, :self.popsize // 2] - self.mu[:, np.newaxis]) @ (x_sorted[:, :self.popsize // 2] - self.mu[:, np.newaxis]).T / self.popsize\n\n        self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / np.sqrt(self.dim) - 1))\n\n        # Adapt population size based on success rate\n        best_idx = np.argmin(fvals)\n        if fvals[best_idx] < self.f_opt:\n            self.success_rate = (1 - self.adaptation_rate) * self.success_rate + self.adaptation_rate * 1\n        else:\n            self.success_rate = (1 - self.adaptation_rate) * self.success_rate + self.adaptation_rate * 0\n\n        if self.success_rate > 0.6:\n            self.popsize = min(self.popsize + 1, 2 * self.dim)\n            self.mueff = self.popsize / 2\n        elif self.success_rate < 0.4:\n            self.popsize = max(self.popsize - 1, 2)\n            self.mueff = self.popsize / 2\n\n    def update_eigen(self):\n        try:\n            self.D, self.B = np.linalg.eigh(self.C)\n        except np.linalg.LinAlgError:\n            self.C = self.C + 1e-8 * np.eye(self.dim)\n            self.D, self.B = np.linalg.eigh(self.C)\n        self.D = np.sqrt(np.maximum(self.D, 1e-20))\n        self.eigen_updated = True\n\n    def __call__(self, func):\n        while self.count_evals < self.budget:\n            if not self.eigen_updated:\n                self.update_eigen()\n\n            x = self.sample_population()\n            fvals = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.count_evals += self.popsize\n\n            if np.min(fvals) < self.f_opt:\n                self.f_opt = np.min(fvals)\n                self.x_opt = x[:, np.argmin(fvals)]\n\n            self.update_distribution(x, fvals)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "An exception occurred: index 4 is out of bounds for axis 1 with size 4.", "error": "", "parent_ids": ["d0d2c086-4c96-4e38-906e-48c9dda93c2a"], "operator": null, "metadata": {}}
{"id": "5de4ba37-c9db-41f1-a248-e17899cdfab5", "fitness": -Infinity, "name": "OrthogonalCMAES", "description": "Covariance matrix adaptation evolution strategy with orthogonal sampling and a self-adaptive population size to balance exploration and exploitation.", "code": "import numpy as np\n\nclass OrthogonalCMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damps=1, c_mu=0.3, c_1=0.3, stagnation_threshold=50, initial_sigma=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.initial_popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.popsize = self.initial_popsize\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.m = None\n        self.sigma = initial_sigma\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.chiN = None\n        self.cs = cs\n        self.damps = damps\n        self.c_mu = c_mu\n        self.c_1 = c_1\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.lb = -5.0\n        self.ub = 5.0\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_f_history = []\n        self.rng = np.random.default_rng() #Use numpy random generator\n\n    def sample(self):\n        # Orthogonal sampling\n        z = self.rng.normal(0, 1, size=(self.dim, self.popsize))\n        Q, _ = np.linalg.qr(z)\n        x = self.m[:, None] + self.sigma * np.dot(self.A, Q)\n        return x\n\n    def check_bounds(self, x):\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        self.m = self.rng.uniform(self.lb, self.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.A = np.eye(self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1/(4*self.dim) + 1/(21*self.dim**2))\n        self.eval_count = 0\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.stagnation_counter = 0\n        self.best_f_history = []\n        self.popsize = self.initial_popsize\n\n        while self.eval_count < self.budget:\n            x = self.sample()\n            x = self.check_bounds(x)\n            \n            f = np.array([func(x[:,i]) if self.eval_count < self.budget else np.inf for i in range(self.popsize)])\n            self.eval_count += self.popsize\n            \n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)].copy()\n                self.stagnation_counter = 0\n            else:\n                self.stagnation_counter += 1\n\n            self.best_f_history.append(self.f_opt)\n            if len(self.best_f_history) > self.stagnation_threshold:\n                self.best_f_history.pop(0)\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                # Restart with increased population size\n                self.m = self.rng.uniform(self.lb, self.ub, size=self.dim)\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.A = np.eye(self.dim)\n                self.sigma = 0.5\n                self.stagnation_counter = 0\n                self.popsize = min(self.popsize * 2, self.budget // 2)  # Increase popsize, but not too much\n                self.mu = self.popsize // 2\n                self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n                self.weights = self.weights / np.sum(self.weights)\n\n            idx = np.argsort(f)\n            x_mu = x[:, idx[:self.mu]]\n\n            m_old = self.m.copy()\n            self.m = np.sum(x_mu * self.weights[None, :], axis=1)\n\n            z_mu = np.linalg.solve(self.A, (x_mu - m_old[:, None]))\n            \n            self.ps = (1 - self.cs) * self.ps + (self.cs * (2 - self.cs))**0.5 * (self.m - m_old) / self.sigma\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.eval_count / self.popsize)) / self.chiN < 1.4 + 2/(self.dim+1))\n            self.pc = (1 - self.damps) * self.pc + hsig * (self.damps * (2 - self.damps))**0.5 * (self.m - m_old) / self.sigma\n\n            self.C = (1 - self.c_1 - self.c_mu) * self.C + self.c_1 * (self.pc[:, None] @ self.pc[None, :]) + self.c_mu * (z_mu * self.weights[None, :]) @ z_mu.T\n            \n            self.sigma *= np.exp((np.linalg.norm(self.ps) / self.chiN - 1) * self.cs / self.damps)\n\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            self.C = np.diag(np.diag(self.C)) + 0.9 * (self.C - np.diag(np.diag(self.C)))\n            try:\n                self.A = np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-10 * np.eye(self.dim)  # Add a small constant to the diagonal\n                self.A = np.linalg.cholesky(self.C)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "An exception occurred: index 2 is out of bounds for axis 1 with size 2.", "error": "", "parent_ids": ["434d72f5-d481-420f-a3cc-17a7c96d8894"], "operator": null, "metadata": {}}
{"id": "b51657d9-9cbc-4295-9c4d-05b3d1b38237", "fitness": 0.0, "name": "AdaptiveGaussianAdaptationV2", "description": "Adaptive Gaussian with covariance matrix adaptation, dynamic population size, and search space shrinking, incorporating a restart mechanism and multiple mutation strategies.", "code": "import numpy as np\n\nclass AdaptiveGaussianAdaptationV2:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=20, lr=0.1, initial_scale=0.5, restart_trigger=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.lr = lr\n        self.lb = -5.0\n        self.ub = 5.0\n        self.scale = initial_scale\n        self.scale_decay = 0.995\n        self.min_scale = 0.01\n        self.momentum_lb = 0.0\n        self.momentum_ub = 0.0\n        self.momentum_alpha = 0.9\n        self.covariance = np.eye(dim)  # Initialize covariance matrix\n        self.restart_trigger = int(restart_trigger * budget)\n        self.eval_count = 0\n        self.stagnation_counter = 0\n        self.max_stagnation = 500  # Maximum stagnation before restart\n        self.expansion_factor = 1.2 # Scale increase after restarts\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        self.eval_count += self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n        best_fitness_history = [self.f_opt]\n\n        # Shrinking search space parameters\n        current_lb = self.lb\n        current_ub = self.ub\n\n        while self.budget > 0:\n            # Find best individual\n            best_idx = np.argmin(fitness)\n            best_x = population[best_idx]\n            best_f = fitness[best_idx]\n\n            # Adapt covariance matrix\n            diff = population - best_x\n            self.covariance = 0.9 * self.covariance + 0.1 * np.cov(diff.T)\n            self.covariance += np.eye(self.dim) * 1e-6  # Add small value for stability\n\n            # Generate new solutions using Gaussian distribution\n            try:\n                new_population = np.random.multivariate_normal(mean=best_x, cov=self.scale**2 * self.covariance, size=self.pop_size)\n            except np.linalg.LinAlgError:\n                # If covariance matrix is not positive semidefinite, reset it\n                self.covariance = np.eye(self.dim)\n                new_population = np.random.normal(loc=best_x, scale=self.scale * (current_ub - current_lb), size=(self.pop_size, self.dim))\n\n            # Clip new solutions to the current search space\n            new_population = np.clip(new_population, current_lb, current_ub)\n\n            # Evaluate new solutions\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n            self.eval_count += self.pop_size\n\n            # Update best solution\n            best_new_idx = np.argmin(new_fitness)\n            if new_fitness[best_new_idx] < self.f_opt:\n                self.f_opt = new_fitness[best_new_idx]\n                self.x_opt = new_population[best_new_idx]\n                self.stagnation_counter = 0 # Reset stagnation counter\n            else:\n                self.stagnation_counter += self.pop_size\n            \n            # Update population (replace worst with best from new population)\n            worst_idx = np.argmax(fitness)\n            if new_fitness[best_new_idx] < fitness[worst_idx]:\n                population[worst_idx] = new_population[best_new_idx]\n                fitness[worst_idx] = new_fitness[best_new_idx]\n            \n            # Shrink search space with momentum\n            lb_change = self.lr * (self.lb - best_x.min())\n            ub_change = self.lr * (self.ub - best_x.max())\n\n            self.momentum_lb = self.momentum_alpha * self.momentum_lb + (1 - self.momentum_alpha) * lb_change\n            self.momentum_ub = self.momentum_alpha * self.momentum_ub + (1 - self.momentum_alpha) * ub_change\n\n            current_lb = max(self.lb, best_x.min() + self.momentum_lb)\n            current_ub = min(self.ub, best_x.max() + self.momentum_ub)\n            \n            # Ensure lb < ub\n            if np.any(current_lb >= current_ub):\n                current_lb = np.minimum(current_lb, current_ub - 1e-6)\n                current_ub = np.maximum(current_ub, current_lb + 1e-6)\n            \n            #Decay the scale\n            self.scale = max(self.scale * self.scale_decay, self.min_scale)\n\n            # Dynamic population size\n            if self.eval_count > self.restart_trigger:\n                self.pop_size = min(int(self.pop_size * 1.05), 100) #Increase pop size\n                self.restart_trigger = np.inf #Deactivate pop size increase\n            \n            if self.stagnation_counter > self.max_stagnation:\n                 # Restart: Re-initialize population and expand the search space\n                population = np.random.uniform(current_lb, current_ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.f_opt = np.min(fitness)\n                self.x_opt = population[np.argmin(fitness)]\n                self.stagnation_counter = 0 #reset counter\n                self.scale *= self.expansion_factor #Increase scale\n                self.covariance = np.eye(self.dim) #Reset covariance\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveGaussianAdaptationV2 scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["61f0db2b-1df7-42f4-892e-cd2996cd77c2"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "7c69f5a1-64a0-416b-b215-0d46a5965fef", "fitness": 0.06540826882414175, "name": "AdaptiveCMAES", "description": "Introducing a learning rate for covariance matrix adaptation and population size adaptation to dynamically balance exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveCMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, sigma0=0.5, cs=0.3, cc=None, c1=None, cmu=None, learning_rate_C=0.2, learning_rate_sigma=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.sigma = sigma0\n        self.cs = cs\n        self.learning_rate_C = learning_rate_C\n        self.learning_rate_sigma = learning_rate_sigma\n\n        self.mu = np.random.uniform(-2.0, 2.0, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.B = np.eye(self.dim)\n        self.D = np.ones(self.dim)\n        self.eigen_updated = False\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.count_evals = 0\n\n        self.mueff = self.popsize / 2\n        self.cc = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim) if cc is None else cc\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff) if c1 is None else c1\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2)**2 + self.mueff)) if cmu is None else cmu\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n\n    def sample_population(self):\n        z = np.random.randn(self.dim, self.popsize)\n        x = self.mu[:, np.newaxis] + self.sigma * (self.B @ (self.D[:, np.newaxis] * z))\n        x = np.clip(x, -5.0, 5.0)\n        return x\n\n    def update_distribution(self, x, fvals):\n        idx = np.argsort(fvals)\n        x_sorted = x[:, idx]\n        x_mu = x_sorted[:, :self.popsize // 2].mean(axis=1)\n        y = (x_mu - self.mu) / self.sigma\n\n        self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * (self.B @ y)\n        hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**2 * self.count_evals / self.budget) < 1.4 + 2 / (self.dim + 1))\n        delta_h = (1 - hsig) * self.cc * (2 - self.cc)\n        self.pc = (1 - self.cc) * self.pc + np.sqrt(self.cc * (2 - self.cc) * self.mueff) * y\n\n        self.mu = x_mu\n\n        # Apply learning rate to C update\n        self.C = (1 - self.learning_rate_C) * self.C + self.learning_rate_C * (self.c1 * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :] + delta_h * self.C) + \\\n                   self.cmu * (x_sorted[:, :self.popsize // 2] - self.mu[:, np.newaxis]) @ (x_sorted[:, :self.popsize // 2] - self.mu[:, np.newaxis]).T / self.popsize)\n        \n        # Apply learning rate to sigma update\n        self.sigma *= np.exp((self.learning_rate_sigma * self.cs / self.damps) * (np.linalg.norm(self.ps) / np.sqrt(self.dim) - 1))\n\n        # Adapt population size based on performance\n        if self.count_evals % (2*self.popsize) == 0:\n          if np.std(fvals) < 0.01:  #Stagnation detection\n            self.popsize = min(self.popsize * 2, 2*self.dim) #Increase population to explore\n          else:\n            self.popsize = max(4 + int(3 * np.log(self.dim)), self.popsize // 2)  # Reduce if not stagnant\n          self.mueff = self.popsize / 2\n          self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2)**2 + self.mueff))\n\n\n    def update_eigen(self):\n        try:\n            self.D, self.B = np.linalg.eigh(self.C)\n        except np.linalg.LinAlgError:\n            self.C = self.C + 1e-8 * np.eye(self.dim)\n            self.D, self.B = np.linalg.eigh(self.C)\n        self.D = np.sqrt(np.maximum(self.D, 1e-20))\n        self.eigen_updated = True\n\n    def __call__(self, func):\n        while self.count_evals < self.budget:\n            if not self.eigen_updated:\n                self.update_eigen()\n\n            x = self.sample_population()\n            fvals = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.count_evals += self.popsize\n\n            if np.min(fvals) < self.f_opt:\n                self.f_opt = np.min(fvals)\n                self.x_opt = x[:, np.argmin(fvals)]\n\n            self.update_distribution(x, fvals)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveCMAES scored 0.065 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d0d2c086-4c96-4e38-906e-48c9dda93c2a"], "operator": null, "metadata": {"aucs": [0.1308165376482835, 0]}}
{"id": "dc0b1200-9600-41ed-a896-07b46aab6203", "fitness": 0.4022734416422188, "name": "AdaptiveGaussianAdaptation", "description": "Adaptive Gaussian mutation focusing on best solution with simplified search space adaptation and scale decay.", "code": "import numpy as np\n\nclass AdaptiveGaussianAdaptation:\n    def __init__(self, budget=10000, dim=10, pop_size=10, lr=0.05, initial_scale=0.5, scale_decay=0.995, min_scale=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lr = lr\n        self.lb = -5.0\n        self.ub = 5.0\n        self.scale = initial_scale\n        self.scale_decay = scale_decay\n        self.min_scale = min_scale\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        # Shrinking search space parameters\n        current_lb = self.lb\n        current_ub = self.ub\n\n        while self.budget > 0:\n            # Find best individual\n            best_idx = np.argmin(fitness)\n            best_x = population[best_idx]\n            best_f = fitness[best_idx]\n\n            # Generate new solutions around the best individual using Gaussian distribution\n            new_population = np.random.normal(loc=best_x, scale=self.scale * (current_ub - current_lb), size=(self.pop_size, self.dim))\n\n            # Clip new solutions to the current search space\n            new_population = np.clip(new_population, current_lb, current_ub)\n\n            # Evaluate new solutions\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n\n            # Update best solution\n            best_new_idx = np.argmin(new_fitness)\n            if new_fitness[best_new_idx] < self.f_opt:\n                self.f_opt = new_fitness[best_new_idx]\n                self.x_opt = new_population[best_new_idx]\n            \n            # Update population (replace worst with best from new population)\n            worst_idx = np.argmax(fitness)\n            if new_fitness[best_new_idx] < fitness[worst_idx]:\n                population[worst_idx] = new_population[best_new_idx]\n                fitness[worst_idx] = new_fitness[best_new_idx]\n            \n            # Shrink search space\n            current_lb = max(self.lb, best_x.min() - self.lr * (self.ub - self.lb))\n            current_ub = min(self.ub, best_x.max() + self.lr * (self.ub - self.lb))\n            \n            # Ensure lb < ub\n            if np.any(current_lb >= current_ub):\n                current_lb = np.minimum(current_lb, current_ub - 1e-6)\n                current_ub = np.maximum(current_ub, current_lb + 1e-6)\n            \n            #Decay the scale\n            self.scale = max(self.scale * self.scale_decay, self.min_scale)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveGaussianAdaptation scored 0.402 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["61f0db2b-1df7-42f4-892e-cd2996cd77c2"], "operator": null, "metadata": {"aucs": [0.18393586986335175, 0.38547358465987047, 0.40198819549594744, 0.5658510104208323, 0.29733699571752825, 0.36749386470592904, 0.2935808039718637, 0.325333402105748, 0.3023071790912357, 0.24046540869353428, 0.49964163895953984, 0.9989431604900136, 0.28919539293614094, 0.29286395622204986, 0.742166187160551, 0.3921441658096042, 0.3295346735634763, 0.4464905311527102, 0.19590322733099053, 0.494819584493458]}}
{"id": "314117cc-d642-46c7-8cba-4208678cb9d2", "fitness": 0.30304019412706285, "name": "AdaptiveNeighborhoodSearch", "description": "Neighborhood Search with Adaptive Step Size and Gradient Estimation, using a history of successful moves to adjust the step size and direction.", "code": "import numpy as np\n\nclass AdaptiveNeighborhoodSearch:\n    def __init__(self, budget=10000, dim=10, initial_step_size=0.1, success_history_size=10, gradient_estimation_samples=5):\n        self.budget = budget\n        self.dim = dim\n        self.x = None\n        self.f = np.Inf\n        self.step_size = initial_step_size\n        self.success_history = []\n        self.success_history_size = success_history_size\n        self.gradient_estimation_samples = gradient_estimation_samples\n        self.lb = -5.0\n        self.ub = 5.0\n        self.eval_count = 0\n        self.gradient = np.zeros(self.dim)\n\n    def clip(self, x):\n        return np.clip(x, self.lb, self.ub)\n\n    def estimate_gradient(self, func, x):\n        \"\"\"Estimates the gradient using finite differences.\"\"\"\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            delta = np.zeros(self.dim)\n            delta[i] = self.step_size  # Use step_size for gradient estimation\n            x_plus = self.clip(x + delta)\n            x_minus = self.clip(x - delta)\n\n            f_plus = func(x_plus)\n            f_minus = func(x_minus)\n            \n            self.eval_count += 2 # two function evaluations per dimension\n            gradient[i] = (f_plus - f_minus) / (2 * self.step_size)\n        return gradient\n\n    def __call__(self, func):\n        self.x = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.f = func(self.x)\n        self.eval_count = 1\n        self.best_x = self.x.copy()\n        self.best_f = self.f\n\n        while self.eval_count < self.budget:\n            # Estimate gradient direction\n            if self.eval_count + 2 * self.dim < self.budget:\n                self.gradient = self.estimate_gradient(func, self.x)\n            else:\n                self.gradient = np.random.randn(self.dim)\n\n            # Normalize gradient\n            gradient_norm = np.linalg.norm(self.gradient)\n            if gradient_norm > 0:\n                direction = -self.gradient / gradient_norm\n            else:\n                direction = np.random.randn(self.dim)\n                direction /= np.linalg.norm(direction)\n\n            # Take a step\n            x_new = self.clip(self.x + self.step_size * direction)\n            f_new = func(x_new)\n            self.eval_count += 1\n\n            # Check for improvement\n            if f_new < self.f:\n                self.success_history.append(True)\n                self.x = x_new\n                self.f = f_new\n                if f_new < self.best_f:\n                    self.best_f = f_new\n                    self.best_x = x_new.copy()\n            else:\n                self.success_history.append(False)\n\n            # Adjust step size based on success history\n            if len(self.success_history) > self.success_history_size:\n                self.success_history.pop(0)\n\n            success_rate = np.mean(self.success_history) if self.success_history else 0.5\n\n            if success_rate > 0.6:\n                self.step_size *= 1.1  # Increase step size\n            elif success_rate < 0.4:\n                self.step_size *= 0.9  # Decrease step size\n            \n            self.step_size = np.clip(self.step_size, 1e-6, 1.0) # prevent step size from becoming too small or too large\n        return self.best_f, self.best_x", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveNeighborhoodSearch scored 0.303 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["434d72f5-d481-420f-a3cc-17a7c96d8894"], "operator": null, "metadata": {"aucs": [0.07214598081062396, 0.09110640839015782, 0.2028169160542277, 0.1630734953319396, 0.8770264125591407, 0.0959982813698016, 0.2236808223877299, 0.7872932851925705, 0.18072349336802074, 0.15488978200050263, 0.1968876932224698, 0.1851472672302078, 0.21116519552151114, 0.2751358519371798, 0.7097939261087198, 0.28538359589622675, 0.27936699095685447, 0.8749957917681126, 0.0791601350540464, 0.11501255738121308]}}
{"id": "b6310af5-996a-4b3c-bad8-2cf94dc47ae4", "fitness": 0.42870263860365954, "name": "AdaptiveGaussianAging", "description": "An adaptive Gaussian mutation algorithm with dynamic learning rate, covariance matrix adaptation using rank-one update, and aging mechanism to diversify the population.", "code": "import numpy as np\n\nclass AdaptiveGaussianAging:\n    def __init__(self, budget=10000, dim=10, pop_size_init=20, lr_init=0.1, sigma_init=0.1, aging_rate=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_init = pop_size_init\n        self.lr_init = lr_init  # Initial learning rate for adapting the search space\n        self.sigma_init = sigma_init\n        self.lb = -5.0\n        self.ub = 5.0\n        self.aging_rate = aging_rate  # Rate at which individuals age\n        self.ages = None\n\n    def __call__(self, func):\n        # Initialize population\n        pop_size = self.pop_size_init\n        population = np.random.uniform(self.lb, self.ub, size=(pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= pop_size\n        self.ages = np.zeros(pop_size)  # Initialize ages\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        # Shrinking search space parameters\n        current_lb = self.lb\n        current_ub = self.ub\n        sigma = self.sigma_init * (self.ub - self.lb)  # Initial standard deviation\n        lr = self.lr_init\n\n        # Covariance matrix adaptation (Rank-One Update)\n        C = np.eye(self.dim)  # Initialize covariance matrix\n        learning_rate_C = 0.1\n\n        while self.budget > 0:\n            # Find best individual\n            best_idx = np.argmin(fitness)\n            best_x = population[best_idx]\n            best_f = fitness[best_idx]\n\n            # Adaptive population size\n            pop_size = min(self.pop_size_init + int(self.budget / 1000), self.budget)\n            if pop_size <= 0:\n               pop_size = 1  # ensure pop size is at least 1\n\n            # Generate new solutions around the best individual using Gaussian distribution with covariance matrix\n            new_population = np.random.multivariate_normal(mean=best_x, cov=sigma**2 * C, size=pop_size)\n\n            # Clip new solutions to the current search space\n            new_population = np.clip(new_population, current_lb, current_ub)\n\n            # Evaluate new solutions\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= pop_size\n\n            # Update best solution\n            best_new_idx = np.argmin(new_fitness)\n            if new_fitness[best_new_idx] < self.f_opt:\n                self.f_opt = new_fitness[best_new_idx]\n                self.x_opt = new_population[best_new_idx]\n\n            # Update population (replace oldest with best from new population)\n            oldest_idx = np.argmax(self.ages)\n            best_new_idx = np.argmin(new_fitness)\n            if new_fitness[best_new_idx] < fitness[oldest_idx]:\n                population[oldest_idx] = new_population[best_new_idx]\n                fitness[oldest_idx] = new_fitness[best_new_idx]\n                self.ages[oldest_idx] = 0  # Reset age\n\n            # Increase ages of all individuals\n            self.ages += 1\n\n            # Shrink search space\n            current_lb = max(self.lb, best_x.min() + lr * (self.lb - best_x.min()))  # Adapt lb\n            current_ub = min(self.ub, best_x.max() + lr * (self.ub - best_x.max()))  # Adapt ub\n\n            # Ensure lb < ub\n            if np.any(current_lb >= current_ub):\n                current_lb = np.minimum(current_lb, current_ub - 1e-6)\n                current_ub = np.maximum(current_ub, current_lb + 1e-6)\n\n            # Adapt step size (sigma)\n            if self.f_opt < best_f:\n                sigma *= 0.95\n                lr = min(self.lr_init * 1.1, 0.5)  # Increase learning rate if improving\n            else:\n                sigma *= 1.05\n                lr = max(self.lr_init * 0.9, 0.01) # Reduce learning rate if not improving\n\n            # Update covariance matrix (Rank-One Update)\n            diff = population[best_idx] - best_x\n            C = (1 - learning_rate_C) * C + learning_rate_C * np.outer(diff, diff)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveGaussianAging scored 0.429 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6a048c90-9466-4f63-a43a-1c78e7eea32b"], "operator": null, "metadata": {"aucs": [0.14397136782521924, 0.1790172154409444, 0.4860061398534359, 0.1794371521205319, 0.40943389026466703, 0.4574311448363332, 0.28846607274779457, 0.39830891281312797, 0.4460928576192338, 0.1952541093274872, 0.879601991102228, 0.9651396072047782, 0.28769273270303153, 0.27179408545800454, 0.8678183616342944, 0.46140401509040185, 0.38764603106103146, 0.6025084350428236, 0.18731004620736724, 0.47971860372045416]}}
{"id": "0fe38f9a-74f4-44f9-a0e6-5b8bce531f58", "fitness": 0.5574877309236598, "name": "AdaptiveGaussianAdaptation", "description": "Adaptive Gaussian mutation with dynamic population size, covariance matrix adaptation, search space shrinkage with momentum, and restart mechanism based on stagnation detection.", "code": "import numpy as np\n\nclass AdaptiveGaussianAdaptation:\n    def __init__(self, budget=10000, dim=10, pop_size=20, lr=0.1, initial_scale=0.5, stagnation_threshold=500):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lr = lr  # Learning rate for adapting the search space\n        self.lb = -5.0\n        self.ub = 5.0\n        self.scale = initial_scale  # Initial scale for Gaussian distribution\n        self.scale_decay = 0.995  # Decay factor for the scale\n        self.min_scale = 0.01  # Minimum scale value\n        self.momentum_lb = 0.0\n        self.momentum_ub = 0.0\n        self.momentum_alpha = 0.9\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.restart_counter = 0\n        self.covariance = np.eye(dim)  # Initialize covariance matrix\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n        self.best_fitness_history = [self.f_opt]\n\n        # Shrinking search space parameters\n        current_lb = self.lb\n        current_ub = self.ub\n\n        while self.budget > 0:\n            # Find best individual\n            best_idx = np.argmin(fitness)\n            best_x = population[best_idx]\n            best_f = fitness[best_idx]\n\n            # Generate new solutions around the best individual using Gaussian distribution with covariance matrix\n            new_population = np.random.multivariate_normal(mean=best_x, cov=self.scale**2 * self.covariance, size=self.pop_size)\n\n\n            # Clip new solutions to the current search space\n            new_population = np.clip(new_population, current_lb, current_ub)\n\n            # Evaluate new solutions\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n\n            # Update best solution\n            best_new_idx = np.argmin(new_fitness)\n            if new_fitness[best_new_idx] < self.f_opt:\n                self.f_opt = new_fitness[best_new_idx]\n                self.x_opt = new_population[best_new_idx]\n                self.best_fitness_history.append(self.f_opt)\n                self.stagnation_counter = 0  # Reset stagnation counter\n            else:\n                self.best_fitness_history.append(self.f_opt)\n                self.stagnation_counter += 1\n\n            # Update population (replace worst with best from new population)\n            worst_idx = np.argmax(fitness)\n            best_new_idx = np.argmin(new_fitness)  # Ensure we use the index from the new_fitness\n            if new_fitness[best_new_idx] < fitness[worst_idx]:\n                population[worst_idx] = new_population[best_new_idx]\n                fitness[worst_idx] = new_fitness[best_new_idx]\n\n            # Shrink search space with momentum\n            lb_change = self.lr * (self.lb - best_x.min())\n            ub_change = self.lr * (self.ub - best_x.max())\n\n            self.momentum_lb = self.momentum_alpha * self.momentum_lb + (1 - self.momentum_alpha) * lb_change\n            self.momentum_ub = self.momentum_alpha * self.momentum_ub + (1 - self.momentum_alpha) * ub_change\n\n            current_lb = max(self.lb, best_x.min() + self.momentum_lb)  # Adapt lb\n            current_ub = min(self.ub, best_x.max() + self.momentum_ub)  # Adapt ub\n\n            # Ensure lb < ub\n            if np.any(current_lb >= current_ub):\n                current_lb = np.minimum(current_lb, current_ub - 1e-6)\n                current_ub = np.maximum(current_ub, current_lb + 1e-6)\n\n            # Decay the scale\n            self.scale = max(self.scale * self.scale_decay, self.min_scale)\n\n            # Update covariance matrix (simple adaptation)\n            diff = population - np.mean(population, axis=0)\n            self.covariance = np.cov(diff.T) + 1e-9 * np.eye(self.dim)  # Add a small value to avoid singular matrix\n\n            # Stagnation detection and restart\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.restart_counter += 1\n                # Reset population, fitness, search space, and covariance\n                population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                current_lb = self.lb\n                current_ub = self.ub\n                self.covariance = np.eye(self.dim)\n                self.scale = self.initial_scale\n                self.stagnation_counter = 0\n                self.best_fitness_history.append(np.min(fitness))\n                if np.min(fitness) < self.f_opt:\n                  self.f_opt = np.min(fitness)\n                  self.x_opt = population[np.argmin(fitness)]\n                self.budget -= self.pop_size\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveGaussianAdaptation scored 0.557 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["61f0db2b-1df7-42f4-892e-cd2996cd77c2"], "operator": null, "metadata": {"aucs": [0.11018001826812684, 0.23541303585021944, 0.8741211510348544, 0.9522798627242616, 0.21689925429506252, 0.9202649159759978, 0.277077217143353, 0.773866525953391, 0.35962288091395544, 0.17622249348750296, 0.9240186724253451, 0.9948182278720292, 0.35181887223493113, 0.23442421883478592, 0.9542449168128555, 0.7617526401480065, 0.4217898294332868, 0.9438243863822735, 0.18885738856868006, 0.47825811011427943]}}
{"id": "47b0182b-9407-42e4-b079-9d479ee0adc0", "fitness": 0.1539831542934243, "name": "OrthogonalCMAES", "description": "Orthogonal CMA-ES with enhanced exploration through orthogonal sampling and a simplified covariance matrix update for faster adaptation.", "code": "import numpy as np\n\nclass OrthogonalCMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, sigma0=0.5, cs=0.3, cc=None, c1=None, cmu=None):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.sigma = sigma0\n        self.cs = cs\n\n        self.mu = np.random.uniform(-2.0, 2.0, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.B = np.eye(self.dim)\n        self.D = np.ones(self.dim)\n        self.eigen_updated = False\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.count_evals = 0\n\n        self.mueff = self.popsize / 2\n        self.cc = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim) if cc is None else cc\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff) if c1 is None else c1\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2)**2 + self.mueff)) if cmu is None else cmu\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n\n    def sample_orthogonal_population(self):\n        # Generate an orthogonal matrix using the Gram-Schmidt process\n        Z = np.random.randn(self.popsize, self.dim)\n        Q, _ = np.linalg.qr(Z)  # Q is the orthogonal matrix\n\n        # Sample from the orthogonal matrix\n        z = Q.T  # Transpose to get the same shape as in standard CMA-ES\n        x = self.mu[:, np.newaxis] + self.sigma * (self.B @ (self.D[:, np.newaxis] * z))\n        x = np.clip(x, -5.0, 5.0)\n        return x\n\n    def update_distribution(self, x, fvals):\n        idx = np.argsort(fvals)\n        x_sorted = x[:, idx]\n        x_mu = x_sorted[:, :self.popsize // 2].mean(axis=1)\n        y = (x_mu - self.mu) / self.sigma\n\n        self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * (self.B @ y)\n        hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**2 * self.count_evals / self.budget) < 1.4 + 2 / (self.dim + 1))\n        delta_h = (1 - hsig) * self.cc * (2 - self.cc)\n        self.pc = (1 - self.cc) * self.pc + np.sqrt(self.cc * (2 - self.cc) * self.mueff) * y\n\n        self.mu = x_mu\n\n        # Simplified covariance matrix adaptation using only rank-one update\n        self.C = (1 - self.c1) * self.C + self.c1 * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :])\n\n        self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / np.sqrt(self.dim) - 1))\n\n    def update_eigen(self):\n        try:\n            self.D, self.B = np.linalg.eigh(self.C)\n        except np.linalg.LinAlgError:\n            self.C = self.C + 1e-8 * np.eye(self.dim)\n            self.D, self.B = np.linalg.eigh(self.C)\n        self.D = np.sqrt(np.maximum(self.D, 1e-20))\n        self.eigen_updated = True\n\n    def __call__(self, func):\n        while self.count_evals < self.budget:\n            if not self.eigen_updated:\n                self.update_eigen()\n\n            x = self.sample_orthogonal_population()\n            fvals = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.count_evals += self.popsize\n\n            if np.min(fvals) < self.f_opt:\n                self.f_opt = np.min(fvals)\n                self.x_opt = x[:, np.argmin(fvals)]\n\n            self.update_distribution(x, fvals)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm OrthogonalCMAES scored 0.154 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d0d2c086-4c96-4e38-906e-48c9dda93c2a"], "operator": null, "metadata": {"aucs": [0.09384237171673682, 0.1690456180780111, 0.2317194126974217, 0.09226675088301761, 0.13895897120958967, 0.14644275298274512, 0.14947318607634952, 0.1453229654289233, 0.14727535410712267, 0.12146599841151706, 0.15362523515063453, 0.25170391103921097, 0.23848055400749213, 0.11971052010139371, 0.11214481418207356, 0.23182220597046566, 0.16342876313923915, 0.1272672435092227, 0.09152744927311018, 0.15413900790420887]}}
{"id": "a01846fa-5dd3-4e1f-b67a-92fa673437e8", "fitness": 0.5382094290663144, "name": "AdaptiveDifferentialEvolutionCMA", "description": "An adaptive differential evolution strategy with covariance matrix adaptation and dynamic parameter control for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolutionCMA:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, Cr=0.9, lr=0.1, sigma_init=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Differential weight\n        self.Cr = Cr  # Crossover probability\n        self.lr = lr  # Learning rate for bounds adaptation\n        self.sigma_init = sigma_init\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        # Bounds\n        current_lb = self.lb\n        current_ub = self.ub\n        sigma = self.sigma_init * (self.ub - self.lb)\n\n        # CMA Initialization\n        C = np.eye(self.dim)\n        learning_rate_C = 0.1\n\n        # Adaptive F and Cr\n        F = self.F\n        Cr = self.Cr\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Differential Evolution mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n                \n                # Mutation and crossover\n                v = x_r1 + F * (x_r2 - x_r3)\n                u = np.zeros(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < Cr:\n                        u[j] = v[j]\n                    else:\n                        u[j] = population[i, j]\n                \n                # Clip to bounds\n                u = np.clip(u, current_lb, current_ub)\n                \n                # Evaluate trial vector\n                f_u = func(u)\n                self.budget -= 1\n                \n                if f_u < fitness[i]:\n                    population[i] = u\n                    fitness[i] = f_u\n                    \n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n            \n            # Bounds adaptation\n            best_x = population[np.argmin(fitness)]\n            current_lb = max(self.lb, best_x.min() + self.lr * (self.lb - best_x.min()))\n            current_ub = min(self.ub, best_x.max() + self.lr * (self.ub - best_x.max()))\n            \n            if np.any(current_lb >= current_ub):\n                current_lb = np.minimum(current_lb, current_ub - 1e-6)\n                current_ub = np.maximum(current_ub, current_lb + 1e-6)\n            \n            # CMA Update (using best individual)\n            diff = population[np.argmin(fitness)] - self.x_opt \n            C = (1 - learning_rate_C) * C + learning_rate_C * np.outer(diff, diff)\n\n            #Adapt step size\n            if self.f_opt < np.min(fitness):\n              sigma *= 0.95\n            else:\n              sigma *= 1.05\n\n            # Adaptive F and Cr\n            if np.random.rand() < 0.1:\n                F = np.random.normal(self.F, 0.1)\n                F = np.clip(F, 0.1, 1.0)\n            if np.random.rand() < 0.1:\n                Cr = np.random.normal(self.Cr, 0.1)\n                Cr = np.clip(Cr, 0.1, 1.0)\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDifferentialEvolutionCMA scored 0.538 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6a048c90-9466-4f63-a43a-1c78e7eea32b"], "operator": null, "metadata": {"aucs": [0.12714845350244963, 0.31915298885249643, 0.845898362883686, 0.9632127015308037, 0.1991938902406798, 0.877837981801072, 0.30328679544822135, 0.5561001245768977, 0.28048465812908785, 0.311545919633779, 0.922304678360349, 0.9934756244451707, 0.27570604319478664, 0.2466009426046487, 0.9691285939614956, 0.710092840785409, 0.40999493386042285, 0.7768816257115985, 0.196649017103812, 0.4794924046994211]}}
{"id": "decb1e0b-71ca-46a7-a57e-e1c16f0ef187", "fitness": 0.4683440710209917, "name": "SimplifiedCMAES", "description": "Simplified CMA-ES with less parameters and adaptive sigma update based on success rate for better exploration-exploitation balance.", "code": "import numpy as np\n\nclass SimplifiedCMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.m = None\n        self.sigma = 0.5\n        self.C = None\n        self.success_rate = 0.5\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n\n\n    def sample(self):\n        z = np.random.randn(self.dim, self.popsize)\n        x = self.m[:, None] + self.sigma * np.dot(self.A, z)\n        return x\n\n    def check_bounds(self, x):\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        self.m = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.A = np.eye(self.dim)\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        \n        while self.eval_count < self.budget:\n            x = self.sample()\n            x = self.check_bounds(x)\n            \n            f = np.array([func(x[:,i]) if self.eval_count < self.budget else np.inf for i in range(self.popsize)])\n            self.eval_count += self.popsize\n            \n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)].copy()\n                self.success_rate = 0.8  # Increase sigma if successful\n            else:\n                self.success_rate = 0.2 # Decrease sigma if unsuccessful\n\n            idx = np.argsort(f)\n            x_mu = x[:, idx[:self.mu]]\n            m_old = self.m.copy()\n            self.m = np.sum(x_mu * self.weights[None, :], axis=1)\n\n            z_mu = np.linalg.solve(self.A, (x_mu - m_old[:, None]))\n            self.C = (0.5) * self.C + 0.5 * (z_mu * self.weights[None, :]) @ z_mu.T\n\n            self.sigma *= np.exp(self.success_rate - 0.5) \n\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            self.C = np.diag(np.diag(self.C)) + 0.9 * (self.C - np.diag(np.diag(self.C)))\n            try:\n                self.A = np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-10 * np.eye(self.dim)  # Add a small constant to the diagonal\n                self.A = np.linalg.cholesky(self.C)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm SimplifiedCMAES scored 0.468 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["434d72f5-d481-420f-a3cc-17a7c96d8894"], "operator": null, "metadata": {"aucs": [0.14161946263355696, 0.2486149772590105, 0.5903409655506477, 0.9606936185067398, 0.9539796692824276, 0.5337333074826889, 0.31266449366014193, 0.5567280981098548, 0.17709056165928327, 0.1386463603645276, 0.9480823486979233, 0.23165750672348395, 0.2698292179094728, 0.3105215180832165, 0.6518272942475217, 0.40616014160707936, 0.3048546755322247, 0.9407350638719363, 0.20866519728316824, 0.4804369419549276]}}
{"id": "aa048cb5-d176-40bc-ad9d-a951a4690418", "fitness": 0.3488987998010903, "name": "BiasedCMAES", "description": "Covariance Matrix Adaptation Evolution Strategy with Biased Sampling and Selective Pressure Adaptation to dynamically adjust search behavior based on function evaluations.", "code": "import numpy as np\n\nclass BiasedCMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damps=1, c_mu=0.3, c_1=0.3, stagnation_threshold=50, bias_factor=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.m = None\n        self.sigma = 0.5\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.chiN = None\n        self.cs = cs\n        self.damps = damps\n        self.c_mu = c_mu\n        self.c_1 = c_1\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.lb = -5.0\n        self.ub = 5.0\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_f_history = []\n        self.bias_factor = bias_factor  # Introduce bias factor\n        self.selective_pressure = 1.0 # Initialize selective pressure\n        self.pressure_adaptation_rate = 0.05\n\n\n    def sample(self):\n        z = np.random.randn(self.dim, self.popsize)\n\n        # Introduce biased sampling\n        if self.f_opt != np.inf:\n            bias_direction = (self.x_opt - self.m)\n            z += self.bias_factor * bias_direction[:,None] / (np.linalg.norm(bias_direction) + 1e-8) # Add bias towards best solution\n\n        x = self.m[:, None] + self.sigma * np.dot(self.A, z)\n        return x\n\n    def check_bounds(self, x):\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        self.m = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.A = np.eye(self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1/(4*self.dim) + 1/(21*self.dim**2))\n        self.eval_count = 0\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.stagnation_counter = 0\n        self.best_f_history = []\n        self.selective_pressure = 1.0 #Reinitialize selective pressure\n\n        while self.eval_count < self.budget:\n            x = self.sample()\n            x = self.check_bounds(x)\n            \n            f = np.array([func(x[:,i]) if self.eval_count < self.budget else np.inf for i in range(self.popsize)])\n            self.eval_count += self.popsize\n            \n            if np.min(f) < self.f_opt:\n                    self.f_opt = np.min(f)\n                    self.x_opt = x[:, np.argmin(f)].copy()\n                    self.stagnation_counter = 0\n                    self.selective_pressure = min(1.0, self.selective_pressure + self.pressure_adaptation_rate) # Increase pressure when improving\n\n            else:\n                self.stagnation_counter += 1\n                self.selective_pressure = max(0.1, self.selective_pressure - self.pressure_adaptation_rate) #Decrease pressure when stagnating\n\n\n            self.best_f_history.append(self.f_opt)\n            if len(self.best_f_history) > self.stagnation_threshold:\n                self.best_f_history.pop(0)\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                # Restart mechanism\n                self.m = np.random.uniform(self.lb, self.ub, size=self.dim)\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.A = np.eye(self.dim)\n                self.sigma = 0.5\n                self.stagnation_counter = 0\n                self.selective_pressure = 1.0 #Reinitialize selective pressure\n\n\n            # Apply selective pressure to weights\n            adjusted_weights = self.weights**self.selective_pressure\n            adjusted_weights /= np.sum(adjusted_weights)\n\n            idx = np.argsort(f)\n            x_mu = x[:, idx[:self.mu]]\n\n            m_old = self.m.copy()\n            self.m = np.sum(x_mu * adjusted_weights[None, :], axis=1)\n\n            z_mu = np.linalg.solve(self.A, (x_mu - m_old[:, None]))\n            \n            self.ps = (1 - self.cs) * self.ps + (self.cs * (2 - self.cs))**0.5 * (self.m - m_old) / self.sigma\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.eval_count / self.popsize)) / self.chiN < 1.4 + 2/(self.dim+1))\n            self.pc = (1 - self.damps) * self.pc + hsig * (self.damps * (2 - self.damps))**0.5 * (self.m - m_old) / self.sigma\n\n            self.C = (1 - self.c_1 - self.c_mu) * self.C + self.c_1 * (self.pc[:, None] @ self.pc[None, :]) + self.c_mu * (z_mu * adjusted_weights[None, :]) @ z_mu.T\n            \n            self.sigma *= np.exp((np.linalg.norm(self.ps) / self.chiN - 1) * self.cs / self.damps)\n\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            self.C = np.diag(np.diag(self.C)) + 0.9 * (self.C - np.diag(np.diag(self.C)))\n            try:\n                self.A = np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-10 * np.eye(self.dim)  # Add a small constant to the diagonal\n                self.A = np.linalg.cholesky(self.C)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm BiasedCMAES scored 0.349 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["434d72f5-d481-420f-a3cc-17a7c96d8894"], "operator": null, "metadata": {"aucs": [0.18484747877380914, 0.27053066609321663, 0.32008604959829345, 0.29009328574496573, 0.264257499452625, 0.3737680927714093, 0.2655371555446119, 0.3338762982516974, 0.3148839624357944, 0.2243910813250254, 0.35744777507423076, 0.958268577760838, 0.2634752660255836, 0.29708382117497145, 0.6965520921368922, 0.31898473450842213, 0.25501829336236537, 0.3738471314982662, 0.1699269092506751, 0.44509982523811253]}}
{"id": "b4f4758c-12e6-4927-8cd3-2f9d47a2fd5f", "fitness": 0.0, "name": "HybridDE_NM", "description": "Hybrid optimization algorithm combining differential evolution (DE) with a Nelder-Mead simplex for local search, and adaptive parameter control for DE.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridDE_NM:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, Cr=0.9, local_search_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.Cr = Cr\n        self.local_search_prob = local_search_prob\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)].copy() # Important to copy\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Differential Evolution mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n                v = x_r1 + self.F * (x_r2 - x_r3)\n\n                # Crossover\n                u = np.zeros(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.Cr:\n                        u[j] = v[j]\n                    else:\n                        u[j] = population[i, j]\n\n                # Clip to bounds\n                u = np.clip(u, self.lb, self.ub)\n\n                # Local Search (Nelder-Mead)\n                if np.random.rand() < self.local_search_prob:\n                    try:\n                        res = minimize(func, u, method='Nelder-Mead', bounds=list(zip([self.lb]*self.dim, [self.ub]*self.dim)),\n                               options={'maxfev': min(50, self.budget)})  # Limit FE to avoid exceeding budget\n                        \n                        if res.success:\n                            u = res.x\n                            \n                        self.budget -= res.nfev\n                            \n                    except Exception as e:\n                        pass # Handle cases where Nelder-Mead fails\n                else:\n                    f_u = func(u)\n                    self.budget -= 1\n                    if f_u < fitness[i]:\n                      population[i] = u\n                      fitness[i] = f_u\n                      if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u.copy()\n\n                if self.local_search_prob > 0:\n                  if self.budget > 0 and np.random.rand() >= self.local_search_prob:\n                    f_u = func(u)\n                    self.budget -= 1\n                    if f_u < fitness[i]:\n                      population[i] = u\n                      fitness[i] = f_u\n                      if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u.copy()\n\n\n            # Adaptive F and Cr (optional)\n            if np.random.rand() < 0.1:\n                self.F = np.random.normal(self.F, 0.1)\n                self.F = np.clip(self.F, 0.1, 1.0)\n            if np.random.rand() < 0.1:\n                self.Cr = np.random.normal(self.Cr, 0.1)\n                self.Cr = np.clip(self.Cr, 0.1, 1.0)\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm HybridDE_NM scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a01846fa-5dd3-4e1f-b67a-92fa673437e8"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "62035964-71e9-4a70-9fe9-ac9ea0cccc57", "fitness": -Infinity, "name": "EnhancedCMAES", "description": "CMA-ES with dynamic population size, adaptive covariance matrix update, and a more robust handling of the Cholesky decomposition.", "code": "import numpy as np\n\nclass EnhancedCMAES:\n    def __init__(self, budget=10000, dim=10):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = int(4 + 3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.m = np.random.uniform(-5, 5, size=self.dim)\n        self.sigma = 0.5\n        self.C = np.eye(self.dim)\n        self.D = np.ones(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1/(4*self.dim)) + 1/(21 * self.dim**2))\n        self.c_sigma = (self.mu + 2) / (self.dim + self.mu + 5)\n        self.d_sigma = 1 + 2*max(0, np.sqrt((self.mu-1)/(self.dim+1)) - 1) + self.c_sigma\n        self.c_c = (4 + self.mu/self.dim) / (self.dim + 4 + 2*self.mu/self.dim)\n        self.c_1 = 2 / ((self.dim + 1.3)**2 + self.mu)\n        self.c_mu = min(1 - self.c_1, 2 * (self.mu - 2 + 1/self.mu) / ((self.dim + 2)**2 + self.mu))\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.lb = -5\n        self.ub = 5\n        self.min_sigma = 1e-12\n\n    def sample(self):\n        z = np.random.randn(self.dim, self.popsize)\n        x = self.m[:, None] + self.sigma * (self.D[:, None] * z)\n        return x\n\n    def check_bounds(self, x):\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def update_covariance(self, x, x_mu):\n        z_mu = (x_mu - self.m) / self.sigma\n        self.ps = (1 - self.c_sigma) * self.ps + np.sqrt(self.c_sigma * (2 - self.c_sigma)) * (z_mu)\n        norm_ps = np.linalg.norm(self.ps)\n        self.sigma *= np.exp((self.c_sigma / self.d_sigma) * (norm_ps / self.chiN - 1))\n        self.sigma = max(self.sigma, self.min_sigma)\n\n        self.pc = (1 - self.c_c) * self.pc + np.sqrt(self.c_c * (2 - self.c_c)) * (np.sqrt(self.dim) * (x_mu - self.m) / self.sigma)\n\n        delta = (x - self.m[:, None]) / self.sigma\n        rank_one = self.c_1 * self.pc[:, None] @ self.pc[None, :]\n        rank_mu = self.c_mu * (delta * self.weights[None, :]) @ delta.T\n        self.C = (1 - self.c_1 - self.c_mu) * self.C + rank_one + rank_mu\n\n    def __call__(self, func):\n        self.m = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.D = np.ones(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n\n        while self.eval_count < self.budget:\n            x = self.sample()\n            x = self.check_bounds(x)\n\n            f = np.array([func(x[:,i]) if self.eval_count < self.budget else np.inf for i in range(self.popsize)])\n            self.eval_count += self.popsize\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)].copy()\n\n            idx = np.argsort(f)[:self.mu]\n            x_mu = np.mean(x[:, idx], axis=1)\n\n            self.update_covariance(x, x_mu)\n            self.m = x_mu\n\n            try:\n                self.D, B = np.linalg.eigh(self.C)\n                self.D = np.sqrt(np.maximum(self.D, 1e-20))\n            except np.linalg.LinAlgError:\n                self.C += 1e-8 * np.eye(self.dim) # Regularize\n                self.D, B = np.linalg.eigh(self.C)\n                self.D = np.sqrt(np.maximum(self.D, 1e-20))\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "An exception occurred: operands could not be broadcast together with shapes (2,6) (1,3) .", "error": "", "parent_ids": ["decb1e0b-71ca-46a7-a57e-e1c16f0ef187"], "operator": null, "metadata": {}}
{"id": "3fa7f46b-86d9-4d01-9d57-df06a2ac3a42", "fitness": -Infinity, "name": "AdaptiveGaussianArchive", "description": "An adaptive Gaussian mutation algorithm using a combination of rank-one and rank- covariance matrix updates with a separate archive for diversity and exploration.", "code": "import numpy as np\n\nclass AdaptiveGaussianArchive:\n    def __init__(self, budget=10000, dim=10, pop_size_init=20, lr_init=0.1, sigma_init=0.1, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_init = pop_size_init\n        self.lr_init = lr_init\n        self.sigma_init = sigma_init\n        self.lb = -5.0\n        self.ub = 5.0\n        self.archive_size = archive_size\n        self.archive_x = []\n        self.archive_f = []\n\n    def __call__(self, func):\n        # Initialize population\n        pop_size = self.pop_size_init\n        population = np.random.uniform(self.lb, self.ub, size=(pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        # Shrinking search space parameters\n        current_lb = self.lb\n        current_ub = self.ub\n        sigma = self.sigma_init * (self.ub - self.lb)\n        lr = self.lr_init\n\n        # Covariance matrix adaptation (Rank-One and Rank-mu Update)\n        C = np.eye(self.dim)  # Initialize covariance matrix\n        learning_rate_C_rank_one = 0.1\n        learning_rate_C_rank_mu = 0.1\n        mu = pop_size // 4  # Number of individuals for rank-mu update\n\n        while self.budget > 0:\n            # Find best individual\n            best_idx = np.argmin(fitness)\n            best_x = population[best_idx]\n            best_f = fitness[best_idx]\n\n            # Adaptive population size\n            pop_size = min(self.pop_size_init + int(self.budget / 1000), self.budget)\n            if pop_size <= 0:\n               pop_size = 1  # ensure pop size is at least 1\n\n            # Generate new solutions around the best individual using Gaussian distribution with covariance matrix\n            new_population = np.random.multivariate_normal(mean=best_x, cov=sigma**2 * C, size=pop_size)\n\n            # Clip new solutions to the current search space\n            new_population = np.clip(new_population, current_lb, current_ub)\n\n            # Evaluate new solutions\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= pop_size\n\n            # Update best solution\n            best_new_idx = np.argmin(new_fitness)\n            if new_fitness[best_new_idx] < self.f_opt:\n                self.f_opt = new_fitness[best_new_idx]\n                self.x_opt = new_population[best_new_idx]\n\n            # Update population (replace worst with best from new population)\n            worst_idx = np.argmax(fitness)\n            best_new_idx = np.argmin(new_fitness)\n            if new_fitness[best_new_idx] < fitness[worst_idx]:\n                population[worst_idx] = new_population[best_new_idx]\n                fitness[worst_idx] = new_fitness[best_new_idx]\n\n            # Update archive\n            for i in range(pop_size):\n                if len(self.archive_x) < self.archive_size:\n                    self.archive_x.append(new_population[i])\n                    self.archive_f.append(new_fitness[i])\n                else:\n                    # Replace worst in archive\n                    worst_archive_idx = np.argmax(self.archive_f)\n                    if new_fitness[i] < self.archive_f[worst_archive_idx]:\n                        self.archive_x[worst_archive_idx] = new_population[i]\n                        self.archive_f[worst_archive_idx] = new_fitness[i]\n\n            # Diversity injection from archive\n            if len(self.archive_x) > 0:\n                archive_idx = np.random.randint(len(self.archive_x))\n                population[np.random.randint(pop_size)] = self.archive_x[archive_idx]\n                fitness[np.random.randint(pop_size)] = self.archive_f[archive_idx]\n\n            # Shrink search space\n            current_lb = max(self.lb, best_x.min() + lr * (self.lb - best_x.min()))\n            current_ub = min(self.ub, best_x.max() + lr * (self.ub - best_x.max()))\n\n            # Ensure lb < ub\n            if np.any(current_lb >= current_ub):\n                current_lb = np.minimum(current_lb, current_ub - 1e-6)\n                current_ub = np.maximum(current_ub, current_lb + 1e-6)\n\n            # Adapt step size (sigma)\n            if self.f_opt < best_f:\n                sigma *= 0.95\n                lr = min(self.lr_init * 1.1, 0.5)\n            else:\n                sigma *= 1.05\n                lr = max(self.lr_init * 0.9, 0.01)\n\n            # Update covariance matrix (Rank-One Update)\n            diff = population[best_idx] - best_x\n            C = (1 - learning_rate_C_rank_one) * C + learning_rate_C_rank_one * np.outer(diff, diff)\n\n            # Update covariance matrix (Rank-Mu Update)\n            sorted_indices = np.argsort(fitness)\n            diff_mu = population[sorted_indices[:mu]] - best_x\n            C = (1 - learning_rate_C_rank_mu) * C + learning_rate_C_rank_mu * np.mean([np.outer(diff_mu[i], diff_mu[i]) for i in range(mu)], axis=0)\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "An exception occurred: index 22 is out of bounds for axis 0 with size 20.", "error": "", "parent_ids": ["b6310af5-996a-4b3c-bad8-2cf94dc47ae4"], "operator": null, "metadata": {}}
{"id": "6751573c-16e9-4720-b9f0-05846bdaf8f9", "fitness": -Infinity, "name": "EnhancedSimplifiedCMAES", "description": "Enhanced Simplified CMA-ES with adaptive population size, dynamic covariance matrix adaptation, and a more robust step size control mechanism for improved exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedSimplifiedCMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.m = None\n        self.sigma = 0.5\n        self.C = None\n        self.ps = np.zeros(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.cs = 0.3\n        self.cc = 0.3\n        self.damps = 1 + (2*max(0, np.log((self.dim+1)/3))) # damping for sigma\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.chiN = self.dim**0.5*(1-1/(4*self.dim)+1/(21*self.dim**2)) # expectation of ||N(0,I)||\n\n\n    def sample(self):\n        z = np.random.randn(self.dim, self.popsize)\n        x = self.m[:, None] + self.sigma * np.dot(self.A, z)\n        return x\n\n    def check_bounds(self, x):\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        self.m = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.A = np.eye(self.dim)\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.ps = np.zeros(self.dim)\n        self.pc = np.zeros(self.dim)\n        \n        while self.eval_count < self.budget:\n            x = self.sample()\n            x = self.check_bounds(x)\n            \n            f = np.array([func(x[:,i]) if self.eval_count < self.budget else np.inf for i in range(self.popsize)])\n            self.eval_count += self.popsize\n            \n            idx = np.argsort(f)\n            x_mu = x[:, idx[:self.mu]]\n            m_old = self.m.copy()\n            self.m = np.sum(x_mu * self.weights[None, :], axis=1)\n            z_mu = np.linalg.solve(self.A, (x_mu - m_old[:, None]))\n            \n            # Covariance matrix adaptation\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs)) * np.sum(z_mu * self.weights[None, :], axis=1)\n            norm_ps = np.linalg.norm(self.ps)\n            self.pc = (1 - self.cc) * self.pc + np.sqrt(self.cc * (2 - self.cc)) * np.linalg.solve(self.A, (self.m - m_old))\n            \n            delta_h_sigma = (1-self.cc) * self.cc * (2-self.cc) < 0.25\n            \n            self.C = (1-1) * self.C + (1/np.sum(self.weights**2)) * self.pc[:, None] @ self.pc[None, :] + (1- (1-1)) * (z_mu * self.weights[None, :]) @ z_mu.T\n            \n            self.sigma *= np.exp((self.cs/self.damps)*(norm_ps/self.chiN -1))\n            \n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            try:\n                self.A = np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-10 * np.eye(self.dim)\n                self.A = np.linalg.cholesky(self.C)\n            \n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)].copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "An exception occurred: Matrix is not positive definite.", "error": "", "parent_ids": ["decb1e0b-71ca-46a7-a57e-e1c16f0ef187"], "operator": null, "metadata": {}}
{"id": "9caa8409-3460-4c07-b332-4a43983cb763", "fitness": 0.37558024275566904, "name": "AdaptiveGaussianAgingSimple", "description": "Adaptive Gaussian mutation with simplified covariance matrix adaptation using a moving average, adaptive step size, and aging.", "code": "import numpy as np\n\nclass AdaptiveGaussianAgingSimple:\n    def __init__(self, budget=10000, dim=10, pop_size_init=20, sigma_init=0.1, aging_rate=0.05, learning_rate_C=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_init = pop_size_init\n        self.sigma_init = sigma_init\n        self.lb = -5.0\n        self.ub = 5.0\n        self.aging_rate = aging_rate\n        self.ages = None\n        self.learning_rate_C = learning_rate_C\n\n    def __call__(self, func):\n        pop_size = self.pop_size_init\n        population = np.random.uniform(self.lb, self.ub, size=(pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= pop_size\n        self.ages = np.zeros(pop_size)\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        sigma = self.sigma_init * (self.ub - self.lb)\n        C = np.eye(self.dim)\n\n        while self.budget > 0:\n            best_idx = np.argmin(fitness)\n            best_x = population[best_idx]\n\n            pop_size = min(self.pop_size_init + int(self.budget / 1000), self.budget)\n            if pop_size <= 0:\n               pop_size = 1\n\n            new_population = np.random.multivariate_normal(mean=best_x, cov=sigma**2 * C, size=pop_size)\n            new_population = np.clip(new_population, self.lb, self.ub)\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= pop_size\n\n            best_new_idx = np.argmin(new_fitness)\n            if new_fitness[best_new_idx] < self.f_opt:\n                self.f_opt = new_fitness[best_new_idx]\n                self.x_opt = new_population[best_new_idx]\n\n            oldest_idx = np.argmax(self.ages)\n            if new_fitness[best_new_idx] < fitness[oldest_idx]:\n                population[oldest_idx] = new_population[best_new_idx]\n                fitness[oldest_idx] = new_fitness[best_new_idx]\n                self.ages[oldest_idx] = 0\n\n            self.ages += 1\n\n            # Simplified CMA-ES-like update: Moving average instead of rank-one\n            diff = population[best_idx] - best_x\n            C = (1 - self.learning_rate_C) * C + self.learning_rate_C * np.outer(diff, diff)\n\n            # Adaptive step size (simplified)\n            if np.min(new_fitness) < np.min(fitness):\n                sigma *= 0.95\n            else:\n                sigma *= 1.05\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveGaussianAgingSimple scored 0.376 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b6310af5-996a-4b3c-bad8-2cf94dc47ae4"], "operator": null, "metadata": {"aucs": [0.14422785365850654, 0.2998161068899219, 0.3643653973415647, 0.41465653771332545, 0.28303913957607774, 0.35883619888188545, 0.28524208454979016, 0.30948534543510864, 0.28515634625982544, 0.17866738427209472, 0.4149916575120943, 0.9982819274621539, 0.2855591039992854, 0.28795214512080025, 0.7351758888176257, 0.3562899902450105, 0.30710337278739264, 0.440473526957479, 0.2644853835592952, 0.497799464074143]}}
{"id": "90ec2908-fddf-4eaa-aef7-995ab0c1e537", "fitness": 0.5320400395714826, "name": "AdaptiveGaussianAdaptationSimple", "description": "Simplified adaptive Gaussian mutation with dynamic scale and covariance matrix adaptation using rank-one update, along with search space adaptation and stagnation-based restarts.", "code": "import numpy as np\n\nclass AdaptiveGaussianAdaptationSimple:\n    def __init__(self, budget=10000, dim=10, pop_size=20, lr=0.1, initial_scale=0.5, stagnation_threshold=500):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lr = lr\n        self.lb = -5.0\n        self.ub = 5.0\n        self.scale = initial_scale\n        self.scale_decay = 0.995\n        self.min_scale = 0.01\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.covariance = np.eye(dim)\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def __call__(self, func):\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_idx = np.argmin(fitness)\n        self.x_opt = population[best_idx]\n        self.f_opt = fitness[best_idx]\n\n        current_lb = self.lb\n        current_ub = self.ub\n\n        while self.budget > 0:\n            # Generate new solutions\n            new_population = np.random.multivariate_normal(mean=self.x_opt, cov=self.scale**2 * self.covariance, size=self.pop_size)\n            new_population = np.clip(new_population, current_lb, current_ub)\n\n            # Evaluate new solutions\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n\n            # Update best solution\n            best_new_idx = np.argmin(new_fitness)\n            if new_fitness[best_new_idx] < self.f_opt:\n                self.f_opt = new_fitness[best_new_idx]\n                self.x_opt = new_population[best_new_idx]\n                self.stagnation_counter = 0\n            else:\n                self.stagnation_counter += 1\n\n            # Update population (replace worst with best from new population if better)\n            worst_idx = np.argmax(fitness)\n            if new_fitness[best_new_idx] < fitness[worst_idx]:\n                population[worst_idx] = new_population[best_new_idx]\n                fitness[worst_idx] = new_fitness[best_new_idx]\n\n            # Adapt search space (simplified)\n            current_lb = max(self.lb, self.x_opt.min() - self.lr * (self.ub - self.lb))\n            current_ub = min(self.ub, self.x_opt.max() + self.lr * (self.ub - self.lb))\n            current_lb = np.minimum(current_lb, current_ub - 1e-6)\n            current_ub = np.maximum(current_ub, current_lb + 1e-6)\n            \n            # Decay scale\n            self.scale = max(self.scale * self.scale_decay, self.min_scale)\n\n            # Adapt covariance matrix (Rank-1 update)\n            z = self.x_opt - np.mean(population, axis=0)\n            self.covariance = (1 - self.lr) * self.covariance + self.lr * np.outer(z, z) + 1e-9 * np.eye(self.dim)\n\n            # Stagnation detection and restart\n            if self.stagnation_counter > self.stagnation_threshold:\n                population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n\n                best_idx = np.argmin(fitness)\n                self.x_opt = population[best_idx]\n                self.f_opt = fitness[best_idx]\n                self.stagnation_counter = 0\n                self.covariance = np.eye(self.dim)\n                self.scale = initial_scale\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveGaussianAdaptationSimple scored 0.532 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0fe38f9a-74f4-44f9-a0e6-5b8bce531f58"], "operator": null, "metadata": {"aucs": [0.14961715169373635, 0.22533737839791224, 0.7821105893433298, 0.9276587321077808, 0.30503472250220653, 0.8504697971247097, 0.3416156147481929, 0.6830117741515198, 0.21526987486137394, 0.19191527656307894, 0.9257942011746292, 0.9951475581767603, 0.3288261258797758, 0.334461984742934, 0.9420710434387682, 0.4763708390442417, 0.4208553267931434, 0.894001175326064, 0.19067281993555985, 0.460558805423935]}}
{"id": "51a19267-5814-440a-8974-431d3146e00a", "fitness": 0.45459726575779164, "name": "AdaptiveLocalGaussian", "description": "An adaptive Gaussian mutation algorithm that uses a decaying local search probability and a dynamic population size adjustment based on the success rate of local search steps.", "code": "import numpy as np\n\nclass AdaptiveLocalGaussian:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=20, lr=0.1, initial_scale=0.5, success_threshold=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.lr = lr\n        self.lb = -5.0\n        self.ub = 5.0\n        self.scale = initial_scale\n        self.scale_decay = 0.995\n        self.min_scale = 0.01\n        self.success_threshold = success_threshold\n        self.success_rate = 0.0\n        self.success_counter = 0\n        self.total_iterations = 0\n        self.local_search_prob = 0.9  # Probability of performing local search\n        self.local_search_decay = 0.99  # Decay factor for local search probability\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_idx = np.argmin(fitness)\n        self.f_opt = fitness[best_idx]\n        self.x_opt = population[best_idx]\n\n        while self.budget > 0:\n            # Iterate through the population and perform local search with a certain probability\n            for i in range(self.pop_size):\n                if np.random.rand() < self.local_search_prob:\n                    # Generate a new solution around the current individual using Gaussian distribution\n                    new_solution = np.random.normal(population[i], self.scale, size=self.dim)\n                    new_solution = np.clip(new_solution, self.lb, self.ub)\n\n                    # Evaluate the new solution\n                    if self.budget > 0:\n                        new_fitness = func(new_solution)\n                        self.budget -= 1\n                        self.total_iterations += 1\n\n                        # Update best solution\n                        if new_fitness < self.f_opt:\n                            self.f_opt = new_fitness\n                            self.x_opt = new_solution\n                            self.success_counter += 1\n\n                        # Replace the current individual if the new solution is better\n                        if new_fitness < fitness[i]:\n                            population[i] = new_solution\n                            fitness[i] = new_fitness\n                else:\n                  # Global search step: random perturbation\n                  new_solution = population[i] + np.random.uniform(-self.scale, self.scale, size=self.dim)\n                  new_solution = np.clip(new_solution, self.lb, self.ub)\n\n                  # Evaluate the new solution\n                  if self.budget > 0:\n                      new_fitness = func(new_solution)\n                      self.budget -= 1\n                      self.total_iterations += 1\n\n                      # Update best solution\n                      if new_fitness < self.f_opt:\n                          self.f_opt = new_fitness\n                          self.x_opt = new_solution\n                          self.success_counter += 1\n\n                      # Replace the current individual if the new solution is better\n                      if new_fitness < fitness[i]:\n                          population[i] = new_solution\n                          fitness[i] = new_fitness\n\n            # Update success rate\n            if self.total_iterations > 0:\n                self.success_rate = self.success_counter / self.total_iterations\n\n            # Adjust population size based on success rate\n            if self.success_rate > self.success_threshold and self.pop_size < 2 * self.dim:\n                self.pop_size += 1\n                new_individual = np.random.uniform(self.lb, self.ub, size=self.dim)\n                new_fitness = func(new_individual)\n                self.budget -= 1\n                population = np.vstack((population, new_individual))\n                fitness = np.append(fitness, new_fitness)\n            elif self.success_rate < self.success_threshold / 2 and self.pop_size > 10:\n                self.pop_size -= 1\n                worst_idx = np.argmax(fitness)\n                population = np.delete(population, worst_idx, axis=0)\n                fitness = np.delete(fitness, worst_idx)\n\n            # Decay the scale\n            self.scale = max(self.scale * self.scale_decay, self.min_scale)\n\n            # Decay local search probability\n            self.local_search_prob *= self.local_search_decay\n            self.local_search_prob = max(self.local_search_prob, 0.1) # Ensure a minimum probability\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveLocalGaussian scored 0.455 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0fe38f9a-74f4-44f9-a0e6-5b8bce531f58"], "operator": null, "metadata": {"aucs": [0.10501494887285101, 0.24646999706571482, 0.4776046783441159, 0.734168127545625, 0.4186790155240825, 0.5163887655514365, 0.30591850010070987, 0.4102140732714702, 0.40958122118216056, 0.19498648843703126, 0.754142659752976, 0.9892861882632954, 0.2609845738018104, 0.279550825233457, 0.8757200577063662, 0.39904986217447724, 0.376040633799954, 0.6392993962360649, 0.19957369150477955, 0.4992716107874531]}}
{"id": "31816bcd-af46-4bd8-9913-d3b33c9cc12e", "fitness": 0.1612937870592193, "name": "HybridDELevyOrthogonal", "description": "Hybrid Differential Evolution with Lvy flight mutation, orthogonal crossover, and adaptive parameter control based on success history.", "code": "import numpy as np\n\nclass HybridDELevyOrthogonal:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, Cr=0.9, levy_exponent=1.5, lr=0.1, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.Cr = Cr\n        self.levy_exponent = levy_exponent\n        self.lr = lr\n        self.archive_size = archive_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.archive = []\n        self.archive_fitness = []\n\n        # Adaptive F and Cr: Success history\n        self.sf = []\n        self.scr = []\n        self.msf = self.F\n        self.mcr = self.Cr\n\n    def levy_flight(self, beta):\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) / (np.math.gamma((1 + beta) / 2) * beta * (2 ** ((beta - 1) / 2)))) ** (1 / beta)\n        u = np.random.normal(0, sigma, size=self.dim)\n        v = np.random.normal(0, 1, size=self.dim)\n        step = u / (abs(v) ** (1 / beta))\n        return step\n\n    def orthogonal_crossover(self, x, v):\n        # Create an orthogonal array (simplified, for demonstration)\n        H = np.array([[1, 1], [1, -1]])\n        u = np.copy(x)\n        for j in range(self.dim):\n            idx = j % 2\n            if H[0, idx] == 1:\n                u[j] = x[j]\n            else:\n                u[j] = v[j]\n        return u\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        # Bounds\n        current_lb = self.lb\n        current_ub = self.ub\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Differential Evolution mutation with Levy flight\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n                levy_step = self.levy_flight(self.levy_exponent)\n                v = x_r1 + self.msf * (x_r2 - x_r3) + 0.01 * levy_step  # Added Levy flight\n\n                # Orthogonal crossover\n                u = self.orthogonal_crossover(population[i], v)\n\n                # Clip to bounds\n                u = np.clip(u, current_lb, current_ub)\n\n                # Evaluate trial vector\n                f_u = func(u)\n                self.budget -= 1\n\n                if f_u < fitness[i]:\n                    self.sf.append(self.msf)\n                    self.scr.append(self.mcr)\n\n                    population[i] = u\n                    fitness[i] = f_u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n                        \n                else:\n                    # Add to archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i].copy())\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                        # Replace a random element in the archive\n                        rand_idx = np.random.randint(self.archive_size)\n                        self.archive[rand_idx] = population[i].copy()\n                        self.archive_fitness[rand_idx] = fitness[i]\n                        \n            # Bounds adaptation\n            best_x = population[np.argmin(fitness)]\n            current_lb = max(self.lb, best_x.min() + self.lr * (self.lb - best_x.min()))\n            current_ub = min(self.ub, best_x.max() + self.lr * (self.ub - best_x.max()))\n\n            if np.any(current_lb >= current_ub):\n                current_lb = np.minimum(current_lb, current_ub - 1e-6)\n                current_ub = np.maximum(current_ub, current_lb + 1e-6)\n\n            # Adaptive F and Cr update (using success history)\n            if self.sf:\n                self.msf = np.mean(self.sf)\n                self.mcr = np.mean(self.scr)\n\n                self.F = np.clip(np.random.normal(self.msf, 0.1), 0.1, 1.0)\n                self.Cr = np.clip(np.random.normal(self.mcr, 0.1), 0.1, 1.0)\n\n                self.sf = []\n                self.scr = []\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm HybridDELevyOrthogonal scored 0.161 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a01846fa-5dd3-4e1f-b67a-92fa673437e8"], "operator": null, "metadata": {"aucs": [0.05578281875546598, 0.09406121840642023, 0.22965849774985536, 0.13602627427044134, 0.13656773501997954, 0.15556151552742736, 0.18603141458057793, 0.1496244338605749, 0.1783595972672214, 0.10608644378060272, 0.15827396019810314, 0.16851836165402378, 0.05863553948488587, 0.15087785906632267, 0.15327413033887238, 0.2204755705945396, 0.17823837254170938, 0.16286811437483373, 0.1448210923320592, 0.402132791380469]}}
{"id": "382a6540-ec16-412d-b71f-2d8b9262d4ac", "fitness": 0.399957196871091, "name": "AdaptiveDifferentialEvolutionCMA", "description": "Simplified adaptive differential evolution with covariance matrix adaptation, focusing on parameter control and population diversity through mutation enhancements.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolutionCMA:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, Cr=0.9, sigma_init=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.Cr = Cr\n        self.sigma_init = sigma_init\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        sigma = self.sigma_init * (self.ub - self.lb)\n        C = np.eye(self.dim)\n        learning_rate_C = 0.1\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n\n                # Enhanced mutation with CMA influence\n                v = x_r1 + self.F * (x_r2 - x_r3) + sigma * np.random.multivariate_normal(np.zeros(self.dim), C)\n                u = np.zeros(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.Cr:\n                        u[j] = v[j]\n                    else:\n                        u[j] = population[i, j]\n\n                u = np.clip(u, self.lb, self.ub)\n                \n                f_u = func(u)\n                self.budget -= 1\n                \n                if f_u < fitness[i]:\n                    population[i] = u\n                    fitness[i] = f_u\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n\n            # CMA update\n            diff = population[np.argmin(fitness)] - self.x_opt\n            C = (1 - learning_rate_C) * C + learning_rate_C * np.outer(diff, diff)\n\n            # Adapt step size\n            if self.f_opt > np.min(fitness):\n                sigma *= 0.95\n            else:\n                sigma *= 1.05\n\n            # Parameter adaptation (simplified)\n            if np.random.rand() < 0.1:\n                self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n                self.Cr = np.clip(np.random.normal(self.Cr, 0.1), 0.1, 1.0)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDifferentialEvolutionCMA scored 0.400 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a01846fa-5dd3-4e1f-b67a-92fa673437e8"], "operator": null, "metadata": {"aucs": [0.17988968956423157, 0.3283514544791851, 0.3837357351279925, 0.5461605750232535, 0.288809195232974, 0.41122677098404203, 0.3123364826186199, 0.32884944477003086, 0.3035637172753721, 0.28090684977375935, 0.428426869162822, 0.997165478130368, 0.2614982186318614, 0.3054013841727068, 0.7515523254132934, 0.3784065570788935, 0.33120626464313785, 0.46075967671655105, 0.22455030458007308, 0.49634694404265334]}}
{"id": "201b5f2d-d492-4620-8e7e-3abcc024ec24", "fitness": 0.2594100928691493, "name": "SimplifiedCMAES", "description": "Simplified CMA-ES with adaptive covariance update and sigma adaptation based on cumulative success rate, using rank-one update and a damping factor for sigma.", "code": "import numpy as np\n\nclass SimplifiedCMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damp=0.8):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.m = None\n        self.sigma = 0.5\n        self.C = None\n        self.pc = np.zeros(self.dim)\n        self.cs = cs\n        self.damp = damp\n        self.success_rate = 0.5\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n\n\n    def sample(self):\n        z = np.random.randn(self.dim, self.popsize)\n        x = self.m[:, None] + self.sigma * z\n        return x\n\n    def check_bounds(self, x):\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def __call__(self, func):\n        self.m = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        \n        while self.eval_count < self.budget:\n            x = self.sample()\n            x = self.check_bounds(x)\n            \n            f = np.array([func(x[:,i]) if self.eval_count < self.budget else np.inf for i in range(self.popsize)])\n            self.eval_count += self.popsize\n            \n            idx = np.argsort(f)\n            x_mu = x[:, idx[:self.mu]]\n            m_old = self.m.copy()\n            self.m = np.sum(x_mu * self.weights[None, :], axis=1)\n\n            z_mu = (x_mu - m_old[:, None]) / self.sigma\n            \n            self.pc = (1 - self.cs) * self.pc + np.sqrt(self.cs * (2 - self.cs)) * np.sum(z_mu * self.weights[None, :], axis=1)\n            self.C = (1 - self.cs) * self.C + self.cs * np.outer(self.pc, self.pc)\n            \n            self.sigma *= np.exp((self.cs/self.damp)*(np.linalg.norm(self.pc)/np.sqrt(self.dim) - 1))\n\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            self.C = np.diag(np.diag(self.C)) + 0.9 * (self.C - np.diag(np.diag(self.C)))\n            \n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-10 * np.eye(self.dim)\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)].copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm SimplifiedCMAES scored 0.259 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["decb1e0b-71ca-46a7-a57e-e1c16f0ef187"], "operator": null, "metadata": {"aucs": [0.04531667353787128, 0.1933064407224352, 0.9540539697698209, 0.18219605530839944, 0.17824268995090098, 0.13922727506771093, 0.26432728364219427, 0.4015142749752214, 0.1647590128753449, 0.11417144011496128, 0.2767308829385865, 0.16144820815000216, 0.2100541202155033, 0.20607761902442934, 0.5329547609606428, 0.25951307838925974, 0.21529188511757624, 0.1691856280524373, 0.10171495169676348, 0.4181156068729246]}}
{"id": "2b89b99f-0cd4-49c1-9d7d-782fa64b1c68", "fitness": 0.5774646382599127, "name": "AdaptiveGaussianAdaptation", "description": "Adaptive Gaussian mutation with simplified covariance matrix adaptation, dynamic scaling, and a restart mechanism based on stagnation.", "code": "import numpy as np\n\nclass AdaptiveGaussianAdaptation:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_scale=0.5, stagnation_threshold=500):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.scale = initial_scale  # Initial scale for Gaussian distribution\n        self.scale_decay = 0.995  # Decay factor for the scale\n        self.min_scale = 0.01  # Minimum scale value\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.covariance = np.eye(dim)  # Initialize covariance matrix\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            # Find best individual\n            best_idx = np.argmin(fitness)\n            best_x = population[best_idx]\n\n            # Generate new solutions around the best individual using Gaussian distribution\n            new_population = np.random.multivariate_normal(mean=best_x, cov=self.scale**2 * self.covariance, size=self.pop_size)\n\n            # Clip new solutions to the search space\n            new_population = np.clip(new_population, self.lb, self.ub)\n\n            # Evaluate new solutions\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n\n            # Update best solution\n            best_new_idx = np.argmin(new_fitness)\n            if new_fitness[best_new_idx] < self.f_opt:\n                self.f_opt = new_fitness[best_new_idx]\n                self.x_opt = new_population[best_new_idx]\n                self.stagnation_counter = 0  # Reset stagnation counter\n            else:\n                self.stagnation_counter += 1\n\n            # Update population (replace worst with best from new population)\n            worst_idx = np.argmax(fitness)\n            if new_fitness[best_new_idx] < fitness[worst_idx]:\n                population[worst_idx] = new_population[best_new_idx]\n                fitness[worst_idx] = new_fitness[best_new_idx]\n\n            # Decay the scale\n            self.scale = max(self.scale * self.scale_decay, self.min_scale)\n\n            # Update covariance matrix (simplified adaptation)\n            diff = population - np.mean(population, axis=0)\n            self.covariance = 0.9 * self.covariance + 0.1 * np.cov(diff.T) + 1e-9 * np.eye(self.dim) # Moving average and add small value\n\n            # Stagnation detection and restart\n            if self.stagnation_counter > self.stagnation_threshold:\n                # Reset population\n                population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.covariance = np.eye(self.dim)\n                self.scale = self.initial_scale\n                self.stagnation_counter = 0\n                if np.min(fitness) < self.f_opt:\n                  self.f_opt = np.min(fitness)\n                  self.x_opt = population[np.argmin(fitness)]\n                self.budget -= self.pop_size\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveGaussianAdaptation scored 0.577 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0fe38f9a-74f4-44f9-a0e6-5b8bce531f58"], "operator": null, "metadata": {"aucs": [0.14992273336404616, 0.17191410397198215, 0.7617208592295477, 0.9195028269026503, 0.8042641841516402, 0.8260830383760722, 0.3275756974399504, 0.6571991367623213, 0.7980220850679307, 0.20829689409708496, 0.897089160180419, 0.995990462544147, 0.31437397228676023, 0.7962358009337291, 0.5892155055920252, 0.382998642257773, 0.3395641003166182, 0.8775182786639049, 0.23346064066673666, 0.4983446423929154]}}
{"id": "7a543c9d-c17b-4649-88f5-1baae7283cd3", "fitness": -Infinity, "name": "SOMDifferentialEvolution", "description": "Combines Differential Evolution with a Self-Organizing Map (SOM) to adaptively explore the search space and refine promising regions.", "code": "import numpy as np\n\nclass SOMDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, Cr=0.9, som_grid_size=5, som_learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Differential weight\n        self.Cr = Cr  # Crossover probability\n        self.lb = -5.0\n        self.ub = 5.0\n        self.som_grid_size = som_grid_size # Size of the SOM grid (som_grid_size x som_grid_size)\n        self.som_learning_rate = som_learning_rate\n        self.som_sigma = som_grid_size / 3.0 # Initial neighborhood radius\n\n        # Initialize SOM\n        self.som = np.random.uniform(self.lb, self.ub, size=(self.som_grid_size, self.som_grid_size, self.dim))\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n        \n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Differential Evolution mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n                \n                # Mutation and crossover\n                v = x_r1 + self.F * (x_r2 - x_r3)\n                u = np.zeros(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.Cr:\n                        u[j] = v[j]\n                    else:\n                        u[j] = population[i, j]\n                \n                # Clip to bounds\n                u = np.clip(u, self.lb, self.ub)\n                \n                # Evaluate trial vector\n                f_u = func(u)\n                self.budget -= 1\n                \n                if f_u < fitness[i]:\n                    population[i] = u\n                    fitness[i] = f_u\n                    \n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n                \n                # SOM Update: Find best matching unit (BMU)\n                bmu_row, bmu_col = self.find_bmu(population[i])\n                \n                # Update SOM weights around the BMU\n                for row in range(self.som_grid_size):\n                    for col in range(self.som_grid_size):\n                        distance = np.sqrt((row - bmu_row)**2 + (col - bmu_col)**2)\n                        influence = np.exp(-distance**2 / (2 * self.som_sigma**2))  # Gaussian neighborhood function\n                        \n                        self.som[row, col] += self.som_learning_rate * influence * (population[i] - self.som[row, col])\n            \n            #Adapt SOM parameters\n            self.som_learning_rate *= 0.99 # Decay learning rate\n            self.som_sigma *= 0.99 # Decay neighborhood radius\n\n            #Adaptive F and Cr\n            if np.random.rand() < 0.1:\n                self.F = np.random.normal(self.F, 0.1)\n                self.F = np.clip(self.F, 0.1, 1.0)\n            if np.random.rand() < 0.1:\n                self.Cr = np.random.normal(self.Cr, 0.1)\n                self.Cr = np.clip(self.Cr, 0.1, 1.0)\n\n\n        return self.f_opt, self.x_opt\n\n    def find_bmu(self, vector):\n        \"\"\"Find the best matching unit (BMU) in the SOM for a given vector.\"\"\"\n        min_dist = np.inf\n        bmu_row, bmu_col = -1, -1\n        \n        for row in range(self.som_grid_size):\n            for col in range(self.som_grid_size):\n                dist = np.linalg.norm(vector - self.som[row, col])\n                if dist < min_dist:\n                    min_dist = dist\n                    bmu_row, bmu_col = row, col\n        \n        return bmu_row, bmu_col", "configspace": "", "generation": 3, "feedback": "An exception occurred: Evaluation timed out after 60 seconds..", "error": "", "parent_ids": ["a01846fa-5dd3-4e1f-b67a-92fa673437e8"], "operator": null, "metadata": {}}
{"id": "d3307919-30a8-423c-99b5-e2f9190015f3", "fitness": -Infinity, "name": "AdaptiveGaussianAdaptationOrtho", "description": "Adaptive Gaussian mutation with dynamic scaling, covariance matrix adaptation using rank-one updates, orthogonal sampling for population diversity, and a restart mechanism based on stagnation detection.", "code": "import numpy as np\n\nclass AdaptiveGaussianAdaptationOrtho:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_scale=0.5, stagnation_threshold=500, ortho_group_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.scale = initial_scale  # Initial scale for Gaussian distribution\n        self.scale_decay = 0.995  # Decay factor for the scale\n        self.min_scale = 0.01  # Minimum scale value\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.covariance = np.eye(dim)  # Initialize covariance matrix\n        self.ortho_group_size = ortho_group_size\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            # Find best individual\n            best_idx = np.argmin(fitness)\n            best_x = population[best_idx]\n\n            # Generate new solutions around the best individual using Gaussian distribution\n            new_population = np.random.multivariate_normal(mean=best_x, cov=self.scale**2 * self.covariance, size=self.pop_size)\n\n            # Clip new solutions to the search space\n            new_population = np.clip(new_population, self.lb, self.ub)\n\n            # Apply orthogonal sampling\n            for i in range(0, self.pop_size, self.ortho_group_size):\n                group_size = min(self.ortho_group_size, self.pop_size - i)\n                if group_size > 1:  # Orthogonal sampling needs at least 2 points\n                    group = new_population[i:i+group_size]\n                    Q, _ = np.linalg.qr(group.T)\n                    orthogonal_points = np.dot(Q.T, np.random.randn(group_size, self.dim))\n                    new_population[i:i+group_size] = np.clip(best_x + self.scale * orthogonal_points, self.lb, self.ub)\n\n\n            # Evaluate new solutions\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n\n            # Update best solution\n            best_new_idx = np.argmin(new_fitness)\n            if new_fitness[best_new_idx] < self.f_opt:\n                self.f_opt = new_fitness[best_new_idx]\n                self.x_opt = new_population[best_new_idx]\n                self.stagnation_counter = 0  # Reset stagnation counter\n            else:\n                self.stagnation_counter += 1\n\n            # Update population (replace worst with best from new population)\n            worst_idx = np.argmax(fitness)\n            if new_fitness[best_new_idx] < fitness[worst_idx]:\n                population[worst_idx] = new_population[best_new_idx]\n                fitness[worst_idx] = new_fitness[best_new_idx]\n\n            # Decay the scale\n            self.scale = max(self.scale * self.scale_decay, self.min_scale)\n\n            # Update covariance matrix (simplified adaptation using rank-one update)\n            diff = population - best_x\n            self.covariance = (1 - 0.1) * self.covariance + 0.1 * (diff.T @ diff) / self.pop_size + 1e-9 * np.eye(self.dim) # Rank-one update\n\n            # Stagnation detection and restart\n            if self.stagnation_counter > self.stagnation_threshold:\n                # Reset population\n                population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.covariance = np.eye(self.dim)\n                self.scale = self.initial_scale\n                self.stagnation_counter = 0\n                if np.min(fitness) < self.f_opt:\n                  self.f_opt = np.min(fitness)\n                  self.x_opt = population[np.argmin(fitness)]\n                self.budget -= self.pop_size\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "An exception occurred: shapes (2,2) and (5,2) not aligned: 2 (dim 1) != 5 (dim 0).", "error": "", "parent_ids": ["2b89b99f-0cd4-49c1-9d7d-782fa64b1c68"], "operator": null, "metadata": {}}
{"id": "bab77af5-d2b8-4fe8-a6e8-1fe935b82038", "fitness": 0.0, "name": "AdaptiveGaussianOrthogonalLearning", "description": "An adaptive Gaussian mutation algorithm with orthogonal learning and a simplified aging mechanism to promote exploration and exploitation balance.", "code": "import numpy as np\n\nclass AdaptiveGaussianOrthogonalLearning:\n    def __init__(self, budget=10000, dim=10, pop_size=20, lr=0.1, initial_scale=0.5, stagnation_threshold=500, aging_rate=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lr = lr\n        self.lb = -5.0\n        self.ub = 5.0\n        self.scale = initial_scale\n        self.scale_decay = 0.995\n        self.min_scale = 0.01\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.covariance = np.eye(dim)\n        self.x_opt = None\n        self.f_opt = np.inf\n        self.aging_rate = aging_rate\n        self.ages = np.zeros(pop_size)\n\n    def __call__(self, func):\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_idx = np.argmin(fitness)\n        self.x_opt = population[best_idx]\n        self.f_opt = fitness[best_idx]\n\n        current_lb = self.lb\n        current_ub = self.ub\n\n        while self.budget > 0:\n            # Generate new solutions using Gaussian mutation\n            new_population = np.random.multivariate_normal(mean=self.x_opt, cov=self.scale**2 * self.covariance, size=self.pop_size)\n            new_population = np.clip(new_population, current_lb, current_ub)\n\n            # Orthogonal Learning: Generate alternative solutions based on the best solution\n            orthogonal_population = np.zeros_like(new_population)\n            for i in range(self.pop_size):\n                direction = np.random.normal(size=self.dim)\n                direction /= np.linalg.norm(direction) # Normalize\n                orthogonal_point = self.x_opt + self.scale * direction\n                orthogonal_population[i] = np.clip(orthogonal_point, current_lb, current_ub)\n            \n            # Evaluate new solutions\n            new_fitness = np.array([func(x) for x in new_population])\n            orthogonal_fitness = np.array([func(x) for x in orthogonal_population])\n            self.budget -= 2 * self.pop_size #Orthogonal population also costs budget.\n\n            # Update best solution\n            best_new_idx = np.argmin(new_fitness)\n            best_orthogonal_idx = np.argmin(orthogonal_fitness)\n\n            if new_fitness[best_new_idx] < self.f_opt:\n                self.f_opt = new_fitness[best_new_idx]\n                self.x_opt = new_population[best_new_idx]\n                self.stagnation_counter = 0\n            elif orthogonal_fitness[best_orthogonal_idx] < self.f_opt:\n                self.f_opt = orthogonal_fitness[best_orthogonal_idx]\n                self.x_opt = orthogonal_population[best_orthogonal_idx]\n                self.stagnation_counter = 0\n            else:\n                self.stagnation_counter += 1\n\n            # Update population using replacement and aging mechanism\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    population[i] = new_population[i]\n                    fitness[i] = new_fitness[i]\n                    self.ages[i] = 0  # Reset age\n                elif orthogonal_fitness[i] < fitness[i]:\n                    population[i] = orthogonal_population[i]\n                    fitness[i] = orthogonal_fitness[i]\n                    self.ages[i] = 0  # Reset age\n                else:\n                    self.ages[i] += self.aging_rate # Increase age\n\n            # Remove the oldest individual\n            oldest_idx = np.argmax(self.ages)\n            \n            # Replace the oldest individual with a random individual near the best\n            population[oldest_idx] = np.random.multivariate_normal(mean=self.x_opt, cov=self.scale**2 * self.covariance, size=1).flatten()\n            population[oldest_idx] = np.clip(population[oldest_idx], current_lb, current_ub)\n            fitness[oldest_idx] = func(population[oldest_idx])\n            self.budget -= 1\n            self.ages[oldest_idx] = 0\n\n            # Adapt search space (simplified)\n            current_lb = max(self.lb, self.x_opt.min() - self.lr * (self.ub - self.lb))\n            current_ub = min(self.ub, self.x_opt.max() + self.lr * (self.ub - self.lb))\n            current_lb = np.minimum(current_lb, current_ub - 1e-6)\n            current_ub = np.maximum(current_ub, current_lb + 1e-6)\n            \n            # Decay scale\n            self.scale = max(self.scale * self.scale_decay, self.min_scale)\n\n            # Adapt covariance matrix (Rank-1 update)\n            z = self.x_opt - np.mean(population, axis=0)\n            self.covariance = (1 - self.lr) * self.covariance + self.lr * np.outer(z, z) + 1e-9 * np.eye(self.dim)\n\n            # Stagnation detection and restart\n            if self.stagnation_counter > self.stagnation_threshold:\n                population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n\n                best_idx = np.argmin(fitness)\n                self.x_opt = population[best_idx]\n                self.f_opt = fitness[best_idx]\n                self.stagnation_counter = 0\n                self.covariance = np.eye(self.dim)\n                self.scale = initial_scale\n                self.ages = np.zeros(self.pop_size)\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveGaussianOrthogonalLearning scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["90ec2908-fddf-4eaa-aef7-995ab0c1e537"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "5b91fe3a-0a01-455f-81a8-db80cf77293e", "fitness": 0.41326320345002204, "name": "AdaptiveGaussianAdaptation", "description": "Adaptive Gaussian mutation with dynamic scaling, simplified covariance matrix adaptation using rank-one update, and periodic restarts for exploration.", "code": "import numpy as np\n\nclass AdaptiveGaussianAdaptation:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_scale=0.5, restart_interval=500):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.scale = initial_scale\n        self.scale_decay = 0.995\n        self.min_scale = 0.01\n        self.restart_interval = restart_interval\n        self.restart_counter = 0\n        self.covariance = np.eye(dim)\n        self.mean = np.zeros(dim) # Mean for CMA-ES\n\n    def __call__(self, func):\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            best_idx = np.argmin(fitness)\n            best_x = population[best_idx]\n\n            # Generate new solutions\n            new_population = np.random.multivariate_normal(mean=self.mean, cov=self.scale**2 * self.covariance, size=self.pop_size)\n            new_population = np.clip(new_population, self.lb, self.ub)\n\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n\n            best_new_idx = np.argmin(new_fitness)\n            if new_fitness[best_new_idx] < self.f_opt:\n                self.f_opt = new_fitness[best_new_idx]\n                self.x_opt = new_population[best_new_idx]\n\n            # Update population (elitist replacement)\n            for i in range(self.pop_size):\n              if new_fitness[i] < fitness[i]:\n                population[i] = new_population[i]\n                fitness[i] = new_fitness[i]\n            \n            #Update mean\n            self.mean = np.mean(population, axis=0)\n\n            # Decay the scale\n            self.scale = max(self.scale * self.scale_decay, self.min_scale)\n\n            # Update covariance matrix (Rank-1 update)\n            diff = population[best_idx] - self.mean\n            self.covariance = (1 - 0.1) * self.covariance + 0.1 * np.outer(diff, diff) / (np.linalg.norm(diff)**2 + 1e-8) + 1e-9 * np.eye(self.dim)\n            self.covariance = 0.5 * (self.covariance + self.covariance.T) # Ensure symmetry\n            \n            # Restart mechanism\n            self.restart_counter += 1\n            if self.restart_counter > self.restart_interval:\n                population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.mean = np.mean(population, axis=0)\n                self.covariance = np.eye(self.dim)\n                self.scale = self.initial_scale\n                self.restart_counter = 0\n                if np.min(fitness) < self.f_opt:\n                  self.f_opt = np.min(fitness)\n                  self.x_opt = population[np.argmin(fitness)]\n                self.budget -= self.pop_size\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveGaussianAdaptation scored 0.413 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2b89b99f-0cd4-49c1-9d7d-782fa64b1c68"], "operator": null, "metadata": {"aucs": [0.14342817389429796, 0.19523385190492404, 0.5009511029368299, 0.17068137310487397, 0.2349714373615126, 0.5430579245431777, 0.30239191274339505, 0.43978897297706276, 0.45447621082633305, 0.18712162480182293, 0.8163976095071044, 0.9991521005440286, 0.3414442669099561, 0.20078347042128986, 0.8175787617177885, 0.37311257530230624, 0.2477632587431261, 0.6495917701429162, 0.19602528336176184, 0.4513123872559327]}}
{"id": "516f7eb6-22bb-499d-947b-95b5dc1f717e", "fitness": 0.22445336367366814, "name": "AdaptiveGaussianMutation", "description": "An adaptive Gaussian mutation algorithm with dynamic population size and learning rate adjustment based on the moving average of fitness improvements.", "code": "import numpy as np\n\nclass AdaptiveGaussianMutation:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=20, initial_lr=0.1, initial_scale=0.5, improvement_window=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.lr = initial_lr\n        self.lb = -5.0\n        self.ub = 5.0\n        self.scale = initial_scale\n        self.scale_decay = 0.995\n        self.min_scale = 0.01\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.fitness_history = []\n        self.improvement_window = improvement_window\n        self.improvement_threshold = 0.001\n        self.lr_decay = 0.99\n        self.lr_growth = 1.01\n\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_idx = np.argmin(fitness)\n        self.f_opt = fitness[best_idx]\n        self.x_opt = population[best_idx]\n        self.fitness_history.append(self.f_opt)\n\n        while self.budget > 0:\n            # Iterate through the population\n            for i in range(self.pop_size):\n                # Generate a new solution around the current individual using Gaussian distribution\n                new_solution = population[i] + np.random.normal(0, self.scale, size=self.dim) * self.lr\n                new_solution = np.clip(new_solution, self.lb, self.ub)\n\n                # Evaluate the new solution\n                if self.budget > 0:\n                    new_fitness = func(new_solution)\n                    self.budget -= 1\n\n                    # Update best solution\n                    if new_fitness < self.f_opt:\n                        self.f_opt = new_fitness\n                        self.x_opt = new_solution\n                        self.fitness_history.append(self.f_opt)\n                        \n                        # Replace the current individual if the new solution is better\n                    if new_fitness < fitness[i]:\n                        population[i] = new_solution\n                        fitness[i] = new_fitness\n                        self.fitness_history.append(new_fitness)\n                    else:\n                        self.fitness_history.append(fitness[i])\n\n\n            # Adjust population size based on improvement\n            if len(self.fitness_history) > self.improvement_window:\n                window_fitness = self.fitness_history[-self.improvement_window:]\n                improvement = window_fitness[0] - window_fitness[-1]\n                if improvement < self.improvement_threshold and self.pop_size < 2 * self.dim:\n                    self.pop_size += 1\n                    new_individual = np.random.uniform(self.lb, self.ub, size=self.dim)\n                    new_fitness = func(new_individual)\n                    self.budget -= 1\n                    population = np.vstack((population, new_individual))\n                    fitness = np.append(fitness, new_fitness)\n                    self.fitness_history.append(new_fitness)\n                elif improvement > 2* self.improvement_threshold and self.pop_size > 10:\n                    self.pop_size -= 1\n                    worst_idx = np.argmax(fitness)\n                    population = np.delete(population, worst_idx, axis=0)\n                    fitness = np.delete(fitness, worst_idx)\n\n\n            # Adjust learning rate\n            if len(self.fitness_history) > self.improvement_window:\n                window_fitness = self.fitness_history[-self.improvement_window:]\n                improvement = window_fitness[0] - window_fitness[-1]\n                if improvement < self.improvement_threshold:\n                    self.lr *= self.lr_decay\n                else:\n                    self.lr *= self.lr_growth\n\n            # Decay the scale\n            self.scale = max(self.scale * self.scale_decay, self.min_scale)\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveGaussianMutation scored 0.224 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["51a19267-5814-440a-8974-431d3146e00a"], "operator": null, "metadata": {"aucs": [0.09219348537706196, 0.20341945499161185, 0.21959426830908912, 0.15851252952596628, 0.21495171274141045, 0.3061791910673356, 0.2200106569606587, 0.12644862331295714, 0.189732588047129, 0.16414164432401535, 0.45641506355692407, 0.22817854824347494, 0.2220669586133217, 0.17233012828472516, 0.21994760602559027, 0.2384108182453426, 0.29252014495331446, 0.19592561427373012, 0.12320778641604901, 0.4448804502036543]}}
{"id": "8190f68f-3717-43b8-b2de-fe6eb1cfc54f", "fitness": 0.4536930624135772, "name": "AdaptiveLocalGaussian", "description": "Simplified adaptive Gaussian mutation with dynamic scaling, local search probability decay, and population size adjustment based on success rate.", "code": "import numpy as np\n\nclass AdaptiveLocalGaussian:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=20, lr=0.1, initial_scale=0.5, success_threshold=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.lr = lr\n        self.lb = -5.0\n        self.ub = 5.0\n        self.scale = initial_scale\n        self.scale_decay = 0.995\n        self.min_scale = 0.01\n        self.success_threshold = success_threshold\n        self.success_rate = 0.0\n        self.success_counter = 0\n        self.total_iterations = 0\n        self.local_search_prob = 0.9  # Probability of performing local search\n        self.local_search_decay = 0.99  # Decay factor for local search probability\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_idx = np.argmin(fitness)\n        self.f_opt = fitness[best_idx]\n        self.x_opt = population[best_idx]\n\n        while self.budget > 0:\n            # Iterate through the population\n            for i in range(self.pop_size):\n                # Perform local search with a certain probability\n                if np.random.rand() < self.local_search_prob:\n                    # Generate a new solution around the current individual using Gaussian distribution\n                    new_solution = np.random.normal(population[i], self.scale, size=self.dim)\n                else:\n                    # Global search step: random perturbation\n                    new_solution = population[i] + np.random.uniform(-self.scale, self.scale, size=self.dim)\n\n                new_solution = np.clip(new_solution, self.lb, self.ub)\n\n                # Evaluate the new solution\n                if self.budget > 0:\n                    new_fitness = func(new_solution)\n                    self.budget -= 1\n                    self.total_iterations += 1\n\n                    # Update best solution\n                    if new_fitness < self.f_opt:\n                        self.f_opt = new_fitness\n                        self.x_opt = new_solution\n                        self.success_counter += 1\n\n                    # Replace the current individual if the new solution is better\n                    if new_fitness < fitness[i]:\n                        population[i] = new_solution\n                        fitness[i] = new_fitness\n\n            # Update success rate\n            if self.total_iterations > 0:\n                self.success_rate = self.success_counter / self.total_iterations\n\n            # Adjust population size based on success rate\n            if self.success_rate > self.success_threshold and self.pop_size < 2 * self.dim and self.budget > 0:\n                self.pop_size += 1\n                new_individual = np.random.uniform(self.lb, self.ub, size=self.dim)\n                new_fitness = func(new_individual)\n                self.budget -= 1\n                population = np.vstack((population, new_individual))\n                fitness = np.append(fitness, new_fitness)\n            elif self.success_rate < self.success_threshold / 2 and self.pop_size > 10:\n                self.pop_size -= 1\n                worst_idx = np.argmax(fitness)\n                population = np.delete(population, worst_idx, axis=0)\n                fitness = np.delete(fitness, worst_idx)\n\n            # Decay the scale\n            self.scale = max(self.scale * self.scale_decay, self.min_scale)\n\n            # Decay local search probability\n            self.local_search_prob *= self.local_search_decay\n            self.local_search_prob = max(self.local_search_prob, 0.1) # Ensure a minimum probability\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveLocalGaussian scored 0.454 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["51a19267-5814-440a-8974-431d3146e00a"], "operator": null, "metadata": {"aucs": [0.11574306817930369, 0.22232225284646034, 0.47782226705722675, 0.8004656551292963, 0.2797415742908774, 0.525393406133075, 0.3008977350467722, 0.4120672625317904, 0.4312485628561833, 0.2097845031966704, 0.8055037423817996, 0.9900652178413399, 0.30151580739234196, 0.2760330733600058, 0.816510806895786, 0.4801740062137193, 0.28634344937620493, 0.636267888331251, 0.2005853891706313, 0.5053755800408072]}}
{"id": "c2d1427e-e6dc-4a6f-9c9f-c4bb922eb46c", "fitness": 0.4545907857523825, "name": "AdaptiveGaussianAdaptationSimpleHistory", "description": "Adaptive Gaussian mutation with dynamic scaling, covariance matrix adaptation using rank-one update, search space adaptation, stagnation-based restarts, and a history-based step size adaptation.", "code": "import numpy as np\n\nclass AdaptiveGaussianAdaptationSimpleHistory:\n    def __init__(self, budget=10000, dim=10, pop_size=20, lr=0.1, initial_scale=0.5, stagnation_threshold=500, history_length=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lr = lr\n        self.lb = -5.0\n        self.ub = 5.0\n        self.scale = initial_scale\n        self.scale_decay = 0.995\n        self.min_scale = 0.01\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.covariance = np.eye(dim)\n        self.x_opt = None\n        self.f_opt = np.inf\n        self.history_length = history_length\n        self.scale_history = [initial_scale] * history_length\n        self.history_index = 0\n\n    def __call__(self, func):\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_idx = np.argmin(fitness)\n        self.x_opt = population[best_idx]\n        self.f_opt = fitness[best_idx]\n\n        current_lb = self.lb\n        current_ub = self.ub\n\n        while self.budget > 0:\n            # Generate new solutions\n            new_population = np.random.multivariate_normal(mean=self.x_opt, cov=self.scale**2 * self.covariance, size=self.pop_size)\n            new_population = np.clip(new_population, current_lb, current_ub)\n\n            # Evaluate new solutions\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n\n            # Update best solution\n            best_new_idx = np.argmin(new_fitness)\n            if new_fitness[best_new_idx] < self.f_opt:\n                self.f_opt = new_fitness[best_new_idx]\n                self.x_opt = new_population[best_new_idx]\n                self.stagnation_counter = 0\n                self.update_scale_history(1)  # Reward successful step size\n            else:\n                self.stagnation_counter += 1\n                self.update_scale_history(-0.5) # Penalize unsuccessful step size\n\n            # Update population (replace worst with best from new population if better)\n            worst_idx = np.argmax(fitness)\n            if new_fitness[best_new_idx] < fitness[worst_idx]:\n                population[worst_idx] = new_population[best_new_idx]\n                fitness[worst_idx] = new_fitness[best_new_idx]\n\n            # Adapt search space (simplified)\n            current_lb = max(self.lb, self.x_opt.min() - self.lr * (self.ub - self.lb))\n            current_ub = min(self.ub, self.x_opt.max() + self.lr * (self.ub - self.lb))\n            current_lb = np.minimum(current_lb, current_ub - 1e-6)\n            current_ub = np.maximum(current_ub, current_lb + 1e-6)\n            \n            # Decay scale - use the mean of the scale history\n            self.scale = max(np.mean(self.scale_history) * self.scale_decay, self.min_scale)\n\n            # Adapt covariance matrix (Rank-1 update)\n            z = self.x_opt - np.mean(population, axis=0)\n            self.covariance = (1 - self.lr) * self.covariance + self.lr * np.outer(z, z) + 1e-9 * np.eye(self.dim)\n            \n            # Ensure positive definiteness of covariance\n            try:\n                np.linalg.cholesky(self.covariance)\n            except np.linalg.LinAlgError:\n                self.covariance = np.eye(self.dim)  # Reset to identity matrix if not positive definite\n\n\n            # Stagnation detection and restart\n            if self.stagnation_counter > self.stagnation_threshold:\n                population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n\n                best_idx = np.argmin(fitness)\n                self.x_opt = population[best_idx]\n                self.f_opt = fitness[best_idx]\n                self.stagnation_counter = 0\n                self.covariance = np.eye(self.dim)\n                self.scale = initial_scale\n                self.scale_history = [initial_scale] * self.history_length # Reset history too\n\n\n        return self.f_opt, self.x_opt\n\n    def update_scale_history(self, reward):\n        self.scale_history[self.history_index] = self.scale_history[self.history_index] * np.exp(0.1*reward) # Multiplicative update\n        self.history_index = (self.history_index + 1) % self.history_length\n        self.scale_history = np.clip(self.scale_history, self.min_scale, 1.0) # Clip the scale history", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveGaussianAdaptationSimpleHistory scored 0.455 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["90ec2908-fddf-4eaa-aef7-995ab0c1e537"], "operator": null, "metadata": {"aucs": [0.186100076892193, 0.1835456727350373, 0.7327881272354895, 0.19797810325248422, 0.18758902548408984, 0.22594968805860072, 0.28293707032774196, 0.663251355047592, 0.789984778968372, 0.18823146112923084, 0.9199926310881459, 0.9989428542671638, 0.3230183780763627, 0.2683810526125694, 0.5854042927799091, 0.3507212821476, 0.464411781566341, 0.8971079106016794, 0.15885289095441257, 0.4866272818226345]}}
{"id": "02be0cd3-321f-4a16-9447-28b037c1f7b9", "fitness": 0.4583553006038346, "name": "AdaptiveLocalGaussian", "description": "Simplified adaptive Gaussian mutation with dynamic scale, local search probability decay, and population size adjustment based on success rate, removing explicit success counter.", "code": "import numpy as np\n\nclass AdaptiveLocalGaussian:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=20, lr=0.1, initial_scale=0.5, success_threshold=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.lr = lr\n        self.lb = -5.0\n        self.ub = 5.0\n        self.scale = initial_scale\n        self.scale_decay = 0.995\n        self.min_scale = 0.01\n        self.success_threshold = success_threshold\n        self.success_rate = 0.0\n        self.local_search_prob = 0.9  # Probability of performing local search\n        self.local_search_decay = 0.99  # Decay factor for local search probability\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_idx = np.argmin(fitness)\n        self.f_opt = fitness[best_idx]\n        self.x_opt = population[best_idx]\n\n        evaluations = self.pop_size\n\n        while self.budget > 0:\n            successful_iterations = 0\n            # Iterate through the population and perform local search with a certain probability\n            for i in range(self.pop_size):\n                if np.random.rand() < self.local_search_prob:\n                    # Generate a new solution around the current individual using Gaussian distribution\n                    new_solution = np.random.normal(population[i], self.scale, size=self.dim)\n                    new_solution = np.clip(new_solution, self.lb, self.ub)\n\n                    # Evaluate the new solution\n                    if self.budget > 0:\n                        new_fitness = func(new_solution)\n                        self.budget -= 1\n                        evaluations += 1\n\n                        # Update best solution\n                        if new_fitness < self.f_opt:\n                            self.f_opt = new_fitness\n                            self.x_opt = new_solution\n                            successful_iterations +=1 #Count successful iterations.\n\n                        # Replace the current individual if the new solution is better\n                        if new_fitness < fitness[i]:\n                            population[i] = new_solution\n                            fitness[i] = new_fitness\n                else:\n                  # Global search step: random perturbation\n                  new_solution = population[i] + np.random.uniform(-self.scale, self.scale, size=self.dim)\n                  new_solution = np.clip(new_solution, self.lb, self.ub)\n\n                  # Evaluate the new solution\n                  if self.budget > 0:\n                      new_fitness = func(new_solution)\n                      self.budget -= 1\n                      evaluations += 1\n\n                      # Update best solution\n                      if new_fitness < self.f_opt:\n                          self.f_opt = new_fitness\n                          self.x_opt = new_solution\n                          successful_iterations += 1 #Count successful iterations.\n\n                      # Replace the current individual if the new solution is better\n                      if new_fitness < fitness[i]:\n                          population[i] = new_solution\n                          fitness[i] = new_fitness\n\n            # Update success rate\n            self.success_rate = successful_iterations / self.pop_size #Simplified success rate calculation\n\n            # Adjust population size based on success rate\n            if self.success_rate > self.success_threshold and self.pop_size < 2 * self.dim:\n                self.pop_size += 1\n                new_individual = np.random.uniform(self.lb, self.ub, size=self.dim)\n                if self.budget > 0:\n                    new_fitness = func(new_individual)\n                    self.budget -= 1\n                    evaluations += 1\n                    population = np.vstack((population, new_individual))\n                    fitness = np.append(fitness, new_fitness)\n            elif self.success_rate < self.success_threshold / 2 and self.pop_size > 10:\n                self.pop_size -= 1\n                worst_idx = np.argmax(fitness)\n                population = np.delete(population, worst_idx, axis=0)\n                fitness = np.delete(fitness, worst_idx)\n\n            # Decay the scale\n            self.scale = max(self.scale * self.scale_decay, self.min_scale)\n\n            # Decay local search probability\n            self.local_search_prob *= self.local_search_decay\n            self.local_search_prob = max(self.local_search_prob, 0.1) # Ensure a minimum probability\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveLocalGaussian scored 0.458 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["51a19267-5814-440a-8974-431d3146e00a"], "operator": null, "metadata": {"aucs": [0.12055108704267292, 0.2091237405387173, 0.4712069030613967, 0.7563944275380357, 0.4230365103654615, 0.516249316114082, 0.2691320479867737, 0.4221001576115275, 0.4480095189689173, 0.18283533189727552, 0.7535514827923274, 0.985656389354448, 0.3009092201715545, 0.42248937821669297, 0.8797020050231139, 0.3747714365250787, 0.3214128961686342, 0.6332389442560107, 0.19243714882491603, 0.4842980696190543]}}
{"id": "a45bac83-858a-40ad-b2a9-512313764c7d", "fitness": 0.2919570802039856, "name": "AdaptiveLocalGaussian", "description": "Adaptive Gaussian mutation with dynamic population size, local and global search phases controlled by success rate, and a momentum-based scale adaptation.", "code": "import numpy as np\n\nclass AdaptiveLocalGaussian:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=20, lr=0.1, initial_scale=0.5, success_threshold=0.2, momentum=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.lr = lr\n        self.lb = -5.0\n        self.ub = 5.0\n        self.scale = initial_scale\n        self.min_scale = 0.01\n        self.success_threshold = success_threshold\n        self.success_rate = 0.0\n        self.success_counter = 0\n        self.total_iterations = 0\n        self.local_search_prob = 0.9  # Probability of performing local search\n        self.momentum = momentum\n        self.scale_velocity = 0.0\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.global_search_prob = 0.1 # Probability of performing global search\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_idx = np.argmin(fitness)\n        self.f_opt = fitness[best_idx]\n        self.x_opt = population[best_idx]\n\n        while self.budget > 0:\n            # Iterate through the population\n            for i in range(self.pop_size):\n                # Determine search strategy (local or global) based on probabilities\n                if np.random.rand() < self.local_search_prob:\n                    # Local search: Gaussian mutation around the current individual\n                    new_solution = np.random.normal(population[i], self.scale, size=self.dim)\n                elif np.random.rand() < self.global_search_prob:\n                    # Global search: Uniform sampling within bounds\n                    new_solution = np.random.uniform(self.lb, self.ub, size=self.dim)\n                else:\n                    # Intermediate search: Perturbation with a larger scale\n                    new_solution = population[i] + np.random.uniform(-2 * self.scale, 2 * self.scale, size=self.dim)\n\n                new_solution = np.clip(new_solution, self.lb, self.ub)\n\n                # Evaluate the new solution\n                if self.budget > 0:\n                    new_fitness = func(new_solution)\n                    self.budget -= 1\n                    self.total_iterations += 1\n\n                    # Update best solution\n                    if new_fitness < self.f_opt:\n                        self.f_opt = new_fitness\n                        self.x_opt = new_solution\n                        self.success_counter += 1\n\n                    # Replace the current individual if the new solution is better\n                    if new_fitness < fitness[i]:\n                        population[i] = new_solution\n                        fitness[i] = new_fitness\n\n            # Update success rate\n            if self.total_iterations > 0:\n                self.success_rate = self.success_counter / self.total_iterations\n\n            # Adjust population size based on success rate\n            if self.success_rate > self.success_threshold and self.pop_size < 2 * self.dim:\n                self.pop_size += 1\n                new_individual = np.random.uniform(self.lb, self.ub, size=self.dim)\n                if self.budget > 0:\n                    new_fitness = func(new_individual)\n                    self.budget -= 1\n                    population = np.vstack((population, new_individual))\n                    fitness = np.append(fitness, new_fitness)\n            elif self.success_rate < self.success_threshold / 2 and self.pop_size > 10:\n                self.pop_size -= 1\n                worst_idx = np.argmax(fitness)\n                population = np.delete(population, worst_idx, axis=0)\n                fitness = np.delete(fitness, worst_idx)\n\n            # Adjust scale using momentum\n            if self.success_rate > self.success_threshold:\n                scale_adjust = 1 + self.lr * (self.success_rate - self.success_threshold)\n            else:\n                scale_adjust = 1 - self.lr * (self.success_threshold - self.success_rate)\n\n            self.scale_velocity = self.momentum * self.scale_velocity + (1 - self.momentum) * (scale_adjust - 1)\n            self.scale *= (1 + self.scale_velocity)\n            self.scale = max(self.scale, self.min_scale)\n            \n            # Dynamic adjustment of local and global search probabilities\n            if self.success_rate > self.success_threshold:\n                self.local_search_prob = min(self.local_search_prob + self.lr, 0.99)\n                self.global_search_prob = max(self.global_search_prob - self.lr, 0.01)\n            else:\n                self.local_search_prob = max(self.local_search_prob - self.lr, 0.01)\n                self.global_search_prob = min(self.global_search_prob + self.lr, 0.99)\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveLocalGaussian scored 0.292 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["51a19267-5814-440a-8974-431d3146e00a"], "operator": null, "metadata": {"aucs": [0.12870451511641445, 0.2120116412656462, 0.27721716455325585, 0.20766537330599688, 0.20104472778187243, 0.23254504780600327, 0.23417183812946807, 0.22705986698720937, 0.21445360606180963, 0.16165169802204638, 0.26516751927024784, 0.9914234551668608, 0.2846193435324291, 0.2071637907595567, 0.5620857441560685, 0.27982726520661827, 0.22995687427753853, 0.27288379041212607, 0.19172079309502765, 0.45776754917351514]}}
{"id": "4345f3d7-bebd-4cd5-94bf-5ab5999fe7a1", "fitness": 0.48614248248838765, "name": "AdaptiveLocalGaussianArchive", "description": "Adaptive Gaussian mutation with dynamic scaling, local search probability, population size adjustment, and an archive to preserve promising solutions and enhance exploration.", "code": "import numpy as np\n\nclass AdaptiveLocalGaussianArchive:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=20, lr=0.1, initial_scale=0.5, success_threshold=0.2, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.lr = lr\n        self.lb = -5.0\n        self.ub = 5.0\n        self.scale = initial_scale\n        self.scale_decay = 0.995\n        self.min_scale = 0.01\n        self.success_threshold = success_threshold\n        self.success_rate = 0.0\n        self.success_counter = 0\n        self.total_iterations = 0\n        self.local_search_prob = 0.9  # Probability of performing local search\n        self.local_search_decay = 0.99  # Decay factor for local search probability\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.archive_size = archive_size\n        self.archive_x = []\n        self.archive_f = []\n\n    def update_archive(self, x, f):\n        if len(self.archive_f) < self.archive_size:\n            self.archive_x.append(x)\n            self.archive_f.append(f)\n        else:\n            max_archive_fitness_idx = np.argmax(self.archive_f)\n            if f < self.archive_f[max_archive_fitness_idx]:\n                self.archive_x[max_archive_fitness_idx] = x\n                self.archive_f[max_archive_fitness_idx] = f\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_idx = np.argmin(fitness)\n        self.f_opt = fitness[best_idx]\n        self.x_opt = population[best_idx]\n\n        self.update_archive(self.x_opt, self.f_opt)\n\n        while self.budget > 0:\n            # Iterate through the population and perform local search with a certain probability\n            for i in range(self.pop_size):\n                if np.random.rand() < self.local_search_prob:\n                    # Generate a new solution around the current individual using Gaussian distribution\n                    new_solution = np.random.normal(population[i], self.scale, size=self.dim)\n                    new_solution = np.clip(new_solution, self.lb, self.ub)\n\n                    # Evaluate the new solution\n                    if self.budget > 0:\n                        new_fitness = func(new_solution)\n                        self.budget -= 1\n                        self.total_iterations += 1\n\n                        # Update best solution\n                        if new_fitness < self.f_opt:\n                            self.f_opt = new_fitness\n                            self.x_opt = new_solution\n                            self.success_counter += 1\n                            self.update_archive(self.x_opt, self.f_opt)\n\n                        # Replace the current individual if the new solution is better\n                        if new_fitness < fitness[i]:\n                            population[i] = new_solution\n                            fitness[i] = new_fitness\n                            self.update_archive(new_solution, new_fitness)\n                else:\n                  # Global search step: random perturbation or exploitation of archive\n                  if len(self.archive_x) > 0 and np.random.rand() < 0.2:  #Exploit the archive with probability 0.2\n                      archive_idx = np.random.randint(len(self.archive_x))\n                      new_solution = np.random.normal(self.archive_x[archive_idx], self.scale, size=self.dim)\n                  else:\n                      new_solution = population[i] + np.random.uniform(-self.scale, self.scale, size=self.dim)\n                  new_solution = np.clip(new_solution, self.lb, self.ub)\n\n                  # Evaluate the new solution\n                  if self.budget > 0:\n                      new_fitness = func(new_solution)\n                      self.budget -= 1\n                      self.total_iterations += 1\n\n                      # Update best solution\n                      if new_fitness < self.f_opt:\n                          self.f_opt = new_fitness\n                          self.x_opt = new_solution\n                          self.success_counter += 1\n                          self.update_archive(self.x_opt, self.f_opt)\n\n                      # Replace the current individual if the new solution is better\n                      if new_fitness < fitness[i]:\n                          population[i] = new_solution\n                          fitness[i] = new_fitness\n                          self.update_archive(new_solution, new_fitness)\n\n            # Update success rate\n            if self.total_iterations > 0:\n                self.success_rate = self.success_counter / self.total_iterations\n\n            # Adjust population size based on success rate\n            if self.success_rate > self.success_threshold and self.pop_size < 2 * self.dim:\n                self.pop_size += 1\n                new_individual = np.random.uniform(self.lb, self.ub, size=self.dim)\n                new_fitness = func(new_individual)\n                self.budget -= 1\n                population = np.vstack((population, new_individual))\n                fitness = np.append(fitness, new_fitness)\n                self.update_archive(new_individual, new_fitness)\n            elif self.success_rate < self.success_threshold / 2 and self.pop_size > 10:\n                self.pop_size -= 1\n                worst_idx = np.argmax(fitness)\n                population = np.delete(population, worst_idx, axis=0)\n                fitness = np.delete(fitness, worst_idx)\n\n            # Decay the scale\n            self.scale = max(self.scale * self.scale_decay, self.min_scale)\n\n            # Decay local search probability\n            self.local_search_prob *= self.local_search_decay\n            self.local_search_prob = max(self.local_search_prob, 0.1) # Ensure a minimum probability\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveLocalGaussianArchive scored 0.486 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["51a19267-5814-440a-8974-431d3146e00a"], "operator": null, "metadata": {"aucs": [0.14350909412644963, 0.20286587780748122, 0.46944895108740414, 0.8244920689492438, 0.4378505457256844, 0.5251610028048704, 0.3690940305188495, 0.4220240823007704, 0.45195491214459593, 0.18467137032815584, 0.7898343524060885, 0.9934230950821953, 0.2863153316482484, 0.4530436085263648, 0.8497143200475428, 0.5259231029157816, 0.41109108223975666, 0.6379605137446316, 0.21615901954243866, 0.528313287821198]}}
{"id": "8d050bf6-92d5-425b-b843-0dc5ffa53cb4", "fitness": 0.6986380247915803, "name": "AdaptiveDifferentialEvolutionCMA", "description": "Adaptive differential evolution with covariance matrix adaptation, incorporating a mirrored sampling strategy to enhance exploration and exploitation around the best solutions found so far.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolutionCMA:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, Cr=0.9, sigma_init=0.1, mirror_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.Cr = Cr\n        self.sigma_init = sigma_init\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mirror_rate = mirror_rate\n\n    def __call__(self, func):\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)].copy()  # Ensure x_opt is a copy\n\n        sigma = self.sigma_init * (self.ub - self.lb)\n        C = np.eye(self.dim)\n        learning_rate_C = 0.1\n        learning_rate_sigma = 0.1\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n\n                # Enhanced mutation with CMA influence\n                v = x_r1 + self.F * (x_r2 - x_r3) + sigma * np.random.multivariate_normal(np.zeros(self.dim), C)\n                u = np.zeros(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.Cr:\n                        u[j] = v[j]\n                    else:\n                        u[j] = population[i, j]\n\n                u = np.clip(u, self.lb, self.ub)\n\n                # Mirrored sampling\n                if np.random.rand() < self.mirror_rate:\n                    u = self.x_opt + (self.x_opt - u)\n                    u = np.clip(u, self.lb, self.ub)\n                \n                f_u = func(u)\n                self.budget -= 1\n                \n                if f_u < fitness[i]:\n                    population[i] = u\n                    fitness[i] = f_u\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u.copy() # Ensure x_opt is a copy\n\n            # CMA update\n            diff = population[np.argmin(fitness)] - self.x_opt\n            C = (1 - learning_rate_C) * C + learning_rate_C * np.outer(diff, diff)\n\n            # Adapt step size\n            success_rate = np.mean(fitness > np.min(fitness))\n            if success_rate > 0.8:\n                sigma *= (1 - learning_rate_sigma) + learning_rate_sigma * 1.2\n            elif success_rate < 0.2:\n                sigma *= (1 - learning_rate_sigma) + learning_rate_sigma * 0.8\n            \n            sigma = np.clip(sigma, self.sigma_init/10, (self.ub - self.lb)) #prevent sigma from going to 0\n\n            # Parameter adaptation (simplified)\n            if np.random.rand() < 0.1:\n                self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n                self.Cr = np.clip(np.random.normal(self.Cr, 0.1), 0.1, 1.0)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDifferentialEvolutionCMA scored 0.699 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["382a6540-ec16-412d-b71f-2d8b9262d4ac"], "operator": null, "metadata": {"aucs": [0.31700569788707034, 0.7738350205064886, 0.7454408139248812, 0.9036376327781724, 0.7401391165743227, 0.7679330123655606, 0.4399651569444206, 0.6513806111045173, 0.6701971402349616, 0.7082004600862666, 0.7762147392347958, 0.9979088983654267, 0.381747048913168, 0.6647235553856526, 0.9293548833769154, 0.7574577064078872, 0.7045475182377019, 0.8210347726704381, 0.706787724815032, 0.5152489860179259]}}
{"id": "3b8560be-ba92-4436-9285-a9b5b1ab6438", "fitness": 0.5270458738851993, "name": "AdaptiveGaussianAdaptation", "description": "Adaptive Gaussian mutation with simplified covariance matrix adaptation, dynamic scaling, restart mechanism, and a population update focusing on replacing the worst with a probability based on fitness improvement.", "code": "import numpy as np\n\nclass AdaptiveGaussianAdaptation:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_scale=0.5, stagnation_threshold=500, selection_pressure=2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.scale = initial_scale  # Initial scale for Gaussian distribution\n        self.scale_decay = 0.995  # Decay factor for the scale\n        self.min_scale = 0.01  # Minimum scale value\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.covariance = np.eye(dim)  # Initialize covariance matrix\n        self.selection_pressure = selection_pressure  # Adjust selection pressure\n\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            # Find best individual\n            best_idx = np.argmin(fitness)\n            best_x = population[best_idx]\n\n            # Generate new solutions around the best individual using Gaussian distribution\n            new_population = np.random.multivariate_normal(mean=best_x, cov=self.scale**2 * self.covariance, size=self.pop_size)\n\n            # Clip new solutions to the search space\n            new_population = np.clip(new_population, self.lb, self.ub)\n\n            # Evaluate new solutions\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n\n            # Update best solution\n            best_new_idx = np.argmin(new_fitness)\n            if new_fitness[best_new_idx] < self.f_opt:\n                self.f_opt = new_fitness[best_new_idx]\n                self.x_opt = new_population[best_new_idx]\n                self.stagnation_counter = 0  # Reset stagnation counter\n            else:\n                self.stagnation_counter += 1\n\n            # Update population (replace worst with best from new population probabilistically)\n            worst_idx = np.argmax(fitness)\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[worst_idx]:\n                    #Replace worst individual with probability proportional to fitness improvement\n                    probability = (fitness[worst_idx] - new_fitness[i]) / (fitness[worst_idx] - self.f_opt + 1e-9) # Avoid division by zero\n                    if np.random.rand() < probability**self.selection_pressure: # Selection Pressure\n                        population[worst_idx] = new_population[i]\n                        fitness[worst_idx] = new_fitness[i]\n                        worst_idx = np.argmax(fitness) # Find new worst after replacement\n\n            # Decay the scale\n            self.scale = max(self.scale * self.scale_decay, self.min_scale)\n\n            # Update covariance matrix (simplified adaptation)\n            diff = population - np.mean(population, axis=0)\n            self.covariance = 0.9 * self.covariance + 0.1 * np.cov(diff.T) + 1e-9 * np.eye(self.dim) # Moving average and add small value\n\n            # Stagnation detection and restart\n            if self.stagnation_counter > self.stagnation_threshold:\n                # Reset population\n                population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.covariance = np.eye(self.dim)\n                self.scale = self.initial_scale\n                self.stagnation_counter = 0\n                if np.min(fitness) < self.f_opt:\n                  self.f_opt = np.min(fitness)\n                  self.x_opt = population[np.argmin(fitness)]\n                self.budget -= self.pop_size\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveGaussianAdaptation scored 0.527 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2b89b99f-0cd4-49c1-9d7d-782fa64b1c68"], "operator": null, "metadata": {"aucs": [0.15667879224054648, 0.16587940420486869, 0.7806947336245786, 0.16557555361708076, 0.2604123996540927, 0.8555503180953925, 0.3030105083758884, 0.6943240805539697, 0.8267274215357967, 0.17557002352844142, 0.9393807563112085, 0.9982310049683305, 0.22443694483627086, 0.27371696105574894, 0.959041763090083, 0.8028920860844018, 0.33181379295068814, 0.9083917533544109, 0.21247057843018624, 0.5061186011920014]}}
{"id": "89df4188-125f-476c-87a4-3d68df1bcdfe", "fitness": 0.7312157501384627, "name": "AdaptiveDifferentialEvolutionCMA", "description": "Adaptive Differential Evolution with simplified CMA-ES-inspired mutation and parameter adaptation, using a rank-one covariance update and dynamic step size.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolutionCMA:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, Cr=0.9, sigma_init=0.1, learning_rate_C=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.Cr = Cr\n        self.sigma_init = sigma_init\n        self.lb = -5.0\n        self.ub = 5.0\n        self.learning_rate_C = learning_rate_C\n\n    def __call__(self, func):\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)].copy()\n\n        sigma = self.sigma_init * (self.ub - self.lb)\n        C = np.eye(self.dim)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n\n                # Simplified mutation with CMA influence\n                v = x_r1 + self.F * (x_r2 - x_r3) + sigma * np.random.normal(0, np.diag(C)**0.5, size=self.dim)\n                u = np.where(np.random.rand(self.dim) < self.Cr, v, population[i])\n                u = np.clip(u, self.lb, self.ub)\n                \n                f_u = func(u)\n                self.budget -= 1\n                \n                if f_u < fitness[i]:\n                    population[i] = u\n                    fitness[i] = f_u\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u.copy()\n\n            # CMA rank-one update\n            diff = population[np.argmin(fitness)] - self.x_opt\n            C = (1 - self.learning_rate_C) * C + self.learning_rate_C * np.outer(diff, diff)\n            C = (C + C.T) / 2  # Ensure symmetry\n            \n            # Keep C positive definite\n            C += 1e-8 * np.eye(self.dim)\n            \n            # Adapt step size\n            if np.min(fitness) < self.f_opt:\n                sigma *= 1.02  # Increase sigma if improvement\n            else:\n                sigma *= 0.98  # Decrease sigma if no improvement\n\n            # Parameter adaptation (simplified)\n            self.F = np.clip(np.random.normal(self.F, 0.05), 0.1, 1.0)\n            self.Cr = np.clip(np.random.normal(self.Cr, 0.05), 0.1, 1.0)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDifferentialEvolutionCMA scored 0.731 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["382a6540-ec16-412d-b71f-2d8b9262d4ac"], "operator": null, "metadata": {"aucs": [0.4870338935412648, 0.6874354017901563, 0.8207672438258043, 0.8806135890661001, 0.8416020032725327, 0.8729808009836095, 0.7033880579890712, 0.48161368452585107, 0.8048907548202265, 0.7906631145080334, 0.9245768558521872, 0.997633227253277, 0.36458100292391515, 0.8482463358317601, 0.9210780569661198, 0.8455999776356949, 0.4472510018568798, 0.8962789172529809, 0.21918733056293327, 0.7888937523108536]}}
{"id": "a53ceb4f-72b4-4bf2-beff-2fb2399b308b", "fitness": -Infinity, "name": "AdaptiveDifferentialEvolutionCMA", "description": "Improved Adaptive Differential Evolution with CMA-ES inspired mutation, dynamic population size, orthogonal crossover, and adaptive parameter control.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolutionCMA:\n    def __init__(self, budget=10000, dim=10, pop_size_init=20, F=0.5, Cr=0.9, sigma_init=0.1, learning_rate_C=0.1, pop_size_adapt=True):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size_init\n        self.F = F\n        self.Cr = Cr\n        self.sigma_init = sigma_init\n        self.lb = -5.0\n        self.ub = 5.0\n        self.learning_rate_C = learning_rate_C\n        self.pop_size_adapt = pop_size_adapt\n        self.archive_factor = 2  # Archive size as a multiple of population size\n        self.archive = []\n        self.archive_fitness = []\n\n    def __call__(self, func):\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)].copy()\n\n        sigma = self.sigma_init * (self.ub - self.lb)\n        C = np.eye(self.dim)\n\n        mu = population.copy() # Store the best solutions\n\n        while self.budget > 0:\n            # Population size adaptation\n            if self.pop_size_adapt:\n                self.pop_size = min(max(int(4 + self.budget / 200), 6), 50) # Adaptive population size\n\n            if population.shape[0] != self.pop_size:\n                population = population[:self.pop_size]\n                fitness = fitness[:self.pop_size]\n                mu = mu[:self.pop_size]\n\n            for i in range(self.pop_size):\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n\n                # Simplified mutation with CMA influence\n                v = x_r1 + self.F * (x_r2 - x_r3) + sigma * np.random.normal(0, np.diag(C)**0.5, size=self.dim)\n                \n                # Orthogonal Crossover\n                u = self.orthogonal_crossover(population[i], v)\n\n                u = np.clip(u, self.lb, self.ub)\n                \n                f_u = func(u)\n                self.budget -= 1\n                \n                if f_u < fitness[i]:\n                    population[i] = u\n                    fitness[i] = f_u\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u.copy()\n                        \n                        # Update mu with the improved solution\n                        mu[i] = u.copy()\n\n                    # Archive the new solution\n                    self.archive.append(u)\n                    self.archive_fitness.append(f_u)\n\n            # CMA rank-one update\n            diff = population[np.argmin(fitness)] - self.x_opt\n            C = (1 - self.learning_rate_C) * C + self.learning_rate_C * np.outer(diff, diff)\n            C = (C + C.T) / 2  # Ensure symmetry\n            \n            # Keep C positive definite\n            C += 1e-8 * np.eye(self.dim)\n            \n            # Adapt step size\n            if np.min(fitness) < self.f_opt:\n                sigma *= 1.02  # Increase sigma if improvement\n            else:\n                sigma *= 0.98  # Decrease sigma if no improvement\n\n            # Parameter adaptation (simplified)\n            self.F = np.clip(np.random.normal(self.F, 0.05), 0.1, 1.0)\n            self.Cr = np.clip(np.random.normal(self.Cr, 0.05), 0.1, 1.0)\n\n            # Archive Management: Keep top solutions and limit archive size\n            archive_indices = np.argsort(self.archive_fitness)[:min(len(self.archive_fitness), self.archive_factor * self.pop_size)] # Keep best from archive\n            self.archive = [self.archive[i] for i in archive_indices]\n            self.archive_fitness = [self.archive_fitness[i] for i in archive_indices]\n\n        return self.f_opt, self.x_opt\n\n    def orthogonal_crossover(self, x1, x2):\n        # Simple averaging for orthogonal crossover (can be replaced with more sophisticated methods)\n        return 0.5 * (x1 + x2)", "configspace": "", "generation": 5, "feedback": "An exception occurred: index 40 is out of bounds for axis 0 with size 20.", "error": "", "parent_ids": ["89df4188-125f-476c-87a4-3d68df1bcdfe"], "operator": null, "metadata": {}}
{"id": "63eb3f3e-0020-4358-b83f-e201a502e854", "fitness": 0.0, "name": "AdaptiveDEOrthogonalArchive", "description": "An adaptive differential evolution strategy with a self-adaptive population size, orthogonal crossover, and a mutation that combines information from the best individual and a randomly selected archive member.", "code": "import numpy as np\n\nclass AdaptiveDEOrthogonalArchive:\n    def __init__(self, budget=10000, dim=10, pop_size_init=20, archive_size=10, F=0.5, Cr=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_init = pop_size_init\n        self.archive_size = archive_size\n        self.F = F\n        self.Cr = Cr\n        self.lb = -5.0\n        self.ub = 5.0\n        self.archive = []\n        self.archive_fitness = []\n\n    def __call__(self, func):\n        pop_size = self.pop_size_init\n        population = np.random.uniform(self.lb, self.ub, size=(pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)].copy()\n\n        while self.budget > 0:\n            # Adapt population size\n            if np.random.rand() < 0.1:\n                pop_size = int(self.pop_size_init * (1 + 0.2 * np.random.randn()))\n                pop_size = np.clip(pop_size, 10, 50)  # limit the size\n                \n                if pop_size != population.shape[0]:\n                    population = np.random.uniform(self.lb, self.ub, size=(pop_size, self.dim))\n                    fitness = np.array([func(x) for x in population])\n                    self.budget -= pop_size\n                    self.f_opt = np.min(fitness)\n                    self.x_opt = population[np.argmin(fitness)].copy()\n\n            for i in range(population.shape[0]):\n                idxs = np.random.choice(population.shape[0], 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n\n                # Mutation: incorporate best and archive\n                if self.archive:\n                    archive_idx = np.random.randint(len(self.archive))\n                    x_archive = self.archive[archive_idx]\n                    v = self.x_opt + self.F * (x_r1 - x_r2) + self.F * (x_archive - x_r3)\n                else:\n                    v = x_r1 + self.F * (x_r2 - x_r3)\n                \n                v = np.clip(v, self.lb, self.ub)\n\n                # Orthogonal Crossover\n                u = np.zeros(self.dim)\n                basis = np.random.randn(self.dim, self.dim)\n                Q, R = np.linalg.qr(basis)\n                \n                for j in range(self.dim):\n                    if np.random.rand() < self.Cr:\n                         u[j] = v[j]\n                    else:\n                        u[j] = population[i, j]\n                \n                f_u = func(u)\n                self.budget -= 1\n\n                if f_u < fitness[i]:\n                    population[i] = u\n                    fitness[i] = f_u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u.copy()\n\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(u)\n                        self.archive_fitness.append(f_u)\n                    else:\n                        max_archive_idx = np.argmax(self.archive_fitness)\n                        if f_u < self.archive_fitness[max_archive_idx]:\n                            self.archive[max_archive_idx] = u\n                            self.archive_fitness[max_archive_idx] = f_u\n\n            # Parameter adaptation (simplified)\n            if np.random.rand() < 0.1:\n                self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n                self.Cr = np.clip(np.random.normal(self.Cr, 0.1), 0.1, 1.0)\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDEOrthogonalArchive scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["8d050bf6-92d5-425b-b843-0dc5ffa53cb4"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "400c5830-4ddd-4835-9f4a-c1e9c1af1ae0", "fitness": 0.11498577267065141, "name": "AdaptiveDifferentialEvolutionCMA", "description": "Adaptive Differential Evolution with CMA-ES-inspired mutation and parameter adaptation, incorporating a mirrored sampling strategy and adaptive population size to enhance exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolutionCMA:\n    def __init__(self, budget=10000, dim=10, pop_size_init=20, F=0.5, Cr=0.9, sigma_init=0.1, learning_rate_C=0.1, adapt_pop_every=500):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size_init\n        self.pop_size_init = pop_size_init\n        self.F = F\n        self.Cr = Cr\n        self.sigma_init = sigma_init\n        self.lb = -5.0\n        self.ub = 5.0\n        self.learning_rate_C = learning_rate_C\n        self.adapt_pop_every = adapt_pop_every\n        self.eval_count = 0\n\n    def __call__(self, func):\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.eval_count += self.pop_size\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)].copy()\n\n        sigma = self.sigma_init * (self.ub - self.lb)\n        C = np.eye(self.dim)\n\n        archive = [] # Archive for mirrored sampling\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n\n                # Simplified mutation with CMA influence and mirrored sampling\n                if len(archive) > 0 and np.random.rand() < 0.1: # Use archive with small probability\n                    x_archive = archive[np.random.choice(len(archive))]\n                    v = x_r1 + self.F * (x_r2 - x_r3) + sigma * np.random.normal(0, np.diag(C)**0.5, size=self.dim) + 0.1 * (x_archive - population[i])\n                else:\n                    v = x_r1 + self.F * (x_r2 - x_r3) + sigma * np.random.normal(0, np.diag(C)**0.5, size=self.dim)\n                \n                u = np.where(np.random.rand(self.dim) < self.Cr, v, population[i])\n                u = np.clip(u, self.lb, self.ub)\n                \n                f_u = func(u)\n                self.eval_count += 1\n                self.budget -= 1\n                \n                if f_u < fitness[i]:\n                    population[i] = u\n                    fitness[i] = f_u\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u.copy()\n                        archive.append(u.copy())\n                        if len(archive) > 10:\n                            archive.pop(0) # Keep archive size limited\n\n            # CMA rank-one update\n            diff = population[np.argmin(fitness)] - self.x_opt\n            C = (1 - self.learning_rate_C) * C + self.learning_rate_C * np.outer(diff, diff)\n            C = (C + C.T) / 2  # Ensure symmetry\n            \n            # Keep C positive definite\n            C += 1e-8 * np.eye(self.dim)\n            \n            # Adapt step size\n            if np.min(fitness) < self.f_opt:\n                sigma *= 1.02  # Increase sigma if improvement\n            else:\n                sigma *= 0.98  # Decrease sigma if no improvement\n\n            # Parameter adaptation (simplified)\n            self.F = np.clip(np.random.normal(self.F, 0.05), 0.1, 1.0)\n            self.Cr = np.clip(np.random.normal(self.Cr, 0.05), 0.1, 1.0)\n\n            # Adapt population size\n            if self.eval_count % self.adapt_pop_every == 0:\n                if np.std(fitness) < 0.01 * (self.ub - self.lb):\n                    self.pop_size = min(self.pop_size + 5, self.pop_size_init * 5)  # Increase population size if converged\n                else:\n                    self.pop_size = max(self.pop_size - 2, self.pop_size_init)   # Decrease if not converged\n                \n                # Regenerate population\n                new_population = np.random.uniform(self.lb, self.ub, size=(self.pop_size - len(population), self.dim))\n                new_fitness = np.array([func(x) for x in new_population])\n                self.eval_count += (self.pop_size - len(population))\n                self.budget -= (self.pop_size - len(population))\n\n                population = np.vstack((population, new_population))\n                fitness = np.concatenate((fitness, new_fitness))\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDifferentialEvolutionCMA scored 0.115 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["89df4188-125f-476c-87a4-3d68df1bcdfe"], "operator": null, "metadata": {"aucs": [0.22997154534130282, 0]}}
{"id": "5f209070-18f6-4b02-8ee6-de3b2044fa42", "fitness": 0.4381797526983096, "name": "AdaptiveGaussianAdaptation", "description": "Simplified adaptive Gaussian mutation with dynamic scaling and probabilistic population updates, focusing on efficiency and reducing complexity.", "code": "import numpy as np\n\nclass AdaptiveGaussianAdaptation:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_scale=0.5, stagnation_threshold=500):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.scale = initial_scale\n        self.scale_decay = 0.995\n        self.min_scale = 0.01\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n\n    def __call__(self, func):\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            best_idx = np.argmin(fitness)\n            best_x = population[best_idx]\n\n            # Generate new solutions\n            new_population = best_x + self.scale * np.random.normal(0, 1, size=(self.pop_size, self.dim))\n\n            # Clip new solutions\n            new_population = np.clip(new_population, self.lb, self.ub)\n\n            # Evaluate new solutions\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n\n            # Update best solution\n            best_new_idx = np.argmin(new_fitness)\n            if new_fitness[best_new_idx] < self.f_opt:\n                self.f_opt = new_fitness[best_new_idx]\n                self.x_opt = new_population[best_new_idx]\n                self.stagnation_counter = 0\n            else:\n                self.stagnation_counter += 1\n\n            # Probabilistic population update (replace if better)\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    population[i] = new_population[i]\n                    fitness[i] = new_fitness[i]\n\n            # Decay the scale\n            self.scale = max(self.scale * self.scale_decay, self.min_scale)\n\n            # Stagnation detection and restart\n            if self.stagnation_counter > self.stagnation_threshold:\n                population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.scale = 0.5 # Reset Scale\n                self.stagnation_counter = 0\n                if np.min(fitness) < self.f_opt:\n                  self.f_opt = np.min(fitness)\n                  self.x_opt = population[np.argmin(fitness)]\n                self.budget -= self.pop_size\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveGaussianAdaptation scored 0.438 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3b8560be-ba92-4436-9285-a9b5b1ab6438"], "operator": null, "metadata": {"aucs": [0.12282818378843785, 0.23247805972389457, 0.41972084002509735, 0.5924176282761315, 0.34802199676056633, 0.4313576324074738, 0.28416603209238633, 0.3586333965826298, 0.36129878893657563, 0.21469757174750248, 0.8582159062373687, 0.9963057042619172, 0.27366456611145806, 0.3641462858773471, 0.8087999852086265, 0.423910397695605, 0.3756414803012572, 0.5521973531574231, 0.2804049360674842, 0.46468830870700994]}}
{"id": "3deeedd5-8749-4103-ae2c-9ff1297b562e", "fitness": 0.5428206456625653, "name": "AdaptiveGaussianAdaptation", "description": "Adaptive Gaussian mutation with dynamic scaling, covariance adaptation, and a simplified replacement strategy based on fitness improvement.", "code": "import numpy as np\n\nclass AdaptiveGaussianAdaptation:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_scale=0.5, stagnation_threshold=500):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.scale = initial_scale\n        self.scale_decay = 0.995\n        self.min_scale = 0.01\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.covariance = np.eye(dim)\n\n    def __call__(self, func):\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            best_idx = np.argmin(fitness)\n            best_x = population[best_idx]\n\n            new_population = np.random.multivariate_normal(mean=best_x, cov=self.scale**2 * self.covariance, size=self.pop_size)\n            new_population = np.clip(new_population, self.lb, self.ub)\n\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n\n            if np.min(new_fitness) < self.f_opt:\n                self.f_opt = np.min(new_fitness)\n                self.x_opt = new_population[np.argmin(new_fitness)]\n                self.stagnation_counter = 0\n            else:\n                self.stagnation_counter += 1\n\n            # Simplified replacement: replace if better\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    population[i] = new_population[i]\n                    fitness[i] = new_fitness[i]\n\n            self.scale = max(self.scale * self.scale_decay, self.min_scale)\n\n            # Covariance update (simplified)\n            diff = population - np.mean(population, axis=0)\n            self.covariance = 0.9 * self.covariance + 0.1 * np.cov(diff.T) + 1e-9 * np.eye(self.dim)\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.covariance = np.eye(self.dim)\n                self.scale = self.initial_scale\n                self.stagnation_counter = 0\n                if np.min(fitness) < self.f_opt:\n                  self.f_opt = np.min(fitness)\n                  self.x_opt = population[np.argmin(fitness)]\n                self.budget -= self.pop_size\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveGaussianAdaptation scored 0.543 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3b8560be-ba92-4436-9285-a9b5b1ab6438"], "operator": null, "metadata": {"aucs": [0.2334433782007188, 0.16038287923915562, 0.7901176954754916, 0.19878969016022807, 0.8382700886857167, 0.8640806571443803, 0.307582898127309, 0.6936863732729688, 0.8269321457370272, 0.1815014673783688, 0.9309406342334237, 0.9999138158413434, 0.2492576305865697, 0.7940077040519167, 0.5859306547038086, 0.38382022411481587, 0.2582589641802773, 0.9133169574969935, 0.21829870288721254, 0.42788035173358163]}}
{"id": "cae65679-a5c8-42c2-985e-c1f565ebb321", "fitness": 0.41102624326069376, "name": "LevyFlightOptimizer", "description": "A population-based algorithm employing a Lvy flight for exploration and adaptive step size control with a focus on intensification around elite solutions and diversification through orthogonal learning.", "code": "import numpy as np\n\nclass LevyFlightOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, beta=1.5, initial_scale=0.1, elite_fraction=0.2, orthogonal_learning_rate=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.beta = beta  # Lvy flight parameter\n        self.lb = -5.0\n        self.ub = 5.0\n        self.scale = initial_scale  # Initial scale for Lvy flights\n        self.elite_fraction = elite_fraction\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n    def levy_flight(self, size):\n        # Generate steps from a Lvy distribution\n        num = np.random.normal(0, scale=(self.sigma**2), size=size)\n        den = np.random.normal(0, scale=1, size=size)\n        step = num / (np.abs(den)**(1/self.beta))\n        return step\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        self.sigma = (np.math.gamma(1 + self.beta) * np.sin(np.pi * self.beta / 2) / (np.math.gamma((1 + self.beta) / 2) * self.beta * (2**((self.beta - 1) / 2))))**(1 / self.beta)\n\n        while self.budget > 0:\n            # Sort population by fitness\n            sorted_indices = np.argsort(fitness)\n            population = population[sorted_indices]\n            fitness = fitness[sorted_indices]\n\n            # Select elite individuals\n            num_elites = int(self.elite_fraction * self.pop_size)\n            elites = population[:num_elites]\n\n            # Generate new solutions using Lvy flights around elite individuals\n            new_population = np.zeros((self.pop_size, self.dim))\n            for i in range(self.pop_size):\n                # Choose an elite individual randomly\n                elite_idx = np.random.randint(num_elites)\n                elite = elites[elite_idx]\n\n                # Generate Lvy flight step\n                step = self.scale * self.levy_flight(self.dim)\n                new_solution = elite + step\n\n                # Clip new solutions to the search space\n                new_solution = np.clip(new_solution, self.lb, self.ub)\n                new_population[i] = new_solution\n\n            # Orthogonal Learning\n            for i in range(self.pop_size):\n                if np.random.rand() < self.orthogonal_learning_rate:\n                    # Randomly select two dimensions\n                    d1, d2 = np.random.choice(self.dim, size=2, replace=False)\n                    \n                    # Orthogonal direction\n                    orthogonal_direction = np.zeros(self.dim)\n                    orthogonal_direction[d1] = -new_population[i, d2]\n                    orthogonal_direction[d2] = new_population[i, d1]\n                    \n                    # Normalize\n                    norm = np.linalg.norm(orthogonal_direction)\n                    if norm > 0:\n                        orthogonal_direction /= norm\n                        \n                    # Step size\n                    step_size = np.random.uniform(0, self.scale)\n                    \n                    # Update solution\n                    new_solution = new_population[i] + step_size * orthogonal_direction\n                    new_solution = np.clip(new_solution, self.lb, self.ub)\n                    new_population[i] = new_solution\n\n            # Evaluate new solutions\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n\n            # Update best solution\n            best_new_idx = np.argmin(new_fitness)\n            if new_fitness[best_new_idx] < self.f_opt:\n                self.f_opt = new_fitness[best_new_idx]\n                self.x_opt = new_population[best_new_idx]\n\n            # Update population (replace the worst)\n            worst_idx = np.argmax(fitness)\n            population[worst_idx] = new_population[best_new_idx] # replace worst with best of new\n            fitness[worst_idx] = new_fitness[best_new_idx]\n            \n\n            # Adaptive scale adjustment\n            self.scale *= 0.99  # Reduce scale over time\n            self.scale = max(self.scale, 0.001) # Avoid scale going to zero\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm LevyFlightOptimizer scored 0.411 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3b8560be-ba92-4436-9285-a9b5b1ab6438"], "operator": null, "metadata": {"aucs": [0.17467241809415368, 0.1729099756169451, 0.5647366691324511, 0.26216091361714644, 0.2724879770088241, 0.6290446769564895, 0.27371996899615547, 0.49655282361306785, 0.21862724933935151, 0.20144943985062225, 0.8105296276327474, 0.9839175691091535, 0.2418373403085503, 0.20753054348221844, 0.6902637851534128, 0.3079997772955443, 0.2558512348291542, 0.7949390844770743, 0.1849967984249954, 0.476296992275817]}}
{"id": "fb4f29e1-b01a-481c-8635-b0ae4111afef", "fitness": 0.2992107974222794, "name": "AdaptiveLocalGaussianArchive", "description": "Adaptive Gaussian mutation with dynamic scaling, local search, and an archive, simplified by removing population size adjustment and focusing on efficient archive exploitation.", "code": "import numpy as np\n\nclass AdaptiveLocalGaussianArchive:\n    def __init__(self, budget=10000, dim=10, initial_scale=0.5, archive_size=10, local_search_prob=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.scale = initial_scale\n        self.scale_decay = 0.995\n        self.min_scale = 0.01\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.archive_size = archive_size\n        self.archive_x = []\n        self.archive_f = []\n        self.local_search_prob = local_search_prob\n        self.local_search_decay = 0.99\n\n    def update_archive(self, x, f):\n        if len(self.archive_f) < self.archive_size:\n            self.archive_x.append(x)\n            self.archive_f.append(f)\n        else:\n            max_archive_fitness_idx = np.argmax(self.archive_f)\n            if f < self.archive_f[max_archive_fitness_idx]:\n                self.archive_x[max_archive_fitness_idx] = x\n                self.archive_f[max_archive_fitness_idx] = f\n\n    def __call__(self, func):\n        # Initialize solution\n        x = np.random.uniform(self.lb, self.ub, size=self.dim)\n        f = func(x)\n        self.budget -= 1\n        self.f_opt = f\n        self.x_opt = x\n        self.update_archive(x, f)\n\n        while self.budget > 0:\n            # Local search or archive exploitation\n            if np.random.rand() < self.local_search_prob:\n                # Local Search: Gaussian mutation around the current best\n                new_solution = np.random.normal(self.x_opt, self.scale, size=self.dim)\n                new_solution = np.clip(new_solution, self.lb, self.ub)\n            else:\n                # Archive exploitation\n                if len(self.archive_x) > 0:\n                    archive_idx = np.random.randint(len(self.archive_x))\n                    new_solution = np.random.normal(self.archive_x[archive_idx], self.scale, size=self.dim)\n                    new_solution = np.clip(new_solution, self.lb, self.ub)\n                else:\n                    new_solution = np.random.uniform(self.lb, self.ub, size=self.dim)  # Fallback: random sampling\n                    \n            # Evaluate and update\n            new_fitness = func(new_solution)\n            self.budget -= 1\n\n            if new_fitness < self.f_opt:\n                self.f_opt = new_fitness\n                self.x_opt = new_solution\n                self.update_archive(new_solution, new_fitness)\n                \n            # Decay scale and local search prob\n            self.scale = max(self.scale * self.scale_decay, self.min_scale)\n            self.local_search_prob *= self.local_search_decay\n            self.local_search_prob = max(self.local_search_prob, 0.1)\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveLocalGaussianArchive scored 0.299 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["4345f3d7-bebd-4cd5-94bf-5ab5999fe7a1"], "operator": null, "metadata": {"aucs": [0.07391137793623404, 0.1278632574104246, 0.5428347143476642, 0.17414589205687603, 0.1769469508756112, 0.622667541947768, 0.24901264488324393, 0.23097584871446675, 0.31187531528373125, 0.09890443306527519, 0.9069230188225007, 0.14749325658772483, 0.26695796450345743, 0.1960467392083851, 0.8396358477610396, 0.24611093968296494, 0.21756392830811333, 0.19761232753048863, 0.19620650236043424, 0.1605274471591842]}}
{"id": "9cab6e52-6b6c-4511-8cee-620b66abc091", "fitness": 0.3159141986740526, "name": "AdaptiveDifferentialEvolutionMirror", "description": "Simplified adaptive differential evolution with mirrored sampling and step size adaptation, removing covariance matrix adaptation for efficiency.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolutionMirror:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, Cr=0.9, sigma_init=0.1, mirror_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.Cr = Cr\n        self.sigma_init = sigma_init\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mirror_rate = mirror_rate\n\n    def __call__(self, func):\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)].copy()\n\n        sigma = self.sigma_init * (self.ub - self.lb)\n        learning_rate_sigma = 0.1\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n\n                v = x_r1 + self.F * (x_r2 - x_r3) + sigma * np.random.normal(0, 1, self.dim)\n                u = np.zeros(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.Cr:\n                        u[j] = v[j]\n                    else:\n                        u[j] = population[i, j]\n\n                u = np.clip(u, self.lb, self.ub)\n\n                if np.random.rand() < self.mirror_rate:\n                    u = self.x_opt + (self.x_opt - u)\n                    u = np.clip(u, self.lb, self.ub)\n                \n                f_u = func(u)\n                self.budget -= 1\n                \n                if f_u < fitness[i]:\n                    population[i] = u\n                    fitness[i] = f_u\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u.copy()\n\n            success_rate = np.mean(fitness > np.min(fitness))\n            if success_rate > 0.8:\n                sigma *= (1 - learning_rate_sigma) + learning_rate_sigma * 1.2\n            elif success_rate < 0.2:\n                sigma *= (1 - learning_rate_sigma) + learning_rate_sigma * 0.8\n            \n            sigma = np.clip(sigma, self.sigma_init/10, (self.ub - self.lb))\n\n            if np.random.rand() < 0.1:\n                self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n                self.Cr = np.clip(np.random.normal(self.Cr, 0.1), 0.1, 1.0)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDifferentialEvolutionMirror scored 0.316 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["8d050bf6-92d5-425b-b843-0dc5ffa53cb4"], "operator": null, "metadata": {"aucs": [0.13741841701415325, 0.1979517409872894, 0.2855537813047304, 0.2738523361744677, 0.23661055205579573, 0.2492652474777266, 0.27013466138574826, 0.23810919117115814, 0.237783332023773, 0.18198985449841443, 0.2730405189319046, 0.9976501330151403, 0.2649713820178745, 0.2621804119804152, 0.6353251167518825, 0.294857804592906, 0.2656882447243848, 0.3443369639251407, 0.18977242049073706, 0.48179186295741006]}}
{"id": "14664a31-a1e9-4fbb-8bd7-6da7ca2dea27", "fitness": 0.47279222196220994, "name": "AdaptiveLocalGaussianArchiveOrthogonal", "description": "Combines adaptive Gaussian mutation with archive exploitation, orthogonal sampling, and population diversity maintenance to balance exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveLocalGaussianArchiveOrthogonal:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=20, lr=0.1, initial_scale=0.5, success_threshold=0.2, archive_size=10, orthogonal_component=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.lr = lr\n        self.lb = -5.0\n        self.ub = 5.0\n        self.scale = initial_scale\n        self.scale_decay = 0.995\n        self.min_scale = 0.01\n        self.success_threshold = success_threshold\n        self.success_rate = 0.0\n        self.success_counter = 0\n        self.total_iterations = 0\n        self.local_search_prob = 0.9  # Probability of performing local search\n        self.local_search_decay = 0.99  # Decay factor for local search probability\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.archive_size = archive_size\n        self.archive_x = []\n        self.archive_f = []\n        self.orthogonal_component = orthogonal_component #Strength of orthogonal component\n\n    def update_archive(self, x, f):\n        if len(self.archive_f) < self.archive_size:\n            self.archive_x.append(x)\n            self.archive_f.append(f)\n        else:\n            max_archive_fitness_idx = np.argmax(self.archive_f)\n            if f < self.archive_f[max_archive_fitness_idx]:\n                self.archive_x[max_archive_fitness_idx] = x\n                self.archive_f[max_archive_fitness_idx] = f\n\n    def orthogonal_sampling(self, x):\n        # Generate a random orthogonal vector\n        d = self.dim\n        H = np.eye(d)\n        v = np.random.randn(d)\n        v = v / np.linalg.norm(v)\n        H = H - 2 * np.outer(v, v)\n\n        # Apply orthogonal transformation to the solution\n        x_orth = H @ x\n        return x_orth\n\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_idx = np.argmin(fitness)\n        self.f_opt = fitness[best_idx]\n        self.x_opt = population[best_idx]\n\n        self.update_archive(self.x_opt, self.f_opt)\n\n        while self.budget > 0:\n            # Iterate through the population and perform local search with a certain probability\n            for i in range(self.pop_size):\n                if np.random.rand() < self.local_search_prob:\n                    # Generate a new solution around the current individual using Gaussian distribution\n                    new_solution = np.random.normal(population[i], self.scale, size=self.dim)\n                    new_solution = np.clip(new_solution, self.lb, self.ub)\n\n                    # Evaluate the new solution\n                    if self.budget > 0:\n                        new_fitness = func(new_solution)\n                        self.budget -= 1\n                        self.total_iterations += 1\n\n                        # Update best solution\n                        if new_fitness < self.f_opt:\n                            self.f_opt = new_fitness\n                            self.x_opt = new_solution\n                            self.success_counter += 1\n                            self.update_archive(self.x_opt, self.f_opt)\n\n                        # Replace the current individual if the new solution is better\n                        if new_fitness < fitness[i]:\n                            population[i] = new_solution\n                            fitness[i] = new_fitness\n                            self.update_archive(new_solution, new_fitness)\n                else:\n                  # Global search step: random perturbation or exploitation of archive\n                  if len(self.archive_x) > 0 and np.random.rand() < 0.2:  #Exploit the archive with probability 0.2\n                      archive_idx = np.random.randint(len(self.archive_x))\n                      new_solution = np.random.normal(self.archive_x[archive_idx], self.scale, size=self.dim)\n                  else:\n                      new_solution = population[i] + np.random.uniform(-self.scale, self.scale, size=self.dim)\n\n                  # Orthogonal sampling component\n                  orthogonal_vector = self.orthogonal_sampling(population[i])\n                  new_solution = (1 - self.orthogonal_component) * new_solution + self.orthogonal_component * orthogonal_vector\n\n                  new_solution = np.clip(new_solution, self.lb, self.ub)\n\n                  # Evaluate the new solution\n                  if self.budget > 0:\n                      new_fitness = func(new_solution)\n                      self.budget -= 1\n                      self.total_iterations += 1\n\n                      # Update best solution\n                      if new_fitness < self.f_opt:\n                          self.f_opt = new_fitness\n                          self.x_opt = new_solution\n                          self.success_counter += 1\n                          self.update_archive(self.x_opt, self.f_opt)\n\n                      # Replace the current individual if the new solution is better\n                      if new_fitness < fitness[i]:\n                          population[i] = new_solution\n                          fitness[i] = new_fitness\n                          self.update_archive(new_solution, new_fitness)\n\n            # Update success rate\n            if self.total_iterations > 0:\n                self.success_rate = self.success_counter / self.total_iterations\n\n            # Adjust population size based on success rate\n            if self.success_rate > self.success_threshold and self.pop_size < 2 * self.dim:\n                self.pop_size += 1\n                new_individual = np.random.uniform(self.lb, self.ub, size=self.dim)\n                new_fitness = func(new_individual)\n                self.budget -= 1\n                population = np.vstack((population, new_individual))\n                fitness = np.append(fitness, new_fitness)\n                self.update_archive(new_individual, new_fitness)\n            elif self.success_rate < self.success_threshold / 2 and self.pop_size > 10:\n                self.pop_size -= 1\n                worst_idx = np.argmax(fitness)\n                population = np.delete(population, worst_idx, axis=0)\n                fitness = np.delete(fitness, worst_idx)\n\n            # Decay the scale\n            self.scale = max(self.scale * self.scale_decay, self.min_scale)\n\n            # Decay local search probability\n            self.local_search_prob *= self.local_search_decay\n            self.local_search_prob = max(self.local_search_prob, 0.1) # Ensure a minimum probability\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveLocalGaussianArchiveOrthogonal scored 0.473 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["4345f3d7-bebd-4cd5-94bf-5ab5999fe7a1"], "operator": null, "metadata": {"aucs": [0.21522251923418012, 0.3784399843722459, 0.4638931217794203, 0.74895596464294, 0.3972074220566596, 0.48501374708936407, 0.30734241198324075, 0.39542582343722166, 0.4037049636435096, 0.23767406836870264, 0.6866395158675815, 0.9988083802965656, 0.3087209679603573, 0.4046739789775976, 0.7626221422387847, 0.4968156963334467, 0.3920563688786465, 0.5833103233607533, 0.2888189315799685, 0.5004981071430137]}}
{"id": "ffb81665-2265-4d8c-ae60-d2e898356eb3", "fitness": 0.7031990897258314, "name": "AdaptiveDifferentialEvolutionCMA", "description": "Simplified adaptive differential evolution with CMA-inspired updates using a rank-one covariance estimate and mirrored sampling, with reduced parameter adaptation for efficiency.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolutionCMA:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, Cr=0.9, sigma_init=0.1, mirror_rate=0.1, learning_rate_C = 0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.Cr = Cr\n        self.sigma_init = sigma_init\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mirror_rate = mirror_rate\n        self.learning_rate_C = learning_rate_C\n\n    def __call__(self, func):\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)].copy()\n\n        sigma = self.sigma_init * (self.ub - self.lb)\n        C = np.eye(self.dim)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n\n                v = x_r1 + self.F * (x_r2 - x_r3) + sigma * np.random.multivariate_normal(np.zeros(self.dim), C)\n                u = np.where(np.random.rand(self.dim) < self.Cr, v, population[i])\n                u = np.clip(u, self.lb, self.ub)\n\n                if np.random.rand() < self.mirror_rate:\n                    u = self.x_opt + (self.x_opt - u)\n                    u = np.clip(u, self.lb, self.ub)\n                \n                f_u = func(u)\n                self.budget -= 1\n                \n                if f_u < fitness[i]:\n                    population[i] = u\n                    fitness[i] = f_u\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u.copy()\n\n            diff = population[np.argmin(fitness)] - self.x_opt\n            C = (1 - self.learning_rate_C) * C + self.learning_rate_C * np.outer(diff, diff)\n            sigma *= np.exp(0.5 * (np.mean(fitness > np.min(fitness)) - 0.5) / 0.2)  # Simplified step-size adaptation\n            sigma = np.clip(sigma, self.sigma_init/10, (self.ub - self.lb)) #prevent sigma from going to 0\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDifferentialEvolutionCMA scored 0.703 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["8d050bf6-92d5-425b-b843-0dc5ffa53cb4"], "operator": null, "metadata": {"aucs": [0.4719162116302934, 0.742427400663545, 0.7287669914547219, 0.8512673110000225, 0.7387469845615802, 0.7630584915811841, 0.683833213906923, 0.6764026729598096, 0.7349376719661089, 0.7102839334025093, 0.8397829162160654, 0.9948674913325918, 0.4028879713966137, 0.7377207783740662, 0.9078603557431287, 0.7621225871184961, 0.6961818234434412, 0.8170970396742214, 0.28181875100309384, 0.5220011970882127]}}
{"id": "7fb7e90b-f4de-4c59-a8ae-7beabdee8b48", "fitness": 0.1922042179374682, "name": "AdaptiveOrthogonalCMA", "description": "Population-based algorithm with dynamic Gaussian mutation, covariance matrix adaptation via rank-one update, and orthogonal learning to enhance diversity and convergence.", "code": "import numpy as np\n\nclass AdaptiveOrthogonalCMA:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=20, lr=0.1, initial_scale=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.lr = lr\n        self.lb = -5.0\n        self.ub = 5.0\n        self.scale = initial_scale\n        self.scale_decay = 0.995\n        self.min_scale = 0.01\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.C = np.eye(dim)  # Covariance matrix\n        self.C_rank_one = np.zeros((dim, dim)) # Rank-one update matrix for C\n\n    def orthogonal_learning(self, population, fitness):\n        \"\"\"\n        Performs orthogonal learning to improve population diversity and convergence.\n        \"\"\"\n        # Select two random individuals\n        idx1, idx2 = np.random.choice(len(population), 2, replace=False)\n        x1, x2 = population[idx1], population[idx2]\n\n        # Create an orthogonal direction\n        direction = x2 - x1\n\n        # Generate a new solution along the orthogonal direction\n        alpha = np.random.uniform(-self.scale, self.scale)  # Adapt range\n        new_solution = x1 + alpha * direction\n        new_solution = np.clip(new_solution, self.lb, self.ub)\n        return new_solution\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_idx = np.argmin(fitness)\n        self.f_opt = fitness[best_idx]\n        self.x_opt = population[best_idx]\n\n        mean = self.x_opt # Initialize mean\n\n        while self.budget > 0:\n            # Generate new population using CMA-ES-like sampling\n            z = np.random.multivariate_normal(np.zeros(self.dim), self.C, size=self.pop_size)\n            new_population = mean + self.scale * z\n            new_population = np.clip(new_population, self.lb, self.ub)\n\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n\n            # Update best solution\n            best_idx = np.argmin(new_fitness)\n            if new_fitness[best_idx] < self.f_opt:\n                self.f_opt = new_fitness[best_idx]\n                self.x_opt = new_population[best_idx]\n            \n            # Select top individuals for updating the mean and covariance matrix\n            elite_indices = np.argsort(new_fitness)[:self.pop_size // 2]\n            elite_solutions = new_population[elite_indices]\n            \n            # Update mean\n            old_mean = mean\n            mean = np.mean(elite_solutions, axis=0)\n\n            # Rank-one update of covariance matrix\n            delta = (mean - old_mean).reshape(-1, 1)\n            self.C_rank_one = (1 - self.lr) * self.C_rank_one + self.lr * delta @ delta.T\n            self.C = self.C_rank_one + np.eye(self.dim) * 1e-6\n\n            # Orthogonal learning\n            for i in range(self.pop_size):\n                if self.budget > 0:\n                    orthogonal_sol = self.orthogonal_learning(new_population, new_fitness)\n                    orthogonal_fitness = func(orthogonal_sol)\n                    self.budget -= 1\n                    if orthogonal_fitness < self.f_opt:\n                        self.f_opt = orthogonal_fitness\n                        self.x_opt = orthogonal_sol\n\n            # Decay the scale\n            self.scale = max(self.scale * self.scale_decay, self.min_scale)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveOrthogonalCMA scored 0.192 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["4345f3d7-bebd-4cd5-94bf-5ab5999fe7a1"], "operator": null, "metadata": {"aucs": [0.08997517763302276, 0.1914008244409826, 0.2382460804290989, 0.1550544523008588, 0.12028727289892127, 0.21114547705346853, 0.21711154269464727, 0.18284225887845706, 0.19274030124887875, 0.12917310682905025, 0.1993474495185733, 0.18656097421507467, 0.27089166288952193, 0.15453996062705666, 0.13657166780631902, 0.2518402014913511, 0.1814889077447598, 0.17044083478733796, 0.12862121076265454, 0.4358049944993284]}}
{"id": "b741e8ef-6c84-4206-9751-0f22fee04022", "fitness": 0.5505829499161606, "name": "HybridGaussianDifferentialEvolution", "description": "A population-based algorithm employing a combination of Gaussian mutation, differential evolution, and a selection mechanism biased towards diversity to balance exploration and exploitation.", "code": "import numpy as np\n\nclass HybridGaussianDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, CR=0.7, initial_scale=0.5, diversity_weight=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Differential evolution parameter\n        self.CR = CR  # Crossover rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.scale = initial_scale  # Initial scale for Gaussian mutation\n        self.scale_decay = 0.995\n        self.min_scale = 0.01\n        self.diversity_weight = diversity_weight\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Gaussian Mutation\n                gaussian_mutant = population[i] + self.scale * np.random.normal(0, 1, self.dim)\n                gaussian_mutant = np.clip(gaussian_mutant, self.lb, self.ub)\n\n                # Differential Evolution\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = population[a] + self.F * (population[b] - population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluate trial solution\n                f_trial = func(trial)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                # Diversity metric (Euclidean distance to nearest neighbor)\n                distances = np.linalg.norm(population - population[i], axis=1)\n                distances[i] = np.inf  # Exclude self-distance\n                diversity = np.min(distances)\n\n                # Selection (biased towards diversity)\n                fitness_improvement = fitness[i] - f_trial\n                selection_probability = fitness_improvement + self.diversity_weight * diversity\n                if selection_probability > 0:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Decay the scale\n            self.scale = max(self.scale * self.scale_decay, self.min_scale)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm HybridGaussianDifferentialEvolution scored 0.551 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3b8560be-ba92-4436-9285-a9b5b1ab6438"], "operator": null, "metadata": {"aucs": [0.44623242867633817, 0.5675151009653381, 0.38861032173015153, 0.7043139199776944, 0.6114872510053266, 0.4939305489153706, 0.6595708739692729, 0.646564938218684, 0.6142184336812779, 0.6472122828959712, 0.5251879057414772, 0.9994049301937401, 0.33677317966365805, 0.6100609639383159, 0.6087125192671349, 0.38452358235368334, 0.5576982063618192, 0.49202259197649045, 0.268637433209583, 0.4489815855818835]}}
{"id": "4633fbe2-7a83-412d-953d-897df842c7cf", "fitness": 0.5204714350456718, "name": "AdaptiveGaussianAdaptation", "description": "Simplified adaptive Gaussian mutation with dynamic scaling, covariance adaptation using rank-one updates, and a threshold-based global restart for faster adaptation.", "code": "import numpy as np\n\nclass AdaptiveGaussianAdaptation:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_scale=0.5, stagnation_threshold=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.scale = initial_scale\n        self.scale_decay = 0.99\n        self.min_scale = 0.01\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.covariance = np.eye(dim)\n        self.mean = np.zeros(dim)\n\n    def __call__(self, func):\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n        best_x = self.x_opt.copy()\n\n        while self.budget > 0:\n            new_population = np.random.multivariate_normal(mean=best_x, cov=self.scale**2 * self.covariance, size=self.pop_size)\n            new_population = np.clip(new_population, self.lb, self.ub)\n\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n\n            best_new_idx = np.argmin(new_fitness)\n            if new_fitness[best_new_idx] < self.f_opt:\n                self.f_opt = new_fitness[best_new_idx]\n                self.x_opt = new_population[best_new_idx]\n                self.stagnation_counter = 0\n                best_x = self.x_opt.copy()\n            else:\n                self.stagnation_counter += 1\n\n            # Simplified replacement: replace if better\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    population[i] = new_population[i]\n                    fitness[i] = new_fitness[i]\n\n            self.scale = max(self.scale * self.scale_decay, self.min_scale)\n\n            # Covariance update (simplified rank-one)\n            diff = best_x - self.mean\n            self.mean = 0.9 * self.mean + 0.1 * best_x\n            self.covariance = 0.9 * self.covariance + 0.1 * (diff[:, None] @ diff[None, :]) + 1e-9 * np.eye(self.dim)\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                best_idx = np.argmin(fitness)\n                best_x = population[best_idx]\n                self.covariance = np.eye(self.dim)\n                self.scale = 0.5 #self.initial_scale\n                self.stagnation_counter = 0\n                if np.min(fitness) < self.f_opt:\n                  self.f_opt = np.min(fitness)\n                  self.x_opt = population[np.argmin(fitness)]\n                self.budget -= self.pop_size\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveGaussianAdaptation scored 0.520 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3deeedd5-8749-4103-ae2c-9ff1297b562e"], "operator": null, "metadata": {"aucs": [0.1915897717312689, 0.23148333684106992, 0.8383927046067037, 0.17109711640103964, 0.46678434355829945, 0.8591815128203767, 0.28115275197910883, 0.6368330428160139, 0.8410793983727373, 0.2087463070352965, 0.9413080441198812, 0.9990873807795239, 0.2747442503216002, 0.24171526357912632, 0.8462475046826705, 0.33531732459992236, 0.449647131991471, 0.8888923107456986, 0.25668193376886705, 0.4494472701627633]}}
{"id": "f4166f71-544a-4ddb-9643-77846f5ddb3e", "fitness": 0.4829885618786622, "name": "SimplifiedAdaptiveGaussianArchiveOrthogonal", "description": "Simplified Adaptive Gaussian Mutation with Archive and Orthogonal sampling, streamlining parameter adaptation and search strategy for efficiency.", "code": "import numpy as np\n\nclass SimplifiedAdaptiveGaussianArchiveOrthogonal:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=10, lr=0.1, initial_scale=0.5, archive_size=5, orthogonal_component=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.lr = lr\n        self.lb = -5.0\n        self.ub = 5.0\n        self.scale = initial_scale\n        self.scale_decay = 0.995\n        self.min_scale = 0.01\n        self.archive_size = archive_size\n        self.archive_x = []\n        self.archive_f = []\n        self.orthogonal_component = orthogonal_component\n\n    def update_archive(self, x, f):\n        if len(self.archive_f) < self.archive_size:\n            self.archive_x.append(x)\n            self.archive_f.append(f)\n        else:\n            max_archive_fitness_idx = np.argmax(self.archive_f)\n            if f < self.archive_f[max_archive_fitness_idx]:\n                self.archive_x[max_archive_fitness_idx] = x\n                self.archive_f[max_archive_fitness_idx] = f\n\n    def orthogonal_sampling(self, x):\n        d = self.dim\n        H = np.eye(d)\n        v = np.random.randn(d)\n        v = v / np.linalg.norm(v)\n        H = H - 2 * np.outer(v, v)\n        x_orth = H @ x\n        return x_orth\n\n    def __call__(self, func):\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_idx = np.argmin(fitness)\n        f_opt = fitness[best_idx]\n        x_opt = population[best_idx]\n        self.update_archive(x_opt, f_opt)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                if len(self.archive_x) > 0 and np.random.rand() < 0.2:\n                    archive_idx = np.random.randint(len(self.archive_x))\n                    new_solution = np.random.normal(self.archive_x[archive_idx], self.scale, size=self.dim)\n                else:\n                    new_solution = np.random.normal(population[i], self.scale, size=self.dim)\n\n                if np.random.rand() < self.orthogonal_component:\n                    orthogonal_vector = self.orthogonal_sampling(population[i])\n                    new_solution = orthogonal_vector #Pure orthogonal update, better exploration\n\n                new_solution = np.clip(new_solution, self.lb, self.ub)\n\n                if self.budget > 0:\n                    new_fitness = func(new_solution)\n                    self.budget -= 1\n\n                    if new_fitness < f_opt:\n                        f_opt = new_fitness\n                        x_opt = new_solution\n                        self.update_archive(x_opt, f_opt)\n\n                    if new_fitness < fitness[i]:\n                        population[i] = new_solution\n                        fitness[i] = new_fitness\n                        self.update_archive(new_solution, new_fitness)\n\n            self.scale = max(self.scale * self.scale_decay, self.min_scale)\n\n        return f_opt, x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm SimplifiedAdaptiveGaussianArchiveOrthogonal scored 0.483 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["14664a31-a1e9-4fbb-8bd7-6da7ca2dea27"], "operator": null, "metadata": {"aucs": [0.256987456921574, 0.49659653124665715, 0.46970458491423783, 0.7630034852297111, 0.42073618365105303, 0.5053235102312649, 0.2997851826828982, 0.4019347228221605, 0.4247630474223931, 0.20687903365898141, 0.7417207558112844, 0.9824464036608188, 0.3079794701755021, 0.4346753446753683, 0.847238672047519, 0.3495090705449354, 0.41156010714880875, 0.6158867728408997, 0.19712124192887803, 0.5259196599582978]}}
{"id": "076e6b42-cb4f-4a2e-ba27-bae6afcdc86e", "fitness": 0.4284794443634028, "name": "CauchyDifferentialEvolutionArchive", "description": "Combines differential evolution with a Cauchy mutation operator and an archive of successful solutions for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass CauchyDifferentialEvolutionArchive:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, CR=0.7, archive_size=10, cauchy_scale=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Differential evolution parameter\n        self.CR = CR  # Crossover rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.archive_size = archive_size\n        self.archive = []\n        self.cauchy_scale = cauchy_scale\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Differential Evolution\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant_de = population[a] + self.F * (population[b] - population[c])\n\n                # Cauchy Mutation\n                mutant_cauchy = population[i] + self.cauchy_scale * np.random.standard_cauchy(size=self.dim)\n\n                mutant = np.clip((mutant_de + mutant_cauchy)/2, self.lb, self.ub)  #averaging two mutants\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluate trial solution\n                f_trial = func(trial)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                if f_trial < fitness[i]:\n                    # Replace individual\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                    else:\n                        # Replace oldest member\n                        self.archive[np.random.randint(0, self.archive_size)] = trial\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm CauchyDifferentialEvolutionArchive scored 0.428 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b741e8ef-6c84-4206-9751-0f22fee04022"], "operator": null, "metadata": {"aucs": [0.2054057900913563, 0.2024196153531117, 0.45180277759780474, 0.20483792967168857, 0.36121371406341407, 0.5374305660497363, 0.3260847940982957, 0.37947394240606214, 0.4101335164097174, 0.18575847435740844, 0.687171756852606, 0.9924630979635357, 0.2834443983301894, 0.4296842633519756, 0.7323611212193044, 0.4434642934170572, 0.3746993788737586, 0.6205249350140274, 0.2458988706494768, 0.49531565149752965]}}
{"id": "b637d349-9d6c-4244-9344-28042da16810", "fitness": 0.7529304796124971, "name": "SimplifiedHybridDE", "description": "Simplified hybrid algorithm using Gaussian mutation and differential evolution with adaptive scaling and a greedy selection.", "code": "import numpy as np\n\nclass SimplifiedHybridDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, CR=0.7, initial_scale=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.lb = -5.0\n        self.ub = 5.0\n        self.scale = initial_scale\n        self.scale_decay = 0.995\n        self.min_scale = 0.01\n\n    def __call__(self, func):\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Gaussian Mutation\n                gaussian_mutant = population[i] + self.scale * np.random.normal(0, 1, self.dim)\n                gaussian_mutant = np.clip(gaussian_mutant, self.lb, self.ub)\n\n                # Differential Evolution\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = population[a] + self.F * (population[b] - population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluate trial solution\n                f_trial = func(trial)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                # Greedy Selection\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            self.scale = max(self.scale * self.scale_decay, self.min_scale)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm SimplifiedHybridDE scored 0.753 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b741e8ef-6c84-4206-9751-0f22fee04022"], "operator": null, "metadata": {"aucs": [0.4550193555236085, 0.7824285416346546, 0.7489006562200011, 0.9197839605080624, 0.8753267457261895, 0.8998320881065658, 0.8185457276092936, 0.8229042354792527, 0.8828244898597101, 0.2747027252578894, 0.9044257164639429, 0.9933876466858801, 0.2447343127497944, 0.8929402808651977, 0.8628329775510344, 0.8883721692055866, 0.7670363063347995, 0.9186429641507683, 0.33053479447596334, 0.7754338978417455]}}
{"id": "3e21ba62-95db-4515-aa01-fa02b4d75b6b", "fitness": 0.5495736234400732, "name": "SimplifiedHybridDE", "description": "Simplified hybrid Gaussian differential evolution with dynamic scaling and adaptive diversity, focusing on efficiency and parameter reduction.", "code": "import numpy as np\n\nclass SimplifiedHybridDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, CR=0.7, initial_scale=0.3, diversity_weight=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.lb = -5.0\n        self.ub = 5.0\n        self.scale = initial_scale\n        self.scale_decay = 0.99\n        self.min_scale = 0.01\n        self.diversity_weight = diversity_weight\n\n    def __call__(self, func):\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Differential Evolution Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = population[a] + self.F * (population[b] - population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Gaussian perturbation\n                mutant = mutant + self.scale * np.random.normal(0, 1, self.dim)\n                mutant = np.clip(mutant, self.lb, self.ub)\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluate and selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Scale adaptation\n            self.scale = max(self.scale * self.scale_decay, self.min_scale)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm SimplifiedHybridDE scored 0.550 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b741e8ef-6c84-4206-9751-0f22fee04022"], "operator": null, "metadata": {"aucs": [0.23880782358428243, 0.5205669570628875, 0.4952743490854261, 0.8571700942371614, 0.47911286666538877, 0.5510788199244829, 0.36507682501688754, 0.43133665926837716, 0.4823742827762487, 0.4299416955508576, 0.8045942201635116, 0.992997341058189, 0.36247761754057983, 0.4816708237741164, 0.8967374771741139, 0.578465544766548, 0.4428222715541241, 0.6717932973182542, 0.31200630712647526, 0.5971671951535529]}}
{"id": "9655c98e-7cc8-407f-aa2a-2d95ae116a30", "fitness": 0.43648832854028186, "name": "SimplifiedHybridDE", "description": "Simplified hybrid algorithm using differential evolution with adaptive scaling and Gaussian mutation, focusing on efficiency and reducing parameter complexity.", "code": "import numpy as np\n\nclass SimplifiedHybridDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, CR=0.7, initial_scale=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.lb = -5.0\n        self.ub = 5.0\n        self.scale = initial_scale\n        self.scale_decay = 0.995\n        self.min_scale = 0.01\n\n    def __call__(self, func):\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Differential Evolution Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = population[a] + self.F * (population[b] - population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Gaussian Local Search\n                trial = trial + self.scale * np.random.normal(0, 1, self.dim)\n                trial = np.clip(trial, self.lb, self.ub)\n\n                # Evaluate trial solution\n                f_trial = func(trial)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                # Selection\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Adaptive Scaling\n            self.scale = max(self.scale * self.scale_decay, self.min_scale)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm SimplifiedHybridDE scored 0.436 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b741e8ef-6c84-4206-9751-0f22fee04022"], "operator": null, "metadata": {"aucs": [0.20076325358993852, 0.3633637782173612, 0.40546545497790465, 0.6256710888960098, 0.3353902363241711, 0.4289363293744005, 0.3045637772042329, 0.3545825172586745, 0.3412432013269925, 0.2720487828824333, 0.5613623358251876, 0.9965819488885249, 0.3125746629294638, 0.3406742435265605, 0.8025296190288322, 0.41797141885380684, 0.35822100938886137, 0.5109383687040077, 0.28595075011783433, 0.5109337934904384]}}
{"id": "fbe522bb-a232-4578-b206-0f4ac7c20318", "fitness": 0.4470612238028613, "name": "SimplifiedAdaptiveLocalGaussian", "description": "Simplified adaptive Gaussian mutation with archive exploitation and orthogonal sampling, focusing on efficient parameter updates and reduced complexity for better performance.", "code": "import numpy as np\n\nclass SimplifiedAdaptiveLocalGaussian:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=10, lr=0.1, initial_scale=0.5, archive_size=5, orthogonal_component=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.lr = lr\n        self.lb = -5.0\n        self.ub = 5.0\n        self.scale = initial_scale\n        self.scale_decay = 0.99\n        self.min_scale = 0.01\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.archive_size = archive_size\n        self.archive_x = []\n        self.archive_f = []\n        self.orthogonal_component = orthogonal_component\n\n    def update_archive(self, x, f):\n        if len(self.archive_f) < self.archive_size:\n            self.archive_x.append(x)\n            self.archive_f.append(f)\n        else:\n            max_archive_fitness_idx = np.argmax(self.archive_f)\n            if f < self.archive_f[max_archive_fitness_idx]:\n                self.archive_x[max_archive_fitness_idx] = x\n                self.archive_f[max_archive_fitness_idx] = f\n\n    def orthogonal_sampling(self, x):\n        d = self.dim\n        H = np.eye(d)\n        v = np.random.randn(d)\n        v = v / np.linalg.norm(v)\n        H = H - 2 * np.outer(v, v)\n        x_orth = H @ x\n        return x_orth\n\n    def __call__(self, func):\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_idx = np.argmin(fitness)\n        self.f_opt = fitness[best_idx]\n        self.x_opt = population[best_idx]\n\n        self.update_archive(self.x_opt, self.f_opt)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Archive exploitation or Gaussian mutation\n                if len(self.archive_x) > 0 and np.random.rand() < 0.3:\n                    archive_idx = np.random.randint(len(self.archive_x))\n                    new_solution = np.random.normal(self.archive_x[archive_idx], self.scale, size=self.dim)\n                else:\n                    new_solution = np.random.normal(population[i], self.scale, size=self.dim)\n\n                # Orthogonal sampling\n                new_solution = (1 - self.orthogonal_component) * new_solution + self.orthogonal_component * self.orthogonal_sampling(population[i])\n                new_solution = np.clip(new_solution, self.lb, self.ub)\n\n                if self.budget > 0:\n                    new_fitness = func(new_solution)\n                    self.budget -= 1\n\n                    if new_fitness < self.f_opt:\n                        self.f_opt = new_fitness\n                        self.x_opt = new_solution\n                        self.update_archive(self.x_opt, self.f_opt)\n\n                    if new_fitness < fitness[i]:\n                        population[i] = new_solution\n                        fitness[i] = new_fitness\n                        self.update_archive(new_solution, new_fitness)\n\n            self.scale = max(self.scale * self.scale_decay, self.min_scale)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm SimplifiedAdaptiveLocalGaussian scored 0.447 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["14664a31-a1e9-4fbb-8bd7-6da7ca2dea27"], "operator": null, "metadata": {"aucs": [0.15110068163947232, 0.3431602367114005, 0.48107446356596173, 0.19881080619238323, 0.4287825280975045, 0.5036361575527055, 0.31991039309906677, 0.42032696807157055, 0.41814364634585877, 0.23956353681452724, 0.8358691951149781, 0.9917997090081911, 0.2871127682402145, 0.36589913287788445, 0.8117205312126632, 0.3457523736412894, 0.4194240324305172, 0.6402187223580389, 0.23430854173209426, 0.5046100513509029]}}
{"id": "5aeeece0-d60b-46b1-a17d-0947b9aefc88", "fitness": 0.44224336898043537, "name": "AdaptiveLocalGaussianArchiveOrthogonalV2", "description": "Enhanced adaptive Gaussian mutation with archive, orthogonal sampling, dynamic population size, and a simplified rank-based update for improved exploration and exploitation balance.", "code": "import numpy as np\n\nclass AdaptiveLocalGaussianArchiveOrthogonalV2:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=20, lr=0.1, initial_scale=0.5, success_threshold=0.2, archive_size=10, orthogonal_component=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.lr = lr\n        self.lb = -5.0\n        self.ub = 5.0\n        self.scale = initial_scale\n        self.scale_decay = 0.995\n        self.min_scale = 0.01\n        self.success_threshold = success_threshold\n        self.success_rate = 0.0\n        self.success_counter = 0\n        self.total_iterations = 0\n        self.local_search_prob = 0.9  # Probability of performing local search\n        self.local_search_decay = 0.99  # Decay factor for local search probability\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.archive_size = archive_size\n        self.archive_x = []\n        self.archive_f = []\n        self.orthogonal_component = orthogonal_component #Strength of orthogonal component\n\n    def update_archive(self, x, f):\n        if len(self.archive_f) < self.archive_size:\n            self.archive_x.append(x)\n            self.archive_f.append(f)\n        else:\n            max_archive_fitness_idx = np.argmax(self.archive_f)\n            if f < self.archive_f[max_archive_fitness_idx]:\n                self.archive_x[max_archive_fitness_idx] = x\n                self.archive_f[max_archive_fitness_idx] = f\n\n    def orthogonal_sampling(self, x):\n        # Generate a random orthogonal vector\n        d = self.dim\n        H = np.eye(d)\n        v = np.random.randn(d)\n        v = v / np.linalg.norm(v)\n        H = H - 2 * np.outer(v, v)\n\n        # Apply orthogonal transformation to the solution\n        x_orth = H @ x\n        return x_orth\n\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_idx = np.argmin(fitness)\n        self.f_opt = fitness[best_idx]\n        self.x_opt = population[best_idx]\n\n        self.update_archive(self.x_opt, self.f_opt)\n\n        while self.budget > 0:\n            # Sort population based on fitness (rank-based selection)\n            ranked_indices = np.argsort(fitness)\n            population = population[ranked_indices]\n            fitness = fitness[ranked_indices]\n\n            # Iterate through the population and perform local search with a certain probability\n            for i in range(self.pop_size):\n                if np.random.rand() < self.local_search_prob:\n                    # Generate a new solution around the current individual using Gaussian distribution\n                    new_solution = np.random.normal(population[i], self.scale, size=self.dim)\n                    new_solution = np.clip(new_solution, self.lb, self.ub)\n\n                    # Evaluate the new solution\n                    if self.budget > 0:\n                        new_fitness = func(new_solution)\n                        self.budget -= 1\n                        self.total_iterations += 1\n\n                        # Update best solution\n                        if new_fitness < self.f_opt:\n                            self.f_opt = new_fitness\n                            self.x_opt = new_solution\n                            self.success_counter += 1\n                            self.update_archive(self.x_opt, self.f_opt)\n\n                        # Replace the current individual if the new solution is better\n                        if new_fitness < fitness[i]:\n                            population[i] = new_solution\n                            fitness[i] = new_fitness\n                            self.update_archive(new_solution, new_fitness)\n                else:\n                  # Global search step: random perturbation or exploitation of archive\n                  if len(self.archive_x) > 0 and np.random.rand() < 0.2:  #Exploit the archive with probability 0.2\n                      archive_idx = np.random.randint(len(self.archive_x))\n                      new_solution = np.random.normal(self.archive_x[archive_idx], self.scale, size=self.dim)\n                  else:\n                      new_solution = population[i] + np.random.uniform(-self.scale, self.scale, size=self.dim)\n\n                  # Orthogonal sampling component\n                  orthogonal_vector = self.orthogonal_sampling(population[i])\n                  new_solution = (1 - self.orthogonal_component) * new_solution + self.orthogonal_component * orthogonal_vector\n\n                  new_solution = np.clip(new_solution, self.lb, self.ub)\n\n                  # Evaluate the new solution\n                  if self.budget > 0:\n                      new_fitness = func(new_solution)\n                      self.budget -= 1\n                      self.total_iterations += 1\n\n                      # Update best solution\n                      if new_fitness < self.f_opt:\n                          self.f_opt = new_fitness\n                          self.x_opt = new_solution\n                          self.success_counter += 1\n                          self.update_archive(self.x_opt, self.f_opt)\n\n                      # Replace the current individual if the new solution is better\n                      if new_fitness < fitness[i]:\n                          population[i] = new_solution\n                          fitness[i] = new_fitness\n                          self.update_archive(new_solution, new_fitness)\n\n            # Update success rate\n            if self.total_iterations > 0:\n                self.success_rate = self.success_counter / self.total_iterations\n\n            # Adjust population size based on success rate\n            if self.success_rate > self.success_threshold and self.pop_size < 2 * self.dim:\n                self.pop_size += 1\n                new_individual = np.random.uniform(self.lb, self.ub, size=self.dim)\n                new_fitness = func(new_individual)\n                self.budget -= 1\n                population = np.vstack((population, new_individual))\n                fitness = np.append(fitness, new_fitness)\n                self.update_archive(new_individual, new_fitness)\n            elif self.success_rate < self.success_threshold / 2 and self.pop_size > 10:\n                self.pop_size -= 1\n                worst_idx = np.argmax(fitness)\n                population = np.delete(population, worst_idx, axis=0)\n                fitness = np.delete(fitness, worst_idx)\n\n            # Decay the scale\n            self.scale = max(self.scale * self.scale_decay, self.min_scale)\n\n            # Decay local search probability\n            self.local_search_prob *= self.local_search_decay\n            self.local_search_prob = max(self.local_search_prob, 0.1) # Ensure a minimum probability\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveLocalGaussianArchiveOrthogonalV2 scored 0.442 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["14664a31-a1e9-4fbb-8bd7-6da7ca2dea27"], "operator": null, "metadata": {"aucs": [0.2273768082558567, 0.41867279039457195, 0.45597624313423113, 0.19576379477413375, 0.3814075839793335, 0.48320940664477763, 0.32980643477968785, 0.3868354193531821, 0.38748447682637766, 0.22827389856256386, 0.7313119349604404, 0.987063468374505, 0.28070583250397463, 0.3731980593058102, 0.774502624091432, 0.4838895310023754, 0.3805633112947474, 0.5919669956370981, 0.24693985854342582, 0.4999189071901815]}}
{"id": "1b7839b3-bee7-4cc0-a75d-e6f2dce3beac", "fitness": 0.4613915653835596, "name": "EnhancedHybridGaussianDifferentialEvolution", "description": "Enhanced Hybrid Gaussian Differential Evolution with adaptive parameter control, improved diversity maintenance using a crowding distance, and a restart mechanism.", "code": "import numpy as np\n\nclass EnhancedHybridGaussianDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, CR=0.7, initial_scale=0.5, diversity_weight=0.2, restart_prob=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Differential evolution parameter\n        self.CR = CR  # Crossover rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.scale = initial_scale  # Initial scale for Gaussian mutation\n        self.scale_decay = 0.995\n        self.min_scale = 0.01\n        self.diversity_weight = diversity_weight\n        self.restart_prob = restart_prob  # Probability of restarting a solution\n        self.stagnation_counter = 0\n        self.max_stagnation = 50  # Maximum stagnation before increasing diversity weight\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n        \n        best_fitness_history = [self.f_opt]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Gaussian Mutation\n                gaussian_mutant = population[i] + self.scale * np.random.normal(0, 1, self.dim)\n                gaussian_mutant = np.clip(gaussian_mutant, self.lb, self.ub)\n\n                # Differential Evolution\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = population[a] + self.F * (population[b] - population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluate trial solution\n                f_trial = func(trial)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                # Crowding Distance (Diversity Metric)\n                crowding_distance = self.calculate_crowding_distance(population, i)\n\n                # Selection (biased towards diversity)\n                fitness_improvement = fitness[i] - f_trial\n                selection_probability = fitness_improvement + self.diversity_weight * crowding_distance\n                if selection_probability > 0:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.stagnation_counter = 0  # Reset stagnation counter\n                else:\n                    self.stagnation_counter += 1\n\n                # Restart mechanism\n                if np.random.rand() < self.restart_prob:\n                    population[i] = np.random.uniform(self.lb, self.ub, self.dim)\n                    fitness[i] = func(population[i])\n                    self.budget -= 1\n                    if self.budget <= 0:\n                        break\n                    if fitness[i] < self.f_opt:\n                        self.f_opt = fitness[i]\n                        self.x_opt = population[i]\n                        self.stagnation_counter = 0\n\n            # Decay the scale\n            self.scale = max(self.scale * self.scale_decay, self.min_scale)\n\n            # Adaptive diversity weight\n            if self.stagnation_counter > self.max_stagnation:\n                self.diversity_weight *= 1.1  # Increase diversity weight\n                self.stagnation_counter = 0  # Reset stagnation counter\n            else:\n                self.diversity_weight *= 0.99 # Decrease diversity weight\n                \n            # Store the best fitness\n            best_fitness_history.append(self.f_opt)\n            \n            # Adaptive F and CR parameters\n            self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 0.9)  # Adjust F\n            self.CR = np.clip(np.random.normal(0.7, 0.1), 0.1, 0.9)  # Adjust CR\n\n        return self.f_opt, self.x_opt\n\n    def calculate_crowding_distance(self, population, index):\n        \"\"\"Calculates the crowding distance for a given individual.\"\"\"\n        distances = np.linalg.norm(population - population[index], axis=1)\n        distances[index] = np.inf  # Exclude self-distance\n        return np.min(distances)", "configspace": "", "generation": 6, "feedback": "The algorithm EnhancedHybridGaussianDifferentialEvolution scored 0.461 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b741e8ef-6c84-4206-9751-0f22fee04022"], "operator": null, "metadata": {"aucs": [0.21959192527298232, 0.4018210177837753, 0.3598585502669217, 0.4899036867778418, 0.5808410045642236, 0.4830061534093135, 0.31682517707575886, 0.44284069792833813, 0.5469777319633855, 0.43602520314736104, 0.420189184250638, 0.9950757292849, 0.31816110927045227, 0.5283183156737989, 0.6555767600767419, 0.39176917546004353, 0.3860455777605213, 0.4518506050739661, 0.3466687325479818, 0.45648497008224564]}}
{"id": "d81986b4-b21b-4814-85a6-ff83313c8616", "fitness": 0.4129238160772456, "name": "AdaptiveDifferentialEvolutionCMA", "description": "Integrates a Cauchy mutation operator with adaptive step size control and orthogonal learning for enhanced exploration in challenging landscapes.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolutionCMA:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, Cr=0.9, sigma_init=0.1, mirror_rate=0.1, learning_rate_C = 0.1, cauchy_scale=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.Cr = Cr\n        self.sigma_init = sigma_init\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mirror_rate = mirror_rate\n        self.learning_rate_C = learning_rate_C\n        self.cauchy_scale = cauchy_scale  # Scale for Cauchy distribution\n\n    def __call__(self, func):\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)].copy()\n\n        sigma = self.sigma_init * (self.ub - self.lb)\n        C = np.eye(self.dim)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n\n                # Cauchy mutation\n                cauchy_mutation = self.cauchy_scale * np.random.standard_cauchy(size=self.dim)\n                v = x_r1 + self.F * (x_r2 - x_r3) + sigma * np.random.multivariate_normal(np.zeros(self.dim), C) + cauchy_mutation\n\n                u = np.where(np.random.rand(self.dim) < self.Cr, v, population[i])\n                u = np.clip(u, self.lb, self.ub)\n\n                if np.random.rand() < self.mirror_rate:\n                    u = self.x_opt + (self.x_opt - u)\n                    u = np.clip(u, self.lb, self.ub)\n                \n                # Orthogonal learning\n                if np.random.rand() < 0.1:  # Probability of applying orthogonal learning\n                    direction = u - population[i]\n                    orthogonal_point = population[i] - np.dot(direction, self.x_opt - population[i]) / np.linalg.norm(direction)**2 * direction\n                    u = orthogonal_point\n                    u = np.clip(u, self.lb, self.ub)\n                \n                f_u = func(u)\n                self.budget -= 1\n                \n                if f_u < fitness[i]:\n                    population[i] = u\n                    fitness[i] = f_u\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u.copy()\n\n            diff = population[np.argmin(fitness)] - self.x_opt\n            C = (1 - self.learning_rate_C) * C + self.learning_rate_C * np.outer(diff, diff)\n            sigma *= np.exp(0.5 * (np.mean(fitness > np.min(fitness)) - 0.5) / 0.2)  # Simplified step-size adaptation\n            sigma = np.clip(sigma, self.sigma_init/10, (self.ub - self.lb)) #prevent sigma from going to 0\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveDifferentialEvolutionCMA scored 0.413 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ffb81665-2265-4d8c-ae60-d2e898356eb3"], "operator": null, "metadata": {"aucs": [0.19051309939079375, 0.32362133163990225, 0.40219395443721373, 0.5774563327401299, 0.31915105310321, 0.400379145861811, 0.3114116321704721, 0.33554449045176593, 0.32842831681843376, 0.27107084891904576, 0.45622333073216115, 0.9992370034808464, 0.2700423559819085, 0.3307280732228237, 0.7844047535362129, 0.3982082940124214, 0.3466978550826052, 0.5064270638379563, 0.2116975695943598, 0.49503981653083695]}}
{"id": "9ddb3298-2574-47c1-813a-d1bde993fd58", "fitness": 0.3493611188942275, "name": "SelfOrganizingScout", "description": "A self-organizing scout-based optimization algorithm with dynamic resource allocation that combines a global random search with local exploitation around promising solutions.", "code": "import numpy as np\n\nclass SelfOrganizingScout:\n    def __init__(self, budget=10000, dim=10, num_scouts=5, scout_rate=0.05, local_search_iterations=10, lb=-5.0, ub=5.0, exploitation_factor=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.num_scouts = num_scouts\n        self.scout_rate = scout_rate\n        self.local_search_iterations = local_search_iterations\n        self.lb = lb\n        self.ub = ub\n        self.exploitation_factor = exploitation_factor  # Controls the scaling of local search radius\n\n        self.scouts = []  # List of scout agents\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n    def initialize_scouts(self, func):\n        for _ in range(self.num_scouts):\n            x = np.random.uniform(self.lb, self.ub, size=self.dim)\n            f = func(x)\n            self.budget -= 1\n            self.scouts.append({\"x\": x, \"f\": f, \"resources\": 1})  # Initialize with equal resources\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n\n    def local_search(self, func, scout):\n        best_x = scout[\"x\"].copy()\n        best_f = scout[\"f\"]\n        radius = self.exploitation_factor * (self.ub - self.lb)  # Adaptive radius based on problem scale\n\n        for _ in range(self.local_search_iterations):\n            if self.budget <= 0:\n                break\n\n            x = best_x + np.random.uniform(-radius, radius, size=self.dim)  # Gaussian might also work\n            x = np.clip(x, self.lb, self.ub)\n            f = func(x)\n            self.budget -= 1\n\n            if f < best_f:\n                best_f = f\n                best_x = x.copy()\n\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = x.copy()\n        return best_x, best_f\n\n    def scout(self, func):\n        x = np.random.uniform(self.lb, self.ub, size=self.dim)\n        f = func(x)\n        self.budget -= 1\n        if f < self.f_opt:\n            self.f_opt = f\n            self.x_opt = x.copy()\n        return x, f\n\n    def allocate_resources(self):\n        # Adaptive resource allocation based on scout performance\n        fitness_values = np.array([scout[\"f\"] for scout in self.scouts])\n        normalized_fitness = np.max(fitness_values) - fitness_values #invert fitness\n        normalized_fitness = normalized_fitness/np.sum(normalized_fitness)\n        new_resources = np.round(normalized_fitness * self.num_scouts).astype(int)\n\n        #Ensure sum of resources remains constant, adjusting to ensure total = num_scouts\n        resource_diff = np.sum(new_resources) - self.num_scouts\n\n        if resource_diff > 0: #too many resources\n            idx = np.argsort(normalized_fitness)[::-1][:resource_diff] #indices of scouts with the most resources\n            new_resources[idx] -= 1\n        elif resource_diff < 0: #too few resources\n             idx = np.argsort(normalized_fitness)[:abs(resource_diff)]\n             new_resources[idx] +=1\n\n        for i in range(self.num_scouts):\n            self.scouts[i][\"resources\"] = max(1,new_resources[i]) #ensure at least 1 resource to scout\n\n    def __call__(self, func):\n        self.initialize_scouts(func)\n\n        while self.budget > 0:\n            self.allocate_resources()\n\n            for i in range(self.num_scouts):\n                if self.budget <= 0:\n                    break\n                \n                if np.random.rand() < self.scout_rate:\n                    # Send out a new scout\n                    x, f = self.scout(func)\n                    self.scouts[i][\"x\"] = x\n                    self.scouts[i][\"f\"] = f\n                else:\n                   #Perform Local search\n                   best_x, best_f = self.local_search(func, self.scouts[i])\n                   self.scouts[i][\"x\"] = best_x\n                   self.scouts[i][\"f\"] = best_f\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm SelfOrganizingScout scored 0.349 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ffb81665-2265-4d8c-ae60-d2e898356eb3"], "operator": null, "metadata": {"aucs": [0.15986974885908678, 0.22892674655446066, 0.3478289623385352, 0.3658428336686076, 0.26176759653061243, 0.32957681174760933, 0.27474209339932043, 0.2850184022553004, 0.2667835177170167, 0.16812558316911053, 0.3476585636134124, 0.998069404152841, 0.27856026533450196, 0.2642279958206607, 0.6984125133250689, 0.3400621879312171, 0.3094342223956108, 0.3870889835199812, 0.18451865670761614, 0.4907072888439792]}}
{"id": "ebc5986c-3871-4cfc-a66a-5d884be59f8b", "fitness": 0.6982184286237723, "name": "AdaptiveDifferentialEvolutionCMA", "description": "Simplified Adaptive Differential Evolution with CMA-inspired updates and selective pressure on covariance adaptation.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolutionCMA:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, Cr=0.9, sigma_init=0.1, learning_rate_C=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.Cr = Cr\n        self.sigma_init = sigma_init\n        self.lb = -5.0\n        self.ub = 5.0\n        self.learning_rate_C = learning_rate_C\n\n    def __call__(self, func):\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)].copy()\n\n        sigma = self.sigma_init * (self.ub - self.lb)\n        C = np.eye(self.dim)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n\n                v = x_r1 + self.F * (x_r2 - x_r3) + sigma * np.random.multivariate_normal(np.zeros(self.dim), C)\n                u = np.where(np.random.rand(self.dim) < self.Cr, v, population[i])\n                u = np.clip(u, self.lb, self.ub)\n                \n                f_u = func(u)\n                self.budget -= 1\n                \n                if f_u < fitness[i]:\n                    population[i] = u\n                    fitness[i] = f_u\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u.copy()\n\n            # Selective pressure on covariance adaptation\n            best_idx = np.argmin(fitness)\n            diff = population[best_idx] - self.x_opt\n            C = (1 - self.learning_rate_C) * C + self.learning_rate_C * np.outer(diff, diff)\n\n            sigma *= np.exp(0.5 * (np.mean(fitness > self.f_opt) - 0.5) / 0.2)  # Step-size adaptation\n            sigma = np.clip(sigma, self.sigma_init/10, (self.ub - self.lb)) #prevent sigma from going to 0\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveDifferentialEvolutionCMA scored 0.698 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ffb81665-2265-4d8c-ae60-d2e898356eb3"], "operator": null, "metadata": {"aucs": [0.4790340697016534, 0.7306888122585866, 0.7219742829443198, 0.8503110955684744, 0.7400572490233323, 0.764916524800523, 0.6799960551633653, 0.6798393581313071, 0.734577195386491, 0.7183742338768553, 0.8291827338187066, 0.9936130384246811, 0.35208374477536986, 0.7391299686861068, 0.9246698491635686, 0.7608945167000594, 0.6952160285246207, 0.8159836921579051, 0.23738596750057828, 0.5164401558689398]}}
{"id": "6460126d-f6b7-409d-90a5-c10a033f18b6", "fitness": -Infinity, "name": "OrthogonalCMA", "description": "Covariance matrix adaptation with orthogonal sampling and a restart mechanism based on fitness improvement stagnation, enhancing exploration and exploitation.", "code": "import numpy as np\n\nclass OrthogonalCMA:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_scale=0.5, stagnation_threshold=50, orthogonal_components=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.scale = initial_scale\n        self.scale_decay = 0.99\n        self.min_scale = 0.01\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.covariance = np.eye(dim)\n        self.mean = np.zeros(dim)\n        self.orthogonal_components = orthogonal_components\n        self.last_f_opt = np.Inf\n\n    def orthogonal_sample(self):\n        H = np.random.randn(self.dim, self.orthogonal_components)\n        Q, _ = np.linalg.qr(H)\n        return Q\n\n    def __call__(self, func):\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n        best_x = self.x_opt.copy()\n\n        while self.budget > 0:\n            orthogonal_basis = self.orthogonal_sample()\n            new_population = np.zeros((self.pop_size, self.dim))\n            for i in range(self.pop_size):\n                # Sample along orthogonal directions\n                direction_idx = i % self.orthogonal_components\n                direction = orthogonal_basis[:, direction_idx]\n                mutation = self.scale * np.sqrt(np.diag(self.covariance)) * direction\n                new_population[i] = best_x + mutation\n                new_population[i] = np.clip(new_population[i], self.lb, self.ub)\n\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n\n            best_new_idx = np.argmin(new_fitness)\n            if new_fitness[best_new_idx] < self.f_opt:\n                self.f_opt = new_fitness[best_new_idx]\n                self.x_opt = new_population[best_new_idx]\n                self.stagnation_counter = 0\n                best_x = self.x_opt.copy()\n            else:\n                self.stagnation_counter += 1\n\n            # Simplified replacement: replace if better\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    population[i] = new_population[i]\n                    fitness[i] = new_fitness[i]\n\n            self.scale = max(self.scale * self.scale_decay, self.min_scale)\n\n            # Covariance update (simplified rank-one)\n            diff = best_x - self.mean\n            self.mean = 0.9 * self.mean + 0.1 * best_x\n            self.covariance = 0.9 * self.covariance + 0.1 * (diff[:, None] @ diff[None, :]) + 1e-9 * np.eye(self.dim)\n\n            if self.stagnation_counter > self.stagnation_threshold or self.f_opt >= self.last_f_opt:\n                population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                best_idx = np.argmin(fitness)\n                best_x = population[best_idx]\n                self.covariance = np.eye(self.dim)\n                self.scale = 0.5 #self.initial_scale\n                self.stagnation_counter = 0\n                if np.min(fitness) < self.f_opt:\n                  self.f_opt = np.min(fitness)\n                  self.x_opt = population[np.argmin(fitness)]\n                self.budget -= self.pop_size\n\n            self.last_f_opt = self.f_opt\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "An exception occurred: index 2 is out of bounds for axis 1 with size 2.", "error": "", "parent_ids": ["4633fbe2-7a83-412d-953d-897df842c7cf"], "operator": null, "metadata": {}}
{"id": "6b6aa586-769e-45d3-9c03-7aa76bcdbc34", "fitness": 0.0, "name": "OrthogonalAdaptiveGaussian", "description": "A simplified Population-based Adaptive Gaussian mutation with Orthogonal Learning and covariance matrix adaptation, employing a reduced set of parameters for efficiency.", "code": "import numpy as np\n\nclass OrthogonalAdaptiveGaussian:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_scale=0.5, stagnation_threshold=50, orthogonal_trials=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.scale = initial_scale\n        self.scale_decay = 0.99\n        self.min_scale = 0.01\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.covariance = np.eye(dim)\n        self.mean = np.zeros(dim)\n        self.orthogonal_trials = orthogonal_trials # Number of orthogonal trials\n\n    def __call__(self, func):\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n        best_x = self.x_opt.copy()\n\n        while self.budget > 0:\n            new_population = np.random.multivariate_normal(mean=best_x, cov=self.scale**2 * self.covariance, size=(self.pop_size))\n            new_population = np.clip(new_population, self.lb, self.ub)\n\n            # Orthogonal learning trials\n            for i in range(self.pop_size):\n                orthogonal_points = np.zeros((self.orthogonal_trials, self.dim))\n                for j in range(self.orthogonal_trials):\n                    direction = np.random.randn(self.dim)\n                    direction /= np.linalg.norm(direction)\n                    orthogonal_points[j] = new_population[i] + self.scale * direction\n                    orthogonal_points[j] = np.clip(orthogonal_points[j], self.lb, self.ub)\n\n                orthogonal_fitness = np.array([func(x) for x in orthogonal_points])\n                self.budget -= self.orthogonal_trials\n                best_orthogonal_idx = np.argmin(orthogonal_fitness)\n\n                if orthogonal_fitness[best_orthogonal_idx] < fitness[i]:\n                    new_population[i] = orthogonal_points[best_orthogonal_idx]\n            \n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n\n            best_new_idx = np.argmin(new_fitness)\n            if new_fitness[best_new_idx] < self.f_opt:\n                self.f_opt = new_fitness[best_new_idx]\n                self.x_opt = new_population[best_new_idx]\n                self.stagnation_counter = 0\n                best_x = self.x_opt.copy()\n            else:\n                self.stagnation_counter += 1\n\n            # Simplified replacement: replace if better\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    population[i] = new_population[i]\n                    fitness[i] = new_fitness[i]\n\n            self.scale = max(self.scale * self.scale_decay, self.min_scale)\n\n            # Covariance update (simplified rank-one)\n            diff = best_x - self.mean\n            self.mean = 0.9 * self.mean + 0.1 * best_x\n            self.covariance = 0.9 * self.covariance + 0.1 * (diff[:, None] @ diff[None, :]) + 1e-9 * np.eye(self.dim)\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                best_idx = np.argmin(fitness)\n                best_x = population[best_idx]\n                self.covariance = np.eye(self.dim)\n                self.scale = 0.5 #self.initial_scale\n                self.stagnation_counter = 0\n                if np.min(fitness) < self.f_opt:\n                  self.f_opt = np.min(fitness)\n                  self.x_opt = population[np.argmin(fitness)]\n                self.budget -= self.pop_size\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm OrthogonalAdaptiveGaussian scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["4633fbe2-7a83-412d-953d-897df842c7cf"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "27e8d09a-2b83-4da4-a730-6acf73c6ce63", "fitness": 0.0, "name": "AdaptiveGaussianAdaptation", "description": "Adaptive Gaussian mutation with covariance matrix adaptation and dynamic scaling, incorporating a restart mechanism based on stagnation detection and simplified parameter settings for improved efficiency.", "code": "import numpy as np\n\nclass AdaptiveGaussianAdaptation:\n    def __init__(self, budget=10000, dim=10, pop_size=10, initial_scale=0.3, stagnation_threshold=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.scale = initial_scale\n        self.scale_decay = 0.98\n        self.min_scale = 0.01\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.covariance = np.eye(dim) * initial_scale**2\n        self.mean = np.zeros(dim)\n        self.best_fitness_history = []\n\n    def __call__(self, func):\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n        best_x = self.x_opt.copy()\n        self.best_fitness_history.append(self.f_opt)\n\n        while self.budget > 0:\n            new_population = np.random.multivariate_normal(mean=best_x, cov=self.covariance, size=self.pop_size)\n            new_population = np.clip(new_population, self.lb, self.ub)\n\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n\n            best_new_idx = np.argmin(new_fitness)\n            if new_fitness[best_new_idx] < self.f_opt:\n                self.f_opt = new_fitness[best_new_idx]\n                self.x_opt = new_population[best_new_idx]\n                best_x = self.x_opt.copy()\n                self.stagnation_counter = 0\n                self.best_fitness_history.append(self.f_opt)\n            else:\n                self.stagnation_counter += 1\n                self.best_fitness_history.append(self.f_opt)\n\n\n            # Replacement: replace if better\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    population[i] = new_population[i]\n                    fitness[i] = new_fitness[i]\n\n            self.scale = max(self.scale * self.scale_decay, self.min_scale)\n\n            # Covariance update (simplified rank-one)\n            diff = best_x - self.mean\n            self.mean = 0.5 * self.mean + 0.5 * best_x\n            self.covariance = 0.9 * self.covariance + 0.1 * (diff[:, None] @ diff[None, :]) + np.eye(self.dim) * self.scale**2 * 1e-6\n\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                best_idx = np.argmin(fitness)\n                best_x = population[best_idx]\n                self.covariance = np.eye(self.dim) * self.scale**2\n                self.scale = 0.3 #self.initial_scale\n                self.stagnation_counter = 0\n                if np.min(fitness) < self.f_opt:\n                    self.f_opt = np.min(fitness)\n                    self.x_opt = population[np.argmin(fitness)]\n                    self.best_fitness_history.append(self.f_opt)\n                else:\n                  self.best_fitness_history.append(self.f_opt)\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveGaussianAdaptation scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["4633fbe2-7a83-412d-953d-897df842c7cf"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "2d765a4c-c020-41e7-8afd-8f88e0796f5d", "fitness": 0.0, "name": "OrthogonalGaussianAdaptation", "description": "An adaptive algorithm that combines Gaussian mutation with orthogonal learning to enhance exploration and exploitation.", "code": "import numpy as np\n\nclass OrthogonalGaussianAdaptation:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_scale=0.5, stagnation_threshold=50, orthogonal_components=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.scale = initial_scale\n        self.scale_decay = 0.99\n        self.min_scale = 0.01\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.covariance = np.eye(dim)\n        self.mean = np.zeros(dim)\n        self.orthogonal_components = orthogonal_components  # Number of orthogonal vectors to generate\n\n    def __call__(self, func):\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n        best_x = self.x_opt.copy()\n\n        while self.budget > 0:\n            # Gaussian mutation\n            new_population_gaussian = np.random.multivariate_normal(mean=best_x, cov=self.scale**2 * self.covariance, size=self.pop_size)\n            new_population_gaussian = np.clip(new_population_gaussian, self.lb, self.ub)\n            \n            # Orthogonal learning\n            orthogonal_vectors = np.random.randn(self.orthogonal_components, self.dim)\n            orthogonal_vectors = np.linalg.qr(orthogonal_vectors)[0] #Orthonormalize the vectors\n            \n            new_population_orthogonal = np.zeros((self.orthogonal_components, self.dim))\n            for i in range(self.orthogonal_components):\n                new_population_orthogonal[i] = best_x + self.scale * orthogonal_vectors[i]\n                new_population_orthogonal[i] = np.clip(new_population_orthogonal[i], self.lb, self.ub)\n\n            # Evaluate fitness of mutated offspring and orthogonal offspring\n            new_fitness_gaussian = np.array([func(x) for x in new_population_gaussian])\n            self.budget -= self.pop_size\n            if self.budget <= 0:\n                break\n            new_fitness_orthogonal = np.array([func(x) for x in new_population_orthogonal])\n            self.budget -= self.orthogonal_components\n            if self.budget <= 0:\n                break\n\n            best_new_idx_gaussian = np.argmin(new_fitness_gaussian)\n            best_new_idx_orthogonal = np.argmin(new_fitness_orthogonal)\n\n            # Update best solution\n            if new_fitness_gaussian[best_new_idx_gaussian] < self.f_opt:\n                self.f_opt = new_fitness_gaussian[best_new_idx_gaussian]\n                self.x_opt = new_population_gaussian[best_new_idx_gaussian]\n                self.stagnation_counter = 0\n                best_x = self.x_opt.copy()\n            elif new_fitness_orthogonal[best_new_idx_orthogonal] < self.f_opt:\n                self.f_opt = new_fitness_orthogonal[best_new_idx_orthogonal]\n                self.x_opt = new_population_orthogonal[best_new_idx_orthogonal]\n                self.stagnation_counter = 0\n                best_x = self.x_opt.copy()\n            else:\n                self.stagnation_counter += 1\n\n            # Simplified replacement: replace if better (Gaussian offspring)\n            for i in range(self.pop_size):\n                if new_fitness_gaussian[i] < fitness[i]:\n                    population[i] = new_population_gaussian[i]\n                    fitness[i] = new_fitness_gaussian[i]\n\n            self.scale = max(self.scale * self.scale_decay, self.min_scale)\n\n            # Covariance update (simplified rank-one)\n            diff = best_x - self.mean\n            self.mean = 0.9 * self.mean + 0.1 * best_x\n            self.covariance = 0.9 * self.covariance + 0.1 * (diff[:, None] @ diff[None, :]) + 1e-9 * np.eye(self.dim)\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                best_idx = np.argmin(fitness)\n                best_x = population[best_idx]\n                self.covariance = np.eye(self.dim)\n                self.scale = 0.5\n                self.stagnation_counter = 0\n                if np.min(fitness) < self.f_opt:\n                    self.f_opt = np.min(fitness)\n                    self.x_opt = population[np.argmin(fitness)]\n                self.budget -= self.pop_size\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm OrthogonalGaussianAdaptation scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["4633fbe2-7a83-412d-953d-897df842c7cf"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "5b9d83ee-7904-4248-af09-e2549dfae948", "fitness": 0.535330511481547, "name": "AdaptiveCauchyDE", "description": "A hybrid algorithm combining differential evolution with a Cauchy mutation operator and a rank-based selection, adaptively adjusting exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveCauchyDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, CR=0.7, initial_scale=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.lb = -5.0\n        self.ub = 5.0\n        self.scale = initial_scale\n        self.scale_decay = 0.995\n        self.min_scale = 0.001\n        self.exploration_prob = 0.1  # Probability of Cauchy mutation\n\n    def __call__(self, func):\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            ranked_indices = np.argsort(fitness)\n            best_index = ranked_indices[0]\n            median_index = ranked_indices[self.pop_size // 2]\n\n            for i in range(self.pop_size):\n                # Differential Evolution\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = population[a] + self.F * (population[b] - population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n                \n                # Cauchy Mutation (with probability exploration_prob)\n                if np.random.rand() < self.exploration_prob:\n                    cauchy_noise = self.scale * np.random.standard_cauchy(size=self.dim)\n                    mutant = population[i] + cauchy_noise\n                    mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluate trial solution\n                f_trial = func(trial)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                # Rank-based Selection: Replace if trial is better than median\n                if f_trial < fitness[median_index]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            self.scale = max(self.scale * self.scale_decay, self.min_scale)\n            self.exploration_prob = min(0.5, self.exploration_prob * 1.05)  # Increase exploration over time\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveCauchyDE scored 0.535 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b637d349-9d6c-4244-9344-28042da16810"], "operator": null, "metadata": {"aucs": [0.17757554648906282, 0.27642737159777764, 0.4968477157519021, 0.7818405154329686, 0.5633815641255714, 0.5646638955313341, 0.4243275284521051, 0.5168065009292515, 0.5604138429342916, 0.36137873115667174, 0.7335534524380174, 0.9929613489786915, 0.3778958379534797, 0.5053968589404563, 0.8413161646404297, 0.6564150006912824, 0.41964619232310696, 0.7198517705416982, 0.23060821065194337, 0.5053021800708963]}}
{"id": "97382748-a8fc-48cd-ac7d-924afa1b22af", "fitness": 0.5149574081351899, "name": "HybridDECauchyLocal", "description": "Hybrid algorithm combining differential evolution with a Cauchy mutation and a local search strategy based on perturbing the best solution, adaptively adjusting mutation strength.", "code": "import numpy as np\n\nclass HybridDECauchyLocal:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, CR=0.7, initial_scale=0.1, local_search_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.lb = -5.0\n        self.ub = 5.0\n        self.scale = initial_scale\n        self.scale_decay = 0.99\n        self.min_scale = 0.001\n        self.local_search_prob = local_search_prob\n\n    def __call__(self, func):\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Differential Evolution Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = population[a] + self.F * (population[b] - population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Cauchy mutation\n                mutant = mutant + self.scale * np.random.standard_cauchy(size=self.dim)\n                mutant = np.clip(mutant, self.lb, self.ub)\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Local search around the best solution\n                if np.random.rand() < self.local_search_prob:\n                    trial = self.x_opt + self.scale * np.random.normal(0, 1, self.dim)\n                    trial = np.clip(trial, self.lb, self.ub)\n\n                # Evaluate and selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Scale adaptation\n            self.scale = max(self.scale * self.scale_decay, self.min_scale)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm HybridDECauchyLocal scored 0.515 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3e21ba62-95db-4515-aa01-fa02b4d75b6b"], "operator": null, "metadata": {"aucs": [0.260308127779153, 0.21274347436410534, 0.53636170914911, 0.19924522839934344, 0.5504621353141932, 0.6098794010974111, 0.44921143863564816, 0.4782172747478076, 0.5454049153131686, 0.1882146483546786, 0.9357935468093941, 0.9973620142046624, 0.2668918460024411, 0.5605664530728678, 0.8942631831605717, 0.6256860657190304, 0.4869181084026515, 0.7539622045714076, 0.25189034169000146, 0.49576604591614815]}}
{"id": "393e1e12-1442-4950-9b73-cc7c60fff4a0", "fitness": 0.5931833665432349, "name": "HybridDEOD", "description": "Hybrid DE with dynamic diversity control, adaptive mutation scaling based on success rate, and orthogonal design for generating trial vectors.", "code": "import numpy as np\n\nclass HybridDEOD:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, CR=0.7, initial_scale=0.3, diversity_weight=0.1, success_rate_memory=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.lb = -5.0\n        self.ub = 5.0\n        self.scale = initial_scale\n        self.scale_decay = 0.99\n        self.min_scale = 0.01\n        self.diversity_weight = diversity_weight\n        self.success_rate_memory = success_rate_memory\n        self.success_rates = np.zeros(self.success_rate_memory)\n        self.success_idx = 0\n\n    def orthogonal_design(self, num_points, dim):\n        # Simplified orthogonal design (Latin hypercube sampling)\n        points = np.zeros((num_points, dim))\n        for i in range(dim):\n            points[:, i] = np.random.permutation(num_points)\n        points = (points + np.random.rand(num_points, dim)) / num_points\n        return points\n\n    def __call__(self, func):\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Differential Evolution Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = population[a] + self.F * (population[b] - population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Gaussian perturbation\n                mutant = mutant + self.scale * np.random.normal(0, 1, self.dim)\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Orthogonal Design Crossover\n                od_points = self.orthogonal_design(self.dim, 1).flatten()  # Generate orthogonal design points\n                trial = population[i].copy()\n                for j in range(self.dim):\n                    if od_points[j] < self.CR:\n                        trial[j] = mutant[j]\n\n                # Evaluate and selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                    self.success_rates[self.success_idx] = 1\n                else:\n                    self.success_rates[self.success_idx] = 0\n\n            # Scale adaptation based on success rate\n            success_rate = np.mean(self.success_rates)\n            if success_rate > 0.2:\n                self.scale *= 1.1  # Increase scale if doing well\n            else:\n                self.scale *= 0.9  # Decrease scale if not improving\n            self.scale = max(self.scale * self.scale_decay, self.min_scale)\n            self.success_idx = (self.success_idx + 1) % self.success_rate_memory\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm HybridDEOD scored 0.593 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3e21ba62-95db-4515-aa01-fa02b4d75b6b"], "operator": null, "metadata": {"aucs": [0.2957333291472509, 0.6127161224771116, 0.5364434160888283, 0.9243532558099452, 0.5514957985409121, 0.6183974259257918, 0.45624633534269665, 0.48290888462537407, 0.5389232487599975, 0.4837945520751047, 0.8736719598520978, 0.997866882277207, 0.2839753413911531, 0.5579923997297715, 0.9197799948115813, 0.6467680037731005, 0.4882174196955923, 0.7429186111441679, 0.3218129114953153, 0.5296514379016963]}}
{"id": "d885774a-8501-40ae-9589-72ee92c224d4", "fitness": 0.4688651302394084, "name": "EnhancedHybridDE", "description": "Enhanced hybrid DE with adaptive parameter control using success history, combined with orthogonal learning for improved exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedHybridDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, CR=0.7, initial_scale=0.3, diversity_weight=0.1, ortho_group_size=3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.lb = -5.0\n        self.ub = 5.0\n        self.scale = initial_scale\n        self.scale_decay = 0.99\n        self.min_scale = 0.01\n        self.diversity_weight = diversity_weight\n        self.ortho_group_size = ortho_group_size\n        self.success_f = []\n        self.success_cr = []\n        self.memory_size = 10\n        self.p_best = 0.1\n\n    def __call__(self, func):\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Adaptive parameter control using success history\n                if self.success_f:\n                    self.F = np.clip(np.random.normal(np.mean(self.success_f), np.std(self.success_f)), 0.1, 1.0)\n                    self.CR = np.clip(np.random.normal(np.mean(self.success_cr), np.std(self.success_cr)), 0.1, 1.0)\n\n                # Differential Evolution Mutation with p_best selection\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                if np.random.rand() < self.p_best:\n                  j_best = np.argmin(fitness)\n                  a, b = np.random.choice(idxs, 2, replace=False)\n                  mutant = population[i] + self.F * (population[j_best] - population[i]) + self.F * (population[a] - population[b])\n                else:\n                  a, b, c = np.random.choice(idxs, 3, replace=False)\n                  mutant = population[a] + self.F * (population[b] - population[c])\n\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Gaussian perturbation\n                mutant = mutant + self.scale * np.random.normal(0, 1, self.dim)\n                mutant = np.clip(mutant, self.lb, self.ub)\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Orthogonal learning: perturb only few dimensions\n                if self.dim > self.ortho_group_size:\n                    group_indices = np.random.choice(self.dim, size=self.ortho_group_size, replace=False)\n                    trial_ortho = trial.copy()\n                    trial_ortho[group_indices] = np.random.uniform(self.lb, self.ub, size=self.ortho_group_size)\n                else:\n                    trial_ortho = np.random.uniform(self.lb, self.ub, size=self.dim)\n\n                # Evaluate and selection\n                f_trial = func(trial)\n                f_trial_ortho = func(trial_ortho)\n                self.budget -= 2\n                if self.budget <= 0:\n                    break\n\n                if f_trial < fitness[i] and f_trial <= f_trial_ortho:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.success_f.append(self.F)\n                        self.success_cr.append(self.CR)\n\n                elif f_trial_ortho < fitness[i] and f_trial_ortho <= f_trial:\n                    population[i] = trial_ortho\n                    fitness[i] = f_trial_ortho\n                    if f_trial_ortho < self.f_opt:\n                        self.f_opt = f_trial_ortho\n                        self.x_opt = trial_ortho\n                        self.success_f.append(self.F)\n                        self.success_cr.append(self.CR)\n                \n                # Memory management\n                if len(self.success_f) > self.memory_size:\n                    self.success_f.pop(0)\n                    self.success_cr.pop(0)\n\n            # Scale adaptation\n            self.scale = max(self.scale * self.scale_decay, self.min_scale)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm EnhancedHybridDE scored 0.469 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3e21ba62-95db-4515-aa01-fa02b4d75b6b"], "operator": null, "metadata": {"aucs": [0.19954691874750974, 0.37996918146213177, 0.4303173746045206, 0.7190583114139526, 0.3863219614161768, 0.45574070590272164, 0.314587978811657, 0.37547637167274694, 0.376520942127642, 0.31598079216415154, 0.74242383428281, 0.9972745854794974, 0.27944213840940557, 0.40105541471247774, 0.8000051118574519, 0.4923149792902848, 0.3662886096598382, 0.5651112563816166, 0.2687208236830261, 0.511145312708549]}}
{"id": "88a29709-a214-448e-9ced-ba1f8be325e6", "fitness": 0.557659196778731, "name": "EnhancedHybridDE", "description": "Enhanced hybrid DE with CMA-ES inspired covariance adaptation, dynamic F/CR, and population diversity maintenance.", "code": "import numpy as np\n\nclass EnhancedHybridDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_scale=0.3, diversity_weight=0.1, cma_rank_one=0.1, f_adapt=True, cr_adapt=True):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.scale = initial_scale\n        self.scale_decay = 0.99\n        self.min_scale = 0.01\n        self.diversity_weight = diversity_weight\n        self.cma_rank_one = cma_rank_one\n        self.f_adapt = f_adapt\n        self.cr_adapt = cr_adapt\n\n        self.mean = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.p_c = np.zeros(self.dim)\n        self.p_sigma = np.zeros(self.dim)\n        self.c_sigma = 1 / np.sqrt(self.budget / self.pop_size)\n        self.d_sigma = 1 + 2 * max(0, np.sqrt((self.dim - 1) / (self.dim + 3)) - 1) + self.c_sigma\n\n        self.archive = []  # For diversity maintenance\n        self.archive_size = pop_size * 10\n        self.F = 0.5 # initial value\n        self.CR = 0.7 # initial value\n        self.memory_F = np.ones(self.pop_size) * self.F\n        self.memory_CR = np.ones(self.pop_size) * self.CR\n\n    def __call__(self, func):\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                if self.f_adapt:\n                    self.F = np.random.normal(0.5, 0.3) # gaussian\n                    self.F = np.clip(self.F, 0.1, 1.0)\n                    self.memory_F[i] = self.F\n                else:\n                    self.F = self.memory_F[i]\n\n                if self.cr_adapt:\n                    self.CR = np.random.normal(0.7, 0.1) # gaussian\n                    self.CR = np.clip(self.CR, 0.1, 0.9)\n                    self.memory_CR[i] = self.CR\n                else:\n                    self.CR = self.memory_CR[i]\n                \n                # Differential Evolution Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = population[a] + self.F * (population[b] - population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Gaussian perturbation with CMA-ES adaptation\n                z = np.random.normal(0, 1, self.dim)\n                mutant = mutant + self.scale * np.dot(self.C, z)\n                mutant = np.clip(mutant, self.lb, self.ub)\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluate and selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n                    # CMA-ES Rank-One Update\n                    self.p_c = (1 - self.cma_rank_one) * self.p_c + np.sqrt(self.cma_rank_one * (2 - self.cma_rank_one)) * z\n                    self.C = (1 - self.cma_rank_one) * self.C + self.cma_rank_one * np.outer(self.p_c, self.p_c)\n                    \n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                \n                # Archive for diversity\n                if len(self.archive) < self.archive_size:\n                    self.archive.append(population[i].copy())\n                else:\n                    idx = np.random.randint(0, self.archive_size)\n                    self.archive[idx] = population[i].copy()\n            \n            # Scale adaptation\n            self.scale = max(self.scale * self.scale_decay, self.min_scale)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm EnhancedHybridDE scored 0.558 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3e21ba62-95db-4515-aa01-fa02b4d75b6b"], "operator": null, "metadata": {"aucs": [0.25039725206611907, 0.5378462959376713, 0.506796448284109, 0.8763593974441215, 0.5010678101973844, 0.5880339051385469, 0.4120948606157846, 0.4376564245521023, 0.486962382540326, 0.45075969218254386, 0.810782248593662, 0.9959296840499925, 0.34604484877738384, 0.4894424599848395, 0.8963052029160309, 0.581951453892456, 0.43979050134012165, 0.7067677263498942, 0.3084230178027051, 0.5297723229088276]}}
{"id": "c8810f88-c6a0-44fa-abf8-001bb3bfe8f0", "fitness": 0.47281078765863027, "name": "ImprovedHybridDE", "description": "Improved hybrid DE with dynamic parameter adaptation, orthogonal design for diversity, and a Cauchy mutation component for escaping local optima.", "code": "import numpy as np\n\nclass ImprovedHybridDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, CR=0.7, initial_scale=0.3, diversity_weight=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.lb = -5.0\n        self.ub = 5.0\n        self.scale = initial_scale\n        self.scale_decay = 0.99\n        self.min_scale = 0.01\n        self.diversity_weight = diversity_weight\n        self.cauchy_scale = 0.1\n\n    def __call__(self, func):\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        # Orthogonal initialization for diversity\n        orthogonal_matrix = self._generate_orthogonal_array(self.pop_size, self.dim)\n        population = self.lb + (self.ub - self.lb) * orthogonal_matrix\n\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        # Adaptive F and CR\n        F_values = np.full(self.pop_size, self.F)\n        CR_values = np.full(self.pop_size, self.CR)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Adaptive F and CR based on success\n                F_adaptive = F_values[i] + 0.1 * np.random.normal(0, 1)\n                CR_adaptive = CR_values[i] + 0.1 * np.random.normal(0, 1)\n                F_adaptive = np.clip(F_adaptive, 0.1, 0.9)\n                CR_adaptive = np.clip(CR_adaptive, 0.1, 0.9)\n\n\n                # Differential Evolution Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = population[a] + F_adaptive * (population[b] - population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Gaussian perturbation\n                mutant = mutant + self.scale * np.random.normal(0, 1, self.dim)\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Cauchy perturbation for escape\n                mutant = mutant + self.cauchy_scale * np.random.standard_cauchy(size=self.dim)\n                mutant = np.clip(mutant, self.lb, self.ub)\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR_adaptive\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluate and selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        # Update successful F and CR\n                        F_values[i] = F_adaptive\n                        CR_values[i] = CR_adaptive\n            \n\n            # Scale adaptation\n            self.scale = max(self.scale * self.scale_decay, self.min_scale)\n\n        return self.f_opt, self.x_opt\n\n    def _generate_orthogonal_array(self, n, k):\n        # A simple orthogonal array construction (L18)\n        if n <= 18:\n            if k <= 7:\n                H = np.array([\n                    [0, 0, 0, 0, 0, 0, 0],\n                    [0, 1, 1, 1, 1, 0, 0],\n                    [0, 2, 2, 2, 0, 1, 1],\n                    [0, 3, 3, 0, 2, 2, 1],\n                    [0, 4, 0, 3, 3, 2, 2],\n                    [0, 0, 4, 4, 3, 3, 2],\n                    [0, 1, 2, 4, 0, 4, 3],\n                    [0, 2, 4, 1, 4, 0, 3],\n                    [0, 3, 1, 2, 4, 4, 0],\n                    [0, 4, 3, 1, 0, 4, 4],\n                    [0, 0, 3, 2, 1, 4, 4],\n                    [0, 1, 4, 3, 2, 1, 4],\n                    [0, 2, 0, 4, 3, 1, 4],\n                    [0, 3, 2, 0, 4, 1, 1],\n                    [0, 4, 1, 2, 0, 1, 1],\n                    [1, 1, 1, 1, 1, 1, 1],\n                    [2, 2, 2, 2, 2, 2, 2]\n                ])\n                \n                oa = np.zeros((18, k))\n                for i in range(18):\n                    for j in range(k):\n                        oa[i, j] = H[i % 18, j % 7] / 4.0  # Normalize to [0, 1]\n\n                return oa[:n, :k]\n\n        return np.random.rand(n, k)", "configspace": "", "generation": 7, "feedback": "The algorithm ImprovedHybridDE scored 0.473 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3e21ba62-95db-4515-aa01-fa02b4d75b6b"], "operator": null, "metadata": {"aucs": [0.2026376590168515, 0.3977111338958286, 0.45318677732322676, 0.7590360444892169, 0.3991931117322407, 0.4958024682794663, 0.3198406252393683, 0.37900041328378253, 0.39761137989175255, 0.3293866082811161, 0.6844089360105141, 0.9972277761028501, 0.2532749748404234, 0.36452785533404364, 0.7771710310139616, 0.5003848489017525, 0.3796200347629929, 0.6214793166473955, 0.2327068636034596, 0.5120078945223623]}}
{"id": "20616d5c-b759-4d74-ba91-093e797f1dd0", "fitness": 0.2942538501512283, "name": "SimplifiedHybridDE", "description": "Streamlined hybrid DE with adaptive scaling, simplified mutation, and population-wide fitness improvement, balancing exploration and exploitation for efficiency.", "code": "import numpy as np\n\nclass SimplifiedHybridDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, CR=0.7, initial_scale=0.3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.lb = -5.0\n        self.ub = 5.0\n        self.scale = initial_scale\n        self.scale_decay = 0.99\n        self.min_scale = 0.01\n\n    def __call__(self, func):\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            # Find best individual in population\n            best_idx = np.argmin(fitness)\n            best_individual = population[best_idx]\n            \n            for i in range(self.pop_size):\n                # Differential Evolution Mutation using best individual\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b = np.random.choice(idxs, 2, replace=False)\n                mutant = population[i] + self.F * (best_individual - population[i]) + self.scale * (population[a] - population[b])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluate and selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Scale adaptation\n            self.scale = max(self.scale * self.scale_decay, self.min_scale)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm SimplifiedHybridDE scored 0.294 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3e21ba62-95db-4515-aa01-fa02b4d75b6b"], "operator": null, "metadata": {"aucs": [0.14159075745419292, 0.18257178117788164, 0.3338700917267281, 0.35651575159985227, 0.28726909352849506, 0.203669355399547, 0.25835508939269125, 0.22512082059676264, 0.3678534012009761, 0.20472798464258968, 0.16953127682937263, 0.9964833493531394, 0.267975934680464, 0.2210224366322916, 0.1733120933914002, 0.2966909955932868, 0.2849925419999312, 0.244557915768982, 0.21022584589709536, 0.4587404861588851]}}
{"id": "ca4e1856-9549-4c94-a2b0-baaf27e114f1", "fitness": 0.48222559386425995, "name": "CauchyHerdingDE", "description": "Combines differential evolution with a Cauchy mutation, adaptive parameter control, and a population herding mechanism to improve convergence and exploration.", "code": "import numpy as np\n\nclass CauchyHerdingDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, CR=0.7, initial_scale=0.3, herding_factor=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.lb = -5.0\n        self.ub = 5.0\n        self.scale = initial_scale\n        self.scale_decay = 0.99\n        self.min_scale = 0.01\n        self.herding_factor = herding_factor\n\n        # Adaptive F and CR\n        self.F_history = []\n        self.CR_history = []\n        self.success_threshold = 0.1\n\n    def __call__(self, func):\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Differential Evolution Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = population[a] + self.F * (population[b] - population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Cauchy mutation\n                mutant = mutant + self.scale * np.random.standard_cauchy(size=self.dim)\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Population Herding - Move towards the best solution\n                trial = trial + self.herding_factor * (self.x_opt - trial)\n                trial = np.clip(trial, self.lb, self.ub)\n                \n                # Evaluate and selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                if f_trial < fitness[i]:\n                    self.F_history.append(self.F)\n                    self.CR_history.append(self.CR)\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Adaptive F and CR using success history\n            if len(self.F_history) > 10:\n                success_rate = sum(1 for f in self.F_history[-10:] if f > 0) / 10  # Assuming F>0 indicates success\n                if success_rate < self.success_threshold:\n                    self.F = min(1.0, self.F + 0.1)\n                    self.CR = min(1.0, self.CR + 0.1)\n                else:\n                    self.F = max(0.1, self.F - 0.05)\n                    self.CR = max(0.1, self.CR - 0.05)\n\n            # Scale adaptation\n            self.scale = max(self.scale * self.scale_decay, self.min_scale)\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm CauchyHerdingDE scored 0.482 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3e21ba62-95db-4515-aa01-fa02b4d75b6b"], "operator": null, "metadata": {"aucs": [0.1685956406585305, 0.26123637728300686, 0.4133539770148733, 0.7984223481905988, 0.3747789939346544, 0.5996615521766946, 0.3553867188588804, 0.44165364858735834, 0.5349664829757098, 0.2483963627896223, 0.440065511788848, 0.996840699623088, 0.29572387506664344, 0.41768660525167367, 0.7786905839670062, 0.6380160760317564, 0.38698146454776683, 0.7085455559766263, 0.23722508697714884, 0.5482843155847136]}}
{"id": "85fad35b-0ba8-4cbd-b3cf-c7644f462ebf", "fitness": -Infinity, "name": "OrthogonalRestartDE", "description": "DE with orthogonal learning and a restart mechanism based on stagnation detection for improved exploration.", "code": "import numpy as np\nfrom scipy.stats import norm\n\nclass OrthogonalRestartDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, orthogonal_components=5, stagnation_threshold=1000, orthogonal_learning_probability=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.orthogonal_components = orthogonal_components\n        self.stagnation_threshold = stagnation_threshold\n        self.orthogonal_learning_probability = orthogonal_learning_probability\n        self.F = 0.5\n        self.CR = 0.7\n        self.best_fitness_history = []\n\n    def generate_orthogonal_array(self, n, k, levels):\n        \"\"\"Generates an orthogonal array using Plackett-Burman design.\"\"\"\n        if n == 12 and levels == 2 and k <= 11:\n            oa = np.array([\n                [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n                [1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0],\n                [1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1],\n                [1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1],\n                [1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1],\n                [1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1],\n                [1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0],\n                [1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1],\n                [1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1],\n                [1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1],\n                [1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0],\n                [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n            ]) - 1  # Convert to 0/1\n            return oa[:k].T\n        else:\n            raise ValueError(\"Unsupported orthogonal array parameters.\")\n\n    def orthogonal_learning(self, population, fitness, i):\n        \"\"\"Performs orthogonal learning on the current individual.\"\"\"\n        \n        # Select orthogonal components\n        selected_indices = np.random.choice(self.dim, self.orthogonal_components, replace=False)\n        \n        # Generate orthogonal array\n        oa = self.generate_orthogonal_array(12, self.orthogonal_components, 2)\n\n        # Create test points based on the orthogonal array\n        test_points = np.zeros((12, self.dim))\n        for j in range(12):\n            test_point = population[i].copy()\n            for k, idx in enumerate(selected_indices):\n                level = oa[j, k]\n                # perturb each of the selected dimensions using a Gaussian distribution\n                test_point[idx] = population[i][idx] + np.random.normal(0, (self.ub - self.lb) / 10)\n                test_point[idx] = np.clip(test_point[idx], self.lb, self.ub) # Clip the dimensions\n            test_points[j] = test_point\n\n        # Evaluate test points\n        test_fitness = np.array([func(x) for x in test_points])\n        self.budget -= 12 # decrease the budget by the number of calls made in orthogonal learning\n        if self.budget <= 0:\n            return population[i], fitness[i]\n\n        # Find the best test point\n        best_test_index = np.argmin(test_fitness)\n        \n        # Return the best individual\n        if test_fitness[best_test_index] < fitness[i]:\n            return test_points[best_test_index], test_fitness[best_test_index]\n        else:\n            return population[i], fitness[i]\n\n    def __call__(self, func):\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n        self.best_fitness_history.append(self.f_opt)\n\n        generation_count = 0\n        stagnation_counter = 0\n\n        while self.budget > 0:\n            generation_count += 1\n            for i in range(self.pop_size):\n                # Differential Evolution Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = population[a] + self.F * (population[b] - population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_learning_probability:\n                    trial, f_trial = self.orthogonal_learning(population, fitness, i) # Use the function\n\n                else:\n                    # Evaluate and selection\n                    f_trial = func(trial)\n                    self.budget -= 1\n                    if self.budget <= 0:\n                        break\n                \n                if self.orthogonal_learning_probability < 1:\n                    if f_trial < fitness[i]:\n                        population[i] = trial\n                        fitness[i] = f_trial\n\n                        if f_trial < self.f_opt:\n                            self.f_opt = f_trial\n                            self.x_opt = trial\n\n            current_best_fitness = np.min(fitness)\n            self.best_fitness_history.append(current_best_fitness)\n\n            if current_best_fitness >= self.f_opt:\n                stagnation_counter += 1\n            else:\n                self.f_opt = current_best_fitness\n                self.x_opt = population[np.argmin(fitness)]\n                stagnation_counter = 0\n\n            if stagnation_counter > self.stagnation_threshold:\n                # Restart population\n                population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                self.f_opt = np.min(fitness)\n                self.x_opt = population[np.argmin(fitness)]\n                self.best_fitness_history.append(self.f_opt)\n                stagnation_counter = 0 # Reset the counter after restart\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "An exception occurred: Cannot take a larger sample than population when 'replace=False'.", "error": "", "parent_ids": ["88a29709-a214-448e-9ced-ba1f8be325e6"], "operator": null, "metadata": {}}
{"id": "e3e53d43-50db-48b4-9e49-be10f682a0f9", "fitness": 0.7732308801229705, "name": "AdaptiveCauchyDE", "description": "Simplified Adaptive Differential Evolution with Cauchy mutation, reducing parameter count and complexity while retaining key features.", "code": "import numpy as np\n\nclass AdaptiveCauchyDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, CR=0.7, initial_scale=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.lb = -5.0\n        self.ub = 5.0\n        self.scale = initial_scale\n        self.scale_decay = 0.995\n        self.exploration_prob = 0.1\n\n    def __call__(self, func):\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Differential Evolution\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                a, b, c = population[idxs]\n                mutant = a + self.F * (b - c)\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Cauchy Mutation\n                if np.random.rand() < self.exploration_prob:\n                    cauchy_noise = self.scale * np.random.standard_cauchy(size=self.dim)\n                    mutant += cauchy_noise\n                    mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluate trial solution\n                f_trial = func(trial)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                # Selection\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            self.scale = max(self.scale * self.scale_decay, 0.001)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveCauchyDE scored 0.773 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5b9d83ee-7904-4248-af09-e2549dfae948"], "operator": null, "metadata": {"aucs": [0.38933432649512145, 0.7270556354600106, 0.8254783583967267, 0.9191434751997501, 0.8779406061725752, 0.8809944281309144, 0.8262630723774987, 0.8160077751849106, 0.870784910354769, 0.8355016277151255, 0.8745590591735011, 0.9952960188136214, 0.466907825639865, 0.8635873773605215, 0.9327974785980986, 0.8730860949698953, 0.7750503901458625, 0.9125520792179447, 0.27310490129713294, 0.5291721617555653]}}
{"id": "620ce6ac-18bb-4640-9808-7ee64c903495", "fitness": 0.697388257591437, "name": "AdaptiveCauchyDE", "description": "Simplified Adaptive Cauchy DE with reduced parameters, focusing on Cauchy mutation for exploration and DE for exploitation, adapting exploration probability and Cauchy scale.", "code": "import numpy as np\n\nclass AdaptiveCauchyDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, CR=0.7, initial_scale=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.lb = -5.0\n        self.ub = 5.0\n        self.scale = initial_scale\n        self.scale_decay = 0.995\n        self.min_scale = 0.001\n        self.exploration_prob = 0.1  # Probability of Cauchy mutation\n\n    def __call__(self, func):\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Differential Evolution\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = population[a] + self.F * (population[b] - population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n                \n                # Cauchy Mutation (with probability exploration_prob)\n                if np.random.rand() < self.exploration_prob:\n                    cauchy_noise = self.scale * np.random.standard_cauchy(size=self.dim)\n                    mutant = population[i] + cauchy_noise\n                    mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluate trial solution\n                f_trial = func(trial)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                # Selection: Replace if trial is better\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            self.scale = max(self.scale * self.scale_decay, self.min_scale)\n            self.exploration_prob = min(0.5, self.exploration_prob * 1.05)  # Increase exploration over time\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveCauchyDE scored 0.697 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5b9d83ee-7904-4248-af09-e2549dfae948"], "operator": null, "metadata": {"aucs": [0.2237035941463953, 0.6479397359756445, 0.6525473744740989, 0.9041440019707837, 0.7561385143048236, 0.8326935886207463, 0.6337593568612608, 0.7073621401686966, 0.7842404192790413, 0.7222046567340424, 0.8755104881807496, 0.9974512793704269, 0.40012918279634346, 0.7171884151392864, 0.9268283990645322, 0.8437755573271659, 0.6359304415611944, 0.8748730048119674, 0.24725719825663528, 0.5640878027849041]}}
{"id": "3a49f4aa-7d9c-4379-9df6-0704c22d8b7f", "fitness": 0.3092567312284674, "name": "AdaptiveHybridDECauchyLocalRestart", "description": "Adaptive hybrid DE with Cauchy mutation, orthogonal crossover, local search, and restart mechanism for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveHybridDECauchyLocalRestart:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, CR=0.7, initial_scale=0.1, local_search_prob=0.1, restart_prob=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.lb = -5.0\n        self.ub = 5.0\n        self.scale = initial_scale\n        self.scale_decay = 0.99\n        self.min_scale = 0.001\n        self.local_search_prob = local_search_prob\n        self.restart_prob = restart_prob\n\n    def __call__(self, func):\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Differential Evolution Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = population[a] + self.F * (population[b] - population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Cauchy mutation\n                mutant = mutant + self.scale * np.random.standard_cauchy(size=self.dim)\n                mutant = np.clip(mutant, self.lb, self.ub)\n                \n                # Orthogonal Crossover\n                trial = self.orthogonal_crossover(population[i], mutant)\n                \n                # Local search around the best solution\n                if np.random.rand() < self.local_search_prob:\n                    trial = self.x_opt + self.scale * np.random.normal(0, 1, self.dim)\n                    trial = np.clip(trial, self.lb, self.ub)\n\n                # Evaluate and selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Scale adaptation\n            self.scale = max(self.scale * self.scale_decay, self.min_scale)\n            \n            # Restart mechanism\n            if np.random.rand() < self.restart_prob:\n                population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                if self.budget <= 0:\n                    break\n\n                self.f_opt = np.min(fitness)\n                self.x_opt = population[np.argmin(fitness)]\n\n        return self.f_opt, self.x_opt\n\n    def orthogonal_crossover(self, x, mutant, num_combinations=3):\n        trial = x.copy()\n        for _ in range(num_combinations):\n            i, j = np.random.choice(self.dim, 2, replace=False)\n            rand_val = np.random.rand()\n            trial[i] = rand_val * x[i] + (1 - rand_val) * mutant[i]\n            trial[j] = (1 - rand_val) * x[j] + rand_val * mutant[j]\n            trial = np.clip(trial, self.lb, self.ub)\n        return trial", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveHybridDECauchyLocalRestart scored 0.309 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["97382748-a8fc-48cd-ac7d-924afa1b22af"], "operator": null, "metadata": {"aucs": [0.2186496564451974, 0.2746304006969621, 0.46720033312430886, 0.3885882861223572, 0.28188971912445626, 0.545312596098054, 0.3115739022939933, 0.38409310756017434, 0.31189540796089843, 0.21799063408673947, 0]}}
{"id": "4a8eaec5-e9c2-4449-aa26-705fd33990fa", "fitness": 0.573778311124014, "name": "HybridDECauchy", "description": "Simplified hybrid DE with Cauchy mutation and adaptive scaling, removing local search for efficiency.", "code": "import numpy as np\n\nclass HybridDECauchy:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, CR=0.7, initial_scale=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.lb = -5.0\n        self.ub = 5.0\n        self.scale = initial_scale\n        self.scale_decay = 0.99\n        self.min_scale = 0.001\n\n    def __call__(self, func):\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Differential Evolution Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = population[a] + self.F * (population[b] - population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Cauchy mutation\n                mutant = mutant + self.scale * np.random.standard_cauchy(size=self.dim)\n                mutant = np.clip(mutant, self.lb, self.ub)\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluate and selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Scale adaptation\n            self.scale = max(self.scale * self.scale_decay, self.min_scale)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm HybridDECauchy scored 0.574 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["97382748-a8fc-48cd-ac7d-924afa1b22af"], "operator": null, "metadata": {"aucs": [0.2404172779557331, 0.4613845736979948, 0.5195699450920035, 0.8945747327778482, 0.536523295201181, 0.6118570577080222, 0.38838916344763696, 0.46012024842605304, 0.5341593694482628, 0.4795995303947017, 0.8500204545189617, 0.9970353318291256, 0.297702377280747, 0.5331733563331528, 0.9201507910058577, 0.6048909736203368, 0.454890460697729, 0.7236112057236828, 0.4349959217293029, 0.5325001555919446]}}
{"id": "d6380b5b-f8d7-4cd9-a0f3-2777363be15b", "fitness": 0.33199484800132895, "name": "AdaptiveDEPolynomialLocalSearch", "description": "Hybrid DE with a self-adaptive strategy based on past successful mutation scales and directions, combined with a polynomial mutation-based local search.", "code": "import numpy as np\n\nclass AdaptiveDEPolynomialLocalSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, CR=0.7, local_search_prob=0.1, ls_eta=20):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.lb = -5.0\n        self.ub = 5.0\n        self.local_search_prob = local_search_prob\n        self.ls_eta = ls_eta  # Polynomial mutation parameter\n        self.archive = []  # Archive of successful mutation vectors\n\n    def __call__(self, func):\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Differential Evolution Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                \n                # Adaptive F: sample from archive if available\n                if self.archive:\n                    idx = np.random.randint(len(self.archive))\n                    F = self.archive[idx][0]  # Use successful F\n                    mutation_vector = self.archive[idx][1] # Use a successful direction\n                    mutant = population[a] + F * mutation_vector\n                else:\n                    mutant = population[a] + self.F * (population[b] - population[c]) # Original DE\n\n                mutant = np.clip(mutant, self.lb, self.ub)\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Polynomial Mutation based Local Search\n                if np.random.rand() < self.local_search_prob:\n                    delta = (self.ub - self.lb)\n                    u = np.random.rand(self.dim)\n                    delta1 = (trial - self.lb) / delta\n                    delta2 = (self.ub - trial) / delta\n                    mut_pow = 1.0 / (self.ls_eta + 1.0)\n                    \n                    for j in range(self.dim):\n                        if u[j] < 0.5:\n                            trial[j] = trial[j] + (delta * (2 * u[j])**(mut_pow) * delta1[j])\n                        else:\n                            trial[j] = trial[j] - (delta * (2 * (1 - u[j]))**(mut_pow) * delta2[j])\n                    \n                    trial = np.clip(trial, self.lb, self.ub)\n\n\n                # Evaluate and selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                if f_trial < fitness[i]:\n                    # Archive successful F and mutation direction\n                    if self.archive and len(self.archive) > self.pop_size:\n                        self.archive.pop(0)\n                    \n                    if self.archive and len(self.archive) > self.pop_size:\n                        self.archive.pop(0)\n\n                    if not self.archive or len(self.archive) < self.pop_size:\n                        mutation_vector = population[b] - population[c]\n                        self.archive.append((self.F, mutation_vector))\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveDEPolynomialLocalSearch scored 0.332 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["97382748-a8fc-48cd-ac7d-924afa1b22af"], "operator": null, "metadata": {"aucs": [0.15379518311832385, 0.23621624893437, 0.3204846754988828, 0.28828337675219173, 0.23202335808061836, 0.31909406976712684, 0.2767742742457463, 0.27599266282403656, 0.2582998094357237, 0.21490771738569514, 0.2803236706689407, 0.9980227049892911, 0.23144861505084036, 0.2584251633061484, 0.6693766758169017, 0.3198957762547696, 0.2697584321625558, 0.36526580970741906, 0.18855392706769625, 0.48295480895930076]}}
{"id": "2ae79d4e-ca92-44e4-949a-b02645c4d284", "fitness": 0.5576398590803358, "name": "SimplifiedHybridDE", "description": "Simplified hybrid DE with adaptive mutation scaling based on success and CMA-ES-inspired perturbation for improved exploration.", "code": "import numpy as np\n\nclass SimplifiedHybridDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_scale=0.3, cma_rank_one=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.scale = initial_scale\n        self.scale_decay = 0.99\n        self.min_scale = 0.01\n        self.cma_rank_one = cma_rank_one\n\n        self.mean = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.p_c = np.zeros(self.dim)\n\n        self.F = 0.5\n        self.CR = 0.7\n\n    def __call__(self, func):\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Differential Evolution Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = population[a] + self.F * (population[b] - population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Gaussian perturbation with CMA-ES adaptation\n                z = np.random.normal(0, 1, self.dim)\n                mutant = mutant + self.scale * np.dot(self.C, z)\n                mutant = np.clip(mutant, self.lb, self.ub)\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluate and selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n                    # CMA-ES Rank-One Update\n                    self.p_c = (1 - self.cma_rank_one) * self.p_c + np.sqrt(self.cma_rank_one * (2 - self.cma_rank_one)) * z\n                    self.C = (1 - self.cma_rank_one) * self.C + self.cma_rank_one * np.outer(self.p_c, self.p_c)\n                    \n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n            \n            # Scale adaptation\n            self.scale = max(self.scale * self.scale_decay, self.min_scale)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm SimplifiedHybridDE scored 0.558 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["88a29709-a214-448e-9ced-ba1f8be325e6"], "operator": null, "metadata": {"aucs": [0.25677674494119995, 0.4774454854674003, 0.5081399045843571, 0.9076174461760654, 0.5140251926249912, 0.5897555724101353, 0.4075192111165291, 0.4575377172596461, 0.49043751585570283, 0.43685114902966093, 0.8210461761841473, 0.9968737758485886, 0.3496910873555882, 0.5159985478771825, 0.8973824295317874, 0.5830930990028664, 0.4408137072866525, 0.6961716767198678, 0.28550200131720205, 0.5201187410171447]}}
{"id": "9c919029-ec58-4a7e-8388-34f05019dc36", "fitness": 0.44405416138268167, "name": "HybridDEOD", "description": "Simplified Hybrid DE with adaptive scaling, orthogonal crossover, and a jitter-based diversity mechanism.", "code": "import numpy as np\n\nclass HybridDEOD:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, CR=0.7, initial_scale=0.3, diversity_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.lb = -5.0\n        self.ub = 5.0\n        self.scale = initial_scale\n        self.diversity_prob = diversity_prob  # Probability of applying diversity mechanism\n\n    def orthogonal_design(self, num_points, dim):\n        points = np.zeros((num_points, dim))\n        for i in range(dim):\n            points[:, i] = np.random.permutation(num_points)\n        points = (points + np.random.rand(num_points, dim)) / num_points\n        return points\n\n    def __call__(self, func):\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Differential Evolution Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = population[a] + self.F * (population[b] - population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Adaptive Gaussian perturbation\n                mutant = mutant + self.scale * np.random.normal(0, 1, self.dim)\n                mutant = np.clip(mutant, self.lb, self.ub)\n                \n                # Orthogonal Design Crossover\n                od_points = self.orthogonal_design(self.dim, 1).flatten()\n                trial = population[i].copy()\n                for j in range(self.dim):\n                    if od_points[j] < self.CR:\n                        trial[j] = mutant[j]\n\n                # Diversity mechanism: Random jittering\n                if np.random.rand() < self.diversity_prob:\n                    trial += np.random.uniform(-self.scale, self.scale, self.dim)\n                    trial = np.clip(trial, self.lb, self.ub)\n\n                # Evaluate and selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Scale adaptation\n            if np.random.rand() < 0.1:\n                self.scale *= 0.9  # Occasional scale reduction\n            elif np.random.rand() < 0.1:\n                self.scale *= 1.1\n\n            self.scale = np.clip(self.scale, 0.01, 0.5) #keep scaling factor in reasonable bounds\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm HybridDEOD scored 0.444 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["393e1e12-1442-4950-9b73-cc7c60fff4a0"], "operator": null, "metadata": {"aucs": [0.21649438861326387, 0.36583663270995415, 0.41750070926665983, 0.5961566750724967, 0.32288183714890795, 0.4659449552184336, 0.32122298003154237, 0.35949808138535244, 0.3488552009558208, 0.26227063412662766, 0.5783776046321009, 0.9983623727725146, 0.3146191577799111, 0.40322285201512365, 0.8497866263028957, 0.4078364382358435, 0.36297684688971243, 0.4796020830706703, 0.28733056257650647, 0.5223065888492928]}}
{"id": "4bd4556e-9cc2-4347-a11a-feaf800d3eb3", "fitness": 0.5456002362774949, "name": "EnhancedHybridDE", "description": "Simplified hybrid DE with adaptive F/CR, CMA-ES inspired Gaussian perturbation, and population diversity using the archive.", "code": "import numpy as np\n\nclass EnhancedHybridDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_scale=0.3, diversity_weight=0.1, f_adapt=True, cr_adapt=True):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.scale = initial_scale\n        self.scale_decay = 0.99\n        self.min_scale = 0.01\n        self.diversity_weight = diversity_weight\n        self.f_adapt = f_adapt\n        self.cr_adapt = cr_adapt\n\n        self.archive = []  # For diversity maintenance\n        self.archive_size = pop_size * 10\n        self.F = 0.5 # initial value\n        self.CR = 0.7 # initial value\n\n    def __call__(self, func):\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                if self.f_adapt:\n                    F = np.random.normal(0.5, 0.3) # gaussian\n                    F = np.clip(F, 0.1, 1.0)\n                else:\n                    F = self.F\n\n                if self.cr_adapt:\n                    CR = np.random.normal(0.7, 0.1) # gaussian\n                    CR = np.clip(CR, 0.1, 0.9)\n                else:\n                    CR = self.CR\n                \n                # Differential Evolution Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = population[a] + F * (population[b] - population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Gaussian perturbation\n                mutant = mutant + self.scale * np.random.normal(0, 1, self.dim)\n                mutant = np.clip(mutant, self.lb, self.ub)\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluate and selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    \n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                \n                # Archive for diversity\n                if len(self.archive) < self.archive_size:\n                    self.archive.append(population[i].copy())\n                else:\n                    idx = np.random.randint(0, self.archive_size)\n                    self.archive[idx] = population[i].copy()\n            \n            # Scale adaptation\n            self.scale = max(self.scale * self.scale_decay, self.min_scale)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm EnhancedHybridDE scored 0.546 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["88a29709-a214-448e-9ced-ba1f8be325e6"], "operator": null, "metadata": {"aucs": [0.24473863561259268, 0.5038786562521038, 0.4981496087462909, 0.8390333599196647, 0.4869887750698889, 0.5571281697652215, 0.36946659837430473, 0.43421031461427784, 0.4834154792373967, 0.4231785817188648, 0.8300636901523744, 0.9995559306127427, 0.3386591307437191, 0.4885264866024296, 0.8903815505295211, 0.5815889796631433, 0.43022234050985453, 0.6695730527049201, 0.31182418862776995, 0.531421196092817]}}
{"id": "cb28f943-4ec5-4523-ab68-c19d6ce2cfbd", "fitness": 0.3245334653512249, "name": "CovarianceMatrixDE", "description": "Combines differential evolution with a Gaussian mutation based on the population's covariance matrix and CMA-ES-inspired selection.", "code": "import numpy as np\n\nclass CovarianceMatrixDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, CR=0.7, initial_sigma=0.1, learning_rate=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.lb = -5.0\n        self.ub = 5.0\n        self.sigma = initial_sigma\n        self.learning_rate = learning_rate\n        self.mean = None\n        self.C = None  # Covariance matrix\n        \n\n    def __call__(self, func):\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n        self.mean = np.mean(population, axis=0)\n        self.C = np.cov(population.T)\n\n        while self.budget > 0:\n            new_population = np.zeros_like(population)\n            new_fitness = np.zeros_like(fitness)\n\n            for i in range(self.pop_size):\n                # Differential Evolution Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = population[a] + self.F * (population[b] - population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Covariance Matrix Adaptation Gaussian Mutation\n                mutant = mutant + self.sigma * np.random.multivariate_normal(np.zeros(self.dim), self.C)\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluate and selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                new_population[i] = trial\n                new_fitness[i] = f_trial\n                \n            # CMA-ES inspired update (simplified)\n            best_idx = np.argmin(new_fitness)\n            diff = new_population[best_idx] - self.mean\n            self.mean = (1 - self.learning_rate) * self.mean + self.learning_rate * new_population[best_idx]\n            self.C = (1 - self.learning_rate) * self.C + self.learning_rate * np.outer(diff, diff)\n\n            #Elitism\n            worst_idx = np.argmax(new_fitness)\n            if self.f_opt < new_fitness[worst_idx]:\n                new_population[worst_idx] = self.x_opt\n                new_fitness[worst_idx] = self.f_opt\n\n\n            population = new_population\n            fitness = new_fitness\n\n            self.f_opt = np.min(fitness)\n            self.x_opt = population[np.argmin(fitness)]\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm CovarianceMatrixDE scored 0.325 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["97382748-a8fc-48cd-ac7d-924afa1b22af"], "operator": null, "metadata": {"aucs": [0.1489207921721225, 0.1728359138806349, 0.2885711441444191, 0.3438057922551563, 0.2546550513636039, 0.29102343023104704, 0.2596646200850635, 0.23286644842104898, 0.22782871019196782, 0.17851919533970717, 0.32057302204983074, 0.9993225104931516, 0.2463649964724398, 0.25329651920400964, 0.6667224465706694, 0.3333537874657265, 0.24501512804852854, 0.35387551172552134, 0.18764032190348812, 0.48581396500636165]}}
{"id": "e1add5b8-f830-4f11-aa0b-e1b7ce1063a8", "fitness": 0.5798578988725227, "name": "SimplifiedHybridDE", "description": "Simplified hybrid DE with CMA-ES-inspired adaptation, focusing on essential components and adaptive parameter control.", "code": "import numpy as np\n\nclass SimplifiedHybridDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_scale=0.2, cma_rank_one=0.1, f_adapt=True, cr_adapt=True):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.scale = initial_scale\n        self.scale_decay = 0.99\n        self.min_scale = 0.01\n        self.cma_rank_one = cma_rank_one\n        self.f_adapt = f_adapt\n        self.cr_adapt = cr_adapt\n\n        self.C = np.eye(self.dim)\n        self.p_c = np.zeros(self.dim)\n\n        self.F = 0.5 # initial value\n        self.CR = 0.7 # initial value\n\n    def __call__(self, func):\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                if self.f_adapt:\n                    self.F = np.clip(np.random.normal(0.5, 0.3), 0.1, 1.0)\n                if self.cr_adapt:\n                    self.CR = np.clip(np.random.normal(0.7, 0.1), 0.1, 0.9)\n                \n                # Differential Evolution Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = population[a] + self.F * (population[b] - population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Gaussian perturbation with CMA-ES adaptation\n                z = np.random.normal(0, 1, self.dim)\n                mutant = mutant + self.scale * np.dot(self.C, z)\n                mutant = np.clip(mutant, self.lb, self.ub)\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluate and selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n                    # CMA-ES Rank-One Update\n                    self.p_c = (1 - self.cma_rank_one) * self.p_c + np.sqrt(self.cma_rank_one * (2 - self.cma_rank_one)) * z\n                    self.C = (1 - self.cma_rank_one) * self.C + self.cma_rank_one * np.outer(self.p_c, self.p_c)\n                    \n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n            \n            # Scale adaptation\n            self.scale = max(self.scale * self.scale_decay, self.min_scale)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm SimplifiedHybridDE scored 0.580 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["88a29709-a214-448e-9ced-ba1f8be325e6"], "operator": null, "metadata": {"aucs": [0.3031092878018755, 0.564559890376001, 0.5263686505765237, 0.8811953452094187, 0.530309133889349, 0.6123088153241896, 0.4311253195681518, 0.4615653694523587, 0.5306840051130719, 0.4560253302257262, 0.864855633262068, 0.9917869503260032, 0.33273263507535633, 0.5226493206148547, 0.9142957158569467, 0.5937533057407682, 0.45460792811218875, 0.7477355488114901, 0.3503499938774165, 0.5271397982366948]}}
{"id": "4143acaa-8e10-4437-afc1-80bad620f7b3", "fitness": 0.4484317251621463, "name": "AdaptiveDE", "description": "An adaptive differential evolution algorithm with a self-adjusting strategy based on the performance feedback of Cauchy and Gaussian mutations.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, CR=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.success_cauchy = 0\n        self.success_gaussian = 0\n        self.total_cauchy = 0\n        self.total_gaussian = 0\n        self.cauchy_prob = 0.5  # Initial probability for Cauchy mutation\n        self.gaussian_scale = 0.1\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n    def mutate(self, i):\n        idxs = [idx for idx in range(self.pop_size) if idx != i]\n        a, b, c = np.random.choice(idxs, 3, replace=False)\n        mutant = self.population[a] + self.F * (self.population[b] - self.population[c])\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def crossover(self, mutant, i):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, self.population[i])\n        return trial\n    \n    def cauchy_mutation(self, x):\n        self.total_cauchy += 1\n        cauchy_noise = 0.1 * np.random.standard_cauchy(size=self.dim) # Fixed scale for Cauchy\n        mutated_x = x + cauchy_noise\n        return np.clip(mutated_x, self.lb, self.ub)\n\n    def gaussian_mutation(self, x):\n         self.total_gaussian += 1\n         gaussian_noise = self.gaussian_scale * np.random.normal(size=self.dim)\n         mutated_x = x + gaussian_noise\n         return np.clip(mutated_x, self.lb, self.ub)\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                mutant = self.mutate(i)\n                trial = self.crossover(mutant, i)\n\n                # Adaptive Mutation Selection\n                if np.random.rand() < self.cauchy_prob:\n                    trial_mutation = self.cauchy_mutation(trial)\n                else:\n                    trial_mutation = self.gaussian_mutation(trial)\n\n                f_trial = func(trial_mutation)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                if f_trial < self.fitness[i]:\n                    if np.random.rand() < self.cauchy_prob:\n                        self.success_cauchy += 1\n                    else:\n                        self.success_gaussian += 1\n                    self.population[i] = trial_mutation\n                    self.fitness[i] = f_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial_mutation\n                \n                # Adjust gaussian scale\n                self.gaussian_scale = 0.1 * np.exp(np.random.normal(0, 0.1))  # Log-normal update\n\n            # Update Cauchy probability based on success\n            if self.total_cauchy > 0 and self.total_gaussian > 0:\n                success_rate_cauchy = self.success_cauchy / self.total_cauchy\n                success_rate_gaussian = self.success_gaussian / self.total_gaussian\n                self.cauchy_prob = success_rate_cauchy / (success_rate_cauchy + success_rate_gaussian + 1e-9)\n\n            # Reset counters\n            self.success_cauchy = 0\n            self.success_gaussian = 0\n            self.total_cauchy = 0\n            self.total_gaussian = 0\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveDE scored 0.448 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5b9d83ee-7904-4248-af09-e2549dfae948"], "operator": null, "metadata": {"aucs": [0.1842821190185686, 0.3731391369867343, 0.43410207835655656, 0.5280415961867115, 0.3301561151197735, 0.4692976796274725, 0.321921571540595, 0.34985410925881355, 0.38196640916879565, 0.27908108932679543, 0.7316617297514698, 0.9946098862499206, 0.29503198630024485, 0.3860285255107896, 0.7747182870276643, 0.41490265602347853, 0.3653516771256875, 0.5745699518575169, 0.262738979203893, 0.5171789196014437]}}
