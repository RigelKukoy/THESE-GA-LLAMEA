{"id": "a62acce2-bd54-49de-8819-44f4d23c1329", "fitness": 0.0, "name": "HybridPSO_DE", "description": "Population-based algorithm that combines aspects of particle swarm optimization (PSO) and differential evolution (DE) with velocity clamping and adaptive parameter control.", "code": "import numpy as np\n\nclass HybridPSO_DE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, w=0.7, c1=1.5, c2=1.5, cr=0.7, f=0.8, v_max_ratio=0.2):\n        \"\"\"\n        Initialize the Hybrid PSO-DE algorithm.\n\n        Args:\n            budget (int): Total number of function evaluations.\n            dim (int): Dimensionality of the problem.\n            pop_size (int): Population size.\n            w (float): Inertia weight for PSO.\n            c1 (float): Cognitive coefficient for PSO.\n            c2 (float): Social coefficient for PSO.\n            cr (float): Crossover rate for DE.\n            f (float): Scaling factor for DE.\n            v_max_ratio (float): Ratio to determine maximum velocity.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w = w\n        self.c1 = c1\n        self.c2 = c2\n        self.cr = cr\n        self.f = f\n        self.v_max_ratio = v_max_ratio\n        self.pop = None\n        self.fitness = None\n        self.velocities = None\n        self.x_best_global = None\n        self.f_best_global = np.inf\n        self.lb = None\n        self.ub = None\n        self.v_max = None\n\n    def initialize_population(self, func):\n        \"\"\"\n        Initialize the population within the bounds of the function.\n\n        Args:\n            func: The black-box optimization function.\n        \"\"\"\n        self.lb = func.bounds.lb\n        self.ub = func.bounds.ub\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.velocities = np.zeros((self.pop_size, self.dim))\n\n        self.x_best_global = self.pop[np.argmin(self.fitness)].copy()\n        self.f_best_global = np.min(self.fitness)\n        self.v_max = self.v_max_ratio * (self.ub - self.lb)\n\n    def pso_update(self, func):\n        \"\"\"\n        Update the population using PSO principles.\n\n        Args:\n            func: The black-box optimization function.\n        \"\"\"\n        personal_best_locations = self.pop.copy()\n        personal_best_fitness = self.fitness.copy()\n\n        for i in range(self.pop_size):\n            # Update velocities\n            r1 = np.random.rand(self.dim)\n            r2 = np.random.rand(self.dim)\n            self.velocities[i] = self.w * self.velocities[i] + \\\n                                 self.c1 * r1 * (personal_best_locations[i] - self.pop[i]) + \\\n                                 self.c2 * r2 * (self.x_best_global - self.pop[i])\n\n            # Velocity clamping\n            self.velocities[i] = np.clip(self.velocities[i], -self.v_max, self.v_max)\n\n            # Update positions\n            self.pop[i] = self.pop[i] + self.velocities[i]\n\n            # Boundary handling (clip to bounds)\n            self.pop[i] = np.clip(self.pop[i], self.lb, self.ub)\n            \n            #Evaluate new positions\n            new_fitness = func(self.pop[i])\n\n            if new_fitness < self.fitness[i]:\n                self.fitness[i] = new_fitness\n                personal_best_locations[i] = self.pop[i].copy()\n\n            if new_fitness < self.f_best_global:\n                 self.f_best_global = new_fitness\n                 self.x_best_global = self.pop[i].copy()\n\n    def de_update(self, func):\n        \"\"\"\n        Update the population using Differential Evolution principles.\n\n        Args:\n            func: The black-box optimization function.\n        \"\"\"\n        for i in range(self.pop_size):\n            # Choose three random indices, distinct from each other and i\n            idxs = list(range(self.pop_size))\n            idxs.remove(i)\n            a, b, c = np.random.choice(idxs, 3, replace=False)\n\n            # Mutation\n            v = self.pop[a] + self.f * (self.pop[b] - self.pop[c])\n            v = np.clip(v, self.lb, self.ub)\n\n            # Crossover\n            u = np.zeros(self.dim)\n            j_rand = np.random.randint(0, self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.cr or j == j_rand:\n                    u[j] = v[j]\n                else:\n                    u[j] = self.pop[i, j]\n\n            # Selection\n            f_u = func(u)\n            if f_u < self.fitness[i]:\n                self.pop[i] = u\n                self.fitness[i] = f_u\n\n                if f_u < self.f_best_global:\n                    self.f_best_global = f_u\n                    self.x_best_global = u.copy()\n\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using the Hybrid PSO-DE algorithm.\n\n        Args:\n            func: The black-box optimization function.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        self.initialize_population(func)\n        eval_count = self.pop_size # Account for initial population evaluation\n\n        while eval_count < self.budget:\n            # Adaptive parameter adjustment (example: linear decrease of inertia weight)\n            self.w = 0.7 - 0.5 * (eval_count / self.budget)\n            \n            #Alternate between PSO and DE updates\n            if eval_count % 2 == 0:\n                self.pso_update(func)\n            else:\n                self.de_update(func)\n            \n            eval_count = np.sum([1 for i in range(self.pop_size) if i in range(self.pop_size)])+ self.pop_size #Simple way to account for the budget.\n\n            eval_count = np.sum([1 for i in range(self.pop_size) if i in range(self.pop_size)]) + self.pop_size\n\n\n            if eval_count > self.budget:\n                break\n\n        return self.f_best_global, self.x_best_global", "configspace": "", "generation": 0, "feedback": "The algorithm HybridPSO_DE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0]}}
{"id": "a7a19607-6565-4500-b116-c21e24a52f8b", "fitness": 0.6221893106202938, "name": "CMAES", "description": "Covariance matrix adaptation evolution strategy with resampling and budget-aware adaptation.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, mu_factor=0.25, initial_sigma=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.mu = int(dim * mu_factor)  # Number of parents\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cc = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.mean = np.zeros(dim)\n        self.sigma = initial_sigma\n        self.pc = np.zeros(dim)\n        self.ps = np.zeros(dim)\n        self.C = np.eye(dim)\n        self.B = np.eye(dim)\n        self.D = np.ones(dim)\n        self.C_eigen_age = 0\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.archive_x = []\n        self.archive_f = []\n\n    def sample_population(self, popsize, func):\n        z = np.random.normal(0, 1, size=(popsize, self.dim))\n        x = self.mean + self.sigma * (self.B @ (self.D * z).T).T\n        \n        # Handle boundary constraints using resampling\n        for i in range(popsize):\n            while not func.bounds.lb.min() <= x[i].min() and x[i].max() <= func.bounds.ub.max():\n                z[i] = np.random.normal(0, 1, self.dim)\n                x[i] = self.mean + self.sigma * (self.B @ (self.D * z[i]).T).T\n        \n        f = np.array([func(xi) for xi in x])\n        return x, f, z\n\n    def update_distribution(self, x, f, z, popsize):\n        idx = np.argsort(f)\n        x = x[idx]\n        z = z[idx]\n        \n        x_mu = x[:self.mu]\n        z_mu = z[:self.mu]\n        \n        self.mean = np.sum(self.weights.reshape(-1, 1) * x_mu, axis=0)\n\n        zmean = np.sum(self.weights.reshape(-1, 1) * z_mu, axis=0)\n        self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * (self.B @ zmean)\n        hsig = np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * (self.budget // popsize))) < 1.4 + 2 / (self.dim + 1)\n        self.pc = (1 - self.cc) * self.pc + hsig * np.sqrt(self.cc * (2 - self.cc) * self.mueff) * (self.mean - self.mean) / self.sigma \n\n        artmp = (1 / self.sigma) * (x_mu - self.mean).T\n        self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * (self.pc.reshape(-1, 1) @ self.pc.reshape(1, -1) + (1-hsig) * self.cc * (2-self.cc) * self.C) + self.cmu * artmp @ np.diag(self.weights) @ artmp.T\n\n        self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * (self.budget // popsize))) / np.sqrt(self.dim) - 1))\n\n    def __call__(self, func):\n        popsize = 4 + int(3 * np.log(self.dim))\n        evals = 0\n\n        while evals < self.budget:\n            x, f, z = self.sample_population(popsize, func)\n            evals += popsize\n\n            for i in range(popsize):\n                if f[i] < self.f_opt:\n                    self.f_opt = f[i]\n                    self.x_opt = x[i]\n                    \n            self.update_distribution(x, f, z, popsize)\n            self.C_eigen_age += 1\n            if self.C_eigen_age > self.budget // (10 * popsize): # Re-compute eigenvalue decomposition after a while\n                self.C_eigen_age = 0\n                self.C = np.triu(self.C) + np.triu(self.C, 1).T\n                self.D, self.B = np.linalg.eigh(self.C)\n                self.D = np.sqrt(self.D)\n                \n                # Avoid tiny values\n                self.D[self.D < 1e-10] = 1e-10\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm CMAES scored 0.622 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1827404923494228, 0.18290181833190255, 0.948638860176059, 0.9755832495940577, 0.9472154982265432, 0.9551158755964891, 0.3332837681090951, 0.9386398293250072, 0.9492415250978216, 0.1545580017080883, 0.9486573651594475, 0.9910294653733174, 0.23184443232931495, 0.35504241643578904, 0.6007989179507356, 0.3171676931099987, 0.7449435618089205, 0.9613559412632026, 0.23344861440546327, 0.49157888605520084]}}
{"id": "48c1b8ff-bf5a-49b7-8be8-4781b231a43a", "fitness": 0.4217794505266698, "name": "AdaptivePopulationSearch", "description": "Population-based algorithm with adaptive exploration and exploitation based on fitness landscape analysis, using a combination of differential evolution and local search.", "code": "import numpy as np\n\nclass AdaptivePopulationSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=50, de_rate=0.7, ls_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.de_rate = de_rate\n        self.ls_rate = ls_rate\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n        \n        archive = []\n\n        while self.budget > 0:\n            # Adaptive strategy selection\n            if np.random.rand() < self.de_rate:\n                # Differential Evolution\n                for i in range(self.pop_size):\n                    if self.budget <= 0:\n                        break\n\n                    idxs = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = population[idxs]\n                    \n                    mutated = population[i] + 0.5 * (x_r2 - x_r3)\n                    \n                    # Crossover\n                    j_rand = np.random.randint(self.dim)\n                    for j in range(self.dim):\n                        if np.random.rand() > 0.9 and j != j_rand:\n                            mutated[j] = population[i][j]\n                    \n                    mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)    \n                    f_mutated = func(mutated)\n                    self.budget -= 1\n\n                    if f_mutated < fitness[i]:\n                        fitness[i] = f_mutated\n                        population[i] = mutated\n\n                        if f_mutated < self.f_opt:\n                            self.f_opt = f_mutated\n                            self.x_opt = mutated\n                            \n            elif np.random.rand() < self.ls_rate:\n                # Local Search (perturb the best solution)\n                if self.budget <= 0:\n                    break\n                \n                x_perturbed = self.x_opt + np.random.normal(0, 0.1, size=self.dim)\n                x_perturbed = np.clip(x_perturbed, func.bounds.lb, func.bounds.ub)\n                \n                f_perturbed = func(x_perturbed)\n                self.budget -= 1\n                \n                if f_perturbed < self.f_opt:\n                    self.f_opt = f_perturbed\n                    self.x_opt = x_perturbed\n            else:\n                # Global search, random restart with probability 1-de_rate-ls_rate\n                if self.budget <= 0:\n                    break\n                \n                x_rand = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                f_rand = func(x_rand)\n                self.budget -= 1\n                \n                if f_rand < self.f_opt:\n                    self.f_opt = f_rand\n                    self.x_opt = x_rand\n\n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n              self.f_opt = fitness[best_index]\n              self.x_opt = population[best_index]\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptivePopulationSearch scored 0.422 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1485610311111505, 0.3117474464291802, 0.40482134733722397, 0.592540779645649, 0.31696361999505784, 0.45382439427029775, 0.28853071512796247, 0.34442904342459446, 0.3260783030398442, 0.18731860625189212, 0.6248713440771438, 0.9860660536263224, 0.3851292684228965, 0.3172952359337732, 0.7668838802999223, 0.43600439600863006, 0.33303335983829385, 0.5300225143169673, 0.19586487611809889, 0.4856027952584956]}}
{"id": "a7bac7ad-af96-47be-8d44-4e02f1467af3", "fitness": 0.4453609250723517, "name": "AdaptiveDE", "description": "An adaptive differential evolution strategy that adjusts mutation and crossover rates based on success.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Initial mutation factor\n        self.CR = CR  # Initial crossover rate\n        self.archive_size = int(self.pop_size * 0.2)  # Archive size for storing successful solutions\n        self.archive = []\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        # Evolution loop\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[indices]\n\n                # Adaptive F: Perturb F based on success\n                F_current = self.F + np.random.normal(0, 0.05)\n                F_current = np.clip(F_current, 0.1, 1.0)\n\n                v = x_r1 + F_current * (x_r2 - x_r3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.copy(population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        u[j] = v[j]\n\n                # Evaluation\n                f_u = func(u)\n                self.budget -= 1\n                if f_u < fitness[i]:\n                    # Replacement\n                    fitness[i] = f_u\n                    population[i] = u\n\n                    # Update archive (if necessary)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                    else:\n                        # Replace a random element in the archive\n                        replace_index = np.random.randint(self.archive_size)\n                        self.archive[replace_index] = population[i]\n\n                    # Adaptive CR: Adjust CR if this solution is better than current best\n                    if f_u < self.f_opt:\n                        self.CR = self.CR + 0.1 * (1-self.CR) # Increase CR\n                        self.CR = np.clip(self.CR, 0.1, 0.9)\n                else:\n                    self.CR = self.CR - 0.1 * self.CR # Decrease CR\n                    self.CR = np.clip(self.CR, 0.1, 0.9)\n\n                # Update best solution\n                if f_u < self.f_opt:\n                    self.f_opt = f_u\n                    self.x_opt = u.copy() # Important to make a copy!\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDE scored 0.445 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1606015247939545, 0.23237047482685425, 0.44094658371204587, 0.4784788566381596, 0.3539913597526484, 0.6053257522091481, 0.3230109398053407, 0.422471768163333, 0.4772333485997253, 0.20809234133180143, 0.42045216918065886, 0.9943314253357606, 0.29411472399347915, 0.33086006492149456, 0.8254807131928338, 0.6448676191620921, 0.3539893721487013, 0.6364649322694456, 0.20044559302234832, 0.5036889383872085]}}
{"id": "6fcd1e34-3850-4960-a595-dcc67540dc34", "fitness": -Infinity, "name": "SelfAdaptiveDE", "description": "A differential evolution strategy with self-adaptive population size and a restart mechanism.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, F=0.5, CR=0.7, min_pop_size=10, max_pop_size=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.initial_pop_size = initial_pop_size\n        self.F = F  # Initial mutation factor\n        self.CR = CR  # Initial crossover rate\n        self.min_pop_size = min_pop_size\n        self.max_pop_size = max_pop_size\n        self.success_history = []\n        self.restart_counter = 0\n        self.restart_threshold = 50\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        # Evolution loop\n        while self.budget > 0:\n            new_population = np.zeros_like(population)\n            new_fitness = np.zeros_like(fitness)\n\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[indices]\n\n                v = x_r1 + self.F * (x_r2 - x_r3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.copy(population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        u[j] = v[j]\n\n                # Evaluation\n                f_u = func(u)\n                self.budget -= 1\n\n                if f_u < fitness[i]:\n                    new_fitness[i] = f_u\n                    new_population[i] = u\n                    self.success_history.append(1)\n\n                else:\n                    new_fitness[i] = fitness[i]\n                    new_population[i] = population[i]\n                    self.success_history.append(0)\n\n                # Update best solution\n                if f_u < self.f_opt:\n                    self.f_opt = f_u\n                    self.x_opt = u.copy()\n\n                if self.budget <= 0:\n                    return self.f_opt, self.x_opt\n\n            population = new_population\n            fitness = new_fitness\n\n            # Adjust population size based on success rate\n            success_rate = np.mean(self.success_history[-min(len(self.success_history), self.pop_size):]) if self.success_history else 0.5\n            \n            if success_rate > 0.25:\n                self.pop_size = min(int(self.pop_size * 1.1), self.max_pop_size)  # Increase pop size\n            elif success_rate < 0.15:\n                self.pop_size = max(int(self.pop_size * 0.9), self.min_pop_size)  # Decrease pop size\n            \n            # Restart mechanism: if no improvement after a certain number of iterations, reinitialize\n            self.restart_counter += 1\n            if self.restart_counter > self.restart_threshold:\n                self.restart_counter = 0\n                # Reinitialize population (only a portion)\n                num_reinitialize = int(self.pop_size * 0.2)\n                indices_to_reinitialize = np.random.choice(self.pop_size, num_reinitialize, replace=False)\n                population[indices_to_reinitialize] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(num_reinitialize, self.dim))\n                fitness[indices_to_reinitialize] = [func(x) for x in population[indices_to_reinitialize]]\n                self.budget -= num_reinitialize\n                \n                best_index = np.argmin(fitness)\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n            \n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 144, in evaluate\n    algorithm(f_new)\n  File \"<string>\", line 35, in __call__\nIndexError: index 52 is out of bounds for axis 0 with size 50\n.", "error": "", "parent_ids": ["a7bac7ad-af96-47be-8d44-4e02f1467af3"], "operator": null, "metadata": {}}
{"id": "e5302c8b-17e9-46b2-8446-4a87b8108492", "fitness": 0.0, "name": "SHADE_LocalSearch", "description": "A self-adjusting differential evolution algorithm that leverages a success history based mutation factor and probabilistic local search to enhance exploitation.", "code": "import numpy as np\n\nclass SHADE_LocalSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=50, memory_size=10, p_local_search=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.memory_size = memory_size\n        self.memory_CR = np.full(self.memory_size, 0.5)\n        self.memory_F = np.full(self.memory_size, 0.5)\n        self.archive = []\n        self.archive_size = pop_size\n        self.p_local_search = p_local_search\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.zeros_like(population)\n            new_fitness = np.zeros_like(fitness)\n\n            for i in range(self.pop_size):\n                # Selection of CR and F\n                rand_index = np.random.randint(self.memory_size)\n                CR = self.memory_CR[rand_index]\n                F = self.memory_F[rand_index]\n\n                # Mutation\n                p_best_index = np.argmin(fitness)\n                x_pbest = population[p_best_index]\n                indices = np.random.choice(self.pop_size, 2, replace=False)\n                x_r1, x_r2 = population[indices]\n\n                v = population[i] + F * (x_pbest - population[i]) + F * (x_r1 - x_r2)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.copy(population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        u[j] = v[j]\n\n                # Evaluation\n                f_u = func(u)\n                self.budget -= 1\n                if f_u < fitness[i]:\n                    new_fitness[i] = f_u\n                    new_population[i] = u\n                else:\n                    new_fitness[i] = fitness[i]\n                    new_population[i] = population[i]\n                \n                #Probabilistic Local Search\n                if np.random.rand() < self.p_local_search:\n                    x_local = np.clip(population[i] + np.random.normal(0, 0.05, self.dim), func.bounds.lb, func.bounds.ub)\n                    f_local = func(x_local)\n                    self.budget -= 1\n                    if f_local < new_fitness[i]:\n                        new_fitness[i] = f_local\n                        new_population[i] = x_local\n                \n                # Update best solution\n                if new_fitness[i] < self.f_opt:\n                    self.f_opt = new_fitness[i]\n                    self.x_opt = new_population[i].copy() # Important to make a copy!\n            \n            # Update population and fitness\n            population = new_population\n            fitness = new_fitness\n\n            # SHADE Memory Update (simplified)\n            successful_CRs = []\n            successful_Fs = []\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    successful_CRs.append(self.memory_CR[np.random.randint(self.memory_size)])\n                    successful_Fs.append(self.memory_F[np.random.randint(self.memory_size)])\n            \n            if successful_CRs: #Check if there are successful crossovers before calculating mean\n                self.memory_CR[np.random.randint(self.memory_size)] = np.mean(successful_CRs)\n            if successful_Fs: #Check if there are successful Fs before calculating mean\n                self.memory_F[np.random.randint(self.memory_size)] = np.mean(successful_Fs)\n                self.memory_F[np.random.randint(self.memory_size)] = np.clip(self.memory_F[np.random.randint(self.memory_size)], 0.1, 1.0)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm SHADE_LocalSearch scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a7bac7ad-af96-47be-8d44-4e02f1467af3"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "2bba2cc6-ea89-4acd-a69c-1cd5373ac5ce", "fitness": 0.0, "name": "CauchyPSO", "description": "Combines PSO with a Cauchy mutation operator for enhanced exploration and a restart mechanism to escape local optima.", "code": "import numpy as np\n\nclass CauchyPSO:\n    def __init__(self, budget=10000, dim=10, pop_size=20, w=0.7, c1=1.5, c2=1.5, v_max_ratio=0.2, cauchy_scale=1.0, restart_freq=500):\n        \"\"\"\n        Initialize the Cauchy PSO algorithm.\n\n        Args:\n            budget (int): Total number of function evaluations.\n            dim (int): Dimensionality of the problem.\n            pop_size (int): Population size.\n            w (float): Inertia weight for PSO.\n            c1 (float): Cognitive coefficient for PSO.\n            c2 (float): Social coefficient for PSO.\n            v_max_ratio (float): Ratio to determine maximum velocity.\n            cauchy_scale (float): Scale parameter for Cauchy mutation.\n            restart_freq (int): Frequency of restarting the population.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w = w\n        self.c1 = c1\n        self.c2 = c2\n        self.v_max_ratio = v_max_ratio\n        self.cauchy_scale = cauchy_scale\n        self.restart_freq = restart_freq\n        self.pop = None\n        self.fitness = None\n        self.velocities = None\n        self.x_best_global = None\n        self.f_best_global = np.inf\n        self.lb = None\n        self.ub = None\n        self.v_max = None\n        self.eval_count = 0\n\n    def initialize_population(self, func):\n        \"\"\"\n        Initialize the population within the bounds of the function.\n\n        Args:\n            func: The black-box optimization function.\n        \"\"\"\n        self.lb = func.bounds.lb\n        self.ub = func.bounds.ub\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.velocities = np.zeros((self.pop_size, self.dim))\n\n        self.x_best_global = self.pop[np.argmin(self.fitness)].copy()\n        self.f_best_global = np.min(self.fitness)\n        self.v_max = self.v_max_ratio * (self.ub - self.lb)\n        self.eval_count += self.pop_size\n\n    def cauchy_mutation(self):\n        \"\"\"\n        Apply Cauchy mutation to a randomly selected particle.\n        \"\"\"\n        i = np.random.randint(self.pop_size)\n        mutation = self.cauchy_scale * np.random.standard_cauchy(size=self.dim)\n        self.pop[i] = np.clip(self.pop[i] + mutation, self.lb, self.ub)\n        return i # Return mutated index to update fitness\n\n    def pso_update(self, func):\n        \"\"\"\n        Update the population using PSO principles.\n\n        Args:\n            func: The black-box optimization function.\n        \"\"\"\n        personal_best_locations = self.pop.copy()\n        personal_best_fitness = self.fitness.copy()\n\n        for i in range(self.pop_size):\n            # Update velocities\n            r1 = np.random.rand(self.dim)\n            r2 = np.random.rand(self.dim)\n            self.velocities[i] = self.w * self.velocities[i] + \\\n                                 self.c1 * r1 * (personal_best_locations[i] - self.pop[i]) + \\\n                                 self.c2 * r2 * (self.x_best_global - self.pop[i])\n\n            # Velocity clamping\n            self.velocities[i] = np.clip(self.velocities[i], -self.v_max, self.v_max)\n\n            # Update positions\n            self.pop[i] = self.pop[i] + self.velocities[i]\n\n            # Boundary handling (clip to bounds)\n            self.pop[i] = np.clip(self.pop[i], self.lb, self.ub)\n            \n            #Evaluate new positions\n            f = func(self.pop[i])\n            self.eval_count +=1\n\n            if f < self.fitness[i]:\n                self.fitness[i] = f\n                personal_best_locations[i] = self.pop[i].copy()\n\n            if f < self.f_best_global:\n                 self.f_best_global = f\n                 self.x_best_global = self.pop[i].copy()\n\n    def restart_population(self, func):\n        \"\"\"\n        Restart the population with new random solutions.\n        \"\"\"\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.x_best_global = self.pop[np.argmin(self.fitness)].copy()\n        self.f_best_global = np.min(self.fitness)\n        self.eval_count += self.pop_size\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using the Cauchy PSO algorithm.\n\n        Args:\n            func: The black-box optimization function.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            # PSO update\n            self.pso_update(func)\n\n            # Cauchy mutation (exploration)\n            if self.eval_count < self.budget:\n                mutated_idx = self.cauchy_mutation()\n                f_mutated = func(self.pop[mutated_idx])\n                self.eval_count += 1\n                self.fitness[mutated_idx] = f_mutated\n                if f_mutated < self.f_best_global:\n                    self.f_best_global = f_mutated\n                    self.x_best_global = self.pop[mutated_idx].copy()\n\n            # Restart mechanism\n            if self.eval_count > 0 and self.eval_count % self.restart_freq == 0 and self.eval_count < self.budget:\n                self.restart_population(func)\n\n        return self.f_best_global, self.x_best_global", "configspace": "", "generation": 1, "feedback": "The algorithm CauchyPSO scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a62acce2-bd54-49de-8819-44f4d23c1329"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "22c14823-9520-4662-a307-657d79a3940e", "fitness": 0.0, "name": "AdaptiveHybridPSO_LS", "description": "An adaptive hybrid algorithm that combines PSO and a gradient-based local search, dynamically adjusting the frequency of each based on the improvement rate.", "code": "import numpy as np\n\nclass AdaptiveHybridPSO_LS:\n    def __init__(self, budget=10000, dim=10, pop_size=20, w=0.7, c1=1.5, c2=1.5, ls_prob=0.1, step_size=0.1):\n        \"\"\"\n        Initialize the Adaptive Hybrid PSO-LS algorithm.\n\n        Args:\n            budget (int): Total number of function evaluations.\n            dim (int): Dimensionality of the problem.\n            pop_size (int): Population size.\n            w (float): Inertia weight for PSO.\n            c1 (float): Cognitive coefficient for PSO.\n            c2 (float): Social coefficient for PSO.\n            ls_prob (float): Probability of applying local search to a particle.\n            step_size (float): Step size for the local search.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w = w\n        self.c1 = c1\n        self.c2 = c2\n        self.ls_prob = ls_prob\n        self.step_size = step_size\n        self.pop = None\n        self.fitness = None\n        self.velocities = None\n        self.x_best_global = None\n        self.f_best_global = np.inf\n        self.lb = None\n        self.ub = None\n        self.eval_count = 0\n        self.pso_frequency = 0.5 # Initial frequency for PSO\n\n    def initialize_population(self, func):\n        \"\"\"\n        Initialize the population within the bounds of the function.\n\n        Args:\n            func: The black-box optimization function.\n        \"\"\"\n        self.lb = func.bounds.lb\n        self.ub = func.bounds.ub\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.velocities = np.zeros((self.pop_size, self.dim))\n\n        self.x_best_global = self.pop[np.argmin(self.fitness)].copy()\n        self.f_best_global = np.min(self.fitness)\n        self.eval_count += self.pop_size\n\n    def pso_update(self, func):\n        \"\"\"\n        Update the population using PSO principles.\n\n        Args:\n            func: The black-box optimization function.\n        \"\"\"\n        personal_best_locations = self.pop.copy()\n        personal_best_fitness = self.fitness.copy()\n\n        for i in range(self.pop_size):\n            # Update velocities\n            r1 = np.random.rand(self.dim)\n            r2 = np.random.rand(self.dim)\n            self.velocities[i] = self.w * self.velocities[i] + \\\n                                 self.c1 * r1 * (personal_best_locations[i] - self.pop[i]) + \\\n                                 self.c2 * r2 * (self.x_best_global - self.pop[i])\n\n            # Update positions\n            self.pop[i] = self.pop[i] + self.velocities[i]\n\n            # Boundary handling (clip to bounds)\n            self.pop[i] = np.clip(self.pop[i], self.lb, self.ub)\n            \n            #Evaluate new positions\n            new_fitness = func(self.pop[i])\n            self.eval_count += 1\n\n            if new_fitness < self.fitness[i]:\n                self.fitness[i] = new_fitness\n                personal_best_locations[i] = self.pop[i].copy()\n\n            if new_fitness < self.f_best_global:\n                 self.f_best_global = new_fitness\n                 self.x_best_global = self.pop[i].copy()\n\n    def local_search(self, func, x):\n        \"\"\"\n        Perform a simple gradient-based local search.\n\n        Args:\n            func: The black-box optimization function.\n            x (np.ndarray): Starting point for the local search.\n\n        Returns:\n            tuple: Improved solution and its fitness.\n        \"\"\"\n        x_current = x.copy()\n        f_current = func(x_current)\n        self.eval_count += 1\n        \n        for _ in range(5): # Limited iterations for budget concerns\n            # Generate a random direction\n            direction = np.random.uniform(-1, 1, size=self.dim)\n            direction = direction / np.linalg.norm(direction)  # Normalize\n\n            # Take a step in that direction\n            x_new = x_current + self.step_size * direction\n            x_new = np.clip(x_new, self.lb, self.ub)  # Boundary handling\n            \n            f_new = func(x_new)\n            self.eval_count += 1\n\n            if f_new < f_current:\n                x_current = x_new\n                f_current = f_new\n\n        return x_current, f_current\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using the Adaptive Hybrid PSO-LS algorithm.\n\n        Args:\n            func: The black-box optimization function.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        self.initialize_population(func)\n\n        last_best_fitness = self.f_best_global\n        pso_success = 0\n        ls_success = 0\n\n        while self.eval_count < self.budget:\n            # Adaptive frequency adjustment\n            if self.pso_frequency < 0.9 and self.eval_count % self.pop_size == 0:\n                if self.f_best_global < last_best_fitness:\n                  pso_success += 1\n                last_best_fitness = self.f_best_global\n            if self.pso_frequency > 0.1 and self.eval_count % self.pop_size == 0:\n                if self.f_best_global < last_best_fitness:\n                  ls_success += 1\n                last_best_fitness = self.f_best_global\n\n            if np.random.rand() < self.pso_frequency:\n                self.pso_update(func)\n            else:\n                for i in range(self.pop_size):\n                    if np.random.rand() < self.ls_prob:\n                        x_new, f_new = self.local_search(func, self.pop[i])\n                        if f_new < self.fitness[i]:\n                            self.pop[i] = x_new\n                            self.fitness[i] = f_new\n                            if f_new < self.f_best_global:\n                                self.f_best_global = f_new\n                                self.x_best_global = x_new.copy()\n\n            # Adaptive adjustment of PSO frequency\n            if pso_success > ls_success:\n                self.pso_frequency = min(self.pso_frequency + 0.05, 0.9)\n            else:\n                self.pso_frequency = max(self.pso_frequency - 0.05, 0.1)\n            \n            pso_success = 0\n            ls_success = 0\n            last_best_fitness = self.f_best_global\n\n\n        return self.f_best_global, self.x_best_global", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveHybridPSO_LS scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a62acce2-bd54-49de-8819-44f4d23c1329"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "2eb944f2-d1cf-4077-93a8-378d986cbd38", "fitness": 0.14870721798383996, "name": "QuadraticTrustRegion", "description": "A novel search strategy based on quadratic approximation and trust region, adaptively adjusting the trust region size based on the success of the quadratic model.", "code": "import numpy as np\n\nclass QuadraticTrustRegion:\n    def __init__(self, budget=10000, dim=10, initial_radius=0.1, min_radius=1e-6, expand_factor=2.0, contract_factor=0.5, success_threshold=0.25):\n        self.budget = budget\n        self.dim = dim\n        self.radius = initial_radius\n        self.min_radius = min_radius\n        self.expand_factor = expand_factor\n        self.contract_factor = contract_factor\n        self.success_threshold = success_threshold\n        self.x_best = np.random.uniform(-5, 5, size=dim)\n        self.f_best = np.inf\n        self.evals = 0\n\n    def approximate_quadratic(self, func, x):\n        \"\"\"Approximate the function locally with a quadratic model.\"\"\"\n        delta = 1e-3  # Small perturbation for derivative estimation\n        grad = np.zeros(self.dim)\n        for i in range(self.dim):\n            x_plus = x.copy()\n            x_minus = x.copy()\n            x_plus[i] += delta\n            x_minus[i] -= delta\n            \n            # Evaluate only if within budget\n            if self.evals + 2 <= self.budget:\n                grad[i] = (func(x_plus) - func(x_minus)) / (2 * delta)\n                self.evals += 2\n            else:\n                return None, None # Return None if out of budget\n            \n        hess = np.zeros((self.dim, self.dim))\n        for i in range(self.dim):\n            for j in range(self.dim):\n                x_plus_i_plus_j = x.copy()\n                x_plus_i_minus_j = x.copy()\n                x_minus_i_plus_j = x.copy()\n                x_minus_i_minus_j = x.copy()\n\n                x_plus_i_plus_j[i] += delta\n                x_plus_i_plus_j[j] += delta\n\n                x_plus_i_minus_j[i] += delta\n                x_plus_i_minus_j[j] -= delta\n                \n                x_minus_i_plus_j[i] -= delta\n                x_minus_i_plus_j[j] += delta\n\n                x_minus_i_minus_j[i] -= delta\n                x_minus_i_minus_j[j] -= delta\n\n                # Evaluate only if within budget\n                if self.evals + 4 <= self.budget:\n                    hess[i, j] = (func(x_plus_i_plus_j) - func(x_plus_i_minus_j) - func(x_minus_i_plus_j) + func(x_minus_i_minus_j)) / (4 * delta * delta)\n                    self.evals += 4\n                else:\n                    return None, None  # Return None if out of budget\n        return grad, hess\n\n    def solve_trust_region_subproblem(self, grad, hess):\n        \"\"\"Solve the trust region subproblem using a simple approach.\"\"\"\n        try:\n            # Attempt to solve directly using Newton step\n            delta_x = -np.linalg.solve(hess, grad)\n            if np.linalg.norm(delta_x) <= self.radius:\n                return delta_x\n        except np.linalg.LinAlgError:\n            pass  # Hessian is singular or not positive definite\n        \n        # If Newton step fails, use gradient descent within the trust region\n        delta_x = -self.radius * grad / np.linalg.norm(grad) if np.linalg.norm(grad) > 0 else np.random.uniform(-self.radius, self.radius, self.dim)\n        return delta_x\n\n    def __call__(self, func):\n        self.x_best = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.f_best = func(self.x_best)\n        self.evals += 1\n\n        while self.evals < self.budget:\n            grad, hess = self.approximate_quadratic(func, self.x_best)\n            if grad is None or hess is None:\n                break # Break if approximate_quadratic returns None (out of budget)\n            \n            delta_x = self.solve_trust_region_subproblem(grad, hess)\n            x_new = self.x_best + delta_x\n\n            # Ensure x_new stays within bounds using clipping\n            x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n\n            f_new = func(x_new)\n            self.evals += 1\n            \n            actual_reduction = self.f_best - f_new\n            predicted_reduction = -grad @ delta_x - 0.5 * delta_x @ hess @ delta_x\n            \n            if predicted_reduction > 0:\n                rho = actual_reduction / predicted_reduction\n            else:\n                rho = 0  # Avoid division by zero\n            \n            if rho > self.success_threshold:\n                self.x_best = x_new\n                self.f_best = f_new\n                self.radius *= self.expand_factor\n            else:\n                self.radius *= self.contract_factor\n\n            self.radius = max(self.radius, self.min_radius)\n\n        return self.f_best, self.x_best", "configspace": "", "generation": 1, "feedback": "The algorithm QuadraticTrustRegion scored 0.149 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a7a19607-6565-4500-b116-c21e24a52f8b"], "operator": null, "metadata": {"aucs": [0.0, 0.06000242587557636, 0.15154399709885025, 0.18983931364241813, 0.038151654139314095, 0.11835208115711116, 0.17799794056247153, 0.13770251956441515, 0.10741092850991696, 0.07719449273349654, 0.09033213068503487, 0.14685766771729358, 0.181178853363124, 0.09005068543765704, 0.31702868579549326, 0.2798982525868986, 0.10252084613242896, 0.5117674591569203, 0.06442616436520421, 0.13188826115317476]}}
{"id": "f5d355ca-ae64-4797-902f-7e31b8a0e7c2", "fitness": 0.6282062363482936, "name": "MirroredCMAES", "description": "A CMA-ES variant that uses a mirrored sampling strategy to enhance exploration, especially in constrained or rugged landscapes.", "code": "import numpy as np\n\nclass MirroredCMAES:\n    def __init__(self, budget=10000, dim=10, mu_factor=0.25, initial_sigma=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.mu = int(dim * mu_factor)  # Number of parents\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cc = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.mean = np.zeros(dim)\n        self.sigma = initial_sigma\n        self.pc = np.zeros(dim)\n        self.ps = np.zeros(dim)\n        self.C = np.eye(dim)\n        self.B = np.eye(dim)\n        self.D = np.ones(dim)\n        self.C_eigen_age = 0\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n    def sample_population(self, popsize, func):\n        z = np.random.normal(0, 1, size=(popsize, self.dim))\n        x = self.mean + self.sigma * (self.B @ (self.D * z).T).T\n        \n        # Mirrored sampling: Reflect points around the mean\n        x_mirrored = 2 * self.mean - x\n\n        # Combine original and mirrored samples\n        x_combined = np.vstack((x, x_mirrored))\n        z_combined = np.vstack((z, -z)) # Corresponding mirrored z values.\n        \n        # Handle boundary constraints using clipping\n        x_combined = np.clip(x_combined, func.bounds.lb, func.bounds.ub)\n        \n        f = np.array([func(xi) for xi in x_combined])\n        return x_combined, f, z_combined\n\n    def update_distribution(self, x, f, z, popsize):\n        idx = np.argsort(f)\n        x = x[idx]\n        z = z[idx]\n        \n        x_mu = x[:self.mu]\n        z_mu = z[:self.mu]\n        \n        self.mean = np.sum(self.weights.reshape(-1, 1) * x_mu, axis=0)\n\n        zmean = np.sum(self.weights.reshape(-1, 1) * z_mu, axis=0)\n        self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * (self.B @ zmean)\n        hsig = np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * (self.budget // (2*popsize)))) < 1.4 + 2 / (self.dim + 1)\n        self.pc = (1 - self.cc) * self.pc + hsig * np.sqrt(self.cc * (2 - self.cc) * self.mueff) * (self.mean - self.mean) / self.sigma \n\n        artmp = (1 / self.sigma) * (x_mu - self.mean).T\n        self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * (self.pc.reshape(-1, 1) @ self.pc.reshape(1, -1) + (1-hsig) * self.cc * (2-self.cc) * self.C) + self.cmu * artmp @ np.diag(self.weights) @ artmp.T\n\n        self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * (self.budget // (2*popsize)))) / np.sqrt(self.dim) - 1))\n\n    def __call__(self, func):\n        popsize = 4 + int(3 * np.log(self.dim))\n        evals = 0\n\n        while evals < self.budget:\n            x, f, z = self.sample_population(popsize, func)\n            evals += 2*popsize # Because we are evaluating mirrored samples as well.\n\n            for i in range(2*popsize):\n                if f[i] < self.f_opt:\n                    self.f_opt = f[i]\n                    self.x_opt = x[i]\n                    \n            self.update_distribution(x, f, z, popsize)\n            self.C_eigen_age += 1\n            if self.C_eigen_age > self.budget // (10 * popsize): # Re-compute eigenvalue decomposition after a while\n                self.C_eigen_age = 0\n                self.C = np.triu(self.C) + np.triu(self.C, 1).T\n                self.D, self.B = np.linalg.eigh(self.C)\n                self.D = np.sqrt(self.D)\n                \n                # Avoid tiny values\n                self.D[self.D < 1e-10] = 1e-10\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm MirroredCMAES scored 0.628 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a7a19607-6565-4500-b116-c21e24a52f8b"], "operator": null, "metadata": {"aucs": [0.2531417954837619, 0.18124406030961226, 0.9107568482458109, 0.9581813619638931, 0.9154175892994144, 0.9172552964188556, 0.3114822831762072, 0.8980210693503369, 0.9254318673412987, 0.18546769463813317, 0.9376620650533978, 0.9816522705415133, 0.2611548415874426, 0.2503620569221795, 0.8796270902982508, 0.33486942319245905, 0.7604356262488156, 0.9345922159205076, 0.2419419025955929, 0.5254273683783863]}}
{"id": "c34ebcb7-05b8-4920-a45d-624ee0469d03", "fitness": 0.3076833579006252, "name": "SelfOrganizingScoutBees", "description": "A self-organizing scout bees algorithm that dynamically adjusts its search behavior based on the fitness landscape, utilizing memory and adaptive step sizes.", "code": "import numpy as np\n\nclass SelfOrganizingScoutBees:\n    def __init__(self, budget=10000, dim=10, n_scouts=10, scout_memory=5, initial_step_size=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.n_scouts = n_scouts\n        self.scout_memory = scout_memory\n        self.step_size = initial_step_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.scouts_pos = np.random.uniform(self.lb, self.ub, size=(self.n_scouts, self.dim))\n        self.scouts_fitness = np.full(self.n_scouts, np.inf)\n        self.scouts_memory_pos = np.zeros((self.n_scouts, self.scout_memory, self.dim))\n        self.scouts_memory_fitness = np.full((self.n_scouts, self.scout_memory), np.inf)\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n\n    def scout_move(self, scout_id, func):\n        # Adaptive step size adjustment\n        explore_prob = 0.2\n        if np.random.rand() < explore_prob:\n            # Exploration: Random move with adaptive step size\n            new_pos = self.scouts_pos[scout_id] + np.random.uniform(-self.step_size, self.step_size, self.dim)\n        else:\n            # Exploitation: Move towards best memory\n            best_memory_idx = np.argmin(self.scouts_memory_fitness[scout_id])\n            new_pos = self.scouts_pos[scout_id] + np.random.uniform(-self.step_size, self.step_size, self.dim) * (self.scouts_memory_pos[scout_id, best_memory_idx] - self.scouts_pos[scout_id])\n\n        new_pos = np.clip(new_pos, self.lb, self.ub)\n        new_fitness = func(new_pos)\n        self.eval_count += 1\n\n        # Update scout's position and fitness\n        if new_fitness < self.scouts_fitness[scout_id]:\n            self.scouts_pos[scout_id] = new_pos\n            self.scouts_fitness[scout_id] = new_fitness\n\n            # Update scout's memory\n            self.scouts_memory_pos[scout_id, :-1] = self.scouts_memory_pos[scout_id, 1:]\n            self.scouts_memory_fitness[scout_id, :-1] = self.scouts_memory_fitness[scout_id, 1:]\n            self.scouts_memory_pos[scout_id, -1] = new_pos\n            self.scouts_memory_fitness[scout_id, -1] = new_fitness\n        \n        return new_fitness, new_pos\n\n    def adapt_step_size(self):\n      # Adjust step size based on success rate (simplified)\n      success_rate = np.sum(self.scouts_fitness < np.inf) / self.n_scouts\n      if success_rate > 0.5:\n          self.step_size *= 1.1  # Increase step size if successful\n      else:\n          self.step_size *= 0.9  # Decrease step size if unsuccessful\n\n      self.step_size = np.clip(self.step_size, 0.01, 1.0)  # Limit step size\n\n    def __call__(self, func):\n        # Initialize scouts and their memory\n        for i in range(self.n_scouts):\n            self.scouts_fitness[i] = func(self.scouts_pos[i])\n            self.eval_count += 1\n            self.scouts_memory_pos[i, -1] = self.scouts_pos[i]\n            self.scouts_memory_fitness[i, -1] = self.scouts_fitness[i]\n\n            if self.scouts_fitness[i] < self.f_opt:\n                self.f_opt = self.scouts_fitness[i]\n                self.x_opt = self.scouts_pos[i]\n\n        while self.eval_count < self.budget:\n            for i in range(self.n_scouts):\n                f, x = self.scout_move(i, func)\n\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = x\n\n            self.adapt_step_size()\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm SelfOrganizingScoutBees scored 0.308 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a7a19607-6565-4500-b116-c21e24a52f8b"], "operator": null, "metadata": {"aucs": [0.1320269346930183, 0.20254764544873627, 0.3119833545124764, 0.2505927082654317, 0.2408107955488319, 0.27860285512058425, 0.25997414400933494, 0.25503643945821963, 0.23432191672411873, 0.1331431404165434, 0.27673228875830547, 0.9650260428656032, 0.27773899209493513, 0.2519614055666418, 0.5683570252815169, 0.31049787264224793, 0.2595549799333168, 0.3282738987695135, 0.15530612597543825, 0.4611785919276886]}}
{"id": "9ab915cb-d3cb-42d5-a5e1-1f246e4e4eba", "fitness": 0.43810658595423846, "name": "HybridPSODE_SA", "description": "A population-based algorithm combining aspects of PSO, DE, and Simulated Annealing (SA) for diversification and intensification.", "code": "import numpy as np\n\nclass HybridPSODE_SA:\n    def __init__(self, budget=10000, dim=10, pop_size=20, w=0.7, c1=1.5, c2=1.5, cr=0.7, f=0.8, v_max_ratio=0.2, temp_init=1.0, temp_decay=0.99):\n        \"\"\"\n        Initialize the Hybrid PSO-DE-SA algorithm.\n\n        Args:\n            budget (int): Total number of function evaluations.\n            dim (int): Dimensionality of the problem.\n            pop_size (int): Population size.\n            w (float): Inertia weight for PSO.\n            c1 (float): Cognitive coefficient for PSO.\n            c2 (float): Social coefficient for PSO.\n            cr (float): Crossover rate for DE.\n            f (float): Scaling factor for DE.\n            v_max_ratio (float): Ratio to determine maximum velocity.\n            temp_init (float): Initial temperature for SA.\n            temp_decay (float): Temperature decay rate for SA.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w = w\n        self.c1 = c1\n        self.c2 = c2\n        self.cr = cr\n        self.f = f\n        self.v_max_ratio = v_max_ratio\n        self.temp_init = temp_init\n        self.temp_decay = temp_decay\n        self.temp = temp_init\n        self.pop = None\n        self.fitness = None\n        self.velocities = None\n        self.x_best_global = None\n        self.f_best_global = np.inf\n        self.lb = None\n        self.ub = None\n        self.v_max = None\n\n    def initialize_population(self, func):\n        \"\"\"\n        Initialize the population within the bounds of the function.\n\n        Args:\n            func: The black-box optimization function.\n        \"\"\"\n        self.lb = func.bounds.lb\n        self.ub = func.bounds.ub\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.velocities = np.zeros((self.pop_size, self.dim))\n\n        self.x_best_global = self.pop[np.argmin(self.fitness)].copy()\n        self.f_best_global = np.min(self.fitness)\n        self.v_max = self.v_max_ratio * (self.ub - self.lb)\n\n    def pso_update(self, func):\n        \"\"\"\n        Update the population using PSO principles.\n\n        Args:\n            func: The black-box optimization function.\n        \"\"\"\n        personal_best_locations = self.pop.copy()\n        personal_best_fitness = self.fitness.copy()\n\n        for i in range(self.pop_size):\n            # Update velocities\n            r1 = np.random.rand(self.dim)\n            r2 = np.random.rand(self.dim)\n            self.velocities[i] = self.w * self.velocities[i] + \\\n                                 self.c1 * r1 * (personal_best_locations[i] - self.pop[i]) + \\\n                                 self.c2 * r2 * (self.x_best_global - self.pop[i])\n\n            # Velocity clamping\n            self.velocities[i] = np.clip(self.velocities[i], -self.v_max, self.v_max)\n\n            # Update positions\n            self.pop[i] = self.pop[i] + self.velocities[i]\n\n            # Boundary handling (clip to bounds)\n            self.pop[i] = np.clip(self.pop[i], self.lb, self.ub)\n            \n            #Evaluate new positions\n            new_fitness = func(self.pop[i])\n\n            if new_fitness < self.fitness[i]:\n                self.fitness[i] = new_fitness\n                personal_best_locations[i] = self.pop[i].copy()\n\n            if new_fitness < self.f_best_global:\n                 self.f_best_global = new_fitness\n                 self.x_best_global = self.pop[i].copy()\n\n    def de_update(self, func):\n        \"\"\"\n        Update the population using Differential Evolution principles.\n\n        Args:\n            func: The black-box optimization function.\n        \"\"\"\n        for i in range(self.pop_size):\n            # Choose three random indices, distinct from each other and i\n            idxs = list(range(self.pop_size))\n            idxs.remove(i)\n            a, b, c = np.random.choice(idxs, 3, replace=False)\n\n            # Mutation\n            v = self.pop[a] + self.f * (self.pop[b] - self.pop[c])\n            v = np.clip(v, self.lb, self.ub)\n\n            # Crossover\n            u = np.zeros(self.dim)\n            j_rand = np.random.randint(0, self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.cr or j == j_rand:\n                    u[j] = v[j]\n                else:\n                    u[j] = self.pop[i, j]\n\n            # Selection\n            f_u = func(u)\n            delta = f_u - self.fitness[i]\n            if delta < 0 or np.random.rand() < np.exp(-delta / self.temp):\n                self.pop[i] = u\n                self.fitness[i] = f_u\n\n                if f_u < self.f_best_global:\n                    self.f_best_global = f_u\n                    self.x_best_global = u.copy()\n\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using the Hybrid PSO-DE-SA algorithm.\n\n        Args:\n            func: The black-box optimization function.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        self.initialize_population(func)\n        eval_count = self.pop_size # Account for initial population evaluation\n\n        while eval_count < self.budget:\n            # Adaptive parameter adjustment (example: linear decrease of inertia weight)\n            self.w = 0.7 - 0.5 * (eval_count / self.budget)\n            \n            #Alternate between PSO and DE updates\n            if eval_count % 2 == 0:\n                self.pso_update(func)\n            else:\n                self.de_update(func)\n            \n            eval_count += self.pop_size\n\n            self.temp *= self.temp_decay #Cooling\n\n            if eval_count > self.budget:\n                break\n\n        return self.f_best_global, self.x_best_global", "configspace": "", "generation": 1, "feedback": "The algorithm HybridPSODE_SA scored 0.438 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a62acce2-bd54-49de-8819-44f4d23c1329"], "operator": null, "metadata": {"aucs": [0.13173447904606694, 0.22243911562643748, 0.8985568265105339, 0.26200624432454234, 0.25638115538125594, 0.2815709611314372, 0.36093403335633734, 0.5596787536116474, 0.8968139010809649, 0.19821973016438044, 0.289555936392779, 0.9994806730129683, 0.30759140374786453, 0.2852970232272535, 0.648468141537824, 0.904744905609361, 0.2726665958177187, 0.381616606812615, 0.2862902878375969, 0.3180849448551849]}}
{"id": "e6287ebd-8657-4245-a7af-77fddddbf2dd", "fitness": 0.4712056503969782, "name": "PSO_SA", "description": "A population-based algorithm combining aspects of PSO and simulated annealing (SA) with an adaptive temperature schedule and velocity clamping.", "code": "import numpy as np\n\nclass PSO_SA:\n    def __init__(self, budget=10000, dim=10, pop_size=20, w=0.7, c1=1.5, c2=1.5, temp_init=1.0, temp_decay=0.99, v_max_ratio=0.2):\n        \"\"\"\n        Initialize the PSO with Simulated Annealing algorithm.\n\n        Args:\n            budget (int): Total number of function evaluations.\n            dim (int): Dimensionality of the problem.\n            pop_size (int): Population size.\n            w (float): Inertia weight for PSO.\n            c1 (float): Cognitive coefficient for PSO.\n            c2 (float): Social coefficient for PSO.\n            temp_init (float): Initial temperature for SA.\n            temp_decay (float): Temperature decay rate for SA.\n            v_max_ratio (float): Ratio to determine maximum velocity.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w = w\n        self.c1 = c1\n        self.c2 = c2\n        self.temp_init = temp_init\n        self.temp_decay = temp_decay\n        self.v_max_ratio = v_max_ratio\n        self.pop = None\n        self.fitness = None\n        self.velocities = None\n        self.x_best_global = None\n        self.f_best_global = np.inf\n        self.lb = None\n        self.ub = None\n        self.v_max = None\n        self.temp = temp_init\n\n    def initialize_population(self, func):\n        \"\"\"\n        Initialize the population within the bounds of the function.\n\n        Args:\n            func: The black-box optimization function.\n        \"\"\"\n        self.lb = func.bounds.lb\n        self.ub = func.bounds.ub\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.velocities = np.zeros((self.pop_size, self.dim))\n\n        self.x_best_global = self.pop[np.argmin(self.fitness)].copy()\n        self.f_best_global = np.min(self.fitness)\n        self.v_max = self.v_max_ratio * (self.ub - self.lb)\n\n    def pso_update(self, func):\n        \"\"\"\n        Update the population using PSO principles with SA acceptance.\n\n        Args:\n            func: The black-box optimization function.\n        \"\"\"\n        personal_best_locations = self.pop.copy()\n        personal_best_fitness = self.fitness.copy()\n\n        for i in range(self.pop_size):\n            # Update velocities\n            r1 = np.random.rand(self.dim)\n            r2 = np.random.rand(self.dim)\n            self.velocities[i] = self.w * self.velocities[i] + \\\n                                 self.c1 * r1 * (personal_best_locations[i] - self.pop[i]) + \\\n                                 self.c2 * r2 * (self.x_best_global - self.pop[i])\n\n            # Velocity clamping\n            self.velocities[i] = np.clip(self.velocities[i], -self.v_max, self.v_max)\n\n            # Update positions\n            new_position = self.pop[i] + self.velocities[i]\n\n            # Boundary handling (clip to bounds)\n            new_position = np.clip(new_position, self.lb, self.ub)\n            \n            #Evaluate new positions\n            new_fitness = func(new_position)\n\n            # Simulated Annealing acceptance criterion\n            delta_e = new_fitness - self.fitness[i]\n            if delta_e < 0 or np.random.rand() < np.exp(-delta_e / self.temp):\n                self.pop[i] = new_position\n                self.fitness[i] = new_fitness\n                if new_fitness < personal_best_fitness[i]:\n                    personal_best_locations[i] = self.pop[i].copy()\n                    personal_best_fitness[i] = new_fitness\n\n                if new_fitness < self.f_best_global:\n                    self.f_best_global = new_fitness\n                    self.x_best_global = self.pop[i].copy()\n        \n        self.temp *= self.temp_decay #Cooling\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using the PSO-SA algorithm.\n\n        Args:\n            func: The black-box optimization function.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        self.initialize_population(func)\n        eval_count = self.pop_size # Account for initial population evaluation\n\n        while eval_count < self.budget:\n            # Adaptive parameter adjustment (example: linear decrease of inertia weight)\n            self.w = 0.7 - 0.5 * (eval_count / self.budget)\n            \n            self.pso_update(func)\n            \n            eval_count += self.pop_size # Account for population evaluation\n\n\n            if eval_count > self.budget:\n                break\n\n        return self.f_best_global, self.x_best_global", "configspace": "", "generation": 1, "feedback": "The algorithm PSO_SA scored 0.471 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a62acce2-bd54-49de-8819-44f4d23c1329"], "operator": null, "metadata": {"aucs": [0.16203363652412228, 0.17588261319257814, 0.8930892316687582, 0.2623900943551294, 0.26412962068669255, 0.9097979654013734, 0.31320687013343873, 0.621791634189965, 0.1701242152797644, 0.19779997051290044, 0.9413772823947549, 0.9967838061908549, 0.31550003405912286, 0.2545026941649168, 0.648823475393476, 0.9022361976541934, 0.41327833387027024, 0.29062346420741314, 0.17597150566560882, 0.5147703623942284]}}
{"id": "15d46a40-bd89-473b-946f-484189b9f23e", "fitness": 0.4838455219654042, "name": "CauchyAdaptiveDE", "description": "An adaptive differential evolution strategy that incorporates a Cauchy mutation operator and dynamically adjusts its parameters based on the fitness landscape and population diversity.", "code": "import numpy as np\n\nclass CauchyAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, cauchy_scale=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Initial mutation factor\n        self.CR = CR  # Initial crossover rate\n        self.cauchy_scale = cauchy_scale # Scale parameter for Cauchy distribution\n        self.archive_size = int(self.pop_size * 0.2)  # Archive size for storing successful solutions\n        self.archive = []\n        self.success_F = []\n        self.success_CR = []\n        self.memory_size = 10\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        # Evolution loop\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[indices]\n\n                # Adaptive F using Cauchy distribution\n                F_current = self.cauchy_scale * np.random.standard_cauchy()\n                F_current = np.clip(F_current, 0.1, 1.0)\n\n                v = x_r1 + F_current * (x_r2 - x_r3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.copy(population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        u[j] = v[j]\n\n                # Evaluation\n                f_u = func(u)\n                self.budget -= 1\n                if f_u < fitness[i]:\n                    # Replacement\n                    fitness[i] = f_u\n                    population[i] = u\n\n                    # Update archive (if necessary)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                    else:\n                        # Replace a random element in the archive\n                        replace_index = np.random.randint(self.archive_size)\n                        self.archive[replace_index] = population[i]\n\n                    # Store successful F and CR values\n                    self.success_F.append(F_current)\n                    self.success_CR.append(self.CR)\n                    if len(self.success_F) > self.memory_size:\n                        self.success_F.pop(0)\n                        self.success_CR.pop(0)\n                    \n                    # Adaptive CR: Adjust CR based on success history\n                    if self.success_CR:\n                        self.CR = np.mean(self.success_CR)\n                    \n                    self.CR = self.CR + np.random.normal(0, 0.05)\n                    self.CR = np.clip(self.CR, 0.1, 0.9)\n                    \n                else:\n                     # Reduce CR if the trial vector is not better\n                     self.CR = self.CR - 0.05 # Decrease CR\n                     self.CR = np.clip(self.CR, 0.1, 0.9)\n\n                # Update best solution\n                if f_u < self.f_opt:\n                    self.f_opt = f_u\n                    self.x_opt = u.copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm CauchyAdaptiveDE scored 0.484 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a7bac7ad-af96-47be-8d44-4e02f1467af3"], "operator": null, "metadata": {"aucs": [0.16447739304652076, 0.2559814822889843, 0.4331284719158386, 0.7013050590841228, 0.3730612344462987, 0.6696717028321137, 0.4760921938287199, 0.4776612892167563, 0.6369836961315237, 0.3022267298979314, 0.5031642541931856, 0.9981901572822929, 0.2959959769404268, 0.4317037787227469, 0.7264343272016138, 0.4567925068822918, 0.39486782945150933, 0.652832200199297, 0.21851505909883295, 0.5078250966470772]}}
{"id": "ec312464-f014-4d0c-9b56-cd2cc061bd22", "fitness": 0.43586782871386837, "name": "OrthogonalAdaptiveDE", "description": "An adaptive differential evolution strategy with orthogonal learning to enhance diversity and convergence.", "code": "import numpy as np\n\nclass OrthogonalAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, orthogonal_sample_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Initial mutation factor\n        self.CR = CR  # Initial crossover rate\n        self.orthogonal_sample_size = orthogonal_sample_size # Number of orthogonal samples to generate\n        self.archive_size = int(self.pop_size * 0.2)  # Archive size for storing successful solutions\n        self.archive = []\n\n    def __orthogonal_design(self, n, k):\n        \"\"\"\n        Generates an orthogonal design matrix.\n        n: number of runs (sample size)\n        k: number of factors (dimensions)\n        \"\"\"\n        if n == 4:\n            design = np.array([\n                [-1, -1],\n                [ 1, -1],\n                [-1,  1],\n                [ 1,  1]\n            ])\n        elif n == 8:\n            design = np.array([\n                [-1, -1, -1],\n                [ 1, -1, -1],\n                [-1,  1, -1],\n                [ 1,  1, -1],\n                [-1, -1,  1],\n                [ 1, -1,  1],\n                [-1,  1,  1],\n                [ 1,  1,  1]\n            ])\n        else:\n            raise ValueError(\"Orthogonal design only implemented for n=4 or n=8\")\n\n        # Repeat columns to match k\n        design_extended = np.tile(design[:, :min(design.shape[1],k)], (1, int(np.ceil(k/design.shape[1]))))\n        return design_extended[:, :k]\n\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        # Evolution loop\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[indices]\n\n                # Adaptive F: Perturb F based on success\n                F_current = self.F + np.random.normal(0, 0.05)\n                F_current = np.clip(F_current, 0.1, 1.0)\n\n                v = x_r1 + F_current * (x_r2 - x_r3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.copy(population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        u[j] = v[j]\n\n                # Orthogonal Learning\n                if self.orthogonal_sample_size > 1 and self.dim > 1:\n                    try:\n                        orthogonal_matrix = self.__orthogonal_design(self.orthogonal_sample_size, self.dim)\n                        candidates = np.zeros((self.orthogonal_sample_size, self.dim))\n                        for k in range(self.orthogonal_sample_size):\n                            candidate = np.copy(population[i])\n                            for j in range(self.dim):\n                                if orthogonal_matrix[k, j] == 1:\n                                    candidate[j] = v[j]  # Use mutated component\n                            candidates[k] = candidate\n                        \n                        candidate_fitnesses = [func(candidates[k]) for k in range(self.orthogonal_sample_size)]\n                        self.budget -= self.orthogonal_sample_size\n                        \n                        best_candidate_index = np.argmin(candidate_fitnesses)\n                        if candidate_fitnesses[best_candidate_index] < func(u):\n                            u = candidates[best_candidate_index]\n                            \n                    except ValueError:\n                        pass\n\n\n                # Evaluation\n                f_u = func(u)\n                self.budget -= 1\n\n                if f_u < fitness[i]:\n                    # Replacement\n                    fitness[i] = f_u\n                    population[i] = u\n\n                    # Update archive (if necessary)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                    else:\n                        # Replace a random element in the archive\n                        replace_index = np.random.randint(self.archive_size)\n                        self.archive[replace_index] = population[i]\n                    \n\n                    # Adaptive CR: Adjust CR if this solution is better than current best\n                    if f_u < self.f_opt:\n                        self.CR = self.CR + 0.1 * (1-self.CR) # Increase CR\n                        self.CR = np.clip(self.CR, 0.1, 0.9)\n                else:\n                    self.CR = self.CR - 0.1 * self.CR # Decrease CR\n                    self.CR = np.clip(self.CR, 0.1, 0.9)\n\n                # Update best solution\n                if f_u < self.f_opt:\n                    self.f_opt = f_u\n                    self.x_opt = u.copy() # Important to make a copy!\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm OrthogonalAdaptiveDE scored 0.436 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a7bac7ad-af96-47be-8d44-4e02f1467af3"], "operator": null, "metadata": {"aucs": [0.16636385455468194, 0.2545053975271516, 0.43864347222517186, 0.40929950307582275, 0.3912858652769391, 0.6120908267875533, 0.31523096212717816, 0.4141600208920524, 0.37993048827058473, 0.22539775189214395, 0.3586078106434365, 0.9982440540275725, 0.2636593074309035, 0.3626513712115971, 0.7616920104079615, 0.6635674276648196, 0.359072210300179, 0.6081447750412705, 0.2281233223347292, 0.5066861425856177]}}
{"id": "30c77ee1-192f-4452-ba2c-a0452ddce534", "fitness": 0.6855769359957808, "name": "EnsembleAdaptiveDE", "description": "A self-adaptive differential evolution algorithm with ensemble mutation strategies and archive-based learning.", "code": "import numpy as np\n\nclass EnsembleAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, popsize=50, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize\n        self.archive_size = archive_size\n        self.F = 0.5  # Initial scaling factor\n        self.CR = 0.7  # Initial crossover rate\n        self.population = None\n        self.fitness = None\n        self.archive = []\n        self.archive_fitness = []\n        self.best_fitness = np.inf\n        self.best_solution = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.popsize, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.best_fitness = np.min(self.fitness)\n        self.best_solution = self.population[np.argmin(self.fitness)]\n\n    def mutate(self, i):\n        mutation_strategy = np.random.choice(['DE/rand/1', 'DE/best/1', 'DE/current-to-rand/1', 'DE/current-to-best/1'])\n\n        if mutation_strategy == 'DE/rand/1':\n            indices = np.random.choice(self.popsize, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[indices]\n            v = x_r1 + self.F * (x_r2 - x_r3)\n        elif mutation_strategy == 'DE/best/1':\n            indices = np.random.choice(self.popsize, 2, replace=False)\n            x_r1, x_r2 = self.population[indices]\n            v = self.best_solution + self.F * (x_r1 - x_r2)\n        elif mutation_strategy == 'DE/current-to-rand/1':\n            indices = np.random.choice(self.popsize, 2, replace=False)\n            x_r1, x_r2 = self.population[indices]\n            v = self.population[i] + self.F * (x_r1 - x_r2)\n        elif mutation_strategy == 'DE/current-to-best/1':\n            v = self.population[i] + self.F * (self.best_solution - self.population[i]) + self.F * (self.population[np.random.randint(self.popsize)] - self.population[np.random.randint(self.popsize)])\n        else:\n            raise ValueError(\"Invalid mutation strategy\")\n\n        return v\n\n    def crossover(self, i, mutant):\n        trial_vector = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.CR or j == j_rand:\n                trial_vector[j] = mutant[j]\n        return trial_vector\n\n    def handle_boundary(self, trial_vector, func):\n        return np.clip(trial_vector, func.bounds.lb, func.bounds.ub)\n    \n    def update_archive(self, x, f):\n        if len(self.archive) < self.archive_size:\n            self.archive.append(x)\n            self.archive_fitness.append(f)\n        else:\n            if f < np.max(self.archive_fitness):\n                worst_index = np.argmax(self.archive_fitness)\n                self.archive[worst_index] = x\n                self.archive_fitness[worst_index] = f\n            \n    def __call__(self, func):\n        self.initialize_population(func)\n        evals = self.popsize\n\n        while evals < self.budget:\n            for i in range(self.popsize):\n                mutant = self.mutate(i)\n                trial_vector = self.crossover(i, mutant)\n                trial_vector = self.handle_boundary(trial_vector, func)\n                \n                f_trial = func(trial_vector)\n                evals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.update_archive(self.population[i], self.fitness[i])\n                    self.population[i] = trial_vector\n                    self.fitness[i] = f_trial\n\n                    if f_trial < self.best_fitness:\n                        self.best_fitness = f_trial\n                        self.best_solution = trial_vector\n                else:\n                    self.update_archive(trial_vector, f_trial)\n\n                if evals >= self.budget:\n                    break\n\n            # Adaptive F and CR using archive information (simplified)\n            if len(self.archive) > 0:\n                self.F = np.random.uniform(0.4, 0.9)\n                self.CR = np.random.uniform(0.3, 1.0)\n\n\n        return self.best_fitness, self.best_solution", "configspace": "", "generation": 1, "feedback": "The algorithm EnsembleAdaptiveDE scored 0.686 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a7a19607-6565-4500-b116-c21e24a52f8b"], "operator": null, "metadata": {"aucs": [0.22455026401481182, 0.5108887082403807, 0.6872868304600729, 0.8812711500281074, 0.7400427879761081, 0.8016166311488906, 0.6117077459448573, 0.664312416171645, 0.7757589494986676, 0.71075115329944, 0.8565405172179895, 0.9916106295390975, 0.49024860393742575, 0.7048562265964968, 0.9197293900606616, 0.8101474328427777, 0.6704710171484356, 0.8601034015847115, 0.21158854270339267, 0.5880563215016457]}}
{"id": "300f0f4f-22a2-4838-900a-06637b6d5174", "fitness": -Infinity, "name": "SOMGuidedDE", "description": "A differential evolution strategy that incorporates a self-organizing map (SOM) to guide the search process by clustering solutions and adapting mutation strategies based on cluster properties.", "code": "import numpy as np\nfrom minisom import MiniSom  # Requires: pip install MiniSom\n\nclass SOMGuidedDE:\n    def __init__(self, budget=10000, dim=10, popsize=50, som_grid_size=10, F=0.5, CR=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize\n        self.som_grid_size = som_grid_size\n        self.F = F\n        self.CR = CR\n        self.population = None\n        self.fitness = None\n        self.som = None\n        self.best_fitness = np.inf\n        self.best_solution = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.popsize, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.best_fitness = np.min(self.fitness)\n        self.best_solution = self.population[np.argmin(self.fitness)]\n\n    def train_som(self):\n        self.som = MiniSom(self.som_grid_size, self.som_grid_size, self.dim, sigma=0.3, learning_rate=0.5)\n        self.som.train(self.population, 1000)  # Train for 1000 iterations\n\n    def mutate(self, i):\n        # Get the winning neuron for the current individual\n        winner = self.som.winner(self.population[i])\n        \n        # Calculate coordinates in the SOM grid\n        x, y = winner\n\n        # Adjust F and CR based on the SOM neuron's position (example)\n        F = self.F + (x / self.som_grid_size - 0.5) * 0.2  # Small adjustment\n        CR = self.CR + (y / self.som_grid_size - 0.5) * 0.2  # Small adjustment\n        F = np.clip(F, 0.1, 0.9)\n        CR = np.clip(CR, 0.1, 0.9)\n\n        # DE/rand/1 mutation with adjusted parameters\n        indices = np.random.choice(self.popsize, 3, replace=False)\n        x_r1, x_r2, x_r3 = self.population[indices]\n        v = x_r1 + F * (x_r2 - x_r3)\n\n        return v, CR\n\n    def crossover(self, i, mutant, CR):\n        trial_vector = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < CR or j == j_rand:\n                trial_vector[j] = mutant[j]\n        return trial_vector\n\n    def handle_boundary(self, trial_vector, func):\n        return np.clip(trial_vector, func.bounds.lb, func.bounds.ub)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.train_som()  # Train SOM initially\n        evals = self.popsize\n\n        generation = 0\n\n        while evals < self.budget:\n            for i in range(self.popsize):\n                mutant, CR = self.mutate(i)\n                trial_vector = self.crossover(i, mutant, CR)\n                trial_vector = self.handle_boundary(trial_vector, func)\n\n                f_trial = func(trial_vector)\n                evals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = f_trial\n\n                    if f_trial < self.best_fitness:\n                        self.best_fitness = f_trial\n                        self.best_solution = trial_vector\n\n                if evals >= self.budget:\n                    break\n            \n            generation +=1\n            if generation % 10 == 0:\n               self.train_som() #Retrain every 10 generations\n\n        return self.best_fitness, self.best_solution", "configspace": "", "generation": 2, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 111, in evaluate\n    exec(code, safe_globals, local_env)\n  File \"<string>\", line 2, in <module>\nModuleNotFoundError: No module named 'minisom'\n.", "error": "", "parent_ids": ["30c77ee1-192f-4452-ba2c-a0452ddce534"], "operator": null, "metadata": {}}
{"id": "a24f0cc0-5901-4d55-a1a9-45693389db2a", "fitness": -Infinity, "name": "DynamicPopulationDE", "description": "A Differential Evolution strategy with a dynamically adjusted population size and a combination of global and local search phases.", "code": "import numpy as np\n\nclass DynamicPopulationDE:\n    def __init__(self, budget=10000, dim=10, initial_popsize=50, min_popsize=10, max_popsize=100):\n        self.budget = budget\n        self.dim = dim\n        self.initial_popsize = initial_popsize\n        self.min_popsize = min_popsize\n        self.max_popsize = max_popsize\n        self.popsize = initial_popsize\n        self.F = 0.5\n        self.CR = 0.7\n        self.population = None\n        self.fitness = None\n        self.best_fitness = np.inf\n        self.best_solution = None\n        self.evals = 0\n        self.phase = \"global\"  # Start in global search phase\n        self.phase_switch_threshold = 0.05 # relative fitness improvement threshold\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.popsize, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.best_fitness = np.min(self.fitness)\n        self.best_solution = self.population[np.argmin(self.fitness)]\n        self.evals += self.popsize\n\n    def mutate(self, i, func):\n        if self.phase == \"global\":\n            # DE/rand/1\n            indices = np.random.choice(self.popsize, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[indices]\n            mutant = x_r1 + self.F * (x_r2 - x_r3)\n        elif self.phase == \"local\":\n            # DE/current-to-best/1 with small perturbation\n            mutant = self.population[i] + self.F * (self.best_solution - self.population[i]) + np.random.normal(0, 0.01, self.dim)\n        else:\n            raise ValueError(\"Invalid phase\")\n        \n        mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n        return mutant\n\n    def crossover(self, i, mutant):\n        trial_vector = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.CR or j == j_rand:\n                trial_vector[j] = mutant[j]\n        return trial_vector\n\n    def adjust_population_size(self):\n        # Dynamically adjust population size based on progress\n        if self.evals > self.budget // 2: # Reduce population size in later stages\n            self.popsize = max(self.min_popsize, int(self.popsize * 0.9))\n        elif self.evals < self.budget // 4 and self.popsize < self.max_popsize: #Increase population early on if evaluations are cheap\n            self.popsize = min(self.max_popsize, int(self.popsize * 1.1))\n        \n        self.popsize = int(self.popsize) # Ensure integer population size\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        last_best_fitness = self.best_fitness\n\n        while self.evals < self.budget:\n            self.adjust_population_size()\n\n            new_population = np.zeros((self.popsize, self.dim))\n            new_fitness = np.zeros(self.popsize)\n\n            for i in range(self.popsize):\n                mutant = self.mutate(i, func)\n                trial_vector = self.crossover(i, mutant)\n                \n                f_trial = func(trial_vector)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    new_population[i] = trial_vector\n                    new_fitness[i] = f_trial\n                    if f_trial < self.best_fitness:\n                        self.best_fitness = f_trial\n                        self.best_solution = trial_vector\n                else:\n                    new_population[i] = self.population[i]\n                    new_fitness[i] = self.fitness[i]\n\n                if self.evals >= self.budget:\n                    break\n            \n            self.population = new_population\n            self.fitness = new_fitness\n\n            # Phase switching logic\n            relative_improvement = (last_best_fitness - self.best_fitness) / last_best_fitness if last_best_fitness != 0 else 0\n            if relative_improvement < self.phase_switch_threshold and self.phase == \"global\":\n                self.phase = \"local\"\n                self.F = 0.3 # Reduce F for local search\n                self.CR = 0.9 # Increase CR for local search\n\n            last_best_fitness = self.best_fitness\n\n        return self.best_fitness, self.best_solution", "configspace": "", "generation": 2, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 144, in evaluate\n    algorithm(f_new)\n  File \"<string>\", line 72, in __call__\n  File \"<string>\", line 32, in mutate\nIndexError: index 53 is out of bounds for axis 0 with size 50\n.", "error": "", "parent_ids": ["30c77ee1-192f-4452-ba2c-a0452ddce534"], "operator": null, "metadata": {}}
{"id": "99cba04f-a9eb-48fa-ac29-26d4f76192ef", "fitness": -Infinity, "name": "WaveletAdaptiveDE", "description": "A differential evolution strategy employing a wavelet mutation operator for enhanced exploration and a self-adaptive mechanism for adjusting parameters based on the success history.", "code": "import numpy as np\nimport pywt\n\nclass WaveletAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, wavelet='db4'):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Initial mutation factor\n        self.CR = CR  # Initial crossover rate\n        self.wavelet = wavelet # Wavelet type\n        self.archive_size = int(self.pop_size * 0.2)  # Archive size for storing successful solutions\n        self.archive = []\n        self.success_F = []\n        self.success_CR = []\n        self.memory_size = 10\n\n    def wavelet_mutation(self, x_r1, x_r2, x_r3):\n        \"\"\"Applies wavelet transform to the difference vector.\"\"\"\n        diff = x_r2 - x_r3\n        coeffs = pywt.wavedec(diff, self.wavelet, level=1)  # Decompose to level 1\n\n        # Add noise to detail coefficients (high-frequency components)\n        coeffs[1] += np.random.normal(0, 0.1, size=coeffs[1].shape)  # Small noise\n        \n        # Reconstruct the signal\n        mutated_diff = pywt.waverec(coeffs, self.wavelet)\n        \n        # Ensure the mutated difference has the same dimension\n        if len(mutated_diff) != self.dim:\n            mutated_diff = np.resize(mutated_diff, self.dim)\n        \n        return x_r1 + self.F * mutated_diff\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        # Evolution loop\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[indices]\n\n                # Wavelet mutation\n                v = self.wavelet_mutation(x_r1, x_r2, x_r3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.copy(population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        u[j] = v[j]\n\n                # Evaluation\n                f_u = func(u)\n                self.budget -= 1\n                if f_u < fitness[i]:\n                    # Replacement\n                    fitness[i] = f_u\n                    population[i] = u\n\n                    # Update archive (if necessary)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                    else:\n                        # Replace a random element in the archive\n                        replace_index = np.random.randint(self.archive_size)\n                        self.archive[replace_index] = population[i]\n\n                    # Store successful F and CR values\n                    self.success_F.append(self.F)\n                    self.success_CR.append(self.CR)\n                    if len(self.success_F) > self.memory_size:\n                        self.success_F.pop(0)\n                        self.success_CR.pop(0)\n                    \n                    # Adaptive F and CR: Adjust based on success history\n                    if self.success_F:\n                        self.F = np.mean(self.success_F)\n                    \n                    if self.success_CR:\n                        self.CR = np.mean(self.success_CR)\n\n                    self.F = self.F + np.random.normal(0, 0.05)\n                    self.CR = self.CR + np.random.normal(0, 0.05)\n                    self.F = np.clip(self.F, 0.1, 1.0)\n                    self.CR = np.clip(self.CR, 0.1, 0.9)\n                else:\n                     # Reduce CR if the trial vector is not better\n                     self.CR = self.CR - 0.05 # Decrease CR\n                     self.CR = np.clip(self.CR, 0.1, 0.9)\n\n                # Update best solution\n                if f_u < self.f_opt:\n                    self.f_opt = f_u\n                    self.x_opt = u.copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 111, in evaluate\n    exec(code, safe_globals, local_env)\n  File \"<string>\", line 2, in <module>\nModuleNotFoundError: No module named 'pywt'\n.", "error": "", "parent_ids": ["15d46a40-bd89-473b-946f-484189b9f23e"], "operator": null, "metadata": {}}
{"id": "da2d5480-31c0-411d-ad03-2d5e7f505717", "fitness": -Infinity, "name": "OrthogonalCMAES", "description": "CMA-ES with orthogonal sampling and covariance matrix adaptation based on rank-one updates and selective pressure.", "code": "import numpy as np\n\nclass OrthogonalCMAES:\n    def __init__(self, budget=10000, dim=10, mu_factor=0.25, initial_sigma=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.mu = int(dim * mu_factor)  # Number of parents\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cc = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.mean = np.zeros(dim)\n        self.sigma = initial_sigma\n        self.pc = np.zeros(dim)\n        self.ps = np.zeros(dim)\n        self.C = np.eye(dim)\n        self.B = np.eye(dim)\n        self.D = np.ones(dim)\n        self.C_eigen_age = 0\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n    def sample_population(self, popsize, func):\n        # Generate an orthogonal matrix\n        H = np.random.normal(0, 1, size=(popsize, self.dim))\n        Q, R = np.linalg.qr(H)\n\n        z = Q  # Use the orthogonal matrix as the basis for sampling\n        x = self.mean + self.sigma * (self.B @ (self.D * z).T).T\n\n        # Handle boundary constraints using clipping\n        x = np.clip(x, func.bounds.lb, func.bounds.ub)\n\n        f = np.array([func(xi) for xi in x])\n        return x, f, z\n\n    def update_distribution(self, x, f, z):\n        idx = np.argsort(f)\n        x = x[idx]\n        z = z[idx]\n        \n        x_mu = x[:self.mu]\n        z_mu = z[:self.mu]\n        \n        self.mean = np.sum(self.weights.reshape(-1, 1) * x_mu, axis=0)\n\n        zmean = np.sum(self.weights.reshape(-1, 1) * z_mu, axis=0)\n        self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * (self.B @ zmean)\n        hsig = np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * (self.budget // (popsize)))) < 1.4 + 2 / (self.dim + 1)\n        self.pc = (1 - self.cc) * self.pc + hsig * np.sqrt(self.cc * (2 - self.cc) * self.mueff) * (self.mean - self.mean) / self.sigma \n\n        artmp = (1 / self.sigma) * (x_mu - self.mean).T\n        self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * (self.pc.reshape(-1, 1) @ self.pc.reshape(1, -1) + (1-hsig) * self.cc * (2-self.cc) * self.C) + self.cmu * artmp @ np.diag(self.weights) @ artmp.T\n\n        self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * (self.budget // (popsize)))) / np.sqrt(self.dim) - 1))\n\n    def __call__(self, func):\n        popsize = 4 + int(3 * np.log(self.dim))\n        evals = 0\n\n        while evals < self.budget:\n            x, f, z = self.sample_population(popsize, func)\n            evals += popsize\n\n            for i in range(popsize):\n                if f[i] < self.f_opt:\n                    self.f_opt = f[i]\n                    self.x_opt = x[i]\n                    \n            self.update_distribution(x, f, z)\n            self.C_eigen_age += 1\n            if self.C_eigen_age > self.budget // (10 * popsize): # Re-compute eigenvalue decomposition after a while\n                self.C_eigen_age = 0\n                self.C = np.triu(self.C) + np.triu(self.C, 1).T\n                self.D, self.B = np.linalg.eigh(self.C)\n                self.D = np.sqrt(self.D)\n                \n                # Avoid tiny values\n                self.D[self.D < 1e-10] = 1e-10\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 74, in __call__\n  File \"<string>\", line 53, in update_distribution\nNameError: name 'popsize' is not defined\n.", "error": "", "parent_ids": ["f5d355ca-ae64-4797-902f-7e31b8a0e7c2"], "operator": null, "metadata": {}}
{"id": "5a5197bb-6e2a-4748-8486-daf68884a83c", "fitness": -Infinity, "name": "DynamicDE", "description": "A differential evolution strategy with a dynamically adjusted population size and a restart mechanism based on stagnation detection, promoting exploration and exploitation.", "code": "import numpy as np\n\nclass DynamicDE:\n    def __init__(self, budget=10000, dim=10, initial_popsize=50, min_popsize=10, max_popsize=100, stagnation_threshold=500):\n        self.budget = budget\n        self.dim = dim\n        self.initial_popsize = initial_popsize\n        self.min_popsize = min_popsize\n        self.max_popsize = max_popsize\n        self.stagnation_threshold = stagnation_threshold\n        self.popsize = initial_popsize\n        self.population = None\n        self.fitness = None\n        self.best_fitness = np.inf\n        self.best_solution = None\n        self.F = 0.5\n        self.CR = 0.7\n        self.evals = 0\n        self.stagnation_counter = 0\n\n    def initialize_population(self, func):\n        self.popsize = self.initial_popsize\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.popsize, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.best_fitness = np.min(self.fitness)\n        self.best_solution = self.population[np.argmin(self.fitness)]\n        self.evals = self.popsize\n\n    def mutate(self, i):\n        indices = np.random.choice(self.popsize, 3, replace=False)\n        x_r1, x_r2, x_r3 = self.population[indices]\n        v = x_r1 + self.F * (x_r2 - x_r3)\n        return v\n\n    def crossover(self, i, mutant):\n        trial_vector = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.CR or j == j_rand:\n                trial_vector[j] = mutant[j]\n        return trial_vector\n\n    def handle_boundary(self, trial_vector, func):\n        return np.clip(trial_vector, func.bounds.lb, func.bounds.ub)\n\n    def adjust_population_size(self):\n        if self.stagnation_counter > self.stagnation_threshold:\n            # Increase population size to promote exploration\n            self.popsize = min(self.popsize * 2, self.max_popsize)\n            self.stagnation_counter = 0  # Reset stagnation counter\n        elif self.popsize > self.initial_popsize and self.evals > self.budget * 0.75:\n             # Reduce population size to promote exploitation towards the end\n             self.popsize = max(self.popsize // 2, self.min_popsize)\n\n    def restart_population(self, func):\n        # Re-initialize the population if stagnation is detected\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.popsize, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.best_fitness:\n            self.best_fitness = self.fitness[best_index]\n            self.best_solution = self.population[best_index]\n        self.evals += self.popsize\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.evals < self.budget:\n            best_fitness_before_gen = self.best_fitness  # Store best fitness before the generation\n            for i in range(self.popsize):\n                mutant = self.mutate(i)\n                trial_vector = self.crossover(i, mutant)\n                trial_vector = self.handle_boundary(trial_vector, func)\n\n                f_trial = func(trial_vector)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = f_trial\n\n                    if f_trial < self.best_fitness:\n                        self.best_fitness = f_trial\n                        self.best_solution = trial_vector\n\n                if self.evals >= self.budget:\n                    break\n            \n            if self.best_fitness >= best_fitness_before_gen:\n                self.stagnation_counter += self.popsize  # Increment stagnation counter\n\n            if self.stagnation_counter > self.stagnation_threshold and self.evals < self.budget * 0.8:\n                self.restart_population(func)\n                self.stagnation_counter = 0\n\n            self.adjust_population_size() # dynamic population sizing\n           \n\n            # Adaptive F and CR (simplified)\n            self.F = np.random.uniform(0.4, 0.9)\n            self.CR = np.random.uniform(0.3, 1.0)\n\n        return self.best_fitness, self.best_solution", "configspace": "", "generation": 2, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 144, in evaluate\n    algorithm(f_new)\n  File \"<string>\", line 71, in __call__\n  File \"<string>\", line 31, in mutate\nIndexError: index 55 is out of bounds for axis 0 with size 50\n.", "error": "", "parent_ids": ["30c77ee1-192f-4452-ba2c-a0452ddce534"], "operator": null, "metadata": {}}
{"id": "2addb395-2fbd-4586-a196-2511b5d741eb", "fitness": 0.0, "name": "LevyAdaptiveDE", "description": "An adaptive differential evolution algorithm that utilizes a Lvy flight for mutation and incorporates a local search strategy to refine promising solutions.", "code": "import numpy as np\n\nclass LevyAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, levy_exponent=1.5, local_search_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Initial mutation factor\n        self.CR = CR  # Initial crossover rate\n        self.levy_exponent = levy_exponent # Exponent for Levy flight\n        self.local_search_prob = local_search_prob # Probability of performing local search\n        self.archive_size = int(self.pop_size * 0.2)  # Archive size\n        self.archive = []\n        self.success_F = []\n        self.success_CR = []\n        self.memory_size = 10\n\n    def levy_flight(self, size, exponent):\n        num = np.random.randn(size) * np.sqrt(\n            np.math.gamma(1 + exponent) * np.sin(np.pi * exponent / 2) / (np.math.gamma((1 + exponent) / 2) * exponent * (2 ** ((exponent - 1) / 2))))\n        den = np.abs(np.random.randn(size)) ** (1 / exponent)\n        return num / den\n\n    def local_search(self, x, func, bounds):\n        # Perform a simple local search around x\n        delta = np.random.uniform(-0.1, 0.1, size=self.dim)  # Small random perturbation\n        x_new = x + delta\n        x_new = np.clip(x_new, bounds.lb, bounds.ub)  # Keep within bounds\n        f_new = func(x_new)\n        self.budget -= 1\n        if f_new < func(x):\n            return x_new, f_new\n        else:\n            return x, func(x)\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        # Evolution loop\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[indices]\n\n                # Adaptive F using success history\n                if self.success_F:\n                    F_current = np.mean(self.success_F)\n                else:\n                    F_current = self.F\n                F_current = np.clip(F_current, 0.1, 1.0)\n\n                # Levy flight mutation\n                levy_steps = self.levy_flight(self.dim, self.levy_exponent)\n                v = x_r1 + F_current * (x_r2 - x_r3) + 0.01 * levy_steps * (self.x_opt - population[i])\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.copy(population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        u[j] = v[j]\n\n                # Evaluation\n                f_u = func(u)\n                self.budget -= 1\n\n                # Local search with probability local_search_prob\n                if np.random.rand() < self.local_search_prob:\n                    u, f_u = self.local_search(u, func, func.bounds)\n\n                if f_u < fitness[i]:\n                    # Replacement\n                    fitness[i] = f_u\n                    population[i] = u\n\n                    # Update archive (if necessary)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                    else:\n                        # Replace a random element in the archive\n                        replace_index = np.random.randint(self.archive_size)\n                        self.archive[replace_index] = population[i]\n\n                    # Store successful F and CR values\n                    self.success_F.append(F_current)\n                    self.success_CR.append(self.CR)\n                    if len(self.success_F) > self.memory_size:\n                        self.success_F.pop(0)\n                        self.success_CR.pop(0)\n\n                    # Adaptive CR: Adjust CR based on success history\n                    if self.success_CR:\n                        self.CR = np.mean(self.success_CR)\n\n                    self.CR = self.CR + np.random.normal(0, 0.05)\n                    self.CR = np.clip(self.CR, 0.1, 0.9)\n                else:\n                    # Reduce CR if the trial vector is not better\n                    self.CR = self.CR - 0.05  # Decrease CR\n                    self.CR = np.clip(self.CR, 0.1, 0.9)\n\n                # Update best solution\n                if f_u < self.f_opt:\n                    self.f_opt = f_u\n                    self.x_opt = u.copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm LevyAdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["15d46a40-bd89-473b-946f-484189b9f23e"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "b85aa2a7-566c-43c2-949d-15d685a164e3", "fitness": -Infinity, "name": "CooperativeCMAES", "description": "Cooperative CMA-ES with orthogonal subspace sampling, which uses CMA-ES to optimize a subset of dimensions and samples the remaining dimensions from an orthogonal subspace.", "code": "import numpy as np\n\nclass CooperativeCMAES:\n    def __init__(self, budget=10000, dim=10, num_groups=2, initial_sigma=0.5, mu_factor=0.25):\n        self.budget = budget\n        self.dim = dim\n        self.num_groups = num_groups\n        self.group_size = dim // num_groups  # Number of dimensions per group\n        self.initial_sigma = initial_sigma\n        self.mu_factor = mu_factor\n        self.groups = [list(range(i * self.group_size, (i + 1) * self.group_size)) for i in range(num_groups)]\n        remaining_dims = dim % num_groups\n        for i in range(remaining_dims):\n            self.groups[i].append(num_groups * self.group_size + i)\n\n        self.cmaes_optimizers = [CMAES(budget=budget, dim=len(group), initial_sigma=initial_sigma, mu_factor=mu_factor) for group in self.groups]\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n\n    def __call__(self, func):\n        while self.evals < self.budget:\n            x = np.zeros(self.dim)\n            group_solutions = []\n            group_fitnesses = []\n            \n            # Optimize each group using CMA-ES\n            for i, group in enumerate(self.groups):\n                cmaes = self.cmaes_optimizers[i]\n                f_group, x_group = cmaes(SubFunction(func, group))\n                \n                group_solutions.append(x_group)\n                group_fitnesses.append(f_group)\n\n                x[group] = x_group\n\n            f = func(x)\n            self.evals += 1\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x\n        return self.f_opt, self.x_opt\n\nclass SubFunction:\n    def __init__(self, func, dims):\n        self.func = func\n        self.dims = dims\n        self.bounds = func.bounds\n        self.lb = func.bounds.lb\n        self.ub = func.bounds.ub\n\n    def __call__(self, x_sub):\n        x = np.zeros(self.func.bounds.ub.shape[0])\n        x[:] = np.nan\n        for i, idx in enumerate(self.dims):\n            x[idx] = x_sub[i]\n        \n        if np.any(np.isnan(x)):\n            raise ValueError(\"x contains NaN values. This should not happen.\")\n            \n        return self.func(x)\n\nimport numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, mu_factor=0.25, initial_sigma=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.mu = int(dim * mu_factor)  # Number of parents\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cc = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.mean = np.zeros(dim)\n        self.sigma = initial_sigma\n        self.pc = np.zeros(dim)\n        self.ps = np.zeros(dim)\n        self.C = np.eye(dim)\n        self.B = np.eye(dim)\n        self.D = np.ones(dim)\n        self.C_eigen_age = 0\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n\n    def sample_population(self, popsize, func):\n        z = np.random.normal(0, 1, size=(popsize, self.dim))\n        x = self.mean + self.sigma * (self.B @ (self.D * z).T).T\n\n        # Handle boundary constraints using clipping\n        x = np.clip(x, func.bounds.lb, func.bounds.ub)\n        \n        f = np.array([func(xi) for xi in x])\n        return x, f, z\n\n    def update_distribution(self, x, f, z, popsize):\n        idx = np.argsort(f)\n        x = x[idx]\n        z = z[idx]\n        \n        x_mu = x[:self.mu]\n        z_mu = z[:self.mu]\n        \n        self.mean = np.sum(self.weights.reshape(-1, 1) * x_mu, axis=0)\n\n        zmean = np.sum(self.weights.reshape(-1, 1) * z_mu, axis=0)\n        self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * (self.B @ zmean)\n        hsig = np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * (self.budget // popsize))) < 1.4 + 2 / (self.dim + 1)\n        self.pc = (1 - self.cc) * self.pc + hsig * np.sqrt(self.cc * (2 - self.cc) * self.mueff) * (self.mean - self.mean) / self.sigma \n\n        artmp = (1 / self.sigma) * (x_mu - self.mean).T\n        self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * (self.pc.reshape(-1, 1) @ self.pc.reshape(1, -1) + (1-hsig) * self.cc * (2-self.cc) * self.C) + self.cmu * artmp @ np.diag(self.weights) @ artmp.T\n\n        self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * (self.budget // popsize))) / np.sqrt(self.dim) - 1))\n\n    def __call__(self, func):\n        popsize = 4 + int(3 * np.log(self.dim))\n\n        while self.evals < self.budget:\n            x, f, z = self.sample_population(popsize, func)\n            self.evals += popsize\n\n            for i in range(popsize):\n                if f[i] < self.f_opt:\n                    self.f_opt = f[i]\n                    self.x_opt = x[i]\n                    \n            self.update_distribution(x, f, z, popsize)\n            self.C_eigen_age += 1\n            if self.C_eigen_age > self.budget // (10 * popsize): # Re-compute eigenvalue decomposition after a while\n                self.C_eigen_age = 0\n                self.C = np.triu(self.C) + np.triu(self.C, 1).T\n                self.D, self.B = np.linalg.eigh(self.C)\n                self.D = np.sqrt(self.D)\n                \n                # Avoid tiny values\n                self.D[self.D < 1e-10] = 1e-10\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 120, in evaluate\n    algorithm = local_env[algorithm_name](budget=100, dim=2)\n  File \"<string>\", line 16, in __init__\n  File \"<string>\", line 16, in <listcomp>\nNameError: name 'CMAES' is not defined\n.", "error": "", "parent_ids": ["f5d355ca-ae64-4797-902f-7e31b8a0e7c2"], "operator": null, "metadata": {}}
{"id": "707ae99f-9f38-488d-81db-87ff0b4e3660", "fitness": -Infinity, "name": "AdaptivePopulationDE", "description": "A differential evolution strategy with self-adaptive population sizing and a fitness-dependent mutation scaling factor.", "code": "import numpy as np\n\nclass AdaptivePopulationDE:\n    def __init__(self, budget=10000, dim=10, initial_popsize=50, min_popsize=10):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = initial_popsize\n        self.min_popsize = min_popsize\n        self.population = None\n        self.fitness = None\n        self.F = 0.5\n        self.CR = 0.7\n        self.best_fitness = np.inf\n        self.best_solution = None\n        self.evals = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.popsize, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals += self.popsize\n        self.best_fitness = np.min(self.fitness)\n        self.best_solution = self.population[np.argmin(self.fitness)]\n\n    def mutate(self, i, func):\n        indices = np.random.choice(self.popsize, 3, replace=False)\n        x_r1, x_r2, x_r3 = self.population[indices]\n        \n        # Fitness-dependent scaling factor\n        scaling_factor = self.F * (1 + (self.fitness[i] - self.best_fitness) / (np.max(self.fitness) - self.best_fitness + 1e-8))\n        \n        v = x_r1 + scaling_factor * (x_r2 - x_r3)\n        v = np.clip(v, func.bounds.lb, func.bounds.ub)\n        return v\n\n    def crossover(self, i, mutant):\n        trial_vector = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.CR or j == j_rand:\n                trial_vector[j] = mutant[j]\n        return trial_vector\n\n    def adjust_population_size(self):\n         # Adjust population size based on stagnation (simplified)\n        if self.evals % (self.budget // 10) == 0: # Check stagnation every 10% of budget\n            unique_fitness = len(np.unique(self.fitness))\n            if unique_fitness < self.popsize // 5 and self.popsize > self.min_popsize:\n                self.popsize = max(self.popsize // 2, self.min_popsize)  # Reduce population if stagnating\n                print(f\"Reducing population size to {self.popsize}\")\n            elif unique_fitness > self.popsize // 2 and self.popsize < 200:\n                self.popsize = min(self.popsize * 2, 200)\n                print(f\"Increasing population size to {self.popsize}\")\n\n            # Repopulate to the new population size\n            new_population = np.copy(self.population[:self.popsize])\n            new_fitness = np.copy(self.fitness[:self.popsize])\n            self.population = new_population\n            self.fitness = new_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.evals < self.budget:\n            self.adjust_population_size() # Dynamically adjusts the population size\n\n            for i in range(self.popsize):\n                mutant = self.mutate(i, func)\n                trial_vector = self.crossover(i, mutant)\n                \n                f_trial = func(trial_vector)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = f_trial\n\n                    if f_trial < self.best_fitness:\n                        self.best_fitness = f_trial\n                        self.best_solution = trial_vector\n\n                if self.evals >= self.budget:\n                    break\n\n            # Adaptive F and CR (simplified - can be improved)\n            self.F = np.random.uniform(0.4, 0.9)\n            self.CR = np.random.uniform(0.3, 1.0)\n\n\n        return self.best_fitness, self.best_solution", "configspace": "", "generation": 2, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 67, in __call__\n  File \"<string>\", line 26, in mutate\nIndexError: index 70 is out of bounds for axis 0 with size 50\n.", "error": "", "parent_ids": ["30c77ee1-192f-4452-ba2c-a0452ddce534"], "operator": null, "metadata": {}}
{"id": "4f2d65a5-fef4-493f-92b4-f3047b5dd698", "fitness": 0.5122788445415312, "name": "EnhancedPSO", "description": "An enhanced PSO variant that incorporates a velocity mutation strategy inspired by differential evolution and a self-adaptive exploration rate.", "code": "import numpy as np\n\nclass EnhancedPSO:\n    def __init__(self, budget=10000, dim=10, pop_size=20, w=0.7, c1=1.5, c2=1.5, mutation_rate=0.1, exploration_rate=0.3):\n        \"\"\"\n        Initialize the Enhanced PSO algorithm.\n\n        Args:\n            budget (int): Total number of function evaluations.\n            dim (int): Dimensionality of the problem.\n            pop_size (int): Population size.\n            w (float): Inertia weight for PSO.\n            c1 (float): Cognitive coefficient for PSO.\n            c2 (float): Social coefficient for PSO.\n            mutation_rate (float): Probability of applying velocity mutation.\n            exploration_rate (float): Probability of random exploration.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w = w\n        self.c1 = c1\n        self.c2 = c2\n        self.mutation_rate = mutation_rate\n        self.exploration_rate = exploration_rate\n        self.pop = None\n        self.fitness = None\n        self.velocities = None\n        self.x_best_global = None\n        self.f_best_global = np.inf\n        self.lb = None\n        self.ub = None\n        self.v_max = None\n\n    def initialize_population(self, func):\n        \"\"\"\n        Initialize the population within the bounds of the function.\n\n        Args:\n            func: The black-box optimization function.\n        \"\"\"\n        self.lb = func.bounds.lb\n        self.ub = func.bounds.ub\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.velocities = np.zeros((self.pop_size, self.dim))\n        self.x_best_global = self.pop[np.argmin(self.fitness)].copy()\n        self.f_best_global = np.min(self.fitness)\n        self.v_max = 0.2 * (self.ub - self.lb)\n\n\n    def velocity_mutation(self, i):\n        \"\"\"\n        Apply differential evolution-inspired velocity mutation.\n\n        Args:\n            i (int): Index of the particle to mutate.\n        \"\"\"\n        idxs = np.random.choice(self.pop_size, 3, replace=False)\n        while i in idxs:\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n\n        F = 0.8  # Mutation factor\n        self.velocities[i] += F * (self.velocities[idxs[0]] - self.velocities[idxs[1]]) + F * (self.velocities[idxs[2]] - self.velocities[i])\n\n    def pso_update(self, func):\n        \"\"\"\n        Update the population using PSO principles with velocity mutation and adaptive exploration.\n\n        Args:\n            func: The black-box optimization function.\n        \"\"\"\n        for i in range(self.pop_size):\n            # Update velocities\n            r1 = np.random.rand(self.dim)\n            r2 = np.random.rand(self.dim)\n            self.velocities[i] = self.w * self.velocities[i] + \\\n                                 self.c1 * r1 * (self.pop[i] - self.pop[i]) + \\\n                                 self.c2 * r2 * (self.x_best_global - self.pop[i])\n\n            # Velocity clamping\n            self.velocities[i] = np.clip(self.velocities[i], -self.v_max, self.v_max)\n\n            # Velocity mutation\n            if np.random.rand() < self.mutation_rate:\n                self.velocity_mutation(i)\n\n            # Update positions\n            new_position = self.pop[i] + self.velocities[i]\n\n            # Boundary handling (clip to bounds)\n            new_position = np.clip(new_position, self.lb, self.ub)\n\n            # Adaptive exploration: replace particle with random position\n            if np.random.rand() < self.exploration_rate:\n                new_position = np.random.uniform(self.lb, self.ub)\n\n            new_fitness = func(new_position)  # Evaluate new position\n            \n            if new_fitness < self.fitness[i]:\n                self.pop[i] = new_position\n                self.fitness[i] = new_fitness\n                if new_fitness < self.f_best_global:\n                    self.f_best_global = new_fitness\n                    self.x_best_global = self.pop[i].copy()\n        \n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using the Enhanced PSO algorithm.\n\n        Args:\n            func: The black-box optimization function.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        self.initialize_population(func)\n        eval_count = self.pop_size # Account for initial population evaluation\n\n        while eval_count < self.budget:\n            # Adaptive parameter adjustment (example: linear decrease of inertia weight)\n            self.w = 0.7 - 0.5 * (eval_count / self.budget)\n            self.pso_update(func)\n            eval_count += self.pop_size # Account for population evaluation\n\n            if eval_count > self.budget:\n                break\n\n        return self.f_best_global, self.x_best_global", "configspace": "", "generation": 2, "feedback": "The algorithm EnhancedPSO scored 0.512 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e6287ebd-8657-4245-a7af-77fddddbf2dd"], "operator": null, "metadata": {"aucs": [0.16613244603868038, 0.3683078857527198, 0.7460494157701294, 0.8200183607266082, 0.3236565720860295, 0.2829630942731419, 0.29387241036095335, 0.401337894830161, 0.7826809824045637, 0.19225575495269165, 0.7516114384601877, 0.9962532868440992, 0.2904651847484002, 0.25706732842141955, 0.875431779533091, 0.8247589533344649, 0.3839661723710356, 0.7421173725691226, 0.24002357056142465, 0.5066069867916996]}}
{"id": "b506287c-816e-41a8-bcbb-422cb8c10696", "fitness": 0.7568001680255396, "name": "MultiStrategyAdaptiveDE", "description": "An adaptive differential evolution strategy that uses a combination of multiple mutation strategies with probabilistic selection and dynamically adjusts parameters based on the success rate of different mutation operators.", "code": "import numpy as np\n\nclass MultiStrategyAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Initial mutation factor\n        self.CR = CR  # Initial crossover rate\n        self.mutation_strategies = [self.mutation_strategy_1, self.mutation_strategy_2, self.mutation_strategy_3]\n        self.mutation_probs = np.ones(len(self.mutation_strategies)) / len(self.mutation_strategies)  # Initial probabilities for each strategy\n        self.success_counts = np.zeros(len(self.mutation_strategies))\n        self.strategy_usage = np.zeros(len(self.mutation_strategies))\n        self.learning_rate = 0.1\n\n\n    def mutation_strategy_1(self, population, i):\n        # DE/rand/1\n        indices = np.random.choice(self.pop_size, 3, replace=False)\n        x_r1, x_r2, x_r3 = population[indices]\n        return population[i] + self.F * (x_r1 - population[i]) + self.F * (x_r2 - x_r3)\n\n    def mutation_strategy_2(self, population, i):\n        # DE/current-to-rand/1\n        indices = np.random.choice(self.pop_size, 3, replace=False)\n        x_r1, x_r2, x_r3 = population[indices]\n        return population[i] + self.F * (x_r1 - population[i]) + self.F * (x_r2 - x_r3)\n\n    def mutation_strategy_3(self, population, i, best_x):\n         # DE/best/1\n        indices = np.random.choice(self.pop_size, 2, replace=False)\n        x_r1, x_r2 = population[indices]\n        return best_x + self.F * (x_r1 - x_r2)\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n        best_x = self.x_opt.copy()\n\n        # Evolution loop\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Strategy selection\n                strategy_index = np.random.choice(len(self.mutation_strategies), p=self.mutation_probs)\n                self.strategy_usage[strategy_index] += 1\n                mutation_strategy = self.mutation_strategies[strategy_index]\n                \n                # Mutation\n                if mutation_strategy == self.mutation_strategy_3:\n                    v = mutation_strategy(population, i, best_x)\n                else:\n                    v = mutation_strategy(population, i)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.copy(population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        u[j] = v[j]\n\n                # Evaluation\n                f_u = func(u)\n                self.budget -= 1\n\n                if f_u < fitness[i]:\n                    # Replacement\n                    fitness[i] = f_u\n                    population[i] = u\n\n                    # Update best solution\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u.copy()\n                        best_x = self.x_opt.copy()\n\n                    self.success_counts[strategy_index] += 1\n\n            # Update mutation probabilities\n            total_usage = np.sum(self.strategy_usage)\n            if total_usage > 0:\n                success_rates = self.success_counts / self.strategy_usage\n                for k in range(len(self.mutation_strategies)):\n                    self.mutation_probs[k] += self.learning_rate * (success_rates[k] - self.mutation_probs[k])\n                self.mutation_probs = np.maximum(self.mutation_probs, 0.01)  # Avoid zero probabilities\n                self.mutation_probs /= np.sum(self.mutation_probs)\n\n            self.success_counts[:] = 0\n            self.strategy_usage[:] = 0\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm MultiStrategyAdaptiveDE scored 0.757 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["15d46a40-bd89-473b-946f-484189b9f23e"], "operator": null, "metadata": {"aucs": [0.4752413375593374, 0.23384758964025754, 0.8076481052676519, 0.9153032063527071, 0.8581176588473404, 0.8904551569991683, 0.7670786515396602, 0.8128640886716194, 0.8504478107552234, 0.7728229827043895, 0.9117602113000752, 0.9926254001148997, 0.6091316453193878, 0.8530449394741241, 0.9392879011651594, 0.8777366632439746, 0.7423571803758638, 0.9088510594808693, 0.4078975813942909, 0.5094841903047922]}}
{"id": "5be486cb-6a4a-41cb-be61-dd275c0bd84d", "fitness": 0.31013957323565616, "name": "ToroidalDE", "description": "An adaptive DE algorithm that uses a toroidal topology for selecting mutation vectors and adjusts F and CR based on a learning automaton.", "code": "import numpy as np\n\nclass ToroidalDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.learning_rate = learning_rate\n        self.population = None\n        self.fitness = None\n        self.best_index = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.F_history = []\n        self.CR_history = []\n        self.automaton_states = [0, 1]  # Two states for F and CR adaptation\n        self.F_probabilities = [0.5, 0.5]  # Initial probabilities for F choices\n        self.CR_probabilities = [0.5, 0.5]  # Initial probabilities for CR choices\n        self.F_choices = [0.3, 0.7]  # Two F values to choose from\n        self.CR_choices = [0.2, 0.8]  # Two CR values to choose from\n\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index].copy()\n\n\n    def toroidal_mutation(self, i):\n        p = self.pop_size\n        r1 = (i + 1) % p\n        r2 = (i - 1 + p) % p\n        r3 = (i + 2) % p\n\n        return self.population[r1], self.population[r2], self.population[r3]\n\n    def learning_automaton_update(self, is_successful, param_type):\n        if param_type == 'F':\n            probabilities = self.F_probabilities\n            choices = self.F_choices\n            state = self.automaton_states[0] # F has only one automaton state\n        elif param_type == 'CR':\n            probabilities = self.CR_probabilities\n            choices = self.CR_choices\n            state = self.automaton_states[0] # CR has only one automaton state\n        else:\n            return\n\n        if is_successful:\n            probabilities[state] += self.learning_rate * (1 - probabilities[state])\n        else:\n            probabilities[state] -= self.learning_rate * probabilities[state]\n\n        # Normalize probabilities\n        total = sum(probabilities)\n        probabilities[0] /= total\n        probabilities[1] /= total\n\n        if param_type == 'F':\n            self.F_probabilities = probabilities\n        elif param_type == 'CR':\n            self.CR_probabilities = probabilities\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Select F and CR using learning automaton\n                F_current = np.random.choice(self.F_choices, p=self.F_probabilities)\n                CR_current = np.random.choice(self.CR_choices, p=self.CR_probabilities)\n\n                # Mutation using toroidal topology\n                x_r1, x_r2, x_r3 = self.toroidal_mutation(i)\n                v = self.population[i] + F_current * (x_r1 - x_r2)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = self.population[i].copy()\n                for j in range(self.dim):\n                    if np.random.rand() < CR_current or j == j_rand:\n                        u[j] = v[j]\n\n                # Evaluation\n                f_u = func(u)\n                self.budget -= 1\n\n                # Selection\n                if f_u < self.fitness[i]:\n                    # Success\n                    self.learning_automaton_update(True, 'F')\n                    self.learning_automaton_update(True, 'CR')\n                    self.fitness[i] = f_u\n                    self.population[i] = u\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u.copy()\n                else:\n                    # Failure\n                    self.learning_automaton_update(False, 'F')\n                    self.learning_automaton_update(False, 'CR')\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm ToroidalDE scored 0.310 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["15d46a40-bd89-473b-946f-484189b9f23e"], "operator": null, "metadata": {"aucs": [0.10936300280819755, 0.18375668662367572, 0.29814833088058856, 0.3133376605248386, 0.23797907099170112, 0.2825916491076037, 0.25348765551207175, 0.23022642644934965, 0.21113164468804113, 0.16175729585385656, 0.23127757235149626, 0.999909763288943, 0.26982187118762846, 0.2613003574841777, 0.6050994030038082, 0.3231598924127951, 0.2567443745264253, 0.339511209325968, 0.16587922356867457, 0.46830837412328297]}}
{"id": "a2c37750-5f11-443f-9d8c-527210118117", "fitness": 0.5543640784178068, "name": "AdaptiveCMAES", "description": "A CMA-ES variant that adaptively adjusts its population size based on the landscape's ruggedness, using a novelty search component to escape local optima.", "code": "import numpy as np\n\nclass AdaptiveCMAES:\n    def __init__(self, budget=10000, dim=10, mu_factor=0.25, initial_sigma=0.5, novelty_threshold=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.mu = int(dim * mu_factor)  # Number of parents\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cc = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.mean = np.zeros(dim)\n        self.sigma = initial_sigma\n        self.pc = np.zeros(dim)\n        self.ps = np.zeros(dim)\n        self.C = np.eye(dim)\n        self.B = np.eye(dim)\n        self.D = np.ones(dim)\n        self.C_eigen_age = 0\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.novelty_archive = []\n        self.novelty_threshold = novelty_threshold\n\n    def sample_population(self, popsize, func):\n        z = np.random.normal(0, 1, size=(popsize, self.dim))\n        x = self.mean + self.sigma * (self.B @ (self.D * z).T).T\n        x = np.clip(x, func.bounds.lb, func.bounds.ub)  # Boundary handling\n        f = np.array([func(xi) for xi in x])\n        return x, f, z\n\n    def calculate_novelty(self, x):\n        if not self.novelty_archive:\n            return np.inf  # High novelty for first sample\n        distances = [np.linalg.norm(x - archive_x) for archive_x in self.novelty_archive]\n        return np.min(distances)\n\n    def update_distribution(self, x, f, z, popsize, func):\n        idx = np.argsort(f)\n        x = x[idx]\n        z = z[idx]\n        x_mu = x[:self.mu]\n        z_mu = z[:self.mu]\n\n        self.mean = np.sum(self.weights.reshape(-1, 1) * x_mu, axis=0)\n\n        zmean = np.sum(self.weights.reshape(-1, 1) * z_mu, axis=0)\n        self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * (self.B @ zmean)\n        hsig = np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * (self.budget // popsize))) < 1.4 + 2 / (self.dim + 1)\n        self.pc = (1 - self.cc) * self.pc + hsig * np.sqrt(self.cc * (2 - self.cc) * self.mueff) * (self.mean - self.mean) / self.sigma\n\n        artmp = (1 / self.sigma) * (x_mu - self.mean).T\n        self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * (self.pc.reshape(-1, 1) @ self.pc.reshape(1, -1) + (1-hsig) * self.cc * (2-self.cc) * self.C) + self.cmu * artmp @ np.diag(self.weights) @ artmp.T\n\n        self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * (self.budget // popsize))) / np.sqrt(self.dim) - 1))\n\n        # Novelty Search component: Add diverse solutions to archive\n        for xi in x:\n            novelty = self.calculate_novelty(xi)\n            if novelty > self.novelty_threshold:\n                self.novelty_archive.append(xi)\n                if len(self.novelty_archive) > 50:  # Limit archive size\n                    self.novelty_archive.pop(0)\n\n    def __call__(self, func):\n        popsize = 4 + int(3 * np.log(self.dim))\n        evals = 0\n\n        while evals < self.budget:\n            x, f, z = self.sample_population(popsize, func)\n            evals += popsize\n\n            for i in range(popsize):\n                if f[i] < self.f_opt:\n                    self.f_opt = f[i]\n                    self.x_opt = x[i]\n\n            self.update_distribution(x, f, z, popsize, func)\n            self.C_eigen_age += 1\n\n            if self.C_eigen_age > self.budget // (10 * popsize): # Re-compute eigenvalue decomposition after a while\n                self.C_eigen_age = 0\n                self.C = np.triu(self.C) + np.triu(self.C, 1).T\n                try:\n                    self.D, self.B = np.linalg.eigh(self.C)\n                    self.D = np.sqrt(self.D)\n                    self.D[self.D < 1e-10] = 1e-10  # Avoid tiny values\n                except np.linalg.LinAlgError:\n                    # Handle non-positive definite matrix\n                    self.C = np.eye(self.dim)  # Reset covariance matrix\n                    self.B = np.eye(self.dim)\n                    self.D = np.ones(self.dim)\n                    \n\n            # Adaptive Population Size Adjustment (example)\n            if evals % (self.budget // 10) == 0:\n                 # Rough estimate of landscape ruggedness\n                fitness_std = np.std(f)\n                if fitness_std < 1e-3:  # Flat landscape, reduce popsize\n                   popsize = max(4, popsize // 2)\n                elif fitness_std > 1:   # Rugged landscape, increase popsize\n                   popsize = min(self.dim * 10, popsize * 2)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveCMAES scored 0.554 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f5d355ca-ae64-4797-902f-7e31b8a0e7c2"], "operator": null, "metadata": {"aucs": [0.15900296676986925, 0.1855336952148442, 0.9331796359523793, 0.17981944291680352, 0.30479808772549455, 0.9539813409614202, 0.33690995331546636, 0.7610243248736683, 0.8797921509196827, 0.245896335881424, 0.9707264940413715, 0.9950773632433385, 0.28017347692021444, 0.6098352308283164, 0.9313353524423931, 0.3163952321102451, 0.36061567598230415, 0.9642127579003528, 0.21237239481011272, 0.5065996555464372]}}
{"id": "62be0301-43eb-4214-abf5-3156eabe0b8f", "fitness": -Infinity, "name": "AdaptiveDERS", "description": "An adaptive differential evolution strategy that uses a dynamically adjusted population size based on fitness diversity and introduces a restart mechanism to escape local optima.", "code": "import numpy as np\n\nclass AdaptiveDERS:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, min_pop_size=10, max_pop_size=100, F=0.5, CR=0.7, diversity_threshold=0.01, restart_frequency=500):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = initial_pop_size\n        self.min_pop_size = min_pop_size\n        self.max_pop_size = max_pop_size\n        self.pop_size = initial_pop_size\n        self.F = F\n        self.CR = CR\n        self.diversity_threshold = diversity_threshold\n        self.restart_frequency = restart_frequency\n        self.population = None\n        self.fitness = None\n        self.best_index = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.generation = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index].copy()\n\n    def calculate_diversity(self):\n        distances = np.linalg.norm(self.population - np.mean(self.population, axis=0), axis=1)\n        diversity = np.std(distances)\n        return diversity\n\n    def adjust_population_size(self):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold:\n            self.pop_size = max(self.min_pop_size, int(self.pop_size * 0.8))  # Reduce population size\n        else:\n            self.pop_size = min(self.max_pop_size, int(self.pop_size * 1.2))  # Increase population size\n        self.pop_size = int(self.pop_size) # Ensure integer pop_size\n\n\n    def restart_population(self, func):\n         self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n         self.fitness = np.array([func(x) for x in self.population])\n         self.eval_count += self.pop_size\n         best_index = np.argmin(self.fitness)\n         if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index].copy()\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            self.generation += 1\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                v = self.population[i] + self.F * (x_r1 - x_r2)\n\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = self.population[i].copy()\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        u[j] = v[j]\n\n                # Evaluation\n                f_u = func(u)\n                self.eval_count += 1\n\n                # Selection\n                if f_u < self.fitness[i]:\n                    self.fitness[i] = f_u\n                    self.population[i] = u\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u.copy()\n\n                if self.eval_count >= self.budget:\n                    break\n            \n            if self.generation % 10 == 0:  # Adjust population every 10 generations\n                self.adjust_population_size()\n\n            if self.generation % self.restart_frequency == 0:\n                self.restart_population(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 144, in evaluate\n    algorithm(f_new)\n  File \"<string>\", line 63, in __call__\nIndexError: index 51 is out of bounds for axis 0 with size 50\n.", "error": "", "parent_ids": ["5be486cb-6a4a-41cb-be61-dd275c0bd84d"], "operator": null, "metadata": {}}
{"id": "d59456bb-4dcb-4c1a-a38e-23edfcf51634", "fitness": -Infinity, "name": "OrthogonalDE", "description": "A differential evolution strategy employing orthogonal learning to enhance population diversity and convergence speed by probabilistically replacing components of individuals with values derived from orthogonal arrays.", "code": "import numpy as np\n\nclass OrthogonalDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, orthogonal_learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.population = None\n        self.fitness = None\n        self.best_index = None\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index].copy()\n\n    def orthogonal_array_generation(self, n, k, level):\n        # A simplified orthogonal array generation for demonstration.\n        # In practice, use dedicated OA libraries for efficiency.\n        # This generates an L9 array (9 runs, 4 factors, 3 levels).\n        if n == 9 and k == 4 and level == 3:\n            oa = np.array([\n                [0, 0, 0, 0],\n                [0, 1, 1, 1],\n                [0, 2, 2, 2],\n                [1, 0, 1, 2],\n                [1, 1, 2, 0],\n                [1, 2, 0, 1],\n                [2, 0, 2, 1],\n                [2, 1, 0, 2],\n                [2, 2, 1, 0]\n            ])\n            return oa\n        else:\n            return None\n\n    def orthogonal_learning(self, individual, func):\n        oa = self.orthogonal_array_generation(n=9, k=min(4, self.dim), level=3)  # Simplified L9 OA\n        if oa is None:\n            return individual\n\n        # Select 'k' dimensions for orthogonal learning\n        dimensions = np.random.choice(range(self.dim), size=min(4, self.dim), replace=False)\n        levels = np.linspace(func.bounds.lb, func.bounds.ub, 3)  # 3 levels based on bounds\n\n        # Generate candidate solutions based on OA\n        candidates = []\n        for row in oa:\n            candidate = individual.copy()\n            for i, dim_index in enumerate(dimensions):\n                candidate[dim_index] = levels[row[i]]\n            candidates.append(candidate)\n\n        # Evaluate candidates\n        fitness_values = [func(x) for x in candidates]\n        self.budget -= len(candidates)\n        if self.budget <= 0:\n            return individual\n\n        # Select the best candidate\n        best_index = np.argmin(fitness_values)\n        if fitness_values[best_index] < func(individual):  # Compare with original\n            return candidates[best_index]\n        else:\n            return individual\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[idxs]\n                v = self.population[i] + self.F * (x_r1 - x_r2)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = self.population[i].copy()\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        u[j] = v[j]\n\n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_learning_rate and self.budget > 0:\n                    u = self.orthogonal_learning(u, func)\n\n                # Evaluation\n                f_u = func(u)\n                self.budget -= 1\n\n                # Selection\n                if f_u < self.fitness[i]:\n                    self.fitness[i] = f_u\n                    self.population[i] = u\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u.copy()\n\n            if self.budget <=0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "An exception occured: TypeError: only size-1 arrays can be converted to Python scalars\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 144, in evaluate\n    algorithm(f_new)\n  File \"<string>\", line 95, in __call__\n  File \"<string>\", line 59, in orthogonal_learning\nValueError: setting an array element with a sequence.\n.", "error": "", "parent_ids": ["5be486cb-6a4a-41cb-be61-dd275c0bd84d"], "operator": null, "metadata": {}}
{"id": "79a001d1-9366-4d50-992b-f77d99b8f4c5", "fitness": -Infinity, "name": "FuzzyAdaptiveDE", "description": "A self-organizing differential evolution algorithm that dynamically adjusts population diversity and mutation strength using a fuzzy logic controller based on population entropy and fitness variance.", "code": "import numpy as np\nimport skfuzzy as fuzz\nfrom skfuzzy import control as ctrl\n\nclass FuzzyAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_initial=0.7, CR=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_initial\n        self.CR = CR\n        self.lb = -5.0\n        self.ub = 5.0\n\n        # Fuzzy Logic Controller for F adaptation\n        self.entropy = ctrl.Antecedent(np.linspace(0, 1, 100), 'entropy')\n        self.variance = ctrl.Antecedent(np.linspace(0, 1, 100), 'variance')\n        self.F_output = ctrl.Consequent(np.linspace(0.1, 1.0, 100), 'F')\n\n        # Define fuzzy membership functions (adjust as needed)\n        self.entropy['low'] = fuzz.trimf(self.entropy.universe, [0, 0, 0.5])\n        self.entropy['medium'] = fuzz.trimf(self.entropy.universe, [0.25, 0.5, 0.75])\n        self.entropy['high'] = fuzz.trimf(self.entropy.universe, [0.5, 1, 1])\n\n        self.variance['low'] = fuzz.trimf(self.variance.universe, [0, 0, 0.5])\n        self.variance['medium'] = fuzz.trimf(self.variance.universe, [0.25, 0.5, 0.75])\n        self.variance['high'] = fuzz.trimf(self.variance.universe, [0.5, 1, 1])\n\n        self.F_output['small'] = fuzz.trimf(self.F_output.universe, [0.1, 0.1, 0.5])\n        self.F_output['medium'] = fuzz.trimf(self.F_output.universe, [0.3, 0.5, 0.7])\n        self.F_output['large'] = fuzz.trimf(self.F_output.universe, [0.5, 1, 1])\n\n        # Define fuzzy rules (adjust as needed)\n        rule1 = ctrl.Rule(self.entropy['low'] & self.variance['low'], self.F_output['large'])\n        rule2 = ctrl.Rule(self.entropy['low'] & self.variance['medium'], self.F_output['medium'])\n        rule3 = ctrl.Rule(self.entropy['low'] & self.variance['high'], self.F_output['small'])\n        rule4 = ctrl.Rule(self.entropy['medium'] & self.variance['low'], self.F_output['medium'])\n        rule5 = ctrl.Rule(self.entropy['medium'] & self.variance['medium'], self.F_output['medium'])\n        rule6 = ctrl.Rule(self.entropy['medium'] & self.variance['high'], self.F_output['small'])\n        rule7 = ctrl.Rule(self.entropy['high'] & self.variance['low'], self.F_output['large'])\n        rule8 = ctrl.Rule(self.entropy['high'] & self.variance['medium'], self.F_output['small'])\n        rule9 = ctrl.Rule(self.entropy['high'] & self.variance['high'], self.F_output['small'])\n\n        self.F_ctrl = ctrl.ControlSystem([rule1, rule2, rule3, rule4, rule5, rule6, rule7, rule8, rule9])\n        self.F_sim = ctrl.ControlSystemSimulation(self.F_ctrl)\n\n    def calculate_entropy(self, population):\n        # Estimate probability distribution (simplified)\n        hist, _ = np.histogram(population.flatten(), bins=10, range=(self.lb, self.ub), density=True)\n        hist = hist / np.sum(hist)  # Normalize\n        hist = hist[hist > 0] # Avoid log(0)\n        return -np.sum(hist * np.log(hist)) / np.log(len(hist))  # Normalize entropy\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Calculate entropy and variance\n            entropy = self.calculate_entropy(population)\n            variance = np.var(fitness)\n            # Normalize variance between 0 and 1\n            variance = (variance - np.min(fitness)) / (np.max(fitness) - np.min(fitness) + 1e-8) # Avoid division by zero\n\n            # Fuzzy inference for F\n            self.F_sim.input['entropy'] = entropy\n            self.F_sim.input['variance'] = variance\n            self.F_sim.compute()\n            self.F = self.F_sim.output['F']\n\n            for i in range(self.pop_size):\n                # Mutation (DE/rand/1)\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[indices]\n                v = population[i] + self.F * (x_r1 - population[i]) + self.F * (x_r2 - x_r3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.copy(population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        u[j] = v[j]\n\n                # Evaluation\n                f_u = func(u)\n                self.budget -= 1\n\n                if f_u < fitness[i]:\n                    # Replacement\n                    fitness[i] = f_u\n                    population[i] = u\n\n                    # Update best solution\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u.copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 111, in evaluate\n    exec(code, safe_globals, local_env)\n  File \"<string>\", line 2, in <module>\nModuleNotFoundError: No module named 'skfuzzy'\n.", "error": "", "parent_ids": ["b506287c-816e-41a8-bcbb-422cb8c10696"], "operator": null, "metadata": {}}
{"id": "ebaf8feb-b76c-43be-ae64-505129468592", "fitness": 0.0, "name": "OrthogonalDE", "description": "A differential evolution strategy with self-adaptive population size reduction and a diversity maintenance mechanism based on orthogonal design.", "code": "import numpy as np\n\nclass OrthogonalDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=100, min_pop_size=10, F=0.5, CR=0.7, reduction_factor=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.min_pop_size = min_pop_size\n        self.initial_pop_size = initial_pop_size\n        self.F = F\n        self.CR = CR\n        self.reduction_factor = reduction_factor\n        self.population = None\n        self.fitness = None\n        self.best_index = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.generation = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index].copy()\n\n    def orthogonal_design(self, N, k, levels):\n        # Generate an orthogonal array using Plackett-Burman design\n        if N == 12 and k <= 11:\n            H = np.array([\n                [+1, +1, +1, +1, +1, +1, +1, +1, +1, +1, +1, +1],\n                [-1, +1, -1, +1, -1, +1, +1, -1, -1, +1, -1, -1],\n                [-1, -1, +1, -1, +1, +1, -1, -1, +1, -1, +1, -1],\n                [-1, -1, -1, +1, -1, +1, +1, -1, -1, +1, -1, +1],\n                [-1, -1, -1, -1, +1, -1, +1, +1, -1, -1, +1, +1],\n                [-1, -1, -1, -1, -1, +1, -1, +1, +1, -1, -1, +1],\n                [-1, -1, -1, -1, -1, -1, +1, -1, +1, +1, -1, +1],\n                [-1, -1, -1, -1, -1, -1, -1, +1, -1, +1, +1, +1],\n                [-1, -1, -1, -1, -1, -1, -1, -1, +1, -1, +1, +1],\n                [-1, -1, -1, -1, -1, -1, -1, -1, -1, +1, -1, +1],\n                [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, +1, +1]\n            ])\n            oa = H[:k, :].T\n            oa = (oa + 1) / 2 * (levels - 1)\n            return oa.astype(int)\n        else:\n            raise ValueError(\"Orthogonal design only supported for N=12 and k<=11\")\n\n\n    def reduce_population(self):\n        if self.pop_size > self.min_pop_size:\n            num_to_remove = max(1, int((self.pop_size - self.min_pop_size) * (1 - self.reduction_factor)))\n            \n            # Remove the worst individuals\n            worst_indices = np.argsort(self.fitness)[-num_to_remove:]\n            self.population = np.delete(self.population, worst_indices, axis=0)\n            self.fitness = np.delete(self.fitness, worst_indices)\n            self.pop_size = self.population.shape[0]\n            self.best_index = np.argmin(self.fitness)  # Update best index after removal\n            self.f_opt = self.fitness[self.best_index]\n            self.x_opt = self.population[self.best_index].copy()\n\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            self.generation += 1\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                v = self.population[i] + self.F * (x_r1 - x_r2)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = self.population[i].copy()\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        u[j] = v[j]\n\n                # Evaluation\n                f_u = func(u)\n                self.budget -= 1\n\n                # Selection\n                if f_u < self.fitness[i]:\n                    self.fitness[i] = f_u\n                    self.population[i] = u\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u.copy()\n            \n            # Population reduction every few generations\n            if self.generation % 5 == 0:\n                self.reduce_population()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm OrthogonalDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5be486cb-6a4a-41cb-be61-dd275c0bd84d"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "95e9a1ef-ff76-4427-a6dd-fa7cdf931524", "fitness": -Infinity, "name": "RankDiversityPSO", "description": "A PSO variant that employs a modified velocity update rule based on the fitness ranking of particles and introduces a diversity maintenance mechanism using a crowding distance metric.", "code": "import numpy as np\n\nclass RankDiversityPSO:\n    def __init__(self, budget=10000, dim=10, pop_size=20, w=0.7, c1=1.5, c2=1.5, diversity_rate=0.1):\n        \"\"\"\n        Initialize the Rank Diversity PSO algorithm.\n\n        Args:\n            budget (int): Total number of function evaluations.\n            dim (int): Dimensionality of the problem.\n            pop_size (int): Population size.\n            w (float): Inertia weight for PSO.\n            c1 (float): Cognitive coefficient for PSO.\n            c2 (float): Social coefficient for PSO.\n            diversity_rate (float): Probability of applying diversity maintenance.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w = w\n        self.c1 = c1\n        self.c2 = c2\n        self.diversity_rate = diversity_rate\n        self.pop = None\n        self.fitness = None\n        self.velocities = None\n        self.x_best_global = None\n        self.f_best_global = np.inf\n        self.lb = None\n        self.ub = None\n        self.v_max = None\n\n    def initialize_population(self, func):\n        \"\"\"\n        Initialize the population within the bounds of the function.\n\n        Args:\n            func: The black-box optimization function.\n        \"\"\"\n        self.lb = func.bounds.lb\n        self.ub = func.bounds.ub\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.velocities = np.zeros((self.pop_size, self.dim))\n        self.x_best_global = self.pop[np.argmin(self.fitness)].copy()\n        self.f_best_global = np.min(self.fitness)\n        self.v_max = 0.2 * (self.ub - self.lb)\n\n    def crowding_distance(self):\n        \"\"\"\n        Calculate the crowding distance for each particle.\n        \"\"\"\n        distances = np.zeros(self.pop_size)\n        for m in range(self.dim):\n            # Sort population based on the m-th dimension\n            sorted_indices = np.argsort(self.pop[:, m])\n            distances[sorted_indices[0]] = np.inf\n            distances[sorted_indices[-1]] = np.inf\n            for i in range(1, self.pop_size - 1):\n                distances[sorted_indices[i]] += (self.pop[sorted_indices[i+1], m] - self.pop[sorted_indices[i-1], m]) / (self.ub - self.lb)\n        return distances\n\n    def rank_based_velocity_update(self):\n        \"\"\"\n        Update velocities based on fitness ranking.\n        \"\"\"\n        ranked_indices = np.argsort(self.fitness)\n        for i in range(self.pop_size):\n            r1 = np.random.rand(self.dim)\n            r2 = np.random.rand(self.dim)\n            # Top 20% particles are considered \"good\"\n            if i < int(0.2 * self.pop_size):  \n                self.velocities[ranked_indices[i]] = self.w * self.velocities[ranked_indices[i]] + \\\n                                                     self.c1 * r1 * (self.x_best_global - self.pop[ranked_indices[i]]) + \\\n                                                     self.c2 * r2 * (self.pop[ranked_indices[0]] - self.pop[ranked_indices[i]]) # Attracted to best particle\n            else:\n                self.velocities[ranked_indices[i]] = self.w * self.velocities[ranked_indices[i]] + \\\n                                                     self.c1 * r1 * (self.x_best_global - self.pop[ranked_indices[i]]) + \\\n                                                     self.c2 * r2 * (self.pop[ranked_indices[int(0.2 * self.pop_size)]] - self.pop[ranked_indices[i]])  # Attracted to top 20%\n\n            self.velocities[ranked_indices[i]] = np.clip(self.velocities[ranked_indices[i]], -self.v_max, self.v_max)\n\n    def diversity_maintenance(self):\n        \"\"\"\n        Maintain population diversity using crowding distance.\n        \"\"\"\n        distances = self.crowding_distance()\n        worst_particle_index = np.argmin(distances)\n        self.pop[worst_particle_index] = np.random.uniform(self.lb, self.ub)\n        self.fitness[worst_particle_index] = np.inf  # Mark as needing evaluation\n\n    def pso_update(self, func):\n        \"\"\"\n        Update the population using PSO principles with rank-based velocity update and diversity maintenance.\n\n        Args:\n            func: The black-box optimization function.\n        \"\"\"\n        self.rank_based_velocity_update()\n\n        for i in range(self.pop_size):\n            new_position = self.pop[i] + self.velocities[i]\n            new_position = np.clip(new_position, self.lb, self.ub)\n            new_fitness = func(new_position)\n\n            if new_fitness < self.fitness[i]:\n                self.pop[i] = new_position\n                self.fitness[i] = new_fitness\n                if new_fitness < self.f_best_global:\n                    self.f_best_global = new_fitness\n                    self.x_best_global = self.pop[i].copy()\n\n        if np.random.rand() < self.diversity_rate:\n            self.diversity_maintenance()\n            \n            #Evaluate fitness of the newly generated particle after diversity maintenance\n            distances = self.crowding_distance()\n            worst_particle_index = np.argmin(distances)\n            self.fitness[worst_particle_index] = func(self.pop[worst_particle_index])\n            \n            if self.fitness[worst_particle_index] < self.f_best_global:\n                self.f_best_global = self.fitness[worst_particle_index]\n                self.x_best_global = self.pop[worst_particle_index].copy()\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using the Rank Diversity PSO algorithm.\n\n        Args:\n            func: The black-box optimization function.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        self.initialize_population(func)\n        eval_count = self.pop_size\n\n        while eval_count < self.budget:\n            self.w = 0.7 - 0.5 * (eval_count / self.budget)\n            self.pso_update(func)\n            eval_count += self.pop_size \n            \n            if eval_count > self.budget:\n                break\n\n        return self.f_best_global, self.x_best_global", "configspace": "", "generation": 3, "feedback": "An exception occured: TypeError: only size-1 arrays can be converted to Python scalars\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 144, in evaluate\n    algorithm(f_new)\n  File \"<string>\", line 140, in __call__\n  File \"<string>\", line 114, in pso_update\n  File \"<string>\", line 87, in diversity_maintenance\n  File \"<string>\", line 60, in crowding_distance\nValueError: setting an array element with a sequence.\n.", "error": "", "parent_ids": ["4f2d65a5-fef4-493f-92b4-f3047b5dd698"], "operator": null, "metadata": {}}
{"id": "ed1a4981-4772-4744-9018-47674290ec03", "fitness": -Infinity, "name": "SOMPSO", "description": "A PSO variant with a Lvy flight-based exploration strategy and a self-organizing map (SOM) for population diversity maintenance.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass SOMPSO:\n    def __init__(self, budget=10000, dim=10, pop_size=20, w=0.7, c1=1.5, c2=1.5, levy_scale=0.01, som_grid_size=5, som_learning_rate=0.1, som_sigma=1.0):\n        \"\"\"\n        Initialize the SOM-PSO algorithm.\n\n        Args:\n            budget (int): Total number of function evaluations.\n            dim (int): Dimensionality of the problem.\n            pop_size (int): Population size.\n            w (float): Inertia weight for PSO.\n            c1 (float): Cognitive coefficient for PSO.\n            c2 (float): Social coefficient for PSO.\n            levy_scale (float): Scale parameter for the Lvy flight.\n            som_grid_size (int): Size of the SOM grid (som_grid_size x som_grid_size).\n            som_learning_rate (float): Learning rate for the SOM.\n            som_sigma (float): Initial sigma for the SOM neighborhood function.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w = w\n        self.c1 = c1\n        self.c2 = c2\n        self.levy_scale = levy_scale\n        self.som_grid_size = som_grid_size\n        self.som_learning_rate = som_learning_rate\n        self.som_sigma = som_sigma\n        self.pop = None\n        self.fitness = None\n        self.velocities = None\n        self.x_best_global = None\n        self.f_best_global = np.inf\n        self.lb = None\n        self.ub = None\n        self.v_max = None\n        self.som = None  # Self-Organizing Map\n\n    def initialize_population(self, func):\n        \"\"\"\n        Initialize the population within the bounds of the function.\n\n        Args:\n            func: The black-box optimization function.\n        \"\"\"\n        self.lb = func.bounds.lb\n        self.ub = func.bounds.ub\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.velocities = np.zeros((self.pop_size, self.dim))\n        self.x_best_global = self.pop[np.argmin(self.fitness)].copy()\n        self.f_best_global = np.min(self.fitness)\n        self.v_max = 0.2 * (self.ub - self.lb)\n        self.initialize_som()\n\n\n    def initialize_som(self):\n        \"\"\"\n        Initialize the Self-Organizing Map.\n        \"\"\"\n        self.som = np.random.uniform(self.lb, self.ub, size=(self.som_grid_size, self.som_grid_size, self.dim))\n\n    def levy_flight(self):\n        \"\"\"\n        Generate a Lvy flight step.\n        \"\"\"\n        return self.levy_scale * levy.rvs(0.5, loc=0, scale=1, size=self.dim)\n\n    def find_closest_neuron(self, x):\n        \"\"\"\n        Find the closest neuron in the SOM to a given particle.\n\n        Args:\n            x (numpy.ndarray): Particle's position.\n\n        Returns:\n            tuple: Coordinates of the closest neuron (row, column).\n        \"\"\"\n        distances = np.sum((self.som - x)**2, axis=2)\n        row, col = np.unravel_index(np.argmin(distances), distances.shape)\n        return row, col\n    \n    def update_som(self, x, row, col):\n        \"\"\"\n        Update the SOM based on the particle's position.\n\n        Args:\n            x (numpy.ndarray): Particle's position.\n            row (int): Row index of the closest neuron.\n            col (int): Column index of the closest neuron.\n        \"\"\"\n        for i in range(self.som_grid_size):\n            for j in range(self.som_grid_size):\n                distance = np.sqrt((i - row)**2 + (j - col)**2)\n                influence = np.exp(-distance**2 / (2 * self.som_sigma**2))\n                self.som[i, j] += self.som_learning_rate * influence * (x - self.som[i, j])\n\n    def pso_update(self, func):\n        \"\"\"\n        Update the population using PSO principles with Lvy flight and SOM integration.\n\n        Args:\n            func: The black-box optimization function.\n        \"\"\"\n        for i in range(self.pop_size):\n            # Update velocities\n            r1 = np.random.rand(self.dim)\n            r2 = np.random.rand(self.dim)\n            self.velocities[i] = self.w * self.velocities[i] + \\\n                                 self.c1 * r1 * (self.pop[i] - self.pop[i]) + \\\n                                 self.c2 * r2 * (self.x_best_global - self.pop[i])\n\n            # Velocity clamping\n            self.velocities[i] = np.clip(self.velocities[i], -self.v_max, self.v_max)\n\n            # Update positions\n            new_position = self.pop[i] + self.velocities[i] + self.levy_flight()\n\n            # Boundary handling (clip to bounds)\n            new_position = np.clip(new_position, self.lb, self.ub)\n\n            new_fitness = func(new_position)  # Evaluate new position\n\n            if new_fitness < self.fitness[i]:\n                self.pop[i] = new_position\n                self.fitness[i] = new_fitness\n                if new_fitness < self.f_best_global:\n                    self.f_best_global = new_fitness\n                    self.x_best_global = self.pop[i].copy()\n\n            # SOM update\n            row, col = self.find_closest_neuron(self.pop[i])\n            self.update_som(self.pop[i], row, col)\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using the SOM-PSO algorithm.\n\n        Args:\n            func: The black-box optimization function.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        self.initialize_population(func)\n        eval_count = self.pop_size  # Account for initial population evaluation\n\n        while eval_count < self.budget:\n            # Adaptive parameter adjustment (example: linear decrease of inertia weight)\n            self.w = 0.7 - 0.5 * (eval_count / self.budget)\n            self.som_learning_rate = 0.1 * (1 - (eval_count / self.budget)) # Reduce SOM learning rate over time\n            self.som_sigma = max(0.1, 1.0 * (1 - (eval_count / self.budget))) # reduce SOM sigma over time\n            \n            self.pso_update(func)\n            eval_count += self.pop_size  # Account for population evaluation\n\n            if eval_count > self.budget:\n                break\n\n        return self.f_best_global, self.x_best_global", "configspace": "", "generation": 3, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 156, in __call__\n  File \"<string>\", line 119, in pso_update\n  File \"<string>\", line 69, in levy_flight\nNameError: name 'levy' is not defined\n.", "error": "", "parent_ids": ["4f2d65a5-fef4-493f-92b4-f3047b5dd698"], "operator": null, "metadata": {}}
{"id": "c0496c6a-4259-4c28-9d4c-5e8bdf7a80c9", "fitness": 0.0, "name": "SOSpDE", "description": "A self-organizing speciation-based differential evolution with adaptive operator selection and local search enhancement, promoting diversity and exploitation.", "code": "import numpy as np\n\nclass SOSpDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, num_species=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.num_species = num_species\n        self.species = [[] for _ in range(self.num_species)]\n        self.centroids = None\n        self.mutation_strategies = [self.mutation_rand1, self.mutation_current_to_best, self.mutation_best2]\n        self.mutation_probs = np.ones(len(self.mutation_strategies)) / len(self.mutation_strategies)\n        self.success_counts = np.zeros(len(self.mutation_strategies))\n        self.strategy_usage = np.zeros(len(self.mutation_strategies))\n        self.learning_rate = 0.1\n        self.F = 0.5\n        self.CR = 0.7\n        self.local_search_prob = 0.1\n\n    def initialize_species(self, population):\n        distances = np.linalg.norm(population[:, None, :] - self.centroids[None, :, :], axis=2)\n        species_ids = np.argmin(distances, axis=1)\n        for i in range(self.pop_size):\n            self.species[species_ids[i]].append(i)\n\n    def update_centroids(self, population):\n        for i in range(self.num_species):\n            if self.species[i]:\n                self.centroids[i] = np.mean(population[self.species[i]], axis=0)\n            else:\n                self.centroids[i] = np.random.uniform(-5, 5, size=self.dim) #reinitialize if species is empty\n\n    def mutation_rand1(self, population, i):\n        indices = np.random.choice(self.pop_size, 3, replace=False)\n        x_r1, x_r2, x_r3 = population[indices]\n        return population[i] + self.F * (x_r1 - x_r2) + self.F * (x_r3 - population[i])\n\n    def mutation_current_to_best(self, population, i, best_x):\n        indices = np.random.choice(self.pop_size, 2, replace=False)\n        x_r1, x_r2 = population[indices]\n        return population[i] + self.F * (best_x - population[i]) + self.F * (x_r1 - x_r2)\n\n    def mutation_best2(self, population, i, best_x):\n        indices = np.random.choice(self.pop_size, 2, replace=False)\n        x_r1, x_r2 = population[indices]\n        return best_x + self.F * (population[i] - x_r1) + self.F * (x_r2 - population[i])\n\n    def local_search(self, x, func):\n        # Simple random local search around x\n        x_new = x + np.random.normal(0, 0.1, size=self.dim)\n        x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n        f_new = func(x_new)\n        self.budget -= 1\n        if f_new < func(x):\n            return x_new, f_new\n        else:\n            return x, func(x)\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n        best_x = self.x_opt.copy()\n\n        # Initialize centroids randomly\n        self.centroids = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.num_species, self.dim))\n        self.initialize_species(population)\n\n        # Evolution loop\n        while self.budget > 0:\n            # Clear species\n            self.species = [[] for _ in range(self.num_species)]\n            \n            for i in range(self.pop_size):\n                # Strategy selection\n                strategy_index = np.random.choice(len(self.mutation_strategies), p=self.mutation_probs)\n                self.strategy_usage[strategy_index] += 1\n                mutation_strategy = self.mutation_strategies[strategy_index]\n\n                # Mutation\n                if mutation_strategy in [self.mutation_current_to_best, self.mutation_best2]:\n                    v = mutation_strategy(population, i, best_x)\n                else:\n                    v = mutation_strategy(population, i)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.copy(population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        u[j] = v[j]\n\n                # Evaluation\n                f_u = func(u)\n                self.budget -= 1\n\n                if f_u < fitness[i]:\n                    # Replacement\n                    fitness[i] = f_u\n                    population[i] = u\n\n                    # Local Search\n                    if np.random.rand() < self.local_search_prob and self.budget > 0:\n                        population[i], fitness[i] = self.local_search(population[i], func)\n                        if fitness[i] < f_u:\n                            f_u = fitness[i]\n                    \n\n                    # Update best solution\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u.copy()\n                        best_x = self.x_opt.copy()\n                    self.success_counts[strategy_index] += 1\n\n            # Update mutation probabilities\n            total_usage = np.sum(self.strategy_usage)\n            if total_usage > 0:\n                success_rates = self.success_counts / self.strategy_usage\n                for k in range(len(self.mutation_strategies)):\n                    self.mutation_probs[k] += self.learning_rate * (success_rates[k] - self.mutation_probs[k])\n                self.mutation_probs = np.maximum(self.mutation_probs, 0.01)\n                self.mutation_probs /= np.sum(self.mutation_probs)\n            self.success_counts[:] = 0\n            self.strategy_usage[:] = 0\n\n            self.update_centroids(population)\n            self.initialize_species(population)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm SOSpDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b506287c-816e-41a8-bcbb-422cb8c10696"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "321b327f-3ffd-4d07-8b34-856c1454d5f1", "fitness": 0.0, "name": "HybridPSO_CMAES", "description": "A hybrid algorithm combining the global search capabilities of PSO with the local refinement of CMA-ES, adaptively switching between them based on landscape features.", "code": "import numpy as np\n\nclass HybridPSO_CMAES:\n    def __init__(self, budget=10000, dim=10, pso_fraction=0.5, cmaes_fraction=0.5, initial_inertia=0.7, initial_cognitive=1.5, initial_social=1.5):\n        self.budget = budget\n        self.dim = dim\n        self.pso_fraction = pso_fraction\n        self.cmaes_fraction = cmaes_fraction\n        self.inertia = initial_inertia\n        self.cognitive = initial_cognitive\n        self.social = initial_social\n        self.particles = np.random.uniform(-5, 5, size=(dim * 5, dim))\n        self.velocities = np.random.uniform(-1, 1, size=(dim * 5, dim))\n        self.personal_best_positions = self.particles.copy()\n        self.personal_best_values = np.full(dim * 5, np.inf)\n        self.global_best_position = None\n        self.global_best_value = np.inf\n\n        # CMA-ES parameters\n        self.mu = int(dim * 0.25)\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cc = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.mean = np.zeros(dim)\n        self.sigma = 0.5\n        self.pc = np.zeros(dim)\n        self.ps = np.zeros(dim)\n        self.C = np.eye(dim)\n        self.B = np.eye(dim)\n        self.D = np.ones(dim)\n        self.C_eigen_age = 0\n        self.pso_phase = True # Start with PSO\n\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n    def pso_step(self, func):\n        for i in range(self.particles.shape[0]):\n            f = func(self.particles[i])\n            if f < self.personal_best_values[i]:\n                self.personal_best_values[i] = f\n                self.personal_best_positions[i] = self.particles[i].copy()\n\n            if f < self.global_best_value:\n                self.global_best_value = f\n                self.global_best_position = self.particles[i].copy()\n                self.f_opt = f\n                self.x_opt = self.particles[i]\n\n        for i in range(self.particles.shape[0]):\n            r1 = np.random.rand(self.dim)\n            r2 = np.random.rand(self.dim)\n            self.velocities[i] = (self.inertia * self.velocities[i] +\n                                  self.cognitive * r1 * (self.personal_best_positions[i] - self.particles[i]) +\n                                  self.social * r2 * (self.global_best_position - self.particles[i]))\n            self.particles[i] = np.clip(self.particles[i] + self.velocities[i], func.bounds.lb, func.bounds.ub)\n\n    def cmaes_sample_population(self, popsize, func):\n        z = np.random.normal(0, 1, size=(popsize, self.dim))\n        x = self.mean + self.sigma * (self.B @ (self.D * z).T).T\n        x = np.clip(x, func.bounds.lb, func.bounds.ub)\n        f = np.array([func(xi) for xi in x])\n        return x, f, z\n\n    def cmaes_update_distribution(self, x, f, z, popsize, func):\n        idx = np.argsort(f)\n        x = x[idx]\n        z = z[idx]\n        x_mu = x[:self.mu]\n        z_mu = z[:self.mu]\n\n        self.mean = np.sum(self.weights.reshape(-1, 1) * x_mu, axis=0)\n\n        zmean = np.sum(self.weights.reshape(-1, 1) * z_mu, axis=0)\n        self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * (self.B @ zmean)\n        hsig = np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * (self.budget // popsize))) < 1.4 + 2 / (self.dim + 1)\n        self.pc = (1 - self.cc) * self.pc + hsig * np.sqrt(self.cc * (2 - self.cc) * self.mueff) * (self.mean - self.mean) / self.sigma\n\n        artmp = (1 / self.sigma) * (x_mu - self.mean).T\n        self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * (self.pc.reshape(-1, 1) @ self.pc.reshape(1, -1) + (1-hsig) * self.cc * (2-self.cc) * self.C) + self.cmu * artmp @ np.diag(self.weights) @ artmp.T\n\n        self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * (self.budget // popsize))) / np.sqrt(self.dim) - 1))\n\n    def cmaes_step(self, func):\n        popsize = 4 + int(3 * np.log(self.dim))\n        x, f, z = self.cmaes_sample_population(popsize, func)\n\n        for i in range(popsize):\n            if f[i] < self.f_opt:\n                self.f_opt = f[i]\n                self.x_opt = x[i]\n\n        self.cmaes_update_distribution(x, f, z, popsize, func)\n        self.C_eigen_age += 1\n\n        if self.C_eigen_age > self.budget // (10 * popsize):\n            self.C_eigen_age = 0\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            try:\n                self.D, self.B = np.linalg.eigh(self.C)\n                self.D = np.sqrt(self.D)\n                self.D[self.D < 1e-10] = 1e-10\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)\n                self.B = np.eye(self.dim)\n                self.D = np.ones(self.dim)\n\n    def __call__(self, func):\n        evals = 0\n        switch_interval = self.budget // 20 # Switch every 5% of the budget\n        last_switch = 0\n\n        while evals < self.budget:\n            if self.pso_phase:\n                for _ in range(min(switch_interval, self.budget - evals)):\n                    self.pso_step(func)\n                    evals += self.particles.shape[0]\n            else:\n                popsize = 4 + int(3 * np.log(self.dim))\n                num_cmaes_evals = 0\n                while num_cmaes_evals < min(switch_interval, self.budget - evals):\n                    self.cmaes_step(func)\n                    num_cmaes_evals += popsize\n                    evals+=popsize\n\n\n            if evals - last_switch >= switch_interval:\n                 # Simple switching mechanism: alternate between PSO and CMA-ES\n                self.pso_phase = not self.pso_phase\n                last_switch = evals\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm HybridPSO_CMAES scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a2c37750-5f11-443f-9d8c-527210118117"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "2e7164a0-8c11-45f7-95e3-2357dfc15d27", "fitness": -Infinity, "name": "OrthogonalCMAES", "description": "A CMA-ES variant that uses orthogonal sampling to improve exploration and exploitation, and adjusts the covariance matrix adaptation rate based on the success of orthogonal directions.", "code": "import numpy as np\n\nclass OrthogonalCMAES:\n    def __init__(self, budget=10000, dim=10, mu_factor=0.25, initial_sigma=0.5, orthogonal_trials=5):\n        self.budget = budget\n        self.dim = dim\n        self.mu = int(dim * mu_factor)  # Number of parents\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cc = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.mean = np.zeros(dim)\n        self.sigma = initial_sigma\n        self.pc = np.zeros(dim)\n        self.ps = np.zeros(dim)\n        self.C = np.eye(dim)\n        self.B = np.eye(dim)\n        self.D = np.ones(dim)\n        self.C_eigen_age = 0\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.orthogonal_trials = orthogonal_trials\n\n    def sample_population(self, popsize, func):\n        z = np.random.normal(0, 1, size=(popsize, self.dim))\n        x = self.mean + self.sigma * (self.B @ (self.D * z).T).T\n        x = np.clip(x, func.bounds.lb, func.bounds.ub)  # Boundary handling\n        f = np.array([func(xi) for xi in x])\n        return x, f, z\n\n    def generate_orthogonal_sample(self, x, func):\n        \"\"\"Generates an orthogonal sample around a given point x.\"\"\"\n        orthogonal_points = []\n        for _ in range(self.orthogonal_trials):\n            direction = np.random.normal(0, 1, self.dim)\n            direction /= np.linalg.norm(direction)  # Normalize\n            step_size = np.random.uniform(-self.sigma, self.sigma)\n            x_new = x + step_size * direction\n            x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n            orthogonal_points.append((x_new, func(x_new)))\n        return orthogonal_points\n\n    def update_distribution(self, x, f, z, popsize, func):\n        idx = np.argsort(f)\n        x = x[idx]\n        z = z[idx]\n        x_mu = x[:self.mu]\n        z_mu = z[:self.mu]\n\n        self.mean = np.sum(self.weights.reshape(-1, 1) * x_mu, axis=0)\n\n        zmean = np.sum(self.weights.reshape(-1, 1) * z_mu, axis=0)\n        self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * (self.B @ zmean)\n        hsig = np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * (self.budget // popsize))) < 1.4 + 2 / (self.dim + 1)\n        self.pc = (1 - self.cc) * self.pc + hsig * np.sqrt(self.cc * (2 - self.cc) * self.mueff) * (self.mean - self.mean) / self.sigma\n\n        artmp = (1 / self.sigma) * (x_mu - self.mean).T\n        self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * (self.pc.reshape(-1, 1) @ self.pc.reshape(1, -1) + (1-hsig) * self.cc * (2-self.cc) * self.C) + self.cmu * artmp @ np.diag(self.weights) @ artmp.T\n\n        self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * (self.budget // popsize))) / np.sqrt(self.dim) - 1))\n        \n        # Orthogonal sampling influence on adaptation\n        success_rate = 0\n        for i in range(self.mu):\n            orthogonal_samples = self.generate_orthogonal_sample(x_mu[i], func)\n            best_orthogonal_f = min(sample[1] for sample in orthogonal_samples)\n            if best_orthogonal_f < f[i]:\n                success_rate += 1\n\n        adaptation_rate_factor = 1.0 + 0.5 * (success_rate / self.mu - 0.5)  # Adjust factor\n\n        self.c1 *= adaptation_rate_factor\n        self.cmu *= adaptation_rate_factor\n\n\n    def __call__(self, func):\n        popsize = 4 + int(3 * np.log(self.dim))\n        evals = 0\n\n        while evals < self.budget:\n            x, f, z = self.sample_population(popsize, func)\n            evals += popsize\n\n            for i in range(popsize):\n                if f[i] < self.f_opt:\n                    self.f_opt = f[i]\n                    self.x_opt = x[i]\n\n            self.update_distribution(x, f, z, popsize, func)\n            self.C_eigen_age += 1\n\n            if self.C_eigen_age > self.budget // (10 * popsize): # Re-compute eigenvalue decomposition after a while\n                self.C_eigen_age = 0\n                self.C = np.triu(self.C) + np.triu(self.C, 1).T\n                try:\n                    self.D, self.B = np.linalg.eigh(self.C)\n                    self.D = np.sqrt(self.D)\n                    self.D[self.D < 1e-10] = 1e-10  # Avoid tiny values\n                except np.linalg.LinAlgError:\n                    # Handle non-positive definite matrix\n                    self.C = np.eye(self.dim)  # Reset covariance matrix\n                    self.B = np.eye(self.dim)\n                    self.D = np.ones(self.dim)\n                    \n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 93, in __call__\n  File \"<string>\", line 74, in update_distribution\nZeroDivisionError: division by zero\n.", "error": "", "parent_ids": ["a2c37750-5f11-443f-9d8c-527210118117"], "operator": null, "metadata": {}}
{"id": "41931f50-8508-4bd4-b81c-fb1b75937629", "fitness": 0.0, "name": "CMA_NM_SA", "description": "An optimization algorithm combining aspects of CMA-ES, Nelder-Mead, and Simulated Annealing to adaptively adjust exploration and exploitation based on landscape features.", "code": "import numpy as np\n\nclass CMA_NM_SA:\n    def __init__(self, budget=10000, dim=10, initial_sigma=0.5, mu_factor=0.25, sa_initial_temp=1.0, sa_cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.initial_sigma = initial_sigma\n        self.mu = int(dim * mu_factor)\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cc = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.mean = np.zeros(dim)\n        self.sigma = initial_sigma\n        self.pc = np.zeros(dim)\n        self.ps = np.zeros(dim)\n        self.C = np.eye(dim)\n        self.B = np.eye(dim)\n        self.D = np.ones(dim)\n        self.C_eigen_age = 0\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n        # Nelder-Mead parameters\n        self.simplex = None\n        self.nm_alpha = 1.0  # Reflection\n        self.nm_beta = 0.5   # Contraction\n        self.nm_gamma = 2.0   # Expansion\n\n        # Simulated Annealing parameters\n        self.sa_temp = sa_initial_temp\n        self.sa_cooling_rate = sa_cooling_rate\n\n    def sample_population(self, popsize, func):\n        z = np.random.normal(0, 1, size=(popsize, self.dim))\n        x = self.mean + self.sigma * (self.B @ (self.D * z).T).T\n        x = np.clip(x, func.bounds.lb, func.bounds.ub)\n        f = np.array([func(xi) for xi in x])\n        return x, f, z\n\n    def update_distribution(self, x, f, z, popsize, func):\n        idx = np.argsort(f)\n        x = x[idx]\n        z = z[idx]\n        x_mu = x[:self.mu]\n        z_mu = z[:self.mu]\n\n        self.mean = np.sum(self.weights.reshape(-1, 1) * x_mu, axis=0)\n\n        zmean = np.sum(self.weights.reshape(-1, 1) * z_mu, axis=0)\n        self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * (self.B @ zmean)\n        hsig = np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * (self.budget // popsize))) < 1.4 + 2 / (self.dim + 1)\n        self.pc = (1 - self.cc) * self.pc + hsig * np.sqrt(self.cc * (2 - self.cc) * self.mueff) * (self.mean - self.mean) / self.sigma\n\n        artmp = (1 / self.sigma) * (x_mu - self.mean).T\n        self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * (self.pc.reshape(-1, 1) @ self.pc.reshape(1, -1) + (1-hsig) * self.cc * (2-self.cc) * self.C) + self.cmu * artmp @ np.diag(self.weights) @ artmp.T\n\n        self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * (self.budget // popsize))) / np.sqrt(self.dim) - 1))\n    \n    def initialize_simplex(self, x0, func):\n        self.simplex = np.zeros((self.dim + 1, self.dim))\n        self.simplex[0] = x0\n        for i in range(1, self.dim + 1):\n            self.simplex[i] = x0.copy()\n            self.simplex[i, i - 1] += 0.1  # Initial step size\n            self.simplex[i] = np.clip(self.simplex[i], func.bounds.lb, func.bounds.ub)\n        return np.array([func(xi) for xi in self.simplex])\n\n\n    def nelder_mead_step(self, func):\n        f = np.array([func(xi) for xi in self.simplex])\n        idx = np.argsort(f)\n        self.simplex = self.simplex[idx]\n        f = f[idx]\n        \n        centroid = np.mean(self.simplex[:-1], axis=0)\n\n        # Reflection\n        x_r = centroid + self.nm_alpha * (centroid - self.simplex[-1])\n        x_r = np.clip(x_r, func.bounds.lb, func.bounds.ub)\n        f_r = func(x_r)\n\n        if f_r < f[0]:\n            # Expansion\n            x_e = centroid + self.nm_gamma * (x_r - centroid)\n            x_e = np.clip(x_e, func.bounds.lb, func.bounds.ub)\n            f_e = func(x_e)\n            if f_e < f_r:\n                self.simplex[-1] = x_e\n            else:\n                self.simplex[-1] = x_r\n        elif f_r < f[-2]:\n            self.simplex[-1] = x_r\n        else:\n            # Contraction\n            x_c = centroid + self.nm_beta * (self.simplex[-1] - centroid)\n            x_c = np.clip(x_c, func.bounds.lb, func.bounds.ub)\n            f_c = func(x_c)\n            if f_c < f[-1]:\n                self.simplex[-1] = x_c\n            else:\n                # Shrink\n                for i in range(1, self.dim + 1):\n                    self.simplex[i] = self.simplex[0] + 0.5 * (self.simplex[i] - self.simplex[0])\n                    self.simplex[i] = np.clip(self.simplex[i], func.bounds.lb, func.bounds.ub)\n        return np.array([func(xi) for xi in self.simplex])\n\n    def simulated_annealing_step(self, x_current, f_current, func):\n        x_new = x_current + np.random.normal(0, self.sigma, size=self.dim)  # Use CMA-ES sigma\n        x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n        f_new = func(x_new)\n\n        if f_new < f_current:\n            return x_new, f_new\n        else:\n            acceptance_probability = np.exp((f_current - f_new) / self.sa_temp)\n            if np.random.rand() < acceptance_probability:\n                return x_new, f_new\n            else:\n                return x_current, f_current\n\n    def __call__(self, func):\n        popsize = 4 + int(3 * np.log(self.dim))\n        evals = 0\n\n        # Initialization with CMA-ES\n        self.mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim) # Initialize mean\n\n        # Initialize Simplex\n        self.simplex = np.zeros((self.dim + 1, self.dim))\n        self.simplex[0] = self.mean  # Start Nelder-Mead near CMA-ES mean\n        for i in range(1, self.dim + 1):\n            self.simplex[i] = self.mean.copy()\n            self.simplex[i, i - 1] += 0.1  # Initial step size\n            self.simplex[i] = np.clip(self.simplex[i], func.bounds.lb, func.bounds.ub)\n        f_simplex = np.array([func(xi) for xi in self.simplex]) # Evaluate simplex\n        evals += (self.dim + 1)\n\n        while evals < self.budget:\n            # CMA-ES Step\n            x, f, z = self.sample_population(popsize, func)\n            evals += popsize\n\n            for i in range(popsize):\n                if f[i] < self.f_opt:\n                    self.f_opt = f[i]\n                    self.x_opt = x[i]\n\n            self.update_distribution(x, f, z, popsize, func)\n            self.C_eigen_age += 1\n\n            if self.C_eigen_age > self.budget // (10 * popsize):\n                self.C_eigen_age = 0\n                self.C = np.triu(self.C) + np.triu(self.C, 1).T\n                try:\n                    self.D, self.B = np.linalg.eigh(self.C)\n                    self.D = np.sqrt(self.D)\n                    self.D[self.D < 1e-10] = 1e-10\n                except np.linalg.LinAlgError:\n                    self.C = np.eye(self.dim)\n                    self.B = np.eye(self.dim)\n                    self.D = np.ones(self.dim)\n\n            # Nelder-Mead Step\n            f_simplex = self.nelder_mead_step(func)  # Take a Nelder-Mead step\n            evals += (self.dim + 1) #each nm step takes dim+1 evals\n           \n            #Simulated Annealing\n            best_idx = np.argmin(f_simplex)\n            x_current = self.simplex[best_idx]\n            f_current = f_simplex[best_idx]\n            x_new, f_new = self.simulated_annealing_step(x_current, f_current, func)\n            evals +=1\n            if f_new < f_current:\n                self.simplex[best_idx] = x_new\n                f_simplex[best_idx] = f_new\n            if f_new < self.f_opt:\n                self.f_opt = f_new\n                self.x_opt = x_new\n\n            self.sa_temp *= self.sa_cooling_rate # Cool SA temperature\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm CMA_NM_SA scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a2c37750-5f11-443f-9d8c-527210118117"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "69139b67-a214-4358-b8a3-98f43301af5c", "fitness": 0.32030343486980867, "name": "SelfOrganizingPSO", "description": "A PSO variant with a neighborhood-based mutation and self-organizing scouts to enhance exploration and exploitation.", "code": "import numpy as np\n\nclass SelfOrganizingPSO:\n    def __init__(self, budget=10000, dim=10, pop_size=20, w=0.7, c1=1.5, c2=1.5, neighborhood_size=3, scout_rate=0.1):\n        \"\"\"\n        Initialize the Self-Organizing PSO algorithm.\n\n        Args:\n            budget (int): Total number of function evaluations.\n            dim (int): Dimensionality of the problem.\n            pop_size (int): Population size.\n            w (float): Inertia weight for PSO.\n            c1 (float): Cognitive coefficient for PSO.\n            c2 (float): Social coefficient for PSO.\n            neighborhood_size (int): Size of the neighborhood for local best.\n            scout_rate (float): Probability of a particle becoming a scout.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w = w\n        self.c1 = c1\n        self.c2 = c2\n        self.neighborhood_size = neighborhood_size\n        self.scout_rate = scout_rate\n        self.pop = None\n        self.fitness = None\n        self.velocities = None\n        self.x_best_global = None\n        self.f_best_global = np.inf\n        self.lb = None\n        self.ub = None\n        self.v_max = None\n        self.x_best_local = None  # Local best for each particle\n\n    def initialize_population(self, func):\n        \"\"\"\n        Initialize the population within the bounds of the function.\n\n        Args:\n            func: The black-box optimization function.\n        \"\"\"\n        self.lb = func.bounds.lb\n        self.ub = func.bounds.ub\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.velocities = np.zeros((self.pop_size, self.dim))\n        self.x_best_global = self.pop[np.argmin(self.fitness)].copy()\n        self.f_best_global = np.min(self.fitness)\n        self.v_max = 0.2 * (self.ub - self.lb)\n        self.x_best_local = self.pop.copy()  # Initially, local best is the particle itself\n\n    def neighborhood_mutation(self, i):\n        \"\"\"\n        Apply mutation based on the neighborhood's best.\n\n        Args:\n            i (int): Index of the particle to mutate.\n        \"\"\"\n        # Find neighborhood indices\n        neighborhood_indices = [(i + j) % self.pop_size for j in range(-self.neighborhood_size // 2, self.neighborhood_size // 2 + 1)]\n\n        # Find the best particle within the neighborhood\n        neighborhood_fitness = self.fitness[neighborhood_indices]\n        best_neighbor_index = neighborhood_indices[np.argmin(neighborhood_fitness)]\n        x_best_neighbor = self.pop[best_neighbor_index]\n        \n        # Mutate the particle's velocity towards the neighborhood best\n        mutation_strength = 0.5  # Adjust as needed\n        self.velocities[i] += mutation_strength * (x_best_neighbor - self.pop[i])\n\n    def scout_movement(self, i):\n        \"\"\"\n        Replace a particle with a random scout particle.\n\n        Args:\n            i (int): Index of the particle to replace.\n        \"\"\"\n        self.pop[i] = np.random.uniform(self.lb, self.ub)\n        self.velocities[i] = np.zeros(self.dim) # Reset velocity\n    \n    def pso_update(self, func):\n        \"\"\"\n        Update the population using PSO principles with neighborhood mutation and self-organizing scouts.\n\n        Args:\n            func: The black-box optimization function.\n        \"\"\"\n        for i in range(self.pop_size):\n            # Update velocities\n            r1 = np.random.rand(self.dim)\n            r2 = np.random.rand(self.dim)\n            self.velocities[i] = self.w * self.velocities[i] + \\\n                                 self.c1 * r1 * (self.x_best_local[i] - self.pop[i]) + \\\n                                 self.c2 * r2 * (self.x_best_global - self.pop[i])\n\n            # Velocity clamping\n            self.velocities[i] = np.clip(self.velocities[i], -self.v_max, self.v_max)\n\n            # Neighborhood-based mutation\n            self.neighborhood_mutation(i)\n\n            # Update positions\n            new_position = self.pop[i] + self.velocities[i]\n\n            # Boundary handling (clip to bounds)\n            new_position = np.clip(new_position, self.lb, self.ub)\n\n            # Self-organizing scout behavior\n            if np.random.rand() < self.scout_rate:\n                self.scout_movement(i)\n                new_position = self.pop[i] # scout_movement already updates position\n\n            new_fitness = func(new_position)  # Evaluate new position\n\n            if new_fitness < self.fitness[i]:\n                self.pop[i] = new_position\n                self.fitness[i] = new_fitness\n                self.x_best_local[i] = new_position.copy() #Update local best\n                if new_fitness < self.f_best_global:\n                    self.f_best_global = new_fitness\n                    self.x_best_global = self.pop[i].copy()\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using the Self-Organizing PSO algorithm.\n\n        Args:\n            func: The black-box optimization function.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        self.initialize_population(func)\n        eval_count = self.pop_size # Account for initial population evaluation\n\n        while eval_count < self.budget:\n            # Adaptive parameter adjustment (example: linear decrease of inertia weight)\n            self.w = 0.7 - 0.5 * (eval_count / self.budget)\n            self.pso_update(func)\n            eval_count += self.pop_size # Account for population evaluation\n\n            if eval_count > self.budget:\n                break\n\n        return self.f_best_global, self.x_best_global", "configspace": "", "generation": 3, "feedback": "The algorithm SelfOrganizingPSO scored 0.320 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["4f2d65a5-fef4-493f-92b4-f3047b5dd698"], "operator": null, "metadata": {"aucs": [0.1269930209062231, 0.19491190156028304, 0.3103025595981532, 0.2803510383635214, 0.23527307396795205, 0.2574318188732305, 0.2688789130613418, 0.3067448633864597, 0.24036471072066556, 0.17182229554181072, 0.3074912319951467, 0.9971413534299035, 0.254868503084585, 0.25702565832448876, 0.6712377594124346, 0.30043426250701966, 0.24486444539535046, 0.31361860881868076, 0.18566918174261104, 0.4806434967063111]}}
{"id": "a86fe350-cee7-4c59-9fa9-d90561f984df", "fitness": 0.6254673280832094, "name": "MirroredSamplingAdaptiveDE", "description": "An adaptive DE with a mirrored sampling technique to enhance exploration and exploitation, dynamically adjusting parameters based on population diversity.", "code": "import numpy as np\n\nclass MirroredSamplingAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, mirror_rate=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Initial mutation factor\n        self.CR = CR  # Initial crossover rate\n        self.mirror_rate = mirror_rate # probability of mirroring a solution.\n        self.mutation_strategies = [self.mutation_strategy_1, self.mutation_strategy_2, self.mutation_strategy_3]\n        self.mutation_probs = np.ones(len(self.mutation_strategies)) / len(self.mutation_strategies)  # Initial probabilities for each strategy\n        self.success_counts = np.zeros(len(self.mutation_strategies))\n        self.strategy_usage = np.zeros(len(self.mutation_strategies))\n        self.learning_rate = 0.1\n        self.diversity_threshold = 0.01 # Threshold for diversity check\n        self.diversity_weight = 0.1 # weight for adjusting F and CR based on diversity.\n\n\n    def mutation_strategy_1(self, population, i):\n        # DE/rand/1\n        indices = np.random.choice(self.pop_size, 3, replace=False)\n        x_r1, x_r2, x_r3 = population[indices]\n        return population[i] + self.F * (x_r1 - population[i]) + self.F * (x_r2 - x_r3)\n\n    def mutation_strategy_2(self, population, i):\n        # DE/current-to-rand/1\n        indices = np.random.choice(self.pop_size, 3, replace=False)\n        x_r1, x_r2, x_r3 = population[indices]\n        return population[i] + self.F * (x_r1 - population[i]) + self.F * (x_r2 - x_r3)\n\n    def mutation_strategy_3(self, population, i, best_x):\n         # DE/best/1\n        indices = np.random.choice(self.pop_size, 2, replace=False)\n        x_r1, x_r2 = population[indices]\n        return best_x + self.F * (x_r1 - x_r2)\n\n    def __call__(self, func):\n        # Initialization\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n        best_x = self.x_opt.copy()\n\n        # Evolution loop\n        while self.budget > 0:\n            # Calculate population diversity\n            diversity = np.std(population)\n\n            for i in range(self.pop_size):\n                # Strategy selection\n                strategy_index = np.random.choice(len(self.mutation_strategies), p=self.mutation_probs)\n                self.strategy_usage[strategy_index] += 1\n                mutation_strategy = self.mutation_strategies[strategy_index]\n                \n                # Mutation\n                if mutation_strategy == self.mutation_strategy_3:\n                    v = mutation_strategy(population, i, best_x)\n                else:\n                    v = mutation_strategy(population, i)\n                v = np.clip(v, lb, ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.copy(population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        u[j] = v[j]\n\n                # Mirrored sampling\n                if np.random.rand() < self.mirror_rate:\n                    mirror_point = np.random.uniform(lb, ub, self.dim)  # Generate a random \"midpoint\" for mirroring.\n                    u = 2 * mirror_point - u # mirror u around mirror_point\n\n                    u = np.clip(u, lb, ub) # Keep solution within bounds\n\n                # Evaluation\n                f_u = func(u)\n                self.budget -= 1\n\n                if f_u < fitness[i]:\n                    # Replacement\n                    fitness[i] = f_u\n                    population[i] = u\n\n                    # Update best solution\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u.copy()\n                        best_x = self.x_opt.copy()\n\n                    self.success_counts[strategy_index] += 1\n\n            # Update mutation probabilities\n            total_usage = np.sum(self.strategy_usage)\n            if total_usage > 0:\n                success_rates = self.success_counts / self.strategy_usage\n                for k in range(len(self.mutation_strategies)):\n                    self.mutation_probs[k] += self.learning_rate * (success_rates[k] - self.mutation_probs[k])\n                self.mutation_probs = np.maximum(self.mutation_probs, 0.01)  # Avoid zero probabilities\n                self.mutation_probs /= np.sum(self.mutation_probs)\n\n            self.success_counts[:] = 0\n            self.strategy_usage[:] = 0\n\n             # Dynamic parameter adaptation based on diversity\n            if diversity < self.diversity_threshold:\n                self.F *= (1 + self.diversity_weight)  # Increase F to enhance exploration\n                self.CR *= (1 - self.diversity_weight)  # Decrease CR to focus exploitation\n            else:\n                self.F *= (1 - self.diversity_weight/2)  # Reduce F to exploit better\n                self.CR *= (1 + self.diversity_weight/2)   # Increase CR for more exploration\n\n            self.F = np.clip(self.F, 0.1, 1.0) # bound F\n            self.CR = np.clip(self.CR, 0.1, 0.9) # bound CR\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm MirroredSamplingAdaptiveDE scored 0.625 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b506287c-816e-41a8-bcbb-422cb8c10696"], "operator": null, "metadata": {"aucs": [0.23667527666578725, 0.49046484650092614, 0.6307682124030858, 0.934553434298648, 0.8651285678824728, 0.7443073090628343, 0.3867213692015763, 0.598200792138463, 0.8930672906836417, 0.6371237255060394, 0.9177993442656769, 0.9923977541203124, 0.31914592722114155, 0.38632254710285585, 0.937763121451525, 0.47790221929169885, 0.29316907298126127, 0.9349861472767872, 0.3124484458266815, 0.520401157782773]}}
{"id": "6e6be02e-b0be-46fd-bd18-7c535f64ca6f", "fitness": 0.0, "name": "LevyFlightDE", "description": "A DE algorithm employing a Lvy flight-based mutation operator with a dynamically adjusted step size and a restart mechanism based on stagnation detection.", "code": "import numpy as np\n\nclass LevyFlightDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, levy_exponent=1.5, stagnation_threshold=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.levy_exponent = levy_exponent  # Exponent for Lvy flight\n        self.stagnation_threshold = stagnation_threshold # Number of iterations without improvement to trigger restart.\n        self.best_fitness_history = []\n\n    def levy_flight(self, size):\n        # Generate Lvy flight steps\n        num = np.random.randn(size) * np.sqrt(self.sigma(self.levy_exponent))\n        den = np.power(np.abs(np.random.randn(size)), (1/self.levy_exponent))\n        steps = num / den\n        return steps\n\n    def sigma(self, beta):\n        num = np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2)\n        den = np.math.gamma((1 + beta) / 2) * beta * np.power(2, (beta - 1) / 2)\n        sigma = np.power((num / den), (1 / beta))\n        return sigma\n\n    def __call__(self, func):\n        # Initialization\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        self.best_fitness_history.append(self.f_opt)\n\n        # Evolution loop\n        iteration = 0\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation using Lvy flight\n                levy_steps = self.levy_flight(self.dim)\n                mutated_vector = population[i] + self.F * levy_steps * (self.x_opt - population[i]) # biased towards best solution, combined with Levy flight\n\n                mutated_vector = np.clip(mutated_vector, lb, ub) # keep within bounds\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                trial_vector = np.copy(population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        trial_vector[j] = mutated_vector[j]\n\n                # Evaluation\n                f_trial = func(trial_vector)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    # Replacement\n                    fitness[i] = f_trial\n                    population[i] = trial_vector\n\n                    # Update best solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial_vector.copy()\n                        self.best_fitness_history.append(self.f_opt)\n                else:\n                    self.best_fitness_history.append(self.f_opt)\n\n            # Stagnation Check and Restart Mechanism\n            if iteration > self.stagnation_threshold and np.std(self.best_fitness_history[-self.stagnation_threshold:]) < 1e-6:\n                # Restart: Re-initialize a portion of the population\n                num_to_restart = int(0.2 * self.pop_size) # Restart 20% of population\n                indices_to_restart = np.random.choice(self.pop_size, num_to_restart, replace=False)\n                population[indices_to_restart] = np.random.uniform(lb, ub, size=(num_to_restart, self.dim))\n                fitness[indices_to_restart] = np.array([func(x) for x in population[indices_to_restart]])\n                self.budget -= num_to_restart # Update budget\n                \n                best_index = np.argmin(fitness) #Recalculate best solution\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n                self.best_fitness_history.append(self.f_opt)\n                \n                self.F = min(self.F * 1.2, 1.0) # slightly increase F to promote diversity after restart.\n\n\n            iteration += 1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm LevyFlightDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a86fe350-cee7-4c59-9fa9-d90561f984df"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "6e6be02e-b0be-46fd-bd18-7c535f64ca6f", "fitness": 0.0, "name": "LevyFlightDE", "description": "A DE algorithm employing a Lvy flight-based mutation operator with a dynamically adjusted step size and a restart mechanism based on stagnation detection.", "code": "import numpy as np\n\nclass LevyFlightDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, levy_exponent=1.5, stagnation_threshold=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.levy_exponent = levy_exponent  # Exponent for Lvy flight\n        self.stagnation_threshold = stagnation_threshold # Number of iterations without improvement to trigger restart.\n        self.best_fitness_history = []\n\n    def levy_flight(self, size):\n        # Generate Lvy flight steps\n        num = np.random.randn(size) * np.sqrt(self.sigma(self.levy_exponent))\n        den = np.power(np.abs(np.random.randn(size)), (1/self.levy_exponent))\n        steps = num / den\n        return steps\n\n    def sigma(self, beta):\n        num = np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2)\n        den = np.math.gamma((1 + beta) / 2) * beta * np.power(2, (beta - 1) / 2)\n        sigma = np.power((num / den), (1 / beta))\n        return sigma\n\n    def __call__(self, func):\n        # Initialization\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        self.best_fitness_history.append(self.f_opt)\n\n        # Evolution loop\n        iteration = 0\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation using Lvy flight\n                levy_steps = self.levy_flight(self.dim)\n                mutated_vector = population[i] + self.F * levy_steps * (self.x_opt - population[i]) # biased towards best solution, combined with Levy flight\n\n                mutated_vector = np.clip(mutated_vector, lb, ub) # keep within bounds\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                trial_vector = np.copy(population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        trial_vector[j] = mutated_vector[j]\n\n                # Evaluation\n                f_trial = func(trial_vector)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    # Replacement\n                    fitness[i] = f_trial\n                    population[i] = trial_vector\n\n                    # Update best solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial_vector.copy()\n                        self.best_fitness_history.append(self.f_opt)\n                else:\n                    self.best_fitness_history.append(self.f_opt)\n\n            # Stagnation Check and Restart Mechanism\n            if iteration > self.stagnation_threshold and np.std(self.best_fitness_history[-self.stagnation_threshold:]) < 1e-6:\n                # Restart: Re-initialize a portion of the population\n                num_to_restart = int(0.2 * self.pop_size) # Restart 20% of population\n                indices_to_restart = np.random.choice(self.pop_size, num_to_restart, replace=False)\n                population[indices_to_restart] = np.random.uniform(lb, ub, size=(num_to_restart, self.dim))\n                fitness[indices_to_restart] = np.array([func(x) for x in population[indices_to_restart]])\n                self.budget -= num_to_restart # Update budget\n                \n                best_index = np.argmin(fitness) #Recalculate best solution\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n                self.best_fitness_history.append(self.f_opt)\n                \n                self.F = min(self.F * 1.2, 1.0) # slightly increase F to promote diversity after restart.\n\n\n            iteration += 1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm LevyFlightDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a86fe350-cee7-4c59-9fa9-d90561f984df"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "6e6be02e-b0be-46fd-bd18-7c535f64ca6f", "fitness": 0.0, "name": "LevyFlightDE", "description": "A DE algorithm employing a Lvy flight-based mutation operator with a dynamically adjusted step size and a restart mechanism based on stagnation detection.", "code": "import numpy as np\n\nclass LevyFlightDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, levy_exponent=1.5, stagnation_threshold=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.levy_exponent = levy_exponent  # Exponent for Lvy flight\n        self.stagnation_threshold = stagnation_threshold # Number of iterations without improvement to trigger restart.\n        self.best_fitness_history = []\n\n    def levy_flight(self, size):\n        # Generate Lvy flight steps\n        num = np.random.randn(size) * np.sqrt(self.sigma(self.levy_exponent))\n        den = np.power(np.abs(np.random.randn(size)), (1/self.levy_exponent))\n        steps = num / den\n        return steps\n\n    def sigma(self, beta):\n        num = np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2)\n        den = np.math.gamma((1 + beta) / 2) * beta * np.power(2, (beta - 1) / 2)\n        sigma = np.power((num / den), (1 / beta))\n        return sigma\n\n    def __call__(self, func):\n        # Initialization\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        self.best_fitness_history.append(self.f_opt)\n\n        # Evolution loop\n        iteration = 0\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation using Lvy flight\n                levy_steps = self.levy_flight(self.dim)\n                mutated_vector = population[i] + self.F * levy_steps * (self.x_opt - population[i]) # biased towards best solution, combined with Levy flight\n\n                mutated_vector = np.clip(mutated_vector, lb, ub) # keep within bounds\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                trial_vector = np.copy(population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        trial_vector[j] = mutated_vector[j]\n\n                # Evaluation\n                f_trial = func(trial_vector)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    # Replacement\n                    fitness[i] = f_trial\n                    population[i] = trial_vector\n\n                    # Update best solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial_vector.copy()\n                        self.best_fitness_history.append(self.f_opt)\n                else:\n                    self.best_fitness_history.append(self.f_opt)\n\n            # Stagnation Check and Restart Mechanism\n            if iteration > self.stagnation_threshold and np.std(self.best_fitness_history[-self.stagnation_threshold:]) < 1e-6:\n                # Restart: Re-initialize a portion of the population\n                num_to_restart = int(0.2 * self.pop_size) # Restart 20% of population\n                indices_to_restart = np.random.choice(self.pop_size, num_to_restart, replace=False)\n                population[indices_to_restart] = np.random.uniform(lb, ub, size=(num_to_restart, self.dim))\n                fitness[indices_to_restart] = np.array([func(x) for x in population[indices_to_restart]])\n                self.budget -= num_to_restart # Update budget\n                \n                best_index = np.argmin(fitness) #Recalculate best solution\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n                self.best_fitness_history.append(self.f_opt)\n                \n                self.F = min(self.F * 1.2, 1.0) # slightly increase F to promote diversity after restart.\n\n\n            iteration += 1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm LevyFlightDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a86fe350-cee7-4c59-9fa9-d90561f984df"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "6e6be02e-b0be-46fd-bd18-7c535f64ca6f", "fitness": 0.0, "name": "LevyFlightDE", "description": "A DE algorithm employing a Lvy flight-based mutation operator with a dynamically adjusted step size and a restart mechanism based on stagnation detection.", "code": "import numpy as np\n\nclass LevyFlightDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, levy_exponent=1.5, stagnation_threshold=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.levy_exponent = levy_exponent  # Exponent for Lvy flight\n        self.stagnation_threshold = stagnation_threshold # Number of iterations without improvement to trigger restart.\n        self.best_fitness_history = []\n\n    def levy_flight(self, size):\n        # Generate Lvy flight steps\n        num = np.random.randn(size) * np.sqrt(self.sigma(self.levy_exponent))\n        den = np.power(np.abs(np.random.randn(size)), (1/self.levy_exponent))\n        steps = num / den\n        return steps\n\n    def sigma(self, beta):\n        num = np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2)\n        den = np.math.gamma((1 + beta) / 2) * beta * np.power(2, (beta - 1) / 2)\n        sigma = np.power((num / den), (1 / beta))\n        return sigma\n\n    def __call__(self, func):\n        # Initialization\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        self.best_fitness_history.append(self.f_opt)\n\n        # Evolution loop\n        iteration = 0\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation using Lvy flight\n                levy_steps = self.levy_flight(self.dim)\n                mutated_vector = population[i] + self.F * levy_steps * (self.x_opt - population[i]) # biased towards best solution, combined with Levy flight\n\n                mutated_vector = np.clip(mutated_vector, lb, ub) # keep within bounds\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                trial_vector = np.copy(population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        trial_vector[j] = mutated_vector[j]\n\n                # Evaluation\n                f_trial = func(trial_vector)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    # Replacement\n                    fitness[i] = f_trial\n                    population[i] = trial_vector\n\n                    # Update best solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial_vector.copy()\n                        self.best_fitness_history.append(self.f_opt)\n                else:\n                    self.best_fitness_history.append(self.f_opt)\n\n            # Stagnation Check and Restart Mechanism\n            if iteration > self.stagnation_threshold and np.std(self.best_fitness_history[-self.stagnation_threshold:]) < 1e-6:\n                # Restart: Re-initialize a portion of the population\n                num_to_restart = int(0.2 * self.pop_size) # Restart 20% of population\n                indices_to_restart = np.random.choice(self.pop_size, num_to_restart, replace=False)\n                population[indices_to_restart] = np.random.uniform(lb, ub, size=(num_to_restart, self.dim))\n                fitness[indices_to_restart] = np.array([func(x) for x in population[indices_to_restart]])\n                self.budget -= num_to_restart # Update budget\n                \n                best_index = np.argmin(fitness) #Recalculate best solution\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n                self.best_fitness_history.append(self.f_opt)\n                \n                self.F = min(self.F * 1.2, 1.0) # slightly increase F to promote diversity after restart.\n\n\n            iteration += 1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm LevyFlightDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a86fe350-cee7-4c59-9fa9-d90561f984df"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "6e6be02e-b0be-46fd-bd18-7c535f64ca6f", "fitness": 0.0, "name": "LevyFlightDE", "description": "A DE algorithm employing a Lvy flight-based mutation operator with a dynamically adjusted step size and a restart mechanism based on stagnation detection.", "code": "import numpy as np\n\nclass LevyFlightDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, levy_exponent=1.5, stagnation_threshold=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.levy_exponent = levy_exponent  # Exponent for Lvy flight\n        self.stagnation_threshold = stagnation_threshold # Number of iterations without improvement to trigger restart.\n        self.best_fitness_history = []\n\n    def levy_flight(self, size):\n        # Generate Lvy flight steps\n        num = np.random.randn(size) * np.sqrt(self.sigma(self.levy_exponent))\n        den = np.power(np.abs(np.random.randn(size)), (1/self.levy_exponent))\n        steps = num / den\n        return steps\n\n    def sigma(self, beta):\n        num = np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2)\n        den = np.math.gamma((1 + beta) / 2) * beta * np.power(2, (beta - 1) / 2)\n        sigma = np.power((num / den), (1 / beta))\n        return sigma\n\n    def __call__(self, func):\n        # Initialization\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        self.best_fitness_history.append(self.f_opt)\n\n        # Evolution loop\n        iteration = 0\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation using Lvy flight\n                levy_steps = self.levy_flight(self.dim)\n                mutated_vector = population[i] + self.F * levy_steps * (self.x_opt - population[i]) # biased towards best solution, combined with Levy flight\n\n                mutated_vector = np.clip(mutated_vector, lb, ub) # keep within bounds\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                trial_vector = np.copy(population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        trial_vector[j] = mutated_vector[j]\n\n                # Evaluation\n                f_trial = func(trial_vector)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    # Replacement\n                    fitness[i] = f_trial\n                    population[i] = trial_vector\n\n                    # Update best solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial_vector.copy()\n                        self.best_fitness_history.append(self.f_opt)\n                else:\n                    self.best_fitness_history.append(self.f_opt)\n\n            # Stagnation Check and Restart Mechanism\n            if iteration > self.stagnation_threshold and np.std(self.best_fitness_history[-self.stagnation_threshold:]) < 1e-6:\n                # Restart: Re-initialize a portion of the population\n                num_to_restart = int(0.2 * self.pop_size) # Restart 20% of population\n                indices_to_restart = np.random.choice(self.pop_size, num_to_restart, replace=False)\n                population[indices_to_restart] = np.random.uniform(lb, ub, size=(num_to_restart, self.dim))\n                fitness[indices_to_restart] = np.array([func(x) for x in population[indices_to_restart]])\n                self.budget -= num_to_restart # Update budget\n                \n                best_index = np.argmin(fitness) #Recalculate best solution\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n                self.best_fitness_history.append(self.f_opt)\n                \n                self.F = min(self.F * 1.2, 1.0) # slightly increase F to promote diversity after restart.\n\n\n            iteration += 1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm LevyFlightDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a86fe350-cee7-4c59-9fa9-d90561f984df"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "6e6be02e-b0be-46fd-bd18-7c535f64ca6f", "fitness": 0.0, "name": "LevyFlightDE", "description": "A DE algorithm employing a Lvy flight-based mutation operator with a dynamically adjusted step size and a restart mechanism based on stagnation detection.", "code": "import numpy as np\n\nclass LevyFlightDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, levy_exponent=1.5, stagnation_threshold=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.levy_exponent = levy_exponent  # Exponent for Lvy flight\n        self.stagnation_threshold = stagnation_threshold # Number of iterations without improvement to trigger restart.\n        self.best_fitness_history = []\n\n    def levy_flight(self, size):\n        # Generate Lvy flight steps\n        num = np.random.randn(size) * np.sqrt(self.sigma(self.levy_exponent))\n        den = np.power(np.abs(np.random.randn(size)), (1/self.levy_exponent))\n        steps = num / den\n        return steps\n\n    def sigma(self, beta):\n        num = np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2)\n        den = np.math.gamma((1 + beta) / 2) * beta * np.power(2, (beta - 1) / 2)\n        sigma = np.power((num / den), (1 / beta))\n        return sigma\n\n    def __call__(self, func):\n        # Initialization\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        self.best_fitness_history.append(self.f_opt)\n\n        # Evolution loop\n        iteration = 0\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation using Lvy flight\n                levy_steps = self.levy_flight(self.dim)\n                mutated_vector = population[i] + self.F * levy_steps * (self.x_opt - population[i]) # biased towards best solution, combined with Levy flight\n\n                mutated_vector = np.clip(mutated_vector, lb, ub) # keep within bounds\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                trial_vector = np.copy(population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        trial_vector[j] = mutated_vector[j]\n\n                # Evaluation\n                f_trial = func(trial_vector)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    # Replacement\n                    fitness[i] = f_trial\n                    population[i] = trial_vector\n\n                    # Update best solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial_vector.copy()\n                        self.best_fitness_history.append(self.f_opt)\n                else:\n                    self.best_fitness_history.append(self.f_opt)\n\n            # Stagnation Check and Restart Mechanism\n            if iteration > self.stagnation_threshold and np.std(self.best_fitness_history[-self.stagnation_threshold:]) < 1e-6:\n                # Restart: Re-initialize a portion of the population\n                num_to_restart = int(0.2 * self.pop_size) # Restart 20% of population\n                indices_to_restart = np.random.choice(self.pop_size, num_to_restart, replace=False)\n                population[indices_to_restart] = np.random.uniform(lb, ub, size=(num_to_restart, self.dim))\n                fitness[indices_to_restart] = np.array([func(x) for x in population[indices_to_restart]])\n                self.budget -= num_to_restart # Update budget\n                \n                best_index = np.argmin(fitness) #Recalculate best solution\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n                self.best_fitness_history.append(self.f_opt)\n                \n                self.F = min(self.F * 1.2, 1.0) # slightly increase F to promote diversity after restart.\n\n\n            iteration += 1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm LevyFlightDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a86fe350-cee7-4c59-9fa9-d90561f984df"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "6e6be02e-b0be-46fd-bd18-7c535f64ca6f", "fitness": 0.0, "name": "LevyFlightDE", "description": "A DE algorithm employing a Lvy flight-based mutation operator with a dynamically adjusted step size and a restart mechanism based on stagnation detection.", "code": "import numpy as np\n\nclass LevyFlightDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, levy_exponent=1.5, stagnation_threshold=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.levy_exponent = levy_exponent  # Exponent for Lvy flight\n        self.stagnation_threshold = stagnation_threshold # Number of iterations without improvement to trigger restart.\n        self.best_fitness_history = []\n\n    def levy_flight(self, size):\n        # Generate Lvy flight steps\n        num = np.random.randn(size) * np.sqrt(self.sigma(self.levy_exponent))\n        den = np.power(np.abs(np.random.randn(size)), (1/self.levy_exponent))\n        steps = num / den\n        return steps\n\n    def sigma(self, beta):\n        num = np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2)\n        den = np.math.gamma((1 + beta) / 2) * beta * np.power(2, (beta - 1) / 2)\n        sigma = np.power((num / den), (1 / beta))\n        return sigma\n\n    def __call__(self, func):\n        # Initialization\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        self.best_fitness_history.append(self.f_opt)\n\n        # Evolution loop\n        iteration = 0\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation using Lvy flight\n                levy_steps = self.levy_flight(self.dim)\n                mutated_vector = population[i] + self.F * levy_steps * (self.x_opt - population[i]) # biased towards best solution, combined with Levy flight\n\n                mutated_vector = np.clip(mutated_vector, lb, ub) # keep within bounds\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                trial_vector = np.copy(population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        trial_vector[j] = mutated_vector[j]\n\n                # Evaluation\n                f_trial = func(trial_vector)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    # Replacement\n                    fitness[i] = f_trial\n                    population[i] = trial_vector\n\n                    # Update best solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial_vector.copy()\n                        self.best_fitness_history.append(self.f_opt)\n                else:\n                    self.best_fitness_history.append(self.f_opt)\n\n            # Stagnation Check and Restart Mechanism\n            if iteration > self.stagnation_threshold and np.std(self.best_fitness_history[-self.stagnation_threshold:]) < 1e-6:\n                # Restart: Re-initialize a portion of the population\n                num_to_restart = int(0.2 * self.pop_size) # Restart 20% of population\n                indices_to_restart = np.random.choice(self.pop_size, num_to_restart, replace=False)\n                population[indices_to_restart] = np.random.uniform(lb, ub, size=(num_to_restart, self.dim))\n                fitness[indices_to_restart] = np.array([func(x) for x in population[indices_to_restart]])\n                self.budget -= num_to_restart # Update budget\n                \n                best_index = np.argmin(fitness) #Recalculate best solution\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n                self.best_fitness_history.append(self.f_opt)\n                \n                self.F = min(self.F * 1.2, 1.0) # slightly increase F to promote diversity after restart.\n\n\n            iteration += 1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm LevyFlightDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a86fe350-cee7-4c59-9fa9-d90561f984df"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "6e6be02e-b0be-46fd-bd18-7c535f64ca6f", "fitness": 0.0, "name": "LevyFlightDE", "description": "A DE algorithm employing a Lvy flight-based mutation operator with a dynamically adjusted step size and a restart mechanism based on stagnation detection.", "code": "import numpy as np\n\nclass LevyFlightDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, levy_exponent=1.5, stagnation_threshold=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.levy_exponent = levy_exponent  # Exponent for Lvy flight\n        self.stagnation_threshold = stagnation_threshold # Number of iterations without improvement to trigger restart.\n        self.best_fitness_history = []\n\n    def levy_flight(self, size):\n        # Generate Lvy flight steps\n        num = np.random.randn(size) * np.sqrt(self.sigma(self.levy_exponent))\n        den = np.power(np.abs(np.random.randn(size)), (1/self.levy_exponent))\n        steps = num / den\n        return steps\n\n    def sigma(self, beta):\n        num = np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2)\n        den = np.math.gamma((1 + beta) / 2) * beta * np.power(2, (beta - 1) / 2)\n        sigma = np.power((num / den), (1 / beta))\n        return sigma\n\n    def __call__(self, func):\n        # Initialization\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        self.best_fitness_history.append(self.f_opt)\n\n        # Evolution loop\n        iteration = 0\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation using Lvy flight\n                levy_steps = self.levy_flight(self.dim)\n                mutated_vector = population[i] + self.F * levy_steps * (self.x_opt - population[i]) # biased towards best solution, combined with Levy flight\n\n                mutated_vector = np.clip(mutated_vector, lb, ub) # keep within bounds\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                trial_vector = np.copy(population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        trial_vector[j] = mutated_vector[j]\n\n                # Evaluation\n                f_trial = func(trial_vector)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    # Replacement\n                    fitness[i] = f_trial\n                    population[i] = trial_vector\n\n                    # Update best solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial_vector.copy()\n                        self.best_fitness_history.append(self.f_opt)\n                else:\n                    self.best_fitness_history.append(self.f_opt)\n\n            # Stagnation Check and Restart Mechanism\n            if iteration > self.stagnation_threshold and np.std(self.best_fitness_history[-self.stagnation_threshold:]) < 1e-6:\n                # Restart: Re-initialize a portion of the population\n                num_to_restart = int(0.2 * self.pop_size) # Restart 20% of population\n                indices_to_restart = np.random.choice(self.pop_size, num_to_restart, replace=False)\n                population[indices_to_restart] = np.random.uniform(lb, ub, size=(num_to_restart, self.dim))\n                fitness[indices_to_restart] = np.array([func(x) for x in population[indices_to_restart]])\n                self.budget -= num_to_restart # Update budget\n                \n                best_index = np.argmin(fitness) #Recalculate best solution\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n                self.best_fitness_history.append(self.f_opt)\n                \n                self.F = min(self.F * 1.2, 1.0) # slightly increase F to promote diversity after restart.\n\n\n            iteration += 1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm LevyFlightDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a86fe350-cee7-4c59-9fa9-d90561f984df"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "6e6be02e-b0be-46fd-bd18-7c535f64ca6f", "fitness": 0.0, "name": "LevyFlightDE", "description": "A DE algorithm employing a Lvy flight-based mutation operator with a dynamically adjusted step size and a restart mechanism based on stagnation detection.", "code": "import numpy as np\n\nclass LevyFlightDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, levy_exponent=1.5, stagnation_threshold=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.levy_exponent = levy_exponent  # Exponent for Lvy flight\n        self.stagnation_threshold = stagnation_threshold # Number of iterations without improvement to trigger restart.\n        self.best_fitness_history = []\n\n    def levy_flight(self, size):\n        # Generate Lvy flight steps\n        num = np.random.randn(size) * np.sqrt(self.sigma(self.levy_exponent))\n        den = np.power(np.abs(np.random.randn(size)), (1/self.levy_exponent))\n        steps = num / den\n        return steps\n\n    def sigma(self, beta):\n        num = np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2)\n        den = np.math.gamma((1 + beta) / 2) * beta * np.power(2, (beta - 1) / 2)\n        sigma = np.power((num / den), (1 / beta))\n        return sigma\n\n    def __call__(self, func):\n        # Initialization\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        self.best_fitness_history.append(self.f_opt)\n\n        # Evolution loop\n        iteration = 0\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation using Lvy flight\n                levy_steps = self.levy_flight(self.dim)\n                mutated_vector = population[i] + self.F * levy_steps * (self.x_opt - population[i]) # biased towards best solution, combined with Levy flight\n\n                mutated_vector = np.clip(mutated_vector, lb, ub) # keep within bounds\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                trial_vector = np.copy(population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        trial_vector[j] = mutated_vector[j]\n\n                # Evaluation\n                f_trial = func(trial_vector)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    # Replacement\n                    fitness[i] = f_trial\n                    population[i] = trial_vector\n\n                    # Update best solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial_vector.copy()\n                        self.best_fitness_history.append(self.f_opt)\n                else:\n                    self.best_fitness_history.append(self.f_opt)\n\n            # Stagnation Check and Restart Mechanism\n            if iteration > self.stagnation_threshold and np.std(self.best_fitness_history[-self.stagnation_threshold:]) < 1e-6:\n                # Restart: Re-initialize a portion of the population\n                num_to_restart = int(0.2 * self.pop_size) # Restart 20% of population\n                indices_to_restart = np.random.choice(self.pop_size, num_to_restart, replace=False)\n                population[indices_to_restart] = np.random.uniform(lb, ub, size=(num_to_restart, self.dim))\n                fitness[indices_to_restart] = np.array([func(x) for x in population[indices_to_restart]])\n                self.budget -= num_to_restart # Update budget\n                \n                best_index = np.argmin(fitness) #Recalculate best solution\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n                self.best_fitness_history.append(self.f_opt)\n                \n                self.F = min(self.F * 1.2, 1.0) # slightly increase F to promote diversity after restart.\n\n\n            iteration += 1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm LevyFlightDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a86fe350-cee7-4c59-9fa9-d90561f984df"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "d33a1050-2a7c-4257-9866-e775e520e6cc", "fitness": 0.12756178259202308, "name": "CauchyRankDE", "description": "DE with a Cauchy mutation operator, a rank-based selection and restarts to escape local optima.", "code": "import numpy as np\n\nclass CauchyRankDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, restart_prob=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.restart_prob = restart_prob\n        self.population = None\n        self.fitness = None\n        self.best_index = None\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index].copy()\n\n    def cauchy_mutation(self, x_r1, x_r2, F):\n        return x_r1 + F * np.random.standard_cauchy(size=x_r1.shape) * (x_r1 - x_r2)\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                \n                # Cauchy mutation\n                v = self.cauchy_mutation(x_r1, x_r2, self.F)\n\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = self.population[i].copy()\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        u[j] = v[j]\n\n                # Evaluation\n                f_u = func(u)\n                self.budget -= 1\n\n                # Rank-based selection\n                if f_u < self.fitness[i]:\n                    self.fitness[i] = f_u\n                    self.population[i] = u\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u.copy()\n\n            # Restart mechanism\n            if np.random.rand() < self.restart_prob:\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.budget -= self.pop_size\n                self.best_index = np.argmin(self.fitness)\n                self.f_opt = self.fitness[self.best_index]\n                self.x_opt = self.population[self.best_index].copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm CauchyRankDE scored 0.128 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ebaf8feb-b76c-43be-ae64-505129468592"], "operator": null, "metadata": {"aucs": [0.1683488771974908, 0.2143364705785784, 0]}}
{"id": "f61c5b16-945b-4763-8c73-881a5e25148c", "fitness": 0.0, "name": "CooperativeSwarmOptimization", "description": "Cooperative Swarm Optimization with Adaptive Landscape Exploration, employing multiple interacting swarms with dynamically adjusted exploration rates based on local landscape characteristics and inter-swarm communication.", "code": "import numpy as np\n\nclass CooperativeSwarmOptimization:\n    def __init__(self, budget=10000, dim=10, num_swarms=3, swarm_size=20):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = num_swarms\n        self.swarm_size = swarm_size\n        self.swarms = []\n        self.swarm_positions = []\n        self.swarm_velocities = []\n        self.swarm_fitness = []\n        self.swarm_best_positions = []\n        self.swarm_best_fitness = []\n        self.exploration_rates = np.ones(self.num_swarms) * 0.5 # Initial exploration rate for each swarm\n        self.inertia_weights = np.ones(self.num_swarms) * 0.7\n        self.cognitive_coeffs = np.ones(self.num_swarms) * 1.5\n        self.social_coeffs = np.ones(self.num_swarms) * 1.5\n        self.communication_probability = 0.1 #probability of inter-swarm communication\n\n\n    def initialize_swarms(self, func):\n        for i in range(self.num_swarms):\n            positions = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.swarm_size, self.dim))\n            velocities = np.random.uniform(-1, 1, size=(self.swarm_size, self.dim)) * 0.1\n            fitness = np.array([func(x) for x in positions])\n            self.budget -= self.swarm_size\n            best_indices = np.argmin(fitness)\n            best_positions = positions[best_indices].copy()\n            best_fitness = fitness[best_indices]\n            self.swarms.append(i)\n            self.swarm_positions.append(positions)\n            self.swarm_velocities.append(velocities)\n            self.swarm_fitness.append(fitness)\n            self.swarm_best_positions.append(best_positions)\n            self.swarm_best_fitness.append(best_fitness)\n\n    def update_velocities(self, swarm_index, global_best_position):\n        inertia_weight = self.inertia_weights[swarm_index]\n        cognitive_coeff = self.cognitive_coeffs[swarm_index]\n        social_coeff = self.social_coeffs[swarm_index]\n        exploration_rate = self.exploration_rates[swarm_index]\n        for i in range(self.swarm_size):\n            r1 = np.random.rand(self.dim)\n            r2 = np.random.rand(self.dim)\n            self.swarm_velocities[swarm_index][i] = (\n                inertia_weight * self.swarm_velocities[swarm_index][i]\n                + cognitive_coeff * r1 * (self.swarm_best_positions[swarm_index] - self.swarm_positions[swarm_index][i])\n                + social_coeff * r2 * (global_best_position - self.swarm_positions[swarm_index][i])\n                + exploration_rate * np.random.uniform(-1, 1, size=self.dim) # Exploration term\n            )\n\n    def update_positions(self, swarm_index, func):\n        self.swarm_positions[swarm_index] += self.swarm_velocities[swarm_index]\n        self.swarm_positions[swarm_index] = np.clip(self.swarm_positions[swarm_index], func.bounds.lb, func.bounds.ub)\n        fitness = np.array([func(x) for x in self.swarm_positions[swarm_index]])\n        self.budget -= self.swarm_size\n        \n        for i in range(self.swarm_size):\n            if fitness[i] < self.swarm_fitness[swarm_index][i]:\n                self.swarm_fitness[swarm_index][i] = fitness[i]\n                if fitness[i] < self.swarm_best_fitness[swarm_index]:\n                    self.swarm_best_fitness[swarm_index] = fitness[i]\n                    self.swarm_best_positions[swarm_index] = self.swarm_positions[swarm_index][i].copy()\n\n    def adaptive_exploration(self, swarm_index):\n         # Simple adjustment based on swarm's progress. Increase if stagnant, decrease if improving\n        if np.std(self.swarm_fitness[swarm_index]) < 1e-3: #Stagnation\n            self.exploration_rates[swarm_index] = min(1.0, self.exploration_rates[swarm_index] * 1.1)\n        else:\n            self.exploration_rates[swarm_index] = max(0.01, self.exploration_rates[swarm_index] * 0.9)\n\n    def inter_swarm_communication(self):\n        # Exchange information between swarms with a probability\n        for i in range(self.num_swarms):\n            if np.random.rand() < self.communication_probability:\n                # Select another swarm to communicate with\n                other_swarm = np.random.choice([s for s in range(self.num_swarms) if s != i])\n                # Exchange best positions. The better swarm influences the other.\n                if self.swarm_best_fitness[i] < self.swarm_best_fitness[other_swarm]:\n                    self.swarm_best_positions[other_swarm] = self.swarm_best_positions[i].copy()\n                    self.swarm_best_fitness[other_swarm] = self.swarm_best_fitness[i]\n                else:\n                     self.swarm_best_positions[i] = self.swarm_best_positions[other_swarm].copy()\n                     self.swarm_best_fitness[i] = self.swarm_best_fitness[other_swarm]\n\n    def __call__(self, func):\n        # Initialization\n        self.initialize_swarms(func)\n        global_best_swarm = np.argmin(self.swarm_best_fitness)\n        self.f_opt = self.swarm_best_fitness[global_best_swarm]\n        self.x_opt = self.swarm_best_positions[global_best_swarm].copy()\n\n        # Evolution loop\n        while self.budget > 0:\n            # Update velocities and positions for each swarm\n            for i in range(self.num_swarms):\n                self.update_velocities(i, self.x_opt)\n                self.update_positions(i, func)\n                self.adaptive_exploration(i)\n            \n            self.inter_swarm_communication()\n            \n            # Update global best\n            global_best_swarm = np.argmin(self.swarm_best_fitness)\n            if self.swarm_best_fitness[global_best_swarm] < self.f_opt:\n                self.f_opt = self.swarm_best_fitness[global_best_swarm]\n                self.x_opt = self.swarm_best_positions[global_best_swarm].copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm CooperativeSwarmOptimization scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c0496c6a-4259-4c28-9d4c-5e8bdf7a80c9"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "9e6a543f-9c6c-4bd8-ae93-3e679d755242", "fitness": 0.38956909802830364, "name": "LearningCauchyDE", "description": "A differential evolution strategy incorporating a learning-based mutation and a Cauchy-distributed perturbation for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass LearningCauchyDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.learning_rate = learning_rate\n        self.population = None\n        self.fitness = None\n        self.best_index = None\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index].copy()\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Learning-based mutation: Adapt F based on success\n                if np.random.rand() < 0.5:  # Apply with a probability\n                    success = False\n                    \n                    # Mutation\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    v = self.population[i] + self.F * (x_r1 - x_r2)\n                    v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                    # Crossover\n                    j_rand = np.random.randint(self.dim)\n                    u = self.population[i].copy()\n                    for j in range(self.dim):\n                        if np.random.rand() < self.CR or j == j_rand:\n                            u[j] = v[j]\n                    \n                    # Cauchy perturbation\n                    cauchy_noise = np.random.standard_cauchy(size=self.dim) * 0.01  # Adjust scale\n                    u = np.clip(u + cauchy_noise, func.bounds.lb, func.bounds.ub)\n\n                    # Evaluation\n                    f_u = func(u)\n                    self.budget -= 1\n\n                    # Selection\n                    if f_u < self.fitness[i]:\n                        success = True\n                        self.fitness[i] = f_u\n                        self.population[i] = u\n                        if f_u < self.f_opt:\n                            self.f_opt = f_u\n                            self.x_opt = u.copy()\n\n                    # Update F based on success\n                    if success:\n                        self.F += self.learning_rate * (1 - self.F)  # Increase F if successful\n                    else:\n                        self.F -= self.learning_rate * self.F  # Decrease F if unsuccessful\n                    self.F = np.clip(self.F, 0.1, 1.0)  # Keep F within bounds\n\n                else:\n                    # Standard DE mutation if learning not applied\n\n                    # Mutation\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    v = self.population[i] + self.F * (x_r1 - x_r2)\n                    v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                    # Crossover\n                    j_rand = np.random.randint(self.dim)\n                    u = self.population[i].copy()\n                    for j in range(self.dim):\n                        if np.random.rand() < self.CR or j == j_rand:\n                            u[j] = v[j]\n\n                    # Cauchy perturbation\n                    cauchy_noise = np.random.standard_cauchy(size=self.dim) * 0.01  # Adjust scale\n                    u = np.clip(u + cauchy_noise, func.bounds.lb, func.bounds.ub)\n                    \n                    # Evaluation\n                    f_u = func(u)\n                    self.budget -= 1\n\n                    # Selection\n                    if f_u < self.fitness[i]:\n                        self.fitness[i] = f_u\n                        self.population[i] = u\n                        if f_u < self.f_opt:\n                            self.f_opt = f_u\n                            self.x_opt = u.copy()\n                            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm LearningCauchyDE scored 0.390 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ebaf8feb-b76c-43be-ae64-505129468592"], "operator": null, "metadata": {"aucs": [0.13301805484306983, 0.24484081810802982, 0.3859647620194713, 0.4536172863874445, 0.315384333543811, 0.40353482288326026, 0.2843872820399209, 0.3406448031015078, 0.2991146966442052, 0.17682970969405487, 0.5090641460457229, 0.9995045504338291, 0.32866224813138367, 0.3033606626522667, 0.7303646430440778, 0.38948674246758563, 0.3230749164857716, 0.4818452681074946, 0.19856028908954615, 0.4901219248436184]}}
{"id": "edc9d194-f30d-467c-b6e2-21071746cf26", "fitness": -Infinity, "name": "DistanceAdaptiveDE", "description": "Differential Evolution with a distance-based mutation and a self-adaptive population size that adjusts according to the optimization progress.", "code": "import numpy as np\n\nclass DistanceAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, pop_size_adaptation_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.pop_size_adaptation_rate = pop_size_adaptation_rate\n        self.population = None\n        self.fitness = None\n        self.best_index = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.archive = []\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index].copy()\n\n    def calculate_distance(self, x):\n        \"\"\"Calculates the average Euclidean distance to other population members.\"\"\"\n        distances = np.linalg.norm(self.population - x, axis=1)\n        distances = distances[distances > 0] # Avoid distance to self\n        return np.mean(distances) if len(distances) > 0 else 0\n\n    def adapt_population_size(self):\n        \"\"\"Dynamically adjust population size based on the diversity of the population.\"\"\"\n        avg_distance = np.mean([self.calculate_distance(x) for x in self.population])\n\n        if avg_distance > 0.1:  # High diversity: Increase population size\n            self.pop_size = min(int(self.pop_size * (1 + self.pop_size_adaptation_rate)), 100)\n        else:  # Low diversity: Decrease population size\n            self.pop_size = max(int(self.pop_size * (1 - self.pop_size_adaptation_rate)), 10)\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            self.adapt_population_size()\n            new_population = np.zeros((self.pop_size, self.dim))\n            new_fitness = np.zeros(self.pop_size)\n\n            for i in range(self.pop_size):\n                # Distance-based mutation\n                distances = np.array([self.calculate_distance(x) for x in self.population])\n                probabilities = distances / np.sum(distances)\n                indices = np.random.choice(self.pop_size, 3, replace=False, p=probabilities)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                \n                v = self.population[i] + self.F * (x_r1 - x_r2)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = self.population[i].copy()\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        u[j] = v[j]\n\n                # Evaluation\n                f_u = func(u)\n                self.budget -= 1\n\n                # Selection\n                if f_u < self.fitness[i]:\n                    new_fitness[i] = f_u\n                    new_population[i] = u\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u.copy()\n                else:\n                    new_fitness[i] = self.fitness[i]\n                    new_population[i] = self.population[i]\n            \n            self.fitness = new_fitness\n            self.population = new_population\n            self.best_index = np.argmin(self.fitness)\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 54, in __call__\n  File \"mtrand.pyx\", line 951, in numpy.random.mtrand.RandomState.choice\nValueError: 'a' and 'p' must have same size\n.", "error": "", "parent_ids": ["9e6a543f-9c6c-4bd8-ae93-3e679d755242"], "operator": null, "metadata": {}}
{"id": "88a89f6b-2f6d-4194-bce2-734196b40e37", "fitness": 0.0, "name": "AdaptiveCauchyDE", "description": "Differential Evolution with a self-adaptive mutation rate, Cauchy perturbation, and a local search operator to improve exploitation.", "code": "import numpy as np\n\nclass AdaptiveCauchyDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_init=0.5, CR=0.7, local_search_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_init = F_init\n        self.CR = CR\n        self.local_search_prob = local_search_prob\n        self.population = None\n        self.fitness = None\n        self.best_index = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.F = np.full(pop_size, F_init) # Self-adaptive mutation rate for each individual\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index].copy()\n\n    def cauchy_mutation(self, x_r1, x_r2, F):\n        return x_r1 + F * np.random.standard_cauchy(size=x_r1.shape) * (x_r1 - x_r2)\n\n    def local_search(self, x, func, bounds):\n        # Simple local search: perturb each dimension with a small random value\n        x_new = x.copy()\n        for i in range(self.dim):\n            x_new[i] += np.random.uniform(-0.1, 0.1) #Smaller perturbation\n        x_new = np.clip(x_new, bounds.lb, bounds.ub)\n        f_new = func(x_new)\n        self.budget -=1\n        return x_new, f_new\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n\n                # Cauchy mutation with adaptive F\n                v = self.cauchy_mutation(x_r1, x_r2, self.F[i])\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = self.population[i].copy()\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        u[j] = v[j]\n\n                # Evaluation\n                f_u = func(u)\n                self.budget -= 1\n\n                # Selection\n                if f_u < self.fitness[i]:\n                    self.fitness[i] = f_u\n                    self.population[i] = u\n                    # Update adaptive F based on success\n                    self.F[i] = self.F_init * np.random.uniform(0.8, 1.2) # Adjust F slightly\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u.copy()\n                else:\n                    # If the trial vector is not better, reduce F\n                    self.F[i] = self.F[i] * np.random.uniform(0.5, 0.8) # Reduce F if unsuccessful\n\n                self.F[i] = np.clip(self.F[i], 0.1, 1.0) # Keep F within reasonable bounds\n\n                # Local search\n                if np.random.rand() < self.local_search_prob and self.budget > 0:\n                    x_local, f_local = self.local_search(self.population[i], func, func.bounds)\n                    if f_local < self.fitness[i]:\n                        self.fitness[i] = f_local\n                        self.population[i] = x_local\n                        if f_local < self.f_opt:\n                            self.f_opt = f_local\n                            self.x_opt = x_local.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveCauchyDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d33a1050-2a7c-4257-9866-e775e520e6cc"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "76dbffef-405e-400f-a102-b4e8b699141a", "fitness": 0.0, "name": "HybridPSO", "description": "Hybrid Particle Swarm Optimization with Lvy flight dispersal and covariance matrix adaptation to accelerate convergence and maintain diversity.", "code": "import numpy as np\n\nclass HybridPSO:\n    def __init__(self, budget=10000, dim=10, swarm_size=30, levy_flight_probability=0.1, cma_learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.levy_flight_probability = levy_flight_probability\n        self.cma_learning_rate = cma_learning_rate\n        self.swarm_positions = None\n        self.swarm_velocities = None\n        self.swarm_fitness = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = None\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.covariance_matrix = None\n\n    def initialize_swarm(self, func):\n        self.swarm_positions = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.swarm_size, self.dim))\n        self.swarm_velocities = np.random.uniform(-1, 1, size=(self.swarm_size, self.dim)) * 0.1\n        self.swarm_fitness = np.array([func(x) for x in self.swarm_positions])\n        self.budget -= self.swarm_size\n        self.personal_best_positions = self.swarm_positions.copy()\n        self.personal_best_fitness = self.swarm_fitness.copy()\n\n        best_index = np.argmin(self.swarm_fitness)\n        self.global_best_position = self.swarm_positions[best_index].copy()\n        self.global_best_fitness = self.swarm_fitness[best_index]\n        self.covariance_matrix = np.eye(self.dim)  # Initialize covariance matrix\n\n    def levy_flight(self, beta=1.5):\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) / (np.math.gamma((1 + beta) / 2) * beta * (2 ** ((beta - 1) / 2)))) ** (1 / beta)\n        u = np.random.normal(0, sigma, size=self.dim)\n        v = np.random.normal(0, 1, size=self.dim)\n        step = u / (abs(v) ** (1 / beta))\n        return step\n\n    def update_velocities(self, cognitive_coeff=1.5, social_coeff=1.5, inertia_weight=0.7):\n        for i in range(self.swarm_size):\n            r1 = np.random.rand(self.dim)\n            r2 = np.random.rand(self.dim)\n\n            self.swarm_velocities[i] = (inertia_weight * self.swarm_velocities[i]\n                                       + cognitive_coeff * r1 * (self.personal_best_positions[i] - self.swarm_positions[i])\n                                       + social_coeff * r2 * (self.global_best_position - self.swarm_positions[i]))\n\n            # Lvy flight dispersal\n            if np.random.rand() < self.levy_flight_probability:\n                levy_step = self.levy_flight()\n                self.swarm_velocities[i] += 0.01 * levy_step  # scale levy step\n                \n\n    def update_positions(self, func):\n        self.swarm_positions += self.swarm_velocities\n        self.swarm_positions = np.clip(self.swarm_positions, func.bounds.lb, func.bounds.ub)\n        fitness = np.array([func(x) for x in self.swarm_positions])\n        self.budget -= self.swarm_size\n\n        for i in range(self.swarm_size):\n            if fitness[i] < self.swarm_fitness[i]:\n                self.swarm_fitness[i] = fitness[i]\n                if fitness[i] < self.personal_best_fitness[i]:\n                    self.personal_best_fitness[i] = fitness[i]\n                    self.personal_best_positions[i] = self.swarm_positions[i].copy()\n                    if fitness[i] < self.global_best_fitness:\n                        self.global_best_fitness = fitness[i]\n                        self.global_best_position = self.swarm_positions[i].copy()\n\n    def update_covariance_matrix(self):\n        # Simple CMA update: adjust covariance matrix based on successful steps\n        diff = self.swarm_positions - self.global_best_position\n        self.covariance_matrix = (1 - self.cma_learning_rate) * self.covariance_matrix + self.cma_learning_rate * np.cov(diff.T)\n        #Ensure positive definiteness\n        try:\n            np.linalg.cholesky(self.covariance_matrix)\n        except np.linalg.LinAlgError:\n            self.covariance_matrix = np.eye(self.dim) #reset if not positive definite\n            \n\n    def __call__(self, func):\n        self.initialize_swarm(func)\n\n        while self.budget > 0:\n            self.update_velocities()\n            self.update_positions(func)\n            self.update_covariance_matrix()  # Update covariance matrix\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 5, "feedback": "The algorithm HybridPSO scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f61c5b16-945b-4763-8c73-881a5e25148c"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "1c94c51d-6e9d-429c-9d7c-97259e040323", "fitness": 0.3373704303952438, "name": "AdaptiveOrthogonalDE", "description": "Differential Evolution with a self-adaptive mutation strategy using a combination of current-to-best and random differential evolution, adapting the mutation factor based on population diversity, and incorporating orthogonal learning for enhanced exploration.", "code": "import numpy as np\n\nclass AdaptiveOrthogonalDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_initial=0.5, CR=0.7, diversity_threshold=0.1, orthogonal_learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_initial = F_initial  # Initial mutation factor\n        self.CR = CR  # Crossover rate\n        self.diversity_threshold = diversity_threshold # Threshold for triggering mutation adaptation\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.F = np.full(pop_size, F_initial)\n\n    def calculate_diversity(self, population):\n        # Calculate the average pairwise distance between individuals in the population\n        distances = np.sum((population[:, np.newaxis, :] - population[np.newaxis, :, :])**2, axis=2)\n        diversity = np.mean(distances)\n        return diversity\n        \n    def orthogonal_design(self, x, num_samples=5):\n        # Generate orthogonal samples around x\n        orthogonal_samples = []\n        for _ in range(num_samples):\n            direction = np.random.randn(self.dim)\n            direction /= np.linalg.norm(direction) # Normalize direction\n            step_size = np.random.uniform(-self.orthogonal_learning_rate, self.orthogonal_learning_rate)\n            sample = x + step_size * direction\n            orthogonal_samples.append(sample)\n        return np.array(orthogonal_samples)\n\n    def __call__(self, func):\n        # Initialization\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        # Evolution loop\n        while self.budget > 0:\n            diversity = self.calculate_diversity(population)\n            \n            for i in range(self.pop_size):\n                # Mutation strategy selection based on diversity\n                if diversity > self.diversity_threshold:\n                    # Current-to-best mutation\n                    r1, r2 = np.random.choice(self.pop_size, 2, replace=False)\n                    mutated_vector = population[i] + self.F[i] * (self.x_opt - population[i]) + self.F[i] * (population[r1] - population[r2])\n                else:\n                    # Random differential mutation\n                    r1, r2, r3 = np.random.choice(self.pop_size, 3, replace=False)\n                    mutated_vector = population[r1] + self.F[i] * (population[r2] - population[r3])\n\n                mutated_vector = np.clip(mutated_vector, lb, ub) # keep within bounds\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                trial_vector = np.copy(population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        trial_vector[j] = mutated_vector[j]\n\n                # Evaluation\n                f_trial = func(trial_vector)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    # Replacement\n                    fitness[i] = f_trial\n                    population[i] = trial_vector\n                    \n                    # Orthogonal Learning\n                    orthogonal_samples = self.orthogonal_design(trial_vector)\n                    orthogonal_fitness = [func(sample) for sample in orthogonal_samples]\n                    self.budget -= len(orthogonal_samples)\n\n                    best_orthogonal_index = np.argmin(orthogonal_fitness)\n                    if orthogonal_fitness[best_orthogonal_index] < f_trial:\n                         population[i] = orthogonal_samples[best_orthogonal_index]\n                         fitness[i] = orthogonal_fitness[best_orthogonal_index]\n                         f_trial = orthogonal_fitness[best_orthogonal_index]\n                    # Update best solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = population[i].copy()\n                \n                # Adapt mutation factor\n                if f_trial < fitness[i]:\n                    self.F[i] = self.F_initial # Reset if improvement found\n                else:\n                    self.F[i] = min(self.F[i] * 1.1, 1.0) # Increase slightly if no improvement\n                \n                if self.budget <= 0:\n                    break\n            if self.budget <= 0:\n                break\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveOrthogonalDE scored 0.337 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6e6be02e-b0be-46fd-bd18-7c535f64ca6f"], "operator": null, "metadata": {"aucs": [0.17738164780009047, 0.3590470425490292, 0.4097710967891255, 0.615727436480063, 0.36166709870170755, 0.42614310190055826, 0.33744834801453827, 0.34914810132208185, 0]}}
{"id": "ac1b2e9f-1ff6-4f34-b985-8e78c941c8f7", "fitness": 0.0, "name": "SaCmaLocalDE", "description": "Differential Evolution with self-adaptive mutation parameters, covariance matrix adaptation for crossover, and a local search operator to refine solutions.", "code": "import numpy as np\n\nclass SaCmaLocalDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, local_search_prob=0.1, ls_radius=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.local_search_prob = local_search_prob\n        self.ls_radius = ls_radius\n        self.population = None\n        self.fitness = None\n        self.best_index = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.covariance_matrix = None\n        self.mean = None\n\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index].copy()\n        self.mean = np.mean(self.population, axis=0)\n        self.covariance_matrix = np.cov(self.population, rowvar=False)\n        if np.sum(np.isnan(self.covariance_matrix)) > 0:\n            self.covariance_matrix = np.eye(self.dim) * 0.01 # fallback\n\n\n    def mutation(self, i):\n        indices = np.random.choice(self.pop_size, 3, replace=False)\n        x_r1, x_r2, x_r3 = self.population[indices]\n\n        # Self-adaptive F\n        F = np.random.normal(self.F, 0.1)\n        F = np.clip(F, 0.1, 1.0)\n\n        v = x_r1 + F * (x_r2 - x_r3)\n        return v\n\n\n    def crossover(self, mutant, target):\n         # CMA-ES like crossover\n        u = target.copy()\n        j_rand = np.random.randint(self.dim)\n        \n        # Ensure covariance matrix is positive semi-definite by adding a small constant to the diagonal\n        covariance_matrix = self.covariance_matrix + np.eye(self.dim) * 1e-6\n        \n        try:\n          L = np.linalg.cholesky(covariance_matrix)\n          z = np.random.normal(0, 1, size=self.dim)\n          offspring = self.mean + np.dot(L, z)\n          offspring = np.clip(offspring, -5.0, 5.0)\n        \n          for j in range(self.dim):\n              if np.random.rand() < self.CR or j == j_rand:\n                u[j] = mutant[j] # mutant[j]\n              else:\n                u[j] = offspring[j]\n        except np.linalg.LinAlgError:\n          for j in range(self.dim):\n              if np.random.rand() < self.CR or j == j_rand:\n                  u[j] = mutant[j]\n        return u\n\n\n    def local_search(self, x, func):\n        x_new = x + np.random.uniform(-self.ls_radius, self.ls_radius, size=self.dim)\n        x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n        f_new = func(x_new)\n        self.budget -= 1\n        return x_new, f_new\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                v = self.mutation(i)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                u = self.crossover(v, self.population[i])\n                u = np.clip(u, func.bounds.lb, func.bounds.ub)\n\n                # Evaluation\n                f_u = func(u)\n                self.budget -= 1\n\n                # Selection\n                if f_u < self.fitness[i]:\n                    self.fitness[i] = f_u\n                    self.population[i] = u\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u.copy()\n\n                # Local search\n                if np.random.rand() < self.local_search_prob and self.budget > 0:\n                    x_ls, f_ls = self.local_search(self.population[i], func)\n                    if f_ls < self.fitness[i]:\n                        self.fitness[i] = f_ls\n                        self.population[i] = x_ls\n                        if f_ls < self.f_opt:\n                            self.f_opt = f_ls\n                            self.x_opt = x_ls.copy()\n            \n            self.mean = np.mean(self.population, axis=0)\n            self.covariance_matrix = np.cov(self.population, rowvar=False)\n            if np.sum(np.isnan(self.covariance_matrix)) > 0:\n                self.covariance_matrix = np.eye(self.dim) * 0.01\n            \n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm SaCmaLocalDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d33a1050-2a7c-4257-9866-e775e520e6cc"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "b10b0fc9-0be9-4804-9c0a-d93e46faec93", "fitness": 0.0, "name": "SelfAdaptivePopulationDE", "description": "A DE variant with a self-adaptive population size, where the population shrinks or grows based on performance, and orthogonal learning is incorporated.", "code": "import numpy as np\n\nclass SelfAdaptivePopulationDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, F=0.5, CR=0.7, adaptive_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.initial_pop_size = initial_pop_size\n        self.F = F\n        self.CR = CR\n        self.adaptive_rate = adaptive_rate\n        self.population = None\n        self.fitness = None\n        self.best_index = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.generation = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index].copy()\n\n    def orthogonal_learning(self, func, x):\n        # Orthogonal array design for local search around x\n        levels = 3  # Number of levels for each dimension\n        orthogonal_array = np.array([\n            [-1, -1],\n            [-1, 1],\n            [1, -1],\n            [1, 1],\n            [0,0]\n        ])\n        \n        best_f = np.inf\n        best_x = None\n        \n        for i in range(orthogonal_array.shape[0]):\n            new_x = x.copy()\n            step_size = 0.05 # Small step size\n            for j in range(min(self.dim, orthogonal_array.shape[1])):\n                 new_x[j] = x[j] + orthogonal_array[i, j] * step_size\n                 new_x = np.clip(new_x, func.bounds.lb, func.bounds.ub)\n            f = func(new_x)\n            self.budget -= 1\n            if f < best_f:\n                best_f = f\n                best_x = new_x\n        \n        return best_f, best_x\n\n    def adjust_population_size(self):\n        # Dynamically adjust population size based on stagnation\n        if self.generation % 10 == 0:\n            if self.f_opt == self.f_opt_prev:\n                # Stagnation detected: Increase population size\n                self.pop_size = min(int(self.pop_size * (1 + self.adaptive_rate)), 2 * self.initial_pop_size)\n            else:\n                # Improvement: Decrease population size\n                self.pop_size = max(int(self.pop_size * (1 - self.adaptive_rate)), self.initial_pop_size // 2)\n            \n            # Ensure population size remains within bounds and is an integer\n            self.pop_size = max(10, min(self.pop_size, 100))\n            self.pop_size = int(self.pop_size)\n\n            # Resize population (crude, but effective) - reinitialize\n            if self.pop_size != self.population.shape[0]:\n                self.initialize_population(self.func_ref) # Reinitialize including fitness calculation\n\n\n    def __call__(self, func):\n        self.func_ref = func\n        self.initialize_population(func)\n        self.f_opt_prev = np.inf\n\n        while self.budget > 0:\n            self.generation += 1\n            self.adjust_population_size()\n\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                v = self.population[i] + self.F * (x_r1 - x_r2)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = self.population[i].copy()\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        u[j] = v[j]\n                \n                u = np.clip(u, func.bounds.lb, func.bounds.ub)\n\n                # Evaluation\n                f_u = func(u)\n                self.budget -= 1\n\n                # Selection\n                if f_u < self.fitness[i]:\n                    self.fitness[i] = f_u\n                    self.population[i] = u\n                    if f_u < self.f_opt:\n                        self.f_opt_prev = self.f_opt\n                        self.f_opt = f_u\n                        self.x_opt = u.copy()\n                        #Orthogonal learning around the new best\n                        f_ortho, x_ortho = self.orthogonal_learning(func, self.x_opt)\n                        if f_ortho < self.f_opt:\n                            self.f_opt = f_ortho\n                            self.x_opt = x_ortho.copy()\n                            \n\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm SelfAdaptivePopulationDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9e6a543f-9c6c-4bd8-ae93-3e679d755242"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "28048c96-b6fa-439f-8550-f8b7464e6acb", "fitness": -Infinity, "name": "APS_LS_DE", "description": "Adaptive Population Size Differential Evolution with a global search boost and a local search fine-tuning stage based on the current function evaluations left.", "code": "import numpy as np\n\nclass APS_LS_DE:\n    def __init__(self, budget=10000, dim=10, initial_population_size=50, local_search_iterations=5):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = initial_population_size\n        self.local_search_iterations = local_search_iterations\n        self.F = 0.5  # Differential evolution parameter\n        self.CR = 0.7 # Crossover rate\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.population_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.population_size\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_f = self.fitness[self.best_index]\n\n    def evolve(self, func):\n        for i in range(self.population_size):\n            # Mutation\n            idxs = np.random.choice(self.population_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[idxs]\n            x_mutated = x_r1 + self.F * (x_r2 - x_r3)\n            x_mutated = np.clip(x_mutated, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            x_trial = self.population[i].copy()\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.CR or j == j_rand:\n                    x_trial[j] = x_mutated[j]\n\n            # Selection\n            f_trial = func(x_trial)\n            self.budget -= 1\n            if f_trial < self.fitness[i]:\n                self.fitness[i] = f_trial\n                self.population[i] = x_trial\n                if f_trial < self.best_f:\n                    self.best_f = f_trial\n                    self.best_x = x_trial.copy()\n                    self.best_index = i\n\n    def adjust_population_size(self):\n        # Dynamically adjust population size\n        if self.budget > 0:\n            if np.std(self.fitness) < 1e-4: # Stagnation\n                self.population_size = min(self.population_size * 2, 200) #increase, max pop size of 200\n            else:\n                self.population_size = max(int(self.population_size * 0.9), 10) #decrease, min pop size of 10\n            \n            # Resize the population: add new individuals or remove the worst ones\n            if self.population_size > self.population.shape[0]:\n                 num_new = self.population_size - self.population.shape[0]\n                 new_individuals = np.random.uniform(self.population[0], self.population[-1], size=(num_new, self.dim))\n                 self.population = np.vstack([self.population, new_individuals])\n                 new_fitness = np.array([func(x) for x in new_individuals])\n                 self.fitness = np.concatenate([self.fitness, new_fitness])\n                 self.budget -= num_new\n            elif self.population_size < self.population.shape[0]:\n                num_remove = self.population.shape[0] - self.population_size\n                worst_indices = np.argsort(self.fitness)[-num_remove:]\n                self.population = np.delete(self.population, worst_indices, axis=0)\n                self.fitness = np.delete(self.fitness, worst_indices)\n            \n        if self.budget <= 0:\n            self.population_size = 1\n\n    def global_search_boost(self, func):\n        # Add a small percentage of random solutions to increase global search\n        num_new_solutions = int(0.1 * self.population_size)\n        if num_new_solutions > 0 and self.budget > 0:\n            new_solutions = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(num_new_solutions, self.dim))\n            new_fitness = np.array([func(x) for x in new_solutions])\n            self.budget -= num_new_solutions\n            self.population = np.vstack([self.population, new_solutions])\n            self.fitness = np.concatenate([self.fitness, new_fitness])\n            \n            # Update best solution\n            best_index_new = np.argmin(new_fitness)\n            if new_fitness[best_index_new] < self.best_f:\n                 self.best_f = new_fitness[best_index_new]\n                 self.best_x = new_solutions[best_index_new].copy()\n\n    def local_search(self, func):\n        # Perform local search around the best solution\n        for _ in range(min(self.local_search_iterations, self.budget)):\n            perturbation = np.random.normal(0, 0.01, size=self.dim)\n            x_local = self.best_x + perturbation\n            x_local = np.clip(x_local, func.bounds.lb, func.bounds.ub)\n            f_local = func(x_local)\n            self.budget -= 1\n            if f_local < self.best_f:\n                self.best_f = f_local\n                self.best_x = x_local.copy()\n                \n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > self.local_search_iterations:\n            self.evolve(func)\n            self.adjust_population_size()\n            self.global_search_boost(func)\n\n        self.local_search(func)\n\n        return self.best_f, self.best_x", "configspace": "", "generation": 5, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 144, in evaluate\n    algorithm(f_new)\n  File \"<string>\", line 104, in __call__\n  File \"<string>\", line 59, in adjust_population_size\n  File \"<string>\", line 59, in <listcomp>\nNameError: name 'func' is not defined\n.", "error": "", "parent_ids": ["f61c5b16-945b-4763-8c73-881a5e25148c"], "operator": null, "metadata": {}}
{"id": "1ce014ce-5d73-46a2-afc9-4c54d0c63d6f", "fitness": 0.3812911988655847, "name": "AdaptiveCauchyGaussianDE", "description": "Adaptive Differential Evolution with a combination of Cauchy and Gaussian mutations, population diversity maintenance, and adaptive parameter control.", "code": "import numpy as np\n\nclass AdaptiveCauchyGaussianDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, restart_prob=0.05, cauchy_prob=0.5, F_adaptive=True, CR_adaptive=True):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.restart_prob = restart_prob\n        self.cauchy_prob = cauchy_prob # Probability of using Cauchy mutation\n        self.F_adaptive = F_adaptive\n        self.CR_adaptive = CR_adaptive\n        self.population = None\n        self.fitness = None\n        self.best_index = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.F_history = []\n        self.CR_history = []\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index].copy()\n        self.F_history = [self.F]\n        self.CR_history = [self.CR]\n\n    def cauchy_mutation(self, x_r1, x_r2, F):\n        return x_r1 + F * np.random.standard_cauchy(size=x_r1.shape) * (x_r1 - x_r2)\n\n    def gaussian_mutation(self, x_r1, x_r2, F):\n        return x_r1 + F * np.random.normal(size=x_r1.shape) * (x_r1 - x_r2)\n\n    def update_parameters(self):\n      # Adaptive F and CR based on success history\n        if len(self.F_history) > 10:\n            self.F = np.mean(self.F_history[-10:]) # Moving average\n        if len(self.CR_history) > 10:\n            self.CR = np.mean(self.CR_history[-10:])\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                \n                # Adaptive F and CR\n                if self.F_adaptive or self.CR_adaptive:\n                  self.update_parameters()\n\n                # Choose mutation strategy\n                if np.random.rand() < self.cauchy_prob:\n                    # Cauchy mutation\n                    v = self.cauchy_mutation(x_r1, x_r2, self.F)\n                else:\n                    # Gaussian mutation\n                    v = self.gaussian_mutation(x_r1, x_r2, self.F)\n\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = self.population[i].copy()\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        u[j] = v[j]\n\n                # Evaluation\n                f_u = func(u)\n                self.budget -= 1\n\n                # Rank-based selection\n                if f_u < self.fitness[i]:\n                    self.fitness[i] = f_u\n                    self.population[i] = u\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u.copy()\n                        self.F_history.append(self.F)\n                        self.CR_history.append(self.CR)\n\n            # Restart mechanism\n            if np.random.rand() < self.restart_prob:\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.budget -= self.pop_size\n                self.best_index = np.argmin(self.fitness)\n                self.f_opt = self.fitness[self.best_index]\n                self.x_opt = self.population[self.best_index].copy()\n                self.F_history = [self.F]\n                self.CR_history = [self.CR]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveCauchyGaussianDE scored 0.381 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d33a1050-2a7c-4257-9866-e775e520e6cc"], "operator": null, "metadata": {"aucs": [0.14742466682292166, 0.24300671757224823, 0.35837993065419715, 0.34887191547607843, 0.5208760847911471, 0.4197966560361227, 0.32459545607322826, 0.4481952687349673, 0.2976545280175663, 0.20929304362806778, 0.46409846481513484, 0.9921991924955326, 0.27061118288018793, 0.4265312374323448, 0.6590446709725033, 0.3513713643126912, 0]}}
{"id": "558f0553-82c6-46f8-a3f0-6631e3a39980", "fitness": 0.3048048875140406, "name": "SelfAdaptiveNeighborhoodDE", "description": "Self-Adaptive Differential Evolution with Neighborhood-Based Mutation and a Stochastic Ranking Selection scheme.", "code": "import numpy as np\n\nclass SelfAdaptiveNeighborhoodDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_init=0.5, CR_init=0.7, neighborhood_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_init = F_init\n        self.CR_init = CR_init\n        self.neighborhood_size = neighborhood_size\n        self.population = None\n        self.fitness = None\n        self.F = None\n        self.CR = None\n        self.best_index = None\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.F = np.full(self.pop_size, self.F_init)\n        self.CR = np.full(self.pop_size, self.CR_init)\n        self.budget -= self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index].copy()\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Self-adaptive parameters\n                F = self.F[i]\n                CR = self.CR[i]\n\n                # Neighborhood-based mutation\n                neighbors = np.random.choice(self.pop_size, self.neighborhood_size, replace=False)\n                x_r1, x_r2 = self.population[np.random.choice(neighbors, 2, replace=False)]\n\n                # Mutation\n                v = self.population[i] + F * (x_r1 - x_r2)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = self.population[i].copy()\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        u[j] = v[j]\n\n                # Evaluation\n                f_u = func(u)\n                self.budget -= 1\n\n                # Stochastic Ranking Selection\n                p_f = 0.45  # Probability of comparing based on fitness\n                if np.random.rand() < p_f or self.fitness[i] < self.f_opt or f_u < self.f_opt:\n                    if f_u < self.fitness[i]:\n                        # Update successful\n                        self.fitness[i] = f_u\n                        self.population[i] = u\n                        if f_u < self.f_opt:\n                            self.f_opt = f_u\n                            self.x_opt = u.copy()\n\n                        # Adapt parameters\n                        self.F[i] = np.clip(np.random.normal(F, 0.1), 0.1, 1.0)\n                        self.CR[i] = np.clip(np.random.normal(CR, 0.1), 0.1, 1.0)\n                    else:\n                        # Adapt parameters (failure)\n                        self.F[i] = np.clip(np.random.normal(F, 0.1), 0.1, 1.0)\n                        self.CR[i] = np.clip(np.random.normal(CR, 0.1), 0.1, 1.0)\n                else:\n                    # Compare based on constraint violation (in this case, assume all constraints are satisfied, so compare randomly)\n                    if np.random.rand() < 0.5:  # 50% chance of replacing\n                        self.fitness[i] = f_u\n                        self.population[i] = u\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm SelfAdaptiveNeighborhoodDE scored 0.305 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9e6a543f-9c6c-4bd8-ae93-3e679d755242"], "operator": null, "metadata": {"aucs": [0.12767857782065128, 0.18432274417967232, 0.3018976860651623, 0.23841359081011093, 0.22198062900646964, 0.24126384063230544, 0.24686030459845199, 0.23790111342333842, 0.22052369912356629, 0.1719320291021662, 0.2575981832283606, 0.9846181294435483, 0.26563615428246734, 0.23392159519120004, 0.6739590686750894, 0.29906219063286066, 0.26028722051970477, 0.3162782929145823, 0.15505257327880884, 0.4569101273522954]}}
{"id": "0a2322ae-c329-4b4f-8e1f-71e7441e4e19", "fitness": 0.5812051661800184, "name": "AdaptiveCauchyDE", "description": "Adaptive Cauchy Differential Evolution with shrinking space and dynamic parameter tuning based on success rate.", "code": "import numpy as np\n\nclass AdaptiveCauchyDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, lr_F=0.1, lr_CR=0.1, shrink_factor=0.99):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.lr_F = lr_F\n        self.lr_CR = lr_CR\n        self.shrink_factor = shrink_factor\n        self.population = None\n        self.fitness = None\n        self.best_index = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.success_F = []\n        self.success_CR = []\n        self.archive = []\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index].copy()\n        self.lb = func.bounds.lb\n        self.ub = func.bounds.ub\n\n    def cauchy_mutation(self, x_r1, x_r2, F):\n        return x_r1 + F * np.random.standard_cauchy(size=x_r1.shape) * (x_r1 - x_r2)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            successful_mutations = 0\n            temp_success_F = []\n            temp_success_CR = []\n\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n\n                # Generate trial F and CR\n                trial_F = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n                trial_CR = np.clip(np.random.normal(self.CR, 0.1), 0.1, 1.0)\n\n                # Cauchy mutation\n                v = self.cauchy_mutation(x_r1, x_r2, trial_F)\n                v = np.clip(v, self.lb, self.ub)\n\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = self.population[i].copy()\n                for j in range(self.dim):\n                    if np.random.rand() < trial_CR or j == j_rand:\n                        u[j] = v[j]\n\n                # Evaluation\n                f_u = func(u)\n                self.budget -= 1\n\n                # Selection\n                if f_u < self.fitness[i]:\n                    successful_mutations += 1\n                    temp_success_F.append(trial_F)\n                    temp_success_CR.append(trial_CR)\n\n                    self.fitness[i] = f_u\n                    self.population[i] = u\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u.copy()\n                else:\n                    self.archive.append(u)\n\n            # Adapt F and CR\n            if temp_success_F:\n                self.F = (1 - self.lr_F) * self.F + self.lr_F * np.mean(temp_success_F)\n            if temp_success_CR:\n                self.CR = (1 - self.lr_CR) * self.CR + self.lr_CR * np.mean(temp_success_CR)\n            \n            # Shrink the search space\n            self.lb = self.shrink_factor * (self.lb - self.x_opt) + self.x_opt\n            self.ub = self.shrink_factor * (self.ub - self.x_opt) + self.x_opt\n            self.population = np.clip(self.population, self.lb, self.ub)\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveCauchyDE scored 0.581 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d33a1050-2a7c-4257-9866-e775e520e6cc"], "operator": null, "metadata": {"aucs": [0.21501448295133851, 0.32346455198429713, 0.5322830583512346, 0.7811371437707866, 0.6293100178858353, 0.6803049726074493, 0.5424348459680546, 0.5673390355835674, 0.6714001583918423, 0.6829204877380854, 0.6795133823346381, 0.9995030911494038, 0.27559629063875857, 0.6257325210266891, 0.7041465099107965, 0.7432572753032247, 0.5070392549601066, 0.7279707494158613, 0.2390485130126111, 0.496686980615787]}}
{"id": "4146cfdf-28fb-419c-8373-abc5a7f7f90d", "fitness": 0.3998486052187259, "name": "SelfAdaptiveGaussianDE", "description": "A self-adaptive Differential Evolution algorithm that adjusts its parameters based on the individual success of each member in the population, incorporating a Gaussian-distributed perturbation.", "code": "import numpy as np\n\nclass SelfAdaptiveGaussianDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_init=0.5, CR_init=0.7, learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = np.full(self.pop_size, F_init)\n        self.CR = np.full(self.pop_size, CR_init)\n        self.learning_rate = learning_rate\n        self.population = None\n        self.fitness = None\n        self.best_index = None\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index].copy()\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                v = self.population[i] + self.F[i] * (x_r1 - x_r2)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = self.population[i].copy()\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR[i] or j == j_rand:\n                        u[j] = v[j]\n                \n                # Gaussian perturbation\n                gaussian_noise = np.random.normal(0, 0.01, size=self.dim)  # Adjust scale\n                u = np.clip(u + gaussian_noise, func.bounds.lb, func.bounds.ub)\n\n                # Evaluation\n                f_u = func(u)\n                self.budget -= 1\n\n                # Selection\n                if f_u < self.fitness[i]:\n                    # Update parameters based on success\n                    delta_f = self.fitness[i] - f_u\n                    self.F[i] += self.learning_rate * (1 - self.F[i]) * delta_f  # Adaptive F\n                    self.CR[i] += self.learning_rate * (1 - self.CR[i]) * delta_f  # Adaptive CR\n\n                    self.fitness[i] = f_u\n                    self.population[i] = u\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u.copy()\n                else:\n                    # Penalize parameters if unsuccessful\n                    self.F[i] -= self.learning_rate * self.F[i] * 0.5  # Reduce F if unsuccessful\n                    self.CR[i] -= self.learning_rate * self.CR[i] * 0.5  # Reduce CR if unsuccessful\n                    \n\n                self.F[i] = np.clip(self.F[i], 0.1, 1.0)\n                self.CR[i] = np.clip(self.CR[i], 0.1, 1.0)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm SelfAdaptiveGaussianDE scored 0.400 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9e6a543f-9c6c-4bd8-ae93-3e679d755242"], "operator": null, "metadata": {"aucs": [0.15515730416449935, 0.24121201143215654, 0.40616302690554296, 0.4411040807623138, 0.33315882229140537, 0.4412642525483895, 0.3140404388922159, 0.34763207355501413, 0.32856182704021675, 0.20041210620147265, 0.4407943087949705, 0.9832937401633837, 0.2870290462888876, 0.3350885951051158, 0.7099147342007301, 0.44066939029803387, 0.330657537411729, 0.49523214098470747, 0.26880029042660303, 0.4967863769071289]}}
{"id": "2048221b-0a66-4643-9bc8-785c91f20063", "fitness": 0.519630082328869, "name": "NeighborhoodDiversityDE", "description": "A DE variant employing a neighborhood-based mutation with dynamic adaptation of the mutation factor and crossover rate based on population diversity.", "code": "import numpy as np\n\nclass NeighborhoodDiversityDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, neighborhood_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.neighborhood_size = neighborhood_size\n        self.population = None\n        self.fitness = None\n        self.best_index = None\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index].copy()\n\n    def calculate_diversity(self):\n        \"\"\"Calculates the average distance between population members.\"\"\"\n        distances = []\n        for i in range(self.pop_size):\n            for j in range(i + 1, self.pop_size):\n                distances.append(np.linalg.norm(self.population[i] - self.population[j]))\n        return np.mean(distances) if distances else 0\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            diversity = self.calculate_diversity()\n            # Adapt F and CR based on diversity\n            self.F = 0.1 + 0.9 * (diversity / (func.bounds.ub[0] - func.bounds.lb[0]))  # Scale F based on diversity\n            self.CR = 0.1 + 0.9 * np.exp(-diversity / (func.bounds.ub[0] - func.bounds.lb[0])) # Scale CR based on diversity\n\n            for i in range(self.pop_size):\n                # Neighborhood-based mutation\n                neighbors_indices = np.random.choice(self.pop_size, self.neighborhood_size, replace=False)\n                neighbors = self.population[neighbors_indices]\n                \n                # Find best neighbor\n                best_neighbor_index = np.argmin(self.fitness[neighbors_indices])\n                x_best_neighbor = neighbors[best_neighbor_index]\n\n                # Mutation using best neighbor and two other random individuals\n                indices = np.random.choice(self.pop_size, 2, replace=False)\n                x_r1, x_r2 = self.population[indices]\n                v = self.population[i] + self.F * (x_best_neighbor - self.population[i]) + self.F * (x_r1 - x_r2)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = self.population[i].copy()\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        u[j] = v[j]\n\n                # Evaluation\n                f_u = func(u)\n                self.budget -= 1\n\n                # Selection\n                if f_u < self.fitness[i]:\n                    self.fitness[i] = f_u\n                    self.population[i] = u\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u.copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm NeighborhoodDiversityDE scored 0.520 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9e6a543f-9c6c-4bd8-ae93-3e679d755242"], "operator": null, "metadata": {"aucs": [0.2183886134191727, 0.6515437612051141, 0.37130900797287014, 0.8946500181025414, 0.4628652524348572, 0.4082878836146939, 0.3335137288477541, 0.43127703892857383, 0.4506473644323974, 0.6448702914170155, 0.9007795994987708, 0.9978235072718338, 0.27321367649347417, 0.4986803872625061, 0.6912599520190117, 0.38102802753990905, 0.46419394104038425, 0.6247285036168091, 0.20215436412137378, 0.4913867273383137]}}
{"id": "0f4cbde0-4808-43f9-99d2-f20d048048e8", "fitness": -Infinity, "name": "CooperativeOrthogonalDE", "description": "Cooperative Differential Evolution with orthogonal learning and dynamic resource allocation based on subspace performance.", "code": "import numpy as np\n\nclass CooperativeOrthogonalDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, num_subspaces=5, F_init=0.5, CR_init=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.num_subspaces = num_subspaces\n        self.F = np.full(self.pop_size, F_init)\n        self.CR = np.full(self.pop_size, CR_init)\n        self.population = None\n        self.fitness = None\n        self.best_index = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.subspace_dims = [np.random.choice(dim, size=dim // num_subspaces, replace=False) for _ in range(num_subspaces)]\n        self.subspace_success = np.zeros(num_subspaces)  # Track success of each subspace\n        self.subspace_evals = np.zeros(num_subspaces)  # Track evaluations in each subspace\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index].copy()\n\n    def orthogonal_learning(self, x, func, subspace_idx):\n        \"\"\"\n        Performs orthogonal learning within a subspace.\n        \"\"\"\n        subspace = self.subspace_dims[subspace_idx]\n        num_samples = min(10, self.budget // self.num_subspaces) # Dynamic number of samples\n        if num_samples <= 0:\n            return x\n\n        best_x = x.copy()\n        best_f = func(x)\n        self.budget -= 1\n        self.subspace_evals[subspace_idx] += 1\n\n        for _ in range(num_samples):\n            x_new = x.copy()\n            for dim_idx in subspace:\n                x_new[dim_idx] = np.random.uniform(func.bounds.lb, func.bounds.ub)\n            f_new = func(x_new)\n            self.budget -= 1\n            self.subspace_evals[subspace_idx] += 1\n\n            if f_new < best_f:\n                best_f = f_new\n                best_x = x_new.copy()\n                self.subspace_success[subspace_idx] += 1  # Subspace was useful\n\n        return best_x\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Choose subspace based on success rate (dynamic resource allocation)\n                subspace_idx = np.argmax(self.subspace_success / (self.subspace_evals + 1e-9)) #Exploitation bias, favor subspaces with better success rates.\n\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                v = self.population[i] + self.F[i] * (x_r1 - x_r2)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = self.population[i].copy()\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR[i] or j == j_rand:\n                        u[j] = v[j]\n                u = np.clip(u, func.bounds.lb, func.bounds.ub)\n\n                # Orthogonal learning in the selected subspace\n                u = self.orthogonal_learning(u, func, subspace_idx)\n\n                # Evaluation\n                f_u = func(u)\n                self.budget -= 1\n\n                # Selection\n                if f_u < self.fitness[i]:\n                    self.fitness[i] = f_u\n                    self.population[i] = u\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u.copy()\n\n                    # Update F and CR (simple adaptation)\n                    self.F[i] = np.clip(self.F[i] + 0.1 * np.random.normal(), 0.1, 1.0)\n                    self.CR[i] = np.clip(self.CR[i] + 0.1 * np.random.normal(), 0.1, 1.0)\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "An exception occured: TypeError: only size-1 arrays can be converted to Python scalars\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 144, in evaluate\n    algorithm(f_new)\n  File \"<string>\", line 80, in __call__\n  File \"<string>\", line 45, in orthogonal_learning\nValueError: setting an array element with a sequence.\n.", "error": "", "parent_ids": ["4146cfdf-28fb-419c-8373-abc5a7f7f90d"], "operator": null, "metadata": {}}
{"id": "4d792535-4420-4220-87c8-0eee605d56ab", "fitness": 0.0, "name": "AdaptivePopulationDE", "description": "Differential Evolution with a dynamically adjusted population size based on stagnation detection and a restart mechanism.", "code": "import numpy as np\n\nclass AdaptivePopulationDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, F=0.5, CR=0.7, stagnation_threshold=1000, pop_size_increment=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.initial_pop_size = initial_pop_size\n        self.F = F\n        self.CR = CR\n        self.stagnation_threshold = stagnation_threshold\n        self.pop_size_increment = pop_size_increment\n        self.population = None\n        self.fitness = None\n        self.best_index = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.stagnation_counter = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index].copy()\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            # Mutation and Crossover\n            for i in range(self.pop_size):\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                v = self.population[i] + self.F * (x_r1 - x_r2)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                j_rand = np.random.randint(self.dim)\n                u = self.population[i].copy()\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        u[j] = v[j]\n\n                # Evaluation\n                f_u = func(u)\n                self.budget -= 1\n\n                # Selection\n                if f_u < self.fitness[i]:\n                    self.fitness[i] = f_u\n                    self.population[i] = u\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u.copy()\n                        self.stagnation_counter = 0  # Reset stagnation counter\n                else:\n                    self.stagnation_counter += 1\n\n            # Stagnation Check and Population Size Adjustment\n            if self.stagnation_counter > self.stagnation_threshold:\n                # Increase population size\n                self.pop_size += self.pop_size_increment\n                # Add new individuals to the population (randomly initialized)\n                new_population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size_increment, self.dim))\n                new_fitness = np.array([func(x) for x in new_population])\n                self.budget -= self.pop_size_increment\n\n                self.population = np.vstack((self.population, new_population))\n                self.fitness = np.concatenate((self.fitness, new_fitness))\n                \n                #Update best solution\n                best_index_new = np.argmin(self.fitness)\n                self.f_opt = self.fitness[best_index_new]\n                self.x_opt = self.population[best_index_new].copy()\n\n                self.stagnation_counter = 0  # Reset stagnation counter\n\n                if self.budget <= 0:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptivePopulationDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2048221b-0a66-4643-9bc8-785c91f20063"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "b71df3fd-c6f2-478f-b6c4-187cba9bdbaf", "fitness": 0.0, "name": "AgingDE", "description": "Differential Evolution with an aging mechanism that replaces the oldest individuals to promote exploration and a dynamic CR adaptation based on population fitness variance.", "code": "import numpy as np\n\nclass AgingDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, age_limit=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.age_limit = age_limit\n        self.population = None\n        self.fitness = None\n        self.ages = None  # Keep track of the age of each individual\n        self.best_index = None\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.ages = np.zeros(self.pop_size, dtype=int)  # Initialize ages to 0\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index].copy()\n\n    def calculate_fitness_variance(self):\n        return np.var(self.fitness)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            fitness_variance = self.calculate_fitness_variance()\n            self.CR = 0.1 + 0.8 * np.exp(-fitness_variance) # Adapt CR based on fitness variance.\n\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                v = self.population[i] + self.F * (x_r1 - x_r2) + self.F * (x_r3 - self.population[i])  # Add current individual to the mutation\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = self.population[i].copy()\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        u[j] = v[j]\n\n                # Evaluation\n                f_u = func(u)\n                self.budget -= 1\n\n                # Selection\n                if f_u < self.fitness[i]:\n                    self.fitness[i] = f_u\n                    self.population[i] = u\n                    self.ages[i] = 0  # Reset age if improved\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u.copy()\n                else:\n                    self.ages[i] += 1  # Increase age if not improved\n\n            # Aging mechanism: replace oldest individuals\n            for i in range(self.pop_size):\n                if self.ages[i] > self.age_limit:\n                    self.population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                    self.fitness[i] = func(self.population[i])\n                    self.budget -= 1\n                    self.ages[i] = 0  # Reset age\n                    if self.fitness[i] < self.f_opt:\n                        self.f_opt = self.fitness[i]\n                        self.x_opt = self.population[i].copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AgingDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2048221b-0a66-4643-9bc8-785c91f20063"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "4f3518e0-40f8-4439-9629-7f41ee91e1f1", "fitness": 0.0, "name": "AgingDE", "description": "Differential Evolution with an aging mechanism that replaces the oldest individuals and dynamically adjusts mutation strength based on the population's stagnation.", "code": "import numpy as np\n\nclass AgingDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, age_limit=50, stagnation_threshold=1e-6):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.age_limit = age_limit\n        self.stagnation_threshold = stagnation_threshold\n        self.population = None\n        self.fitness = None\n        self.ages = None\n        self.best_index = None\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.ages = np.zeros(self.pop_size, dtype=int)\n        self.budget -= self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index].copy()\n        self.lb = func.bounds.lb\n        self.ub = func.bounds.ub\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            old_best_fitness = self.f_opt\n\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n\n                # Stagnation check and F adaptation\n                if np.abs(self.f_opt - old_best_fitness) < self.stagnation_threshold:\n                    self.F = min(self.F + 0.1, 1.0)  # Increase mutation strength if stagnating\n                else:\n                    self.F = max(self.F - 0.05, 0.1)  # Decrease mutation strength if improving\n\n                v = x_r1 + self.F * (x_r2 - x_r3)\n                v = np.clip(v, self.lb, self.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = self.population[i].copy()\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        u[j] = v[j]\n\n                # Evaluation\n                f_u = func(u)\n                self.budget -= 1\n\n                # Selection\n                if f_u < self.fitness[i]:\n                    self.fitness[i] = f_u\n                    self.population[i] = u\n                    self.ages[i] = 0  # Reset age\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u.copy()\n                        self.best_index = i\n                else:\n                    self.ages[i] += 1  # Increment age\n\n            # Aging: Replace oldest individuals\n            for i in range(self.pop_size):\n                if self.ages[i] > self.age_limit:\n                    self.population[i] = np.random.uniform(self.lb, self.ub, size=self.dim)\n                    self.fitness[i] = func(self.population[i])\n                    self.budget -=1\n                    self.ages[i] = 0\n                    if self.fitness[i] < self.f_opt:\n                        self.f_opt = self.fitness[i]\n                        self.x_opt = self.population[i].copy()\n                        self.best_index = i\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AgingDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0a2322ae-c329-4b4f-8e1f-71e7441e4e19"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "39b91ff7-0dcc-4753-8e96-fa0a5f9b3ef9", "fitness": -Infinity, "name": "BudgetAwareCMAES", "description": "Covariance Matrix Adaptation Evolution Strategy (CMA-ES) with budget awareness and restarts.", "code": "import numpy as np\n\nclass BudgetAwareCMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, sigma_init=0.5, restart_trigger=3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.sigma = sigma_init\n        self.mean = None\n        self.C = None\n        self.restart_trigger = restart_trigger\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.success_counter = 0\n        self.mu = self.pop_size // 2  # Number of individuals for recombination\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n        self.cc = (4 + self.mueff/self.dim) / (self.dim + 4 + 2*self.mueff/self.dim)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1-self.c1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2.0)**2 + self.mueff))\n        self.damps = 1 + 2*max(0, np.sqrt((self.mueff-1)/(self.dim+1)) - 1) + self.cs\n\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.B = None\n        self.D = None\n        self.eigen_updated = 0\n\n    def initialize(self, func):\n        self.mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.B = np.eye(self.dim)\n        self.D = np.ones(self.dim)\n        self.eigen_updated = 0\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n\n    def sample_population(self, func):\n        z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n        y = self.B.dot(self.D * z.T).T\n        x = self.mean + self.sigma * y\n        x = np.clip(x, func.bounds.lb, func.bounds.ub)\n        return x\n\n    def update_distribution(self, x, fitness):\n        # Sort by fitness\n        indices = np.argsort(fitness)\n        x_sorted = x[indices]\n\n        # Update mean\n        y = (x_sorted[:self.mu] - self.mean) / self.sigma\n        self.mean += self.sigma * np.sum(self.weights[:, None] * y, axis=0)\n\n        # Update evolution path\n        self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * (self.B @ self.D) @ np.mean(np.random.normal(0, 1, size=(self.mu, self.dim)), axis=0)\n        hsig = np.linalg.norm(self.ps)/np.sqrt(1-(1-self.cs)**(2*self.budget/self.pop_size))/np.sqrt(self.dim+1) < 1.4 + 2/(self.dim+1)\n        self.pc = (1 - self.cc) * self.pc + hsig * np.sqrt(self.cc * (2 - self.cc) * self.mueff) * y[0]\n\n        # Update covariance matrix\n        self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * (self.pc[:, None] @ self.pc[None, :]) + self.cmu * np.sum(self.weights[:,None,None] * y[:,:,None] @ y[:,None,:], axis=0)\n\n        self.sigma *= np.exp((self.cs/self.damps)*(np.linalg.norm(self.ps)/np.sqrt(self.dim)/self.chiN - 1))\n        self.sigma = np.clip(self.sigma, 1e-10, 5)\n\n        self.eigen_updated += 1\n        if self.eigen_updated > self.pop_size / (self.c1 + self.cmu) / self.dim / 10:\n            self.eigen_decomposition()\n            self.eigen_updated = 0\n\n    def eigen_decomposition(self):\n        self.C = np.triu(self.C) + np.triu(self.C, 1).T\n        self.D, self.B = np.linalg.eigh(self.C)\n        self.D = np.sqrt(np.maximum(self.D, 1e-10))\n\n    def __call__(self, func):\n        self.chiN = self.dim**0.5*(1-1/(4*self.dim)+1/(21*self.dim**2))\n        self.initialize(func)\n        \n        iteration = 0\n        while self.budget > self.pop_size:\n            iteration += 1\n            # Sample population\n            x = self.sample_population(func)\n            \n            # Evaluate population\n            fitness = np.array([func(xi) for xi in x])\n            self.budget -= self.pop_size\n\n            # Update best solution\n            best_index_batch = np.argmin(fitness)\n            if fitness[best_index_batch] < self.f_opt:\n                self.f_opt = fitness[best_index_batch]\n                self.x_opt = x[best_index_batch].copy()\n                self.success_counter = 0\n            else:\n                self.success_counter +=1\n\n            # Update distribution parameters\n            self.update_distribution(x, fitness)\n\n            if self.success_counter > self.restart_trigger * self.dim: #Restarting criterion based on stagnation\n                self.initialize(func)\n                self.sigma = 0.5 # Resetting sigma\n                self.success_counter = 0 # Resetting the stagnation counter\n        \n        # Final evaluation (optional, if budget allows)\n        if self.budget > 0:\n            x = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(min(self.budget,self.pop_size), self.dim))\n            fitness = np.array([func(xi) for xi in x])\n            best_index_batch = np.argmin(fitness)\n            if fitness[best_index_batch] < self.f_opt:\n                self.f_opt = fitness[best_index_batch]\n                self.x_opt = x[best_index_batch].copy()\n            self.budget = 0 # making sure budget is not exceeded.\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 86, in __call__\n  File \"<string>\", line 43, in sample_population\nValueError: operands could not be broadcast together with shapes (2,) (2,6) \n.", "error": "", "parent_ids": ["4146cfdf-28fb-419c-8373-abc5a7f7f90d"], "operator": null, "metadata": {}}
{"id": "04c07c6d-c173-4a96-9819-ed0a5ffd4c95", "fitness": -Infinity, "name": "DynamicPopulationDE", "description": "Differential Evolution with a dynamically adjusted population size based on stagnation detection and a diversity-enhancing mutation strategy.", "code": "import numpy as np\n\nclass DynamicPopulationDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, F=0.5, CR=0.7, stagnation_threshold=100, pop_increase_factor=1.2, pop_decrease_factor=0.5, diversity_prob=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.initial_pop_size = initial_pop_size\n        self.F = F\n        self.CR = CR\n        self.stagnation_threshold = stagnation_threshold\n        self.pop_increase_factor = pop_increase_factor\n        self.pop_decrease_factor = pop_decrease_factor\n        self.diversity_prob = diversity_prob\n        self.population = None\n        self.fitness = None\n        self.best_index = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.stagnation_counter = 0\n        self.fitness_history = []\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index].copy()\n        self.fitness_history = [self.f_opt]\n\n    def diversity_mutation(self, i):\n        # Select two distinct random indices\n        indices = np.random.choice(self.pop_size, 2, replace=False)\n        x_r1, x_r2 = self.population[indices]\n\n        # Calculate the difference vector\n        diff_vector = x_r1 - x_r2\n\n        # Add the difference vector to the current individual\n        mutated_vector = self.population[i] + self.F * diff_vector\n\n        return mutated_vector\n\n\n    def adjust_population_size(self):\n        if len(self.fitness_history) > self.stagnation_threshold:\n            if np.std(self.fitness_history[-self.stagnation_threshold:]) < 1e-6:\n                # Stagnation detected\n                self.stagnation_counter += 1\n                if self.stagnation_counter >= 2:\n                    new_pop_size = int(self.pop_size * self.pop_increase_factor)\n                    new_pop_size = min(new_pop_size, 2 * self.initial_pop_size)  # Limit pop size\n                    \n                    # Increase population size\n                    new_population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(new_pop_size - self.pop_size, self.dim))\n                    new_fitness = np.array([func(x) for x in new_population])\n                    self.budget -= (new_pop_size - self.pop_size)\n                    \n                    self.population = np.vstack((self.population, new_population))\n                    self.fitness = np.concatenate((self.fitness, new_fitness))\n                    self.pop_size = new_pop_size\n                    self.best_index = np.argmin(self.fitness)\n                    self.f_opt = self.fitness[self.best_index]\n                    self.x_opt = self.population[self.best_index].copy()\n                    self.stagnation_counter = 0  # Reset stagnation counter\n                    print(f\"Population increased to {self.pop_size}\") #Debug\n\n            else:\n              self.stagnation_counter = 0\n        \n        if self.pop_size > self.initial_pop_size and self.budget < self.budget*0.1: #Reduce pop size if close to budget end\n          new_pop_size = int(self.pop_size * self.pop_decrease_factor)\n          new_pop_size = max(new_pop_size, self.initial_pop_size)\n\n          indices = np.argsort(self.fitness)[:new_pop_size]\n          self.population = self.population[indices]\n          self.fitness = self.fitness[indices]\n          self.pop_size = new_pop_size\n          self.best_index = np.argmin(self.fitness)\n          self.f_opt = self.fitness[self.best_index]\n          self.x_opt = self.population[self.best_index].copy()\n          print(f\"Population decreased to {self.pop_size}\") #Debug\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            self.adjust_population_size()\n\n            for i in range(self.pop_size):\n                # Mutation\n                if np.random.rand() < self.diversity_prob:\n                    v = self.diversity_mutation(i)\n                else:\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    v = x_r1 + self.F * (x_r2 - x_r3)\n\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = self.population[i].copy()\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        u[j] = v[j]\n\n                # Evaluation\n                f_u = func(u)\n                self.budget -= 1\n\n                # Selection\n                if f_u < self.fitness[i]:\n                    self.fitness[i] = f_u\n                    self.population[i] = u\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u.copy()\n                \n            self.fitness_history.append(self.f_opt)\n            self.best_index = np.argmin(self.fitness)\n            self.f_opt = self.fitness[self.best_index]\n            self.x_opt = self.population[self.best_index].copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 144, in evaluate\n    algorithm(f_new)\n  File \"<string>\", line 89, in __call__\n  File \"<string>\", line 56, in adjust_population_size\nNameError: name 'func' is not defined\n.", "error": "", "parent_ids": ["1ce014ce-5d73-46a2-afc9-4c54d0c63d6f"], "operator": null, "metadata": {}}
{"id": "3b5763e8-115f-49c4-a464-258a3eae77a5", "fitness": 0.0, "name": "DynamicPopulationDE", "description": "Differential Evolution with dynamic population sizing and adaptive mutation based on the success history of individuals.", "code": "import numpy as np\n\nclass DynamicPopulationDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, min_pop_size=10, max_pop_size=100, F=0.5, CR=0.7, adapt_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = initial_pop_size\n        self.min_pop_size = min_pop_size\n        self.max_pop_size = max_pop_size\n        self.F = F\n        self.CR = CR\n        self.adapt_prob = adapt_prob  # Probability of adapting F and CR\n        self.population = None\n        self.fitness = None\n        self.success_history = None\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.initial_pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.initial_pop_size\n        self.success_history = np.zeros(self.initial_pop_size)  # Initialize success history for each individual\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n\n    def adjust_population_size(self):\n        \"\"\"Adjust population size based on overall population success.\"\"\"\n        success_rate = np.mean(self.success_history)\n        if success_rate > 0.6 and len(self.population) < self.max_pop_size:\n            # Increase population size if success rate is high\n            num_new = min(int(len(self.population) * 0.2), self.max_pop_size - len(self.population))\n            if num_new > 0:\n                new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(num_new, self.dim))\n                new_fitness = np.array([func(x) for x in new_individuals])\n                self.budget -= num_new\n                self.population = np.vstack((self.population, new_individuals))\n                self.fitness = np.concatenate((self.fitness, new_fitness))\n                self.success_history = np.concatenate((self.success_history, np.zeros(num_new))) # Initialize success history\n        elif success_rate < 0.2 and len(self.population) > self.min_pop_size:\n            # Decrease population size if success rate is low\n            num_remove = min(int(len(self.population) * 0.2), len(self.population) - self.min_pop_size)\n            if num_remove > 0:\n                #Remove worst performing individuals\n                worst_indices = np.argsort(self.fitness)[-num_remove:]\n                keep_indices = np.setdiff1d(np.arange(len(self.population)), worst_indices)\n                self.population = self.population[keep_indices]\n                self.fitness = self.fitness[keep_indices]\n                self.success_history = self.success_history[keep_indices]\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            self.adjust_population_size()\n\n            for i in range(len(self.population)):\n                # Mutation\n                indices = np.random.choice(len(self.population), 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                \n                # Adapt F based on individual success history\n                if np.random.rand() < self.adapt_prob:\n                    self.F = 0.1 + 0.9 * self.success_history[i]\n\n                v = self.population[i] + self.F * (x_r1 - x_r2) + self.F * (x_r3 - self.population[i])\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = self.population[i].copy()\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        u[j] = v[j]\n\n                # Evaluation\n                f_u = func(u)\n                self.budget -= 1\n\n                # Selection\n                if f_u < self.fitness[i]:\n                    self.success_history[i] = 0.9 * self.success_history[i] + 0.1 # Update success history\n                    self.fitness[i] = f_u\n                    self.population[i] = u\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u.copy()\n                else:\n                    self.success_history[i] *= 0.9 # Decrease the success if the trial was not succesful\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm DynamicPopulationDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2048221b-0a66-4643-9bc8-785c91f20063"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "59ae7095-5529-4bcf-8d91-981c7cceb922", "fitness": -Infinity, "name": "OrthogonalDE", "description": "Differential Evolution with orthogonal learning and adaptive mutation scaling, leveraging historical best information and a dynamic population size adjustment.", "code": "import numpy as np\n\nclass OrthogonalDE:\n    def __init__(self, budget=10000, dim=10, pop_size=None, F=0.5, CR=0.7, ortho_trials=3, pop_size_factor=2, adaptation_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else int(4 + 3 * np.log(dim))\n        self.F = F\n        self.CR = CR\n        self.ortho_trials = ortho_trials\n        self.pop_size_factor = pop_size_factor\n        self.adaptation_rate = adaptation_rate\n        self.population = None\n        self.fitness = None\n        self.best_index = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.archive = []\n        self.historical_best_rate = 0.1\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index].copy()\n\n    def orthogonal_learning(self, func, mutant):\n        best_fitness = np.inf\n        best_mutant = None\n        for _ in range(self.ortho_trials):\n            orthogonal_mutant = mutant.copy()\n            j = np.random.randint(self.dim)\n            orthogonal_mutant[j] = np.random.uniform(func.bounds.lb, func.bounds.ub)\n            orthogonal_mutant = np.clip(orthogonal_mutant, func.bounds.lb, func.bounds.ub)\n            fitness = func(orthogonal_mutant)\n            self.budget -= 1\n            if fitness < best_fitness:\n                best_fitness = fitness\n                best_mutant = orthogonal_mutant\n        return best_mutant, best_fitness\n\n    def adaptive_mutation_scaling(self):\n        self.F = np.clip(np.random.normal(0.5, 0.3), 0.1, 1.0)\n        self.CR = np.clip(np.random.normal(0.7, 0.2), 0.1, 1.0)\n\n    def historical_best_mutation(self, x_target):\n        if len(self.archive) > 0 and np.random.rand() < self.historical_best_rate:\n            x_best_historical = self.archive[np.argmin([func(x) for x in self.archive])]\n            return self.F * (x_best_historical - x_target)\n        else:\n            return 0.0\n\n    def dynamic_population_size(self, improvement_rate):\n        if improvement_rate > 0.1:\n            self.pop_size = int(self.pop_size * (1 + self.adaptation_rate))\n        elif improvement_rate < 0.01:\n            self.pop_size = int(self.pop_size * (1 - self.adaptation_rate))\n        self.pop_size = max(4, min(self.pop_size, self.dim * self.pop_size_factor))\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        generation = 0\n\n        while self.budget > 0:\n            generation += 1\n            improvement_count = 0\n\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n\n                mutation_vector = self.F * (x_r2 - x_r3) + self.historical_best_mutation(self.population[i])\n                v = self.population[i] + mutation_vector\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = self.population[i].copy()\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        u[j] = v[j]\n\n                # Orthogonal Learning\n                u, f_u = self.orthogonal_learning(func, u)\n\n                # Selection\n                if f_u < self.fitness[i]:\n                    improvement_count += 1\n                    self.fitness[i] = f_u\n                    self.population[i] = u\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u.copy()\n                        if len(self.archive) < 100:\n                            self.archive.append(u.copy())\n                        else:\n                            self.archive[np.random.randint(100)] = u.copy()\n\n            # Adaptive F and CR\n            self.adaptive_mutation_scaling()\n\n            # Dynamic Population Size\n            improvement_rate = improvement_count / self.pop_size\n            self.dynamic_population_size(improvement_rate)\n            self.pop_size = int(self.pop_size)\n\n            if self.pop_size != self.population.shape[0]:\n                # Resize the population (either by random initialization or truncation)\n                if self.pop_size > self.population.shape[0]:\n                    new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size - self.population.shape[0], self.dim))\n                    new_fitness = np.array([func(x) for x in new_individuals])\n                    self.budget -= new_individuals.shape[0]\n                    self.population = np.vstack((self.population, new_individuals))\n                    self.fitness = np.concatenate((self.fitness, new_fitness))\n                else:\n                    #Truncate population\n                    indices_to_keep = np.argsort(self.fitness)[:self.pop_size]\n                    self.population = self.population[indices_to_keep]\n                    self.fitness = self.fitness[indices_to_keep]\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "An exception occured: TypeError: only size-1 arrays can be converted to Python scalars\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 87, in __call__\n  File \"<string>\", line 35, in orthogonal_learning\nValueError: setting an array element with a sequence.\n.", "error": "", "parent_ids": ["1ce014ce-5d73-46a2-afc9-4c54d0c63d6f"], "operator": null, "metadata": {}}
{"id": "16728a8b-b73f-406a-90f8-787e08fbc64e", "fitness": 0.42165882673692395, "name": "CrowdingDE", "description": "Differential Evolution with a combined mutation strategy that blends current-to-best mutation with a Cauchy mutation, alongside a self-adaptive learning rate for both F and CR, and a diversity maintenance scheme based on crowding distance.", "code": "import numpy as np\n\nclass CrowdingDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, F_lr=0.1, CR_lr=0.1, current_to_best_prob=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.F_lr = F_lr\n        self.CR_lr = CR_lr\n        self.current_to_best_prob = current_to_best_prob\n        self.population = None\n        self.fitness = None\n        self.best_index = None\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index].copy()\n\n    def current_to_best_mutation(self, x, x_best, x_r1, F):\n        return x + F * (x_best - x) + F * (x_r1 - x)\n\n    def cauchy_mutation(self, x_r1, x_r2, F):\n        return x_r1 + F * np.random.standard_cauchy(size=x_r1.shape) * (x_r1 - x_r2)\n\n    def calculate_crowding_distance(self):\n        distances = np.zeros(self.pop_size)\n        for m in range(self.dim):\n            # Sort population based on the m-th dimension\n            sorted_indices = np.argsort(self.population[:, m])\n            \n            # Boundary individuals get maximum distance\n            distances[sorted_indices[0]] = np.inf\n            distances[sorted_indices[-1]] = np.inf\n            \n            # Calculate distance for intermediate individuals\n            for i in range(1, self.pop_size - 1):\n                distances[sorted_indices[i]] += (self.population[sorted_indices[i+1], m] - self.population[sorted_indices[i-1], m])\n\n        return distances\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            crowding_distances = self.calculate_crowding_distance()\n\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 2, replace=False)\n                x_r1, x_r2 = self.population[indices]\n\n                if np.random.rand() < self.current_to_best_prob:\n                    # Current-to-best mutation\n                    v = self.current_to_best_mutation(self.population[i], self.population[self.best_index], x_r1, self.F)\n                else:\n                    # Cauchy mutation\n                    v = self.cauchy_mutation(x_r1, x_r2, self.F)\n\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = self.population[i].copy()\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        u[j] = v[j]\n\n                # Evaluation\n                f_u = func(u)\n                self.budget -= 1\n\n                # Selection based on crowding distance\n                if f_u < self.fitness[i] or (f_u == self.fitness[i] and crowding_distances[i] < crowding_distances[i]):\n                    self.fitness[i] = f_u\n                    self.population[i] = u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u.copy()\n\n                        #Update F and CR learning rate using a simple rule\n                        if f_u < self.fitness[i]:\n                            self.F = max(0.1, min(0.9, self.F + self.F_lr * (np.random.rand() - 0.5)))\n                            self.CR = max(0.1, min(0.9, self.CR + self.CR_lr * (np.random.rand() - 0.5)))\n                \n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm CrowdingDE scored 0.422 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["1ce014ce-5d73-46a2-afc9-4c54d0c63d6f"], "operator": null, "metadata": {"aucs": [0.22160242980332312, 0.17577805764816712, 0.44049772722854297, 0.7369641958431925, 0.28520746367743266, 0.5387877045520156, 0.3243450130686587, 0.3556636703176502, 0.27700513154276285, 0.19995455875516344, 0.5372285138018478, 0.9944448156167588, 0.21582996506241414, 0.31060492071850254, 0.6647183190042592, 0.6636640485510397, 0.2754960690660919, 0.5355647120793385, 0.1939645204339293, 0.48585469796738723]}}
{"id": "954550bc-37d6-4ff7-a9c3-c4ae7214590b", "fitness": 0.2604487356269166, "name": "OrthogonalNichingDE", "description": "Differential Evolution with orthogonal learning, self-adaptive parameters, and a niching strategy to enhance exploration and diversity.", "code": "import numpy as np\n\nclass OrthogonalNichingDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, num_niches=5, niche_radius=0.5, F_adaptive=True, CR_adaptive=True):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.num_niches = num_niches\n        self.niche_radius = niche_radius\n        self.F_adaptive = F_adaptive\n        self.CR_adaptive = CR_adaptive\n        self.population = None\n        self.fitness = None\n        self.niches = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.success_F = []\n        self.success_CR = []\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.niches = self.initialize_niches(func)\n        self.update_niches(func)\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n        self.success_F = []\n        self.success_CR = []\n\n    def initialize_niches(self, func):\n        niches = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.num_niches, self.dim))\n        return niches\n    \n    def assign_to_niches(self):\n        assignments = np.argmin(np.linalg.norm(self.population[:, np.newaxis, :] - self.niches[np.newaxis, :, :], axis=2), axis=1)\n        return assignments\n\n    def update_niches(self, func):\n        assignments = self.assign_to_niches()\n        for i in range(self.num_niches):\n            members = self.population[assignments == i]\n            if len(members) > 0:\n                self.niches[i] = np.mean(members, axis=0)\n\n    def orthogonal_learning(self, x):\n        # Generate an orthogonal array\n        oa = self.generate_orthogonal_array(self.dim)\n        \n        # Sample points based on the orthogonal array\n        samples = []\n        for row in oa:\n            new_x = x.copy()\n            for i, val in enumerate(row):\n                if val == 1:\n                    new_x[i] = np.random.uniform(x[i], 5.0) # Explore upper bound\n                else:\n                    new_x[i] = np.random.uniform(-5.0, x[i]) # Explore lower bound\n            samples.append(new_x)\n        return samples\n\n    def generate_orthogonal_array(self, dim):\n        # A simple example: a 2-level orthogonal array\n        oa = np.zeros((dim + 1, dim))\n        for i in range(dim):\n            oa[0, i] = 0\n            oa[i + 1, i] = 1\n        return oa\n\n    def mutation(self, func, i, assignment):\n        members = self.population[self.assign_to_niches() == assignment]\n        if len(members) < 3:\n            indices = np.random.choice(self.pop_size, 3, replace=True)\n            x_r1, x_r2, x_r3 = self.population[indices]\n        else:\n             indices = np.random.choice(len(members), 3, replace=False)\n             x_r1, x_r2, x_r3 = members[indices]\n        \n        v = self.population[i] + self.F * (x_r1 - x_r2)\n        v = np.clip(v, func.bounds.lb, func.bounds.ub)\n        return v\n\n    def crossover(self, v, i):\n        j_rand = np.random.randint(self.dim)\n        u = self.population[i].copy()\n        for j in range(self.dim):\n            if np.random.rand() < self.CR or j == j_rand:\n                u[j] = v[j]\n        return u\n\n    def update_parameters(self, success):\n        if self.F_adaptive:\n            if success and self.success_F:\n                self.F = 0.9 * self.F + 0.1 * np.mean(self.success_F)\n            else:\n                self.F = np.clip(np.random.normal(0.5, 0.3), 0.1, 1.0)\n            self.F = np.clip(self.F, 0.1, 1.0)\n        if self.CR_adaptive:\n            if success and self.success_CR:\n                self.CR = 0.9 * self.CR + 0.1 * np.mean(self.success_CR)\n            else:\n                self.CR = np.clip(np.random.normal(0.7, 0.3), 0.1, 1.0)\n            self.CR = np.clip(self.CR, 0.1, 1.0)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            assignments = self.assign_to_niches()\n            for i in range(self.pop_size):\n                assignment = assignments[i]\n\n                # Mutation and Crossover\n                v = self.mutation(func, i, assignment)\n                u = self.crossover(v, i)\n\n                # Orthogonal Learning\n                ol_samples = self.orthogonal_learning(u)\n                \n                # Evaluate Orthogonal Learning Samples, use budget efficiently\n                ol_fitnesses = []\n                num_evals = min(len(ol_samples), self.budget)\n                for k in range(num_evals):\n                  f_ol = func(ol_samples[k])\n                  ol_fitnesses.append(f_ol)\n                  self.budget -=1\n                \n                # if no budget left, break\n                if self.budget <= 0:\n                    break\n\n                # Best fitness from OL samples\n                if ol_fitnesses: # to avoid errors if empty\n                  best_ol_index = np.argmin(ol_fitnesses)\n                  f_u = ol_fitnesses[best_ol_index]\n                  u = ol_samples[best_ol_index]\n                else: # to ensure f_u gets assigned a value, use function evaluation only if budget is left\n                  f_u = func(u)\n                  self.budget -=1 #ensure this is done only when needed\n\n                # Selection\n                success = False\n                if f_u < self.fitness[i]:\n                    self.fitness[i] = f_u\n                    self.population[i] = u\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u.copy()\n                        self.success_F.append(self.F)\n                        self.success_CR.append(self.CR)\n                    success = True\n                \n                # Adapt parameters\n                self.update_parameters(success)\n            \n            if self.budget > 0: #update only if enough budget\n                self.update_niches(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm OrthogonalNichingDE scored 0.260 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["1ce014ce-5d73-46a2-afc9-4c54d0c63d6f"], "operator": null, "metadata": {"aucs": [0.1094567597281988, 0.1694978404640467, 0.23793099674932305, 0.21007085699691508, 0.19377573873750664, 0.23600778153418844, 0.22320225875750432, 0.20100475873623813, 0.1881980857335096, 0.1639552647496424, 0.1778180483408005, 0.9979897330116287, 0.23319713980942247, 0.2122351435731522, 0.35149257535729317, 0.24811770872365635, 0.19549987682809167, 0.27423112927457083, 0.1346227687474193, 0.45067024668522315]}}
{"id": "7275b07c-5ed3-4b00-af0f-7de5c1bf2808", "fitness": 0.6959689759669937, "name": "MultiMutationDE", "description": "A Differential Evolution strategy employing a pool of mutation operators and adaptive selection among them based on their recent success.", "code": "import numpy as np\n\nclass MultiMutationDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, mutation_strategies=None, selection_pressure=2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.mutation_strategies = mutation_strategies if mutation_strategies is not None else [\n            self.mutation_DE_rand1,\n            self.mutation_DE_best1,\n            self.mutation_DE_current_to_rand1,\n            self.mutation_DE_current_to_best1,\n        ]\n        self.num_strategies = len(self.mutation_strategies)\n        self.strategy_successes = np.ones(self.num_strategies)  # Initialize with ones to avoid division by zero\n        self.selection_pressure = selection_pressure\n        self.population = None\n        self.fitness = None\n        self.best_index = None\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index].copy()\n        self.lb = func.bounds.lb\n        self.ub = func.bounds.ub\n\n    def mutation_DE_rand1(self, population, i, F):\n        indices = np.random.choice(self.pop_size, 3, replace=False)\n        x_r1, x_r2, x_r3 = population[indices]\n        return population[i] + F * (x_r2 - x_r3)\n\n    def mutation_DE_best1(self, population, i, F, best_index):\n         indices = np.random.choice(self.pop_size, 2, replace=False)\n         x_r1, x_r2 = population[indices]\n         return population[i] + F * (population[best_index] - population[i]) + F * (x_r1 - x_r2)\n\n    def mutation_DE_current_to_rand1(self, population, i, F):\n        indices = np.random.choice(self.pop_size, 3, replace=False)\n        x_r1, x_r2, x_r3 = population[indices]\n        return population[i] + F * (x_r1 - population[i]) + F * (x_r2 - x_r3)\n\n    def mutation_DE_current_to_best1(self, population, i, F, best_index):\n        indices = np.random.choice(self.pop_size, 2, replace=False)\n        x_r1, x_r2 = population[indices]\n        return population[i] + F * (population[best_index] - population[i]) + F * (x_r1 - x_r2)\n    \n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            strategy_probabilities = self.strategy_successes ** self.selection_pressure\n            strategy_probabilities /= np.sum(strategy_probabilities)\n\n            for i in range(self.pop_size):\n                # Strategy selection\n                strategy_index = np.random.choice(self.num_strategies, p=strategy_probabilities)\n                mutation_strategy = self.mutation_strategies[strategy_index]\n\n                # Mutation\n                F = np.random.uniform(0.2, 0.8)\n                try:\n                    v = mutation_strategy(self.population, i, F, self.best_index)\n                except TypeError:\n                    v = mutation_strategy(self.population, i, F)\n\n                v = np.clip(v, self.lb, self.ub)\n\n                # Crossover\n                CR = np.random.uniform(0.3, 0.9)\n                j_rand = np.random.randint(self.dim)\n                u = self.population[i].copy()\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        u[j] = v[j]\n\n                # Evaluation\n                f_u = func(u)\n                self.budget -= 1\n\n                # Selection\n                if f_u < self.fitness[i]:\n                    self.strategy_successes[strategy_index] += 1\n                    self.fitness[i] = f_u\n                    self.population[i] = u\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u.copy()\n                        self.best_index = np.argmin(self.fitness)\n                else:\n                     self.strategy_successes[strategy_index] *= 0.9  # Reduce success if the strategy didn't improve\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm MultiMutationDE scored 0.696 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0a2322ae-c329-4b4f-8e1f-71e7441e4e19"], "operator": null, "metadata": {"aucs": [0.21738001523305406, 0.6039407527998946, 0.7536669674376644, 0.8955546878624714, 0.7775637348701554, 0.8287577530041538, 0.5781615950568177, 0.7146686043984642, 0.7834395045267796, 0.6234474415355149, 0.8625237177257385, 0.9924681847005424, 0.4659402724342866, 0.7711966313808122, 0.9232143594732527, 0.8292645606877049, 0.6747131163621083, 0.8756359879936556, 0.23245334122373473, 0.5153882906330662]}}
{"id": "d8b2555d-452d-4f37-b2fe-d1f9476d54ba", "fitness": 0.5739422941515504, "name": "CooperativeDE", "description": "Cooperative Differential Evolution with dynamically adjusted cooperation rates between sub-populations and a focused local search.", "code": "import numpy as np\n\nclass CooperativeDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, num_sub_pops=5, F=0.5, CR=0.7, local_search_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.num_sub_pops = num_sub_pops\n        self.F = F\n        self.CR = CR\n        self.local_search_prob = local_search_prob\n        self.sub_pop_size = pop_size // num_sub_pops\n        self.populations = [None] * num_sub_pops\n        self.fitnesses = [None] * num_sub_pops\n        self.best_index = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.cooperation_rates = np.ones((num_sub_pops, num_sub_pops)) / num_sub_pops # Initialize cooperation rates\n\n    def initialize_sub_population(self, func, sub_pop_index):\n        self.populations[sub_pop_index] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.sub_pop_size, self.dim))\n        self.fitnesses[sub_pop_index] = np.array([func(x) for x in self.populations[sub_pop_index]])\n        self.budget -= self.sub_pop_size\n\n    def local_search(self, x, func):\n        \"\"\"Performs a local search around x.\"\"\"\n        x_new = x.copy()\n        for i in range(self.dim):\n            # Perturb each dimension with a small random value\n            x_new[i] = x[i] + np.random.normal(0, 0.05 * (func.bounds.ub[i] - func.bounds.lb[i]))\n            x_new[i] = np.clip(x_new[i], func.bounds.lb[i], func.bounds.ub[i])\n\n        f_new = func(x_new)\n        self.budget -= 1\n        return x_new, f_new\n        \n    def update_cooperation_rates(self):\n        \"\"\"Updates cooperation rates based on fitness improvements.\"\"\"\n        for i in range(self.num_sub_pops):\n            for j in range(self.num_sub_pops):\n                if i != j:\n                    # If sub-pop i is doing well and sub-pop j is not, increase cooperation\n                    if np.min(self.fitnesses[i]) < np.mean(self.fitnesses[i]) and np.min(self.fitnesses[j]) > np.mean(self.fitnesses[j]):\n                        self.cooperation_rates[i, j] *= 1.05  # Increase cooperation rate\n                    else:\n                        self.cooperation_rates[i, j] *= 0.95  # Decrease cooperation rate\n                    \n                    self.cooperation_rates[i, j] = np.clip(self.cooperation_rates[i, j], 0.01, 0.5) # Keep rates in a reasonable range\n        # Normalize the cooperation rates\n        for i in range(self.num_sub_pops):\n            self.cooperation_rates[i] /= np.sum(self.cooperation_rates[i])\n\n\n    def __call__(self, func):\n        for i in range(self.num_sub_pops):\n            self.initialize_sub_population(func, i)\n\n        while self.budget > 0:\n            self.update_cooperation_rates()\n\n            for i in range(self.num_sub_pops):\n                for j in range(self.sub_pop_size):\n                    # Choose a sub-population to cooperate with\n                    coop_partner_index = np.random.choice(self.num_sub_pops, p=self.cooperation_rates[i])\n                    if coop_partner_index == i:\n                        coop_partner_index = (i + 1) % self.num_sub_pops\n\n                    # Differential Evolution within the sub-population with cooperation\n                    indices = np.random.choice(self.sub_pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.populations[i][indices]\n                    \n                    # Cooperative component\n                    donor_index = np.random.randint(self.sub_pop_size)\n                    x_coop = self.populations[coop_partner_index][donor_index]\n\n                    v = self.populations[i][j] + self.F * (x_coop - self.populations[i][j]) + self.F * (x_r1 - x_r2)\n                    v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                    # Crossover\n                    j_rand = np.random.randint(self.dim)\n                    u = self.populations[i][j].copy()\n                    for k in range(self.dim):\n                        if np.random.rand() < self.CR or k == j_rand:\n                            u[k] = v[k]\n\n                    # Local search with some probability\n                    if np.random.rand() < self.local_search_prob and self.budget > 1:\n                        u, f_u = self.local_search(u, func)\n                    else:\n                        f_u = func(u)\n                        self.budget -= 1\n\n                    # Selection\n                    if f_u < self.fitnesses[i][j]:\n                        self.fitnesses[i][j] = f_u\n                        self.populations[i][j] = u\n                        if f_u < self.f_opt:\n                            self.f_opt = f_u\n                            self.x_opt = u.copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm CooperativeDE scored 0.574 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2048221b-0a66-4643-9bc8-785c91f20063"], "operator": null, "metadata": {"aucs": [0.17862035466039938, 0.42297305710095134, 0.43647756041853913, 0.787496643218626, 0.7236384338309643, 0.7924333363382123, 0.5379277701452374, 0.48319611116148475, 0.4479394392391626, 0.2385559084537433, 0.8028934480526055, 0.9850133100348509, 0.4114620482367446, 0.623722605986957, 0.8610107360473263, 0.7223032618526655, 0.482459924258166, 0.8396578251772162, 0.20531050439224896, 0.4957536044249078]}}
{"id": "7fd00506-bd05-4eb3-af88-a34e3a7508d4", "fitness": -Infinity, "name": "FuzzyAdaptiveDE", "description": "An Adaptive Differential Evolution strategy with a self-adjusting population size based on stagnation detection and dynamic parameter control using a fuzzy logic controller.", "code": "import numpy as np\nimport skfuzzy as fuzz\nfrom skfuzzy import control as ctrl\n\nclass FuzzyAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, min_pop_size=10, max_pop_size=100, stagnation_threshold=100):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = initial_pop_size\n        self.min_pop_size = min_pop_size\n        self.max_pop_size = max_pop_size\n        self.pop_size = initial_pop_size\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.population = None\n        self.fitness = None\n        self.best_index = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.F = 0.5  # Initial mutation factor\n        self.CR = 0.7 # Initial Crossover rate\n\n        # Fuzzy Logic Controller for F and CR adaptation\n        self.antecedent_stagnation = ctrl.Antecedent(np.linspace(0, self.stagnation_threshold, 1000), 'stagnation')\n        self.antecedent_fitness_variation = ctrl.Antecedent(np.linspace(0, 1, 1000), 'fitness_variation') # Normalized fitness variation\n        self.consequent_F = ctrl.Consequent(np.linspace(0.1, 0.9, 100), 'F')\n        self.consequent_CR = ctrl.Consequent(np.linspace(0.1, 0.9, 100), 'CR')\n        \n        # Define membership functions (example: triangular)\n        self.antecedent_stagnation['low'] = fuzz.trimf(self.antecedent_stagnation.universe, [0, 0, self.stagnation_threshold/2])\n        self.antecedent_stagnation['medium'] = fuzz.trimf(self.antecedent_stagnation.universe, [0, self.stagnation_threshold/2, self.stagnation_threshold])\n        self.antecedent_stagnation['high'] = fuzz.trimf(self.antecedent_stagnation.universe, [self.stagnation_threshold/2, self.stagnation_threshold, self.stagnation_threshold])\n\n        self.antecedent_fitness_variation['low'] = fuzz.trimf(self.antecedent_fitness_variation.universe, [0, 0, 0.5])\n        self.antecedent_fitness_variation['medium'] = fuzz.trimf(self.antecedent_fitness_variation.universe, [0, 0.5, 1])\n        self.antecedent_fitness_variation['high'] = fuzz.trimf(self.antecedent_fitness_variation.universe, [0.5, 1, 1])\n        \n        self.consequent_F['low'] = fuzz.trimf(self.consequent_F.universe, [0.1, 0.1, 0.5])\n        self.consequent_F['medium'] = fuzz.trimf(self.consequent_F.universe, [0.1, 0.5, 0.9])\n        self.consequent_F['high'] = fuzz.trimf(self.consequent_F.universe, [0.5, 0.9, 0.9])\n\n        self.consequent_CR['low'] = fuzz.trimf(self.consequent_CR.universe, [0.1, 0.1, 0.5])\n        self.consequent_CR['medium'] = fuzz.trimf(self.consequent_CR.universe, [0.1, 0.5, 0.9])\n        self.consequent_CR['high'] = fuzz.trimf(self.consequent_CR.universe, [0.5, 0.9, 0.9])\n\n        # Define rules\n        rule1 = ctrl.Rule(self.antecedent_stagnation['low'] & self.antecedent_fitness_variation['high'], [self.consequent_F['low'], self.consequent_CR['high']])\n        rule2 = ctrl.Rule(self.antecedent_stagnation['medium'] & self.antecedent_fitness_variation['medium'], [self.consequent_F['medium'], self.consequent_CR['medium']])\n        rule3 = ctrl.Rule(self.antecedent_stagnation['high'] & self.antecedent_fitness_variation['low'], [self.consequent_F['high'], self.consequent_CR['low']])\n\n        self.control_system = ctrl.ControlSystem([rule1, rule2, rule3])\n        self.parameter_control = ctrl.ControlSystemSimulation(self.control_system)\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index].copy()\n        self.lb = func.bounds.lb\n        self.ub = func.bounds.ub\n        self.best_fitness_history.append(self.f_opt)\n\n    def adjust_population_size(self):\n        if self.stagnation_counter > self.stagnation_threshold:\n            self.pop_size = max(self.min_pop_size, self.pop_size // 2)  # Reduce population\n            self.stagnation_counter = 0\n            print(f\"Population size reduced to {self.pop_size}\")\n        elif self.stagnation_counter > self.stagnation_threshold/2 and self.pop_size < self.max_pop_size:\n            self.pop_size = min(self.max_pop_size, self.pop_size * 2) # Increase population\n            print(f\"Population size increased to {self.pop_size}\")\n            \n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            # Parameter Adaptation using Fuzzy Logic\n            if len(self.best_fitness_history) > 1:\n                fitness_variation = abs(self.best_fitness_history[-1] - self.best_fitness_history[-2]) / max(abs(self.best_fitness_history[-1]), 1e-9)  # Normalize the fitness variation\n            else:\n                fitness_variation = 0.0\n            \n            self.parameter_control.input['stagnation'] = self.stagnation_counter\n            self.parameter_control.input['fitness_variation'] = fitness_variation\n            self.parameter_control.compute()\n\n            self.F = self.parameter_control.output['F']\n            self.CR = self.parameter_control.output['CR']\n            \n\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                v = self.population[i] + self.F * (x_r2 - x_r3)\n                v = np.clip(v, self.lb, self.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = self.population[i].copy()\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        u[j] = v[j]\n\n                # Evaluation\n                f_u = func(u)\n                self.budget -= 1\n\n                # Selection\n                if f_u < self.fitness[i]:\n                    self.fitness[i] = f_u\n                    self.population[i] = u\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u.copy()\n                        self.best_index = np.argmin(self.fitness)\n                        self.stagnation_counter = 0  # Reset stagnation counter\n                    else:\n                        self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter += 1\n            \n            self.best_fitness_history.append(self.f_opt)\n            self.adjust_population_size() # Adjust population size based on stagnation\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 111, in evaluate\n    exec(code, safe_globals, local_env)\n  File \"<string>\", line 2, in <module>\nModuleNotFoundError: No module named 'skfuzzy'\n.", "error": "", "parent_ids": ["7275b07c-5ed3-4b00-af0f-7de5c1bf2808"], "operator": null, "metadata": {}}
{"id": "7fd00506-bd05-4eb3-af88-a34e3a7508d4", "fitness": -Infinity, "name": "FuzzyAdaptiveDE", "description": "An Adaptive Differential Evolution strategy with a self-adjusting population size based on stagnation detection and dynamic parameter control using a fuzzy logic controller.", "code": "import numpy as np\nimport skfuzzy as fuzz\nfrom skfuzzy import control as ctrl\n\nclass FuzzyAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, min_pop_size=10, max_pop_size=100, stagnation_threshold=100):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = initial_pop_size\n        self.min_pop_size = min_pop_size\n        self.max_pop_size = max_pop_size\n        self.pop_size = initial_pop_size\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.population = None\n        self.fitness = None\n        self.best_index = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.F = 0.5  # Initial mutation factor\n        self.CR = 0.7 # Initial Crossover rate\n\n        # Fuzzy Logic Controller for F and CR adaptation\n        self.antecedent_stagnation = ctrl.Antecedent(np.linspace(0, self.stagnation_threshold, 1000), 'stagnation')\n        self.antecedent_fitness_variation = ctrl.Antecedent(np.linspace(0, 1, 1000), 'fitness_variation') # Normalized fitness variation\n        self.consequent_F = ctrl.Consequent(np.linspace(0.1, 0.9, 100), 'F')\n        self.consequent_CR = ctrl.Consequent(np.linspace(0.1, 0.9, 100), 'CR')\n        \n        # Define membership functions (example: triangular)\n        self.antecedent_stagnation['low'] = fuzz.trimf(self.antecedent_stagnation.universe, [0, 0, self.stagnation_threshold/2])\n        self.antecedent_stagnation['medium'] = fuzz.trimf(self.antecedent_stagnation.universe, [0, self.stagnation_threshold/2, self.stagnation_threshold])\n        self.antecedent_stagnation['high'] = fuzz.trimf(self.antecedent_stagnation.universe, [self.stagnation_threshold/2, self.stagnation_threshold, self.stagnation_threshold])\n\n        self.antecedent_fitness_variation['low'] = fuzz.trimf(self.antecedent_fitness_variation.universe, [0, 0, 0.5])\n        self.antecedent_fitness_variation['medium'] = fuzz.trimf(self.antecedent_fitness_variation.universe, [0, 0.5, 1])\n        self.antecedent_fitness_variation['high'] = fuzz.trimf(self.antecedent_fitness_variation.universe, [0.5, 1, 1])\n        \n        self.consequent_F['low'] = fuzz.trimf(self.consequent_F.universe, [0.1, 0.1, 0.5])\n        self.consequent_F['medium'] = fuzz.trimf(self.consequent_F.universe, [0.1, 0.5, 0.9])\n        self.consequent_F['high'] = fuzz.trimf(self.consequent_F.universe, [0.5, 0.9, 0.9])\n\n        self.consequent_CR['low'] = fuzz.trimf(self.consequent_CR.universe, [0.1, 0.1, 0.5])\n        self.consequent_CR['medium'] = fuzz.trimf(self.consequent_CR.universe, [0.1, 0.5, 0.9])\n        self.consequent_CR['high'] = fuzz.trimf(self.consequent_CR.universe, [0.5, 0.9, 0.9])\n\n        # Define rules\n        rule1 = ctrl.Rule(self.antecedent_stagnation['low'] & self.antecedent_fitness_variation['high'], [self.consequent_F['low'], self.consequent_CR['high']])\n        rule2 = ctrl.Rule(self.antecedent_stagnation['medium'] & self.antecedent_fitness_variation['medium'], [self.consequent_F['medium'], self.consequent_CR['medium']])\n        rule3 = ctrl.Rule(self.antecedent_stagnation['high'] & self.antecedent_fitness_variation['low'], [self.consequent_F['high'], self.consequent_CR['low']])\n\n        self.control_system = ctrl.ControlSystem([rule1, rule2, rule3])\n        self.parameter_control = ctrl.ControlSystemSimulation(self.control_system)\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index].copy()\n        self.lb = func.bounds.lb\n        self.ub = func.bounds.ub\n        self.best_fitness_history.append(self.f_opt)\n\n    def adjust_population_size(self):\n        if self.stagnation_counter > self.stagnation_threshold:\n            self.pop_size = max(self.min_pop_size, self.pop_size // 2)  # Reduce population\n            self.stagnation_counter = 0\n            print(f\"Population size reduced to {self.pop_size}\")\n        elif self.stagnation_counter > self.stagnation_threshold/2 and self.pop_size < self.max_pop_size:\n            self.pop_size = min(self.max_pop_size, self.pop_size * 2) # Increase population\n            print(f\"Population size increased to {self.pop_size}\")\n            \n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            # Parameter Adaptation using Fuzzy Logic\n            if len(self.best_fitness_history) > 1:\n                fitness_variation = abs(self.best_fitness_history[-1] - self.best_fitness_history[-2]) / max(abs(self.best_fitness_history[-1]), 1e-9)  # Normalize the fitness variation\n            else:\n                fitness_variation = 0.0\n            \n            self.parameter_control.input['stagnation'] = self.stagnation_counter\n            self.parameter_control.input['fitness_variation'] = fitness_variation\n            self.parameter_control.compute()\n\n            self.F = self.parameter_control.output['F']\n            self.CR = self.parameter_control.output['CR']\n            \n\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                v = self.population[i] + self.F * (x_r2 - x_r3)\n                v = np.clip(v, self.lb, self.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = self.population[i].copy()\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        u[j] = v[j]\n\n                # Evaluation\n                f_u = func(u)\n                self.budget -= 1\n\n                # Selection\n                if f_u < self.fitness[i]:\n                    self.fitness[i] = f_u\n                    self.population[i] = u\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u.copy()\n                        self.best_index = np.argmin(self.fitness)\n                        self.stagnation_counter = 0  # Reset stagnation counter\n                    else:\n                        self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter += 1\n            \n            self.best_fitness_history.append(self.f_opt)\n            self.adjust_population_size() # Adjust population size based on stagnation\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 111, in evaluate\n    exec(code, safe_globals, local_env)\n  File \"<string>\", line 2, in <module>\nModuleNotFoundError: No module named 'skfuzzy'\n.", "error": "", "parent_ids": ["7275b07c-5ed3-4b00-af0f-7de5c1bf2808"], "operator": null, "metadata": {}}
{"id": "7fd00506-bd05-4eb3-af88-a34e3a7508d4", "fitness": -Infinity, "name": "FuzzyAdaptiveDE", "description": "An Adaptive Differential Evolution strategy with a self-adjusting population size based on stagnation detection and dynamic parameter control using a fuzzy logic controller.", "code": "import numpy as np\nimport skfuzzy as fuzz\nfrom skfuzzy import control as ctrl\n\nclass FuzzyAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, min_pop_size=10, max_pop_size=100, stagnation_threshold=100):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = initial_pop_size\n        self.min_pop_size = min_pop_size\n        self.max_pop_size = max_pop_size\n        self.pop_size = initial_pop_size\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.population = None\n        self.fitness = None\n        self.best_index = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.F = 0.5  # Initial mutation factor\n        self.CR = 0.7 # Initial Crossover rate\n\n        # Fuzzy Logic Controller for F and CR adaptation\n        self.antecedent_stagnation = ctrl.Antecedent(np.linspace(0, self.stagnation_threshold, 1000), 'stagnation')\n        self.antecedent_fitness_variation = ctrl.Antecedent(np.linspace(0, 1, 1000), 'fitness_variation') # Normalized fitness variation\n        self.consequent_F = ctrl.Consequent(np.linspace(0.1, 0.9, 100), 'F')\n        self.consequent_CR = ctrl.Consequent(np.linspace(0.1, 0.9, 100), 'CR')\n        \n        # Define membership functions (example: triangular)\n        self.antecedent_stagnation['low'] = fuzz.trimf(self.antecedent_stagnation.universe, [0, 0, self.stagnation_threshold/2])\n        self.antecedent_stagnation['medium'] = fuzz.trimf(self.antecedent_stagnation.universe, [0, self.stagnation_threshold/2, self.stagnation_threshold])\n        self.antecedent_stagnation['high'] = fuzz.trimf(self.antecedent_stagnation.universe, [self.stagnation_threshold/2, self.stagnation_threshold, self.stagnation_threshold])\n\n        self.antecedent_fitness_variation['low'] = fuzz.trimf(self.antecedent_fitness_variation.universe, [0, 0, 0.5])\n        self.antecedent_fitness_variation['medium'] = fuzz.trimf(self.antecedent_fitness_variation.universe, [0, 0.5, 1])\n        self.antecedent_fitness_variation['high'] = fuzz.trimf(self.antecedent_fitness_variation.universe, [0.5, 1, 1])\n        \n        self.consequent_F['low'] = fuzz.trimf(self.consequent_F.universe, [0.1, 0.1, 0.5])\n        self.consequent_F['medium'] = fuzz.trimf(self.consequent_F.universe, [0.1, 0.5, 0.9])\n        self.consequent_F['high'] = fuzz.trimf(self.consequent_F.universe, [0.5, 0.9, 0.9])\n\n        self.consequent_CR['low'] = fuzz.trimf(self.consequent_CR.universe, [0.1, 0.1, 0.5])\n        self.consequent_CR['medium'] = fuzz.trimf(self.consequent_CR.universe, [0.1, 0.5, 0.9])\n        self.consequent_CR['high'] = fuzz.trimf(self.consequent_CR.universe, [0.5, 0.9, 0.9])\n\n        # Define rules\n        rule1 = ctrl.Rule(self.antecedent_stagnation['low'] & self.antecedent_fitness_variation['high'], [self.consequent_F['low'], self.consequent_CR['high']])\n        rule2 = ctrl.Rule(self.antecedent_stagnation['medium'] & self.antecedent_fitness_variation['medium'], [self.consequent_F['medium'], self.consequent_CR['medium']])\n        rule3 = ctrl.Rule(self.antecedent_stagnation['high'] & self.antecedent_fitness_variation['low'], [self.consequent_F['high'], self.consequent_CR['low']])\n\n        self.control_system = ctrl.ControlSystem([rule1, rule2, rule3])\n        self.parameter_control = ctrl.ControlSystemSimulation(self.control_system)\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index].copy()\n        self.lb = func.bounds.lb\n        self.ub = func.bounds.ub\n        self.best_fitness_history.append(self.f_opt)\n\n    def adjust_population_size(self):\n        if self.stagnation_counter > self.stagnation_threshold:\n            self.pop_size = max(self.min_pop_size, self.pop_size // 2)  # Reduce population\n            self.stagnation_counter = 0\n            print(f\"Population size reduced to {self.pop_size}\")\n        elif self.stagnation_counter > self.stagnation_threshold/2 and self.pop_size < self.max_pop_size:\n            self.pop_size = min(self.max_pop_size, self.pop_size * 2) # Increase population\n            print(f\"Population size increased to {self.pop_size}\")\n            \n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            # Parameter Adaptation using Fuzzy Logic\n            if len(self.best_fitness_history) > 1:\n                fitness_variation = abs(self.best_fitness_history[-1] - self.best_fitness_history[-2]) / max(abs(self.best_fitness_history[-1]), 1e-9)  # Normalize the fitness variation\n            else:\n                fitness_variation = 0.0\n            \n            self.parameter_control.input['stagnation'] = self.stagnation_counter\n            self.parameter_control.input['fitness_variation'] = fitness_variation\n            self.parameter_control.compute()\n\n            self.F = self.parameter_control.output['F']\n            self.CR = self.parameter_control.output['CR']\n            \n\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                v = self.population[i] + self.F * (x_r2 - x_r3)\n                v = np.clip(v, self.lb, self.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = self.population[i].copy()\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        u[j] = v[j]\n\n                # Evaluation\n                f_u = func(u)\n                self.budget -= 1\n\n                # Selection\n                if f_u < self.fitness[i]:\n                    self.fitness[i] = f_u\n                    self.population[i] = u\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u.copy()\n                        self.best_index = np.argmin(self.fitness)\n                        self.stagnation_counter = 0  # Reset stagnation counter\n                    else:\n                        self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter += 1\n            \n            self.best_fitness_history.append(self.f_opt)\n            self.adjust_population_size() # Adjust population size based on stagnation\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 111, in evaluate\n    exec(code, safe_globals, local_env)\n  File \"<string>\", line 2, in <module>\nModuleNotFoundError: No module named 'skfuzzy'\n.", "error": "", "parent_ids": ["7275b07c-5ed3-4b00-af0f-7de5c1bf2808"], "operator": null, "metadata": {}}
{"id": "7fd00506-bd05-4eb3-af88-a34e3a7508d4", "fitness": -Infinity, "name": "FuzzyAdaptiveDE", "description": "An Adaptive Differential Evolution strategy with a self-adjusting population size based on stagnation detection and dynamic parameter control using a fuzzy logic controller.", "code": "import numpy as np\nimport skfuzzy as fuzz\nfrom skfuzzy import control as ctrl\n\nclass FuzzyAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, min_pop_size=10, max_pop_size=100, stagnation_threshold=100):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = initial_pop_size\n        self.min_pop_size = min_pop_size\n        self.max_pop_size = max_pop_size\n        self.pop_size = initial_pop_size\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.population = None\n        self.fitness = None\n        self.best_index = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.F = 0.5  # Initial mutation factor\n        self.CR = 0.7 # Initial Crossover rate\n\n        # Fuzzy Logic Controller for F and CR adaptation\n        self.antecedent_stagnation = ctrl.Antecedent(np.linspace(0, self.stagnation_threshold, 1000), 'stagnation')\n        self.antecedent_fitness_variation = ctrl.Antecedent(np.linspace(0, 1, 1000), 'fitness_variation') # Normalized fitness variation\n        self.consequent_F = ctrl.Consequent(np.linspace(0.1, 0.9, 100), 'F')\n        self.consequent_CR = ctrl.Consequent(np.linspace(0.1, 0.9, 100), 'CR')\n        \n        # Define membership functions (example: triangular)\n        self.antecedent_stagnation['low'] = fuzz.trimf(self.antecedent_stagnation.universe, [0, 0, self.stagnation_threshold/2])\n        self.antecedent_stagnation['medium'] = fuzz.trimf(self.antecedent_stagnation.universe, [0, self.stagnation_threshold/2, self.stagnation_threshold])\n        self.antecedent_stagnation['high'] = fuzz.trimf(self.antecedent_stagnation.universe, [self.stagnation_threshold/2, self.stagnation_threshold, self.stagnation_threshold])\n\n        self.antecedent_fitness_variation['low'] = fuzz.trimf(self.antecedent_fitness_variation.universe, [0, 0, 0.5])\n        self.antecedent_fitness_variation['medium'] = fuzz.trimf(self.antecedent_fitness_variation.universe, [0, 0.5, 1])\n        self.antecedent_fitness_variation['high'] = fuzz.trimf(self.antecedent_fitness_variation.universe, [0.5, 1, 1])\n        \n        self.consequent_F['low'] = fuzz.trimf(self.consequent_F.universe, [0.1, 0.1, 0.5])\n        self.consequent_F['medium'] = fuzz.trimf(self.consequent_F.universe, [0.1, 0.5, 0.9])\n        self.consequent_F['high'] = fuzz.trimf(self.consequent_F.universe, [0.5, 0.9, 0.9])\n\n        self.consequent_CR['low'] = fuzz.trimf(self.consequent_CR.universe, [0.1, 0.1, 0.5])\n        self.consequent_CR['medium'] = fuzz.trimf(self.consequent_CR.universe, [0.1, 0.5, 0.9])\n        self.consequent_CR['high'] = fuzz.trimf(self.consequent_CR.universe, [0.5, 0.9, 0.9])\n\n        # Define rules\n        rule1 = ctrl.Rule(self.antecedent_stagnation['low'] & self.antecedent_fitness_variation['high'], [self.consequent_F['low'], self.consequent_CR['high']])\n        rule2 = ctrl.Rule(self.antecedent_stagnation['medium'] & self.antecedent_fitness_variation['medium'], [self.consequent_F['medium'], self.consequent_CR['medium']])\n        rule3 = ctrl.Rule(self.antecedent_stagnation['high'] & self.antecedent_fitness_variation['low'], [self.consequent_F['high'], self.consequent_CR['low']])\n\n        self.control_system = ctrl.ControlSystem([rule1, rule2, rule3])\n        self.parameter_control = ctrl.ControlSystemSimulation(self.control_system)\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index].copy()\n        self.lb = func.bounds.lb\n        self.ub = func.bounds.ub\n        self.best_fitness_history.append(self.f_opt)\n\n    def adjust_population_size(self):\n        if self.stagnation_counter > self.stagnation_threshold:\n            self.pop_size = max(self.min_pop_size, self.pop_size // 2)  # Reduce population\n            self.stagnation_counter = 0\n            print(f\"Population size reduced to {self.pop_size}\")\n        elif self.stagnation_counter > self.stagnation_threshold/2 and self.pop_size < self.max_pop_size:\n            self.pop_size = min(self.max_pop_size, self.pop_size * 2) # Increase population\n            print(f\"Population size increased to {self.pop_size}\")\n            \n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            # Parameter Adaptation using Fuzzy Logic\n            if len(self.best_fitness_history) > 1:\n                fitness_variation = abs(self.best_fitness_history[-1] - self.best_fitness_history[-2]) / max(abs(self.best_fitness_history[-1]), 1e-9)  # Normalize the fitness variation\n            else:\n                fitness_variation = 0.0\n            \n            self.parameter_control.input['stagnation'] = self.stagnation_counter\n            self.parameter_control.input['fitness_variation'] = fitness_variation\n            self.parameter_control.compute()\n\n            self.F = self.parameter_control.output['F']\n            self.CR = self.parameter_control.output['CR']\n            \n\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                v = self.population[i] + self.F * (x_r2 - x_r3)\n                v = np.clip(v, self.lb, self.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = self.population[i].copy()\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        u[j] = v[j]\n\n                # Evaluation\n                f_u = func(u)\n                self.budget -= 1\n\n                # Selection\n                if f_u < self.fitness[i]:\n                    self.fitness[i] = f_u\n                    self.population[i] = u\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u.copy()\n                        self.best_index = np.argmin(self.fitness)\n                        self.stagnation_counter = 0  # Reset stagnation counter\n                    else:\n                        self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter += 1\n            \n            self.best_fitness_history.append(self.f_opt)\n            self.adjust_population_size() # Adjust population size based on stagnation\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 111, in evaluate\n    exec(code, safe_globals, local_env)\n  File \"<string>\", line 2, in <module>\nModuleNotFoundError: No module named 'skfuzzy'\n.", "error": "", "parent_ids": ["7275b07c-5ed3-4b00-af0f-7de5c1bf2808"], "operator": null, "metadata": {}}
{"id": "7fd00506-bd05-4eb3-af88-a34e3a7508d4", "fitness": -Infinity, "name": "FuzzyAdaptiveDE", "description": "An Adaptive Differential Evolution strategy with a self-adjusting population size based on stagnation detection and dynamic parameter control using a fuzzy logic controller.", "code": "import numpy as np\nimport skfuzzy as fuzz\nfrom skfuzzy import control as ctrl\n\nclass FuzzyAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, min_pop_size=10, max_pop_size=100, stagnation_threshold=100):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = initial_pop_size\n        self.min_pop_size = min_pop_size\n        self.max_pop_size = max_pop_size\n        self.pop_size = initial_pop_size\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.population = None\n        self.fitness = None\n        self.best_index = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.F = 0.5  # Initial mutation factor\n        self.CR = 0.7 # Initial Crossover rate\n\n        # Fuzzy Logic Controller for F and CR adaptation\n        self.antecedent_stagnation = ctrl.Antecedent(np.linspace(0, self.stagnation_threshold, 1000), 'stagnation')\n        self.antecedent_fitness_variation = ctrl.Antecedent(np.linspace(0, 1, 1000), 'fitness_variation') # Normalized fitness variation\n        self.consequent_F = ctrl.Consequent(np.linspace(0.1, 0.9, 100), 'F')\n        self.consequent_CR = ctrl.Consequent(np.linspace(0.1, 0.9, 100), 'CR')\n        \n        # Define membership functions (example: triangular)\n        self.antecedent_stagnation['low'] = fuzz.trimf(self.antecedent_stagnation.universe, [0, 0, self.stagnation_threshold/2])\n        self.antecedent_stagnation['medium'] = fuzz.trimf(self.antecedent_stagnation.universe, [0, self.stagnation_threshold/2, self.stagnation_threshold])\n        self.antecedent_stagnation['high'] = fuzz.trimf(self.antecedent_stagnation.universe, [self.stagnation_threshold/2, self.stagnation_threshold, self.stagnation_threshold])\n\n        self.antecedent_fitness_variation['low'] = fuzz.trimf(self.antecedent_fitness_variation.universe, [0, 0, 0.5])\n        self.antecedent_fitness_variation['medium'] = fuzz.trimf(self.antecedent_fitness_variation.universe, [0, 0.5, 1])\n        self.antecedent_fitness_variation['high'] = fuzz.trimf(self.antecedent_fitness_variation.universe, [0.5, 1, 1])\n        \n        self.consequent_F['low'] = fuzz.trimf(self.consequent_F.universe, [0.1, 0.1, 0.5])\n        self.consequent_F['medium'] = fuzz.trimf(self.consequent_F.universe, [0.1, 0.5, 0.9])\n        self.consequent_F['high'] = fuzz.trimf(self.consequent_F.universe, [0.5, 0.9, 0.9])\n\n        self.consequent_CR['low'] = fuzz.trimf(self.consequent_CR.universe, [0.1, 0.1, 0.5])\n        self.consequent_CR['medium'] = fuzz.trimf(self.consequent_CR.universe, [0.1, 0.5, 0.9])\n        self.consequent_CR['high'] = fuzz.trimf(self.consequent_CR.universe, [0.5, 0.9, 0.9])\n\n        # Define rules\n        rule1 = ctrl.Rule(self.antecedent_stagnation['low'] & self.antecedent_fitness_variation['high'], [self.consequent_F['low'], self.consequent_CR['high']])\n        rule2 = ctrl.Rule(self.antecedent_stagnation['medium'] & self.antecedent_fitness_variation['medium'], [self.consequent_F['medium'], self.consequent_CR['medium']])\n        rule3 = ctrl.Rule(self.antecedent_stagnation['high'] & self.antecedent_fitness_variation['low'], [self.consequent_F['high'], self.consequent_CR['low']])\n\n        self.control_system = ctrl.ControlSystem([rule1, rule2, rule3])\n        self.parameter_control = ctrl.ControlSystemSimulation(self.control_system)\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index].copy()\n        self.lb = func.bounds.lb\n        self.ub = func.bounds.ub\n        self.best_fitness_history.append(self.f_opt)\n\n    def adjust_population_size(self):\n        if self.stagnation_counter > self.stagnation_threshold:\n            self.pop_size = max(self.min_pop_size, self.pop_size // 2)  # Reduce population\n            self.stagnation_counter = 0\n            print(f\"Population size reduced to {self.pop_size}\")\n        elif self.stagnation_counter > self.stagnation_threshold/2 and self.pop_size < self.max_pop_size:\n            self.pop_size = min(self.max_pop_size, self.pop_size * 2) # Increase population\n            print(f\"Population size increased to {self.pop_size}\")\n            \n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            # Parameter Adaptation using Fuzzy Logic\n            if len(self.best_fitness_history) > 1:\n                fitness_variation = abs(self.best_fitness_history[-1] - self.best_fitness_history[-2]) / max(abs(self.best_fitness_history[-1]), 1e-9)  # Normalize the fitness variation\n            else:\n                fitness_variation = 0.0\n            \n            self.parameter_control.input['stagnation'] = self.stagnation_counter\n            self.parameter_control.input['fitness_variation'] = fitness_variation\n            self.parameter_control.compute()\n\n            self.F = self.parameter_control.output['F']\n            self.CR = self.parameter_control.output['CR']\n            \n\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                v = self.population[i] + self.F * (x_r2 - x_r3)\n                v = np.clip(v, self.lb, self.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = self.population[i].copy()\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        u[j] = v[j]\n\n                # Evaluation\n                f_u = func(u)\n                self.budget -= 1\n\n                # Selection\n                if f_u < self.fitness[i]:\n                    self.fitness[i] = f_u\n                    self.population[i] = u\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u.copy()\n                        self.best_index = np.argmin(self.fitness)\n                        self.stagnation_counter = 0  # Reset stagnation counter\n                    else:\n                        self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter += 1\n            \n            self.best_fitness_history.append(self.f_opt)\n            self.adjust_population_size() # Adjust population size based on stagnation\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 111, in evaluate\n    exec(code, safe_globals, local_env)\n  File \"<string>\", line 2, in <module>\nModuleNotFoundError: No module named 'skfuzzy'\n.", "error": "", "parent_ids": ["7275b07c-5ed3-4b00-af0f-7de5c1bf2808"], "operator": null, "metadata": {}}
{"id": "7fd00506-bd05-4eb3-af88-a34e3a7508d4", "fitness": -Infinity, "name": "FuzzyAdaptiveDE", "description": "An Adaptive Differential Evolution strategy with a self-adjusting population size based on stagnation detection and dynamic parameter control using a fuzzy logic controller.", "code": "import numpy as np\nimport skfuzzy as fuzz\nfrom skfuzzy import control as ctrl\n\nclass FuzzyAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, min_pop_size=10, max_pop_size=100, stagnation_threshold=100):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = initial_pop_size\n        self.min_pop_size = min_pop_size\n        self.max_pop_size = max_pop_size\n        self.pop_size = initial_pop_size\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.population = None\n        self.fitness = None\n        self.best_index = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.F = 0.5  # Initial mutation factor\n        self.CR = 0.7 # Initial Crossover rate\n\n        # Fuzzy Logic Controller for F and CR adaptation\n        self.antecedent_stagnation = ctrl.Antecedent(np.linspace(0, self.stagnation_threshold, 1000), 'stagnation')\n        self.antecedent_fitness_variation = ctrl.Antecedent(np.linspace(0, 1, 1000), 'fitness_variation') # Normalized fitness variation\n        self.consequent_F = ctrl.Consequent(np.linspace(0.1, 0.9, 100), 'F')\n        self.consequent_CR = ctrl.Consequent(np.linspace(0.1, 0.9, 100), 'CR')\n        \n        # Define membership functions (example: triangular)\n        self.antecedent_stagnation['low'] = fuzz.trimf(self.antecedent_stagnation.universe, [0, 0, self.stagnation_threshold/2])\n        self.antecedent_stagnation['medium'] = fuzz.trimf(self.antecedent_stagnation.universe, [0, self.stagnation_threshold/2, self.stagnation_threshold])\n        self.antecedent_stagnation['high'] = fuzz.trimf(self.antecedent_stagnation.universe, [self.stagnation_threshold/2, self.stagnation_threshold, self.stagnation_threshold])\n\n        self.antecedent_fitness_variation['low'] = fuzz.trimf(self.antecedent_fitness_variation.universe, [0, 0, 0.5])\n        self.antecedent_fitness_variation['medium'] = fuzz.trimf(self.antecedent_fitness_variation.universe, [0, 0.5, 1])\n        self.antecedent_fitness_variation['high'] = fuzz.trimf(self.antecedent_fitness_variation.universe, [0.5, 1, 1])\n        \n        self.consequent_F['low'] = fuzz.trimf(self.consequent_F.universe, [0.1, 0.1, 0.5])\n        self.consequent_F['medium'] = fuzz.trimf(self.consequent_F.universe, [0.1, 0.5, 0.9])\n        self.consequent_F['high'] = fuzz.trimf(self.consequent_F.universe, [0.5, 0.9, 0.9])\n\n        self.consequent_CR['low'] = fuzz.trimf(self.consequent_CR.universe, [0.1, 0.1, 0.5])\n        self.consequent_CR['medium'] = fuzz.trimf(self.consequent_CR.universe, [0.1, 0.5, 0.9])\n        self.consequent_CR['high'] = fuzz.trimf(self.consequent_CR.universe, [0.5, 0.9, 0.9])\n\n        # Define rules\n        rule1 = ctrl.Rule(self.antecedent_stagnation['low'] & self.antecedent_fitness_variation['high'], [self.consequent_F['low'], self.consequent_CR['high']])\n        rule2 = ctrl.Rule(self.antecedent_stagnation['medium'] & self.antecedent_fitness_variation['medium'], [self.consequent_F['medium'], self.consequent_CR['medium']])\n        rule3 = ctrl.Rule(self.antecedent_stagnation['high'] & self.antecedent_fitness_variation['low'], [self.consequent_F['high'], self.consequent_CR['low']])\n\n        self.control_system = ctrl.ControlSystem([rule1, rule2, rule3])\n        self.parameter_control = ctrl.ControlSystemSimulation(self.control_system)\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index].copy()\n        self.lb = func.bounds.lb\n        self.ub = func.bounds.ub\n        self.best_fitness_history.append(self.f_opt)\n\n    def adjust_population_size(self):\n        if self.stagnation_counter > self.stagnation_threshold:\n            self.pop_size = max(self.min_pop_size, self.pop_size // 2)  # Reduce population\n            self.stagnation_counter = 0\n            print(f\"Population size reduced to {self.pop_size}\")\n        elif self.stagnation_counter > self.stagnation_threshold/2 and self.pop_size < self.max_pop_size:\n            self.pop_size = min(self.max_pop_size, self.pop_size * 2) # Increase population\n            print(f\"Population size increased to {self.pop_size}\")\n            \n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            # Parameter Adaptation using Fuzzy Logic\n            if len(self.best_fitness_history) > 1:\n                fitness_variation = abs(self.best_fitness_history[-1] - self.best_fitness_history[-2]) / max(abs(self.best_fitness_history[-1]), 1e-9)  # Normalize the fitness variation\n            else:\n                fitness_variation = 0.0\n            \n            self.parameter_control.input['stagnation'] = self.stagnation_counter\n            self.parameter_control.input['fitness_variation'] = fitness_variation\n            self.parameter_control.compute()\n\n            self.F = self.parameter_control.output['F']\n            self.CR = self.parameter_control.output['CR']\n            \n\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                v = self.population[i] + self.F * (x_r2 - x_r3)\n                v = np.clip(v, self.lb, self.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = self.population[i].copy()\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        u[j] = v[j]\n\n                # Evaluation\n                f_u = func(u)\n                self.budget -= 1\n\n                # Selection\n                if f_u < self.fitness[i]:\n                    self.fitness[i] = f_u\n                    self.population[i] = u\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u.copy()\n                        self.best_index = np.argmin(self.fitness)\n                        self.stagnation_counter = 0  # Reset stagnation counter\n                    else:\n                        self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter += 1\n            \n            self.best_fitness_history.append(self.f_opt)\n            self.adjust_population_size() # Adjust population size based on stagnation\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 111, in evaluate\n    exec(code, safe_globals, local_env)\n  File \"<string>\", line 2, in <module>\nModuleNotFoundError: No module named 'skfuzzy'\n.", "error": "", "parent_ids": ["7275b07c-5ed3-4b00-af0f-7de5c1bf2808"], "operator": null, "metadata": {}}
{"id": "7fd00506-bd05-4eb3-af88-a34e3a7508d4", "fitness": -Infinity, "name": "FuzzyAdaptiveDE", "description": "An Adaptive Differential Evolution strategy with a self-adjusting population size based on stagnation detection and dynamic parameter control using a fuzzy logic controller.", "code": "import numpy as np\nimport skfuzzy as fuzz\nfrom skfuzzy import control as ctrl\n\nclass FuzzyAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, min_pop_size=10, max_pop_size=100, stagnation_threshold=100):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = initial_pop_size\n        self.min_pop_size = min_pop_size\n        self.max_pop_size = max_pop_size\n        self.pop_size = initial_pop_size\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.population = None\n        self.fitness = None\n        self.best_index = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.F = 0.5  # Initial mutation factor\n        self.CR = 0.7 # Initial Crossover rate\n\n        # Fuzzy Logic Controller for F and CR adaptation\n        self.antecedent_stagnation = ctrl.Antecedent(np.linspace(0, self.stagnation_threshold, 1000), 'stagnation')\n        self.antecedent_fitness_variation = ctrl.Antecedent(np.linspace(0, 1, 1000), 'fitness_variation') # Normalized fitness variation\n        self.consequent_F = ctrl.Consequent(np.linspace(0.1, 0.9, 100), 'F')\n        self.consequent_CR = ctrl.Consequent(np.linspace(0.1, 0.9, 100), 'CR')\n        \n        # Define membership functions (example: triangular)\n        self.antecedent_stagnation['low'] = fuzz.trimf(self.antecedent_stagnation.universe, [0, 0, self.stagnation_threshold/2])\n        self.antecedent_stagnation['medium'] = fuzz.trimf(self.antecedent_stagnation.universe, [0, self.stagnation_threshold/2, self.stagnation_threshold])\n        self.antecedent_stagnation['high'] = fuzz.trimf(self.antecedent_stagnation.universe, [self.stagnation_threshold/2, self.stagnation_threshold, self.stagnation_threshold])\n\n        self.antecedent_fitness_variation['low'] = fuzz.trimf(self.antecedent_fitness_variation.universe, [0, 0, 0.5])\n        self.antecedent_fitness_variation['medium'] = fuzz.trimf(self.antecedent_fitness_variation.universe, [0, 0.5, 1])\n        self.antecedent_fitness_variation['high'] = fuzz.trimf(self.antecedent_fitness_variation.universe, [0.5, 1, 1])\n        \n        self.consequent_F['low'] = fuzz.trimf(self.consequent_F.universe, [0.1, 0.1, 0.5])\n        self.consequent_F['medium'] = fuzz.trimf(self.consequent_F.universe, [0.1, 0.5, 0.9])\n        self.consequent_F['high'] = fuzz.trimf(self.consequent_F.universe, [0.5, 0.9, 0.9])\n\n        self.consequent_CR['low'] = fuzz.trimf(self.consequent_CR.universe, [0.1, 0.1, 0.5])\n        self.consequent_CR['medium'] = fuzz.trimf(self.consequent_CR.universe, [0.1, 0.5, 0.9])\n        self.consequent_CR['high'] = fuzz.trimf(self.consequent_CR.universe, [0.5, 0.9, 0.9])\n\n        # Define rules\n        rule1 = ctrl.Rule(self.antecedent_stagnation['low'] & self.antecedent_fitness_variation['high'], [self.consequent_F['low'], self.consequent_CR['high']])\n        rule2 = ctrl.Rule(self.antecedent_stagnation['medium'] & self.antecedent_fitness_variation['medium'], [self.consequent_F['medium'], self.consequent_CR['medium']])\n        rule3 = ctrl.Rule(self.antecedent_stagnation['high'] & self.antecedent_fitness_variation['low'], [self.consequent_F['high'], self.consequent_CR['low']])\n\n        self.control_system = ctrl.ControlSystem([rule1, rule2, rule3])\n        self.parameter_control = ctrl.ControlSystemSimulation(self.control_system)\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index].copy()\n        self.lb = func.bounds.lb\n        self.ub = func.bounds.ub\n        self.best_fitness_history.append(self.f_opt)\n\n    def adjust_population_size(self):\n        if self.stagnation_counter > self.stagnation_threshold:\n            self.pop_size = max(self.min_pop_size, self.pop_size // 2)  # Reduce population\n            self.stagnation_counter = 0\n            print(f\"Population size reduced to {self.pop_size}\")\n        elif self.stagnation_counter > self.stagnation_threshold/2 and self.pop_size < self.max_pop_size:\n            self.pop_size = min(self.max_pop_size, self.pop_size * 2) # Increase population\n            print(f\"Population size increased to {self.pop_size}\")\n            \n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            # Parameter Adaptation using Fuzzy Logic\n            if len(self.best_fitness_history) > 1:\n                fitness_variation = abs(self.best_fitness_history[-1] - self.best_fitness_history[-2]) / max(abs(self.best_fitness_history[-1]), 1e-9)  # Normalize the fitness variation\n            else:\n                fitness_variation = 0.0\n            \n            self.parameter_control.input['stagnation'] = self.stagnation_counter\n            self.parameter_control.input['fitness_variation'] = fitness_variation\n            self.parameter_control.compute()\n\n            self.F = self.parameter_control.output['F']\n            self.CR = self.parameter_control.output['CR']\n            \n\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                v = self.population[i] + self.F * (x_r2 - x_r3)\n                v = np.clip(v, self.lb, self.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = self.population[i].copy()\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        u[j] = v[j]\n\n                # Evaluation\n                f_u = func(u)\n                self.budget -= 1\n\n                # Selection\n                if f_u < self.fitness[i]:\n                    self.fitness[i] = f_u\n                    self.population[i] = u\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u.copy()\n                        self.best_index = np.argmin(self.fitness)\n                        self.stagnation_counter = 0  # Reset stagnation counter\n                    else:\n                        self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter += 1\n            \n            self.best_fitness_history.append(self.f_opt)\n            self.adjust_population_size() # Adjust population size based on stagnation\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 111, in evaluate\n    exec(code, safe_globals, local_env)\n  File \"<string>\", line 2, in <module>\nModuleNotFoundError: No module named 'skfuzzy'\n.", "error": "", "parent_ids": ["7275b07c-5ed3-4b00-af0f-7de5c1bf2808"], "operator": null, "metadata": {}}
{"id": "7fd00506-bd05-4eb3-af88-a34e3a7508d4", "fitness": -Infinity, "name": "FuzzyAdaptiveDE", "description": "An Adaptive Differential Evolution strategy with a self-adjusting population size based on stagnation detection and dynamic parameter control using a fuzzy logic controller.", "code": "import numpy as np\nimport skfuzzy as fuzz\nfrom skfuzzy import control as ctrl\n\nclass FuzzyAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, min_pop_size=10, max_pop_size=100, stagnation_threshold=100):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = initial_pop_size\n        self.min_pop_size = min_pop_size\n        self.max_pop_size = max_pop_size\n        self.pop_size = initial_pop_size\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.population = None\n        self.fitness = None\n        self.best_index = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.F = 0.5  # Initial mutation factor\n        self.CR = 0.7 # Initial Crossover rate\n\n        # Fuzzy Logic Controller for F and CR adaptation\n        self.antecedent_stagnation = ctrl.Antecedent(np.linspace(0, self.stagnation_threshold, 1000), 'stagnation')\n        self.antecedent_fitness_variation = ctrl.Antecedent(np.linspace(0, 1, 1000), 'fitness_variation') # Normalized fitness variation\n        self.consequent_F = ctrl.Consequent(np.linspace(0.1, 0.9, 100), 'F')\n        self.consequent_CR = ctrl.Consequent(np.linspace(0.1, 0.9, 100), 'CR')\n        \n        # Define membership functions (example: triangular)\n        self.antecedent_stagnation['low'] = fuzz.trimf(self.antecedent_stagnation.universe, [0, 0, self.stagnation_threshold/2])\n        self.antecedent_stagnation['medium'] = fuzz.trimf(self.antecedent_stagnation.universe, [0, self.stagnation_threshold/2, self.stagnation_threshold])\n        self.antecedent_stagnation['high'] = fuzz.trimf(self.antecedent_stagnation.universe, [self.stagnation_threshold/2, self.stagnation_threshold, self.stagnation_threshold])\n\n        self.antecedent_fitness_variation['low'] = fuzz.trimf(self.antecedent_fitness_variation.universe, [0, 0, 0.5])\n        self.antecedent_fitness_variation['medium'] = fuzz.trimf(self.antecedent_fitness_variation.universe, [0, 0.5, 1])\n        self.antecedent_fitness_variation['high'] = fuzz.trimf(self.antecedent_fitness_variation.universe, [0.5, 1, 1])\n        \n        self.consequent_F['low'] = fuzz.trimf(self.consequent_F.universe, [0.1, 0.1, 0.5])\n        self.consequent_F['medium'] = fuzz.trimf(self.consequent_F.universe, [0.1, 0.5, 0.9])\n        self.consequent_F['high'] = fuzz.trimf(self.consequent_F.universe, [0.5, 0.9, 0.9])\n\n        self.consequent_CR['low'] = fuzz.trimf(self.consequent_CR.universe, [0.1, 0.1, 0.5])\n        self.consequent_CR['medium'] = fuzz.trimf(self.consequent_CR.universe, [0.1, 0.5, 0.9])\n        self.consequent_CR['high'] = fuzz.trimf(self.consequent_CR.universe, [0.5, 0.9, 0.9])\n\n        # Define rules\n        rule1 = ctrl.Rule(self.antecedent_stagnation['low'] & self.antecedent_fitness_variation['high'], [self.consequent_F['low'], self.consequent_CR['high']])\n        rule2 = ctrl.Rule(self.antecedent_stagnation['medium'] & self.antecedent_fitness_variation['medium'], [self.consequent_F['medium'], self.consequent_CR['medium']])\n        rule3 = ctrl.Rule(self.antecedent_stagnation['high'] & self.antecedent_fitness_variation['low'], [self.consequent_F['high'], self.consequent_CR['low']])\n\n        self.control_system = ctrl.ControlSystem([rule1, rule2, rule3])\n        self.parameter_control = ctrl.ControlSystemSimulation(self.control_system)\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index].copy()\n        self.lb = func.bounds.lb\n        self.ub = func.bounds.ub\n        self.best_fitness_history.append(self.f_opt)\n\n    def adjust_population_size(self):\n        if self.stagnation_counter > self.stagnation_threshold:\n            self.pop_size = max(self.min_pop_size, self.pop_size // 2)  # Reduce population\n            self.stagnation_counter = 0\n            print(f\"Population size reduced to {self.pop_size}\")\n        elif self.stagnation_counter > self.stagnation_threshold/2 and self.pop_size < self.max_pop_size:\n            self.pop_size = min(self.max_pop_size, self.pop_size * 2) # Increase population\n            print(f\"Population size increased to {self.pop_size}\")\n            \n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            # Parameter Adaptation using Fuzzy Logic\n            if len(self.best_fitness_history) > 1:\n                fitness_variation = abs(self.best_fitness_history[-1] - self.best_fitness_history[-2]) / max(abs(self.best_fitness_history[-1]), 1e-9)  # Normalize the fitness variation\n            else:\n                fitness_variation = 0.0\n            \n            self.parameter_control.input['stagnation'] = self.stagnation_counter\n            self.parameter_control.input['fitness_variation'] = fitness_variation\n            self.parameter_control.compute()\n\n            self.F = self.parameter_control.output['F']\n            self.CR = self.parameter_control.output['CR']\n            \n\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                v = self.population[i] + self.F * (x_r2 - x_r3)\n                v = np.clip(v, self.lb, self.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = self.population[i].copy()\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        u[j] = v[j]\n\n                # Evaluation\n                f_u = func(u)\n                self.budget -= 1\n\n                # Selection\n                if f_u < self.fitness[i]:\n                    self.fitness[i] = f_u\n                    self.population[i] = u\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u.copy()\n                        self.best_index = np.argmin(self.fitness)\n                        self.stagnation_counter = 0  # Reset stagnation counter\n                    else:\n                        self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter += 1\n            \n            self.best_fitness_history.append(self.f_opt)\n            self.adjust_population_size() # Adjust population size based on stagnation\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 111, in evaluate\n    exec(code, safe_globals, local_env)\n  File \"<string>\", line 2, in <module>\nModuleNotFoundError: No module named 'skfuzzy'\n.", "error": "", "parent_ids": ["7275b07c-5ed3-4b00-af0f-7de5c1bf2808"], "operator": null, "metadata": {}}
{"id": "7fd00506-bd05-4eb3-af88-a34e3a7508d4", "fitness": -Infinity, "name": "FuzzyAdaptiveDE", "description": "An Adaptive Differential Evolution strategy with a self-adjusting population size based on stagnation detection and dynamic parameter control using a fuzzy logic controller.", "code": "import numpy as np\nimport skfuzzy as fuzz\nfrom skfuzzy import control as ctrl\n\nclass FuzzyAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, min_pop_size=10, max_pop_size=100, stagnation_threshold=100):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = initial_pop_size\n        self.min_pop_size = min_pop_size\n        self.max_pop_size = max_pop_size\n        self.pop_size = initial_pop_size\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.population = None\n        self.fitness = None\n        self.best_index = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.F = 0.5  # Initial mutation factor\n        self.CR = 0.7 # Initial Crossover rate\n\n        # Fuzzy Logic Controller for F and CR adaptation\n        self.antecedent_stagnation = ctrl.Antecedent(np.linspace(0, self.stagnation_threshold, 1000), 'stagnation')\n        self.antecedent_fitness_variation = ctrl.Antecedent(np.linspace(0, 1, 1000), 'fitness_variation') # Normalized fitness variation\n        self.consequent_F = ctrl.Consequent(np.linspace(0.1, 0.9, 100), 'F')\n        self.consequent_CR = ctrl.Consequent(np.linspace(0.1, 0.9, 100), 'CR')\n        \n        # Define membership functions (example: triangular)\n        self.antecedent_stagnation['low'] = fuzz.trimf(self.antecedent_stagnation.universe, [0, 0, self.stagnation_threshold/2])\n        self.antecedent_stagnation['medium'] = fuzz.trimf(self.antecedent_stagnation.universe, [0, self.stagnation_threshold/2, self.stagnation_threshold])\n        self.antecedent_stagnation['high'] = fuzz.trimf(self.antecedent_stagnation.universe, [self.stagnation_threshold/2, self.stagnation_threshold, self.stagnation_threshold])\n\n        self.antecedent_fitness_variation['low'] = fuzz.trimf(self.antecedent_fitness_variation.universe, [0, 0, 0.5])\n        self.antecedent_fitness_variation['medium'] = fuzz.trimf(self.antecedent_fitness_variation.universe, [0, 0.5, 1])\n        self.antecedent_fitness_variation['high'] = fuzz.trimf(self.antecedent_fitness_variation.universe, [0.5, 1, 1])\n        \n        self.consequent_F['low'] = fuzz.trimf(self.consequent_F.universe, [0.1, 0.1, 0.5])\n        self.consequent_F['medium'] = fuzz.trimf(self.consequent_F.universe, [0.1, 0.5, 0.9])\n        self.consequent_F['high'] = fuzz.trimf(self.consequent_F.universe, [0.5, 0.9, 0.9])\n\n        self.consequent_CR['low'] = fuzz.trimf(self.consequent_CR.universe, [0.1, 0.1, 0.5])\n        self.consequent_CR['medium'] = fuzz.trimf(self.consequent_CR.universe, [0.1, 0.5, 0.9])\n        self.consequent_CR['high'] = fuzz.trimf(self.consequent_CR.universe, [0.5, 0.9, 0.9])\n\n        # Define rules\n        rule1 = ctrl.Rule(self.antecedent_stagnation['low'] & self.antecedent_fitness_variation['high'], [self.consequent_F['low'], self.consequent_CR['high']])\n        rule2 = ctrl.Rule(self.antecedent_stagnation['medium'] & self.antecedent_fitness_variation['medium'], [self.consequent_F['medium'], self.consequent_CR['medium']])\n        rule3 = ctrl.Rule(self.antecedent_stagnation['high'] & self.antecedent_fitness_variation['low'], [self.consequent_F['high'], self.consequent_CR['low']])\n\n        self.control_system = ctrl.ControlSystem([rule1, rule2, rule3])\n        self.parameter_control = ctrl.ControlSystemSimulation(self.control_system)\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index].copy()\n        self.lb = func.bounds.lb\n        self.ub = func.bounds.ub\n        self.best_fitness_history.append(self.f_opt)\n\n    def adjust_population_size(self):\n        if self.stagnation_counter > self.stagnation_threshold:\n            self.pop_size = max(self.min_pop_size, self.pop_size // 2)  # Reduce population\n            self.stagnation_counter = 0\n            print(f\"Population size reduced to {self.pop_size}\")\n        elif self.stagnation_counter > self.stagnation_threshold/2 and self.pop_size < self.max_pop_size:\n            self.pop_size = min(self.max_pop_size, self.pop_size * 2) # Increase population\n            print(f\"Population size increased to {self.pop_size}\")\n            \n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            # Parameter Adaptation using Fuzzy Logic\n            if len(self.best_fitness_history) > 1:\n                fitness_variation = abs(self.best_fitness_history[-1] - self.best_fitness_history[-2]) / max(abs(self.best_fitness_history[-1]), 1e-9)  # Normalize the fitness variation\n            else:\n                fitness_variation = 0.0\n            \n            self.parameter_control.input['stagnation'] = self.stagnation_counter\n            self.parameter_control.input['fitness_variation'] = fitness_variation\n            self.parameter_control.compute()\n\n            self.F = self.parameter_control.output['F']\n            self.CR = self.parameter_control.output['CR']\n            \n\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                v = self.population[i] + self.F * (x_r2 - x_r3)\n                v = np.clip(v, self.lb, self.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = self.population[i].copy()\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        u[j] = v[j]\n\n                # Evaluation\n                f_u = func(u)\n                self.budget -= 1\n\n                # Selection\n                if f_u < self.fitness[i]:\n                    self.fitness[i] = f_u\n                    self.population[i] = u\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u.copy()\n                        self.best_index = np.argmin(self.fitness)\n                        self.stagnation_counter = 0  # Reset stagnation counter\n                    else:\n                        self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter += 1\n            \n            self.best_fitness_history.append(self.f_opt)\n            self.adjust_population_size() # Adjust population size based on stagnation\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 111, in evaluate\n    exec(code, safe_globals, local_env)\n  File \"<string>\", line 2, in <module>\nModuleNotFoundError: No module named 'skfuzzy'\n.", "error": "", "parent_ids": ["7275b07c-5ed3-4b00-af0f-7de5c1bf2808"], "operator": null, "metadata": {}}
{"id": "2e6840e3-dae6-4112-a4d3-728a3ba4ea71", "fitness": 0.38777694506451227, "name": "SelfOrganizingDE", "description": "A self-organizing differential evolution algorithm that dynamically adjusts population diversity and mutation scaling factors based on the population's convergence state.", "code": "import numpy as np\n\nclass SelfOrganizingDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, initial_F=0.5, initial_CR=0.7, diversity_threshold=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = initial_F\n        self.CR = initial_CR\n        self.diversity_threshold = diversity_threshold\n        self.population = None\n        self.fitness = None\n        self.best_index = None\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index].copy()\n        self.lb = func.bounds.lb\n        self.ub = func.bounds.ub\n\n    def calculate_diversity(self):\n        \"\"\"Calculates the average distance of each individual from the population mean.\"\"\"\n        mean_position = np.mean(self.population, axis=0)\n        distances = np.linalg.norm(self.population - mean_position, axis=1)\n        return np.mean(distances)\n\n    def adjust_parameters(self):\n        \"\"\"Adjusts F and CR based on population diversity.\"\"\"\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold:\n            # Population is converging, increase exploration\n            self.F *= 1.1  # Increase F\n            self.CR *= 0.9 # Decrease CR\n        else:\n            # Population is diverse, increase exploitation\n            self.F *= 0.9  # Decrease F\n            self.CR *= 1.1 # Increase CR\n\n        self.F = np.clip(self.F, 0.1, 0.9)\n        self.CR = np.clip(self.CR, 0.1, 0.9)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            self.adjust_parameters() # Adjust F and CR dynamically\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                v = self.population[i] + self.F * (x_r2 - x_r3)\n                v = np.clip(v, self.lb, self.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = self.population[i].copy()\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        u[j] = v[j]\n\n                # Evaluation\n                f_u = func(u)\n                self.budget -= 1\n\n                # Selection\n                if f_u < self.fitness[i]:\n                    self.fitness[i] = f_u\n                    self.population[i] = u\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u.copy()\n                        self.best_index = np.argmin(self.fitness)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm SelfOrganizingDE scored 0.388 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["7275b07c-5ed3-4b00-af0f-7de5c1bf2808"], "operator": null, "metadata": {"aucs": [0.14950918348687814, 0.2813649156596756, 0.39018176429095797, 0.3578120471806211, 0.22393212095735093, 0.3874596298462508, 0.27230259217954833, 0.3228020799929008, 0.2822408020559618, 0.18645031544635227, 0.6114751604537811, 0.9933380823113966, 0.3847244640124673, 0.3018770989985765, 0.7311308951358717, 0.38415149951020877, 0.34331970480605545, 0.48510072552237016, 0.1903081941444098, 0.4760576252986106]}}
{"id": "b1ee6e66-8b2f-4aed-8664-22d03de5a5b5", "fitness": 0.26446463205946613, "name": "ArchiveDE", "description": "A Differential Evolution strategy employing a modified current-to-pbest mutation with dynamically adjusted archive and parameter adaptation using a success history.", "code": "import numpy as np\n\nclass ArchiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, pbest_rate=0.1, F=0.5, CR=0.7, F_lr=0.1, CR_lr=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.pbest_rate = pbest_rate\n        self.F = F\n        self.CR = CR\n        self.F_lr = F_lr\n        self.CR_lr = CR_lr\n        self.population = None\n        self.fitness = None\n        self.best_index = None\n        self.archive = None\n        self.archive_fitness = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.success_history_F = []\n        self.success_history_CR = []\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index].copy()\n        self.archive = np.zeros((self.archive_size, self.dim))\n        self.archive_fitness = np.full(self.archive_size, np.inf)\n\n    def current_to_pbest_mutation(self, x, pbest_indices, x_r1, F):\n        x_pbest = self.population[np.random.choice(pbest_indices)]\n        return x + F * (x_pbest - x) + F * (x_r1 - x)\n    \n    def update_archive(self, x, f_x):\n        if np.any(f_x < self.archive_fitness):\n            worst_index = np.argmax(self.archive_fitness)\n            self.archive[worst_index] = x\n            self.archive_fitness[worst_index] = f_x\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Parameter adaptation using success history\n                if self.success_history_F:\n                    self.F = np.random.choice(self.success_history_F)\n                if self.success_history_CR:\n                    self.CR = np.random.choice(self.success_history_CR)\n                    \n                # Mutation\n                pbest_count = max(1, int(self.pbest_rate * self.pop_size))\n                pbest_indices = np.argsort(self.fitness)[:pbest_count]\n                \n                indices = np.random.choice(self.pop_size, 1, replace=False)\n                x_r1 = self.population[indices[0]]\n                \n                v = self.current_to_pbest_mutation(self.population[i], pbest_indices, x_r1, self.F)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = self.population[i].copy()\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        u[j] = v[j]\n\n                # Evaluation\n                f_u = func(u)\n                self.budget -= 1\n\n                # Selection\n                if f_u < self.fitness[i]:\n                    self.success_history_F.append(self.F)\n                    self.success_history_CR.append(self.CR)\n                    \n                    self.fitness[i] = f_u\n                    self.population[i] = u\n                    \n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u.copy()\n                        \n                    self.update_archive(u, f_u)\n\n                # Limit the size of the success history\n                self.success_history_F = self.success_history_F[-10:]\n                self.success_history_CR = self.success_history_CR[-10:]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm ArchiveDE scored 0.264 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["16728a8b-b73f-406a-90f8-787e08fbc64e"], "operator": null, "metadata": {"aucs": [0.1652876217713246, 0.20461862991512147, 0.35578954501245585, 0.24647069940627686, 0.22314192992910464, 0.27646226269139373, 0.2652086231411327, 0.26542090134646323, 0.31893093622654467, 0.16346204096933548, 0.2113129570791964, 0.24304127235986084, 0.26232882463476637, 0.2541533666845701, 0.2978171753096961, 0.33196770163248435, 0.27790836849588896, 0.2422502835867979, 0.21066052304627714, 0.4730589779506318]}}
{"id": "5945435f-193d-49bd-bf55-504e64e6eddc", "fitness": 0.7035331020909122, "name": "SelfOrganizingSpeciationDE", "description": "A self-organizing speciation DE, where individuals adaptively adjust their mutation strategy and niche radius based on local fitness landscape information.", "code": "import numpy as np\n\nclass SelfOrganizingSpeciationDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, num_niches=5, initial_niche_radius=0.5, adaptation_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.num_niches = num_niches\n        self.initial_niche_radius = initial_niche_radius\n        self.adaptation_rate = adaptation_rate\n        self.population = None\n        self.fitness = None\n        self.niche_assignments = None\n        self.niche_radii = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.mutation_strategies = [self.mutation_DE_rand1, self.mutation_DE_best1]\n        self.num_strategies = len(self.mutation_strategies)\n        self.strategy_successes = np.ones((self.num_niches, self.num_strategies)) #Niche-specific strategy successes\n        self.lb = None\n        self.ub = None\n\n    def initialize_population(self, func):\n        self.lb = func.bounds.lb\n        self.ub = func.bounds.ub\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.niche_assignments = np.random.choice(self.num_niches, size=self.pop_size)\n        self.niche_radii = np.full(self.num_niches, self.initial_niche_radius)\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index].copy()\n\n    def mutation_DE_rand1(self, population, i, F):\n        indices = np.random.choice(self.pop_size, 3, replace=False)\n        x_r1, x_r2, x_r3 = population[indices]\n        return population[i] + F * (x_r2 - x_r3)\n\n    def mutation_DE_best1(self, population, i, F):\n        best_index = np.argmin(self.fitness)\n        indices = np.random.choice(self.pop_size, 2, replace=False)\n        x_r1, x_r2 = population[indices]\n        return population[i] + F * (self.population[best_index] - population[i]) + F * (x_r1 - x_r2)\n\n\n    def assign_to_niche(self, individual):\n        distances = np.linalg.norm(self.population - individual, axis=1)\n        niche_fitnesses = np.zeros(self.num_niches)\n        for niche in range(self.num_niches):\n            niche_members = self.population[self.niche_assignments == niche]\n            if len(niche_members) > 0:\n                niche_fitnesses[niche] = np.mean(self.fitness[self.niche_assignments == niche])\n            else:\n                niche_fitnesses[niche] = np.inf  # Penalize empty niches\n\n        # Assign to the niche with the closest member and the best fitness\n        best_niche = np.argmin(niche_fitnesses)\n        return best_niche\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Niche-specific strategy selection\n                niche = self.niche_assignments[i]\n                strategy_probabilities = self.strategy_successes[niche]\n                strategy_probabilities /= np.sum(strategy_probabilities)\n                strategy_index = np.random.choice(self.num_strategies, p=strategy_probabilities)\n                mutation_strategy = self.mutation_strategies[strategy_index]\n\n                # Mutation\n                F = np.random.uniform(0.2, 0.8)\n                v = mutation_strategy(self.population, i, F)\n                v = np.clip(v, self.lb, self.ub)\n\n                # Crossover\n                CR = np.random.uniform(0.3, 0.9)\n                j_rand = np.random.randint(self.dim)\n                u = self.population[i].copy()\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        u[j] = v[j]\n\n                # Evaluation\n                f_u = func(u)\n                self.budget -= 1\n\n                # Selection and Niche Adjustment\n                if f_u < self.fitness[i]:\n                    self.strategy_successes[niche, strategy_index] += 1\n                    self.fitness[i] = f_u\n                    self.population[i] = u\n\n                    # Re-assign niche\n                    new_niche = self.assign_to_niche(u)\n                    self.niche_assignments[i] = new_niche\n                    \n                    # Adapt niche radius (simplified - can be enhanced)\n                    if np.random.rand() < self.adaptation_rate:\n                        self.niche_radii[niche] *= np.random.uniform(0.9, 1.1)\n                        self.niche_radii[niche] = np.clip(self.niche_radii[niche], 0.1, 1.0)\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u.copy()\n                else:\n                     self.strategy_successes[niche, strategy_index] *= 0.9 # Reduce success if the strategy didn't improve\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm SelfOrganizingSpeciationDE scored 0.704 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["7275b07c-5ed3-4b00-af0f-7de5c1bf2808"], "operator": null, "metadata": {"aucs": [0.23639132728875956, 0.7294591408459665, 0.7829432066543419, 0.8982430169104866, 0.7722542974271376, 0.8388650866670204, 0.6332417152433023, 0.7327305058532703, 0.7292620221680366, 0.6069161551401603, 0.8891448707539099, 0.984763780828484, 0.48001166507601745, 0.7475032600974184, 0.9213489552118032, 0.81023484993706, 0.6744001778720679, 0.8726465782874934, 0.21564762594045173, 0.5146538036150556]}}
{"id": "43eb92e0-471c-49b6-aadc-482282714efe", "fitness": -Infinity, "name": "AdaptiveDE", "description": "A DE variant with a self-adaptive population size and a restart mechanism based on stagnation detection.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, F=0.5, CR=0.7, stagnation_threshold=500):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.F = F\n        self.CR = CR\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        if self.fitness[self.best_index] < self.f_opt:\n            self.f_opt = self.fitness[self.best_index]\n            self.x_opt = self.population[self.best_index].copy()\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            if self.budget <= 0:\n                break\n\n            indices = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[indices]\n            \n            v = self.population[i] + self.F * (x_r1 - x_r2)\n            v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n            j_rand = np.random.randint(self.dim)\n            u = self.population[i].copy()\n            for k in range(self.dim):\n                if np.random.rand() < self.CR or k == j_rand:\n                    u[k] = v[k]\n\n            f_u = func(u)\n            self.budget -= 1\n\n            if f_u < self.fitness[i]:\n                self.fitness[i] = f_u\n                self.population[i] = u\n                if f_u < self.f_opt:\n                    self.f_opt = f_u\n                    self.x_opt = u.copy()\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n    def adapt_population_size(self):\n        if self.stagnation_counter > self.stagnation_threshold:\n            # Stagnation detected, restart with a smaller population\n            self.pop_size = max(10, self.pop_size // 2)\n            self.stagnation_counter = 0\n            return True\n        elif self.stagnation_counter < self.stagnation_threshold // 4 and self.pop_size < 200:  # limit growth\n            self.pop_size = min(200, self.pop_size * 2) # increase population if doing well\n        return False\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            self.evolve(func)\n\n            if self.adapt_population_size():\n                # Re-initialize population after adaptation\n                self.initialize_population(func) # re-initialize after restart\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 144, in evaluate\n    algorithm(f_new)\n  File \"<string>\", line 70, in __call__\n  File \"<string>\", line 32, in evolve\nIndexError: index 94 is out of bounds for axis 0 with size 50\n.", "error": "", "parent_ids": ["b2424024-f5c4-4f1c-bfe6-fd36d287e428"], "operator": null, "metadata": {}}
{"id": "d091f4fe-8099-4cfb-b207-c0dc5295d1c7", "fitness": 0.0, "name": "AdaptiveAgingDE", "description": "Adaptive Differential Evolution with Aging and Re-initialization, incorporating both population-wide adaptation and individual-level aging to promote exploration and prevent premature convergence.", "code": "import numpy as np\n\nclass AdaptiveAgingDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, age_limit=50, reinit_prob=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.age_limit = age_limit\n        self.reinit_prob = reinit_prob\n        self.population = None\n        self.fitness = None\n        self.ages = None\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.ages = np.zeros(self.pop_size, dtype=int)\n        self.budget -= self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n\n    def mutate(self, func, i):\n        indices = np.random.choice(self.pop_size, 3, replace=False)\n        x_r1, x_r2, x_r3 = self.population[indices]\n        v = self.population[i] + self.F * (x_r1 - x_r2)\n        v = np.clip(v, func.bounds.lb, func.bounds.ub)\n        return v\n\n    def crossover(self, x, v):\n        j_rand = np.random.randint(self.dim)\n        u = x.copy()\n        for k in range(self.dim):\n            if np.random.rand() < self.CR or k == j_rand:\n                u[k] = v[k]\n        return u\n\n    def reinitialize_individual(self, func, i):\n        self.population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub)\n        self.fitness[i] = func(self.population[i])\n        self.ages[i] = 0\n        self.budget -= 1\n        if self.fitness[i] < self.f_opt:\n            self.f_opt = self.fitness[i]\n            self.x_opt = self.population[i].copy()\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            # Adaptive F and CR (population-wide)\n            self.F = np.random.normal(0.5, 0.1)\n            self.CR = np.random.normal(0.7, 0.1)\n            self.F = np.clip(self.F, 0.1, 0.9)\n            self.CR = np.clip(self.CR, 0.1, 0.9)\n\n            for i in range(self.pop_size):\n                v = self.mutate(func, i)\n                u = self.crossover(self.population[i], v)\n                \n                f_u = func(u)\n                self.budget -= 1\n                \n                if f_u < self.fitness[i]:\n                    self.fitness[i] = f_u\n                    self.population[i] = u\n                    self.ages[i] = 0  # Reset age upon improvement\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u.copy()\n                else:\n                    self.ages[i] += 1  # Increment age\n\n                # Aging mechanism: Re-initialize if age exceeds limit\n                if self.ages[i] > self.age_limit or np.random.rand() < self.reinit_prob:\n                    self.reinitialize_individual(func, i)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveAgingDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b2424024-f5c4-4f1c-bfe6-fd36d287e428"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "04ea838d-4968-4c49-880a-1622409fa5c0", "fitness": -Infinity, "name": "AdaptiveDE", "description": "A population-based algorithm with a self-adjusting strategy that combines differential evolution with a Gaussian mutation operator, dynamically adjusting parameters based on success rates and adaptive population sizing.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=None, F=0.5, CR=0.7, gaussian_prob=0.1, success_rate_memory=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 5 * dim # Adaptive population size\n        self.F = F\n        self.CR = CR\n        self.gaussian_prob = gaussian_prob\n        self.success_rate_memory = success_rate_memory\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.success_F = np.zeros(self.success_rate_memory)\n        self.success_CR = np.zeros(self.success_rate_memory)\n        self.success_idx = 0\n        self.population_history = []\n    \n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        best_idx = np.argmin(self.fitness)\n        if self.fitness[best_idx] < self.f_opt:\n            self.f_opt = self.fitness[best_idx]\n            self.x_opt = self.population[best_idx].copy()\n\n    def gaussian_mutation(self, x, func, scale=0.1):\n        x_new = x + np.random.normal(0, scale * (func.bounds.ub - func.bounds.lb))\n        x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n        return x_new\n    \n    def update_parameters(self, success, F, CR):\n        self.success_F[self.success_idx] = F\n        self.success_CR[self.success_idx] = CR\n        self.success_idx = (self.success_idx + 1) % self.success_rate_memory\n        \n        # Adapt F and CR based on recent successes (simple average)\n        self.F = np.mean(self.success_F[self.success_F != 0]) if np.any(self.success_F != 0) else 0.5\n        self.CR = np.mean(self.success_CR[self.success_CR != 0]) if np.any(self.success_CR != 0) else 0.7\n\n        # Adjust population size if stagnation detected. Store population history\n        self.population_history.append(np.copy(self.population))\n        if len(self.population_history) > 5:\n            recent_populations = self.population_history[-5:]\n            diversity = np.mean([np.linalg.norm(p - recent_populations[-1], axis=1).mean() for p in recent_populations[:-1]])\n            if diversity < 1e-6: # Low diversity: Stagnation\n                self.pop_size = int(self.pop_size * 0.9)  # Reduce population size\n                self.pop_size = max(self.pop_size, 5) # Ensure minimum size\n\n            if diversity > 0.1:  # High diversity\n                self.pop_size = int(self.pop_size * 1.1)  # Increase population size\n                self.pop_size = min(self.pop_size, 5 * self.dim)  # Cap the population size\n\n            self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n            self.fitness = np.array([func(x) for x in self.population])\n            self.budget -= self.pop_size\n            best_idx = np.argmin(self.fitness)\n            if self.fitness[best_idx] < self.f_opt:\n                self.f_opt = self.fitness[best_idx]\n                self.x_opt = self.population[best_idx].copy()\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        \n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                if np.random.rand() < self.gaussian_prob:\n                    v = self.gaussian_mutation(self.population[i], func)\n                else:\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    v = self.population[i] + self.F * (x_r1 - x_r2)\n                    v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = self.population[i].copy()\n                for k in range(self.dim):\n                    if np.random.rand() < self.CR or k == j_rand:\n                        u[k] = v[k]\n                \n                # Evaluation\n                f_u = func(u)\n                self.budget -= 1\n                \n                # Selection\n                if f_u < self.fitness[i]:\n                    self.update_parameters(True, self.F, self.CR)\n                    self.fitness[i] = f_u\n                    self.population[i] = u\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u.copy()\n                else:\n                    self.update_parameters(False, self.F, self.CR)\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 93, in __call__\n  File \"<string>\", line 57, in update_parameters\nNameError: name 'func' is not defined\n.", "error": "", "parent_ids": ["b2424024-f5c4-4f1c-bfe6-fd36d287e428"], "operator": null, "metadata": {}}
{"id": "a69d7110-b16f-4469-944c-60863c609004", "fitness": -Infinity, "name": "RingCooperativeDE", "description": "A cooperative coevolution algorithm with a ring topology, where sub-populations optimize different dimensions and cyclically share information with neighbors.", "code": "import numpy as np\n\nclass RingCooperativeDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, num_subpops=5, F=0.5, CR=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.num_subpops = num_subpops\n        self.F = F\n        self.CR = CR\n        self.subpop_dims = [list(range(i * (dim // num_subpops), (i + 1) * (dim // num_subpops))) for i in range(num_subpops)]\n        remaining_dims = dim % num_subpops\n        for i in range(remaining_dims):\n            self.subpop_dims[i].append(num_subpops * (dim // num_subpops) + i)\n        self.subpops = [np.random.rand(pop_size, len(dims)) for dims in self.subpop_dims]\n        self.fitness = [np.zeros(pop_size) for _ in range(num_subpops)]\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.lb = None\n        self.ub = None\n\n    def evaluate(self, func, subpop_index):\n        for i in range(self.pop_size):\n            x = np.zeros(self.dim)\n            start = 0\n            for j in range(self.num_subpops):\n                if j == subpop_index:\n                    x[np.array(self.subpop_dims[j])] = self.lb[np.array(self.subpop_dims[j])] + self.subpops[j][i] * (self.ub[np.array(self.subpop_dims[j])] - self.lb[np.array(self.subpop_dims[j])])\n                else:\n                    best_idx = np.argmin(self.fitness[j])\n                    x[np.array(self.subpop_dims[j])] = self.lb[np.array(self.subpop_dims[j])] + self.subpops[j][best_idx] * (self.ub[np.array(self.subpop_dims[j])] - self.lb[np.array(self.subpop_dims[j])])\n            f = func(x)\n            self.fitness[subpop_index][i] = f\n            self.budget -= 1\n\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n\n    def evolve_subpop(self, func, subpop_index):\n        dims = self.subpop_dims[subpop_index]\n        lb_sub = self.lb[np.array(dims)]\n        ub_sub = self.ub[np.array(dims)]\n        \n        for i in range(self.pop_size):\n            # Mutation\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x1, x2, x3 = self.subpops[subpop_index][idxs]\n            v = self.subpops[subpop_index][i] + self.F * (x2 - x3)\n            v = np.clip(v, 0, 1)\n\n            # Crossover\n            j_rand = np.random.randint(len(dims))\n            u = self.subpops[subpop_index][i].copy()\n            for j in range(len(dims)):\n                if np.random.rand() < self.CR or j == j_rand:\n                    u[j] = v[j]\n\n            # Create full vector and evaluate fitness\n            x_trial = np.zeros(self.dim)\n            start = 0\n            for k in range(self.num_subpops):\n                if k == subpop_index:\n                    x_trial[np.array(self.subpop_dims[k])] = lb_sub + u * (ub_sub - lb_sub)\n                else:\n                     best_idx = np.argmin(self.fitness[k])\n                     x_trial[np.array(self.subpop_dims[k])] = self.lb[np.array(self.subpop_dims[k])] + self.subpops[k][best_idx] * (self.ub[np.array(self.subpop_dims[k])] - self.lb[np.array(self.subpop_dims[k])])\n           \n            f_trial = func(x_trial)\n            self.budget -= 1\n\n            if f_trial < self.fitness[subpop_index][i]:\n                self.subpops[subpop_index][i] = u\n                self.fitness[subpop_index][i] = f_trial\n                \n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = x_trial.copy()\n\n\n    def __call__(self, func):\n        self.lb = func.bounds.lb\n        self.ub = func.bounds.ub\n        \n        # Initialize sub-populations\n        for i in range(self.num_subpops):\n            lb_sub = self.lb[np.array(self.subpop_dims[i])]\n            ub_sub = self.ub[np.array(self.subpop_dims[i])]\n            self.subpops[i] = np.random.rand(self.pop_size, len(self.subpop_dims[i]))\n            \n        for i in range(self.num_subpops):\n            self.evaluate(func, i)\n        \n        while self.budget > 0:\n            for i in range(self.num_subpops):\n                self.evolve_subpop(func, i)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 87, in __call__\nIndexError: arrays used as indices must be of integer (or boolean) type\n.", "error": "", "parent_ids": ["5945435f-193d-49bd-bf55-504e64e6eddc"], "operator": null, "metadata": {}}
{"id": "c0b69fca-fa27-4985-b306-b8ea2a52a9a9", "fitness": -Infinity, "name": "AdaptiveDELocalSearch", "description": "Adaptive Differential Evolution with a Local Search phase triggered by stagnation detection and restarts when diversity is too low.", "code": "import numpy as np\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=50, initial_F=0.5, initial_CR=0.7, stagnation_threshold=100, diversity_threshold=0.1, local_search_probability=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = initial_F\n        self.CR = initial_CR\n        self.stagnation_threshold = stagnation_threshold\n        self.diversity_threshold = diversity_threshold\n        self.local_search_probability = local_search_probability\n        self.population = None\n        self.fitness = None\n        self.best_index = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.stagnation_counter = 0\n        self.lb = None\n        self.ub = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index].copy()\n        self.lb = func.bounds.lb\n        self.ub = func.bounds.ub\n\n    def calculate_diversity(self):\n        \"\"\"Calculates the average distance of each individual from the population mean.\"\"\"\n        mean_position = np.mean(self.population, axis=0)\n        distances = np.linalg.norm(self.population - mean_position, axis=1)\n        return np.mean(distances)\n\n    def adjust_parameters(self):\n        \"\"\"Adjusts F and CR adaptively.\"\"\"\n        if np.random.rand() < 0.1:  # Adjust parameters with a probability\n            self.F = np.random.uniform(0.3, 0.9)\n            self.CR = np.random.uniform(0.1, 0.9)\n\n\n    def local_search(self, func, x, radius=0.1):\n         \"\"\"Performs a simple local search around a given solution.\"\"\"\n         x_new = x.copy()\n         for i in range(self.dim):\n             delta = np.random.uniform(-radius, radius)\n             x_new[i] = np.clip(x[i] + delta, self.lb, self.ub)\n         f_new = func(x_new)\n         self.budget -=1\n         return f_new, x_new\n\n    def check_stagnation(self):\n        \"\"\"Checks if the optimization is stagnating.\"\"\"\n        current_best_fitness = self.fitness[self.best_index]\n        if current_best_fitness >= self.f_opt:\n            self.stagnation_counter += 1\n        else:\n            self.stagnation_counter = 0\n            self.f_opt = current_best_fitness\n\n        if self.stagnation_counter > self.stagnation_threshold:\n            return True\n        else:\n            return False\n\n    def restart_population(self, func):\n        \"\"\"Restarts the population if diversity is too low.\"\"\"\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index].copy()\n        self.stagnation_counter = 0  # Reset stagnation counter\n\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            self.adjust_parameters()\n\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                v = self.population[i] + self.F * (x_r2 - x_r3)\n                v = np.clip(v, self.lb, self.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = self.population[i].copy()\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        u[j] = v[j]\n\n                # Evaluation\n                f_u = func(u)\n                self.budget -= 1\n\n                # Selection\n                if f_u < self.fitness[i]:\n                    self.fitness[i] = f_u\n                    self.population[i] = u\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u.copy()\n                        self.best_index = np.argmin(self.fitness)\n            \n            # Local Search\n            if np.random.rand() < self.local_search_probability:\n               f_ls, x_ls = self.local_search(func, self.x_opt)\n               if f_ls < self.f_opt:\n                   self.f_opt = f_ls\n                   self.x_opt = x_ls.copy()\n                   self.fitness[self.best_index] = f_ls\n                   self.population[self.best_index] = x_ls.copy()\n\n\n            # Stagnation Check\n            if self.check_stagnation():\n                diversity = self.calculate_diversity()\n                if diversity < self.diversity_threshold:\n                    self.restart_population(func)\n                else:\n                    #If stagnation is detected, but diversity is still good, we increase exploration\n                    self.F = np.clip(self.F*1.2, 0.1, 0.9)\n                    self.CR = np.clip(self.CR*0.8, 0.1, 0.9)\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "An exception occured: TypeError: only size-1 arrays can be converted to Python scalars\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 144, in evaluate\n    algorithm(f_new)\n  File \"<string>\", line 116, in __call__\n  File \"<string>\", line 50, in local_search\nValueError: setting an array element with a sequence.\n.", "error": "", "parent_ids": ["2e6840e3-dae6-4112-a4d3-728a3ba4ea71"], "operator": null, "metadata": {}}
{"id": "6717cf7e-be50-4abb-bcad-ee60c1b1e507", "fitness": 0.0, "name": "AdaptiveDE", "description": "An adaptive Differential Evolution strategy that dynamically adjusts its parameters based on the improvement rate of the population, incorporating a local search mechanism when stagnation is detected.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_init=0.5, CR_init=0.7, local_search_prob=0.1, stagnation_threshold=1e-5, stagnation_iter=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.CR = CR_init\n        self.local_search_prob = local_search_prob\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_iter = stagnation_iter\n        self.population = None\n        self.fitness = None\n        self.best_index = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.inf\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index].copy()\n        self.previous_best_fitness = self.f_opt\n\n    def mutation(self, i):\n        indices = np.random.choice(self.pop_size, 3, replace=False)\n        x_r1, x_r2, x_r3 = self.population[indices]\n        return self.population[i] + self.F * (x_r2 - x_r3)\n\n    def crossover(self, mutant, target):\n        j_rand = np.random.randint(self.dim)\n        trial = target.copy()\n        for j in range(self.dim):\n            if np.random.rand() < self.CR or j == j_rand:\n                trial[j] = mutant[j]\n        return trial\n\n    def local_search(self, x, func, step_size=0.1):\n        x_new = x.copy()\n        for i in range(self.dim):\n            delta = np.random.uniform(-step_size, step_size)\n            x_new[i] = x[i] + delta\n            x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n        return x_new\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                mutant = self.mutation(i)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = self.crossover(mutant, self.population[i])\n\n                # Evaluation\n                f_trial = func(trial)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n                    \n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial.copy()\n                \n                # Local Search\n                if np.random.rand() < self.local_search_prob:\n                    trial_local = self.local_search(self.population[i], func)\n                    f_trial_local = func(trial_local)\n                    self.budget -=1\n                    if f_trial_local < self.fitness[i]:\n                        self.fitness[i] = f_trial_local\n                        self.population[i] = trial_local\n                        if f_trial_local < self.f_opt:\n                            self.f_opt = f_trial_local\n                            self.x_opt = trial_local.copy()\n                            \n            # Stagnation Check and Parameter Adaptation\n            if abs(self.f_opt - self.previous_best_fitness) < self.stagnation_threshold:\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n                self.previous_best_fitness = self.f_opt\n            \n            if self.stagnation_counter >= self.stagnation_iter:\n                # Increase F and CR to promote exploration\n                self.F = min(1.0, self.F * 1.2)\n                self.CR = min(1.0, self.CR * 1.2)\n                self.stagnation_counter = 0\n                # Resetting the population by re-initializing a fraction of the population\n                num_reset = int(0.1 * self.pop_size)\n                reset_indices = np.random.choice(self.pop_size, num_reset, replace=False)\n                self.population[reset_indices] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(num_reset, self.dim))\n                for idx in reset_indices:\n                    self.fitness[idx] = func(self.population[idx])\n                    self.budget -= 1\n                \n                self.best_index = np.argmin(self.fitness)\n                self.f_opt = self.fitness[self.best_index]\n                self.x_opt = self.population[self.best_index].copy()\n                self.previous_best_fitness = self.f_opt\n                \n            else:\n                # Reduce F and CR to promote exploitation\n                self.F = max(0.1, self.F * 0.9)\n                self.CR = max(0.1, self.CR * 0.9)\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b1ee6e66-8b2f-4aed-8664-22d03de5a5b5"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "8777d811-4299-4293-adae-82d21ef86e7b", "fitness": -Infinity, "name": "AdaptiveAgingDE", "description": "An adaptive differential evolution with a dynamic population size and an aging mechanism to remove stagnant individuals.", "code": "import numpy as np\n\nclass AdaptiveAgingDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, archive_size=10, pbest_rate=0.1, F=0.5, CR=0.7, aging_rate=0.05, age_limit=5):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = initial_pop_size\n        self.pop_size = initial_pop_size  # Dynamic population size\n        self.archive_size = archive_size\n        self.pbest_rate = pbest_rate\n        self.F = F\n        self.CR = CR\n        self.aging_rate = aging_rate  # Probability of removing an individual\n        self.age_limit = age_limit  # Max age before removal\n        self.population = None\n        self.fitness = None\n        self.ages = None  # Age of each individual\n        self.best_index = None\n        self.archive = None\n        self.archive_fitness = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.success_history_F = []\n        self.success_history_CR = []\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.ages = np.zeros(self.pop_size)\n        self.budget -= self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index].copy()\n        self.archive = np.zeros((self.archive_size, self.dim))\n        self.archive_fitness = np.full(self.archive_size, np.inf)\n\n    def current_to_pbest_mutation(self, x, pbest_indices, x_r1, F):\n        x_pbest = self.population[np.random.choice(pbest_indices)]\n        return x + F * (x_pbest - x) + F * (x_r1 - x)\n    \n    def update_archive(self, x, f_x):\n        if np.any(f_x < self.archive_fitness):\n            worst_index = np.argmax(self.archive_fitness)\n            self.archive[worst_index] = x\n            self.archive_fitness[worst_index] = f_x\n\n    def remove_stagnant_individuals(self):\n        # Identify stagnant individuals based on age\n        stagnant_indices = np.where(self.ages >= self.age_limit)[0]\n        num_to_remove = len(stagnant_indices)\n\n        if num_to_remove > 0:\n            # Replace stagnant individuals with new random individuals\n            new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(num_to_remove, self.dim))\n            new_fitness = np.array([func(x) for x in new_individuals])\n            self.budget -= num_to_remove\n\n            self.population[stagnant_indices] = new_individuals\n            self.fitness[stagnant_indices] = new_fitness\n            self.ages[stagnant_indices] = 0  # Reset age\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            self.remove_stagnant_individuals()\n\n            for i in range(self.pop_size):\n                # Parameter adaptation using success history\n                if self.success_history_F:\n                    self.F = np.random.choice(self.success_history_F)\n                if self.success_history_CR:\n                    self.CR = np.random.choice(self.success_history_CR)\n                    \n                # Mutation\n                pbest_count = max(1, int(self.pbest_rate * self.pop_size))\n                pbest_indices = np.argsort(self.fitness)[:pbest_count]\n                \n                indices = np.random.choice(self.pop_size, 1, replace=False)\n                x_r1 = self.population[indices[0]]\n                \n                v = self.current_to_pbest_mutation(self.population[i], pbest_indices, x_r1, self.F)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = self.population[i].copy()\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        u[j] = v[j]\n\n                # Evaluation\n                f_u = func(u)\n                self.budget -= 1\n\n                # Selection\n                if f_u < self.fitness[i]:\n                    self.success_history_F.append(self.F)\n                    self.success_history_CR.append(self.CR)\n                    \n                    self.fitness[i] = f_u\n                    self.population[i] = u\n                    self.ages[i] = 0  # Reset age if improved\n                    \n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u.copy()\n                        \n                    self.update_archive(u, f_u)\n                else:\n                    # Increment age if not improved\n                    self.ages[i] += 1\n\n                # Limit the size of the success history\n                self.success_history_F = self.success_history_F[-10:]\n                self.success_history_CR = self.success_history_CR[-10:]\n            \n            #Probabilistically remove individuals from the population\n            removal_prob = self.aging_rate\n            remove = np.random.rand(self.pop_size) < removal_prob\n            num_removed = np.sum(remove)\n\n            if num_removed > 0:\n\n                new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(num_removed, self.dim))\n                new_fitness = np.array([func(x) for x in new_individuals])\n                self.budget -= num_removed\n\n                self.population[remove] = new_individuals\n                self.fitness[remove] = new_fitness\n                self.ages[remove] = 0  # Reset age\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 144, in evaluate\n    algorithm(f_new)\n  File \"<string>\", line 66, in __call__\n  File \"<string>\", line 54, in remove_stagnant_individuals\nNameError: name 'func' is not defined\n.", "error": "", "parent_ids": ["b1ee6e66-8b2f-4aed-8664-22d03de5a5b5"], "operator": null, "metadata": {}}
{"id": "18cfc78c-fdc4-4e49-8809-0a7c890e92a6", "fitness": 0.0, "name": "AdaptiveODE", "description": "Adaptive Differential Evolution with orthogonal learning and a restart mechanism to escape local optima.", "code": "import numpy as np\n\nclass AdaptiveODE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, pbest_rate=0.1, F=0.5, CR=0.7, F_lr=0.1, CR_lr=0.1, restart_trigger=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.pbest_rate = pbest_rate\n        self.F = F\n        self.CR = CR\n        self.F_lr = F_lr\n        self.CR_lr = CR_lr\n        self.population = None\n        self.fitness = None\n        self.best_index = None\n        self.archive = None\n        self.archive_fitness = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.success_history_F = []\n        self.success_history_CR = []\n        self.restart_trigger = restart_trigger\n        self.no_improvement_count = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index].copy()\n        self.archive = np.zeros((self.archive_size, self.dim))\n        self.archive_fitness = np.full(self.archive_size, np.inf)\n\n    def current_to_pbest_mutation(self, x, pbest_indices, x_r1, F):\n        x_pbest = self.population[np.random.choice(pbest_indices)]\n        return x + F * (x_pbest - x) + F * (x_r1 - x)\n    \n    def update_archive(self, x, f_x):\n        if np.any(f_x < self.archive_fitness):\n            worst_index = np.argmax(self.archive_fitness)\n            self.archive[worst_index] = x\n            self.archive_fitness[worst_index] = f_x\n\n    def orthogonal_learning(self, x, func):\n        basis = np.random.randn(self.dim, self.dim)\n        basis, _ = np.linalg.qr(basis)\n        \n        search_range = 0.1 * (func.bounds.ub - func.bounds.lb)\n        coefficients = np.random.uniform(-search_range, search_range, size=self.dim)\n        \n        best_x = x.copy()\n        best_f = func(x)\n        self.budget -= 1\n        \n        for i in range(self.dim):\n            new_x = x + coefficients[i] * basis[:, i]\n            new_x = np.clip(new_x, func.bounds.lb, func.bounds.ub)\n            new_f = func(new_x)\n            self.budget -= 1\n            if new_f < best_f:\n                best_f = new_f\n                best_x = new_x.copy()\n\n        return best_x, best_f\n    \n    def restart_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index].copy()\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Parameter adaptation using success history\n                if self.success_history_F:\n                    self.F = np.random.choice(self.success_history_F)\n                if self.success_history_CR:\n                    self.CR = np.random.choice(self.success_history_CR)\n                    \n                # Mutation\n                pbest_count = max(1, int(self.pbest_rate * self.pop_size))\n                pbest_indices = np.argsort(self.fitness)[:pbest_count]\n                \n                indices = np.random.choice(self.pop_size, 1, replace=False)\n                x_r1 = self.population[indices[0]]\n                \n                v = self.current_to_pbest_mutation(self.population[i], pbest_indices, x_r1, self.F)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = self.population[i].copy()\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        u[j] = v[j]\n\n                # Evaluation\n                f_u = func(u)\n                self.budget -= 1\n\n                # Selection\n                if f_u < self.fitness[i]:\n                    self.success_history_F.append(self.F)\n                    self.success_history_CR.append(self.CR)\n                    \n                    self.fitness[i] = f_u\n                    self.population[i] = u\n                    \n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u.copy()\n                        self.no_improvement_count = 0\n                        \n                    self.update_archive(u, f_u)\n                else:\n                    self.no_improvement_count += 1\n\n                # Limit the size of the success history\n                self.success_history_F = self.success_history_F[-10:]\n                self.success_history_CR = self.success_history_CR[-10:]\n\n            # Orthogonal learning around the best solution\n            best_x, best_f = self.orthogonal_learning(self.x_opt, func)\n            if best_f < self.f_opt:\n                self.f_opt = best_f\n                self.x_opt = best_x.copy()\n                self.no_improvement_count = 0\n\n            if self.no_improvement_count > self.restart_trigger:\n                self.restart_population(func)\n                self.no_improvement_count = 0\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveODE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b1ee6e66-8b2f-4aed-8664-22d03de5a5b5"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "85f33956-998b-4469-9984-134d4f75af54", "fitness": 0.0, "name": "AdaptiveDE", "description": "An adaptive Differential Evolution algorithm with a dynamically adjusted population size and mutation strategy based on performance feedback.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, F=0.5, CR=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.F = F\n        self.CR = CR\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.success_history_F = []\n        self.success_history_CR = []\n        self.archive = []\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        if self.fitness[self.best_index] < self.f_opt:\n            self.f_opt = self.fitness[self.best_index]\n            self.x_opt = self.population[self.best_index].copy()\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            # Mutation strategy adaptation\n            if self.success_history_F:\n                self.F = np.clip(np.random.choice(self.success_history_F), 0.1, 1.0)\n            else:\n                self.F = 0.5\n\n            if self.success_history_CR:\n                self.CR = np.clip(np.random.normal(np.mean(self.success_history_CR), 0.1), 0.0, 1.0)\n            else:\n                self.CR = 0.7\n\n            # Differential Evolution\n            indices = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[indices]\n\n            v = self.population[i] + self.F * (x_r1 - x_r2)\n            v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            j_rand = np.random.randint(self.dim)\n            u = self.population[i].copy()\n            for k in range(self.dim):\n                if np.random.rand() < self.CR or k == j_rand:\n                    u[k] = v[k]\n\n            f_u = func(u)\n            self.budget -= 1\n\n            if f_u < self.fitness[i]:\n                self.archive.append(self.population[i].copy())  # Store replaced individual in archive\n                self.fitness[i] = f_u\n                self.population[i] = u\n                \n                self.success_history_F.append(self.F)\n                self.success_history_CR.append(self.CR)\n                if len(self.success_history_F) > 10:\n                    self.success_history_F.pop(0)\n                    self.success_history_CR.pop(0)\n                \n                if f_u < self.f_opt:\n                    self.f_opt = f_u\n                    self.x_opt = u.copy()\n        \n        # Population size adaptation\n        if len(self.archive) > self.pop_size:\n            self.pop_size = min(self.pop_size + 5, 100)  # Increase population size\n            new_individuals = np.random.choice(len(self.archive), 5, replace=False)\n            new_population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= 5\n            \n            self.population = np.concatenate((self.population, new_population[:5]))\n            self.fitness = np.concatenate((self.fitness, new_fitness[:5]))\n            \n            self.archive = self.archive[-self.pop_size:]  # Keep archive size reasonable\n        elif len(self.archive) < self.pop_size // 2 and self.pop_size > 10:\n            self.pop_size = max(self.pop_size - 5, 10) # Decrease population size if archive is small\n            self.population = self.population[:self.pop_size]\n            self.fitness = self.fitness[:self.pop_size]\n    def __call__(self, func):\n        self.initialize_population(func)\n        while self.budget > 0:\n            self.evolve(func)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b2424024-f5c4-4f1c-bfe6-fd36d287e428"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "2d487d03-1ef5-4dd5-8ace-13ea693aef3c", "fitness": -Infinity, "name": "AdaptivePopulationDE", "description": "A differential evolution algorithm with a dynamically adjusted population size and a diversity maintenance strategy based on the distance to the best solution.", "code": "import numpy as np\n\nclass AdaptivePopulationDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, F=0.5, CR=0.7, diversity_threshold=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.F = F\n        self.CR = CR\n        self.diversity_threshold = diversity_threshold\n        self.population = None\n        self.fitness = None\n        self.best_index = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.age = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.age = np.zeros(self.pop_size)\n        self.budget -= self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index].copy()\n        \n\n    def adjust_population_size(self):\n        \"\"\"Adjust population size based on diversity and stagnation.\"\"\"\n        diversity = np.mean(np.linalg.norm(self.population - self.x_opt, axis=1)) / (np.linalg.norm(self.population[0] - self.population[1]) + 1e-8)\n        \n        if diversity < self.diversity_threshold and self.pop_size < 200:\n            # Increase population size if diversity is low and population is not too large\n            increase_amount = min(10, self.budget // 2)  # Limit increase amount and budget usage\n            if increase_amount > 0:\n                new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(increase_amount, self.dim))\n                new_fitness = np.array([func(x) for x in new_individuals])\n                self.budget -= increase_amount\n\n                self.population = np.vstack((self.population, new_individuals))\n                self.fitness = np.concatenate((self.fitness, new_fitness))\n                self.age = np.concatenate((self.age, np.zeros(increase_amount)))\n                self.pop_size = len(self.population)\n                self.best_index = np.argmin(self.fitness) #update\n                self.f_opt = self.fitness[self.best_index]\n                self.x_opt = self.population[self.best_index].copy()\n        elif np.all(self.age > 50) and self.pop_size > 20:\n            # Reduce population size if the population has stagnated and population is not too small\n            reduction_amount = min(10, self.pop_size // 2) #limit reduction amount\n            indices_to_remove = np.argsort(self.fitness)[-reduction_amount:] # Remove worst individuals\n            self.population = np.delete(self.population, indices_to_remove, axis=0)\n            self.fitness = np.delete(self.fitness, indices_to_remove)\n            self.age = np.delete(self.age, indices_to_remove)\n            self.pop_size = len(self.population)\n            self.best_index = np.argmin(self.fitness) #update\n            self.f_opt = self.fitness[self.best_index]\n            self.x_opt = self.population[self.best_index].copy()\n        \n        self.age +=1  # Increment age for all individuals\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            self.adjust_population_size()\n\n            for i in range(self.pop_size):\n                # Differential Evolution\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n\n                v = self.population[i] + self.F * (x_r1 - x_r2)\n\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = self.population[i].copy()\n                for k in range(self.dim):\n                    if np.random.rand() < self.CR or k == j_rand:\n                        u[k] = v[k]\n\n                f_u = func(u)\n                self.budget -= 1\n\n                # Selection\n                if f_u < self.fitness[i]:\n                    self.fitness[i] = f_u\n                    self.population[i] = u\n                    self.age[i] = 0  # Reset age if improved\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u.copy()\n                        self.best_index = i\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 144, in evaluate\n    algorithm(f_new)\n  File \"<string>\", line 65, in __call__\n  File \"<string>\", line 36, in adjust_population_size\nNameError: name 'func' is not defined\n.", "error": "", "parent_ids": ["b2424024-f5c4-4f1c-bfe6-fd36d287e428"], "operator": null, "metadata": {}}
{"id": "e2bc8851-ca3a-4ff9-ac40-efd39a7ecb1d", "fitness": 0.3874394905665122, "name": "AdaptiveSelfOrganizingDE", "description": "An enhanced self-organizing differential evolution algorithm with adaptive population size and a restart mechanism based on stagnation detection.", "code": "import numpy as np\n\nclass AdaptiveSelfOrganizingDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, min_pop_size=20, max_pop_size=100,\n                 initial_F=0.5, initial_CR=0.7, diversity_threshold=0.1, stagnation_threshold=1000):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.min_pop_size = min_pop_size\n        self.max_pop_size = max_pop_size\n        self.F = initial_F\n        self.CR = initial_CR\n        self.diversity_threshold = diversity_threshold\n        self.stagnation_threshold = stagnation_threshold\n        self.population = None\n        self.fitness = None\n        self.best_index = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.stagnation_counter = 0\n        self.lb = None\n        self.ub = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index].copy()\n        self.lb = func.bounds.lb\n        self.ub = func.bounds.ub\n\n    def calculate_diversity(self):\n        \"\"\"Calculates the average distance of each individual from the population mean.\"\"\"\n        mean_position = np.mean(self.population, axis=0)\n        distances = np.linalg.norm(self.population - mean_position, axis=1)\n        return np.mean(distances)\n\n    def adjust_parameters(self):\n        \"\"\"Adjusts F and CR based on population diversity.\"\"\"\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold:\n            # Population is converging, increase exploration\n            self.F *= 1.1  # Increase F\n            self.CR *= 0.9 # Decrease CR\n        else:\n            # Population is diverse, increase exploitation\n            self.F *= 0.9  # Decrease F\n            self.CR *= 1.1 # Increase CR\n\n        self.F = np.clip(self.F, 0.1, 0.9)\n        self.CR = np.clip(self.CR, 0.1, 0.9)\n\n    def adjust_population_size(self):\n        \"\"\"Dynamically adjusts the population size.\"\"\"\n        if self.stagnation_counter > self.stagnation_threshold:\n            self.pop_size = max(self.min_pop_size, int(self.pop_size * 0.8))  # Reduce population size\n        else:\n            self.pop_size = min(self.max_pop_size, int(self.pop_size * 1.05)) # Increase population size\n\n    def restart_population(self, func):\n        \"\"\"Restarts the population with new random individuals.\"\"\"\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index].copy()\n        self.stagnation_counter = 0  # Reset stagnation counter\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        previous_f_opt = self.f_opt\n\n        while self.budget > 0:\n            self.adjust_parameters()\n            #Stagnation Check\n            if self.f_opt >= previous_f_opt:\n                 self.stagnation_counter += 1\n            else:\n                 self.stagnation_counter = 0\n            previous_f_opt = self.f_opt\n\n            if self.stagnation_counter > self.stagnation_threshold and self.budget > self.pop_size:\n                self.adjust_population_size() # Try to adjust population size before restart\n                self.restart_population(func)\n            \n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                v = self.population[i] + self.F * (x_r2 - x_r3)\n                v = np.clip(v, self.lb, self.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = self.population[i].copy()\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        u[j] = v[j]\n\n                # Evaluation\n                f_u = func(u)\n                self.budget -= 1\n\n                # Selection\n                if f_u < self.fitness[i]:\n                    self.fitness[i] = f_u\n                    self.population[i] = u\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u.copy()\n                        self.best_index = np.argmin(self.fitness)\n            \n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveSelfOrganizingDE scored 0.387 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2e6840e3-dae6-4112-a4d3-728a3ba4ea71"], "operator": null, "metadata": {"aucs": [0.12503242712293994, 0.2870624513515675, 0.38665916143867884, 0.338796303208064, 0.29217862990219445, 0.37962874125028, 0.27378637372035186, 0.3270420509335228, 0.27876495167981197, 0.19866716536794649, 0.5478758062508455, 0.999416428122892, 0.4169731660908216, 0.3007716768349955, 0.7208788539921054, 0.3912637440024196, 0.32354512995622164, 0.45751708901855004, 0.2179913522546385, 0.4849383088313961]}}
{"id": "fdc5e585-55aa-4cf4-966a-ea826560abd6", "fitness": 0.2897901711754945, "name": "AdaptiveDE", "description": "An adaptive Differential Evolution algorithm that adjusts both mutation strategy and population size during the search process, incorporating a local search phase triggered by stagnation detection.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size_init=50, F_init=0.5, CR_init=0.7, F_lr=0.1, CR_lr=0.1, stagnation_threshold=100, local_search_iterations=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size_init\n        self.pop_size_min = 10  # Minimum population size\n        self.pop_size_max = 100 # Maximum population size\n        self.F = F_init\n        self.CR = CR_init\n        self.F_lr = F_lr\n        self.CR_lr = CR_lr\n        self.stagnation_threshold = stagnation_threshold\n        self.local_search_iterations = local_search_iterations\n        self.population = None\n        self.fitness = None\n        self.best_index = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.success_history_F = []\n        self.success_history_CR = []\n        self.stagnation_counter = 0\n        self.last_improvement = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_index]\n        self.x_opt = self.population[self.best_index].copy()\n        self.last_improvement = 0\n        self.stagnation_counter = 0\n\n    def mutation(self, i):\n        indices = np.random.choice(self.pop_size, 3, replace=False)\n        x_r1, x_r2, x_r3 = self.population[indices]\n        return self.population[i] + self.F * (x_r2 - x_r3)\n\n    def crossover(self, v, i):\n        j_rand = np.random.randint(self.dim)\n        u = self.population[i].copy()\n        for j in range(self.dim):\n            if np.random.rand() < self.CR or j == j_rand:\n                u[j] = v[j]\n        return u\n\n    def local_search(self, func):\n        # Perform a simple gradient-based local search around the best solution\n        learning_rate = 0.1\n        x_current = self.x_opt.copy()\n        f_current = self.f_opt\n        for _ in range(self.local_search_iterations):\n            gradient = np.zeros_like(x_current)\n            for j in range(self.dim):\n                x_plus = x_current.copy()\n                x_minus = x_current.copy()\n                delta = 1e-5\n                x_plus[j] += delta\n                x_minus[j] -= delta\n                x_plus = np.clip(x_plus, func.bounds.lb, func.bounds.ub)\n                x_minus = np.clip(x_minus, func.bounds.lb, func.bounds.ub)\n                f_plus = func(x_plus)\n                f_minus = func(x_minus)\n                self.budget -= 2\n                gradient[j] = (f_plus - f_minus) / (2 * delta)\n                if self.budget <= 0:\n                    return\n\n            x_new = x_current - learning_rate * gradient\n            x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n            f_new = func(x_new)\n            self.budget -= 1\n\n            if f_new < f_current:\n                f_current = f_new\n                x_current = x_new\n                self.f_opt = f_new\n                self.x_opt = x_new.copy()\n\n\n    def adjust_population_size(self):\n        # Dynamically adjust population size based on stagnation\n        if self.stagnation_counter > self.stagnation_threshold:\n            self.pop_size = max(self.pop_size // 2, self.pop_size_min)\n            print(f\"Stagnation detected. Reducing population size to {self.pop_size}\")\n            self.stagnation_counter = 0\n            # Reinitialize population\n            #self.initialize_population(func) # Reinitialization should be done outside of this function\n        elif self.stagnation_counter < self.stagnation_threshold / 2 and self.pop_size < self.pop_size_max:\n            self.pop_size = min(self.pop_size * 2, self.pop_size_max)\n            print(f\"Improving. Increasing population size to {self.pop_size}\")\n            #self.initialize_population(func)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            \n            # Stagnation Detection\n            if self.fitness[self.best_index] == self.f_opt:\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n                self.best_index = np.argmin(self.fitness)\n                self.f_opt = self.fitness[self.best_index]\n                self.x_opt = self.population[self.best_index].copy()\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                print(\"Local Search Triggered\")\n                self.local_search(func)\n                self.stagnation_counter = 0\n\n            self.adjust_population_size()\n\n            # Reinitialize population with the new size\n            new_population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n            \n            # Keep best individual from the old population\n            new_population[0] = self.x_opt.copy()\n            new_fitness[0] = self.f_opt\n\n            self.population = new_population\n            self.fitness = new_fitness\n\n            for i in range(self.pop_size):\n                # Parameter adaptation using success history\n                if self.success_history_F:\n                    self.F = np.random.choice(self.success_history_F)\n                if self.success_history_CR:\n                    self.CR = np.random.choice(self.success_history_CR)\n\n                # Mutation\n                v = self.mutation(i)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                u = self.crossover(v, i)\n\n                # Evaluation\n                f_u = func(u)\n                self.budget -= 1\n\n                # Selection\n                if f_u < self.fitness[i]:\n                    self.success_history_F.append(self.F)\n                    self.success_history_CR.append(self.CR)\n\n                    self.fitness[i] = f_u\n                    self.population[i] = u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u.copy()\n                        self.stagnation_counter = 0 #reset stagnation counter\n\n                # Limit the size of the success history\n                self.success_history_F = self.success_history_F[-10:]\n                self.success_history_CR = self.success_history_CR[-10:]\n\n                if self.budget <= 0:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveDE scored 0.290 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b1ee6e66-8b2f-4aed-8664-22d03de5a5b5"], "operator": null, "metadata": {"aucs": [0.13586964484173236, 0.1834415701208031, 0.2675184350742772, 0.2106846690891504, 0.2083201482142848, 0.23681117535478657, 0.23823336927629069, 0.24360005424364695, 0.20648970049273307, 0.17382122302181413, 0.2282055012913029, 0.9936545134221365, 0.2553146870739026, 0.2111777790989149, 0.605109874997422, 0.290057552048836, 0.22611452217490302, 0.2611964578535252, 0.1564934583732165, 0.4636890874462112]}}
