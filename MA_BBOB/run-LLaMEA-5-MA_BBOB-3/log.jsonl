{"id": "fd7af08e-6c06-4cee-af67-428af5c63542", "fitness": -Infinity, "name": "CMAES", "description": "Covariance matrix adaptation evolution strategy (CMA-ES) with restart mechanism and budget-aware adaptation.", "code": "import numpy as np\nfrom scipy.linalg import sqrtm\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damps=1, c_cov=0.1, c_rank_one=None, c_rank_mu=None, mu_factor=0.25):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = int(self.popsize * mu_factor)\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.m = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - 1/(4*self.dim) + 1/(21*self.dim**2))\n\n        self.cs = cs\n        self.damps = damps\n        self.c_cov = c_cov\n\n        self.c_rank_one = c_rank_one if c_rank_one is not None else 2 / ((self.dim + 1.3)**2 + self.mu)\n        self.c_rank_mu = c_rank_mu if c_rank_mu is not None else min(1 - self.c_rank_one, 2 * (self.mu - 1 + 1/self.mu) / ((self.dim + 2)**2 + self.mu))\n        self.eval_count = 0\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.lb = -5\n        self.ub = 5\n\n    def sample_population(self):\n        z = np.random.randn(self.dim, self.popsize)\n        C_sqrt = sqrtm(self.C)\n        x = self.m[:, np.newaxis] + self.sigma * np.dot(C_sqrt, z)\n        x = np.clip(x, self.lb, self.ub)\n        return x, z\n\n    def update_parameters(self, x, z, fitness):\n        x_mu = np.sum(x * self.weights[np.newaxis, :], axis=1)\n        y_mu = x_mu - self.m\n        \n        self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs)) * y_mu / self.sigma\n        \n        hsig = np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * (self.eval_count // self.popsize + 1))) < self.chiN * (1.4 + 2/(self.dim + 1))\n\n        self.pc = (1 - self.c_cov) * self.pc + hsig * np.sqrt(self.c_cov * (2 - self.c_cov)) * y_mu\n        \n        z_mu = np.dot(z, self.weights)\n\n        self.C = (1 - self.c_rank_one - self.c_rank_mu) * self.C + \\\n                   self.c_rank_one * np.outer(self.pc, self.pc) + \\\n                   self.c_rank_mu * np.dot(z[:, :self.mu] * self.weights[np.newaxis, :], z[:, :self.mu].T)\n\n        self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps)/self.chiN - 1))\n        self.m = x_mu\n\n    def __call__(self, func):\n        restart_count = 0\n        while self.eval_count < self.budget:\n            x, z = self.sample_population()\n            fitness = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.eval_count += self.popsize\n            \n            if np.any(fitness < self.f_opt):\n                best_idx = np.argmin(fitness)\n                if fitness[best_idx] < self.f_opt:\n                    self.f_opt = fitness[best_idx]\n                    self.x_opt = x[:, best_idx]\n\n            idx = np.argsort(fitness)\n            x = x[:, idx[:self.mu]]\n            z = z[:, idx[:self.mu]]\n            \n            self.update_parameters(x, z, fitness)\n           \n            if np.linalg.det(self.C) <= 0 or np.any(np.isnan(self.C)):\n                self.C = np.eye(self.dim)\n                self.sigma = 0.5\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.m = np.random.uniform(self.lb, self.ub, size=self.dim)\n\n                restart_count += 1\n\n            if self.eval_count > self.budget:\n                break\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "An exception occurred: name 'sqrtm' is not defined.", "error": "", "parent_ids": [], "operator": null, "metadata": {}}
{"id": "ab236212-044e-4a59-bef6-7d880c15d954", "fitness": -Infinity, "name": "CMAES", "description": "Covariance matrix adaptation evolution strategy (CMA-ES) with restarts, adapting the step size and covariance matrix to efficiently explore the search space.", "code": "import numpy as np\nfrom scipy.linalg import sqrtm\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, mu_percentage=0.5, cs=0.3, damps=1.0, c_cov_rank_one=None, c_cov_mu=None):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = int(self.popsize * mu_percentage)\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n        self.cs = cs\n        self.damps = damps + 2 * max(0, np.sqrt((self.mueff - 1)/(self.dim + 1)) - 1) + self.cs\n        self.c_cov_rank_one = c_cov_rank_one if c_cov_rank_one is not None else 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.c_cov_mu = c_cov_mu if c_cov_mu is not None else min(1 - self.c_cov_rank_one, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2)**2 + self.mueff))\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.n_evals = 0\n        \n        #initialization\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = 0.5\n        C = np.eye(self.dim)\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        \n        restart_count = 0\n        while self.n_evals < self.budget:\n            \n            # Sampling\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            x = mean[:, np.newaxis] + sigma * np.dot(sqrtm(C), z)\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            f = np.array([func(xi) for xi in x.T])\n            self.n_evals += self.popsize\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n            \n            # Selection and Recombination\n            idx = np.argsort(f)\n            x_mu = x[:, idx[:self.mu]]\n            z_mu = z[:, idx[:self.mu]]\n            mean_old = mean.copy()\n            mean = np.dot(x_mu, self.weights)\n            \n            # Step-size adaptation\n            ps = (1 - self.cs) * ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.dot(sqrtm(np.linalg.inv(C)), (mean - mean_old)) / sigma\n            sigma = sigma * np.exp((self.cs/self.damps) * (np.linalg.norm(ps)/np.sqrt(self.dim) - 1))\n            \n            # Covariance matrix adaptation\n            pc = (1 - self.c_cov_rank_one) * pc + np.sqrt(self.c_cov_rank_one * (2 - self.c_cov_rank_one) * self.mueff) * (mean - mean_old) / sigma\n            C = (1 - self.c_cov_rank_one - self.c_cov_mu) * C + self.c_cov_rank_one * np.outer(pc, pc) + self.c_cov_mu * np.dot(z_mu * self.weights, z_mu.T)\n            \n            C = np.triu(C) + np.triu(C, 1).T\n            \n            #handle eigenvalue problem by adding to the diagonal\n            min_eig = np.min(np.real(np.linalg.eigvals(C)))\n            if min_eig < 0:\n                C = C + (abs(min_eig) + 1e-10)*np.eye(self.dim)\n                \n            # Restart if necessary, check budget first\n            if self.n_evals + self.popsize > self.budget:\n                break\n            \n            if sigma < 1e-10: #Stalled\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                sigma = 0.5\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                restart_count += 1\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "An exception occurred: name 'sqrtm' is not defined.", "error": "", "parent_ids": [], "operator": null, "metadata": {}}
{"id": "732959bf-4472-4217-9f4a-3900110ba376", "fitness": 0.4852312593229783, "name": "AdaptiveVelocityParticleSwarm", "description": "Population-based algorithm with velocity updates guided by a combination of global best, local best, and random exploration, with adaptive parameter control.", "code": "import numpy as np\n\nclass AdaptiveVelocityParticleSwarm:\n    def __init__(self, budget=10000, dim=10, pop_size=20, inertia_start=0.9, inertia_end=0.2, c1=2.0, c2=2.0):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.inertia_start = inertia_start\n        self.inertia_end = inertia_end\n        self.c1 = c1\n        self.c2 = c2\n        self.velocities = None\n        self.positions = None\n        self.fitness = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = None\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.eval_count = 0\n\n\n    def initialize_population(self, func):\n        self.positions = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.zeros((self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.positions])\n        self.eval_count += self.pop_size\n        self.personal_best_positions = self.positions.copy()\n        self.personal_best_fitness = self.fitness.copy()\n        self.global_best_position = self.personal_best_positions[np.argmin(self.personal_best_fitness)].copy()\n        self.global_best_fitness = np.min(self.personal_best_fitness)\n\n\n    def update_velocity(self, inertia, position, personal_best_position, global_best_position, func):\n        r1 = np.random.rand(self.dim)\n        r2 = np.random.rand(self.dim)\n        velocity = (inertia * self.velocities[position] +\n                    self.c1 * r1 * (personal_best_position - self.positions[position]) +\n                    self.c2 * r2 * (global_best_position - self.positions[position]))\n\n        return velocity\n    \n\n    def update_position(self, position, velocity, func):\n        new_position = self.positions[position] + velocity\n        \n        # Boundary handling\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n\n        new_position = np.clip(new_position, lb, ub)\n\n        return new_position\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            # Adaptive Inertia Weight\n            inertia = self.inertia_start - (self.inertia_start - self.inertia_end) * (self.eval_count / self.budget)\n\n            for i in range(self.pop_size):\n                velocity = self.update_velocity(inertia, i, self.personal_best_positions[i], self.global_best_position, func)\n                new_position = self.update_position(i, velocity, func)\n\n                f = func(new_position)\n                self.eval_count += 1\n\n                if f < self.personal_best_fitness[i]:\n                    self.personal_best_fitness[i] = f\n                    self.personal_best_positions[i] = new_position.copy()\n\n                    if f < self.global_best_fitness:\n                        self.global_best_fitness = f\n                        self.global_best_position = new_position.copy()\n\n                self.positions[i] = new_position.copy()\n\n                if self.eval_count >= self.budget:\n                    break\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveVelocityParticleSwarm scored 0.485 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.23505948072072702, 0.27843201849769383, 0.587127465506388, 0.1726778882091241, 0.3011471556113483, 0.9629770239521023, 0.3313459071742556, 0.5180375319848162, 0.4699916882734565, 0.1842111005219993, 0.7695954580112414, 0.9982255625667376, 0.2879406891344022, 0.395981364703768, 0.8473371409798194, 0.9669578789338917, 0.3308515378260706, 0.38273459401714394, 0.2334814481618891, 0.45051225167269215]}}
{"id": "a414ffe7-2066-472e-bd66-4b8caa134f4f", "fitness": 0.2961652647169264, "name": "CMAES", "description": "Covariance matrix adaptation evolution strategy with adaptive population size and restart mechanism.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.normC = 0\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2.3)**2 + self.mueff))\n        self.c1a = self.c1\n        self.cmua = self.cmu\n\n        self.lb = -5\n        self.ub = 5\n        self.archive = []\n        self.decay = 0.99  # Decay factor for sigma\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        \n        while self.count_evals < self.budget:\n            \n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z\n            x = np.clip(x, self.lb, self.ub)\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.count_evals += self.popsize\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n\n            idx = np.argsort(f)\n            x = x[:, idx]\n            f = f[idx]\n\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n            \n            ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt.T, (xmean - self.mean) / self.sigma)\n            self.mean = xmean\n            \n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.count_evals / self.popsize)) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            \n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (xmean - self.mean) / self.sigma\n            \n            artmp = (1 / self.sigma) * (x[:, :self.mu] - self.mean[:, np.newaxis])\n            self.C = (1 - self.c1 - self.cmu + self.c1a * self.cmua * (1/(self.sigma**2)) * np.sum(artmp**2)) * self.C + self.c1 * np.outer(self.pc, self.pc) + self.cmu * artmp @ np.diag(self.weights) @ artmp.T\n            \n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(ps) / self.chiN - 1))\n            self.sigma = max(self.sigma, 1e-10)\n\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            \n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-6 * np.eye(self.dim)\n\n            # Restart mechanism\n            if np.max(np.diag(self.C)) > (1e7 * self.sigma):\n                self.sigma *= 0.1\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n\n            # Optional: Adapt population size\n            # self.popsize = int(4 + 3 * np.log(self.dim))\n            # self.mu = self.popsize // 2\n            # self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n            # self.weights = self.weights / np.sum(self.weights)\n            # self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm CMAES scored 0.296 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.13190714494793987, 0.17901766684084286, 0.3178295601034089, 0.19581660299428283, 0.21597496329344923, 0.2794480234639689, 0.23928826833710448, 0.24718947629516042, 0.22678275940877324, 0.1588829282451818, 0.23824024092704277, 0.9948060742380884, 0.2493991426743961, 0.2565709595013286, 0.6057757895634475, 0.26386618701041265, 0.258756251840612, 0.250842009168864, 0.16655792271811865, 0.44635332276610384]}}
{"id": "16b96427-9b2f-49e8-a465-18a3003ff00e", "fitness": -Infinity, "name": "SimplifiedCMAES", "description": "A simplified CMA-ES variant using a rank-one update and a fixed step size schedule to reduce computational complexity and parameter tuning.", "code": "import numpy as np\nfrom scipy.linalg import sqrtm\nimport scipy\n\nclass SimplifiedCMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, mu_percentage=0.5, step_size=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = int(self.popsize * mu_percentage)\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.step_size = step_size  # Fixed step size\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.n_evals = 0\n\n        # Initialization\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        C = np.eye(self.dim)\n        p_c = np.zeros(self.dim)  # Cumulation path\n\n        while self.n_evals < self.budget:\n            # Sampling\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            try:\n                x = mean[:, np.newaxis] + self.step_size * np.dot(scipy.linalg.sqrtm(C), z)\n            except Exception as e:\n                print(f\"Error during sqrtm: {e}\")\n                eigenvalues = np.linalg.eigvals(C)\n                min_eig = np.min(np.real(eigenvalues))\n                if min_eig < 0:\n                    C = C + (abs(min_eig) + 1e-10) * np.eye(self.dim)\n                x = mean[:, np.newaxis] + self.step_size * np.dot(scipy.linalg.sqrtm(C), z)\n\n\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            f = np.array([func(xi) for xi in x.T])\n            self.n_evals += self.popsize\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n\n            # Selection and Recombination\n            idx = np.argsort(f)\n            x_mu = x[:, idx[:self.mu]]\n            z_mu = z[:, idx[:self.mu]]\n            mean_old = mean.copy()\n            mean = np.dot(x_mu, self.weights)\n\n            # Rank-one update for covariance matrix\n            p_c = 0.8 * p_c + np.sqrt(0.2 * (2 - 0.2) * np.sum(self.weights)) * (mean - mean_old) / self.step_size\n            C = (1 - 0.1) * C + 0.1 * np.outer(p_c, p_c)\n\n            # Ensure positive definiteness\n            C = np.triu(C) + np.triu(C, 1).T\n            min_eig = np.min(np.real(np.linalg.eigvals(C)))\n            if min_eig < 0:\n                C = C + (abs(min_eig) + 1e-10) * np.eye(self.dim)\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "An exception occurred: name 'scipy' is not defined.", "error": "", "parent_ids": ["ab236212-044e-4a59-bef6-7d880c15d954"], "operator": null, "metadata": {}}
{"id": "469497dd-2bc3-46b2-b601-ce0fc010d828", "fitness": -Infinity, "name": "CMAES", "description": "CMA-ES with restarts and simplified rank-one update, numerical stability improvements, and bound constraints.", "code": "import numpy as np\nfrom scipy.linalg import sqrtm, LinAlgError\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damps=1, c_cov=0.1, c_rank_one=None, c_rank_mu=None, mu_factor=0.25):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = int(self.popsize * mu_factor)\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.m = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - 1/(4*self.dim) + 1/(21*self.dim**2))\n\n        self.cs = cs\n        self.damps = damps\n        self.c_cov = c_cov\n\n        self.c_rank_one = c_rank_one if c_rank_one is not None else 2 / ((self.dim + 1.3)**2 + self.mu)\n        self.c_rank_mu = c_rank_mu if c_rank_mu is not None else min(1 - self.c_rank_one, 2 * (self.mu - 1 + 1/self.mu) / ((self.dim + 2)**2 + self.mu))\n        self.eval_count = 0\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.lb = -5\n        self.ub = 5\n        self.restart_count = 0\n\n    def sample_population(self):\n        z = np.random.randn(self.dim, self.popsize)\n        try:\n            C_sqrt = sqrtm(self.C)\n        except LinAlgError:\n            self.C = np.eye(self.dim)\n            C_sqrt = np.eye(self.dim) # Fallback to identity matrix\n        x = self.m[:, np.newaxis] + self.sigma * np.dot(C_sqrt, z)\n        x = np.clip(x, self.lb, self.ub)\n        return x, z\n\n    def update_parameters(self, x, z, fitness):\n        x_mu = np.sum(x * self.weights[np.newaxis, :], axis=1)\n        y_mu = x_mu - self.m\n        \n        self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs)) * y_mu / self.sigma\n        \n        hsig = np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * (self.eval_count // self.popsize + 1))) < self.chiN * (1.4 + 2/(self.dim + 1))\n\n        self.pc = (1 - self.c_cov) * self.pc + hsig * np.sqrt(self.c_cov * (2 - self.c_cov)) * y_mu\n        \n        z_mu = np.dot(z, self.weights)\n\n        self.C = (1 - self.c_rank_one - self.c_rank_mu) * self.C + \\\n                   self.c_rank_one * np.outer(self.pc, self.pc) \n\n        self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps)/self.chiN - 1))\n        self.m = x_mu\n\n    def __call__(self, func):\n        while self.eval_count < self.budget:\n            x, z = self.sample_population()\n            fitness = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.eval_count += self.popsize\n            \n            if np.any(fitness < self.f_opt):\n                best_idx = np.argmin(fitness)\n                if fitness[best_idx] < self.f_opt:\n                    self.f_opt = fitness[best_idx]\n                    self.x_opt = x[:, best_idx]\n\n            idx = np.argsort(fitness)\n            x = x[:, idx[:self.mu]]\n            z = z[:, idx[:self.mu]]\n            \n            self.update_parameters(x, z, fitness)\n           \n            if np.linalg.det(self.C) <= 0 or np.any(np.isnan(self.C)):\n                self.C = np.eye(self.dim)\n                self.sigma = 0.5\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.m = np.random.uniform(self.lb, self.ub, size=self.dim)\n                self.restart_count += 1\n                \n            if self.eval_count > self.budget:\n                break\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "An exception occurred: name 'LinAlgError' is not defined.", "error": "", "parent_ids": ["fd7af08e-6c06-4cee-af67-428af5c63542"], "operator": null, "metadata": {}}
{"id": "b5507e32-cc60-4118-82d4-56ba80437f14", "fitness": -Infinity, "name": "AdaptiveNeighborhoodDEPSO", "description": "A population-based algorithm that combines aspects of differential evolution and particle swarm optimization with a self-adaptive neighborhood radius for each particle.", "code": "import numpy as np\n\nclass AdaptiveNeighborhoodDEPSO:\n    def __init__(self, budget=10000, dim=10, popsize=40, neighborhood_size_factor=0.1, inertia_weight=0.7, c1=1.5, c2=1.5, f=0.8, cr=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize\n        self.neighborhood_size_factor = neighborhood_size_factor  # Fraction of population size for neighborhood\n        self.inertia_weight = inertia_weight\n        self.c1 = c1\n        self.c2 = c2\n        self.f = f\n        self.cr = cr\n        self.population = np.random.uniform(low=-5.0, high=5.0, size=(self.popsize, self.dim))\n        self.velocity = np.zeros((self.popsize, self.dim))\n        self.fitness = np.array([np.inf] * self.popsize)\n        self.best_positions = self.population.copy()\n        self.best_fitness = np.array([np.inf] * self.popsize)\n        self.global_best_index = 0\n        self.eval_count = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def evaluate_population(self, func):\n        for i in range(self.popsize):\n            if self.eval_count < self.budget:\n                f = func(self.population[i])\n                self.eval_count += 1\n                self.fitness[i] = f\n                if f < self.best_fitness[i]:\n                    self.best_fitness[i] = f\n                    self.best_positions[i] = self.population[i].copy()\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = self.population[i].copy()\n    \n    def update_neighborhoods(self):\n        self.neighborhoods = []\n        for i in range(self.popsize):\n            distances = np.linalg.norm(self.population - self.population[i], axis=1)\n            neighborhood_size = max(3, int(self.neighborhood_size_factor * self.popsize)) #Ensure neighborhood has at least 3 members\n            neighbors = np.argsort(distances)[:neighborhood_size]\n            self.neighborhoods.append(neighbors)\n\n    def differential_evolution_mutation(self, i):\n        neighborhood = self.neighborhoods[i]\n        \n        # Ensure we have enough unique individuals in the neighborhood\n        if len(neighborhood) < 3:\n            candidates = np.arange(self.popsize)\n            candidates = candidates[candidates != i]\n            replace = 3 - len(neighborhood)\n            \n            supplemental = np.random.choice(candidates, replace=replace, replace=False)\n            neighborhood = np.concatenate((neighborhood, supplemental))\n            \n        idxs = np.random.choice(neighborhood, 3, replace=False)\n        x_r1, x_r2, x_r3 = self.population[idxs[0]], self.population[idxs[1]], self.population[idxs[2]]\n        return x_r1 + self.f * (x_r2 - x_r3)\n\n    def particle_swarm_update(self, i, mutated_vector):\n          # Use neighborhood best for PSO\n          neighborhood = self.neighborhoods[i]\n          neighborhood_best_index = neighborhood[np.argmin(self.best_fitness[neighborhood])]\n\n          self.velocity[i] = (self.inertia_weight * self.velocity[i] +\n                               self.c1 * np.random.rand(self.dim) * (self.best_positions[i] - self.population[i]) +\n                               self.c2 * np.random.rand(self.dim) * (self.best_positions[neighborhood_best_index] - self.population[i]))\n\n          return np.clip(self.population[i] + self.velocity[i], -5, 5)\n\n    def crossover(self, i, mutated_vector):\n        cross_points = np.random.rand(self.dim) < self.cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial_vector = np.where(cross_points, mutated_vector, self.population[i])\n        return trial_vector\n\n    def __call__(self, func):\n        self.evaluate_population(func)\n        \n        while self.eval_count < self.budget:\n            self.update_neighborhoods()\n            new_population = np.zeros_like(self.population)\n            \n            for i in range(self.popsize):\n                mutated_vector = self.differential_evolution_mutation(i)\n                trial_vector = self.crossover(i, mutated_vector)\n                new_vector = self.particle_swarm_update(i, trial_vector)\n                \n                new_population[i] = new_vector\n                \n            self.population = new_population\n            self.evaluate_population(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "An exception occurred: keyword argument repeated: replace (<string>, line 54).", "error": "", "parent_ids": ["fd7af08e-6c06-4cee-af67-428af5c63542"], "operator": null, "metadata": {}}
{"id": "30f7add8-0f23-479d-9acf-a501b4d25966", "fitness": -Infinity, "name": "CMAES", "description": "CMA-ES with restarts, adapting the step size and covariance matrix to efficiently explore the search space, using numpy's cholesky decomposition for matrix square root.", "code": "import numpy as np\nfrom numpy.linalg import cholesky\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, mu_percentage=0.5, cs=0.3, damps=1.0, c_cov_rank_one=None, c_cov_mu=None):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = int(self.popsize * mu_percentage)\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n        self.cs = cs\n        self.damps = damps + 2 * max(0, np.sqrt((self.mueff - 1)/(self.dim + 1)) - 1) + self.cs\n        self.c_cov_rank_one = c_cov_rank_one if c_cov_rank_one is not None else 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.c_cov_mu = c_cov_mu if c_cov_mu is not None else min(1 - self.c_cov_rank_one, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2)**2 + self.mueff))\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.n_evals = 0\n        \n        #initialization\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = 0.5\n        C = np.eye(self.dim)\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        \n        restart_count = 0\n        while self.n_evals < self.budget:\n            \n            # Sampling\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            try:\n                x = mean[:, np.newaxis] + sigma * np.dot(cholesky(C), z)\n            except np.linalg.LinAlgError:\n                #if matrix is not positive definite\n                C = C + 1e-6 * np.eye(self.dim)\n                x = mean[:, np.newaxis] + sigma * np.dot(cholesky(C), z)\n\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            f = np.array([func(xi) for xi in x.T])\n            self.n_evals += self.popsize\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n            \n            # Selection and Recombination\n            idx = np.argsort(f)\n            x_mu = x[:, idx[:self.mu]]\n            z_mu = z[:, idx[:self.mu]]\n            mean_old = mean.copy()\n            mean = np.dot(x_mu, self.weights)\n            \n            # Step-size adaptation\n            try:\n                ps = (1 - self.cs) * ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.dot(np.linalg.inv(cholesky(C)).T, (mean - mean_old)) / sigma\n            except np.linalg.LinAlgError:\n                C = C + 1e-6 * np.eye(self.dim)\n                ps = (1 - self.cs) * ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.dot(np.linalg.inv(cholesky(C)).T, (mean - mean_old)) / sigma\n            sigma = sigma * np.exp((self.cs/self.damps) * (np.linalg.norm(ps)/np.sqrt(self.dim) - 1))\n            \n            # Covariance matrix adaptation\n            pc = (1 - self.c_cov_rank_one) * pc + np.sqrt(self.c_cov_rank_one * (2 - self.c_cov_rank_one) * self.mueff) * (mean - mean_old) / sigma\n            C = (1 - self.c_cov_rank_one - self.c_cov_mu) * C + self.c_cov_rank_one * np.outer(pc, pc) + self.c_cov_mu * np.dot(z_mu * self.weights, z_mu.T)\n            \n            C = np.triu(C) + np.triu(C, 1).T\n            \n            #handle eigenvalue problem by adding to the diagonal\n            min_eig = np.min(np.real(np.linalg.eigvals(C)))\n            if min_eig < 0:\n                C = C + (abs(min_eig) + 1e-10)*np.eye(self.dim)\n                \n            # Restart if necessary, check budget first\n            if self.n_evals + self.popsize > self.budget:\n                break\n            \n            if sigma < 1e-10: #Stalled\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                sigma = 0.5\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                restart_count += 1\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "An exception occurred: name 'cholesky' is not defined.", "error": "", "parent_ids": ["ab236212-044e-4a59-bef6-7d880c15d954"], "operator": null, "metadata": {}}
{"id": "bc262b74-102c-4548-b1bc-34f7d674b521", "fitness": -Infinity, "name": "AdaptiveVelocityParticleSwarm", "description": "Adaptive PSO with a dynamically adjusted population size based on stagnation detection and a mutation operator to escape local optima.", "code": "import numpy as np\n\nclass AdaptiveVelocityParticleSwarm:\n    def __init__(self, budget=10000, dim=10, pop_size_start=20, pop_size_min=10, pop_size_max=50, inertia_start=0.9, inertia_end=0.2, c1=2.0, c2=2.0, stagnation_threshold=1000):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_start = pop_size_start\n        self.pop_size_min = pop_size_min\n        self.pop_size_max = pop_size_max\n        self.pop_size = pop_size_start\n        self.inertia_start = inertia_start\n        self.inertia_end = inertia_end\n        self.c1 = c1\n        self.c2 = c2\n        self.velocities = None\n        self.positions = None\n        self.fitness = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = None\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.eval_count = 0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = stagnation_threshold\n        self.previous_global_best = np.inf\n\n    def initialize_population(self, func):\n        self.pop_size = self.pop_size_start\n        self.positions = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.zeros((self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.positions])\n        self.eval_count += self.pop_size\n        self.personal_best_positions = self.positions.copy()\n        self.personal_best_fitness = self.fitness.copy()\n        self.global_best_position = self.personal_best_positions[np.argmin(self.personal_best_fitness)].copy()\n        self.global_best_fitness = np.min(self.personal_best_fitness)\n        self.previous_global_best = self.global_best_fitness\n\n    def update_velocity(self, inertia, position, personal_best_position, global_best_position, func):\n        r1 = np.random.rand(self.dim)\n        r2 = np.random.rand(self.dim)\n        velocity = (inertia * self.velocities[position] +\n                    self.c1 * r1 * (personal_best_position - self.positions[position]) +\n                    self.c2 * r2 * (global_best_position - self.positions[position]))\n\n        return velocity\n    \n\n    def update_position(self, position, velocity, func):\n        new_position = self.positions[position] + velocity\n        \n        # Boundary handling\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n\n        new_position = np.clip(new_position, lb, ub)\n\n        return new_position\n\n    def mutate_particle(self, position, func, mutation_rate=0.1):\n        for i in range(self.dim):\n            if np.random.rand() < mutation_rate:\n                self.positions[position, i] = np.random.uniform(func.bounds.lb, func.bounds.ub)\n        return self.positions[position]\n\n    def adjust_population_size(self):\n        if self.stagnation_counter > self.stagnation_threshold:\n            # Reduce population size if stagnating\n            self.pop_size = max(self.pop_size - 5, self.pop_size_min)\n            # Reset stagnation counter\n            self.stagnation_counter = 0\n            # Re-initialize population\n            # print(\"Reducing pop size to\", self.pop_size)\n            return True\n        elif self.eval_count < self.budget / 2 and self.pop_size < self.pop_size_max:\n             self.pop_size = min(self.pop_size + 1, self.pop_size_max)\n\n        return False\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            # Adaptive Inertia Weight\n            inertia = self.inertia_start - (self.inertia_start - self.inertia_end) * (self.eval_count / self.budget)\n\n            for i in range(self.pop_size):\n                velocity = self.update_velocity(inertia, i, self.personal_best_positions[i], self.global_best_position, func)\n                new_position = self.update_position(i, velocity, func)\n\n                f = func(new_position)\n                self.eval_count += 1\n\n                if f < self.personal_best_fitness[i]:\n                    self.personal_best_fitness[i] = f\n                    self.personal_best_positions[i] = new_position.copy()\n\n                    if f < self.global_best_fitness:\n                        self.global_best_fitness = f\n                        self.global_best_position = new_position.copy()\n                        self.stagnation_counter = 0  # Reset stagnation counter when finding a better global best\n                else:\n                     self.stagnation_counter +=1\n\n                self.positions[i] = new_position.copy()\n                \n                if self.adjust_population_size():\n                   self.initialize_population(func)  #Reinitialize if population is adjusted\n                   break  #Break inner loop and start with new population\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Stagnation check and mutation\n            if self.global_best_fitness >= self.previous_global_best:\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n\n            if self.stagnation_counter > self.stagnation_threshold/2:\n                # Mutate a random particle if stagnating\n                particle_to_mutate = np.random.randint(0, self.pop_size)\n                self.positions[particle_to_mutate] = self.mutate_particle(particle_to_mutate, func)\n                f = func(self.positions[particle_to_mutate])\n                self.eval_count += 1\n                if f < self.personal_best_fitness[particle_to_mutate]:\n                    self.personal_best_fitness[particle_to_mutate] = f\n                    self.personal_best_positions[particle_to_mutate] = self.positions[particle_to_mutate].copy()\n                    if f < self.global_best_fitness:\n                        self.global_best_fitness = f\n                        self.global_best_position = self.positions[particle_to_mutate].copy()\n                self.stagnation_counter = 0\n\n            self.previous_global_best = self.global_best_fitness\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 1, "feedback": "An exception occurred: index 20 is out of bounds for axis 0 with size 20.", "error": "", "parent_ids": ["732959bf-4472-4217-9f4a-3900110ba376"], "operator": null, "metadata": {}}
{"id": "1693d74a-6335-45e6-ab40-e061737244e2", "fitness": -Infinity, "name": "SimplifiedCMAES", "description": "A simplified CMA-ES variant that avoids matrix square root computations by directly updating a Cholesky decomposition of the covariance matrix, enhancing computational efficiency.", "code": "import numpy as np\nfrom scipy.linalg import cholesky, solve_triangular\n\nclass SimplifiedCMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, mu_percentage=0.5, cs=0.3, damps=1.0, c_cov_rank_one=None, c_cov_mu=None):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = int(self.popsize * mu_percentage)\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n        self.cs = cs\n        self.damps = damps + 2 * max(0, np.sqrt((self.mueff - 1)/(self.dim + 1)) - 1) + self.cs\n        self.c_cov_rank_one = c_cov_rank_one if c_cov_rank_one is not None else 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.c_cov_mu = c_cov_mu if c_cov_mu is not None else min(1 - self.c_cov_rank_one, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2)**2 + self.mueff))\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.n_evals = 0\n        \n        #initialization\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = 0.5\n        C = np.eye(self.dim)\n        A = cholesky(C, lower=True)  # Cholesky decomposition\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        \n        restart_count = 0\n        while self.n_evals < self.budget:\n            \n            # Sampling\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            x = mean[:, np.newaxis] + sigma * np.dot(A, z)\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            f = np.array([func(xi) for xi in x.T])\n            self.n_evals += self.popsize\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n            \n            # Selection and Recombination\n            idx = np.argsort(f)\n            x_mu = x[:, idx[:self.mu]]\n            z_mu = z[:, idx[:self.mu]]\n            mean_old = mean.copy()\n            mean = np.dot(x_mu, self.weights)\n            \n            # Step-size adaptation\n            ps = (1 - self.cs) * ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * solve_triangular(A.T, (mean - mean_old), lower=False) / sigma\n            sigma = sigma * np.exp((self.cs/self.damps) * (np.linalg.norm(ps)/np.sqrt(self.dim) - 1))\n            \n            # Covariance matrix adaptation\n            pc = (1 - self.c_cov_rank_one) * pc + np.sqrt(self.c_cov_rank_one * (2 - self.c_cov_rank_one) * self.mueff) * (mean - mean_old) / sigma\n            \n            # Rank-one update\n            delta = pc[:, np.newaxis] * np.sqrt(self.c_cov_rank_one)\n            A = self.rank_one_cholesky_update(A, delta)\n\n            # Rank-mu update\n            z_weighted = z_mu * np.sqrt(self.weights)\n            A = self.rank_mu_cholesky_update(A, z_weighted * np.sqrt(self.c_cov_mu))\n           \n            C = np.dot(A, A.T) # For min_eig calculation\n\n            #handle eigenvalue problem by adding to the diagonal\n            min_eig = np.min(np.real(np.linalg.eigvals(C)))\n            if min_eig < 0:\n                C = C + (abs(min_eig) + 1e-10)*np.eye(self.dim)\n                A = cholesky(C, lower=True)\n                \n            # Restart if necessary, check budget first\n            if self.n_evals + self.popsize > self.budget:\n                break\n            \n            if sigma < 1e-10: #Stalled\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                sigma = 0.5\n                C = np.eye(self.dim)\n                A = cholesky(C, lower=True)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                restart_count += 1\n        \n        return self.f_opt, self.x_opt\n\n    def rank_one_cholesky_update(self, L, v):\n        \"\"\"\n        Rank 1 update of a Cholesky decomposition.\n        \"\"\"\n        v = v.flatten()\n        for i in range(L.shape[0]):\n            L_row_norm = np.sqrt(np.sum(L[i, :i+1]**2))\n            v_row_norm = np.sqrt(np.sum(v[:i+1]**2))\n\n            a = np.sqrt(L_row_norm**2 + v_row_norm**2)\n            L[i, :i+1] = L[i, :i+1] + ( (a - L_row_norm) / v_row_norm**2 ) * np.dot(v[:i+1], L[:i+1,:i+1].T)\n            v[i+1:] = v[i+1:] - v[:i+1] @ L[:i+1, i, None] # Corrected this line\n\n            L[i, i] = a\n\n        return L\n\n    def rank_mu_cholesky_update(self, L, V):\n      \"\"\"\n      Rank mu update of a Cholesky decomposition.\n      \"\"\"\n      for k in range(V.shape[1]):\n          v = V[:,k]\n          L = self.rank_one_cholesky_update(L, v)\n      return L", "configspace": "", "generation": 1, "feedback": "An exception occurred: name 'cholesky' is not defined.", "error": "", "parent_ids": ["ab236212-044e-4a59-bef6-7d880c15d954"], "operator": null, "metadata": {}}
{"id": "dd36cd2e-68fb-4111-a638-ea5532f5f9a1", "fitness": 0.39335724619031487, "name": "CMAES", "description": "Adaptive CMA-ES with a simplified covariance matrix update and active population size adjustment based on performance.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.normC = 0\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2.3)**2 + self.mueff))\n        self.c1a = self.c1\n        self.cmua = self.cmu\n\n        self.lb = -5\n        self.ub = 5\n        self.archive = []\n        self.decay = 0.99  # Decay factor for sigma\n        self.adapt_popsize = True\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        \n        while self.count_evals < self.budget:\n            \n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z\n            x = np.clip(x, self.lb, self.ub)\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.count_evals += self.popsize\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n\n            idx = np.argsort(f)\n            x = x[:, idx]\n            f = f[idx]\n\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n            \n            ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt.T, (xmean - self.mean) / self.sigma)\n            self.mean = xmean\n            \n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.count_evals / self.popsize)) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            \n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (xmean - self.mean) / self.sigma\n            \n            artmp = (1 / self.sigma) * (x[:, :self.mu] - self.mean[:, np.newaxis])\n            # Simplified covariance update\n            self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * np.outer(self.pc, self.pc) + self.cmu * artmp @ np.diag(self.weights) @ artmp.T\n            \n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(ps) / self.chiN - 1))\n            self.sigma = max(self.sigma, 1e-10)\n\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            \n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-6 * np.eye(self.dim)\n\n            # Restart mechanism\n            if np.max(np.diag(self.C)) > (1e7 * self.sigma):\n                self.sigma *= 0.1\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n\n            # Adapt population size based on stagnation\n            if self.adapt_popsize:\n                if self.count_evals > self.budget // 2 and self.sigma < 1e-3:\n                     self.popsize = max(4, self.popsize // 2)\n                     self.mu = self.popsize // 2\n                     self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n                     self.weights = self.weights / np.sum(self.weights)\n                     self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n                     self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n                     self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n                elif self.count_evals < self.budget // 2 and self.sigma > 1:\n                     self.popsize = min(self.dim * 10, self.popsize * 2)\n                     self.mu = self.popsize // 2\n                     self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n                     self.weights = self.weights / np.sum(self.weights)\n                     self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n                     self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n                     self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n                     \n\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm CMAES scored 0.393 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a414ffe7-2066-472e-bd66-4b8caa134f4f"], "operator": null, "metadata": {"aucs": [0.10251335697234665, 0.18115238453131077, 0.3932623006754037, 0.17799045393851798, 0.2250436477650979, 0.9685192098678794, 0.29277040464777093, 0.6429465268795924, 0.9658104860809289, 0.16511260757295565, 0.25510195803840463, 0.21986143645548994, 0.25236702697954494, 0.18804956447152965, 0.6864163661627383, 0.27951988582217546, 0.24464537802397734, 0.9765874818978144, 0.17815814137860675, 0.4713163056442107]}}
{"id": "bde5923e-302c-4259-baf2-99a2926e9a7a", "fitness": 0.33049379506172805, "name": "CMAES", "description": "CMA-ES with simplified covariance matrix update and adaptive step size control, along with restarts for robustness.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damps=1, c_cov=0.1, mu_factor=0.25):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = int(self.popsize * mu_factor)\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.m = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - 1/(4*self.dim) + 1/(21*self.dim**2))\n\n        self.cs = cs\n        self.damps = damps\n        self.c_cov = c_cov\n        self.c_mu = min(1 - self.c_cov, 2 * (self.mu - 1 + 1/self.mu) / ((self.dim + 2)**2 + self.mu))\n\n        self.eval_count = 0\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.lb = -5\n        self.ub = 5\n\n    def sample_population(self):\n        z = np.random.randn(self.dim, self.popsize)\n        C_sqrt = np.linalg.cholesky(self.C)\n        x = self.m[:, np.newaxis] + self.sigma * np.dot(C_sqrt, z)\n        x = np.clip(x, self.lb, self.ub)\n        return x, z\n\n    def update_parameters(self, x, z, fitness):\n        x_mu = np.sum(x * self.weights[np.newaxis, :], axis=1)\n        y_mu = x_mu - self.m\n        \n        self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs)) * y_mu / self.sigma\n        hsig = np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * (self.eval_count // self.popsize + 1))) < self.chiN * (1.4 + 2/(self.dim + 1))\n        self.pc = (1 - self.c_cov) * self.pc + hsig * np.sqrt(self.c_cov * (2 - self.c_cov)) * y_mu\n\n        z_mu = np.dot(z[:, :self.mu] * self.weights[np.newaxis, :], np.eye(self.mu))\n\n        self.C = (1 - self.c_cov - self.c_mu) * self.C + self.c_cov * np.outer(self.pc, self.pc) + self.c_mu * np.dot(z_mu, z_mu.T)\n        self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps)/self.chiN - 1))\n        self.m = x_mu\n\n    def __call__(self, func):\n        while self.eval_count < self.budget:\n            x, z = self.sample_population()\n            fitness = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.eval_count += self.popsize\n            \n            if np.any(fitness < self.f_opt):\n                best_idx = np.argmin(fitness)\n                if fitness[best_idx] < self.f_opt:\n                    self.f_opt = fitness[best_idx]\n                    self.x_opt = x[:, best_idx]\n\n            idx = np.argsort(fitness)\n            x = x[:, idx[:self.mu]]\n            z = z[:, idx[:self.mu]]\n            \n            self.update_parameters(x, z, fitness)\n           \n            if np.linalg.det(self.C) <= 0 or np.any(np.isnan(self.C)):\n                self.C = np.eye(self.dim)\n                self.sigma = 0.5\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.m = np.random.uniform(self.lb, self.ub, size=self.dim)\n\n            if self.eval_count > self.budget:\n                break\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm CMAES scored 0.330 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["fd7af08e-6c06-4cee-af67-428af5c63542"], "operator": null, "metadata": {"aucs": [0.08155983367532804, 0.19610916001204337, 0.42083077914121136, 0.21889174921548582, 0.20302706690082672, 0.9246044317134865, 0.2626436483953003, 0.18960000409225575, 0.2490180361138914, 0.15436753953722693, 0.9818445831414789, 0.20900845874479324, 0.2534693483820004, 0.18538125238625114, 0.6987913995673622, 0.3395225189287068, 0.22260087540901563, 0.17009423305181626, 0.1626638260347214, 0.48584715679135704]}}
{"id": "b8aac1d0-12a8-43a1-b389-681cf71000a0", "fitness": 0.7337132321741314, "name": "CMAES", "description": "Adaptive Covariance Matrix Adaptation Evolution Strategy with simplified update rules and sigma decay.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.normC = 0\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2.3)**2 + self.mueff))\n        self.lb = -5\n        self.ub = 5\n        self.decay = 0.999  # Decay factor for sigma\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        \n        while self.count_evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z\n            x = np.clip(x, self.lb, self.ub)\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.count_evals += self.popsize\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n\n            idx = np.argsort(f)\n            x = x[:, idx]\n\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n            \n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt.T, (xmean - self.mean) / self.sigma)\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.count_evals / self.popsize))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (xmean - self.mean) / self.sigma\n            \n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n            self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * np.outer(self.pc, self.pc) + self.cmu * artmp @ np.diag(self.weights) @ artmp.T\n            \n            self.mean = xmean\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            self.sigma = max(self.sigma, 1e-10)\n            self.sigma *= self.decay  # Decay sigma\n\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-6 * np.eye(self.dim)\n\n            if np.max(np.diag(self.C)) > (1e7 * self.sigma):\n                self.sigma *= 0.1\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm CMAES scored 0.734 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a414ffe7-2066-472e-bd66-4b8caa134f4f"], "operator": null, "metadata": {"aucs": [0.36033882125557637, 0.4355250621992325, 0.9423299890285461, 0.9729887937772376, 0.9377538344254944, 0.9558268610438635, 0.38262791352372283, 0.9393788852100833, 0.9430914058048488, 0.22048806108422037, 0.9586339159224019, 0.994513504746941, 0.27582787003959475, 0.9154155283519789, 0.9132757006197647, 0.8738881743858942, 0.918708123694299, 0.9673986314783071, 0.2528496647718155, 0.5134039021188036]}}
{"id": "9d9d976c-2601-4a25-a7f7-553a92161f50", "fitness": 0.6603311956493246, "name": "AdaptiveDifferentialEvolution", "description": "An adaptive differential evolution strategy that dynamically adjusts its parameters based on the success rate of generating improved solutions.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, popsize=None, F=0.5, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 10 * self.dim\n        self.F = F  # Mutation factor\n        self.CR = CR # Crossover rate\n        self.population = np.random.uniform(-5, 5, size=(self.popsize, self.dim))\n        self.fitness = np.array([np.inf] * self.popsize)\n        self.eval_count = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.F_history = []\n        self.CR_history = []\n\n    def __call__(self, func):\n        while self.eval_count < self.budget:\n            for i in range(self.popsize):\n                # Mutation\n                idxs = np.random.choice(self.popsize, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[idxs]\n                x_mutated = x_r1 + self.F * (x_r2 - x_r3)\n                x_mutated = np.clip(x_mutated, -5, 5)\n\n                # Crossover\n                x_trial = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Selection\n                f_trial = func(x_trial)\n                self.eval_count += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = x_trial\n\n                    # Adaptive parameter update based on success\n                    self.F_history.append(self.F)\n                    self.CR_history.append(self.CR)\n\n\n                    #Dynamically tune F and CR\n                    self.F = np.clip(np.random.normal(0.5, 0.3), 0.1, 0.9)\n                    self.CR = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0)\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = x_trial\n                    \n\n                if self.eval_count >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.660 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["fd7af08e-6c06-4cee-af67-428af5c63542"], "operator": null, "metadata": {"aucs": [0.2943192840794807, 0.5258706166801901, 0.6273267143259676, 0.8498896145292328, 0.7343392069640603, 0.7699301821957809, 0.5249370820911268, 0.6128626957956936, 0.7295320087233345, 0.669979546477965, 0.8348170565484165, 0.9984072368198178, 0.5474395989648555, 0.6867707491566644, 0.8945918667745242, 0.7679720329710584, 0.5323767088600472, 0.7985342921747296, 0.2952930685089664, 0.5114343503445816]}}
{"id": "83556ad2-9d95-4c5c-89cb-2729194366e3", "fitness": 0.0, "name": "AdaptiveSOMA", "description": "A self-organizing migrating algorithm (SOMA) variant with adaptive step size and migration distance based on fitness landscape exploration.", "code": "import numpy as np\n\nclass AdaptiveSOMA:\n    def __init__(self, budget=10000, dim=10, pop_size=20, path_length=2.0, step_size=0.01, perturbation_chance=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.path_length = path_length\n        self.step_size = step_size\n        self.perturbation_chance = perturbation_chance\n        self.positions = None\n        self.fitness = None\n        self.eval_count = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def initialize_population(self, func):\n        self.positions = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.positions])\n        self.eval_count += self.pop_size\n        \n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.positions[best_index].copy()\n\n    def migrate(self, func):\n        for i in range(self.pop_size):\n            leader_index = np.argmin(self.fitness)  # Find the leader\n            \n            for j in range(self.pop_size):\n                if i == j:\n                    continue\n\n                new_position = self.positions[i].copy()\n                for k in range(self.dim):\n                    PRT = 0\n                    if np.random.rand() < self.perturbation_chance:\n                        PRT = np.random.choice([-1, 1]) # either move towards or away from the leader.\n                    \n                    new_position[k] = self.positions[i][k] + (self.positions[leader_index][k] - self.positions[i][k]) * np.random.uniform(0, self.path_length) * PRT\n                \n                #Boundary Handling\n                lb = func.bounds.lb\n                ub = func.bounds.ub\n                new_position = np.clip(new_position, lb, ub)\n                \n                f = func(new_position)\n                self.eval_count += 1\n                \n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.positions[i] = new_position.copy()\n                    \n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = new_position.copy()\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        \n        while self.eval_count < self.budget:\n            self.migrate(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveSOMA scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["732959bf-4472-4217-9f4a-3900110ba376"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "ccedbdf3-c17c-4add-8786-5f8bd21553f4", "fitness": 0.5098290543106541, "name": "MirroredCMAES", "description": "An adaptive CMA-ES variant that utilizes a mirrored sampling strategy to enhance exploration and exploitation based on success rate, along with a simplified covariance update.", "code": "import numpy as np\n\nclass MirroredCMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.normC = 0\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2.3)**2 + self.mueff))\n\n        self.lb = -5\n        self.ub = 5\n        self.archive = []\n        self.success_rate = 0.5\n        self.success_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        \n        while self.count_evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z\n            x_mirrored = self.mean[:, np.newaxis] - self.sigma * C_sqrt @ z\n\n            x = np.clip(x, self.lb, self.ub)\n            x_mirrored = np.clip(x_mirrored, self.lb, self.ub)\n\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            f_mirrored = np.array([func(x_mirrored[:, i]) for i in range(self.popsize)])\n\n            self.count_evals += 2 * self.popsize\n\n            f_combined = np.concatenate((f, f_mirrored))\n            x_combined = np.concatenate((x, x_mirrored), axis=1)\n\n            if np.min(f_combined) < self.f_opt:\n                self.f_opt = np.min(f_combined)\n                self.x_opt = x_combined[:, np.argmin(f_combined)]\n\n            idx = np.argsort(f_combined)\n            x_selected = x_combined[:, idx[:self.mu]]\n            f_selected = f_combined[idx[:self.mu]]\n            \n            xmean = np.sum(x_selected * self.weights[np.newaxis, :], axis=1)\n\n            ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt.T, (xmean - self.mean) / self.sigma)\n            self.mean = xmean\n\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.count_evals / (2*self.popsize))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (xmean - self.mean) / self.sigma\n\n            artmp = (1 / self.sigma) * (x_selected - self.mean[:, np.newaxis])\n            self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * np.outer(self.pc, self.pc) + self.cmu * artmp @ np.diag(self.weights) @ artmp.T\n            \n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(ps) / self.chiN - 1))\n            self.sigma = max(self.sigma, 1e-10)\n\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-6 * np.eye(self.dim)\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm MirroredCMAES scored 0.510 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a414ffe7-2066-472e-bd66-4b8caa134f4f"], "operator": null, "metadata": {"aucs": [0.20813092008826484, 0.19581148385297775, 0.9403247308840901, 0.19569981245971924, 0.3696135382768607, 0.9403614504382324, 0.3106752894174274, 0.9286353765541283, 0.9448891804233505, 0.1816501532388184, 0.9754819975735087, 0.9855892141543211, 0.2636957139510727, 0.4306046819969557, 0.8046127374322479, 0.32146817763747804, 0.38318918719483785, 0.16908956437752531, 0.18778347759076408, 0.45927439867049913]}}
{"id": "7e16bb20-d63f-44b6-a56f-4d85edf3923d", "fitness": -Infinity, "name": "SelfAdjustingDE", "description": "A self-adjusting Differential Evolution algorithm that uses a population-weighted mutation strategy and adapts the crossover rate based on the diversity of the population.", "code": "import numpy as np\n\nclass SelfAdjustingDE:\n    def __init__(self, budget=10000, dim=10, popsize=None, F=0.5, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 10 * self.dim\n        self.F = F  # Mutation factor\n        self.CR = CR # Crossover rate\n        self.population = np.random.uniform(-5, 5, size=(self.popsize, self.dim))\n        self.fitness = np.array([np.inf] * self.popsize)\n        self.eval_count = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        while self.eval_count < self.budget:\n            # Calculate fitness for the entire population\n            for i in range(self.popsize):\n                if self.fitness[i] == np.inf:  # Evaluate only if not already evaluated\n                    self.fitness[i] = func(self.population[i])\n                    self.eval_count += 1\n                    if self.fitness[i] < self.f_opt:\n                        self.f_opt = self.fitness[i]\n                        self.x_opt = self.population[i]\n                    if self.eval_count >= self.budget:\n                        return self.f_opt, self.x_opt\n\n            # Population weighted mutation\n            fitness_normalized = np.max(self.fitness) - self.fitness\n            fitness_normalized = fitness_normalized / np.sum(fitness_normalized)\n\n            for i in range(self.popsize):\n                # Mutation: Population-weighted selection of parents\n                idxs = np.random.choice(self.popsize, 3, replace=False, p=fitness_normalized)\n                x_r1, x_r2, x_r3 = self.population[idxs]\n                x_mutated = x_r1 + self.F * (x_r2 - x_r3)\n                x_mutated = np.clip(x_mutated, -5, 5)\n\n                # Adaptive Crossover: Adjust CR based on population diversity\n                diversity = np.std(self.population)\n                cr_adaptive = np.clip(self.CR + diversity * 0.1, 0.1, 1.0)\n\n                # Crossover\n                x_trial = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < cr_adaptive or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Selection\n                f_trial = func(x_trial)\n                self.eval_count += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = x_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n\n                if self.eval_count >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "An exception occurred: probabilities contain NaN.", "error": "", "parent_ids": ["9d9d976c-2601-4a25-a7f7-553a92161f50"], "operator": null, "metadata": {}}
{"id": "7a602c9f-2f2d-4bee-b67d-55ecd26335b7", "fitness": 0.0, "name": "AdaptiveCMAES", "description": "A CMA-ES variant that adaptively adjusts the population size and learning rate based on the success rate of improvements, incorporating a restart mechanism to escape local optima.", "code": "import numpy as np\n\nclass AdaptiveCMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.normC = 0\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2.3)**2 + self.mueff))\n\n        self.lb = -5\n        self.ub = 5\n        self.archive = []\n        self.success_rate = 0.5\n        self.success_history = []\n        self.adaptation_rate = 0.1\n        self.restart_trigger = 10 * self.dim\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.success_history = []\n\n        while self.count_evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z\n\n            x = np.clip(x, self.lb, self.ub)\n\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.count_evals += self.popsize\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n                self.success_history.append(1)\n            else:\n                self.success_history.append(0)\n\n            idx = np.argsort(f)\n            x_selected = x[:, idx[:self.mu]]\n            f_selected = f[idx[:self.mu]]\n            \n            xmean = np.sum(x_selected * self.weights[np.newaxis, :], axis=1)\n\n            ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt.T, (xmean - self.mean) / self.sigma)\n            self.mean = xmean\n\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.count_evals / self.popsize)) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (xmean - self.mean) / self.sigma\n\n            artmp = (1 / self.sigma) * (x_selected - self.mean[:, np.newaxis])\n            self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * np.outer(self.pc, self.pc) + self.cmu * artmp @ np.diag(self.weights) @ artmp.T\n            \n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(ps) / self.chiN - 1))\n            self.sigma = max(self.sigma, 1e-10)\n\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-6 * np.eye(self.dim)\n            \n            # Adaptive Population Size\n            if len(self.success_history) > 50:\n                recent_success_rate = np.mean(self.success_history[-50:])\n                if recent_success_rate > 0.25:\n                    self.popsize = min(self.popsize + 1, 20 + int(6 * np.log(self.dim)))\n                elif recent_success_rate < 0.15:\n                    self.popsize = max(self.popsize - 1, 4 + int(3 * np.log(self.dim)))\n                self.mu = self.popsize // 2\n                self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n                self.weights = self.weights / np.sum(self.weights)\n                self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n                self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n                self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2.3)**2 + self.mueff))\n\n            # Restart Mechanism\n            if len(self.success_history) > self.restart_trigger and np.mean(self.success_history[-self.restart_trigger:]) < 0.01:\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n                self.sigma = 0.5\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.success_history = []\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveCMAES scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ccedbdf3-c17c-4add-8786-5f8bd21553f4"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "9e01f811-acab-415f-b7cc-5b81851d2e4c", "fitness": -Infinity, "name": "AdaptiveMirroredCMAES", "description": "Adaptive Mirrored CMA-ES with dynamic population size and learning rate adaptation based on success history, enhancing exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveMirroredCMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.normC = 0\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2.3)**2 + self.mueff))\n\n        self.lb = -5\n        self.ub = 5\n        self.archive = []\n        self.success_rate = 0.5\n        self.success_history = []\n        self.learning_rate = 0.1\n\n    def update_popsize(self):\n          # Dynamic population size adjustment\n        if len(self.success_history) > 10:\n            recent_success = np.mean(self.success_history[-10:])\n            if recent_success > 0.6:\n                self.popsize = max(4, int(self.popsize * 0.9))  # Reduce popsize if consistently successful\n            elif recent_success < 0.2:\n                self.popsize = min(self.budget // 2, int(self.popsize * 1.1))  # Increase popsize if struggling\n            self.mu = self.popsize // 2\n\n    def update_learning_rate(self, success):\n        if success:\n            self.learning_rate = min(1.0, self.learning_rate * 1.05)\n        else:\n            self.learning_rate = max(0.01, self.learning_rate * 0.95)\n        self.c1 = self.learning_rate * 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = self.learning_rate * min(1 - self.c1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2.3)**2 + self.mueff))\n    \n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.success_history = []\n        self.popsize =  4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        \n        while self.count_evals < self.budget:\n            self.update_popsize()\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z\n            x_mirrored = self.mean[:, np.newaxis] - self.sigma * C_sqrt @ z\n\n            x = np.clip(x, self.lb, self.ub)\n            x_mirrored = np.clip(x_mirrored, self.lb, self.ub)\n\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            f_mirrored = np.array([func(x_mirrored[:, i]) for i in range(self.popsize)])\n\n            self.count_evals += 2 * self.popsize\n\n            f_combined = np.concatenate((f, f_mirrored))\n            x_combined = np.concatenate((x, x_mirrored), axis=1)\n\n            best_idx = np.argmin(f_combined)\n            success = False\n            if f_combined[best_idx] < self.f_opt:\n                self.f_opt = f_combined[best_idx]\n                self.x_opt = x_combined[:, best_idx]\n                success = True\n            \n            self.success_history.append(int(success))\n            self.update_learning_rate(success)\n\n            idx = np.argsort(f_combined)\n            x_selected = x_combined[:, idx[:self.mu]]\n            f_selected = f_combined[idx[:self.mu]]\n            \n            xmean = np.sum(x_selected * self.weights[np.newaxis, :], axis=1)\n\n            ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt.T, (xmean - self.mean) / self.sigma)\n            self.mean = xmean\n\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.count_evals / (2*self.popsize))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (xmean - self.mean) / self.sigma\n\n            artmp = (1 / self.sigma) * (x_selected - self.mean[:, np.newaxis])\n            self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * np.outer(self.pc, self.pc) + self.cmu * artmp @ np.diag(self.weights) @ artmp.T\n            \n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(ps) / self.chiN - 1))\n            self.sigma = max(self.sigma, 1e-10)\n\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-6 * np.eye(self.dim)\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "An exception occurred: operands could not be broadcast together with shapes (5,3) (1,4) .", "error": "", "parent_ids": ["ccedbdf3-c17c-4add-8786-5f8bd21553f4"], "operator": null, "metadata": {}}
{"id": "59c86b9d-2edf-40cf-a18b-26f0654870cf", "fitness": 0.3206575854449857, "name": "CMAES", "description": "Simplified CMA-ES with adaptive population size, sigma adaptation, and a restart mechanism based on covariance matrix properties for improved exploration and exploitation.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2.3)**2 + self.mueff))\n        self.lb = -5\n        self.ub = 5\n        self.adapt_popsize = True\n        self.sigma_threshold = 1e-3\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 50\n        self.target_success = 0.25 \n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.stagnation_counter = 0\n\n        while self.count_evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z\n            x = np.clip(x, self.lb, self.ub)\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.count_evals += self.popsize\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n                self.stagnation_counter = 0\n            else:\n                self.stagnation_counter += 1\n            \n            idx = np.argsort(f)\n            x = x[:, idx]\n            f = f[idx]\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n            \n            ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt.T, (xmean - self.mean) / self.sigma)\n            self.mean = xmean\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.count_evals / self.popsize)) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (xmean - self.mean) / self.sigma\n            \n            artmp = (1 / self.sigma) * (x[:, :self.mu] - self.mean[:, np.newaxis])\n            self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * np.outer(self.pc, self.pc) + self.cmu * artmp @ np.diag(self.weights) @ artmp.T\n            \n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(ps) / self.chiN - 1))\n            self.sigma = max(self.sigma, 1e-10)\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            \n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C += 1e-6 * np.eye(self.dim)\n\n            if np.max(np.diag(self.C)) > (1e7 * self.sigma) or self.stagnation_counter > self.stagnation_threshold:\n                self.sigma *= 0.1\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n                self.stagnation_counter = 0\n\n            if self.adapt_popsize:\n                if self.count_evals > self.budget // 2 and self.sigma < self.sigma_threshold:\n                     self.popsize = max(4, self.popsize // 2)\n                     self.mu = self.popsize // 2\n                     self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n                     self.weights /= np.sum(self.weights)\n                     self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n                     self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n                     self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n                elif self.count_evals < self.budget // 2 and self.sigma > 1:\n                     self.popsize = min(self.dim * 10, self.popsize * 2)\n                     self.mu = self.popsize // 2\n                     self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n                     self.weights /= np.sum(self.weights)\n                     self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n                     self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n                     self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n                     \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm CMAES scored 0.321 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["dd36cd2e-68fb-4111-a638-ea5532f5f9a1"], "operator": null, "metadata": {"aucs": [0.11506333488860054, 0.18073737799160594, 0.696419666318972, 0.1652359217593956, 0.20132267793290648, 0.21806541743738506, 0.21686553640540973, 0.5285063324268782, 0.90433249911736, 0.2069768766580763, 0.24949756786110233, 0.20878494776894796, 0.24961450155757026, 0.18404964922530676, 0.726818695627901, 0.2818634809519921, 0.21873354077312035, 0.4570073569994698, 0.16086802581987636, 0.24238830137783518]}}
{"id": "f7b848e0-c9d4-4bd4-8d74-91594e781bd5", "fitness": 0.70607453259281, "name": "CMAES", "description": "Simplified CMA-ES with a linearly decreasing learning rate for covariance adaptation and a more aggressive sigma decay for faster convergence.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.lb = -5\n        self.ub = 5\n        self.c1 = None  # Initialize c1 and cmu\n        self.cmu = None\n        self.sigma_decay = 0.9995 # Aggressive sigma decay\n        self.c_learn_rate = 0.2 # Overall learning rate for covariance matrix\n        \n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        \n        while self.count_evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z\n            x = np.clip(x, self.lb, self.ub)\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.count_evals += self.popsize\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n\n            idx = np.argsort(f)\n            x = x[:, idx]\n\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n            \n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt.T, (xmean - self.mean) / self.sigma)\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.count_evals / self.popsize))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (xmean - self.mean) / self.sigma\n            \n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n\n            # Linearly decaying learning rate\n            c1 = self.c_learn_rate / (self.dim**2)\n            cmu = self.c_learn_rate / self.dim\n            \n            self.C = (1 - c1 - cmu) * self.C + c1 * np.outer(self.pc, self.pc) + cmu * artmp @ np.diag(self.weights) @ artmp.T\n            \n            self.mean = xmean\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            self.sigma = max(self.sigma, 1e-10)\n            self.sigma *= self.sigma_decay  # Decay sigma more aggressively\n\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-6 * np.eye(self.dim)\n\n            if np.max(np.diag(self.C)) > (1e7 * self.sigma):\n                self.sigma *= 0.1\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm CMAES scored 0.706 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b8aac1d0-12a8-43a1-b389-681cf71000a0"], "operator": null, "metadata": {"aucs": [0.22449097902018889, 0.3148161568980897, 0.9469513997250985, 0.957752065251066, 0.9502778755681563, 0.9547730431602736, 0.35209714107349177, 0.9348998560136105, 0.9460982814240682, 0.22246629521574623, 0.9635542701809294, 0.9937099133429971, 0.2530098418626129, 0.9396570606927241, 0.9012404394754797, 0.8691039907533696, 0.6896903415013564, 0.9618173180024799, 0.23907886845064674, 0.5060055142438136]}}
{"id": "6434aa25-fb32-47d5-abe1-f71f586008bc", "fitness": 0.7400637825418153, "name": "CMAES", "description": "CMA-ES with a restart mechanism based on stagnation detection and adaptive sigma decay, along with dynamic population size adjustment based on the optimization progress.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, restart_factor=3):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.normC = 0\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2.3)**2 + self.mueff))\n        self.lb = -5\n        self.ub = 5\n        self.decay = 0.9995  # Decay factor for sigma\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 50 # Number of iterations without improvement to trigger restart\n        self.restart_factor = restart_factor\n        self.archive_f = []\n        self.archive_x = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.stagnation_counter = 0\n        self.archive_f = []\n        self.archive_x = []\n        \n        while self.count_evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z\n            x = np.clip(x, self.lb, self.ub)\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.count_evals += self.popsize\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n                self.stagnation_counter = 0\n            else:\n                self.stagnation_counter += 1\n\n            idx = np.argsort(f)\n            x = x[:, idx]\n\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n            \n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt.T, (xmean - self.mean) / self.sigma)\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.count_evals / self.popsize))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (xmean - self.mean) / self.sigma\n            \n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n            self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * np.outer(self.pc, self.pc) + self.cmu * artmp @ np.diag(self.weights) @ artmp.T\n            \n            self.mean = xmean\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            self.sigma = max(self.sigma, 1e-10)\n            self.sigma *= self.decay  # Decay sigma\n\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-6 * np.eye(self.dim)\n            \n            if np.max(np.diag(self.C)) > (1e7 * self.sigma):\n                self.sigma *= 0.1\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n                self.stagnation_counter = 0\n            \n            if self.stagnation_counter > self.stagnation_threshold:\n                self.sigma *= self.restart_factor\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n                self.stagnation_counter = 0\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm CMAES scored 0.740 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b8aac1d0-12a8-43a1-b389-681cf71000a0"], "operator": null, "metadata": {"aucs": [0.33956215264721945, 0.8646761081661429, 0.951084658812262, 0.968406786012533, 0.9201309012030118, 0.959579637589329, 0.3354603035590712, 0.936662755397651, 0.9495217725881098, 0.613115946795938, 0.9724380106071655, 0.9946232449558229, 0.28098154888983895, 0.9398005732699974, 0.8280176651343778, 0.7802915883467543, 0.4195110654960996, 0.9625988932000558, 0.28146003118654794, 0.5033520069783773]}}
{"id": "0002a00b-c391-41de-8711-07a8836bc85a", "fitness": 0.6727064886592851, "name": "AdaptiveDifferentialEvolution", "description": "Simplified Adaptive Differential Evolution with self-adaptive F and CR based on successful mutations and reduced parameter tuning.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, popsize=None):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 10 * self.dim\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9 # Crossover rate\n        self.population = np.random.uniform(-5, 5, size=(self.popsize, self.dim))\n        self.fitness = np.array([np.inf] * self.popsize)\n        self.eval_count = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        while self.eval_count < self.budget:\n            for i in range(self.popsize):\n                # Mutation\n                idxs = np.random.choice(self.popsize, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[idxs]\n                x_mutated = x_r1 + self.F * (x_r2 - x_r3)\n                x_mutated = np.clip(x_mutated, -5, 5)\n\n                # Crossover\n                x_trial = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Selection\n                f_trial = func(x_trial)\n                self.eval_count += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = x_trial\n\n                    # Simplified adaptive parameter update\n                    if np.random.rand() < 0.1: # Update F and CR with a probability\n                        self.F = np.clip(np.random.normal(0.5, 0.3), 0.1, 0.9)\n                        self.CR = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0)\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = x_trial\n\n                if self.eval_count >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.673 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9d9d976c-2601-4a25-a7f7-553a92161f50"], "operator": null, "metadata": {"aucs": [0.27444534292455003, 0.6432003912743596, 0.6340542504585258, 0.863839906957999, 0.7255655671399859, 0.7298682993192334, 0.6533167081700122, 0.6146594106730497, 0.7026985036959863, 0.5889906992047558, 0.8379160879805401, 0.9986807627800505, 0.6248741306913888, 0.6908905831779188, 0.9352282082831168, 0.7860839952280393, 0.5675977503799298, 0.8172114663967527, 0.2510622314338209, 0.5139454770156852]}}
{"id": "da43c200-c783-424f-b9a5-1e4a642b1980", "fitness": 0.725904522774881, "name": "CMAES_Restart_Orthogonal", "description": "CMA-ES with a population-based covariance matrix adaptation and a restart strategy triggered by stagnation detection, incorporating orthogonal subspace sampling to enhance exploration.", "code": "import numpy as np\n\nclass CMAES_Restart_Orthogonal:\n    def __init__(self, budget=10000, dim=10, popsize=None, restart_trigger=1e-12):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2.3)**2 + self.mueff))\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.lb = -5\n        self.ub = 5\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.count_evals = 0\n        self.restart_trigger = restart_trigger  # Threshold for stagnation detection\n        self.stagnation_counter = 0\n        self.max_stagnation = 50  # Maximum number of iterations before restart\n        self.orthogonal_basis = None\n        self.orthogonal_component = 0.1  # Weight for orthogonal component\n        self.decay = 0.999\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.stagnation_counter = 0\n        self.orthogonal_basis = None\n\n        while self.count_evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z\n\n            # Orthogonal subspace sampling\n            if self.orthogonal_basis is not None:\n                orthogonal_z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n                x += self.orthogonal_component * self.sigma * self.orthogonal_basis @ orthogonal_z\n            \n            x = np.clip(x, self.lb, self.ub)\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.count_evals += self.popsize\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n                self.stagnation_counter = 0  # Reset stagnation counter\n            else:\n                self.stagnation_counter += 1\n\n            idx = np.argsort(f)\n            x = x[:, idx]\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n            \n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt.T, (xmean - self.mean) / self.sigma)\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.count_evals / self.popsize))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (xmean - self.mean) / self.sigma\n            \n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n            self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * np.outer(self.pc, self.pc) + self.cmu * artmp @ np.diag(self.weights) @ artmp.T\n            \n            self.mean = xmean\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            self.sigma = max(self.sigma, 1e-10)\n            self.sigma *= self.decay\n\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-6 * np.eye(self.dim)\n\n            if np.max(np.diag(self.C)) > (1e7 * self.sigma):\n                self.sigma *= 0.1\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n                self.orthogonal_basis = None\n\n            # Restart strategy\n            if self.stagnation_counter > self.max_stagnation:\n                self.restart()\n\n        return self.f_opt, self.x_opt\n\n    def restart(self):\n        self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n        self.sigma = 0.5\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.stagnation_counter = 0\n\n        # Create orthogonal basis using PCA on previous population\n        x = np.random.uniform(self.lb, self.ub, size=(self.dim, self.popsize))\n        \n        # Center the data\n        centered_data = x - np.mean(x, axis=1, keepdims=True)\n\n        # Perform SVD\n        try:\n            U, S, V = np.linalg.svd(centered_data)\n            self.orthogonal_basis = U # Use principal components as orthogonal basis\n        except np.linalg.LinAlgError:\n            self.orthogonal_basis = np.random.normal(0, 1, size=(self.dim, self.dim))\n            Q, R = np.linalg.qr(self.orthogonal_basis)\n            self.orthogonal_basis = Q", "configspace": "", "generation": 2, "feedback": "The algorithm CMAES_Restart_Orthogonal scored 0.726 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b8aac1d0-12a8-43a1-b389-681cf71000a0"], "operator": null, "metadata": {"aucs": [0.22701073416881445, 0.549424943882888, 0.9338709627279725, 0.9585652569617976, 0.9537018576942461, 0.9553323546676847, 0.3764371699771365, 0.932660570553387, 0.9502369998363248, 0.19537345562254604, 0.9619059400195743, 0.9969417745544247, 0.31598186984653465, 0.9367832643129359, 0.9357210656653945, 0.714076369903492, 0.9031428203685294, 0.9648746717488603, 0.2521687370093828, 0.5038796359756952]}}
{"id": "22996ab4-2d91-46ec-8fd8-a00c8353c286", "fitness": 0.6142503520745368, "name": "AdaptiveDifferentialEvolution", "description": "Adaptive Differential Evolution with success-history based parameter adaptation and archive for enhanced exploration.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, popsize=None, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 10 * self.dim\n        self.archive_size = archive_size\n        self.F = 0.5  # Initial Mutation factor\n        self.CR = 0.9 # Initial Crossover rate\n        self.population = np.random.uniform(-5, 5, size=(self.popsize, self.dim))\n        self.fitness = np.array([np.inf] * self.popsize)\n        self.eval_count = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.archive = []\n        self.archive_fitness = []\n        self.success_F = []\n        self.success_CR = []\n        self.success_count = 0\n\n    def __call__(self, func):\n        self.fitness = np.array([func(x) if i >= 0 else np.inf for i, x in enumerate(self.population)])\n        self.eval_count = self.popsize\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        \n        while self.eval_count < self.budget:\n            for i in range(self.popsize):\n                # Mutation\n                idxs = np.random.choice(self.popsize, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[idxs]\n                \n                # Incorporate archive individuals\n                if len(self.archive) > 0 and np.random.rand() < 0.1:  # 10% chance to use archive\n                    idx_archive = np.random.randint(len(self.archive))\n                    x_r3 = self.archive[idx_archive]\n                    \n                x_mutated = x_r1 + self.F * (x_r2 - x_r3)\n                x_mutated = np.clip(x_mutated, -5, 5)\n\n                # Crossover\n                x_trial = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Selection\n                f_trial = func(x_trial)\n                self.eval_count += 1\n\n                if f_trial < self.fitness[i]:\n                    # Success!\n                    self.success_count += 1\n                    self.success_F.append(self.F)\n                    self.success_CR.append(self.CR)\n                    \n                    # Update population and fitness\n                    self.fitness[i] = f_trial\n                    self.population[i] = x_trial\n\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(x_trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        #Replace the worst individual in archive\n                        max_archive_idx = np.argmax(self.archive_fitness)\n                        if f_trial < self.archive_fitness[max_archive_idx]:\n                            self.archive[max_archive_idx] = x_trial\n                            self.archive_fitness[max_archive_idx] = f_trial\n                    \n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n\n                    # Adaptive F and CR update based on success history\n                    if self.success_count > 10:\n                        self.F = np.mean(self.success_F) if len(self.success_F) > 0 else 0.5\n                        self.CR = np.mean(self.success_CR) if len(self.success_CR) > 0 else 0.9\n                        self.F = np.clip(np.random.normal(self.F, 0.3), 0.1, 0.9)\n                        self.CR = np.clip(np.random.normal(self.CR, 0.1), 0.1, 1.0)\n                        self.success_F = []\n                        self.success_CR = []\n                        self.success_count = 0\n                \n                if self.eval_count >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.614 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9d9d976c-2601-4a25-a7f7-553a92161f50"], "operator": null, "metadata": {"aucs": [0.2528767268185338, 0.43531936972527285, 0.5277152039240542, 0.841683184489983, 0.6913667870287816, 0.7238229187971387, 0.5903333783660636, 0.5749989257867834, 0.7034944040981861, 0.5975903386749444, 0.8115737707684214, 0.9925019437777735, 0.39077694067341595, 0.5593892627465566, 0.8702746843404386, 0.7553721826407986, 0.41589754244026766, 0.8268395347531702, 0.22727437726385757, 0.49590556437629574]}}
{"id": "8838deb9-01ab-4b95-ad6c-6aeaba07e4da", "fitness": 0.4937348223568205, "name": "AdaptiveDifferentialEvolution", "description": "Adaptive Differential Evolution with archive for stagnation avoidance and improved parameter adaptation based on past success.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, popsize=None, F=0.5, CR=0.9, archive_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 10 * self.dim\n        self.F = F  # Mutation factor\n        self.CR = CR # Crossover rate\n        self.population = np.random.uniform(-5, 5, size=(self.popsize, self.dim))\n        self.fitness = np.array([np.inf] * self.popsize)\n        self.eval_count = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.F_history = []\n        self.CR_history = []\n        self.archive = []\n        self.archive_fitness = []\n        self.archive_size = archive_size\n\n\n    def __call__(self, func):\n        while self.eval_count < self.budget:\n            for i in range(self.popsize):\n                # Mutation\n                idxs = np.random.choice(self.popsize, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[idxs]\n                \n                #Stagnation avoidance: If no improvement, sample from archive\n                if len(self.archive) > 0 and np.random.rand() < 0.1:\n                    idx_archive = np.random.randint(len(self.archive))\n                    x_r1 = self.archive[idx_archive]\n                \n                x_mutated = x_r1 + self.F * (x_r2 - x_r3)\n                x_mutated = np.clip(x_mutated, -5, 5)\n\n                # Crossover\n                x_trial = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Selection\n                f_trial = func(x_trial)\n                self.eval_count += 1\n\n                if f_trial < self.fitness[i]:\n                    # Archive update\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i])\n                        self.archive_fitness.append(self.fitness[i])\n                    else:\n                        # Replace the worst element in the archive\n                        max_archive_index = np.argmax(self.archive_fitness)\n                        if self.fitness[i] < self.archive_fitness[max_archive_index]:\n                            self.archive[max_archive_index] = self.population[i]\n                            self.archive_fitness[max_archive_index] = self.fitness[i]\n                            \n\n                    self.fitness[i] = f_trial\n                    self.population[i] = x_trial\n\n                    # Adaptive parameter update based on success\n                    self.F_history.append(self.F)\n                    self.CR_history.append(self.CR)\n\n\n                    #Dynamically tune F and CR, using history\n                    if len(self.F_history) > 10:\n                        self.F = np.clip(np.random.normal(np.mean(self.F_history[-10:]), np.std(self.F_history[-10:])), 0.1, 0.9)\n                        self.CR = np.clip(np.random.normal(np.mean(self.CR_history[-10:]), np.std(self.CR_history[-10:])), 0.1, 1.0)\n                    else:\n                        self.F = np.clip(np.random.normal(0.5, 0.3), 0.1, 0.9)\n                        self.CR = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0)\n\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = x_trial\n                    \n\n                if self.eval_count >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.494 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9d9d976c-2601-4a25-a7f7-553a92161f50"], "operator": null, "metadata": {"aucs": [0.2462564791121996, 0.34101030939120847, 0.46627202646339916, 0.8062587723699772, 0.7231143932030852, 0.19024237468991134, 0.409562848351709, 0.5046721383673076, 0.7002025204940802, 0.22838655577744493, 0.3279856985302183, 0.9915738846812963, 0.4085071231797911, 0.36633336000907835, 0.8629524465677725, 0.8691977980961897, 0.43083507543492217, 0.25538375194718277, 0.22976604790239907, 0.5161828425672381]}}
{"id": "1d74b48c-ff8b-4720-9657-403bdc1b5119", "fitness": 0.21035283205120692, "name": "AdaptiveCMAES", "description": "An adaptive CMA-ES variant that dynamically adjusts its covariance matrix update and step size based on a combination of success history and orthogonal subspace exploration.", "code": "import numpy as np\n\nclass AdaptiveCMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.normC = 0\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2.3)**2 + self.mueff))\n        self.lb = -5\n        self.ub = 5\n        self.success_history = []\n        self.success_rate_threshold = 0.3\n        self.orthogonal_basis = np.eye(self.dim)\n        self.rotation_frequency = 10 * self.popsize\n        self.last_rotation = 0\n        self.decay = 0.99\n        self.adapt_popsize = True\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.success_history = []\n        self.orthogonal_basis = np.eye(self.dim)\n        self.last_rotation = 0\n        \n        while self.count_evals < self.budget:\n            \n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z\n            x = np.clip(x, self.lb, self.ub)\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.count_evals += self.popsize\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n\n            idx = np.argsort(f)\n            x = x[:, idx]\n            f = f[idx]\n\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n            \n            ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt.T, (xmean - self.mean) / self.sigma)\n            self.mean = xmean\n            \n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.count_evals / self.popsize)) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            \n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (xmean - self.mean) / self.sigma\n            \n            artmp = (1 / self.sigma) * (x[:, :self.mu] - self.mean[:, np.newaxis])\n            \n            # Adaptive Covariance Matrix Update\n            self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * np.outer(self.pc, self.pc) + self.cmu * artmp @ np.diag(self.weights) @ artmp.T\n            \n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(ps) / self.chiN - 1))\n            self.sigma = max(self.sigma, 1e-10)\n            \n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            \n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-6 * np.eye(self.dim)\n\n            # Orthogonal subspace exploration\n            if self.count_evals - self.last_rotation > self.rotation_frequency:\n                self.last_rotation = self.count_evals\n                Q, _ = np.linalg.qr(np.random.randn(self.dim, self.dim))\n                self.orthogonal_basis = Q\n                self.mean = self.orthogonal_basis @ self.mean\n\n            # Restart mechanism\n            if np.max(np.diag(self.C)) > (1e7 * self.sigma):\n                self.sigma *= 0.1\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n\n            # Adapt population size based on stagnation\n            if self.adapt_popsize:\n                if self.count_evals > self.budget // 2 and self.sigma < 1e-3:\n                     self.popsize = max(4, self.popsize // 2)\n                     self.mu = self.popsize // 2\n                     self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n                     self.weights = self.weights / np.sum(self.weights)\n                     self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n                     self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n                     self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n                elif self.count_evals < self.budget // 2 and self.sigma > 1:\n                     self.popsize = min(self.dim * 10, self.popsize * 2)\n                     self.mu = self.popsize // 2\n                     self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n                     self.weights = self.weights / np.sum(self.weights)\n                     self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n                     self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n                     self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveCMAES scored 0.210 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["dd36cd2e-68fb-4111-a638-ea5532f5f9a1"], "operator": null, "metadata": {"aucs": [0.09138260412023813, 0.161668949802538, 0.31609709971158273, 0.15832448969221535, 0.16410099860900007, 0.2589251246443025, 0.2100006558044002, 0.1728046696219382, 0.2205270000685129, 0.14532689138306232, 0.16660351539432505, 0.24410281714291393, 0.2506429713893772, 0.17532196567488556, 0.20916489971431962, 0.2506957127385494, 0.2304081720343596, 0.18869515533397896, 0.15741257026139244, 0.43485037788224656]}}
{"id": "91be8b86-6346-413f-a982-be14848c9989", "fitness": 0.3789103766491355, "name": "SimplifiedMirroredCMAES", "description": "Simplify Mirrored CMA-ES by removing redundant mirrored samples based on the covariance matrix and adapt sigma.", "code": "import numpy as np\n\nclass SimplifiedMirroredCMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.normC = 0\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2.3)**2 + self.mueff))\n\n        self.lb = -5\n        self.ub = 5\n        self.archive = []\n        self.success_rate = 0.5\n        self.success_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        \n        while self.count_evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z\n\n            x = np.clip(x, self.lb, self.ub)\n\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n\n            self.count_evals += self.popsize\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n\n            idx = np.argsort(f)\n            x_selected = x[:, idx[:self.mu]]\n            f_selected = f[idx[:self.mu]]\n            \n            xmean = np.sum(x_selected * self.weights[np.newaxis, :], axis=1)\n\n            ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt.T, (xmean - self.mean) / self.sigma)\n            self.mean = xmean\n\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.count_evals / (2*self.popsize))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (xmean - self.mean) / self.sigma\n\n            artmp = (1 / self.sigma) * (x_selected - self.mean[:, np.newaxis])\n            self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * np.outer(self.pc, self.pc) + self.cmu * artmp @ np.diag(self.weights) @ artmp.T\n            \n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(ps) / self.chiN - 1))\n            self.sigma = max(self.sigma, 1e-10)\n\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-6 * np.eye(self.dim)\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm SimplifiedMirroredCMAES scored 0.379 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ccedbdf3-c17c-4add-8786-5f8bd21553f4"], "operator": null, "metadata": {"aucs": [0.19818813910689914, 0.2187176960049536, 0.4307403547078751, 0.2569936437483272, 0.4714341587639499, 0.9617597787988393, 0.21762635147992804, 0.2637818746500854, 0.28375153790185603, 0.20040644900432314, 0.32703037431908755, 0.7500791823332625, 0.25476243848835134, 0.2942829120961965, 0.1775611584149599, 0.29886029371250356, 0.24223044514035075, 0.9595515731160609, 0.32513545833241597, 0.4453137128624839]}}
{"id": "1456b36f-0271-4b31-a0db-608649d6cee6", "fitness": -Infinity, "name": "CMAES_Restart_Orthogonal_Adaptive", "description": "Enhanced CMA-ES with adaptive orthogonal subspace sampling, spectral decay of covariance matrix, and dynamic population size adjustment based on optimization progress.", "code": "import numpy as np\n\nclass CMAES_Restart_Orthogonal_Adaptive:\n    def __init__(self, budget=10000, dim=10, popsize=None, restart_trigger=1e-12):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2.3)**2 + self.mueff))\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.lb = -5\n        self.ub = 5\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.count_evals = 0\n        self.restart_trigger = restart_trigger  # Threshold for stagnation detection\n        self.stagnation_counter = 0\n        self.max_stagnation = 50  # Maximum number of iterations before restart\n        self.orthogonal_basis = None\n        self.orthogonal_component = 0.1  # Initial weight for orthogonal component\n        self.orthogonal_component_decay = 0.95 # Decay rate for orthogonal component\n        self.sigma_decay = 0.999 # Decay rate for sigma\n        self.C_decay = 0.999 # Decay rate for C eigenvalues\n        self.decay = 0.999\n        self.min_popsize = 4\n        self.max_popsize = 200\n        self.popsize_increase_factor = 1.1\n        self.popsize_decrease_factor = 0.9\n        self.last_improvement = 0\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.stagnation_counter = 0\n        self.orthogonal_basis = None\n        self.orthogonal_component = 0.1\n        self.popsize = 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.last_improvement = 0\n\n        while self.count_evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z\n\n            # Orthogonal subspace sampling\n            if self.orthogonal_basis is not None:\n                orthogonal_z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n                x += self.orthogonal_component * self.sigma * self.orthogonal_basis @ orthogonal_z\n            \n            x = np.clip(x, self.lb, self.ub)\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.count_evals += self.popsize\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n                self.stagnation_counter = 0  # Reset stagnation counter\n                self.last_improvement = self.count_evals\n                # Increase popsize\n                self.popsize = min(int(self.popsize * self.popsize_increase_factor), self.max_popsize)\n                self.mu = self.popsize // 2\n\n            else:\n                self.stagnation_counter += 1\n                #Decrease popsize\n                if self.count_evals - self.last_improvement > self.budget / 10:\n                    self.popsize = max(int(self.popsize * self.popsize_decrease_factor), self.min_popsize)\n                    self.mu = self.popsize // 2\n\n\n            idx = np.argsort(f)\n            x = x[:, idx]\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n            \n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt.T, (xmean - self.mean) / self.sigma)\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.count_evals / self.popsize))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (xmean - self.mean) / self.sigma\n            \n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n            self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * np.outer(self.pc, self.pc) + self.cmu * artmp @ np.diag(self.weights) @ artmp.T\n            \n            self.mean = xmean\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            self.sigma = max(self.sigma, 1e-10)\n            self.sigma *= self.sigma_decay\n\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-6 * np.eye(self.dim)\n\n            if np.max(np.diag(self.C)) > (1e7 * self.sigma):\n                self.sigma *= 0.1\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n                self.orthogonal_basis = None\n\n            # Restart strategy\n            if self.stagnation_counter > self.max_stagnation:\n                self.restart()\n            \n            self.orthogonal_component *= self.orthogonal_component_decay\n\n        return self.f_opt, self.x_opt\n\n    def restart(self):\n        self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n        self.sigma = 0.5\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.stagnation_counter = 0\n\n        # Create orthogonal basis using PCA on previous population\n        x = np.random.uniform(self.lb, self.ub, size=(self.dim, self.popsize))\n        \n        # Center the data\n        centered_data = x - np.mean(x, axis=1, keepdims=True)\n\n        # Perform SVD\n        try:\n            U, S, V = np.linalg.svd(centered_data)\n            self.orthogonal_basis = U # Use principal components as orthogonal basis\n        except np.linalg.LinAlgError:\n            self.orthogonal_basis = np.random.normal(0, 1, size=(self.dim, self.dim))\n            Q, R = np.linalg.qr(self.orthogonal_basis)\n            self.orthogonal_basis = Q", "configspace": "", "generation": 3, "feedback": "An exception occurred: operands could not be broadcast together with shapes (2,2) (1,3) .", "error": "", "parent_ids": ["da43c200-c783-424f-b9a5-1e4a642b1980"], "operator": null, "metadata": {}}
{"id": "d93d6ec0-26cb-47b2-b266-19865438d152", "fitness": 0.0, "name": "CMAES", "description": "CMA-ES with dynamic learning rate adjustment based on optimization progress, adaptive population sizing, and a more robust covariance matrix adaptation strategy to enhance exploration and exploitation.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.lb = -5\n        self.ub = 5\n        self.c1 = None  # Initialize c1 and cmu\n        self.cmu = None\n        self.sigma_decay = 0.9995 # Aggressive sigma decay\n        self.c_learn_rate = 0.2 # Overall learning rate for covariance matrix\n        self.adapt_popsize = True\n        self.min_popsize = 4 + int(3 * np.log(self.dim))\n        self.max_popsize = 10 + int(10 * np.log(self.dim))\n        self.best_f_list = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        \n        while self.count_evals < self.budget:\n            # Adapt popsize\n            if self.adapt_popsize:\n                if len(self.best_f_list) > 10:\n                    if np.std(self.best_f_list[-10:]) < 1e-3:\n                        self.popsize = min(self.popsize + 1, self.max_popsize)\n                        self.mu = self.popsize // 2\n                        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n                        self.weights = self.weights / np.sum(self.weights)\n                        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n                        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n                        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n                    else:\n                         self.popsize = max(self.popsize - 1, self.min_popsize)\n                         self.mu = self.popsize // 2\n                         self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n                         self.weights = self.weights / np.sum(self.weights)\n                         self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n                         self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n                         self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z\n            x = np.clip(x, self.lb, self.ub)\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.count_evals += self.popsize\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n                self.best_f_list.append(self.f_opt)\n            else:\n                 self.best_f_list.append(self.f_opt)\n\n            idx = np.argsort(f)\n            x = x[:, idx]\n\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n            \n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt.T, (xmean - self.mean) / self.sigma)\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.count_evals / self.popsize))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (xmean - self.mean) / self.sigma\n            \n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n\n            # Dynamic learning rate\n            c1 = self.c_learn_rate / (self.dim**2) * (0.1 + 0.9 * np.exp(-5 * self.count_evals / self.budget))\n            cmu = self.c_learn_rate / self.dim * (0.1 + 0.9 * np.exp(-5 * self.count_evals / self.budget))\n            \n            self.C = (1 - c1 - cmu) * self.C + c1 * np.outer(self.pc, self.pc) + cmu * artmp @ np.diag(self.weights) @ artmp.T\n            \n            self.mean = xmean\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            self.sigma = max(self.sigma, 1e-10)\n            self.sigma *= self.sigma_decay  # Decay sigma more aggressively\n\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-6 * np.eye(self.dim)\n\n            if np.max(np.diag(self.C)) > (1e7 * self.sigma):\n                self.sigma *= 0.1\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm CMAES scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f7b848e0-c9d4-4bd4-8d74-91594e781bd5"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "fcc5ab1a-48a5-4347-851c-a14a73446292", "fitness": 0.7450163170639105, "name": "CMAES_Mirror", "description": "CMA-ES with mirrored sampling to enhance exploration and a decaying learning rate based on function evaluations.", "code": "import numpy as np\n\nclass CMAES_Mirror:\n    def __init__(self, budget=10000, dim=10, popsize=None):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.lb = -5\n        self.ub = 5\n        self.c_learn_rate = 0.5 # Overall learning rate for covariance matrix\n        self.min_sigma = 1e-10\n        self.evals = 0\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n\n        while self.evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            \n            # Mirrored sampling\n            z_mirrored = -z\n            z_combined = np.concatenate([z, z_mirrored], axis=1)\n            \n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z_combined\n            x = np.clip(x, self.lb, self.ub)\n            \n            f = np.array([func(x[:, i]) for i in range(2 * self.popsize)])\n            self.evals += 2 * self.popsize\n            \n            if self.evals > self.budget:\n                f = f[:(self.budget - (self.evals - 2 * self.popsize))]\n                x = x[:, :(self.budget - (self.evals - 2 * self.popsize))]\n                \n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n            \n            idx = np.argsort(f)\n            x = x[:, idx]\n            \n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n            \n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt.T, (xmean - self.mean) / self.sigma)\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.evals / (2*self.popsize)))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (xmean - self.mean) / self.sigma\n            \n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n            \n            # Evaluation based learning rate decay\n            learn_rate = self.c_learn_rate * (1 - (self.evals / self.budget))\n            \n            c1 = learn_rate / (self.dim**2)\n            cmu = learn_rate / self.dim\n            \n            self.C = (1 - c1 - cmu) * self.C + c1 * np.outer(self.pc, self.pc) + cmu * artmp @ np.diag(self.weights) @ artmp.T\n            \n            self.mean = xmean\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            self.sigma = max(self.sigma, self.min_sigma) # avoid sigma getting too small\n\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-6 * np.eye(self.dim)\n                \n            if np.max(np.diag(self.C)) > (1e7 * self.sigma):\n                self.sigma *= 0.1\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm CMAES_Mirror scored 0.745 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f7b848e0-c9d4-4bd4-8d74-91594e781bd5"], "operator": null, "metadata": {"aucs": [0.4260654289959157, 0.381767363007248, 0.9284784403601442, 0.960543575596152, 0.933594194445013, 0.9362455737066855, 0.3549198495317476, 0.9145250279670736, 0.9395818012274075, 0.5214060877141017, 0.966983580617124, 0.9944651835485988, 0.28500138363458083, 0.9338268153263041, 0.9574182809397936, 0.937329678574467, 0.8490677118036872, 0.9587741730424183, 0.20101816392864313, 0.5193140273111021]}}
{"id": "94ef6b0c-e4a3-406d-9c20-b25197d83e85", "fitness": 0.3084023397323945, "name": "CMAES_RankOne", "description": "CMA-ES with a simplified rank-one update and a dynamic sigma adaptation based on the fitness variance of the population, coupled with a basic restart mechanism.", "code": "import numpy as np\n\nclass CMAES_RankOne:\n    def __init__(self, budget=10000, dim=10, popsize=None):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.lb = -5\n        self.ub = 5\n        self.sigma_adapt = 1.0\n        self.count_evals = 0\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.sigma_adapt = 1.0\n\n        stagnation_counter = 0\n        last_fopt = np.Inf\n\n        while self.count_evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z\n            x = np.clip(x, self.lb, self.ub)\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.count_evals += self.popsize\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n                stagnation_counter = 0 # Reset stagnation counter\n            else:\n                stagnation_counter += 1\n\n            idx = np.argsort(f)\n            x = x[:, idx]\n            f = f[idx]\n\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt.T, (xmean - self.mean) / self.sigma)\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.count_evals / self.popsize))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (xmean - self.mean) / self.sigma\n\n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n            \n            # Simplified Rank-One Update\n            self.C = (1 - 0.1) * self.C + 0.1 * np.outer(self.pc, self.pc)\n\n            self.mean = xmean\n\n            # Dynamic Sigma Adaptation based on Fitness Variance\n            fitness_variance = np.var(f)\n            if fitness_variance > 0:\n                self.sigma_adapt = np.exp(0.1 * (np.sqrt(fitness_variance) - 1))\n            else:\n                 self.sigma_adapt = 0.9 # Reduce if variance is zero\n\n            self.sigma *= self.sigma_adapt\n            self.sigma = max(self.sigma, 1e-10)\n\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-6 * np.eye(self.dim)\n                \n            # Basic Restart Mechanism\n            if stagnation_counter > 50 + 10 * self.dim:\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n                self.sigma = 0.5\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                stagnation_counter = 0\n                self.sigma_adapt = 1.0\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm CMAES_RankOne scored 0.308 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f7b848e0-c9d4-4bd4-8d74-91594e781bd5"], "operator": null, "metadata": {"aucs": [0.08288207365829203, 0.15337593533397287, 0.4528198041209335, 0.6898829997713842, 0.5303276024580673, 0.3005522751506534, 0.24517171661034587, 0.37374548262972207, 0.3108747630253028, 0.2273412829059579, 0.27119569121812337, 0.21020414632029782, 0.1394692978295221, 0.34178525413846894, 0.3157953044436004, 0.2789984123013256, 0.3005061601806229, 0.3638033548863464, 0.14505425508053338, 0.4342609825844175]}}
{"id": "65f75978-ad6a-41ae-866a-2777b1ba3856", "fitness": 0.6965764856717371, "name": "CMAES", "description": "CMA-ES with active covariance matrix adaptation, dynamic population size, and adaptive sigma decay, incorporating a rank-one update based on negative weights to improve exploration and prevent premature convergence.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.lb = -5\n        self.ub = 5\n        self.sigma_decay = 0.9995 # Aggressive sigma decay\n        self.c_learn_rate = 0.2 # Overall learning rate for covariance matrix\n        self.c1 = self.c_learn_rate / (self.dim**2)\n        self.cmu = self.c_learn_rate / self.dim\n        self.c_mueff = 0.2 #For rank-one update with negative weights\n        self.weights_negative = np.minimum(0, self.weights) # Negative weights\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        \n        while self.count_evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z\n            x = np.clip(x, self.lb, self.ub)\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.count_evals += self.popsize\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n\n            idx = np.argsort(f)\n            x = x[:, idx]\n\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n            \n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt.T, (xmean - self.mean) / self.sigma)\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.count_evals / self.popsize))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (xmean - self.mean) / self.sigma\n            \n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n            \n            self.C = (1 - self.c1 - self.cmu + self.c1 * self.c_mueff * np.sum(self.weights_negative)) * self.C + self.c1 * np.outer(self.pc, self.pc) + self.cmu * artmp @ np.diag(self.weights) @ artmp.T\n            \n            self.mean = xmean\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            self.sigma = max(self.sigma, 1e-10)\n            self.sigma *= self.sigma_decay  # Decay sigma more aggressively\n\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-6 * np.eye(self.dim)\n\n            if np.max(np.diag(self.C)) > (1e7 * self.sigma):\n                self.sigma *= 0.1\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm CMAES scored 0.697 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f7b848e0-c9d4-4bd4-8d74-91594e781bd5"], "operator": null, "metadata": {"aucs": [0.24259190493880378, 0.22298372319255522, 0.9420344168721829, 0.969525436925236, 0.9452628040792201, 0.9595838233515597, 0.31966976208513576, 0.9432514277673707, 0.954348616181485, 0.26869775612225466, 0.9571019889529367, 0.9949738117134662, 0.2744854646020751, 0.9480106957237854, 0.8932266550253106, 0.3358831573192108, 0.9125114429255063, 0.9655270306484686, 0.39296550190629187, 0.4888942931018876]}}
{"id": "4ed695e6-f73b-444e-a664-dfb07c20dbce", "fitness": 0.7284354270414621, "name": "CMAES", "description": "CMA-ES with adaptive learning rates for covariance matrix adaptation based on the trace of the covariance matrix, aggressive sigma decay and improved restart strategy.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.lb = -5\n        self.ub = 5\n        self.sigma_decay = 0.9995 # Aggressive sigma decay\n        self.c_learn_rate = 0.2 # Overall learning rate for covariance matrix\n        self.trace_C_history = []\n        self.restart_threshold = 1e6  # Threshold for restarting based on trace of C\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 50  # Number of iterations to wait before restarting\n        \n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.trace_C_history = []\n        self.stagnation_counter = 0\n        \n        while self.count_evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z\n            x = np.clip(x, self.lb, self.ub)\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.count_evals += self.popsize\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n                self.stagnation_counter = 0  # Reset stagnation counter when improvement is found\n            else:\n                self.stagnation_counter += 1  # Increment stagnation counter if no improvement\n\n            idx = np.argsort(f)\n            x = x[:, idx]\n\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n            \n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt.T, (xmean - self.mean) / self.sigma)\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.count_evals / self.popsize))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (xmean - self.mean) / self.sigma\n            \n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n\n            # Adaptive learning rates based on trace of C\n            trace_C = np.trace(self.C)\n            self.trace_C_history.append(trace_C)\n            c1 = self.c_learn_rate / (self.dim**2) * (1 + np.log10(1 + trace_C))\n            cmu = self.c_learn_rate / self.dim * (1 + np.log10(1 + trace_C))\n            \n            self.C = (1 - c1 - cmu) * self.C + c1 * np.outer(self.pc, self.pc) + cmu * artmp @ np.diag(self.weights) @ artmp.T\n            \n            self.mean = xmean\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            self.sigma = max(self.sigma, 1e-10)\n            self.sigma *= self.sigma_decay  # Decay sigma more aggressively\n\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-6 * np.eye(self.dim)\n            \n            # Restart strategy based on trace of C and stagnation\n            if trace_C > self.restart_threshold or self.stagnation_counter > self.stagnation_threshold:\n                self.sigma *= 0.1\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n                self.stagnation_counter = 0  # Reset stagnation counter after restart\n                self.trace_C_history = [] # Reset trace history after restart\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm CMAES scored 0.728 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f7b848e0-c9d4-4bd4-8d74-91594e781bd5"], "operator": null, "metadata": {"aucs": [0.2473053909072257, 0.42820169946677766, 0.9477836641538272, 0.955238966673205, 0.9517438177731673, 0.9605207611134582, 0.3309499718391723, 0.9276740299125427, 0.9527693371729435, 0.19851245367485215, 0.9703630868310577, 0.9820716068309587, 0.28212193068187397, 0.9338615516716566, 0.9662657564697303, 0.9412290362738736, 0.8705468770435071, 0.9641190239871924, 0.24453842734422537, 0.5128911510079939]}}
{"id": "92fde212-48e5-4c3a-8f35-b6d13dc35725", "fitness": 0.7653635265531601, "name": "MirroredCMAES", "description": "CMA-ES with a mirrored sampling strategy to enhance exploration and a simplified restart mechanism based on function value comparison.", "code": "import numpy as np\n\nclass MirroredCMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, restart_factor=3):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.normC = 0\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2.3)**2 + self.mueff))\n        self.lb = -5\n        self.ub = 5\n        self.decay = 0.9995  # Decay factor for sigma\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 50 # Number of iterations without improvement to trigger restart\n        self.restart_factor = restart_factor\n        self.archive_f = []\n        self.archive_x = []\n        self.mirror_factor = 0.25 # Probability of mirroring\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.stagnation_counter = 0\n        self.archive_f = []\n        self.archive_x = []\n        \n        while self.count_evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z\n\n            # Mirrored sampling\n            for i in range(self.popsize):\n                if np.random.rand() < self.mirror_factor:\n                    x[:, i] = self.mean + (self.mean - x[:, i])  # Mirror around the mean\n            \n            x = np.clip(x, self.lb, self.ub)\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.count_evals += self.popsize\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n                self.stagnation_counter = 0\n            else:\n                self.stagnation_counter += 1\n\n            idx = np.argsort(f)\n            x = x[:, idx]\n\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n            \n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt.T, (xmean - self.mean) / self.sigma)\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.count_evals / self.popsize))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (xmean - self.mean) / self.sigma\n            \n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n            self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * np.outer(self.pc, self.pc) + self.cmu * artmp @ np.diag(self.weights) @ artmp.T\n            \n            self.mean = xmean\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            self.sigma = max(self.sigma, 1e-10)\n            self.sigma *= self.decay  # Decay sigma\n\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-6 * np.eye(self.dim)\n            \n            if np.max(np.diag(self.C)) > (1e7 * self.sigma):\n                self.sigma *= 0.1\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n                self.stagnation_counter = 0\n            \n            # Simplified Restart: if no improvement for a while, just restart.\n            if self.stagnation_counter > self.stagnation_threshold:\n                if len(self.archive_f) > 5 and self.f_opt > np.min(self.archive_f):\n                    self.mean = self.archive_x[np.argmin(self.archive_f)]\n                else:\n                     self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n                self.sigma *= self.restart_factor\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n\n                self.stagnation_counter = 0\n\n            self.archive_f.append(self.f_opt)\n            self.archive_x.append(self.x_opt)\n            if len(self.archive_f) > 10:\n                self.archive_f.pop(0)\n                self.archive_x.pop(0)\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm MirroredCMAES scored 0.765 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6434aa25-fb32-47d5-abe1-f71f586008bc"], "operator": null, "metadata": {"aucs": [0.34738962412200103, 0.8737069746668111, 0.9513528772963562, 0.9633408684868772, 0.9520645187173139, 0.9574332362892379, 0.32958804847272505, 0.9417683841265019, 0.945936099633796, 0.8031094038056117, 0.9625413785843239, 0.9951119887017149, 0.2749538304583198, 0.9378353455002449, 0.9007060445686238, 0.8686734693061806, 0.6232672217321127, 0.9589107267568084, 0.21721615530911764, 0.5023643345285218]}}
{"id": "dc6563bb-cbad-410e-b3fa-1d80aa4b52cb", "fitness": 0.6813093322060932, "name": "AdaptiveDifferentialEvolutionAgingRestart", "description": "Adaptive Differential Evolution with aging and restart mechanism based on population diversity.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolutionAgingRestart:\n    def __init__(self, budget=10000, dim=10, popsize=None, aging_limit=50):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 10 * self.dim\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9  # Crossover rate\n        self.population = np.random.uniform(-5, 5, size=(self.popsize, self.dim))\n        self.fitness = np.array([np.inf] * self.popsize)\n        self.eval_count = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.aging_limit = aging_limit\n        self.age = np.zeros(self.popsize)\n\n    def __call__(self, func):\n        while self.eval_count < self.budget:\n            for i in range(self.popsize):\n                # Mutation\n                idxs = np.random.choice(self.popsize, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[idxs]\n                x_mutated = x_r1 + self.F * (x_r2 - x_r3)\n                x_mutated = np.clip(x_mutated, -5, 5)\n\n                # Crossover\n                x_trial = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Selection\n                f_trial = func(x_trial)\n                self.eval_count += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = x_trial\n                    self.age[i] = 0  # Reset age\n                    \n                    # Simplified adaptive parameter update\n                    if np.random.rand() < 0.1:  # Update F and CR with a probability\n                        self.F = np.clip(np.random.normal(0.5, 0.3), 0.1, 0.9)\n                        self.CR = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0)\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n                else:\n                    self.age[i] += 1\n\n                if self.eval_count >= self.budget:\n                    break\n            \n            # Aging-based restart mechanism\n            if np.any(self.age > self.aging_limit):\n                # Calculate population diversity\n                diversity = np.std(self.population)\n                if diversity < 0.1:  # If diversity is low, restart\n                    # Restart the population\n                    self.population = np.random.uniform(-5, 5, size=(self.popsize, self.dim))\n                    self.fitness = np.array([np.inf] * self.popsize)\n                    self.age = np.zeros(self.popsize)\n                else:\n                    # Replace old individuals with new random ones\n                    num_to_replace = np.sum(self.age > self.aging_limit)\n                    replace_indices = np.argsort(self.age)[-num_to_replace:] # Replace oldest ones\n\n                    self.population[replace_indices] = np.random.uniform(-5, 5, size=(num_to_replace, self.dim))\n                    self.fitness[replace_indices] = np.array([np.inf] * num_to_replace)\n                    self.age[replace_indices] = 0\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDifferentialEvolutionAgingRestart scored 0.681 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0002a00b-c391-41de-8711-07a8836bc85a"], "operator": null, "metadata": {"aucs": [0.2926853420844945, 0.6187414466607121, 0.6450232058763262, 0.8705444727060747, 0.7248592968411305, 0.7503005801354081, 0.6475133197116497, 0.6076270375017597, 0.7043272702416132, 0.6474567884721665, 0.8347561213391883, 0.9992554207818702, 0.6413603601941122, 0.745772918582239, 0.9186226791066944, 0.790412732649935, 0.5665182090736169, 0.8212681928954391, 0.2873545674105168, 0.5117866818569181]}}
{"id": "70585178-ba87-4586-b662-323072a7c5d0", "fitness": 0.7155300383376685, "name": "AdaptiveCMAESNiching", "description": "An adaptive CMA-ES variant that dynamically adjusts the covariance matrix learning rate based on the success rate of improving solutions and incorporates a niching strategy to promote diversity by penalizing solutions that are too close to each other.", "code": "import numpy as np\n\nclass AdaptiveCMAESNiching:\n    def __init__(self, budget=10000, dim=10, popsize=None, initial_sigma=0.5, niching_radius=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2.3)**2 + self.mueff))\n        self.mean = np.zeros(self.dim)\n        self.sigma = initial_sigma\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.lb = -5\n        self.ub = 5\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.count_evals = 0\n        self.success_rate = 0.5\n        self.learning_rate_cmu = self.cmu # adaptive learning rate for cmu\n        self.niching_radius = niching_radius # radius for niching\n        self.min_sigma = 1e-10\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n\n        while self.count_evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z\n            x = np.clip(x, self.lb, self.ub)\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n\n            # Niching: Penalize solutions too close to each other\n            for i in range(self.popsize):\n                for j in range(i + 1, self.popsize):\n                    if np.linalg.norm(x[:, i] - x[:, j]) < self.niching_radius:\n                        f[i] = f[i] + 0.1*abs(f[i])\n                        f[j] = f[j] + 0.1*abs(f[j])\n\n            self.count_evals += self.popsize\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n\n            idx = np.argsort(f)\n            x = x[:, idx]\n            f = f[idx]\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n\n            # Adaptive CMA-ES part\n            delta_f = self.f_opt - f[0] # improvement\n            if delta_f > 0:\n                self.success_rate = 0.9 * self.success_rate + 0.1 # update success rate\n                self.learning_rate_cmu = min(self.cmu * np.exp(self.success_rate - 0.2), 0.5) #Adapt learning rate\n            else:\n                self.success_rate = 0.9 * self.success_rate\n                self.learning_rate_cmu = max(self.cmu * np.exp(self.success_rate - 0.2), 0.01)\n\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt.T, (xmean - self.mean) / self.sigma)\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.count_evals / self.popsize))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (xmean - self.mean) / self.sigma\n            \n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n            self.C = (1 - self.c1 - self.learning_rate_cmu) * self.C + self.c1 * np.outer(self.pc, self.pc) + self.learning_rate_cmu * artmp @ np.diag(self.weights) @ artmp.T\n            \n            self.mean = xmean\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            self.sigma = max(self.sigma, self.min_sigma)\n\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-6 * np.eye(self.dim)\n\n            if np.max(np.diag(self.C)) > (1e7 * self.sigma):\n                self.sigma *= 0.1\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n                self.learning_rate_cmu = self.cmu # Reset learning rate\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveCMAESNiching scored 0.716 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["da43c200-c783-424f-b9a5-1e4a642b1980"], "operator": null, "metadata": {"aucs": [0.2647797063303068, 0.8926688328037162, 0.9567815901666109, 0.1850765196509091, 0.958858641070062, 0.9531609835764537, 0.35766148498268824, 0.9275700109065392, 0.9510457017589315, 0.23291112678440573, 0.9542372457281559, 0.9943646142958714, 0.2625780444794428, 0.922907315622158, 0.897765473059734, 0.9396871047784959, 0.8894414386836287, 0.9644955381839335, 0.26133670038765366, 0.5432726935036681]}}
{"id": "36189da2-7b65-4695-8617-53b56418d019", "fitness": 0.7331756299311065, "name": "CMAES", "description": "CMA-ES with adaptive population sizing, sigma adaptation based on success rate, and a more robust restart strategy that considers both stagnation and variance collapse.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, restart_factor=3, initial_sigma=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.normC = 0\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = initial_sigma\n        self.initial_sigma = initial_sigma\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2.3)**2 + self.mueff))\n        self.lb = -5\n        self.ub = 5\n        self.decay = 0.9995  # Decay factor for sigma\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 50 # Number of iterations without improvement to trigger restart\n        self.restart_factor = restart_factor\n        self.archive_f = []\n        self.archive_x = []\n        self.success_rate = 0.0\n        self.success_history = []\n        self.adapt_popsize = True\n        self.min_popsize = 4\n        self.max_popsize = 100\n        self.variance_threshold = 1e-12\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = self.initial_sigma\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.stagnation_counter = 0\n        self.archive_f = []\n        self.archive_x = []\n        self.success_rate = 0.0\n        self.success_history = []\n        \n        while self.count_evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z\n            x = np.clip(x, self.lb, self.ub)\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.count_evals += self.popsize\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n                self.stagnation_counter = 0\n            else:\n                self.stagnation_counter += 1\n\n            idx = np.argsort(f)\n            x = x[:, idx]\n\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n            \n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt.T, (xmean - self.mean) / self.sigma)\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.count_evals / self.popsize))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (xmean - self.mean) / self.sigma\n            \n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n            self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * np.outer(self.pc, self.pc) + self.cmu * artmp @ np.diag(self.weights) @ artmp.T\n            \n            self.mean = xmean\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n\n            # Adapt sigma based on success rate\n            successful_individuals = np.sum(f < np.median(f))\n            self.success_rate = successful_individuals / self.popsize\n            self.success_history.append(self.success_rate)\n            \n            if len(self.success_history) > 10:\n                self.success_history = self.success_history[-10:]\n                avg_success_rate = np.mean(self.success_history)\n                if avg_success_rate > 0.7:\n                    self.sigma *= 1.2  # Increase sigma if success rate is high\n                elif avg_success_rate < 0.2:\n                    self.sigma *= 0.8  # Decrease sigma if success rate is low\n            \n            self.sigma = max(self.sigma, 1e-10)\n            self.sigma *= self.decay  # Decay sigma\n\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-6 * np.eye(self.dim)\n            \n            if np.max(np.diag(self.C)) > (1e7 * self.sigma):\n                self.sigma *= 0.1\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n                self.stagnation_counter = 0\n            \n            # Robust restart strategy\n            if self.stagnation_counter > self.stagnation_threshold or np.min(np.diag(self.C)) < self.variance_threshold:\n                self.sigma *= self.restart_factor\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n                self.stagnation_counter = 0\n            \n            # Adaptive Population Sizing\n            if self.adapt_popsize:\n                if self.success_rate > 0.6 and self.popsize > self.min_popsize:\n                    self.popsize = max(self.min_popsize, int(self.popsize * 0.95))\n                    self.mu = self.popsize // 2\n                elif self.success_rate < 0.3 and self.popsize < self.max_popsize:\n                    self.popsize = min(self.max_popsize, int(self.popsize * 1.05))\n                    self.mu = self.popsize // 2\n                self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n                self.weights = self.weights / np.sum(self.weights)\n                self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n                self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n                self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2.3)**2 + self.mueff))\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm CMAES scored 0.733 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6434aa25-fb32-47d5-abe1-f71f586008bc"], "operator": null, "metadata": {"aucs": [0.26682901750646926, 0.7897887753995296, 0.9452912195841352, 0.9613136237738358, 0.9547843775902717, 0.9576974936183232, 0.3414446209795573, 0.9348797307492527, 0.9464823352638617, 0.5272178230775879, 0.9472992091629184, 0.9963961000742766, 0.2940675658512031, 0.9358966201392332, 0.8184668838891715, 0.9189504427093933, 0.4588969338736061, 0.9698418004048146, 0.20075042607046623, 0.4972175989042249]}}
{"id": "94565cc8-2ffa-4a39-91e7-f52ac52027af", "fitness": 0.4461357272523757, "name": "AdaptiveDifferentialEvolution", "description": "Adaptive Differential Evolution with simplified parameter adaptation and local search to refine promising solutions.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, popsize=None):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 10 * self.dim\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.9 # Crossover rate\n        self.population = np.random.uniform(-5, 5, size=(self.popsize, self.dim))\n        self.fitness = np.array([np.inf] * self.popsize)\n        self.eval_count = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def local_search(self, func, x, f_x, step_size=0.1, num_steps=5):\n        \"\"\"Performs a simple local search around a solution.\"\"\"\n        x_best = x\n        f_best = f_x\n\n        for _ in range(num_steps):\n            x_new = x + np.random.uniform(-step_size, step_size, size=self.dim)\n            x_new = np.clip(x_new, -5, 5)\n            f_new = func(x_new)\n            self.eval_count += 1\n\n            if f_new < f_best:\n                f_best = f_new\n                x_best = x_new\n            \n            if self.eval_count >= self.budget:\n                break\n        return x_best, f_best\n\n    def __call__(self, func):\n        while self.eval_count < self.budget:\n            for i in range(self.popsize):\n                # Mutation\n                idxs = np.random.choice(self.popsize, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[idxs]\n                x_mutated = x_r1 + self.F * (x_r2 - x_r3)\n                x_mutated = np.clip(x_mutated, -5, 5)\n\n                # Crossover\n                x_trial = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Selection\n                f_trial = func(x_trial)\n                self.eval_count += 1\n\n                if f_trial < self.fitness[i]:\n                    # Local Search for promising solutions\n                    x_trial, f_trial = self.local_search(func, x_trial, f_trial)\n\n                    self.fitness[i] = f_trial\n                    self.population[i] = x_trial\n\n                    # Simplified adaptive parameter update\n                    self.F = 0.5 + np.random.normal(0, 0.1)\n                    self.CR = 0.9 + np.random.normal(0, 0.05)\n                    self.F = np.clip(self.F, 0.1, 0.9)\n                    self.CR = np.clip(self.CR, 0.1, 1.0)\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = x_trial\n\n                if self.eval_count >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.446 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0002a00b-c391-41de-8711-07a8836bc85a"], "operator": null, "metadata": {"aucs": [0.19892099926456774, 0.3997151209197155, 0.44655550751278783, 0.6957469445924851, 0.4295877650576331, 0.5187456447777337, 0.3484921659923096, 0.3901229836876884, 0.4112064197626334, 0.28465898793691147, 0.6939641873730409, 0.9820477274033782, 0]}}
{"id": "dfec2c5e-5495-4e6e-bb29-160f64d9b595", "fitness": 0.730386533759162, "name": "CMAES", "description": "CMA-ES with dynamic parameter adaptation based on optimization progress, early restarts on stagnation, and a spectral correction of the covariance matrix to improve exploration and exploitation.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.lb = -5\n        self.ub = 5\n        self.c_learn_rate = 0.2\n        self.sigma_decay = 0.9995\n        self.tol_x = 1e-12 * (self.ub - self.lb)\n        self.tol_fun = 1e-12\n        self.stagnation_counter = 0\n        self.max_stagnation = 50\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.stagnation_counter = 0\n        \n        while self.count_evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z\n            x = np.clip(x, self.lb, self.ub)\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.count_evals += self.popsize\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n                self.stagnation_counter = 0  # Reset stagnation counter\n            else:\n                self.stagnation_counter += 1\n\n            idx = np.argsort(f)\n            x = x[:, idx]\n\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n            \n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt.T, (xmean - self.mean) / self.sigma)\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.count_evals / self.popsize))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (xmean - self.mean) / self.sigma\n            \n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n\n            c1 = self.c_learn_rate / (self.dim**2)\n            cmu = self.c_learn_rate / self.dim\n            \n            self.C = (1 - c1 - cmu) * self.C + c1 * np.outer(self.pc, self.pc) + cmu * artmp @ np.diag(self.weights) @ artmp.T\n            \n            self.mean = xmean\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            self.sigma = max(self.sigma, 1e-10)\n            self.sigma *= self.sigma_decay\n\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                # Spectral correction\n                d, Q = np.linalg.eigh(self.C)\n                d = np.maximum(d, 1e-6)\n                self.C = Q @ np.diag(d) @ Q.T\n\n            if np.max(np.diag(self.C)) > (1e7 * self.sigma):\n                self.sigma *= 0.1\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n                self.stagnation_counter = 0\n                \n            if self.stagnation_counter > self.max_stagnation:\n                self.sigma *= 0.5  # Reduce step size\n                self.mean = self.x_opt + np.random.normal(0, 0.1, self.dim) # Local restart\n                self.mean = np.clip(self.mean, self.lb, self.ub)\n                self.stagnation_counter = 0\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm CMAES scored 0.730 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f7b848e0-c9d4-4bd4-8d74-91594e781bd5"], "operator": null, "metadata": {"aucs": [0.39245874328901076, 0.8858195066317954, 0.9526621778394204, 0.9708923580745042, 0.9518143623192165, 0.9528652058148647, 0.31302873740277803, 0.9306118212998227, 0.94248981978294, 0.1981776752915434, 0.960049732095847, 0.991707973512063, 0.26573100800342464, 0.9427057291952575, 0.974564102597098, 0.7213347961661079, 0.5451880810279774, 0.969889857088046, 0.23581873937784814, 0.5099202483736763]}}
{"id": "5618939f-52fa-49d0-bd80-e124a41660e5", "fitness": 0.0, "name": "AdaptiveMirroredCMAES", "description": "Simplified CMA-ES with mirrored sampling, adaptive decay, and dynamic population size adjustment based on stagnation.", "code": "import numpy as np\n\nclass AdaptiveMirroredCMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2.3)**2 + self.mueff))\n        self.lb = -5\n        self.ub = 5\n        self.decay = 0.9995  # Decay factor for sigma\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 50\n        self.mirror_factor = 0.25\n        self.popsize_factor = 1.0\n        self.popsize_min = 4 + int(3 * np.log(self.dim))\n        self.popsize_max = 20 + int(6 * np.log(self.dim))\n        self.adaptive_popsize = True # Enable adaptive popsize\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.stagnation_counter = 0\n        \n        while self.count_evals < self.budget:\n            current_popsize = int(self.popsize * self.popsize_factor)\n            current_popsize = max(self.popsize_min, min(current_popsize, self.popsize_max))\n\n            z = np.random.normal(0, 1, size=(self.dim, current_popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z\n\n            # Mirrored sampling\n            for i in range(current_popsize):\n                if np.random.rand() < self.mirror_factor:\n                    x[:, i] = self.mean + (self.mean - x[:, i])  # Mirror around the mean\n            \n            x = np.clip(x, self.lb, self.ub)\n            f = np.array([func(x[:, i]) for i in range(current_popsize)])\n            self.count_evals += current_popsize\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n                self.stagnation_counter = 0\n            else:\n                self.stagnation_counter += 1\n\n            idx = np.argsort(f)\n            x = x[:, idx]\n\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n            \n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt.T, (xmean - self.mean) / self.sigma)\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.count_evals / current_popsize))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (xmean - self.mean) / self.sigma\n            \n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n            self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * np.outer(self.pc, self.pc) + self.cmu * artmp @ np.diag(self.weights) @ artmp.T\n            \n            self.mean = xmean\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            self.sigma = max(self.sigma, 1e-10)\n            self.sigma *= self.decay\n\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-6 * np.eye(self.dim)\n            \n            if np.max(np.diag(self.C)) > (1e7 * self.sigma):\n                self.sigma *= 0.1\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n                self.stagnation_counter = 0\n\n            # Adaptive Pop Size\n            if self.adaptive_popsize:\n                if self.stagnation_counter > self.stagnation_threshold:\n                    self.popsize_factor *= 0.9 # reduce popsize if stagnation\n                    self.stagnation_counter = 0\n                else:\n                    self.popsize_factor *= 1.01 # increase popsize if improving\n                self.popsize_factor = np.clip(self.popsize_factor, 0.5, 2.0)\n                self.sigma *= 1.05\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveMirroredCMAES scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["92fde212-48e5-4c3a-8f35-b6d13dc35725"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "0b9afae7-dec0-4112-a4b0-04654864eabe", "fitness": -Infinity, "name": "AdaptiveOrthogonalCMAES", "description": "Adaptive CMA-ES with orthogonal sampling to reduce correlation effects and adaptive population size based on function landscape characteristics.", "code": "import numpy as np\n\nclass AdaptiveOrthogonalCMAES:\n    def __init__(self, budget=10000, dim=10, initial_popsize=None):\n        self.budget = budget\n        self.dim = dim\n        self.initial_popsize = initial_popsize if initial_popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.popsize = self.initial_popsize\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.lb = -5\n        self.ub = 5\n        self.c_learn_rate = 0.2\n        self.sigma_decay = 0.9995\n        self.tol_x = 1e-12 * (self.ub - self.lb)\n        self.tol_fun = 1e-12\n        self.stagnation_counter = 0\n        self.max_stagnation = 50\n        self.adapt_popsize_interval = 10  # Adjust population size every n iterations\n        self.iteration = 0\n        self.min_popsize = 4\n        self.max_popsize = 50\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.stagnation_counter = 0\n        self.popsize = self.initial_popsize\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.iteration = 0\n        \n        while self.count_evals < self.budget:\n            self.iteration += 1\n            # Orthogonal sampling\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            Q, _ = np.linalg.qr(z)  # Orthogonal basis\n\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ Q\n            x = np.clip(x, self.lb, self.ub)\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.count_evals += self.popsize\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n                self.stagnation_counter = 0  # Reset stagnation counter\n            else:\n                self.stagnation_counter += 1\n\n            idx = np.argsort(f)\n            x = x[:, idx]\n\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n            \n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt.T, (xmean - self.mean) / self.sigma)\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.count_evals / self.popsize))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (xmean - self.mean) / self.sigma\n            \n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n\n            c1 = self.c_learn_rate / (self.dim**2)\n            cmu = self.c_learn_rate / self.dim\n            \n            self.C = (1 - c1 - cmu) * self.C + c1 * np.outer(self.pc, self.pc) + cmu * artmp @ np.diag(self.weights) @ artmp.T\n            \n            self.mean = xmean\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            self.sigma = max(self.sigma, 1e-10)\n            self.sigma *= self.sigma_decay\n\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                # Spectral correction\n                d, Q = np.linalg.eigh(self.C)\n                d = np.maximum(d, 1e-6)\n                self.C = Q @ np.diag(d) @ Q.T\n\n            if np.max(np.diag(self.C)) > (1e7 * self.sigma):\n                self.sigma *= 0.1\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n                self.stagnation_counter = 0\n                \n            if self.stagnation_counter > self.max_stagnation:\n                self.sigma *= 0.5  # Reduce step size\n                self.mean = self.x_opt + np.random.normal(0, 0.1, self.dim) # Local restart\n                self.mean = np.clip(self.mean, self.lb, self.ub)\n                self.stagnation_counter = 0\n            \n            # Adapt population size\n            if self.iteration % self.adapt_popsize_interval == 0:\n                delta_f = np.std(f)  # Diversity in function values\n                if delta_f < self.tol_fun:\n                    self.popsize = max(self.min_popsize, self.popsize // 2)  # Reduce popsize\n                else:\n                    self.popsize = min(self.max_popsize, self.popsize * 2)  # Increase popsize\n                \n                self.mu = self.popsize // 2\n                self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n                self.weights = self.weights / np.sum(self.weights)\n                self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "An exception occurred: index 2 is out of bounds for axis 1 with size 2.", "error": "", "parent_ids": ["dfec2c5e-5495-4e6e-bb29-160f64d9b595"], "operator": null, "metadata": {}}
{"id": "19f5692b-e61d-47bb-bf6c-e41286483e66", "fitness": -Infinity, "name": "AdaptiveCMAES", "description": "Adaptive CMA-ES with orthogonal sampling, dynamic covariance matrix adaptation based on fitness differences, and a clustering-based restart strategy.", "code": "import numpy as np\nfrom sklearn.cluster import MiniBatchKMeans\n\nclass AdaptiveCMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.lb = -5\n        self.ub = 5\n        self.c_learn_rate = 0.2\n        self.sigma_decay = 0.9995\n        self.tol_x = 1e-12 * (self.ub - self.lb)\n        self.tol_fun = 1e-12\n        self.stagnation_counter = 0\n        self.max_stagnation = 50\n        self.archive_x = []\n        self.archive_f = []\n        self.clustering_frequency = 1000 # Adjust this\n        self.num_clusters = 5\n\n    def orthogonal_sampling(self, num_samples):\n        H = np.random.normal(0, 1, size=(self.dim, num_samples))\n        Q, R = np.linalg.qr(H)\n        return Q\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.stagnation_counter = 0\n        self.archive_x = []\n        self.archive_f = []\n\n        while self.count_evals < self.budget:\n            # Orthogonal Sampling\n            z = self.orthogonal_sampling(self.popsize)\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z\n            x = np.clip(x, self.lb, self.ub)\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.count_evals += self.popsize\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n                self.stagnation_counter = 0  # Reset stagnation counter\n            else:\n                self.stagnation_counter += 1\n\n            idx = np.argsort(f)\n            x = x[:, idx]\n            f = f[idx]\n\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n\n            # Dynamic C update based on fitness differences\n            df = f[0] - f  # Fitness difference relative to best\n            df = df / np.std(f) if np.std(f) > 0 else df # Normalize\n            cmu_adaptive = self.c_learn_rate / self.dim * np.exp(-df[:self.mu]**2) # exp scaling based on fitness diff.\n            \n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt.T, (xmean - self.mean) / self.sigma)\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.count_evals / self.popsize))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (xmean - self.mean) / self.sigma\n            \n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n            c1 = self.c_learn_rate / (self.dim**2)\n\n            self.C = (1 - c1 - np.sum(cmu_adaptive)) * self.C + c1 * np.outer(self.pc, self.pc) + artmp @ np.diag(cmu_adaptive) @ artmp.T\n            \n            self.mean = xmean\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            self.sigma = max(self.sigma, 1e-10)\n            self.sigma *= self.sigma_decay\n\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                # Spectral correction\n                d, Q = np.linalg.eigh(self.C)\n                d = np.maximum(d, 1e-6)\n                self.C = Q @ np.diag(d) @ Q.T\n\n            if np.max(np.diag(self.C)) > (1e7 * self.sigma):\n                self.sigma *= 0.1\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n                self.stagnation_counter = 0\n                \n            if self.stagnation_counter > self.max_stagnation:\n                self.sigma *= 0.5  # Reduce step size\n                self.mean = self.x_opt + np.random.normal(0, 0.1, self.dim) # Local restart\n                self.mean = np.clip(self.mean, self.lb, self.ub)\n                self.stagnation_counter = 0\n\n            # Archive the data\n            for i in range(self.popsize):\n              self.archive_x.append(x[:, i])\n              self.archive_f.append(f[i])\n\n            # Clustering-based restart strategy\n            if self.count_evals % self.clustering_frequency == 0 and len(self.archive_x) > self.num_clusters:\n                try:\n                  kmeans = MiniBatchKMeans(n_clusters=self.num_clusters, random_state=0, n_init='auto').fit(np.array(self.archive_x))\n                  cluster_labels = kmeans.labels_\n                  cluster_centers = kmeans.cluster_centers_\n\n                  # Select the cluster with the best average fitness\n                  cluster_fitnesses = []\n                  for i in range(self.num_clusters):\n                      cluster_fitnesses.append(np.mean([self.archive_f[j] for j in range(len(self.archive_f)) if cluster_labels[j] == i]))\n                  best_cluster_index = np.argmin(cluster_fitnesses)\n\n                  # Restart from the best cluster center with reduced sigma\n                  self.mean = cluster_centers[best_cluster_index]\n                  self.mean = np.clip(self.mean, self.lb, self.ub)\n                  self.sigma *= 0.75\n                  self.C = np.eye(self.dim)\n                  self.pc = np.zeros(self.dim)\n                  self.ps = np.zeros(self.dim)\n                  self.stagnation_counter = 0\n                except:\n                   pass\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "An exception occurred: index 2 is out of bounds for axis 1 with size 2.", "error": "", "parent_ids": ["dfec2c5e-5495-4e6e-bb29-160f64d9b595"], "operator": null, "metadata": {}}
{"id": "8fc992ba-5f1f-44b1-a122-b97eb66b7ae0", "fitness": -Infinity, "name": "CMAES_OrthogonalMirror", "description": "CMA-ES with orthogonal sampling and covariance matrix adaptation using a rank-one update strategy combined with a mirrored sampling scheme for enhanced exploration.", "code": "import numpy as np\n\nclass CMAES_OrthogonalMirror:\n    def __init__(self, budget=10000, dim=10, popsize=None, restart_factor=3, initial_sigma=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.normC = 0\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = initial_sigma\n        self.initial_sigma = initial_sigma\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.c1 = 1 / ((self.dim + 1.3)**2 + self.mueff) # Rank-one update\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2.3)**2 + self.mueff))\n        self.lb = -5\n        self.ub = 5\n        self.decay = 0.9995  # Decay factor for sigma\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 50 # Number of iterations without improvement to trigger restart\n        self.restart_factor = restart_factor\n        self.archive_f = []\n        self.archive_x = []\n        self.success_rate = 0.0\n        self.success_history = []\n        self.adapt_popsize = True\n        self.min_popsize = 4\n        self.max_popsize = 100\n        self.variance_threshold = 1e-12\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = self.initial_sigma\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.stagnation_counter = 0\n        self.archive_f = []\n        self.archive_x = []\n        self.success_rate = 0.0\n        self.success_history = []\n        \n        while self.count_evals < self.budget:\n            # Generate orthogonal sample\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            Q, _ = np.linalg.qr(z)\n\n            # Generate mirrored sample\n            z_mirrored = -Q\n\n            # Combine original and mirrored samples\n            z_combined = np.concatenate((Q, z_mirrored), axis=1)\n            \n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z_combined\n            x = np.clip(x, self.lb, self.ub)\n            \n            f = np.array([func(x[:, i]) for i in range(2 * self.popsize)])\n            self.count_evals += 2 * self.popsize\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n                self.stagnation_counter = 0\n            else:\n                self.stagnation_counter += 1\n\n            idx = np.argsort(f)\n            x = x[:, idx]\n\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n            \n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt.T, (xmean - self.mean) / self.sigma)\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.count_evals / (2 * self.popsize)))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (xmean - self.mean) / self.sigma\n            \n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n            self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * np.outer(self.pc, self.pc) + self.cmu * artmp @ np.diag(self.weights) @ artmp.T\n            \n            self.mean = xmean\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n\n            # Adapt sigma based on success rate\n            successful_individuals = np.sum(f < np.median(f))\n            self.success_rate = successful_individuals / (2 * self.popsize)\n            self.success_history.append(self.success_rate)\n            \n            if len(self.success_history) > 10:\n                self.success_history = self.success_history[-10:]\n                avg_success_rate = np.mean(self.success_history)\n                if avg_success_rate > 0.7:\n                    self.sigma *= 1.2  # Increase sigma if success rate is high\n                elif avg_success_rate < 0.2:\n                    self.sigma *= 0.8  # Decrease sigma if success rate is low\n            \n            self.sigma = max(self.sigma, 1e-10)\n            self.sigma *= self.decay  # Decay sigma\n\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-6 * np.eye(self.dim)\n            \n            if np.max(np.diag(self.C)) > (1e7 * self.sigma):\n                self.sigma *= 0.1\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n                self.stagnation_counter = 0\n            \n            # Robust restart strategy\n            if self.stagnation_counter > self.stagnation_threshold or np.min(np.diag(self.C)) < self.variance_threshold:\n                self.sigma *= self.restart_factor\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n                self.stagnation_counter = 0\n            \n            # Adaptive Population Sizing\n            if self.adapt_popsize:\n                if self.success_rate > 0.6 and self.popsize > self.min_popsize:\n                    self.popsize = max(self.min_popsize, int(self.popsize * 0.95))\n                    self.mu = self.popsize // 2\n                elif self.success_rate < 0.3 and self.popsize < self.max_popsize:\n                    self.popsize = min(self.max_popsize, int(self.popsize * 1.05))\n                    self.mu = self.popsize // 2\n                self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n                self.weights = self.weights / np.sum(self.weights)\n                self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n                self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n                self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2.3)**2 + self.mueff))\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "An exception occurred: index 4 is out of bounds for axis 1 with size 4.", "error": "", "parent_ids": ["36189da2-7b65-4695-8617-53b56418d019"], "operator": null, "metadata": {}}
{"id": "28ca8907-33bf-4df5-96ea-855c7c43af8e", "fitness": -Infinity, "name": "OrthogonalCMAES", "description": "CMA-ES with orthogonal sampling for improved exploration, a simplified covariance matrix update, and adaptive bounds contraction to accelerate convergence.", "code": "import numpy as np\n\nclass OrthogonalCMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, initial_sigma=0.5, restart_factor=3):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.normC = 0\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = initial_sigma\n        self.initial_sigma = initial_sigma\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2.3)**2 + self.mueff))\n        self.lb = -5\n        self.ub = 5\n        self.decay = 0.9995  # Decay factor for sigma\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 50 # Number of iterations without improvement to trigger restart\n        self.restart_factor = restart_factor\n        self.archive_f = []\n        self.archive_x = []\n        self.success_rate = 0.0\n        self.success_history = []\n        self.adapt_popsize = True\n        self.min_popsize = 4\n        self.max_popsize = 100\n        self.variance_threshold = 1e-12\n        self.bound_contraction_rate = 0.995\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = self.initial_sigma\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.stagnation_counter = 0\n        self.archive_f = []\n        self.archive_x = []\n        self.success_rate = 0.0\n        self.success_history = []\n        \n        while self.count_evals < self.budget:\n            # Generate orthogonal samples\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            Q, _ = np.linalg.qr(z)\n            z = Q\n            \n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z\n            x = np.clip(x, self.lb, self.ub)\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.count_evals += self.popsize\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n                self.stagnation_counter = 0\n            else:\n                self.stagnation_counter += 1\n\n            idx = np.argsort(f)\n            x = x[:, idx]\n\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n            \n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt.T, (xmean - self.mean) / self.sigma)\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.count_evals / self.popsize))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (xmean - self.mean) / self.sigma\n            \n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n            # Simplified rank-mu update\n            self.C = (1 - self.cmu) * self.C + self.cmu * artmp @ np.diag(self.weights) @ artmp.T\n            \n            self.mean = xmean\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n\n            # Adapt sigma based on success rate\n            successful_individuals = np.sum(f < np.median(f))\n            self.success_rate = successful_individuals / self.popsize\n            self.success_history.append(self.success_rate)\n            \n            if len(self.success_history) > 10:\n                self.success_history = self.success_history[-10:]\n                avg_success_rate = np.mean(self.success_history)\n                if avg_success_rate > 0.7:\n                    self.sigma *= 1.2  # Increase sigma if success rate is high\n                elif avg_success_rate < 0.2:\n                    self.sigma *= 0.8  # Decrease sigma if success rate is low\n            \n            self.sigma = max(self.sigma, 1e-10)\n            self.sigma *= self.decay  # Decay sigma\n\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-6 * np.eye(self.dim)\n            \n            if np.max(np.diag(self.C)) > (1e7 * self.sigma):\n                self.sigma *= 0.1\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n                self.stagnation_counter = 0\n\n            # Adaptive Bounds Contraction\n            if self.stagnation_counter > self.stagnation_threshold // 2:\n                self.lb = self.bound_contraction_rate * (self.lb - self.x_opt) + self.x_opt\n                self.ub = self.bound_contraction_rate * (self.ub - self.x_opt) + self.x_opt\n                self.lb = np.clip(self.lb, -5, 5)\n                self.ub = np.clip(self.ub, -5, 5)\n            \n            # Robust restart strategy\n            if self.stagnation_counter > self.stagnation_threshold or np.min(np.diag(self.C)) < self.variance_threshold:\n                self.sigma *= self.restart_factor\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n                self.stagnation_counter = 0\n            \n            # Adaptive Population Sizing\n            if self.adapt_popsize:\n                if self.success_rate > 0.6 and self.popsize > self.min_popsize:\n                    self.popsize = max(self.min_popsize, int(self.popsize * 0.95))\n                    self.mu = self.popsize // 2\n                elif self.success_rate < 0.3 and self.popsize < self.max_popsize:\n                    self.popsize = min(self.max_popsize, int(self.popsize * 1.05))\n                    self.mu = self.popsize // 2\n                self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n                self.weights = self.weights / np.sum(self.weights)\n                self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n                self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n                self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2.3)**2 + self.mueff))\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "An exception occurred: index 2 is out of bounds for axis 1 with size 2.", "error": "", "parent_ids": ["36189da2-7b65-4695-8617-53b56418d019"], "operator": null, "metadata": {}}
{"id": "66701f76-815c-4518-b55d-f7252cbf85cd", "fitness": -Infinity, "name": "OrthogonalCMAES", "description": "CMA-ES with orthogonal sampling for better exploration of the search space, combined with a dynamic covariance matrix adaptation based on the history of successful steps.", "code": "import numpy as np\n\nclass OrthogonalCMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, restart_factor=3, initial_sigma=0.5, orthogonal_samples=5):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.normC = 0\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = initial_sigma\n        self.initial_sigma = initial_sigma\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2.3)**2 + self.mueff))\n        self.lb = -5\n        self.ub = 5\n        self.decay = 0.9995  # Decay factor for sigma\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 50 # Number of iterations without improvement to trigger restart\n        self.restart_factor = restart_factor\n        self.archive_f = []\n        self.archive_x = []\n        self.success_rate = 0.0\n        self.success_history = []\n        self.adapt_popsize = True\n        self.min_popsize = 4\n        self.max_popsize = 100\n        self.variance_threshold = 1e-12\n        self.orthogonal_samples = orthogonal_samples\n        self.successful_steps = []\n        self.history_length = 10  # Length of the successful steps history\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = self.initial_sigma\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.stagnation_counter = 0\n        self.archive_f = []\n        self.archive_x = []\n        self.success_rate = 0.0\n        self.success_history = []\n        self.successful_steps = []\n\n        while self.count_evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            \n            # Orthogonal sampling\n            for i in range(self.orthogonal_samples):\n              Q, _ = np.linalg.qr(np.random.normal(0, 1, size=(self.dim, self.dim)))\n              z[:, i*self.dim//self.orthogonal_samples:(i+1)*self.dim//self.orthogonal_samples] = Q[:, :self.dim//self.orthogonal_samples]\n              \n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z\n            x = np.clip(x, self.lb, self.ub)\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.count_evals += self.popsize\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n                self.stagnation_counter = 0\n            else:\n                self.stagnation_counter += 1\n\n            idx = np.argsort(f)\n            x = x[:, idx]\n\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n            \n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt.T, (xmean - self.mean) / self.sigma)\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.count_evals / self.popsize))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (xmean - self.mean) / self.sigma\n            \n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n            self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * np.outer(self.pc, self.pc) + self.cmu * artmp @ np.diag(self.weights) @ artmp.T\n            \n            self.mean = xmean\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n\n            # Adapt sigma based on success rate\n            successful_individuals = np.sum(f < np.median(f))\n            self.success_rate = successful_individuals / self.popsize\n            self.success_history.append(self.success_rate)\n            \n            if len(self.success_history) > 10:\n                self.success_history = self.success_history[-10:]\n                avg_success_rate = np.mean(self.success_history)\n                if avg_success_rate > 0.7:\n                    self.sigma *= 1.2  # Increase sigma if success rate is high\n                elif avg_success_rate < 0.2:\n                    self.sigma *= 0.8  # Decrease sigma if success rate is low\n            \n            self.sigma = max(self.sigma, 1e-10)\n            self.sigma *= self.decay  # Decay sigma\n\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-6 * np.eye(self.dim)\n            \n            if np.max(np.diag(self.C)) > (1e7 * self.sigma):\n                self.sigma *= 0.1\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n                self.stagnation_counter = 0\n\n            # Store successful steps\n            if f[idx[0]] < np.median(f):\n                step = (x[:, idx[0]] - self.mean) / self.sigma\n                self.successful_steps.append(step)\n                if len(self.successful_steps) > self.history_length:\n                    self.successful_steps.pop(0)\n            \n            # Dynamic Covariance Matrix Adaptation\n            if len(self.successful_steps) > 2:\n                stacked_steps = np.stack(self.successful_steps, axis=1)\n                step_covariance = np.cov(stacked_steps)\n                self.C = (0.8 * self.C + 0.2 * step_covariance)  # Combine with current C\n\n            # Robust restart strategy\n            if self.stagnation_counter > self.stagnation_threshold or np.min(np.diag(self.C)) < self.variance_threshold:\n                self.sigma *= self.restart_factor\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n                self.stagnation_counter = 0\n            \n            # Adaptive Population Sizing\n            if self.adapt_popsize:\n                if self.success_rate > 0.6 and self.popsize > self.min_popsize:\n                    self.popsize = max(self.min_popsize, int(self.popsize * 0.95))\n                    self.mu = self.popsize // 2\n                elif self.success_rate < 0.3 and self.popsize < self.max_popsize:\n                    self.popsize = min(self.max_popsize, int(self.popsize * 1.05))\n                    self.mu = self.popsize // 2\n                self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n                self.weights = self.weights / np.sum(self.weights)\n                self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n                self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n                self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2.3)**2 + self.mueff))\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "An exception occurred: could not broadcast input array from shape (2,0) into shape (2,1).", "error": "", "parent_ids": ["36189da2-7b65-4695-8617-53b56418d019"], "operator": null, "metadata": {}}
{"id": "6a625e4e-4a5a-485c-8884-193944b212db", "fitness": 0.7406865181072193, "name": "CMAES_Mirror", "description": "CMA-ES with mirrored sampling, simplified covariance update, and adaptive learning rate based on the condition number of the covariance matrix to balance exploration and exploitation.", "code": "import numpy as np\n\nclass CMAES_Mirror:\n    def __init__(self, budget=10000, dim=10, popsize=None):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.lb = -5\n        self.ub = 5\n        self.evals = 0\n        self.c_learn_rate = 0.5 # Overall learning rate\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n\n        while self.evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            z_mirrored = -z\n            z_combined = np.concatenate([z, z_mirrored], axis=1)\n\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z_combined\n            x = np.clip(x, self.lb, self.ub)\n\n            f = np.array([func(x[:, i]) for i in range(2 * self.popsize)])\n            self.evals += 2 * self.popsize\n\n            if self.evals > self.budget:\n                f = f[:(self.budget - (self.evals - 2 * self.popsize))]\n                x = x[:, :(self.budget - (self.evals - 2 * self.popsize))]\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n\n            idx = np.argsort(f)\n            x = x[:, idx]\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt.T, (xmean - self.mean) / self.sigma)\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.evals / (2*self.popsize)))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (xmean - self.mean) / self.sigma\n\n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n            \n            # Adaptive learning rate based on condition number\n            condition_number = np.linalg.cond(self.C)\n            learn_rate = self.c_learn_rate / (1 + condition_number/1e5)  # Reduce learning when C is ill-conditioned\n            \n            c1 = learn_rate / (self.dim**2)\n            cmu = learn_rate / self.dim\n            \n            self.C = (1 - c1 - cmu) * self.C + c1 * np.outer(self.pc, self.pc) + cmu * artmp @ np.diag(self.weights) @ artmp.T\n\n            self.mean = xmean\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            self.sigma = max(self.sigma, 1e-10)\n\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C += 1e-6 * np.eye(self.dim)\n                \n            if np.max(np.diag(self.C)) > (1e7 * self.sigma):\n                self.sigma *= 0.1\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm CMAES_Mirror scored 0.741 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["fcc5ab1a-48a5-4347-851c-a14a73446292"], "operator": null, "metadata": {"aucs": [0.36677852396987287, 0.23428081454320793, 0.9377739443841017, 0.9608343615697379, 0.9349909275715984, 0.9450533785086441, 0.8450997711537613, 0.9256268508009825, 0.93806715730765, 0.5526747277327642, 0.9592357229002592, 0.9913062009411594, 0.25530070288636275, 0.9246074824971978, 0.9645295130579835, 0.8935121634449209, 0.3421551537233074, 0.9557520392569117, 0.3781907418848195, 0.5079601840091441]}}
{"id": "561120a4-4504-4bbd-8de4-012289879386", "fitness": 0.6952006033223006, "name": "CMAES_Mirror_AdaptiveSigma", "description": "CMA-ES with mirrored sampling, decaying learning rate, and adaptive sigma based on covariance matrix eigenvalues to avoid premature convergence.", "code": "import numpy as np\n\nclass CMAES_Mirror_AdaptiveSigma:\n    def __init__(self, budget=10000, dim=10, popsize=None):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.lb = -5\n        self.ub = 5\n        self.c_learn_rate = 0.5 # Overall learning rate for covariance matrix\n        self.min_sigma = 1e-10\n        self.evals = 0\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n\n        while self.evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            \n            # Mirrored sampling\n            z_mirrored = -z\n            z_combined = np.concatenate([z, z_mirrored], axis=1)\n            \n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z_combined\n            x = np.clip(x, self.lb, self.ub)\n            \n            f = np.array([func(x[:, i]) for i in range(2 * self.popsize)])\n            self.evals += 2 * self.popsize\n            \n            if self.evals > self.budget:\n                f = f[:(self.budget - (self.evals - 2 * self.popsize))]\n                x = x[:, :(self.budget - (self.evals - 2 * self.popsize))]\n                \n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n            \n            idx = np.argsort(f)\n            x = x[:, idx]\n            \n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n            \n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt.T, (xmean - self.mean) / self.sigma)\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.evals / (2*self.popsize)))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (xmean - self.mean) / self.sigma\n            \n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n            \n            # Evaluation based learning rate decay\n            learn_rate = self.c_learn_rate * (1 - (self.evals / self.budget))\n            \n            c1 = learn_rate / (self.dim**2)\n            cmu = learn_rate / self.dim\n            \n            self.C = (1 - c1 - cmu) * self.C + c1 * np.outer(self.pc, self.pc) + cmu * artmp @ np.diag(self.weights) @ artmp.T\n            \n            self.mean = xmean\n            \n            # Adaptive sigma based on covariance matrix eigenvalues\n            eigenvalues = np.linalg.eigvalsh(self.C)\n            condition_number = np.max(eigenvalues) / np.min(eigenvalues)\n            if condition_number > 1e4:\n                self.sigma *= 0.8  # Reduce sigma if condition number is high\n            else:\n                self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n                \n            self.sigma = max(self.sigma, self.min_sigma) # avoid sigma getting too small\n\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-6 * np.eye(self.dim)\n                \n            if np.max(np.diag(self.C)) > (1e7 * self.sigma):\n                self.sigma *= 0.1\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm CMAES_Mirror_AdaptiveSigma scored 0.695 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["fcc5ab1a-48a5-4347-851c-a14a73446292"], "operator": null, "metadata": {"aucs": [0.312056863453538, 0.7553331944811053, 0.9432445970332766, 0.17242427408315086, 0.9301195279605853, 0.9428948812753544, 0.3245325296131931, 0.9170005196177351, 0.9371057331854302, 0.16332870551640988, 0.969028199339247, 0.9920618947278521, 0.265704763766886, 0.9358366868540304, 0.9417947375881451, 0.3854261585704538, 0.7515042589147154, 0.9516778653527052, 0.8092603047478086, 0.503676370364389]}}
{"id": "bb88f228-1407-4c03-b590-72778bee06f1", "fitness": 0.720520889476638, "name": "CMAES_Mirror_Adaptive", "description": "CMA-ES with mirrored sampling, adaptive learning rates, a simplified restart strategy based on fitness stagnation, and covariance matrix clipping to improve stability.", "code": "import numpy as np\n\nclass CMAES_Mirror_Adaptive:\n    def __init__(self, budget=10000, dim=10, popsize=None):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.lb = -5\n        self.ub = 5\n        self.c_learn_rate = 0.5  # Overall learning rate\n        self.c1 = 0.1 # Initial learning rate for rank-one update\n        self.cmu = 0.1 # Initial learning rate for rank-mu update\n        self.min_sigma = 1e-10\n        self.evals = 0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 50 * self.dim # Number of iterations to wait for improvement\n        self.fitness_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n        self.stagnation_counter = 0\n        self.fitness_history = []\n\n        while self.evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            \n            # Mirrored sampling\n            z_mirrored = -z\n            z_combined = np.concatenate([z, z_mirrored], axis=1)\n            \n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z_combined\n            x = np.clip(x, self.lb, self.ub)\n            \n            f = np.array([func(x[:, i]) for i in range(2 * self.popsize)])\n            self.evals += 2 * self.popsize\n            \n            if self.evals > self.budget:\n                f = f[:(self.budget - (self.evals - 2 * self.popsize))]\n                x = x[:, :(self.budget - (self.evals - 2 * self.popsize))]\n            \n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n                self.stagnation_counter = 0\n            else:\n                self.stagnation_counter += 1\n\n            self.fitness_history.append(self.f_opt)\n            \n            idx = np.argsort(f)\n            x = x[:, idx]\n            \n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n            \n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt.T, (xmean - self.mean) / self.sigma)\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.evals / (2*self.popsize)))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (xmean - self.mean) / self.sigma\n            \n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n            \n            # Adaptive learning rates\n            c1 = self.c1 * np.exp(-self.stagnation_counter / self.stagnation_threshold)\n            cmu = self.cmu * np.exp(-self.stagnation_counter / self.stagnation_threshold)\n\n            # Evaluation based learning rate decay\n            learn_rate = self.c_learn_rate * (1 - (self.evals / self.budget))\n            \n            c1 = learn_rate * c1 / (self.dim**2)\n            cmu = learn_rate * cmu / self.dim\n            \n            self.C = (1 - c1 - cmu) * self.C + c1 * np.outer(self.pc, self.pc) + cmu * artmp @ np.diag(self.weights) @ artmp.T\n            \n            self.mean = xmean\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            self.sigma = max(self.sigma, self.min_sigma) # avoid sigma getting too small\n\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            \n            # Covariance matrix adaptation to prevent becoming ill-conditioned\n            max_condition_number = 1e14\n            if np.linalg.cond(self.C) > max_condition_number:\n                self.C = self.C / np.linalg.cond(self.C) * max_condition_number\n                \n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-6 * np.eye(self.dim)\n\n            # Restart strategy\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.sigma = 0.5  # Reset sigma\n                self.C = np.eye(self.dim)  # Reset covariance matrix\n                self.pc = np.zeros(self.dim)  # Reset evolution paths\n                self.ps = np.zeros(self.dim)\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n                self.stagnation_counter = 0\n\n\n            if np.max(np.diag(self.C)) > (1e7 * self.sigma):\n                self.sigma *= 0.1\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n                self.stagnation_counter = 0\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm CMAES_Mirror_Adaptive scored 0.721 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["fcc5ab1a-48a5-4347-851c-a14a73446292"], "operator": null, "metadata": {"aucs": [0.262525211903329, 0.35064412579657844, 0.9239145542408662, 0.9577157016812676, 0.9281347527507029, 0.9380374990511157, 0.37576266397943314, 0.9137514386888614, 0.9297450152927501, 0.1878908791227809, 0.958136967724236, 0.9917760337396494, 0.26362373762166447, 0.9195454756844617, 0.9315164237507826, 0.9265524804378886, 0.8910406733679909, 0.9433723606083412, 0.2760519472816896, 0.5406798468083667]}}
{"id": "7ad60f8e-6a46-4ebb-a72c-59a2153239b9", "fitness": 0.7527967775949139, "name": "AdaptiveMirroredCMAES", "description": "Adaptive Mirrored CMA-ES with dynamic mirroring probability and covariance matrix adaptation based on successful steps.", "code": "import numpy as np\n\nclass AdaptiveMirroredCMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, restart_factor=3):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.normC = 0\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2.3)**2 + self.mueff))\n        self.lb = -5\n        self.ub = 5\n        self.decay = 0.9995  # Decay factor for sigma\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 50 # Number of iterations without improvement to trigger restart\n        self.restart_factor = restart_factor\n        self.archive_f = []\n        self.archive_x = []\n        self.mirror_factor = 0.25 # Probability of mirroring\n        self.success_rate = 0.0\n        self.success_history = []\n        self.adapt_mirr_rate = 0.1\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.stagnation_counter = 0\n        self.archive_f = []\n        self.archive_x = []\n        self.success_rate = 0.0\n        self.success_history = []\n        \n        while self.count_evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z\n\n            # Mirrored sampling\n            for i in range(self.popsize):\n                if np.random.rand() < self.mirror_factor:\n                    x[:, i] = self.mean + (self.mean - x[:, i])  # Mirror around the mean\n            \n            x = np.clip(x, self.lb, self.ub)\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.count_evals += self.popsize\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n                self.stagnation_counter = 0\n                self.success_history.append(1)\n            else:\n                self.stagnation_counter += 1\n                self.success_history.append(0)\n\n            idx = np.argsort(f)\n            x = x[:, idx]\n\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n            \n            y = (xmean - self.mean) / self.sigma\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt.T, y)\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.count_evals / self.popsize))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * y\n            \n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n            \n            # Rank-one update\n            C_new = (1 - self.c1 - self.cmu) * self.C + self.c1 * np.outer(self.pc, self.pc) + self.cmu * artmp @ np.diag(self.weights) @ artmp.T\n            \n            # Check success rate and adapt mirror factor\n            window_size = min(len(self.success_history), 20)\n            if window_size > 0:\n                self.success_rate = np.mean(self.success_history[-window_size:])\n\n            if self.success_rate > 0.3:\n                self.mirror_factor = min(1.0, self.mirror_factor + self.adapt_mirr_rate)\n            elif self.success_rate < 0.1:\n                self.mirror_factor = max(0.0, self.mirror_factor - self.adapt_mirr_rate)\n\n            self.mean = xmean\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            self.sigma = max(self.sigma, 1e-10)\n            self.sigma *= self.decay  # Decay sigma\n\n            self.C = np.triu(C_new) + np.triu(C_new, 1).T\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-6 * np.eye(self.dim)\n            \n            if np.max(np.diag(self.C)) > (1e7 * self.sigma):\n                self.sigma *= 0.1\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n                self.stagnation_counter = 0\n            \n            # Simplified Restart: if no improvement for a while, just restart.\n            if self.stagnation_counter > self.stagnation_threshold:\n                if len(self.archive_f) > 5 and self.f_opt > np.min(self.archive_f):\n                    self.mean = self.archive_x[np.argmin(self.archive_f)]\n                else:\n                     self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n                self.sigma *= self.restart_factor\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n\n                self.stagnation_counter = 0\n\n            self.archive_f.append(self.f_opt)\n            self.archive_x.append(self.x_opt)\n            if len(self.archive_f) > 10:\n                self.archive_f.pop(0)\n                self.archive_x.pop(0)\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveMirroredCMAES scored 0.753 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["92fde212-48e5-4c3a-8f35-b6d13dc35725"], "operator": null, "metadata": {"aucs": [0.2171233500228652, 0.4921178074086877, 0.9482408099217776, 0.9627776674963732, 0.9494973537882564, 0.9506660688762485, 0.3071219847010771, 0.854472884760171, 0.9471073494929422, 0.9316264885764023, 0.9504439831224758, 0.9957595233293739, 0.30939890391124836, 0.9477587503515486, 0.8280247211461645, 0.8820277202855655, 0.8988509679819602, 0.9738000624845196, 0.18352769525058554, 0.5255914589900346]}}
{"id": "f278e0ac-dd52-420e-9265-39877349be38", "fitness": 0.3612977277009216, "name": "CauchyCMAES", "description": "CMA-ES with a Cauchy mutation operator for enhanced exploration and a simplified covariance matrix adaptation strategy focusing on rank-one updates.", "code": "import numpy as np\n\nclass CauchyCMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.lb = -5\n        self.ub = 5\n        self.c_learn_rate = 0.15\n        self.sigma_decay = 0.9995\n        self.tol_x = 1e-12 * (self.ub - self.lb)\n        self.tol_fun = 1e-12\n        self.stagnation_counter = 0\n        self.max_stagnation = 50\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.stagnation_counter = 0\n        \n        while self.count_evals < self.budget:\n            z = np.random.standard_cauchy(size=(self.dim, self.popsize))  # Cauchy random numbers\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z\n            x = np.clip(x, self.lb, self.ub)\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.count_evals += self.popsize\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n                self.stagnation_counter = 0  # Reset stagnation counter\n            else:\n                self.stagnation_counter += 1\n\n            idx = np.argsort(f)\n            x = x[:, idx]\n\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n            \n            self.pc = (1 - self.cs) * self.pc + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * (xmean - self.mean) / self.sigma\n            \n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n\n            c1 = self.c_learn_rate / self.dim # Simplified learning rate\n            \n            self.C = (1 - c1) * self.C + c1 * np.outer(self.pc, self.pc) # Rank-one update\n            \n            self.mean = xmean\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.pc) / np.sqrt(self.dim) - 1))\n            self.sigma = max(self.sigma, 1e-10)\n            self.sigma *= self.sigma_decay\n\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                # Spectral correction\n                d, Q = np.linalg.eigh(self.C)\n                d = np.maximum(d, 1e-6)\n                self.C = Q @ np.diag(d) @ Q.T\n            \n            if np.max(np.diag(self.C)) > (1e7 * self.sigma):\n                self.sigma *= 0.1\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n                self.stagnation_counter = 0\n\n            if self.stagnation_counter > self.max_stagnation:\n                self.sigma *= 0.5  # Reduce step size\n                self.mean = self.x_opt + np.random.normal(0, 0.1, self.dim) # Local restart\n                self.mean = np.clip(self.mean, self.lb, self.ub)\n                self.stagnation_counter = 0\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm CauchyCMAES scored 0.361 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["dfec2c5e-5495-4e6e-bb29-160f64d9b595"], "operator": null, "metadata": {"aucs": [0.16472844623859784, 0.19280796232174402, 0.3193740812622424, 0.42985134297527006, 0.27981692861845453, 0.41084431679001054, 0.2701163075529608, 0.3067945959636392, 0.28150959177138435, 0.18850066195740667, 0.42193581754576226, 0.993151041008736, 0.27905655972056875, 0.3246327512435063, 0.6633003395108231, 0.32709029930490374, 0.2882520911923451, 0.43784839156820976, 0.17128860699579596, 0.4750544204760706]}}
{"id": "f9104e8e-70c4-4ff0-b158-28ac25137367", "fitness": 0.7231346369297204, "name": "MirroredCMAES", "description": "CMA-ES with mirrored sampling, simplified restarts based on stagnation and adaptive sigma decay, and a bound constraint handling mechanism.", "code": "import numpy as np\n\nclass MirroredCMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, restart_factor=3):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.normC = 0\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2.3)**2 + self.mueff))\n        self.lb = -5\n        self.ub = 5\n        self.decay = 0.9995  # Decay factor for sigma\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 50 # Number of iterations without improvement to trigger restart\n        self.restart_factor = restart_factor\n        self.mirror_factor = 0.25 # Probability of mirroring\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.stagnation_counter = 0\n        \n        while self.count_evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z\n\n            # Mirrored sampling\n            for i in range(self.popsize):\n                if np.random.rand() < self.mirror_factor:\n                    x[:, i] = self.mean + (self.mean - x[:, i])  # Mirror around the mean\n            \n            x = np.clip(x, self.lb, self.ub)\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.count_evals += self.popsize\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n                self.stagnation_counter = 0\n            else:\n                self.stagnation_counter += 1\n\n            idx = np.argsort(f)\n            x = x[:, idx]\n\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n            \n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt.T, (xmean - self.mean) / self.sigma)\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.count_evals / self.popsize))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (xmean - self.mean) / self.sigma\n            \n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n            self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * np.outer(self.pc, self.pc) + self.cmu * artmp @ np.diag(self.weights) @ artmp.T\n            \n            self.mean = xmean\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            self.sigma = max(self.sigma, 1e-10)\n            self.sigma *= self.decay  # Decay sigma\n\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-6 * np.eye(self.dim)\n            \n            if np.max(np.diag(self.C)) > (1e7 * self.sigma):\n                self.sigma *= 0.1\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n                self.stagnation_counter = 0\n            \n            # Simplified Restart: if no improvement for a while, just restart.\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n                self.sigma *= self.restart_factor\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.stagnation_counter = 0\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm MirroredCMAES scored 0.723 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["92fde212-48e5-4c3a-8f35-b6d13dc35725"], "operator": null, "metadata": {"aucs": [0.2505478434249483, 0.7676749184876086, 0.9395640407373493, 0.7768003951188056, 0.9488174222964868, 0.9551969946553814, 0.6197575873883232, 0.9325655235086449, 0.9489903101043878, 0.21851343826131298, 0.96447009291477, 0.9959128927670864, 0.29640999074776275, 0.9404054024449474, 0.8509560868223169, 0.873397579609005, 0.4556832969101312, 0.9686112939152915, 0.2371811075444481, 0.5212365209354023]}}
{"id": "14ed1b2e-6e04-46d1-a1f8-fe712bbbb869", "fitness": 0.0, "name": "CMAES_Mirror_Adaptive", "description": "CMA-ES with mirrored sampling, adaptive learning rates, a simplified restart strategy based on stagnation, covariance matrix clipping, and dynamic population size adjustment.", "code": "import numpy as np\n\nclass CMAES_Mirror_Adaptive:\n    def __init__(self, budget=10000, dim=10, popsize_factor=2):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = 4 + int(popsize_factor * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.lb = -5\n        self.ub = 5\n        self.c_learn_rate = 0.5\n        self.c1 = 0.1\n        self.cmu = 0.1\n        self.min_sigma = 1e-10\n        self.evals = 0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 50 * self.dim\n        self.fitness_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n        self.stagnation_counter = 0\n        self.fitness_history = []\n\n        while self.evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            z_mirrored = -z\n            z_combined = np.concatenate([z, z_mirrored], axis=1)\n            \n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z_combined\n            x = np.clip(x, self.lb, self.ub)\n            \n            f = np.array([func(x[:, i]) for i in range(2 * self.popsize)])\n            self.evals += 2 * self.popsize\n            \n            if self.evals > self.budget:\n                f = f[:(self.budget - (self.evals - 2 * self.popsize))]\n                x = x[:, :(self.budget - (self.evals - 2 * self.popsize))]\n            \n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n                self.stagnation_counter = 0\n            else:\n                self.stagnation_counter += 1\n\n            self.fitness_history.append(self.f_opt)\n            \n            idx = np.argsort(f)\n            x = x[:, idx]\n            \n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n            \n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt.T, (xmean - self.mean) / self.sigma)\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.evals / (2*self.popsize)))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (xmean - self.mean) / self.sigma\n            \n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n            \n            c1 = self.c1 * np.exp(-self.stagnation_counter / self.stagnation_threshold)\n            cmu = self.cmu * np.exp(-self.stagnation_counter / self.stagnation_threshold)\n\n            learn_rate = self.c_learn_rate * (1 - (self.evals / self.budget))\n            \n            c1 = learn_rate * c1 / (self.dim**2)\n            cmu = learn_rate * cmu / self.dim\n            \n            self.C = (1 - c1 - cmu) * self.C + c1 * np.outer(self.pc, self.pc) + cmu * artmp @ np.diag(self.weights) @ artmp.T\n            \n            self.mean = xmean\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            self.sigma = max(self.sigma, self.min_sigma)\n\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            \n            max_condition_number = 1e14\n            if np.linalg.cond(self.C) > max_condition_number:\n                self.C = self.C / np.linalg.cond(self.C) * max_condition_number\n                \n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-6 * np.eye(self.dim)\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.sigma = 0.5\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n                self.stagnation_counter = 0\n\n            if np.max(np.diag(self.C)) > (1e7 * self.sigma):\n                self.sigma *= 0.1\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n                self.stagnation_counter = 0\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm CMAES_Mirror_Adaptive scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["bb88f228-1407-4c03-b590-72778bee06f1"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "89bc9b23-d996-4ad5-9d50-3ffc6abaec36", "fitness": 0.0, "name": "DynamicCMAES", "description": "CMA-ES with mirrored sampling, adaptive learning rates based on both condition number and fitness progress, and a dynamic population size adjustment strategy to balance exploration and exploitation.", "code": "import numpy as np\n\nclass DynamicCMAES:\n    def __init__(self, budget=10000, dim=10, initial_popsize=None):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = initial_popsize if initial_popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.lb = -5\n        self.ub = 5\n        self.evals = 0\n        self.c_learn_rate = 0.5 # Overall learning rate\n        self.fitness_history = []\n        self.popsize_history = []\n        self.adapt_popsize_interval = 10\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n        self.fitness_history = []\n        self.popsize_history = []\n\n        while self.evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            z_mirrored = -z\n            z_combined = np.concatenate([z, z_mirrored], axis=1)\n\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z_combined\n            x = np.clip(x, self.lb, self.ub)\n\n            f = np.array([func(x[:, i]) for i in range(2 * self.popsize)])\n            self.evals += 2 * self.popsize\n\n            if self.evals > self.budget:\n                f = f[:(self.budget - (self.evals - 2 * self.popsize))]\n                x = x[:, :(self.budget - (self.evals - 2 * self.popsize))]\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n\n            idx = np.argsort(f)\n            x = x[:, idx]\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt.T, (xmean - self.mean) / self.sigma)\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.evals / (2*self.popsize)))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (xmean - self.mean) / self.sigma\n\n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n            \n            # Adaptive learning rate based on condition number\n            condition_number = np.linalg.cond(self.C)\n            learn_rate_cond = self.c_learn_rate / (1 + condition_number/1e5)  # Reduce learning when C is ill-conditioned\n            \n            # Adaptive learning rate based on fitness progress\n            if len(self.fitness_history) > 5:\n                fitness_change = np.mean(self.fitness_history[-5:]) - np.min(f)\n                if fitness_change < 1e-6:\n                    learn_rate_fitness = 0.1 # Reduce learning if little progress\n                else:\n                    learn_rate_fitness = self.c_learn_rate\n            else:\n                learn_rate_fitness = self.c_learn_rate\n                \n            learn_rate = min(learn_rate_cond, learn_rate_fitness) # Using the minimum of both\n\n            c1 = learn_rate / (self.dim**2)\n            cmu = learn_rate / self.dim\n            \n            self.C = (1 - c1 - cmu) * self.C + c1 * np.outer(self.pc, self.pc) + cmu * artmp @ np.diag(self.weights) @ artmp.T\n\n            self.mean = xmean\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            self.sigma = max(self.sigma, 1e-10)\n\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C += 1e-6 * np.eye(self.dim)\n                \n            if np.max(np.diag(self.C)) > (1e7 * self.sigma):\n                self.sigma *= 0.1\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n                \n            # Dynamic Popsize Adaptation\n            if self.evals // (2*self.popsize) % self.adapt_popsize_interval == 0 and self.evals > 0:\n                if len(self.fitness_history) > self.adapt_popsize_interval:\n                    recent_fitness_improvement = np.mean(self.fitness_history[-self.adapt_popsize_interval:]) - self.f_opt\n                    if recent_fitness_improvement < 1e-7:\n                        self.popsize = int(self.popsize * 1.2)  # Increase popsize for more exploration\n                    else:\n                        self.popsize = max(4 + int(3 * np.log(self.dim)), int(self.popsize * 0.8))  # Decrease popsize for more exploitation\n                    self.mu = self.popsize // 2\n                    self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n                    self.weights = self.weights / np.sum(self.weights)\n                    self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n            self.fitness_history.append(self.f_opt)\n            self.popsize_history.append(self.popsize)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm DynamicCMAES scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6a625e4e-4a5a-485c-8884-193944b212db"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "788d769c-8983-432f-9bdf-01a8c872d61c", "fitness": -Infinity, "name": "CMAES_Mirror_Spectral", "description": "CMA-ES with mirrored sampling, simplified covariance update, spectral correction of the covariance matrix to maintain positive definiteness, and dynamic population size adjustment based on the optimization progress.", "code": "import numpy as np\n\nclass CMAES_Mirror_Spectral:\n    def __init__(self, budget=10000, dim=10, popsize=None):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.lb = -5\n        self.ub = 5\n        self.min_sigma = 1e-10\n        self.evals = 0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 50 * self.dim # Number of iterations to wait for improvement\n        self.fitness_history = []\n        self.dynamic_popsize = True\n\n    def spectral_correction(self, C, target_condition_number=1e14):\n        \"\"\"\n        Corrects the covariance matrix C to maintain positive definiteness using spectral clipping.\n        \"\"\"\n        try:\n            eigenvalues, eigenvectors = np.linalg.eigh(C)\n            min_eigenvalue = np.min(eigenvalues)\n\n            if min_eigenvalue <= 0:\n                eigenvalues[eigenvalues < 0] = 1e-6  # Clip negative eigenvalues\n                C = eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T\n            \n            # Condition number check and correction\n            condition_number = np.max(eigenvalues) / np.min(eigenvalues)\n            if condition_number > target_condition_number:\n                eigenvalues = eigenvalues / condition_number * target_condition_number\n                C = eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T\n\n            return C\n        except np.linalg.LinAlgError:\n            # If eigenvalue decomposition fails, return a slightly perturbed identity matrix\n            return C + 1e-6 * np.eye(self.dim)\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n        self.stagnation_counter = 0\n        self.fitness_history = []\n        initial_popsize = self.popsize\n\n        while self.evals < self.budget:\n            # Dynamic popsize adjustment\n            if self.dynamic_popsize:\n                self.popsize = initial_popsize + int(np.sqrt(self.evals / self.budget) * initial_popsize)\n                self.mu = self.popsize // 2\n\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            \n            # Mirrored sampling\n            z_mirrored = -z\n            z_combined = np.concatenate([z, z_mirrored], axis=1)\n            \n            try:\n                C_sqrt = np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = self.spectral_correction(self.C)\n                C_sqrt = np.linalg.cholesky(self.C)\n\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z_combined\n            x = np.clip(x, self.lb, self.ub)\n            \n            f = np.array([func(x[:, i]) for i in range(2 * self.popsize)])\n            self.evals += 2 * self.popsize\n            \n            if self.evals > self.budget:\n                f = f[:(self.budget - (self.evals - 2 * self.popsize))]\n                x = x[:, :(self.budget - (self.evals - 2 * self.popsize))]\n            \n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n                self.stagnation_counter = 0\n            else:\n                self.stagnation_counter += 1\n\n            self.fitness_history.append(self.f_opt)\n            \n            idx = np.argsort(f)\n            x = x[:, idx]\n            \n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n            \n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt.T, (xmean - self.mean) / self.sigma)\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.evals / (2*self.popsize)))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (xmean - self.mean) / self.sigma\n            \n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n            \n            self.C = (1 - (0.1/self.dim)) * self.C + (0.1/self.dim) * (artmp @ np.diag(self.weights) @ artmp.T + np.outer(self.pc, self.pc))\n            \n            self.mean = xmean\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            self.sigma = max(self.sigma, self.min_sigma) # avoid sigma getting too small\n\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            \n            # Spectral correction of covariance matrix\n            self.C = self.spectral_correction(self.C)\n\n\n            # Restart strategy\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.sigma = 0.5  # Reset sigma\n                self.C = np.eye(self.dim)  # Reset covariance matrix\n                self.pc = np.zeros(self.dim)  # Reset evolution paths\n                self.ps = np.zeros(self.dim)\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n                self.stagnation_counter = 0\n\n            if np.max(np.diag(self.C)) > (1e7 * self.sigma):\n                self.sigma *= 0.1\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n                self.stagnation_counter = 0\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "An exception occurred: operands could not be broadcast together with shapes (2,4) (1,3) .", "error": "", "parent_ids": ["bb88f228-1407-4c03-b590-72778bee06f1"], "operator": null, "metadata": {}}
{"id": "d6ac1980-251c-47e6-946f-dfbaf4b70c88", "fitness": -Infinity, "name": "CMAES_Orthogonal_LocalSearch", "description": "Combines CMA-ES with orthogonal sampling to enhance exploration and a local search operator to improve exploitation, using adaptive parameter control based on stagnation detection and budget consumption.", "code": "import numpy as np\n\nclass CMAES_Orthogonal_LocalSearch:\n    def __init__(self, budget=10000, dim=10, popsize=None):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.lb = -5\n        self.ub = 5\n        self.c_learn_rate = 0.5\n        self.c1 = 0.1\n        self.cmu = 0.1\n        self.min_sigma = 1e-10\n        self.evals = 0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 50 * self.dim\n        self.fitness_history = []\n        self.local_search_prob = 0.1\n        self.local_search_radius = 0.1\n        self.orthogonal_basis = self._generate_orthogonal_basis(self.dim, self.popsize)\n\n    def _generate_orthogonal_basis(self, dim, popsize):\n        basis = np.random.randn(dim, popsize)\n        q, _ = np.linalg.qr(basis)\n        return q\n\n    def _orthogonal_sampling(self):\n        z = self.orthogonal_basis * np.random.randn(self.popsize)\n        return z\n\n    def _local_search(self, x, func):\n        x_new = x + np.random.uniform(-self.local_search_radius, self.local_search_radius, size=self.dim)\n        x_new = np.clip(x_new, self.lb, self.ub)\n        f_new = func(x_new)\n        self.evals += 1\n\n        if f_new < self.f_opt:\n            self.f_opt = f_new\n            self.x_opt = x_new\n            self.stagnation_counter = 0\n\n        if f_new < func(x):\n            return x_new, f_new\n        else:\n            return x, func(x)\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n        self.stagnation_counter = 0\n        self.fitness_history = []\n        self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n\n        while self.evals < self.budget:\n            # Orthogonal sampling\n            z = self._orthogonal_sampling()\n\n            # Mirrored sampling\n            z_mirrored = -z\n            z_combined = np.concatenate([z, z_mirrored], axis=1)\n\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z_combined\n            x = np.clip(x, self.lb, self.ub)\n\n            f = np.array([func(x[:, i]) for i in range(2 * self.popsize)])\n            self.evals += 2 * self.popsize\n\n            if self.evals > self.budget:\n                f = f[:(self.budget - (self.evals - 2 * self.popsize))]\n                x = x[:, :(self.budget - (self.evals - 2 * self.popsize))]\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n                self.stagnation_counter = 0\n            else:\n                self.stagnation_counter += 1\n\n            self.fitness_history.append(self.f_opt)\n\n            idx = np.argsort(f)\n            x = x[:, idx]\n\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt.T, (xmean - self.mean) / self.sigma)\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.evals / (2*self.popsize)))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (xmean - self.mean) / self.sigma\n\n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n\n            # Adaptive learning rates\n            c1 = self.c1 * np.exp(-self.stagnation_counter / self.stagnation_threshold)\n            cmu = self.cmu * np.exp(-self.stagnation_counter / self.stagnation_threshold)\n\n            # Evaluation based learning rate decay\n            learn_rate = self.c_learn_rate * (1 - (self.evals / self.budget))\n\n            c1 = learn_rate * c1 / (self.dim**2)\n            cmu = learn_rate * cmu / self.dim\n\n            self.C = (1 - c1 - cmu) * self.C + c1 * np.outer(self.pc, self.pc) + cmu * artmp @ np.diag(self.weights) @ artmp.T\n\n            self.mean = xmean\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            self.sigma = max(self.sigma, self.min_sigma)\n\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n\n            max_condition_number = 1e14\n            if np.linalg.cond(self.C) > max_condition_number:\n                self.C = self.C / np.linalg.cond(self.C) * max_condition_number\n\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-6 * np.eye(self.dim)\n\n            # Local search\n            if np.random.rand() < self.local_search_prob:\n                self.x_opt, self.f_opt = self._local_search(self.x_opt.copy(), func)\n\n            # Restart strategy\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.sigma = 0.5\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n                self.stagnation_counter = 0\n\n            if np.max(np.diag(self.C)) > (1e7 * self.sigma):\n                self.sigma *= 0.1\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n                self.stagnation_counter = 0\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "An exception occurred: operands could not be broadcast together with shapes (2,2) (6,) .", "error": "", "parent_ids": ["bb88f228-1407-4c03-b590-72778bee06f1"], "operator": null, "metadata": {}}
{"id": "7cd9dace-a293-404b-8180-0cac396930c3", "fitness": -Infinity, "name": "AdaptiveCMAES", "description": "CMA-ES with adaptive population sizing, dynamic coordinate system based on historical successful search directions, and mirrored sampling.", "code": "import numpy as np\n\nclass AdaptiveCMAES:\n    def __init__(self, budget=10000, dim=10, initial_popsize=None, target_success_rate=0.25):\n        self.budget = budget\n        self.dim = dim\n        self.initial_popsize = initial_popsize if initial_popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.popsize = self.initial_popsize\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2.3)**2 + self.mueff))\n        self.lb = -5\n        self.ub = 5\n        self.decay = 0.9995\n        self.target_success_rate = target_success_rate\n        self.success_history = []\n        self.adaptation_rate = 0.1\n        self.min_popsize = 4\n        self.max_popsize = 100\n        self.historical_directions = []\n        self.num_historical = 10\n        self.orthogonal_basis = np.eye(self.dim)\n        self.coordinate_adaptation_frequency = 10 # Adapt coordinate system every N iterations\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.popsize = self.initial_popsize\n        self.success_history = []\n        self.historical_directions = []\n        self.orthogonal_basis = np.eye(self.dim)\n\n        while self.count_evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * self.orthogonal_basis @ C_sqrt @ z #Apply orthogonal basis\n            x = np.clip(x, self.lb, self.ub)\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.count_evals += self.popsize\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n                self.success_history.append(1)\n            else:\n                self.success_history.append(0)\n\n            idx = np.argsort(f)\n            x = x[:, idx]\n\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n            y = (xmean - self.mean) / self.sigma\n            \n            # Store historical successful directions\n            if len(self.historical_directions) < self.num_historical:\n                self.historical_directions.append(y)\n            else:\n                self.historical_directions.pop(0)\n                self.historical_directions.append(y)\n            \n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt.T, y)\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.count_evals / self.popsize))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * y\n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n\n            C_new = (1 - self.c1 - self.cmu) * self.C + self.c1 * np.outer(self.pc, self.pc) + self.cmu * artmp @ np.diag(self.weights) @ artmp.T\n\n            # Adapt population size based on success rate\n            window_size = min(len(self.success_history), 20)\n            if window_size > 0:\n                success_rate = np.mean(self.success_history[-window_size:])\n                if success_rate < self.target_success_rate:\n                    self.popsize = max(self.min_popsize, int(self.popsize * (1 - self.adaptation_rate)))\n                    self.mu = self.popsize // 2\n                    self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n                    self.weights = self.weights / np.sum(self.weights)\n                    self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n                elif success_rate > self.target_success_rate:\n                    self.popsize = min(self.max_popsize, int(self.popsize * (1 + self.adaptation_rate)))\n                    self.mu = self.popsize // 2\n                    self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n                    self.weights = self.weights / np.sum(self.weights)\n                    self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n                \n            self.mean = xmean\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            self.sigma = max(self.sigma, 1e-10)\n            self.sigma *= self.decay\n\n            self.C = np.triu(C_new) + np.triu(C_new, 1).T\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-6 * np.eye(self.dim)\n            \n            if np.max(np.diag(self.C)) > (1e7 * self.sigma):\n                self.sigma *= 0.1\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n\n            if self.count_evals % self.coordinate_adaptation_frequency == 0 and len(self.historical_directions) > 2:\n                # Adapt the coordinate system based on historical successful directions\n                combined_directions = np.column_or((self.historical_directions))\n                Q, R = np.linalg.qr(combined_directions)\n                self.orthogonal_basis = Q # Use Q as new coordinate system\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "An exception occurred: module 'numpy' has no attribute 'column_or'.", "error": "", "parent_ids": ["7ad60f8e-6a46-4ebb-a72c-59a2153239b9"], "operator": null, "metadata": {}}
{"id": "50ad7e1c-6b3d-4691-8e3c-6c93e921617a", "fitness": 0.3598985425672597, "name": "CMAES_Mirror", "description": "Simplified CMA-ES with mirrored sampling, adaptive covariance updates via rank-one adaptation, and heuristic sigma scaling for faster convergence.", "code": "import numpy as np\n\nclass CMAES_Mirror:\n    def __init__(self, budget=10000, dim=10, popsize=None):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.lb = -5\n        self.ub = 5\n        self.evals = 0\n        self.c_learn_rate = 0.5 \n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n\n        while self.evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            z_mirrored = -z\n            z_combined = np.concatenate([z, z_mirrored], axis=1)\n\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z_combined\n            x = np.clip(x, self.lb, self.ub)\n\n            f = np.array([func(x[:, i]) for i in range(2 * self.popsize)])\n            self.evals += 2 * self.popsize\n\n            if self.evals > self.budget:\n                f = f[:(self.budget - (self.evals - 2 * self.popsize))]\n                x = x[:, :(self.budget - (self.evals - 2 * self.popsize))]\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n\n            idx = np.argsort(f)\n            x = x[:, idx]\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n\n            self.pc = (1 - self.cs) * self.pc + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * (xmean - self.mean) / self.sigma\n            \n            c1 = 0.3  # Simplified, fixed learning rate\n            self.C = (1 - c1) * self.C + c1 * np.outer(self.pc, self.pc)\n\n            self.mean = xmean\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.pc) / np.sqrt(self.dim) - 1)) # Heuristic sigma update\n            self.sigma = max(self.sigma, 1e-10)\n\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C += 1e-6 * np.eye(self.dim)\n                \n            if np.max(np.diag(self.C)) > (1e7 * self.sigma):\n                self.sigma *= 0.1\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm CMAES_Mirror scored 0.360 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6a625e4e-4a5a-485c-8884-193944b212db"], "operator": null, "metadata": {"aucs": [0.15778613890360682, 0.18466410393924015, 0.5317810426411012, 0.18050635317934827, 0.22681657739198047, 0.6765393955582912, 0.23711118553725696, 0.3381232852023377, 0.30478422855223397, 0.18407005541713783, 0.3570372134203812, 0.9875125631688417, 0.3111806743770761, 0.23817993210814903, 0.6130151059889228, 0.30562872658846874, 0.26079639243996344, 0.4625426565369287, 0.14103743893829856, 0.498857781455629]}}
{"id": "63d1b1a0-835a-4bd4-ab01-97aa0bc8badf", "fitness": 0.5974261642975369, "name": "CMAES_Mirror", "description": "CMA-ES with mirrored sampling, simplified rank-one covariance update, and adaptive sigma based on trace of covariance matrix to balance exploration and exploitation.", "code": "import numpy as np\n\nclass CMAES_Mirror:\n    def __init__(self, budget=10000, dim=10, popsize=None):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.lb = -5\n        self.ub = 5\n        self.evals = 0\n        self.c_learn_rate = 0.5 # Overall learning rate\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n\n        while self.evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            z_mirrored = -z\n            z_combined = np.concatenate([z, z_mirrored], axis=1)\n\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z_combined\n            x = np.clip(x, self.lb, self.ub)\n\n            f = np.array([func(x[:, i]) for i in range(2 * self.popsize)])\n            self.evals += 2 * self.popsize\n\n            if self.evals > self.budget:\n                f = f[:(self.budget - (self.evals - 2 * self.popsize))]\n                x = x[:, :(self.budget - (self.evals - 2 * self.popsize))]\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n\n            idx = np.argsort(f)\n            x = x[:, idx]\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt.T, (xmean - self.mean) / self.sigma)\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.evals / (2*self.popsize)))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (xmean - self.mean) / self.sigma\n\n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n            \n            c1 = self.c_learn_rate / (self.dim**2)\n            cmu = self.c_learn_rate / self.dim\n            \n            self.C = (1 - c1 - cmu) * self.C + c1 * np.outer(self.pc, self.pc)\n\n            self.mean = xmean\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n\n            # Adaptive sigma based on trace of C\n            self.sigma = max(self.sigma, 1e-10)\n            trace_C = np.trace(self.C)\n            if trace_C > self.dim * 10:\n                self.sigma *= 0.5\n            elif trace_C < 0.1:\n                self.sigma *= 1.2\n\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C += 1e-6 * np.eye(self.dim)\n                \n            if np.max(np.diag(self.C)) > (1e7 * self.sigma):\n                self.sigma *= 0.1\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm CMAES_Mirror scored 0.597 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6a625e4e-4a5a-485c-8884-193944b212db"], "operator": null, "metadata": {"aucs": [0.1435965156448441, 0.21023826749207797, 0.7190652048794302, 0.9453892326179766, 0.8495424637669273, 0.8677256896359246, 0.2908545389523818, 0.6825880914089748, 0.8417419211516046, 0.214682515890686, 0.9650512245502243, 0.9933934071796767, 0.2788307433182574, 0.6963579607503476, 0.7864134072775464, 0.37990345917616153, 0.4373067165962471, 0.9264913528593256, 0.21060454841358833, 0.5087460243885358]}}
{"id": "65bcd4d5-bc8e-4527-84d6-9ee2d959b008", "fitness": 0.6690097321468675, "name": "MirroredCMAES", "description": "Simplified Mirrored CMA-ES with adaptive sigma scaling based on stagnation and a more aggressive covariance adaptation.", "code": "import numpy as np\n\nclass MirroredCMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.c1 = 0.5 / (self.dim + 1)**2\n        self.cmu = min(1 - self.c1, 0.5 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        self.lb = -5\n        self.ub = 5\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 30\n        self.mirror_factor = 0.25\n        self.sigma_decay = 0.995\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.stagnation_counter = 0\n\n        while self.count_evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z\n\n            for i in range(self.popsize):\n                if np.random.rand() < self.mirror_factor:\n                    x[:, i] = self.mean + (self.mean - x[:, i])\n\n            x = np.clip(x, self.lb, self.ub)\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.count_evals += self.popsize\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n                self.stagnation_counter = 0\n            else:\n                self.stagnation_counter += 1\n\n            idx = np.argsort(f)\n            x = x[:, idx]\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n\n            y = (xmean - self.mean) / self.sigma\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt.T, y)\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.count_evals / self.popsize))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * y\n            \n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n            self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * (np.outer(self.pc, self.pc) + self.cmu * artmp @ np.diag(self.weights) @ artmp.T)\n            \n            self.mean = xmean\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            self.sigma = max(self.sigma, 1e-10)\n            self.sigma *= self.sigma_decay\n\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-6 * np.eye(self.dim)\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.sigma *= 2  # Increase sigma more aggressively\n                self.stagnation_counter = 0\n                self.C = np.eye(self.dim) # Reset covariance\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm MirroredCMAES scored 0.669 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f9104e8e-70c4-4ff0-b158-28ac25137367"], "operator": null, "metadata": {"aucs": [0.2964422647310341, 0.17825413655768063, 0.9307827056748655, 0.9716430351357777, 0.9465478858156443, 0.9608944678382769, 0.3225820227714358, 0.9425010191333335, 0.9494252632525838, 0.2068597768933096, 0.955117380346759, 0.992164222053742, 0.2614001958945964, 0.9408170984223242, 0.9371354740539678, 0.37341002319843286, 0.4571169973308863, 0.9667772022354915, 0.28334813572999806, 0.5069753358672116]}}
{"id": "5f1b1935-6e8d-4e69-9c02-a31a331fe0ac", "fitness": 0.7233660201279387, "name": "SpectralCMAES", "description": "CMA-ES with spectral correction, adaptive population sizing, and dynamic boundary handling based on constraint satisfaction to enhance exploration and exploitation.", "code": "import numpy as np\n\nclass SpectralCMAES:\n    def __init__(self, budget=10000, dim=10, initial_sigma=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.initial_sigma = initial_sigma\n        self.popsize = self._adaptive_popsize()\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.mean = np.zeros(self.dim)\n        self.sigma = initial_sigma\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.lb = -5\n        self.ub = 5\n        self.evals = 0\n        self.archive = []\n        self.eigenvalues = np.ones(self.dim)  # Initialize eigenvalues\n\n    def _adaptive_popsize(self):\n         # Dynamically adjust popsize based on dimension\n        return 4 + int(3 * np.log(self.dim))\n\n    def _spectral_correction(self):\n        try:\n            # Eigendecomposition\n            eigenvalues, eigenvectors = np.linalg.eigh(self.C)\n            \n            # Clip eigenvalues to avoid negative or near-zero values\n            eigenvalues = np.clip(eigenvalues, 1e-8, None)\n            \n            # Normalize eigenvalues to sum to dim (spectral correction)\n            eigenvalues = eigenvalues * self.dim / np.sum(eigenvalues)\n            \n            # Reconstruct C using the corrected spectrum\n            self.C = eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T\n            self.eigenvalues = eigenvalues\n        except np.linalg.LinAlgError:\n            # Handle potential errors during eigendecomposition\n            self.C = np.eye(self.dim)\n            self.eigenvalues = np.ones(self.dim)\n            \n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n\n        while self.evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z\n            \n            # Dynamic Boundary Handling: Constraint Satisfaction\n            violated_indices = (x < self.lb) | (x > self.ub)\n            x[violated_indices] = self.lb + np.random.rand(np.sum(violated_indices)) * (self.ub - self.lb)\n            \n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.evals += self.popsize\n\n            if self.evals > self.budget:\n                f = f[:(self.budget - (self.evals - self.popsize))]\n                x = x[:, :(self.budget - (self.evals - self.popsize))]\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n                \n\n            idx = np.argsort(f)\n            x = x[:, idx]\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt.T, (xmean - self.mean) / self.sigma)\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.evals / self.popsize))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (xmean - self.mean) / self.sigma\n\n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n            c1 = 0.5 / (self.dim * self.mueff)\n            cmu = min(0.5, 0.5 * (self.mueff - 2 + 1/self.mueff) / ((self.dim+2)**2 + self.mueff))\n            \n            self.C = (1 - c1 - cmu) * self.C + c1 * np.outer(self.pc, self.pc) + cmu * artmp @ np.diag(self.weights) @ artmp.T\n            \n            self._spectral_correction() # Apply Spectral Correction\n            \n            self.mean = xmean\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            self.sigma = max(self.sigma, 1e-10)  # Avoid sigma collapsing to zero\n            \n            if np.max(np.diag(self.C)) > (1e7 * self.sigma):\n                self.sigma *= 0.1\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n            \n            self.popsize = self._adaptive_popsize()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm SpectralCMAES scored 0.723 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6a625e4e-4a5a-485c-8884-193944b212db"], "operator": null, "metadata": {"aucs": [0.2681712722251235, 0.5847998121855191, 0.9388807842895075, 0.9656631735799832, 0.9449151183516654, 0.9547877068114663, 0.3530248992626597, 0.9162894584208451, 0.938734064847333, 0.18604035196220603, 0.9722496526562382, 0.9933415417801569, 0.28417935234601566, 0.937764340487018, 0.9516091794459548, 0.7024201192909465, 0.9040375111645578, 0.9653870779575431, 0.20070871135120294, 0.5043162741428322]}}
{"id": "4b4401b7-b0bd-4e7a-8937-1d762e4a14f2", "fitness": 0.705519225624905, "name": "AdaptiveMirroredCMAES", "description": "Adaptive Mirrored CMA-ES with dynamic mirroring, spectral correction of covariance matrix, and active covariance matrix adaptation.", "code": "import numpy as np\n\nclass AdaptiveMirroredCMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, restart_factor=3):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2.3)**2 + self.mueff))\n        self.lb = -5\n        self.ub = 5\n        self.decay = 0.9995  # Decay factor for sigma\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 50 # Number of iterations without improvement to trigger restart\n        self.restart_factor = restart_factor\n        self.archive_f = []\n        self.archive_x = []\n        self.mirror_factor = 0.25 # Probability of mirroring\n        self.success_rate = 0.0\n        self.success_history = []\n        self.adapt_mirr_rate = 0.1\n        self.active_cma = True\n        self.eigenvalues = np.ones(self.dim)\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.stagnation_counter = 0\n        self.archive_f = []\n        self.archive_x = []\n        self.success_rate = 0.0\n        self.success_history = []\n        self.eigenvalues = np.ones(self.dim)\n        \n        while self.count_evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z\n\n            # Mirrored sampling\n            for i in range(self.popsize):\n                if np.random.rand() < self.mirror_factor:\n                    x[:, i] = self.mean + (self.mean - x[:, i])  # Mirror around the mean\n            \n            x = np.clip(x, self.lb, self.ub)\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.count_evals += self.popsize\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n                self.stagnation_counter = 0\n                self.success_history.append(1)\n            else:\n                self.stagnation_counter += 1\n                self.success_history.append(0)\n\n            idx = np.argsort(f)\n            x = x[:, idx]\n\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n            \n            y = (xmean - self.mean) / self.sigma\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt.T, y)\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.count_evals / self.popsize))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * y\n            \n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n            \n            # Rank-one update\n            C_new = (1 - self.c1 - self.cmu) * self.C + self.c1 * np.outer(self.pc, self.pc)\n            \n            # Active CMA\n            if self.active_cma:\n                neg_idx = np.where(self.weights < 0)[0]\n                w_neg = np.abs(self.weights[neg_idx])\n                artmp_neg = artmp[:, neg_idx]\n                C_new += self.cmu * artmp_neg @ np.diag(w_neg) @ artmp_neg.T\n            else:\n                C_new += self.cmu * artmp @ np.diag(self.weights) @ artmp.T\n\n            # Spectral Correction\n            try:\n                D, B = np.linalg.eigh(C_new)\n                D = np.maximum(D, 1e-8)\n                C_new = B @ np.diag(D) @ B.T\n                self.eigenvalues = D\n            except np.linalg.LinAlgError:\n                C_new = self.C  # Revert to the old covariance if spectral decomposition fails\n\n            # Check success rate and adapt mirror factor\n            window_size = min(len(self.success_history), 20)\n            if window_size > 0:\n                self.success_rate = np.mean(self.success_history[-window_size:])\n\n            if self.success_rate > 0.3:\n                self.mirror_factor = min(1.0, self.mirror_factor + self.adapt_mirr_rate)\n            elif self.success_rate < 0.1:\n                self.mirror_factor = max(0.0, self.mirror_factor - self.adapt_mirr_rate)\n\n            self.mean = xmean\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            self.sigma = max(self.sigma, 1e-10)\n            self.sigma *= self.decay  # Decay sigma\n\n            self.C = np.triu(C_new) + np.triu(C_new, 1).T\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-6 * np.eye(self.dim)\n            \n            if np.max(np.diag(self.C)) > (1e7 * self.sigma):\n                self.sigma *= 0.1\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n                self.stagnation_counter = 0\n            \n            # Simplified Restart: if no improvement for a while, just restart.\n            if self.stagnation_counter > self.stagnation_threshold:\n                if len(self.archive_f) > 5 and self.f_opt > np.min(self.archive_f):\n                    self.mean = self.archive_x[np.argmin(self.archive_f)]\n                else:\n                     self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n                self.sigma *= self.restart_factor\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n\n                self.stagnation_counter = 0\n\n            self.archive_f.append(self.f_opt)\n            self.archive_x.append(self.x_opt)\n            if len(self.archive_f) > 10:\n                self.archive_f.pop(0)\n                self.archive_x.pop(0)\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveMirroredCMAES scored 0.706 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["7ad60f8e-6a46-4ebb-a72c-59a2153239b9"], "operator": null, "metadata": {"aucs": [0.2695521131062396, 0.5479926223036249, 0.9306526602926014, 0.9680616316621407, 0.9497062900052725, 0.9515675550596925, 0.30842943990948624, 0.6322082238348767, 0.9500611185013614, 0.24807318593287475, 0.9602553255128147, 0.9916742997625018, 0.29419822503428217, 0.8821801253150688, 0.9525469087675331, 0.9575927819858984, 0.6738748005460027, 0.9640477009669841, 0.17656122805050312, 0.5011482759483402]}}
{"id": "a719c031-a816-43e3-b2b6-5eb1b8b22677", "fitness": 0.0, "name": "DynamicPopulationCMAES", "description": "CMA-ES with a dynamic population size based on function landscape ruggedness and adaptive covariance matrix adaptation incorporating a rank-one update using both successful and unsuccessful steps to improve exploration and exploitation.", "code": "import numpy as np\n\nclass DynamicPopulationCMAES:\n    def __init__(self, budget=10000, dim=10, initial_popsize=None, restart_factor=3):\n        self.budget = budget\n        self.dim = dim\n        self.initial_popsize = initial_popsize if initial_popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.popsize = self.initial_popsize\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2.3)**2 + self.mueff))\n        self.lb = -5\n        self.ub = 5\n        self.decay = 0.9995  # Decay factor for sigma\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 50 # Number of iterations without improvement to trigger restart\n        self.restart_factor = restart_factor\n        self.archive_f = []\n        self.archive_x = []\n        self.landscape_ruggedness = 0.0\n        self.ruggedness_history = []\n        self.ruggedness_window = 20\n        self.popsize_adapt_rate = 0.1\n        self.min_popsize = 4\n        self.max_popsize = 100\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.stagnation_counter = 0\n        self.archive_f = []\n        self.archive_x = []\n        self.popsize = self.initial_popsize\n\n        while self.count_evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z\n            x = np.clip(x, self.lb, self.ub)\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.count_evals += self.popsize\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n                self.stagnation_counter = 0\n            else:\n                self.stagnation_counter += 1\n\n            idx = np.argsort(f)\n            x = x[:, idx]\n            f_sorted = f[idx]\n\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n            y = (xmean - self.mean) / self.sigma\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt.T, y)\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.count_evals / self.popsize))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * y\n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n\n            # Rank-one update using both good and bad steps\n            all_artmp = (x - self.mean[:, np.newaxis]) / self.sigma\n            \n            # Original Rank-mu update\n            C_new = (1 - self.c1 - self.cmu) * self.C + self.c1 * np.outer(self.pc, self.pc) + self.cmu * artmp @ np.diag(self.weights) @ artmp.T\n\n            # Landscape Ruggedness Estimation\n            delta_f = np.abs(f_sorted[0] - f_sorted[-1])  # Difference between best and worst\n            avg_f = np.mean(f_sorted)\n            if avg_f != 0:  # Avoid division by zero\n                self.landscape_ruggedness = delta_f / np.abs(avg_f)\n            else:\n                self.landscape_ruggedness = delta_f  # If avg_f is zero, just use delta_f\n            self.ruggedness_history.append(self.landscape_ruggedness)\n\n            if len(self.ruggedness_history) > self.ruggedness_window:\n                self.ruggedness_history.pop(0)\n            \n            avg_ruggedness = np.mean(self.ruggedness_history) if self.ruggedness_history else 0\n\n            # Dynamic Popsize Adjustment\n            if avg_ruggedness > 0.1:  # Increased ruggedness, increase popsize\n                self.popsize = min(self.max_popsize, int(self.popsize * (1 + self.popsize_adapt_rate)))\n            else:  # Smoother landscape, decrease popsize\n                self.popsize = max(self.min_popsize, int(self.popsize * (1 - self.popsize_adapt_rate)))\n            \n            self.mu = self.popsize // 2\n            self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n            self.weights = self.weights / np.sum(self.weights)\n            self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n            self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n\n            self.mean = xmean\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            self.sigma = max(self.sigma, 1e-10)\n            self.sigma *= self.decay  # Decay sigma\n            \n            self.C = np.triu(C_new) + np.triu(C_new, 1).T\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-6 * np.eye(self.dim)\n\n            if np.max(np.diag(self.C)) > (1e7 * self.sigma):\n                self.sigma *= 0.1\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n                self.stagnation_counter = 0\n            \n            # Simplified Restart: if no improvement for a while, just restart.\n            if self.stagnation_counter > self.stagnation_threshold:\n                if len(self.archive_f) > 5 and self.f_opt > np.min(self.archive_f):\n                    self.mean = self.archive_x[np.argmin(self.archive_f)]\n                else:\n                     self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n                self.sigma *= self.restart_factor\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.stagnation_counter = 0\n\n            self.archive_f.append(self.f_opt)\n            self.archive_x.append(self.x_opt)\n            if len(self.archive_f) > 10:\n                self.archive_f.pop(0)\n                self.archive_x.pop(0)\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm DynamicPopulationCMAES scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["7ad60f8e-6a46-4ebb-a72c-59a2153239b9"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "2e2807b1-7f8c-4232-a647-66656f75434f", "fitness": 0.4635301402530711, "name": "AdaptiveMirroredCMAES", "description": "Improved Mirrored CMA-ES with adaptive learning rate decay and dynamic population size adjustment based on stagnation.", "code": "import numpy as np\n\nclass AdaptiveMirroredCMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, restart_factor=3):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2.3)**2 + self.mueff))\n        self.lb = -5\n        self.ub = 5\n        self.sigma_decay = 0.9995  # Decay factor for sigma\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 50 # Number of iterations without improvement to trigger restart\n        self.restart_factor = restart_factor\n        self.mirror_factor = 0.25 # Probability of mirroring\n        self.popsize_adjust_freq = 20\n        self.popsize_increase_factor = 1.2\n        self.popsize_decrease_factor = 0.8\n        self.min_popsize = 4\n        self.decay_adaptation_rate = 0.01 # How quickly the decay adapts\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.stagnation_counter = 0\n        self.popsize = 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        \n        while self.count_evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z\n\n            # Mirrored sampling\n            for i in range(self.popsize):\n                if np.random.rand() < self.mirror_factor:\n                    x[:, i] = self.mean + (self.mean - x[:, i])  # Mirror around the mean\n            \n            x = np.clip(x, self.lb, self.ub)\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.count_evals += self.popsize\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n                self.stagnation_counter = 0\n                self.sigma_decay = min(self.sigma_decay + self.decay_adaptation_rate, 0.9999) # Increase decay slightly upon improvement\n            else:\n                self.stagnation_counter += 1\n                self.sigma_decay = max(self.sigma_decay - self.decay_adaptation_rate, 0.999) # Decrease decay slightly upon stagnation\n\n\n            idx = np.argsort(f)\n            x = x[:, idx]\n\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n            \n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt.T, (xmean - self.mean) / self.sigma)\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.count_evals / self.popsize))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (xmean - self.mean) / self.sigma\n            \n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n            self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * np.outer(self.pc, self.pc) + self.cmu * artmp @ np.diag(self.weights) @ artmp.T\n            \n            self.mean = xmean\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            self.sigma = max(self.sigma, 1e-10)\n            self.sigma *= self.sigma_decay  # Decay sigma\n\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-6 * np.eye(self.dim)\n            \n            if np.max(np.diag(self.C)) > (1e7 * self.sigma):\n                self.sigma *= 0.1\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n                self.stagnation_counter = 0\n            \n            # Simplified Restart: if no improvement for a while, just restart.\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n                self.sigma *= self.restart_factor\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.stagnation_counter = 0\n            \n            if self.count_evals // self.popsize % self.popsize_adjust_freq == 0 and self.count_evals > 0:\n                if self.stagnation_counter > self.stagnation_threshold // 2:\n                    self.popsize = max(self.min_popsize, int(self.popsize * self.popsize_decrease_factor))\n                    self.stagnation_threshold = int(self.stagnation_threshold * self.popsize_decrease_factor)\n                else:\n                    self.popsize = int(self.popsize * self.popsize_increase_factor)\n                    self.stagnation_threshold = int(self.stagnation_threshold * self.popsize_increase_factor)\n\n\n                self.mu = self.popsize // 2\n                self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n                self.weights = self.weights / np.sum(self.weights)\n                self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n                self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n                self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n                self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n                self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2.3)**2 + self.mueff))\n                \n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveMirroredCMAES scored 0.464 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f9104e8e-70c4-4ff0-b158-28ac25137367"], "operator": null, "metadata": {"aucs": [0.1726423217405192, 0.16586732298939566, 0.46481624961254964, 0.9655101282370038, 0.47089784577853633, 0.4106861309444386, 0.3002354066079238, 0.43268647132684623, 0.4091167115954377, 0.184597594876324, 0.5568316265792093, 0.9925283356867906, 0.24729595351059308, 0.4779353267534592, 0.7296199832661291, 0.5677529815568454, 0.3579047616168902, 0.7071277916872392, 0.16970553019144097, 0.4868443305038517]}}
{"id": "fdf079b1-eac6-4920-8cb7-0742fa4cf996", "fitness": -Infinity, "name": "SpectralCMAES", "description": "Spectral CMA-ES with simplified covariance update and adaptive step size based on eigenvalue distribution for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass SpectralCMAES:\n    def __init__(self, budget=10000, dim=10, initial_sigma=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.initial_sigma = initial_sigma\n        self.popsize = self._adaptive_popsize()\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.mean = np.zeros(self.dim)\n        self.sigma = initial_sigma\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.lb = -5\n        self.ub = 5\n        self.evals = 0\n        self.eigenvalues = np.ones(self.dim)\n\n    def _adaptive_popsize(self):\n        return 4 + int(3 * np.log(self.dim))\n\n    def _spectral_correction(self):\n        try:\n            eigenvalues, eigenvectors = np.linalg.eigh(self.C)\n            eigenvalues = np.clip(eigenvalues, 1e-8, None)\n            eigenvalues = eigenvalues * self.dim / np.sum(eigenvalues)\n            self.C = eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T\n            self.eigenvalues = eigenvalues\n        except np.linalg.LinAlgError:\n            self.C = np.eye(self.dim)\n            self.eigenvalues = np.ones(self.dim)\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n\n        while self.evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z\n            \n            # Boundary Handling: Clipping\n            x = np.clip(x, self.lb, self.ub)\n            \n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.evals += self.popsize\n\n            if self.evals > self.budget:\n                f = f[:(self.budget - (self.evals - self.popsize))]\n                x = x[:, :(self.budget - (self.evals - self.popsize))]\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n\n            idx = np.argsort(f)\n            x = x[:, idx]\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt.T, (xmean - self.mean) / self.sigma)\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.evals / self.popsize))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (xmean - self.mean) / self.sigma\n\n            # Simplified rank-one update\n            self.C = (1 - c1) * self.C + c1 * np.outer(self.pc, self.pc)\n            \n            self._spectral_correction()\n            \n            self.mean = xmean\n            \n            # Adaptive sigma based on eigenvalue distribution\n            max_eig = np.max(self.eigenvalues)\n            min_eig = np.min(self.eigenvalues)\n            \n            if max_eig > 1e5 * min_eig:\n                self.sigma *= 0.5  # Reduce sigma if eigenvalues are highly skewed\n            else:\n                self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n                \n            self.sigma = max(self.sigma, 1e-10)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "An exception occurred: name 'c1' is not defined.", "error": "", "parent_ids": ["5f1b1935-6e8d-4e69-9c02-a31a331fe0ac"], "operator": null, "metadata": {}}
{"id": "c571e487-7f46-49b3-aadf-cb8cbb0ad667", "fitness": -Infinity, "name": "CMAES_Orthogonal", "description": "CMA-ES with orthogonal sampling, adaptive restart based on population diversity, and dynamic covariance matrix adaptation using a rank-one update focusing on the best performing samples.", "code": "import numpy as np\n\nclass CMAES_Orthogonal:\n    def __init__(self, budget=10000, dim=10, popsize=None):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.lb = -5\n        self.ub = 5\n        self.evals = 0\n        self.c_learn_rate = 0.5\n        self.diversity_threshold = 0.1\n\n    def orthogonal_sampling(self, C_sqrt, num_samples):\n        H = np.random.normal(0, 1, size=(self.dim, self.dim))\n        Q, _ = np.linalg.qr(H)\n        z = Q[:, :num_samples]\n        return z\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n\n        while self.evals < self.budget:\n            z = self.orthogonal_sampling(np.linalg.cholesky(self.C), self.popsize)\n\n            x = self.mean[:, np.newaxis] + self.sigma * np.linalg.cholesky(self.C) @ z\n            x = np.clip(x, self.lb, self.ub)\n\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.evals += self.popsize\n\n            if self.evals > self.budget:\n                f = f[:(self.budget - (self.evals - self.popsize))]\n                x = x[:, :(self.budget - (self.evals - self.popsize))]\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n\n            idx = np.argsort(f)\n            x = x[:, idx]\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(np.linalg.cholesky(self.C).T, (xmean - self.mean) / self.sigma)\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.evals / self.popsize))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (xmean - self.mean) / self.sigma\n\n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n\n            c1 = self.c_learn_rate / (self.dim**2)\n            cmu = self.c_learn_rate / self.dim\n\n            self.C = (1 - c1 - cmu) * self.C + c1 * np.outer(self.pc, self.pc)\n\n            for i in range(self.mu):\n                 self.C += cmu * self.weights[i] * np.outer(artmp[:, i], artmp[:, i])\n\n            self.mean = xmean\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            self.sigma = max(self.sigma, 1e-10)\n\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C += 1e-6 * np.eye(self.dim)\n            \n            if np.max(np.diag(self.C)) > (1e7 * self.sigma):\n                self.sigma *= 0.1\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n            \n            #Adaptive restart mechanism\n            diversity = np.std(x, axis=1).mean()\n            if diversity < self.diversity_threshold:\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n                self.sigma = 0.5\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "An exception occurred: index 2 is out of bounds for axis 1 with size 2.", "error": "", "parent_ids": ["63d1b1a0-835a-4bd4-ab01-97aa0bc8badf"], "operator": null, "metadata": {}}
{"id": "a47807da-a93a-45a5-8269-6fef2ef5b498", "fitness": -Infinity, "name": "OrthogonalCMAES", "description": "CMA-ES with orthogonal sampling, adaptive population sizing based on function evaluations, and dynamic learning rate adjustment using the rank-mu update's magnitude to enhance convergence.", "code": "import numpy as np\n\nclass OrthogonalCMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.lb = -5\n        self.ub = 5\n        self.evals = 0\n        self.c_learn_rate = 0.5 # Initial learning rate\n        self.min_popsize = 4 # Minimum population size\n        self.max_popsize = 50 # Maximum population size\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n\n        while self.evals < self.budget:\n            # Adaptive population sizing based on remaining budget\n            remaining_evals = self.budget - self.evals\n            self.popsize = min(self.max_popsize, max(self.min_popsize, int(remaining_evals / 10))) # Adjust popsize\n\n            self.mu = self.popsize // 2\n            self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n            self.weights = self.weights / np.sum(self.weights)\n            self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n            # Orthogonal sampling\n            Z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            Q, _ = np.linalg.qr(Z)  # Orthogonal basis\n\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ Q\n            x = np.clip(x, self.lb, self.ub)\n\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.evals += self.popsize\n\n            if self.evals > self.budget:\n                f = f[:(self.budget - (self.evals - self.popsize))]\n                x = x[:, :(self.budget - (self.evals - self.popsize))]\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n\n            idx = np.argsort(f)\n            x = x[:, idx]\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt.T, (xmean - self.mean) / self.sigma)\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.evals / self.popsize))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (xmean - self.mean) / self.sigma\n\n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n\n            c1 = self.c_learn_rate / (self.dim**2)\n            cmu = self.c_learn_rate / self.dim\n            \n            # Rank-mu update magnitude for learning rate adaptation\n            delta_C = np.zeros_like(self.C)\n            for i in range(self.mu):\n                 delta_C += self.weights[i] * np.outer(artmp[:, i], artmp[:, i])\n            \n            self.C = (1 - c1 - cmu) * self.C + c1 * np.outer(self.pc, self.pc) + cmu * delta_C\n            \n            # Adaptive learning rate adjustment\n            magnitude_delta_C = np.linalg.norm(delta_C)\n            if magnitude_delta_C > 0.1: # Threshold for adjustment\n                self.c_learn_rate *= 0.9 # Reduce learning rate if change is large\n            elif magnitude_delta_C < 0.001:\n                self.c_learn_rate *= 1.1 # Increase learning rate if change is small\n            \n            self.c_learn_rate = np.clip(self.c_learn_rate, 0.01, 0.5)\n\n            self.mean = xmean\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            self.sigma = max(self.sigma, 1e-10)\n\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C += 1e-6 * np.eye(self.dim)\n                \n            if np.max(np.diag(self.C)) > (1e7 * self.sigma):\n                self.sigma *= 0.1\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "An exception occurred: index 2 is out of bounds for axis 1 with size 2.", "error": "", "parent_ids": ["63d1b1a0-835a-4bd4-ab01-97aa0bc8badf"], "operator": null, "metadata": {}}
{"id": "60663a93-9483-4795-8c91-8f229e49f15b", "fitness": -Infinity, "name": "OrthogonalCMAES", "description": "CMA-ES with orthogonal sampling using a quasi-Monte Carlo sequence, covariance matrix adaptation with rank-one updates, and adaptive step size control based on the success history.", "code": "import numpy as np\nfrom scipy.stats import norm\n\nclass OrthogonalCMAES:\n    def __init__(self, budget=10000, dim=10, initial_sigma=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.initial_sigma = initial_sigma\n        self.popsize = self._adaptive_popsize()\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.mean = np.zeros(self.dim)\n        self.sigma = initial_sigma\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.lb = -5\n        self.ub = 5\n        self.evals = 0\n        self.archive = []\n        self.success_rate = 0.5\n        self.success_history = []\n        self.step_size_adaptation_rate = 0.2\n\n    def _adaptive_popsize(self):\n        return 4 + int(3 * np.log(self.dim))\n\n    def _sobol_sequence(self, n, d):\n        def gray_code(n):\n            return n ^ (n >> 1)\n\n        V = [0] * d\n        for i in range(d):\n            V[i] = [0] * 32\n        \n        for i in range(d):\n            m = i + 1\n            while m > 0:\n                a = 2 * (m % 2) + 1\n                for j in range(1, 32):\n                    V[i][j] = a % 2\n                    a //= 2\n                m //= 2\n\n        X = np.zeros((n, d))\n        for i in range(1, n):\n            g = gray_code(i)\n            k = 0\n            while g % 2 == 0:\n                g //= 2\n                k += 1\n                \n            for j in range(d):\n                X[i, j] = X[i-1, j] ^ V[j][k]\n\n        return X / (2**32)\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n\n        while self.evals < self.budget:\n            # Generate orthogonal samples using Sobol sequence\n            z = norm.ppf(self._sobol_sequence(self.popsize, self.dim)).T\n\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z\n            \n            # Dynamic Boundary Handling: Constraint Satisfaction\n            violated_indices = (x < self.lb) | (x > self.ub)\n            x[violated_indices] = self.lb + np.random.rand(np.sum(violated_indices)) * (self.ub - self.lb)\n            \n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.evals += self.popsize\n\n            if self.evals > self.budget:\n                f = f[:(self.budget - (self.evals - self.popsize))]\n                x = x[:, :(self.budget - (self.evals - self.popsize))]\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n                \n\n            idx = np.argsort(f)\n            x = x[:, idx]\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * (xmean - self.mean) / self.sigma\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.evals / self.popsize))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (xmean - self.mean) / self.sigma\n\n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n            c1 = 0.5 / (self.dim * self.mueff)\n            cmu = min(0.5, 0.5 * (self.mueff - 2 + 1/self.mueff) / ((self.dim+2)**2 + self.mueff))\n            \n            self.C = (1 - c1 - cmu) * self.C + c1 * np.outer(self.pc, self.pc) + cmu * artmp @ np.diag(self.weights) @ artmp.T\n            \n            self.mean = xmean\n\n            # Adaptive step size control based on success history\n            if len(self.success_history) > 10:\n                self.success_history.pop(0)\n            if self.f_opt == np.min(f):\n                self.success_history.append(1)\n            else:\n                self.success_history.append(0)\n\n            self.success_rate = np.mean(self.success_history)\n            \n            if self.success_rate > 0.6:\n                self.sigma *= (1 + self.step_size_adaptation_rate)\n            elif self.success_rate < 0.4:\n                self.sigma *= (1 - self.step_size_adaptation_rate)\n\n            self.sigma = max(self.sigma, 1e-10)\n            \n            if np.max(np.diag(self.C)) > (1e7 * self.sigma):\n                self.sigma *= 0.1\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n            \n            self.popsize = self._adaptive_popsize()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "An exception occurred: name 'norm' is not defined.", "error": "", "parent_ids": ["5f1b1935-6e8d-4e69-9c02-a31a331fe0ac"], "operator": null, "metadata": {}}
{"id": "03535c81-0560-49a2-9905-263b1e0f7e7c", "fitness": 0.0, "name": "OrthogonalCMAES", "description": "An adaptive CMA-ES variant that uses orthogonal learning to improve covariance matrix adaptation and explore the search space more efficiently.", "code": "import numpy as np\n\nclass OrthogonalCMAES:\n    def __init__(self, budget=10000, dim=10, initial_sigma=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.initial_sigma = initial_sigma\n        self.popsize = self._adaptive_popsize()\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.mean = np.zeros(self.dim)\n        self.sigma = initial_sigma\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.lb = -5\n        self.ub = 5\n        self.evals = 0\n        self.archive = []\n\n    def _adaptive_popsize(self):\n         # Dynamically adjust popsize based on dimension\n        return 4 + int(3 * np.log(self.dim))\n\n    def _orthogonal_learning(self, x):\n        \"\"\"\n        Performs orthogonal learning to generate new candidate solutions.\n        \"\"\"\n        num_new_samples = self.popsize // 2  # Generate half the popsize as new samples\n        new_samples = np.zeros((self.dim, num_new_samples))\n\n        for i in range(num_new_samples):\n            # Select two random solutions from the current population\n            idx1, idx2 = np.random.choice(self.popsize, 2, replace=False)\n            x1 = x[:, idx1]\n            x2 = x[:, idx2]\n\n            # Generate a new solution as a linear combination of the two\n            alpha = np.random.rand()\n            new_samples[:, i] = alpha * x1 + (1 - alpha) * x2\n\n            # Ensure the new solution is within bounds\n            new_samples[:, i] = np.clip(new_samples[:, i], self.lb, self.ub)\n        return new_samples\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n\n        while self.evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z\n            \n            # Dynamic Boundary Handling: Constraint Satisfaction\n            violated_indices = (x < self.lb) | (x > self.ub)\n            x[violated_indices] = self.lb + np.random.rand(np.sum(violated_indices)) * (self.ub - self.lb)\n\n            # Orthogonal Learning\n            new_samples = self._orthogonal_learning(x)\n            x = np.concatenate((x, new_samples), axis=1)\n            \n            f = np.array([func(x[:, i]) for i in range(x.shape[1])])\n            self.evals += x.shape[1]\n\n            if self.evals > self.budget:\n                f = f[:(self.budget - (self.evals - x.shape[1]))]\n                x = x[:, :(self.budget - (self.evals - x.shape[1]))]\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n                \n\n            idx = np.argsort(f)\n            x = x[:, idx]\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt.T, (xmean - self.mean) / self.sigma)\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.evals / self.popsize))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (xmean - self.mean) / self.sigma\n\n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n            c1 = 0.5 / (self.dim * self.mueff)\n            cmu = min(0.5, 0.5 * (self.mueff - 2 + 1/self.mueff) / ((self.dim+2)**2 + self.mueff))\n            \n            self.C = (1 - c1 - cmu) * self.C + c1 * np.outer(self.pc, self.pc) + cmu * artmp @ np.diag(self.weights) @ artmp.T\n            \n            # Keep C positive definite\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)\n\n            self.mean = xmean\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            self.sigma = max(self.sigma, 1e-10)  # Avoid sigma collapsing to zero\n            \n            if np.max(np.diag(self.C)) > (1e7 * self.sigma):\n                self.sigma *= 0.1\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n            \n            self.popsize = self._adaptive_popsize()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm OrthogonalCMAES scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5f1b1935-6e8d-4e69-9c02-a31a331fe0ac"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "5525d129-fb21-42b2-b00a-c3f9d350d48d", "fitness": -Infinity, "name": "OrthogonalCMAES", "description": "CMA-ES with orthogonal sampling, dynamic population size reduction based on function evaluations, and a local search refinement strategy.", "code": "import numpy as np\n\nclass OrthogonalCMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, local_search_iterations=3):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2.3)**2 + self.mueff))\n        self.lb = -5\n        self.ub = 5\n        self.decay = 0.9995\n        self.local_search_iterations = local_search_iterations\n        self.archive_f = []\n        self.archive_x = []\n        self.popsize_reduction_factor = 0.95\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.archive_f = []\n        self.archive_x = []\n        current_popsize = self.popsize  # Initialize current population size\n\n        while self.count_evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, current_popsize))\n\n            # Orthogonal Sampling\n            Q, _ = np.linalg.qr(z)\n            z = Q\n\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z\n            x = np.clip(x, self.lb, self.ub)\n            f = np.array([func(x[:, i]) for i in range(current_popsize)])\n            self.count_evals += current_popsize\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n                self.archive_f.append(self.f_opt)\n                self.archive_x.append(self.x_opt)\n\n            idx = np.argsort(f)\n            x = x[:, idx]\n\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n            \n            y = (xmean - self.mean) / self.sigma\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt.T, y)\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.count_evals / current_popsize))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * y\n            \n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n            C_new = (1 - self.c1 - self.cmu) * self.C + self.c1 * np.outer(self.pc, self.pc) + self.cmu * artmp @ np.diag(self.weights) @ artmp.T\n\n            try:\n                D, B = np.linalg.eigh(C_new)\n                D = np.maximum(D, 1e-8)\n                C_new = B @ np.diag(D) @ B.T\n            except np.linalg.LinAlgError:\n                C_new = self.C\n\n            self.mean = xmean\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            self.sigma = max(self.sigma, 1e-10)\n            self.sigma *= self.decay\n\n            self.C = np.triu(C_new) + np.triu(C_new, 1).T\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-6 * np.eye(self.dim)\n\n            # Local search around the best solution found so far\n            for _ in range(self.local_search_iterations):\n                if self.x_opt is not None:\n                    x_local = self.x_opt + np.random.normal(0, self.sigma/5, self.dim)  # Smaller step size\n                    x_local = np.clip(x_local, self.lb, self.ub)\n                    f_local = func(x_local)\n                    self.count_evals += 1\n                    if f_local < self.f_opt:\n                        self.f_opt = f_local\n                        self.x_opt = x_local\n                        self.archive_f.append(self.f_opt)\n                        self.archive_x.append(self.x_opt)\n\n            # Dynamic population size reduction\n            if self.count_evals < self.budget * 0.7:\n                current_popsize = int(self.popsize * self.popsize_reduction_factor)\n                current_popsize = max(self.mu + 1, current_popsize)  # Ensure popsize > mu\n            else:\n                current_popsize = self.popsize  # Restore original popsize to exploit\n\n            if self.count_evals >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "An exception occurred: index 2 is out of bounds for axis 1 with size 2.", "error": "", "parent_ids": ["4b4401b7-b0bd-4e7a-8937-1d762e4a14f2"], "operator": null, "metadata": {}}
{"id": "e70f90e8-ca66-4087-a982-0647d3414592", "fitness": 0.5364823905590905, "name": "MirroredCMAES", "description": "Simplified Mirrored CMA-ES with adaptive sigma scaling based on stagnation and mirrored sampling, while using a rank-one update for covariance matrix and aggressively reducing sigma to converge faster.", "code": "import numpy as np\n\nclass MirroredCMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.c1 = 1 / (self.dim + 1)**2\n        self.cmu = min(1 - self.c1, 0.5 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        self.lb = -5\n        self.ub = 5\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 30\n        self.mirror_factor = 0.25\n        self.sigma_decay = 0.99\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.stagnation_counter = 0\n\n        while self.count_evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z\n\n            for i in range(self.popsize):\n                if np.random.rand() < self.mirror_factor:\n                    x[:, i] = self.mean + (self.mean - x[:, i])\n\n            x = np.clip(x, self.lb, self.ub)\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.count_evals += self.popsize\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n                self.stagnation_counter = 0\n            else:\n                self.stagnation_counter += 1\n\n            idx = np.argsort(f)\n            x = x[:, idx]\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n\n            y = (xmean - self.mean) / self.sigma\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * y\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.count_evals / self.popsize))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * y\n            \n            self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * np.outer(self.pc, self.pc)  + self.cmu * np.outer(y, y)\n            \n            self.mean = xmean\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            self.sigma = max(self.sigma, 1e-10)\n            self.sigma *= self.sigma_decay\n\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-6 * np.eye(self.dim)\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.sigma *= 2  # Increase sigma more aggressively\n                self.stagnation_counter = 0\n                self.C = np.eye(self.dim) # Reset covariance\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm MirroredCMAES scored 0.536 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["65bcd4d5-bc8e-4527-84d6-9ee2d959b008"], "operator": null, "metadata": {"aucs": [0.1634281156127919, 0.18290835393885785, 0.5363980942907434, 0.1666558549955569, 0.9334999026057992, 0.9679337005621198, 0.33035550600384844, 0.5389870328847088, 0.3633242791246487, 0.19759543116518552, 0.9753642351238002, 0.9959570527796109, 0.2538772399236323, 0.6443242355117633, 0.9660793637442017, 0.4403879594584742, 0.41400492737496974, 0.9744279831691753, 0.17931095673164377, 0.5048275861802785]}}
{"id": "88e74fe9-62f4-4835-99ac-1e02ae4a520d", "fitness": 0.7354281455849176, "name": "SpectralCMAES", "description": "Simplified Spectral CMA-ES with eigenvalue clipping, dynamic population size, and boundary handling for enhanced robustness and performance.", "code": "import numpy as np\n\nclass SpectralCMAES:\n    def __init__(self, budget=10000, dim=10, initial_sigma=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.initial_sigma = initial_sigma\n        self.popsize = self._adaptive_popsize()\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.mean = np.zeros(self.dim)\n        self.sigma = initial_sigma\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.lb = -5\n        self.ub = 5\n        self.evals = 0\n\n    def _adaptive_popsize(self):\n         # Dynamically adjust popsize based on dimension\n        return 4 + int(3 * np.log(self.dim))\n\n    def _spectral_correction(self):\n        try:\n            # Eigendecomposition\n            eigenvalues, eigenvectors = np.linalg.eigh(self.C)\n            \n            # Clip eigenvalues to avoid negative eigenvalues\n            eigenvalues = np.clip(eigenvalues, 1e-8, None)\n            \n            # Reconstruct C using the clipped spectrum\n            self.C = eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T\n        except np.linalg.LinAlgError:\n            # Handle potential errors during eigendecomposition\n            self.C = np.eye(self.dim)\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n\n        while self.evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z\n            \n            # Boundary Handling: Clipping\n            x = np.clip(x, self.lb, self.ub)\n            \n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.evals += self.popsize\n\n            if self.evals > self.budget:\n                f = f[:(self.budget - (self.evals - self.popsize))]\n                x = x[:, :(self.budget - (self.evals - self.popsize))]\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n                \n\n            idx = np.argsort(f)\n            x = x[:, idx]\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt.T, (xmean - self.mean) / self.sigma)\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.evals / self.popsize))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (xmean - self.mean) / self.sigma\n\n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n            c1 = 0.5 / (self.dim * self.mueff)\n            cmu = min(0.5, 0.5 * (self.mueff - 2 + 1/self.mueff) / ((self.dim+2)**2 + self.mueff))\n            \n            self.C = (1 - c1 - cmu) * self.C + c1 * np.outer(self.pc, self.pc) + cmu * artmp @ np.diag(self.weights) @ artmp.T\n            \n            self._spectral_correction() # Apply Spectral Correction\n            \n            self.mean = xmean\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            self.sigma = max(self.sigma, 1e-10)  # Avoid sigma collapsing to zero\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm SpectralCMAES scored 0.735 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5f1b1935-6e8d-4e69-9c02-a31a331fe0ac"], "operator": null, "metadata": {"aucs": [0.2384419828627451, 0.8952007534797665, 0.9481596376534366, 0.9719833955985625, 0.8426293609516373, 0.9587231925203161, 0.8732713994019393, 0.9340608438396507, 0.9525070842161498, 0.33610519605368605, 0.9696969952795691, 0.9954564109825311, 0.30806195753214893, 0.9437341177042474, 0.6089453562674663, 0.35020111239009943, 0.8997307414911501, 0.9643739020047521, 0.20159112934927503, 0.5156883421192237]}}
{"id": "b1e79308-73ba-4b3b-877d-1bee68411d9d", "fitness": 0.7441313852340673, "name": "AdaptiveMirroredCMAES", "description": "Adaptive Mirrored CMA-ES with simplified covariance update, eigenvalue clipping, and dynamic sigma adaptation based on both stagnation and success rate, enhancing exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveMirroredCMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, restart_factor=3):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2.3)**2 + self.mueff))\n        self.lb = -5\n        self.ub = 5\n        self.decay = 0.9995  # Decay factor for sigma\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 50 # Number of iterations without improvement to trigger restart\n        self.restart_factor = restart_factor\n        self.mirror_factor = 0.25 # Probability of mirroring\n        self.success_rate = 0.0\n        self.success_history = []\n        self.adapt_mirr_rate = 0.1\n        self.active_cma = True\n        self.eigen_threshold = 1e-6\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.count_evals = 0\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.stagnation_counter = 0\n        self.success_history = []\n        \n        while self.count_evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z\n\n            # Mirrored sampling\n            for i in range(self.popsize):\n                if np.random.rand() < self.mirror_factor:\n                    x[:, i] = self.mean + (self.mean - x[:, i])  # Mirror around the mean\n            \n            x = np.clip(x, self.lb, self.ub)\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.count_evals += self.popsize\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n                self.stagnation_counter = 0\n                self.success_history.append(1)\n            else:\n                self.stagnation_counter += 1\n                self.success_history.append(0)\n\n            idx = np.argsort(f)\n            x = x[:, idx]\n\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n            y = (xmean - self.mean) / self.sigma\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt.T, y)\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.count_evals / self.popsize))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * y\n            \n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n\n            # Simplified rank-one update\n            C_new = (1 - self.c1 - self.cmu) * self.C + self.c1 * np.outer(self.pc, self.pc) + self.cmu * artmp @ np.diag(self.weights) @ artmp.T\n            \n            # Spectral Correction with eigenvalue clipping\n            try:\n                D, B = np.linalg.eigh(C_new)\n                D = np.maximum(D, self.eigen_threshold)  # Clip small eigenvalues\n                C_new = B @ np.diag(D) @ B.T\n            except np.linalg.LinAlgError:\n                C_new = self.C\n\n            # Check success rate and adapt mirror factor\n            window_size = min(len(self.success_history), 20)\n            if window_size > 0:\n                self.success_rate = np.mean(self.success_history[-window_size:])\n\n            if self.success_rate > 0.3:\n                self.mirror_factor = min(1.0, self.mirror_factor + self.adapt_mirr_rate)\n            elif self.success_rate < 0.1:\n                self.mirror_factor = max(0.0, self.mirror_factor - self.adapt_mirr_rate)\n\n            self.mean = xmean\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n\n            # Dynamic sigma adaptation based on stagnation and success\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.sigma *= 0.8  # Reduce sigma if stagnating\n            if self.success_rate > 0.4:\n                self.sigma *= 1.1 # Increase sigma if successful\n\n            self.sigma = max(self.sigma, 1e-10)\n            self.sigma *= self.decay  # Decay sigma\n\n            self.C = np.triu(C_new) + np.triu(C_new, 1).T\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-6 * np.eye(self.dim)\n            \n            if np.max(np.diag(self.C)) > (1e7 * self.sigma):\n                self.sigma *= 0.1\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n                self.stagnation_counter = 0\n            \n            if self.stagnation_counter > self.stagnation_threshold:\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n                self.sigma *= self.restart_factor\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.stagnation_counter = 0\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveMirroredCMAES scored 0.744 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["4b4401b7-b0bd-4e7a-8937-1d762e4a14f2"], "operator": null, "metadata": {"aucs": [0.26512373044686954, 0.7031675100358886, 0.9376849783994026, 0.9560942223094998, 0.942410231863477, 0.9492086930652833, 0.3348100612136573, 0.9158744531999884, 0.9398987176784647, 0.5515801036013089, 0.9618898057274845, 0.9958181590374512, 0.2943957898074654, 0.9350298516236822, 0.9664663768037923, 0.7232246127860997, 0.8245390049564267, 0.9589046966884102, 0.22716737504635687, 0.49933933039033895]}}
{"id": "dbf0af4a-92e8-4449-893b-bf8362f072be", "fitness": 0.6106716834039473, "name": "OrthogonalCMAES", "description": "CMA-ES with orthogonal sampling, adaptive population size based on function evaluations remaining, and restart mechanism with memory of past good solutions.", "code": "import numpy as np\n\nclass OrthogonalCMAES:\n    def __init__(self, budget=10000, dim=10):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.archive_x = []\n        self.archive_f = []\n        self.best_f = np.inf\n        self.best_x = None\n        self.count_evals = 0\n\n        # CMA-ES parameters (initialized dynamically)\n        self.popsize = None  # Will be set dynamically\n        self.mu = None\n        self.weights = None\n        self.mueff = None\n        self.cs = None\n        self.damps = None\n        self.c1 = None\n        self.cmu = None\n        self.chiN = None\n\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n\n    def initialize_parameters(self):\n        self.popsize = min(self.budget // 10, 4 + int(3 * np.log(self.dim))) # Adaptive popsize\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.c1 = 0.5 / (self.dim + 1)**2\n        self.cmu = min(1 - self.c1, 0.5 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n    def sample_orthogonal_design(self, n_samples):\n      \"\"\"Samples an orthogonal design within the search space.\"\"\"\n      \n      design = np.zeros((self.dim, n_samples))\n      for i in range(self.dim):\n          design[i, :] = np.linspace(self.lb, self.ub, n_samples)\n      \n      # Randomly shuffle each dimension independently to create a Latin Hypercube Sample\n      for i in range(self.dim):\n          np.random.shuffle(design[i, :])\n      \n      return design\n\n    def restart(self):\n        \"\"\"Restarts the algorithm, using a solution from the archive if available.\"\"\"\n        if len(self.archive_x) > 0:\n            idx = np.argmin(self.archive_f)\n            self.mean = self.archive_x[idx]\n            self.sigma = 0.5\n            self.C = np.eye(self.dim)\n            self.pc = np.zeros(self.dim)\n            self.ps = np.zeros(self.dim)\n            self.archive_x.pop(idx)\n            self.archive_f.pop(idx)\n        else:\n            self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n            self.sigma = 0.5\n        \n\n    def __call__(self, func):\n        self.initialize_parameters()\n\n        while self.count_evals < self.budget:\n            # Adaptive population size\n            remaining_budget = self.budget - self.count_evals\n            self.popsize = min(remaining_budget, 4 + int(3 * np.log(self.dim)))\n            self.mu = self.popsize // 2\n            self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n            self.weights /= np.sum(self.weights)\n            self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n            # Generate orthogonal samples\n            x = self.sample_orthogonal_design(self.popsize)\n\n            # Transform samples according to CMA-ES distribution\n            C_sqrt = np.linalg.cholesky(self.C)\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))  # Standard normal samples\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z # Transform via covariance matrix\n\n            x = np.clip(x, self.lb, self.ub)\n\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.count_evals += self.popsize\n\n            if np.min(f) < self.best_f:\n                self.best_f = np.min(f)\n                self.best_x = x[:, np.argmin(f)].copy()\n\n            idx = np.argsort(f)\n            x = x[:, idx]\n            f = f[idx]\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n\n            # CMA-ES update\n            y = (xmean - self.mean) / self.sigma\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * y\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.count_evals / self.popsize))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * y\n\n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n            self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * (np.outer(self.pc, self.pc) + self.cmu * artmp @ np.diag(self.weights) @ artmp.T)\n\n            self.mean = xmean\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n\n            # Ensure sigma remains positive\n            self.sigma = max(self.sigma, 1e-10)\n\n            # Update covariance matrix\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-6 * np.eye(self.dim)\n\n            # Archive worse solutions\n            if len(self.archive_x) < 10:\n                self.archive_x.append(self.best_x)\n                self.archive_f.append(self.best_f)\n            else:\n                if self.best_f > np.max(self.archive_f):\n                   idx = np.argmax(self.archive_f)\n                   self.archive_x[idx] = self.best_x\n                   self.archive_f[idx] = self.best_f\n\n            # Restart mechanism\n            if self.count_evals > self.budget * 0.75:\n                self.restart()\n\n        return self.best_f, self.best_x", "configspace": "", "generation": 6, "feedback": "The algorithm OrthogonalCMAES scored 0.611 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["65bcd4d5-bc8e-4527-84d6-9ee2d959b008"], "operator": null, "metadata": {"aucs": [0.2276648479703679, 0.21270720792351205, 0.6289853808657222, 0.9744933199391592, 0.9612210483892051, 0.9636016875009713, 0.32429067301248693, 0.698710963528671, 0.8291823715469331, 0.18588066018225102, 0.9603800600277453, 0.9948024530777646, 0.28109284056906003, 0.8886900808662657, 0.7703132174752594, 0.35569713485814036, 0.28982804983569044, 0.9676779816070036, 0.19783420496174786, 0.500379483940987]}}
{"id": "f47b964f-fa43-4e1b-a803-56aa70977dd7", "fitness": -Infinity, "name": "SpectralCMAES", "description": "Spectral CMA-ES with adaptive population sizing, dynamic boundary handling using a reflection mechanism, and spectral correction with eigenvalue clipping and damping to stabilize adaptation.", "code": "import numpy as np\n\nclass SpectralCMAES:\n    def __init__(self, budget=10000, dim=10, initial_sigma=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.initial_sigma = initial_sigma\n        self.popsize = self._adaptive_popsize()\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.damps = self.damps * 0.5  # Damp the damping factor\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.mean = np.zeros(self.dim)\n        self.sigma = initial_sigma\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.lb = -5\n        self.ub = 5\n        self.evals = 0\n        self.archive = []\n        self.eigenvalues = np.ones(self.dim)  # Initialize eigenvalues\n\n    def _adaptive_popsize(self):\n         # Dynamically adjust popsize based on dimension\n        return 4 + int(3 * np.log(self.dim))\n\n    def _spectral_correction(self):\n        try:\n            # Eigendecomposition\n            eigenvalues, eigenvectors = np.linalg.eigh(self.C)\n            \n            # Clip eigenvalues to avoid negative or near-zero values\n            eigenvalues = np.clip(eigenvalues, 1e-8, None)\n            \n            # Damping for large eigenvalues\n            eigenvalues = 1.0 / (1.0 + 10 * (eigenvalues / np.max(eigenvalues) - 0.9)) * eigenvalues\n\n            # Normalize eigenvalues to sum to dim (spectral correction)\n            eigenvalues = eigenvalues * self.dim / np.sum(eigenvalues)\n            \n            # Reconstruct C using the corrected spectrum\n            self.C = eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T\n            self.eigenvalues = eigenvalues\n        except np.linalg.LinAlgError:\n            # Handle potential errors during eigendecomposition\n            self.C = np.eye(self.dim)\n            self.eigenvalues = np.ones(self.dim)\n            \n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n\n        while self.evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z\n            \n            # Dynamic Boundary Handling: Reflection\n            violated_low = x < self.lb\n            violated_high = x > self.ub\n            x[violated_low] = 2 * self.lb - x[violated_low]\n            x[violated_high] = 2 * self.ub - x[violated_high]\n\n            # Clip after reflection\n            x = np.clip(x, self.lb, self.ub)\n            \n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.evals += self.popsize\n\n            if self.evals > self.budget:\n                f = f[:(self.budget - (self.evals - self.popsize))]\n                x = x[:, :(self.budget - (self.evals - self.popsize))]\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n                \n\n            idx = np.argsort(f)\n            x = x[:, idx]\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt.T, (xmean - self.mean) / self.sigma)\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.evals / self.popsize))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (xmean - self.mean) / self.sigma\n\n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n            c1 = 0.5 / (self.dim * self.mueff)\n            cmu = min(0.5, 0.5 * (self.mueff - 2 + 1/self.mueff) / ((self.dim+2)**2 + self.mueff))\n            \n            self.C = (1 - c1 - cmu) * self.C + c1 * np.outer(self.pc, self.pc) + cmu * artmp @ np.diag(self.weights) @ artmp.T\n            \n            self._spectral_correction() # Apply Spectral Correction\n            \n            self.mean = xmean\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            self.sigma = max(self.sigma, 1e-10)  # Avoid sigma collapsing to zero\n            \n            if np.max(np.diag(self.C)) > (1e7 * self.sigma):\n                self.sigma *= 0.1\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n            \n            self.popsize = self._adaptive_popsize()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "An exception occurred: Matrix is not positive definite.", "error": "", "parent_ids": ["5f1b1935-6e8d-4e69-9c02-a31a331fe0ac"], "operator": null, "metadata": {}}
{"id": "3ec14dd5-98cf-4b70-8069-c1a4397587c7", "fitness": 0.7137459399349076, "name": "SpectralCMAES", "description": "Simplified Spectral CMA-ES with adaptive population size and boundary handling, focusing on efficient spectral correction and sigma adaptation.", "code": "import numpy as np\n\nclass SpectralCMAES:\n    def __init__(self, budget=10000, dim=10, initial_sigma=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.initial_sigma = initial_sigma\n        self.popsize = self._adaptive_popsize()\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.mean = np.zeros(self.dim)\n        self.sigma = initial_sigma\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.lb = -5\n        self.ub = 5\n        self.evals = 0\n\n    def _adaptive_popsize(self):\n        return 4 + int(3 * np.log(self.dim))\n\n    def _spectral_correction(self):\n        try:\n            eigenvalues, eigenvectors = np.linalg.eigh(self.C)\n            eigenvalues = np.clip(eigenvalues, 1e-8, None)\n            eigenvalues = eigenvalues * self.dim / np.sum(eigenvalues)\n            self.C = eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T\n        except np.linalg.LinAlgError:\n            self.C = np.eye(self.dim)\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n\n        while self.evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z\n\n            # Boundary Handling: Clip to bounds\n            x = np.clip(x, self.lb, self.ub)\n\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.evals += self.popsize\n\n            if self.evals > self.budget:\n                f = f[:(self.budget - (self.evals - self.popsize))]\n                x = x[:, :(self.budget - (self.evals - self.popsize))]\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n\n            idx = np.argsort(f)\n            x = x[:, idx]\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt.T, (xmean - self.mean) / self.sigma)\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.evals / self.popsize))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (xmean - self.mean) / self.sigma\n\n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n            c1 = 0.5 / (self.dim * self.mueff)\n            cmu = min(0.5, 0.5 * (self.mueff - 2 + 1/self.mueff) / ((self.dim+2)**2 + self.mueff))\n\n            self.C = (1 - c1 - cmu) * self.C + c1 * np.outer(self.pc, self.pc) + cmu * artmp @ np.diag(self.weights) @ artmp.T\n\n            self._spectral_correction()\n            self.mean = xmean\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            self.sigma = max(self.sigma, 1e-10)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm SpectralCMAES scored 0.714 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5f1b1935-6e8d-4e69-9c02-a31a331fe0ac"], "operator": null, "metadata": {"aucs": [0.27892039876092223, 0.3549486377396369, 0.9411414357845106, 0.9602397953003317, 0.946353183269346, 0.9544847398418334, 0.33787772283802, 0.9319138763300742, 0.9395840502428799, 0.3019198358915771, 0.9590960860183644, 0.9920847355091288, 0.27378554361122454, 0.9345057579015421, 0.9398801532044337, 0.9081406473538507, 0.6129604465980736, 0.9638706011544833, 0.2288459912327555, 0.5143651601151631]}}
{"id": "af41a26f-f55f-4818-bcef-c7f42cac0ca4", "fitness": 0.7904206933426717, "name": "MirroredRankOneCMAES", "description": "CMA-ES with mirrored sampling, adaptive learning rate for covariance matrix update, and a rank-one update strategy for exploration.", "code": "import numpy as np\n\nclass MirroredRankOneCMAES:\n    def __init__(self, budget=10000, dim=10, initial_sigma=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.initial_sigma = initial_sigma\n        self.popsize = self._adaptive_popsize()\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.mean = np.zeros(self.dim)\n        self.sigma = initial_sigma\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.lb = -5\n        self.ub = 5\n        self.evals = 0\n        self.c1 = 0.1 / (self.dim * self.mueff)  # Adaptive learning rate for rank-one update\n        self.cmu = min(0.1, 0.2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim+2)**2 + self.mueff))\n\n\n    def _adaptive_popsize(self):\n         # Dynamically adjust popsize based on dimension\n        return 4 + int(3 * np.log(self.dim))\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n\n        while self.evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            \n            # Mirrored Sampling\n            z_mirrored = np.concatenate([z, -z], axis=1)\n            x = self.mean[:, np.newaxis] + self.sigma * np.linalg.cholesky(self.C) @ z_mirrored\n            \n            # Boundary Handling: Clipping\n            x = np.clip(x, self.lb, self.ub)\n\n            f = np.array([func(x[:, i]) for i in range(2*self.popsize)])\n            self.evals += 2*self.popsize\n            \n            if self.evals > self.budget:\n                f = f[:(self.budget - (self.evals - 2*self.popsize))]\n                x = x[:, :(self.budget - (self.evals - 2*self.popsize))]\n                \n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n\n            idx = np.argsort(f)\n            x = x[:, idx]\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * (xmean - self.mean) / self.sigma\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.evals / (2*self.popsize)))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (xmean - self.mean) / self.sigma\n\n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n            \n            # Rank-one update for exploration\n            rank_one = np.outer(self.pc, self.pc)\n            self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * rank_one + self.cmu * artmp @ np.diag(self.weights) @ artmp.T\n\n            # Ensure C remains positive definite\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)  # Reset if not positive definite\n\n            self.mean = xmean\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            self.sigma = max(self.sigma, 1e-10)  # Avoid sigma collapsing to zero\n            self.c1 = min(0.4, self.c1 * 1.02) # Adaptive c1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm MirroredRankOneCMAES scored 0.790 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["88e74fe9-62f4-4835-99ac-1e02ae4a520d"], "operator": null, "metadata": {"aucs": [0.3649672748047975, 0.93012558353953, 0.9298902736977253, 0.9461087675396961, 0.9277808538842269, 0.9360149777051338, 0.31179013558140223, 0.9208203416405216, 0.9334002649519306, 0.9129988356650557, 0.9452073258352348, 0.9914923363884225, 0.3717899041102535, 0.9287047131652852, 0.9389563630226909, 0.9177843572640377, 0.8917428683883152, 0.9493242154673525, 0.24077345303858433, 0.5187410211632397]}}
{"id": "146aa401-de56-443d-bb5c-ad176c0d858e", "fitness": 0.2638443403541144, "name": "SpectralCMAES", "description": "Simplified Spectral CMA-ES with eigenvalue clipping, adaptive population size, and rank-one covariance matrix adaptation, reducing computational complexity.", "code": "import numpy as np\n\nclass SpectralCMAES:\n    def __init__(self, budget=10000, dim=10, initial_sigma=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.initial_sigma = initial_sigma\n        self.popsize = self._adaptive_popsize()\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.pc = np.zeros(self.dim)\n        self.mean = np.zeros(self.dim)\n        self.sigma = initial_sigma\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.lb = -5\n        self.ub = 5\n        self.evals = 0\n        self.C = np.eye(self.dim)  # Initialize covariance matrix\n\n    def _adaptive_popsize(self):\n        return 4 + int(3 * np.log(self.dim))\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n\n        while self.evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z\n\n            # Boundary Handling: Clip to bounds\n            x = np.clip(x, self.lb, self.ub)\n\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.evals += self.popsize\n\n            if self.evals > self.budget:\n                f = f[:(self.budget - (self.evals - self.popsize))]\n                x = x[:, :(self.budget - (self.evals - self.popsize))]\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n\n            idx = np.argsort(f)\n            x = x[:, idx]\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n\n            self.pc = (1 - self.cs) * self.pc + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * (xmean - self.mean) / self.sigma\n\n            # Rank-one update\n            self.C = (1 - 0.5) * self.C + 0.5 * np.outer(self.pc, self.pc)\n\n            # Eigenvalue clipping and normalization (Spectral correction)\n            try:\n                eigenvalues, eigenvectors = np.linalg.eigh(self.C)\n                eigenvalues = np.clip(eigenvalues, 1e-8, None)  # Clip small eigenvalues\n                eigenvalues = eigenvalues * self.dim / np.sum(eigenvalues) # Normalize eigenvalues\n                self.C = eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)  # Reset C in case of numerical instability\n\n            self.mean = xmean\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.pc) / np.sqrt(self.dim) - 1))\n            self.sigma = max(self.sigma, 1e-10)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm SpectralCMAES scored 0.264 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3ec14dd5-98cf-4b70-8069-c1a4397587c7"], "operator": null, "metadata": {"aucs": [0.096970962642136, 0.1848872625408935, 0.3062147938709959, 0.1413183878843971, 0.15548889674556488, 0.15133204548208323, 0.21892348593215027, 0.22662515662001204, 0.15508946267030987, 0.16471970371161337, 0.19269934577915449, 0.9947768616170174, 0.2733401979973864, 0.18110917970295082, 0.6048090544860016, 0.2945758267561214, 0.20450427596686904, 0.1569918099668759, 0.14394340370408987, 0.42856669300566597]}}
{"id": "168bf192-07d3-43cf-9a9a-a4738ab8bca8", "fitness": 0.6651657090528685, "name": "SpectralCMAES", "description": "Simplified Spectral CMA-ES with rank-one covariance update, eigenvalue clipping, adaptive sigma, and boundary handling for enhanced efficiency and robustness.", "code": "import numpy as np\n\nclass SpectralCMAES:\n    def __init__(self, budget=10000, dim=10, initial_sigma=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.initial_sigma = initial_sigma\n        self.popsize = 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.mean = np.zeros(self.dim)\n        self.sigma = initial_sigma\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.lb = -5\n        self.ub = 5\n        self.evals = 0\n\n    def _spectral_correction(self):\n        try:\n            eigenvalues, eigenvectors = np.linalg.eigh(self.C)\n            eigenvalues = np.clip(eigenvalues, 1e-8, None)\n            self.C = eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T\n        except np.linalg.LinAlgError:\n            self.C = np.eye(self.dim)\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n\n        while self.evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z\n            x = np.clip(x, self.lb, self.ub)\n            \n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.evals += self.popsize\n\n            if self.evals > self.budget:\n                f = f[:(self.budget - (self.evals - self.popsize))]\n                x = x[:, :(self.budget - (self.evals - self.popsize))]\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n\n            idx = np.argsort(f)\n            x = x[:, idx]\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n\n            y = (xmean - self.mean) / self.sigma\n            self.pc = (1 - self.cs) * self.pc + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * y\n\n            c1 = 0.5 / (self.dim * self.mueff)\n            self.C = (1 - c1) * self.C + c1 * np.outer(self.pc, self.pc)\n            \n            self._spectral_correction()\n            \n            self.mean = xmean\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.pc) / np.sqrt(self.dim) - 1))\n            self.sigma = max(self.sigma, 1e-10)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm SpectralCMAES scored 0.665 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["88e74fe9-62f4-4835-99ac-1e02ae4a520d"], "operator": null, "metadata": {"aucs": [0.2440744057123556, 0.24633006964047088, 0.9570732083152295, 0.9682332989200266, 0.9325934476875753, 0.9646637633479549, 0.3277278231286652, 0.9503217299936668, 0.9623341153976123, 0.2080665910348537, 0.9673091041967351, 0.9964959214630883, 0.7433783452405149, 0.9533618092405561, 0.5879234986651899, 0.33580051259911436, 0.2865573370606259, 0.9685552984352039, 0.19629992166275811, 0.5062139793151726]}}
{"id": "514c25fd-8ec0-4624-8c21-9e1d2882185b", "fitness": 0.6037897361011446, "name": "SpectralCMAES", "description": "Simplified Spectral CMA-ES with adaptive sigma and eigenvalue clipping for enhanced stability and performance, focusing on efficient updates and boundary handling.", "code": "import numpy as np\n\nclass SpectralCMAES:\n    def __init__(self, budget=10000, dim=10, initial_sigma=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.initial_sigma = initial_sigma\n        self.popsize = 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.mean = np.zeros(self.dim)\n        self.sigma = initial_sigma\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.lb = -5\n        self.ub = 5\n        self.evals = 0\n        self.c1 = 0.5 / (self.dim * self.mueff)\n        self.cmu = min(0.5, 0.5 * (self.mueff - 2 + 1/self.mueff) / ((self.dim+2)**2 + self.mueff))\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n\n    def _spectral_correction(self):\n        try:\n            eigenvalues, eigenvectors = np.linalg.eigh(self.C)\n            eigenvalues = np.clip(eigenvalues, 1e-8, None)\n            self.C = eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T\n        except np.linalg.LinAlgError:\n            self.C = np.eye(self.dim)\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n\n        while self.evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z\n            x = np.clip(x, self.lb, self.ub)\n            \n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.evals += self.popsize\n\n            if self.evals > self.budget:\n                f = f[:(self.budget - (self.evals - self.popsize))]\n                x = x[:, :(self.budget - (self.evals - self.popsize))]\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n\n            idx = np.argsort(f)\n            x = x[:, idx]\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n\n            self.pc = (1 - self.cs) * self.pc + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * (xmean - self.mean) / self.sigma\n            \n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma            \n            self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * np.outer(self.pc, self.pc) + self.cmu * artmp @ np.diag(self.weights) @ artmp.T\n            \n            self._spectral_correction()\n            self.mean = xmean\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.pc) / self.chiN - 1))\n            self.sigma = max(self.sigma, 1e-10)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm SpectralCMAES scored 0.604 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["88e74fe9-62f4-4835-99ac-1e02ae4a520d"], "operator": null, "metadata": {"aucs": [0.24964265467279034, 0.19135859509787778, 0.9303500420932461, 0.19972420949512315, 0.880364601762457, 0.9632612427514263, 0.31808016847751475, 0.7714158901448381, 0.9640736221938776, 0.18931119499345883, 0.9634713869989303, 0.996658988656123, 0.7882673589759956, 0.4194117413605166, 0.9448819130179197, 0.3642825775546287, 0.28428667437167443, 0.9696931808736642, 0.18412777367166955, 0.5031309048591631]}}
{"id": "ecb9daa3-c7b1-4426-94d0-a7c7ef571889", "fitness": 0.39792937052722965, "name": "ArchivedRestartCMAES", "description": "CMA-ES with a simplified rank-one covariance update, adaptive step size based on success history, and a restart mechanism using an archive of past solutions.", "code": "import numpy as np\n\nclass ArchivedRestartCMAES:\n    def __init__(self, budget=10000, dim=10, initial_sigma=0.5, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.initial_sigma = initial_sigma\n        self.popsize = self._adaptive_popsize()\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.mean = np.zeros(self.dim)\n        self.sigma = initial_sigma\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.lb = -5\n        self.ub = 5\n        self.evals = 0\n        self.archive_size = archive_size\n        self.archive_x = []\n        self.archive_f = []\n        self.success_history = []\n        self.success_window = 10\n\n    def _adaptive_popsize(self):\n         # Dynamically adjust popsize based on dimension\n        return 4 + int(3 * np.log(self.dim))\n\n    def _rank_one_update(self):\n        self.pc = (1 - self.cs) * self.pc + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * (self.xmean - self.mean) / self.sigma\n        self.C = (1 - 0.5) * self.C + 0.5 * np.outer(self.pc, self.pc)\n        \n        # Ensure C remains positive definite\n        try:\n            np.linalg.cholesky(self.C)\n        except np.linalg.LinAlgError:\n            self.C = np.eye(self.dim)\n\n    def _adjust_sigma(self, success):\n        self.success_history.append(success)\n        if len(self.success_history) > self.success_window:\n            self.success_history.pop(0)\n\n        success_rate = np.mean(self.success_history) if self.success_history else 0.5\n        \n        # Adaptive step-size control based on success rate\n        if success_rate > 0.6:\n            self.sigma *= 1.1  # Increase step size if doing well\n        elif success_rate < 0.2:\n            self.sigma *= 0.8  # Decrease step size if struggling\n        \n        self.sigma = max(self.sigma, 1e-10)\n\n    def _archive_solution(self, x, f):\n        if len(self.archive_x) < self.archive_size:\n            self.archive_x.append(x)\n            self.archive_f.append(f)\n        else:\n            max_archive_f = np.max(self.archive_f)\n            if f < max_archive_f:\n                max_idx = np.argmax(self.archive_f)\n                self.archive_x[max_idx] = x\n                self.archive_f[max_idx] = f\n\n    def _restart(self):\n        if self.archive_x:\n            best_idx = np.argmin(self.archive_f)\n            self.mean = self.archive_x[best_idx]\n        else:\n            self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n        self.sigma = self.initial_sigma\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.success_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n\n        while self.evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z\n            \n            # Boundary Handling: Clipping\n            x = np.clip(x, self.lb, self.ub)\n            \n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.evals += self.popsize\n\n            if self.evals > self.budget:\n                f = f[:(self.budget - (self.evals - self.popsize))]\n                x = x[:, :(self.budget - (self.evals - self.popsize))]\n\n            best_f = np.min(f)\n            best_x = x[:, np.argmin(f)]\n\n            if best_f < self.f_opt:\n                self.f_opt = best_f\n                self.x_opt = best_x\n                self._archive_solution(best_x, best_f) # Archive the best solution\n\n            idx = np.argsort(f)\n            x = x[:, idx]\n            self.xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n\n            self._rank_one_update()\n            \n            self._adjust_sigma(best_f < self.f_opt)\n\n            self.mean = self.xmean\n            \n            # Restart mechanism: If sigma is too small, restart\n            if self.sigma < 1e-8:\n                self._restart()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm ArchivedRestartCMAES scored 0.398 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["88e74fe9-62f4-4835-99ac-1e02ae4a520d"], "operator": null, "metadata": {"aucs": [0.10246241382383503, 0.33509274611247286, 0.42427466785869905, 0.5067551910732179, 0.38289023572422354, 0.49138689789567125, 0.29934807417423326, 0.41113858655426627, 0.2884341641795706, 0.1556938446594972, 0.5929216512531251, 0.9419823454548009, 0.3131210263408184, 0.322262092634217, 0.5462986842456264, 0.31957152752625373, 0.3147611347463639, 0.5567044200104393, 0.2134779727946996, 0.4400097334825612]}}
{"id": "5b73287b-0d54-47a7-bc07-8e6c569c478a", "fitness": 0.5710051930824933, "name": "SpectralRankOneCMAES", "description": "CMA-ES with a simplified rank-one covariance matrix update, spectral correction, and adaptive step size control based on the distribution of function values within the population.", "code": "import numpy as np\n\nclass SpectralRankOneCMAES:\n    def __init__(self, budget=10000, dim=10):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.best_f = np.inf\n        self.best_x = None\n        self.count_evals = 0\n\n        # CMA-ES parameters (initialized dynamically)\n        self.popsize = None  # Will be set dynamically\n        self.mu = None\n        self.weights = None\n        self.mueff = None\n        self.cs = None\n        self.damps = None\n        self.c1 = None\n        self.sigma_threshold = 1e-12\n\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.C = np.eye(self.dim)\n        self.p_sigma = np.zeros(self.dim)\n\n    def initialize_parameters(self):\n        self.popsize = min(self.budget // 10, 4 + int(3 * np.log(self.dim))) # Adaptive popsize\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.c1 = 1 / (self.dim + 1)**2\n\n\n    def __call__(self, func):\n        self.initialize_parameters()\n\n        while self.count_evals < self.budget:\n            # Adaptive population size\n            remaining_budget = self.budget - self.count_evals\n            self.popsize = min(remaining_budget, 4 + int(3 * np.log(self.dim)))\n            self.mu = self.popsize // 2\n            self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n            self.weights /= np.sum(self.weights)\n            self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n            self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n            self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n            self.c1 = 1 / (self.dim + 1)**2\n\n            # Generate and evaluate samples\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            x = self.mean[:, np.newaxis] + self.sigma * z\n            x = np.clip(x, self.lb, self.ub)\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.count_evals += self.popsize\n\n            if np.min(f) < self.best_f:\n                self.best_f = np.min(f)\n                self.best_x = x[:, np.argmin(f)].copy()\n\n            # Sort by fitness\n            idx = np.argsort(f)\n            x = x[:, idx]\n            f = f[idx]\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n\n            # CMA-ES update\n            y = (xmean - self.mean) / self.sigma\n            self.p_sigma = (1 - self.cs) * self.p_sigma + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * y\n            \n            # Simplified rank-one update\n            self.C = (1 - self.c1) * self.C + self.c1 * np.outer(self.p_sigma, self.p_sigma)\n\n            # Spectral correction\n            try:\n                eigvals, eigvecs = np.linalg.eigh(self.C)\n            except np.linalg.LinAlgError:\n                eigvals = np.ones(self.dim)\n                eigvecs = np.eye(self.dim)\n                \n            eigvals = np.maximum(eigvals, 1e-9)  # Prevent negative eigenvalues\n            self.C = eigvecs @ np.diag(eigvals) @ eigvecs.T\n            \n\n            # Adapt step size based on function value distribution\n            f_diff = np.mean(f[:self.mu]) - np.mean(f[self.mu:])\n            if f_diff > 0:\n                self.sigma *= np.exp(0.05 * f_diff / np.std(f))\n            else:\n                 self.sigma *= np.exp(0.025 * f_diff / np.std(f))\n            \n            if self.sigma < self.sigma_threshold:\n                self.sigma = 0.5\n\n            self.mean = xmean\n\n        return self.best_f, self.best_x", "configspace": "", "generation": 7, "feedback": "The algorithm SpectralRankOneCMAES scored 0.571 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["dbf0af4a-92e8-4449-893b-bf8362f072be"], "operator": null, "metadata": {"aucs": [0.1467286905629508, 0.1990555992269819, 0.9059494948012241, 0.20540806022983338, 0.9129867473614531, 0.9239976699808994, 0.2937584353804725, 0.8831796435128512, 0.9105841696784005, 0.1904010855867272, 0.9553405418072715, 0.9763406808192412, 0.24128722438764183, 0.4941662133438708, 0.8113806312225782, 0.33159516921656074, 0.3149050210021105, 0.9445970793802196, 0.2796133041562886, 0.4988283999922887]}}
{"id": "e08af032-190c-4516-89c4-95266bab064e", "fitness": 0.7323921751171472, "name": "SpectralCMAES", "description": "Simplified Spectral CMA-ES with eigenvalue clipping, streamlined update rules, and adaptive population size for efficient optimization.", "code": "import numpy as np\n\nclass SpectralCMAES:\n    def __init__(self, budget=10000, dim=10, initial_sigma=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.initial_sigma = initial_sigma\n        self.popsize = 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.mean = np.zeros(self.dim)\n        self.sigma = initial_sigma\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.lb = -5\n        self.ub = 5\n        self.evals = 0\n        self.c1 = 0.5 / (self.dim * self.mueff)\n        self.cmu = min(0.5, 0.5 * (self.mueff - 2 + 1/self.mueff) / ((self.dim+2)**2 + self.mueff))\n\n\n    def _spectral_correction(self):\n        try:\n            eigenvalues, eigenvectors = np.linalg.eigh(self.C)\n            eigenvalues = np.clip(eigenvalues, 1e-10, None)\n            self.C = eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T\n        except np.linalg.LinAlgError:\n            self.C = np.eye(self.dim)\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n\n        while self.evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z\n            x = np.clip(x, self.lb, self.ub)\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.evals += self.popsize\n\n            if self.evals > self.budget:\n                f = f[:(self.budget - (self.evals - self.popsize))]\n                x = x[:, :(self.budget - (self.evals - self.popsize))]\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n\n            idx = np.argsort(f)\n            x = x[:, idx]\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n\n            ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt.T, (xmean - self.mean) / self.sigma)\n            hsig = (np.linalg.norm(ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.evals / self.popsize))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (xmean - self.mean) / self.sigma\n\n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n            C = (1 - self.c1 - self.cmu) * self.C + self.c1 * np.outer(pc, pc) + self.cmu * artmp @ np.diag(self.weights) @ artmp.T\n            \n            try:\n                eigenvalues, eigenvectors = np.linalg.eigh(C)\n                eigenvalues = np.clip(eigenvalues, 1e-10, None)\n                C = eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T\n            except np.linalg.LinAlgError:\n                C = np.eye(self.dim)\n            \n            self.C = C\n            self.ps = ps\n            self.pc = pc\n\n            self.mean = xmean\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            self.sigma = max(self.sigma, 1e-10)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm SpectralCMAES scored 0.732 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["88e74fe9-62f4-4835-99ac-1e02ae4a520d"], "operator": null, "metadata": {"aucs": [0.3529366285666592, 0.9010283458322406, 0.9436466044809327, 0.970736308288977, 0.9486644321146795, 0.9507929859024339, 0.3770536277990797, 0.9341222459791166, 0.9466882833889733, 0.26069025183461314, 0.9564709678490027, 0.9908571484153741, 0.2686026657533145, 0.9339064325988223, 0.9562251271672939, 0.331869339514516, 0.895059740174861, 0.9648089550811244, 0.2636996768873807, 0.4999837347135453]}}
{"id": "56f5ece3-866e-4640-93a8-4105f2fe05e3", "fitness": 0.6491895650146133, "name": "SpectralCMAES", "description": "Simplified Spectral CMA-ES with dynamic population size, adaptive covariance update, and enhanced boundary handling using a mirroring strategy to improve exploration.", "code": "import numpy as np\n\nclass SpectralCMAES:\n    def __init__(self, budget=10000, dim=10, initial_sigma=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.initial_sigma = initial_sigma\n        self.popsize = self._adaptive_popsize()\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.mean = np.zeros(self.dim)\n        self.sigma = initial_sigma\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.lb = -5\n        self.ub = 5\n        self.evals = 0\n        self.c1 = 0.5 / (self.dim * self.mueff)\n        self.cmu = min(0.5, 0.5 * (self.mueff - 2 + 1/self.mueff) / ((self.dim+2)**2 + self.mueff))\n\n\n    def _adaptive_popsize(self):\n         # Dynamically adjust popsize based on dimension\n        return 4 + int(3 * np.log(self.dim))\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n\n        while self.evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z\n            \n            # Boundary Handling: Mirroring\n            x_mirrored = x.copy()\n            \n            below_lb = x < self.lb\n            above_ub = x > self.ub\n            \n            x_mirrored[below_lb] = 2 * self.lb - x[below_lb]\n            x_mirrored[above_ub] = 2 * self.ub - x[above_ub]\n            \n            x = np.where((x >= self.lb) & (x <= self.ub), x, x_mirrored)\n            \n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.evals += self.popsize\n\n            if self.evals > self.budget:\n                f = f[:(self.budget - (self.evals - self.popsize))]\n                x = x[:, :(self.budget - (self.evals - self.popsize))]\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n                \n\n            idx = np.argsort(f)\n            x = x[:, idx]\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt.T, (xmean - self.mean) / self.sigma)\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.evals / self.popsize))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (xmean - self.mean) / self.sigma\n\n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n            \n            self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * np.outer(self.pc, self.pc) + self.cmu * artmp @ np.diag(self.weights) @ artmp.T\n            \n            eigenvalues, eigenvectors = np.linalg.eigh(self.C)\n            eigenvalues = np.clip(eigenvalues, 1e-8, None)\n            self.C = eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T\n\n            self.mean = xmean\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            self.sigma = max(self.sigma, 1e-10)  # Avoid sigma collapsing to zero\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm SpectralCMAES scored 0.649 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["88e74fe9-62f4-4835-99ac-1e02ae4a520d"], "operator": null, "metadata": {"aucs": [0.41765318537371243, 0.5319263722161816, 0.938987948403225, 0.196203861326393, 0.9538452235613848, 0.9561729848693712, 0.3059120045639231, 0.9332390359708149, 0.9561794225553125, 0.21393494010238512, 0.9621234929849805, 0.9922105364937712, 0.2763901448273409, 0.7161432052030808, 0.9511597844038975, 0.3491490999198972, 0.5949072282527528, 0.9581813386072677, 0.2367499887881579, 0.5427215018684143]}}
{"id": "6a70bc02-6af2-4024-a8f4-298b879d2669", "fitness": 0.685266041636678, "name": "SpectralCMAES", "description": "Spectral CMA-ES with adaptive sigma decay based on condition number and eigenvalue damping to prevent premature convergence.", "code": "import numpy as np\n\nclass SpectralCMAES:\n    def __init__(self, budget=10000, dim=10, initial_sigma=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.initial_sigma = initial_sigma\n        self.popsize = self._adaptive_popsize()\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.mean = np.zeros(self.dim)\n        self.sigma = initial_sigma\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.lb = -5\n        self.ub = 5\n        self.evals = 0\n        self.eigenvalues = None # store eigenvalues\n\n    def _adaptive_popsize(self):\n         # Dynamically adjust popsize based on dimension\n        return 4 + int(3 * np.log(self.dim))\n\n    def _spectral_correction(self):\n        try:\n            # Eigendecomposition\n            eigenvalues, eigenvectors = np.linalg.eigh(self.C)\n            \n            # Clip eigenvalues to avoid negative eigenvalues\n            eigenvalues = np.clip(eigenvalues, 1e-8, None)\n            \n            # Store eigenvalues for sigma adaptation\n            self.eigenvalues = eigenvalues\n            \n            # Reconstruct C using the clipped spectrum\n            self.C = eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T\n        except np.linalg.LinAlgError:\n            # Handle potential errors during eigendecomposition\n            self.C = np.eye(self.dim)\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n\n        while self.evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z\n            \n            # Boundary Handling: Clipping\n            x = np.clip(x, self.lb, self.ub)\n            \n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.evals += self.popsize\n\n            if self.evals > self.budget:\n                f = f[:(self.budget - (self.evals - self.popsize))]\n                x = x[:, :(self.budget - (self.evals - self.popsize))]\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n                \n\n            idx = np.argsort(f)\n            x = x[:, idx]\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt.T, (xmean - self.mean) / self.sigma)\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.evals / self.popsize))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (xmean - self.mean) / self.sigma\n\n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n            c1 = 0.5 / (self.dim * self.mueff)\n            cmu = min(0.5, 0.5 * (self.mueff - 2 + 1/self.mueff) / ((self.dim+2)**2 + self.mueff))\n            \n            self.C = (1 - c1 - cmu) * self.C + c1 * np.outer(self.pc, self.pc) + cmu * artmp @ np.diag(self.weights) @ artmp.T\n            \n            self._spectral_correction() # Apply Spectral Correction\n            \n            self.mean = xmean\n\n            # Adaptive sigma decay based on condition number and eigenvalue damping\n            if self.eigenvalues is not None:\n                condition_number = np.max(self.eigenvalues) / np.min(self.eigenvalues)\n                sigma_decay = 1 + 0.1 * np.log10(condition_number)  # Adjust the 0.1 factor\n                self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1)) / sigma_decay\n            else:\n                self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n                \n            self.sigma = max(self.sigma, 1e-10)  # Avoid sigma collapsing to zero\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm SpectralCMAES scored 0.685 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["88e74fe9-62f4-4835-99ac-1e02ae4a520d"], "operator": null, "metadata": {"aucs": [0.25485368743379533, 0.6801877980422586, 0.9348421699000445, 0.9248106726310347, 0.9248711242146914, 0.9629125870189638, 0.3462011188008792, 0.5853998637879005, 0.9645857965259381, 0.20306692532706638, 0.9744107280131545, 0.9958513674923622, 0.2722833717860199, 0.852994104296057, 0.9447749846951308, 0.3247101560410215, 0.8747562386099775, 0.97075830328826, 0.20215716283362606, 0.510892671995379]}}
{"id": "997f3379-09fa-4bf8-9692-7256d74c67f3", "fitness": 0.3978040235822074, "name": "AdaptiveOrthogonalCMAES", "description": "Orthogonal CMA-ES with adaptive sampling using both orthogonal design and Gaussian distribution, dynamically adjusting population size and restart strategy based on stagnation detection using a moving average of fitness values.", "code": "import numpy as np\n\nclass AdaptiveOrthogonalCMAES:\n    def __init__(self, budget=10000, dim=10):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.archive_x = []\n        self.archive_f = []\n        self.best_f = np.inf\n        self.best_x = None\n        self.count_evals = 0\n        self.stagnation_counter = 0\n        self.fitness_history = []\n        self.history_length = 20  # Length of the moving average window\n        self.tolerance = 1e-6  # Tolerance for stagnation detection\n\n\n        # CMA-ES parameters (initialized dynamically)\n        self.popsize = None  # Will be set dynamically\n        self.mu = None\n        self.weights = None\n        self.mueff = None\n        self.cs = None\n        self.damps = None\n        self.c1 = None\n        self.cmu = None\n        self.chiN = None\n\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n\n    def initialize_parameters(self):\n        self.popsize = min(self.budget // 10, 4 + int(3 * np.log(self.dim))) # Adaptive popsize\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.c1 = 0.5 / (self.dim + 1)**2\n        self.cmu = min(1 - self.c1, 0.5 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n    def sample_orthogonal_design(self, n_samples):\n      \"\"\"Samples an orthogonal design within the search space.\"\"\"\n      \n      design = np.zeros((self.dim, n_samples))\n      for i in range(self.dim):\n          design[i, :] = np.linspace(self.lb, self.ub, n_samples)\n      \n      # Randomly shuffle each dimension independently to create a Latin Hypercube Sample\n      for i in range(self.dim):\n          np.random.shuffle(design[i, :])\n      \n      return design\n\n    def restart(self):\n        \"\"\"Restarts the algorithm, using a solution from the archive if available.\"\"\"\n        if len(self.archive_x) > 0:\n            idx = np.argmin(self.archive_f)\n            self.mean = self.archive_x[idx]\n            self.sigma = 0.5\n            self.C = np.eye(self.dim)\n            self.pc = np.zeros(self.dim)\n            self.ps = np.zeros(self.dim)\n            self.archive_x.pop(idx)\n            self.archive_f.pop(idx)\n        else:\n            self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n            self.sigma = 0.5\n        self.stagnation_counter = 0  # Reset stagnation counter after restart\n\n    def __call__(self, func):\n        self.initialize_parameters()\n\n        while self.count_evals < self.budget:\n            # Adaptive population size\n            remaining_budget = self.budget - self.count_evals\n            self.popsize = min(remaining_budget, 4 + int(3 * np.log(self.dim)))\n            self.mu = self.popsize // 2\n            self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n            self.weights /= np.sum(self.weights)\n            self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n            # Generate samples: mix orthogonal design and CMA-ES sampling\n            num_orthogonal = self.popsize // 2  # Half orthogonal, half CMA-ES\n            num_cmaes = self.popsize - num_orthogonal\n\n            x_orth = self.sample_orthogonal_design(num_orthogonal)\n\n            # Transform samples according to CMA-ES distribution\n            C_sqrt = np.linalg.cholesky(self.C)\n            z = np.random.normal(0, 1, size=(self.dim, num_cmaes))  # Standard normal samples\n            x_cmaes = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z  # Transform via covariance matrix\n\n            x = np.concatenate((x_orth, x_cmaes), axis=1)\n\n            x = np.clip(x, self.lb, self.ub)\n\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.count_evals += self.popsize\n\n            if np.min(f) < self.best_f:\n                self.best_f = np.min(f)\n                self.best_x = x[:, np.argmin(f)].copy()\n\n            # Stagnation detection\n            self.fitness_history.append(self.best_f)\n            if len(self.fitness_history) > self.history_length:\n                self.fitness_history.pop(0)\n\n            if len(self.fitness_history) == self.history_length:\n                if np.std(self.fitness_history) < self.tolerance:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0\n\n            if self.stagnation_counter > 5:  # If stagnated for 5 consecutive windows\n                self.restart()\n                continue # Restart and skip CMA-ES update this iteration\n\n            idx = np.argsort(f)\n            x = x[:, idx]\n            f = f[idx]\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n\n            # CMA-ES update\n            y = (xmean - self.mean) / self.sigma\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * y\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.count_evals / self.popsize))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * y\n\n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n            self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * (np.outer(self.pc, self.pc) + self.cmu * artmp @ np.diag(self.weights) @ artmp.T)\n\n            self.mean = xmean\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n\n            # Ensure sigma remains positive\n            self.sigma = max(self.sigma, 1e-10)\n\n            # Update covariance matrix\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-6 * np.eye(self.dim)\n\n            # Archive worse solutions\n            if len(self.archive_x) < 10:\n                self.archive_x.append(self.best_x)\n                self.archive_f.append(self.best_f)\n            else:\n                if self.best_f > np.max(self.archive_f):\n                   idx = np.argmax(self.archive_f)\n                   self.archive_x[idx] = self.best_x\n                   self.archive_f[idx] = self.best_f\n\n\n\n        return self.best_f, self.best_x", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveOrthogonalCMAES scored 0.398 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["dbf0af4a-92e8-4449-893b-bf8362f072be"], "operator": null, "metadata": {"aucs": [0.19539191749222373, 0.34497177718328265, 0.447202711215835, 0.4654833605090777, 0.25413213980369787, 0.4411145269666654, 0.30404763064898666, 0.38197162862711154, 0.3194950640980272, 0.1889183159025709, 0.4520020122636449, 0.9978998869997922, 0.29143091149230516, 0.30342791670144575, 0.725745143575161, 0.3468784275520873, 0.30217600267912115, 0.4996516915513618, 0.2098358655468231, 0.484303540834926]}}
{"id": "a6bcada0-858d-4ebb-ba66-4927cb201957", "fitness": 0.0, "name": "OrthogonalCMAESLocalSearch", "description": "Combines orthogonal sampling with CMA-ES, adds local search, and dynamically adjusts CMA-ES parameters and restart strategy based on stagnation detection and success rate.", "code": "import numpy as np\n\nclass OrthogonalCMAESLocalSearch:\n    def __init__(self, budget=10000, dim=10):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.archive_x = []\n        self.archive_f = []\n        self.best_f = np.inf\n        self.best_x = None\n        self.count_evals = 0\n        self.stagnation_counter = 0\n        self.success_rate = 0.5\n        self.success_history = []\n\n        # CMA-ES parameters (initialized dynamically)\n        self.popsize = None  # Will be set dynamically\n        self.mu = None\n        self.weights = None\n        self.mueff = None\n        self.cs = None\n        self.damps = None\n        self.c1 = None\n        self.cmu = None\n        self.chiN = None\n\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n\n    def initialize_parameters(self):\n        self.popsize = min(self.budget // 10, 4 + int(3 * np.log(self.dim))) # Adaptive popsize\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.c1 = 0.5 / (self.dim + 1)**2\n        self.cmu = min(1 - self.c1, 0.5 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n\n    def sample_orthogonal_design(self, n_samples):\n      \"\"\"Samples an orthogonal design within the search space.\"\"\"\n      \n      design = np.zeros((self.dim, n_samples))\n      for i in range(self.dim):\n          design[i, :] = np.linspace(self.lb, self.ub, n_samples)\n      \n      # Randomly shuffle each dimension independently to create a Latin Hypercube Sample\n      for i in range(self.dim):\n          np.random.shuffle(design[i, :])\n      \n      return design\n\n    def local_search(self, func, x, num_steps=5):\n        \"\"\"Performs a simple local search around a given point.\"\"\"\n        step_size = self.sigma / 5.0  # Adapt step size to CMA's sigma\n        x_current = x.copy()\n        f_current = func(x_current)\n        self.count_evals += 1\n\n        for _ in range(num_steps):\n            x_neighbor = x_current + np.random.normal(0, step_size, self.dim)\n            x_neighbor = np.clip(x_neighbor, self.lb, self.ub)\n            f_neighbor = func(x_neighbor)\n            self.count_evals += 1\n\n            if f_neighbor < f_current:\n                f_current = f_neighbor\n                x_current = x_neighbor\n\n        return f_current, x_current\n\n\n    def restart(self):\n        \"\"\"Restarts the algorithm, using a solution from the archive if available or re-initializing.\"\"\"\n        if len(self.archive_x) > 0:\n            idx = np.argmin(self.archive_f)\n            self.mean = self.archive_x[idx]\n            self.sigma = 0.5\n            self.C = np.eye(self.dim)\n            self.pc = np.zeros(self.dim)\n            self.ps = np.zeros(self.dim)\n            self.archive_x.pop(idx)\n            self.archive_f.pop(idx)\n        else:\n            self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n            self.sigma = 0.5\n            self.C = np.eye(self.dim)\n            self.pc = np.zeros(self.dim)\n            self.ps = np.zeros(self.dim)\n\n        self.stagnation_counter = 0  # Reset stagnation counter\n\n    def __call__(self, func):\n        self.initialize_parameters()\n        best_f_overall = np.inf\n        best_x_overall = None\n\n        while self.count_evals < self.budget:\n            # Adaptive population size\n            remaining_budget = self.budget - self.count_evals\n            self.popsize = min(remaining_budget, 4 + int(3 * np.log(self.dim)))\n            self.mu = self.popsize // 2\n            self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n            self.weights /= np.sum(self.weights)\n            self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n            # Generate orthogonal samples\n            x = self.sample_orthogonal_design(self.popsize)\n\n            # Transform samples according to CMA-ES distribution\n            C_sqrt = np.linalg.cholesky(self.C)\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))  # Standard normal samples\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z # Transform via covariance matrix\n\n            x = np.clip(x, self.lb, self.ub)\n\n            f = np.zeros(self.popsize)\n            for i in range(self.popsize):\n                f[i], x[:, i] = self.local_search(func, x[:, i])\n                if f[i] < best_f_overall:\n                    best_f_overall = f[i]\n                    best_x_overall = x[:, i].copy()\n\n            idx = np.argsort(f)\n            x = x[:, idx]\n            f = f[idx]\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n\n            # CMA-ES update\n            y = (xmean - self.mean) / self.sigma\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * y\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.count_evals / self.popsize))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * y\n\n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n            self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * (np.outer(self.pc, self.pc) + self.cmu * artmp @ np.diag(self.weights) @ artmp.T)\n\n            self.mean = xmean\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n\n            # Ensure sigma remains positive\n            self.sigma = max(self.sigma, 1e-10)\n\n            # Update covariance matrix\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-6 * np.eye(self.dim)\n\n            # Archive worse solutions\n            if len(self.archive_x) < 10:\n                self.archive_x.append(best_x_overall)\n                self.archive_f.append(best_f_overall)\n            else:\n                if best_f_overall > np.max(self.archive_f):\n                   idx = np.argmax(self.archive_f)\n                   self.archive_x[idx] = best_x_overall\n                   self.archive_f[idx] = best_f_overall\n\n            # Stagnation detection and adaptive restart\n            if best_f_overall < self.best_f:\n                self.best_f = best_f_overall\n                self.best_x = best_x_overall.copy()\n                self.stagnation_counter = 0\n            else:\n                self.stagnation_counter += 1\n\n            if self.stagnation_counter > 500:  # Increased stagnation threshold\n                self.restart()\n\n            # Dynamic sigma adaptation based on success rate\n            if len(self.success_history) > 10:\n                self.success_rate = np.mean(self.success_history[-10:])\n\n            if self.success_rate < 0.2:  # Reduced threshold\n                self.sigma *= 0.8  # Reduce sigma more aggressively\n                self.success_history = []  # Reset success history\n            elif self.success_rate > 0.8:  # Increased threshold\n                self.sigma *= 1.2  # Increase sigma\n\n            # Store success\n            self.success_history.append(int(best_f_overall < self.best_f))\n\n\n        return best_f_overall, best_x_overall", "configspace": "", "generation": 7, "feedback": "The algorithm OrthogonalCMAESLocalSearch scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["dbf0af4a-92e8-4449-893b-bf8362f072be"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "316d7bf9-c203-4753-9c44-cb8cb6dd0672", "fitness": 0.6957683240735925, "name": "SpectralCMAES", "description": "Spectral CMA-ES with Coordinate-wise Adaptive Mutation Step Size, incorporating Individual Learning Rates for each dimension and dynamic population size adjustment.", "code": "import numpy as np\n\nclass SpectralCMAES:\n    def __init__(self, budget=10000, dim=10, initial_sigma=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.initial_sigma = initial_sigma\n        self.popsize = self._adaptive_popsize()\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.mean = np.zeros(self.dim)\n        self.sigma = initial_sigma\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.lb = -5\n        self.ub = 5\n        self.evals = 0\n        self.individual_sigma = np.ones(self.dim) * self.initial_sigma # Individual learning rates\n\n    def _adaptive_popsize(self):\n        return 4 + int(3 * np.log(self.dim)) # Dynamic population size\n\n    def _spectral_correction(self):\n        try:\n            eigenvalues, eigenvectors = np.linalg.eigh(self.C)\n            eigenvalues = np.clip(eigenvalues, 1e-8, None)\n            eigenvalues = eigenvalues * self.dim / np.sum(eigenvalues)\n            self.C = eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T\n        except np.linalg.LinAlgError:\n            self.C = np.eye(self.dim)\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n\n        while self.evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.individual_sigma[:, np.newaxis] * C_sqrt @ z  # Coordinate-wise sigma\n\n            # Boundary Handling: Clip to bounds\n            x = np.clip(x, self.lb, self.ub)\n\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.evals += self.popsize\n\n            if self.evals > self.budget:\n                f = f[:(self.budget - (self.evals - self.popsize))]\n                x = x[:, :(self.budget - (self.evals - self.popsize))]\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n\n            idx = np.argsort(f)\n            x = x[:, idx]\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n\n            delta_mean = (xmean - self.mean)\n\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt.T, delta_mean / self.individual_sigma)\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.evals / self.popsize))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * delta_mean / self.individual_sigma\n\n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.individual_sigma[:, np.newaxis]\n            c1 = 0.5 / (self.dim * self.mueff)\n            cmu = min(0.5, 0.5 * (self.mueff - 2 + 1/self.mueff) / ((self.dim+2)**2 + self.mueff))\n\n            self.C = (1 - c1 - cmu) * self.C + c1 * np.outer(self.pc, self.pc) + cmu * artmp @ np.diag(self.weights) @ artmp.T\n\n            self._spectral_correction()\n            self.mean = xmean\n\n            # Update individual learning rates\n            self.individual_sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            self.individual_sigma = np.maximum(self.individual_sigma, 1e-10)\n\n            # Adapt population size\n            self.popsize = self._adaptive_popsize()\n            self.mu = self.popsize // 2\n            self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n            self.weights = self.weights / np.sum(self.weights)\n            self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n            self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n            self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm SpectralCMAES scored 0.696 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3ec14dd5-98cf-4b70-8069-c1a4397587c7"], "operator": null, "metadata": {"aucs": [0.2519363618446211, 0.31291219228868095, 0.9292832127219356, 0.972475846092349, 0.9366274125485229, 0.9533874881517429, 0.33012394702260894, 0.9254442304875025, 0.9534108426597462, 0.21090310486252417, 0.9675236396462296, 0.9925529285382145, 0.2560671404039314, 0.9363555071306198, 0.8635307210956259, 0.5113312349579695, 0.9007739381930204, 0.9588083840437798, 0.2431609163027484, 0.5087574324794739]}}
{"id": "46801822-e780-4f7e-aff3-2d1ce876b8a8", "fitness": -Infinity, "name": "OrthogonalCMAES", "description": "CMA-ES with orthogonal sampling, dynamic covariance matrix adaptation using a running average, and selective pressure based on fitness ranking.", "code": "import numpy as np\n\nclass OrthogonalCMAES:\n    def __init__(self, budget=10000, dim=10, initial_sigma=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.initial_sigma = initial_sigma\n        self.popsize = self._adaptive_popsize()\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.mean = np.zeros(self.dim)\n        self.sigma = initial_sigma\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.lb = -5\n        self.ub = 5\n        self.evals = 0\n        self.c1 = 0.1 / (self.dim * self.mueff)\n        self.cmu = min(0.1, 0.2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim+2)**2 + self.mueff))\n        self.covariance_history = []\n        self.history_length = 10 \n\n    def _adaptive_popsize(self):\n        return 4 + int(3 * np.log(self.dim))\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n\n        while self.evals < self.budget:\n            z = self._orthogonal_sampling(self.popsize)\n            x = self.mean[:, np.newaxis] + self.sigma * np.linalg.cholesky(self.C) @ z\n            x = np.clip(x, self.lb, self.ub)\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.evals += self.popsize\n\n            if self.evals > self.budget:\n                f = f[:(self.budget - (self.evals - self.popsize))]\n                x = x[:, :(self.budget - (self.evals - self.popsize))]\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n\n            idx = np.argsort(f)\n            x = x[:, idx]\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * (xmean - self.mean) / self.sigma\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.evals / self.popsize))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (xmean - self.mean) / self.sigma\n\n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n            self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * np.outer(self.pc, self.pc) + self.cmu * artmp @ np.diag(self.weights) @ artmp.T\n\n            # Running Average Covariance Adaptation\n            self.covariance_history.append(self.C)\n            if len(self.covariance_history) > self.history_length:\n                self.covariance_history.pop(0)\n            \n            self.C = np.mean(np.stack(self.covariance_history), axis=0)\n            \n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)\n\n            self.mean = xmean\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            self.sigma = max(self.sigma, 1e-10)\n\n        return self.f_opt, self.x_opt\n\n    def _orthogonal_sampling(self, popsize):\n        H = np.random.normal(0, 1, size=(self.dim, self.dim))\n        Q, _ = np.linalg.qr(H)\n        z = Q[:, :popsize]\n        return z", "configspace": "", "generation": 8, "feedback": "An exception occurred: index 2 is out of bounds for axis 1 with size 2.", "error": "", "parent_ids": ["af41a26f-f55f-4818-bcef-c7f42cac0ca4"], "operator": null, "metadata": {}}
{"id": "396c2acc-6909-460f-8990-5fc089978182", "fitness": -Infinity, "name": "AdaptiveVarianceScalingCMAES", "description": "Adaptive Variance Scaling CMA-ES, where the variance of each dimension is adapted based on the success rate of previous steps.", "code": "import numpy as np\n\nclass AdaptiveVarianceScalingCMAES:\n    def __init__(self, budget=10000, dim=10, initial_sigma=0.5, learning_rate_variance=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_sigma = initial_sigma\n        self.popsize = self._adaptive_popsize()\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.mean = np.zeros(self.dim)\n        self.sigma = initial_sigma\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.lb = -5\n        self.ub = 5\n        self.evals = 0\n        self.learning_rate_variance = learning_rate_variance\n        self.variances = np.ones(self.dim) # Individual variances for each dimension\n\n    def _adaptive_popsize(self):\n         # Dynamically adjust popsize based on dimension\n        return 4 + int(3 * np.log(self.dim))\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n\n        while self.evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            \n            # Apply individual variance scaling to each dimension\n            scaled_z = z * np.sqrt(self.variances[:, np.newaxis])\n\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ scaled_z\n            \n            # Boundary Handling: Clipping\n            x = np.clip(x, self.lb, self.ub)\n            \n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.evals += self.popsize\n\n            if self.evals > self.budget:\n                f = f[:(self.budget - (self.evals - self.popsize))]\n                x = x[:, :(self.budget - (self.evals - self.popsize))]\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n                \n\n            idx = np.argsort(f)\n            x = x[:, idx]\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt.T, (xmean - self.mean) / self.sigma)\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.evals / self.popsize))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (xmean - self.mean) / self.sigma\n\n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n            c1 = 0.5 / (self.dim * self.mueff)\n            cmu = min(0.5, 0.5 * (self.mueff - 2 + 1/self.mueff) / ((self.dim+2)**2 + self.mueff))\n            \n            self.C = (1 - c1 - cmu) * self.C + c1 * np.outer(self.pc, self.pc) + cmu * artmp @ np.diag(self.weights) @ artmp.T\n            \n            try:\n                eigenvalues, eigenvectors = np.linalg.eigh(self.C)\n                eigenvalues = np.clip(eigenvalues, 1e-8, None)\n                self.C = eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)\n\n            self.mean = xmean\n\n            # Adapt variances for each dimension\n            success_rate = np.mean((x[:self.mu] - self.mean[:, np.newaxis])**2, axis=1)\n            self.variances = self.variances * np.exp(self.learning_rate_variance * (success_rate - 1))\n            self.variances = np.clip(self.variances, 1e-6, 10)\n\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            self.sigma = max(self.sigma, 1e-10)  # Avoid sigma collapsing to zero\n            \n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "An exception occurred: operands could not be broadcast together with shapes (4,8) (5,1) .", "error": "", "parent_ids": ["6a70bc02-6af2-4024-a8f4-298b879d2669"], "operator": null, "metadata": {}}
{"id": "1d82e821-8ba5-4d56-8292-572305da6564", "fitness": -Infinity, "name": "OrthogonalCMAES", "description": "CMA-ES with orthogonal sampling, adaptive learning rate for both rank-one and rank-mu updates, and dynamic covariance matrix adaptation based on the condition number.", "code": "import numpy as np\n\nclass OrthogonalCMAES:\n    def __init__(self, budget=10000, dim=10, initial_sigma=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.initial_sigma = initial_sigma\n        self.popsize = self._adaptive_popsize()\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.mean = np.zeros(self.dim)\n        self.sigma = initial_sigma\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.lb = -5\n        self.ub = 5\n        self.evals = 0\n        self.c1 = 0.1 / (self.dim * self.mueff)  # Adaptive learning rate for rank-one update\n        self.cmu = min(0.1, 0.2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim+2)**2 + self.mueff))\n        self.c_cond = 10 # Condition number adaptation rate\n\n\n    def _adaptive_popsize(self):\n         # Dynamically adjust popsize based on dimension\n        return 4 + int(3 * np.log(self.dim))\n\n    def _orthogonal_sampling(self, n_samples):\n        # Generate orthogonal samples using a simplified approach\n        H = np.random.normal(0, 1, size=(self.dim, n_samples))\n        Q, _ = np.linalg.qr(H)\n        return Q\n    \n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n\n        while self.evals < self.budget:\n            # Orthogonal Sampling\n            z = self._orthogonal_sampling(self.popsize)\n            x = self.mean[:, np.newaxis] + self.sigma * np.linalg.cholesky(self.C) @ z\n\n            # Boundary Handling: Clipping\n            x = np.clip(x, self.lb, self.ub)\n\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.evals += self.popsize\n            \n            if self.evals > self.budget:\n                f = f[:(self.budget - (self.evals - self.popsize))]\n                x = x[:, :(self.budget - (self.evals - self.popsize))]\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n\n            idx = np.argsort(f)\n            x = x[:, idx]\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * (xmean - self.mean) / self.sigma\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.evals / self.popsize))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (xmean - self.mean) / self.sigma\n\n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n            \n            # Rank-one update\n            rank_one = np.outer(self.pc, self.pc)\n\n            # Rank-mu update\n            rank_mu = self.cmu * artmp @ np.diag(self.weights) @ artmp.T\n\n            # Adaptive learning rates\n            c1_adaptive = self.c1 * np.exp(self.c_cond * (np.linalg.cond(self.C) - 100) / 100)  # Increase c1 when C is ill-conditioned\n            cmu_adaptive = self.cmu * np.exp(-self.c_cond * (np.linalg.cond(self.C) - 100) / 100) # Decrease cmu when C is ill-conditioned\n\n            self.C = (1 - c1_adaptive - cmu_adaptive) * self.C + c1_adaptive * rank_one + cmu_adaptive * rank_mu\n\n            # Ensure C remains positive definite\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)  # Reset if not positive definite\n\n            self.mean = xmean\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            self.sigma = max(self.sigma, 1e-10)  # Avoid sigma collapsing to zero\n            self.c1 = min(0.4, self.c1 * 1.02) # Adaptive c1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "An exception occurred: index 2 is out of bounds for axis 1 with size 2.", "error": "", "parent_ids": ["af41a26f-f55f-4818-bcef-c7f42cac0ca4"], "operator": null, "metadata": {}}
{"id": "35f77000-5926-40d9-8276-6fefc92a732d", "fitness": -Infinity, "name": "OrthogonalRestartCMAES", "description": "CMA-ES with orthogonal sampling, adaptive learning rate for covariance matrix update, spectral correction of the covariance matrix, and restart mechanism based on stagnation detection.", "code": "import numpy as np\n\nclass OrthogonalRestartCMAES:\n    def __init__(self, budget=10000, dim=10, initial_sigma=0.5, restart_factor=2):\n        self.budget = budget\n        self.dim = dim\n        self.initial_sigma = initial_sigma\n        self.popsize = self._adaptive_popsize()\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.mean = np.zeros(self.dim)\n        self.sigma = initial_sigma\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.lb = -5\n        self.ub = 5\n        self.evals = 0\n        self.c1 = 0.1 / (self.dim * self.mueff)  # Adaptive learning rate for rank-one update\n        self.cmu = min(0.1, 0.2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim+2)**2 + self.mueff))\n        self.restart_factor = restart_factor\n        self.last_f_opt = np.Inf\n        self.stagnation_counter = 0\n        self.restart_iterations = 0\n\n\n    def _adaptive_popsize(self):\n        # Dynamically adjust popsize based on dimension\n        return 4 + int(3 * np.log(self.dim))\n\n    def _generate_orthogonal_samples(self, num_samples):\n        # Generate orthogonal samples using a simple approach\n        H = np.random.normal(0, 1, size=(self.dim, num_samples))\n        Q, _ = np.linalg.qr(H)\n        return Q\n    \n    def _spectral_correction(self):\n        try:\n            eigenvalues, eigenvectors = np.linalg.eigh(self.C)\n            eigenvalues = np.maximum(eigenvalues, 1e-10)  # Ensure positive definiteness\n            self.C = eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T\n        except np.linalg.LinAlgError:\n            self.C = np.eye(self.dim)\n\n    def _restart(self):\n        self.mean = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.sigma = self.initial_sigma\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.stagnation_counter = 0\n        self.restart_iterations += 1\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n        self.last_f_opt = np.Inf\n        self.stagnation_counter = 0\n        self.restart_iterations = 0\n\n        while self.evals < self.budget:\n            # Generate orthogonal samples\n            Q = self._generate_orthogonal_samples(self.popsize)\n            x = self.mean[:, np.newaxis] + self.sigma * np.linalg.cholesky(self.C) @ Q\n            \n            # Boundary Handling: Clipping\n            x = np.clip(x, self.lb, self.ub)\n\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.evals += self.popsize\n            \n            if self.evals > self.budget:\n                f = f[:(self.budget - (self.evals - self.popsize))]\n                x = x[:, :(self.budget - (self.evals - self.popsize))]\n                \n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n\n            # Stagnation Detection\n            if self.f_opt >= self.last_f_opt:\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n\n            if self.stagnation_counter > 50 * self.dim:\n                self._restart()\n\n            self.last_f_opt = self.f_opt\n\n            idx = np.argsort(f)\n            x = x[:, idx]\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * (xmean - self.mean) / self.sigma\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.evals / self.popsize))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (xmean - self.mean) / self.sigma\n\n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n            \n            # Rank-one update for exploration\n            rank_one = np.outer(self.pc, self.pc)\n            self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * rank_one + self.cmu * artmp @ np.diag(self.weights) @ artmp.T\n\n            # Spectral Correction to ensure positive definite C\n            self._spectral_correction()\n            \n            self.mean = xmean\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            self.sigma = max(self.sigma, 1e-10)  # Avoid sigma collapsing to zero\n            self.c1 = min(0.4, self.c1 * 1.02) # Adaptive c1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "An exception occurred: index 2 is out of bounds for axis 1 with size 2.", "error": "", "parent_ids": ["af41a26f-f55f-4818-bcef-c7f42cac0ca4"], "operator": null, "metadata": {}}
{"id": "6c2baa80-4c5e-455b-b514-53ad313755aa", "fitness": 0.6611252237776964, "name": "MirroredRankOneCMAES", "description": "CMA-ES with mirrored sampling, simplified rank-one update, adaptive learning rate and sigma damping based on trace of covariance matrix.", "code": "import numpy as np\n\nclass MirroredRankOneCMAES:\n    def __init__(self, budget=10000, dim=10, initial_sigma=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.initial_sigma = initial_sigma\n        self.popsize = self._adaptive_popsize()\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.mean = np.zeros(self.dim)\n        self.sigma = initial_sigma\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.lb = -5\n        self.ub = 5\n        self.evals = 0\n        self.c1 = 0.1 / (self.dim * self.mueff) # Adaptive learning rate for rank-one update\n\n\n    def _adaptive_popsize(self):\n         # Dynamically adjust popsize based on dimension\n        return 4 + int(3 * np.log(self.dim))\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n\n        while self.evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            \n            # Mirrored Sampling\n            z_mirrored = np.concatenate([z, -z], axis=1)\n            x = self.mean[:, np.newaxis] + self.sigma * np.linalg.cholesky(self.C) @ z_mirrored\n            \n            # Boundary Handling: Clipping\n            x = np.clip(x, self.lb, self.ub)\n\n            f = np.array([func(x[:, i]) for i in range(2*self.popsize)])\n            self.evals += 2*self.popsize\n            \n            if self.evals > self.budget:\n                f = f[:(self.budget - (self.evals - 2*self.popsize))]\n                x = x[:, :(self.budget - (self.evals - 2*self.popsize))]\n                \n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n\n            idx = np.argsort(f)\n            x = x[:, idx]\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * (xmean - self.mean) / self.sigma\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.evals / (2*self.popsize)))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (xmean - self.mean) / self.sigma\n\n            # Rank-one update for exploration\n            self.C = (1 - self.c1) * self.C + self.c1 * np.outer(self.pc, self.pc)\n\n\n            # Ensure C remains positive definite, simplified\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)  # Reset if not positive definite\n\n            self.mean = xmean\n            \n            # Sigma damping based on trace of C\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1) + 0.1 * (np.trace(self.C) - self.dim)) #damping with trace\n\n            self.sigma = max(self.sigma, 1e-10)  # Avoid sigma collapsing to zero\n            self.c1 = min(0.4, self.c1 * 1.02) # Adaptive c1\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm MirroredRankOneCMAES scored 0.661 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["af41a26f-f55f-4818-bcef-c7f42cac0ca4"], "operator": null, "metadata": {"aucs": [0.2511332818519163, 0.17975545820366712, 0.9357932844314523, 0.9583242069888847, 0.9431579914463549, 0.9535659714829682, 0.3139598462082196, 0.9389938664589456, 0.9448326162149159, 0.17770787287495826, 0.9579470608435074, 0.9887728586373625, 0.249170664215554, 0.9467340381456137, 0.7867928095323506, 0.381994522451779, 0.6114328551032685, 0.9596055231781213, 0.2342529141245865, 0.5085768331595029]}}
{"id": "1fc02f45-34ff-4cfc-9667-6b2babce701f", "fitness": 0.7216964589295599, "name": "MirroredRankOneCMAES", "description": "Simplified Mirrored CMA-ES with adaptive learning rates, rank-one updates, and boundary handling for efficient exploration and exploitation.", "code": "import numpy as np\n\nclass MirroredRankOneCMAES:\n    def __init__(self, budget=10000, dim=10, initial_sigma=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.initial_sigma = initial_sigma\n        self.popsize = 4 + int(3 * np.log(self.dim))  # Adaptive popsize\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.mean = np.zeros(self.dim)\n        self.sigma = initial_sigma\n        self.lb = -5\n        self.ub = 5\n        self.evals = 0\n        self.c1 = 0.1 / (self.dim * self.mueff)\n        self.cmu = min(0.1, 0.2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim+2)**2 + self.mueff))\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.ps = np.zeros(self.dim)\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n\n        while self.evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            z_mirrored = np.concatenate([z, -z], axis=1)\n            x = self.mean[:, np.newaxis] + self.sigma * np.linalg.cholesky(self.C) @ z_mirrored\n            x = np.clip(x, self.lb, self.ub)\n            f = np.array([func(x[:, i]) for i in range(2*self.popsize)])\n            self.evals += 2*self.popsize\n            \n            if self.evals > self.budget:\n                f = f[:(self.budget - (self.evals - 2*self.popsize))]\n                x = x[:, :(self.budget - (self.evals - 2*self.popsize))]\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n\n            idx = np.argsort(f)\n            x = x[:, idx]\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * (xmean - self.mean) / self.sigma\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.evals / (2*self.popsize)))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (xmean - self.mean) / self.sigma\n\n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n            rank_one = np.outer(self.pc, self.pc)\n            self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * rank_one + self.cmu * artmp @ np.diag(self.weights) @ artmp.T\n\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)\n\n            self.mean = xmean\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            self.sigma = max(self.sigma, 1e-10)\n            self.c1 = min(0.4, self.c1 * 1.02)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm MirroredRankOneCMAES scored 0.722 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["af41a26f-f55f-4818-bcef-c7f42cac0ca4"], "operator": null, "metadata": {"aucs": [0.26445343832447876, 0.3957085041909417, 0.9189592649827595, 0.9573251214898775, 0.9315502865315995, 0.9399701669932238, 0.3299872359410514, 0.9207476510553797, 0.9348528580658457, 0.1838766312633845, 0.9575065459329934, 0.9947212728096804, 0.3553312738123542, 0.9205330832802671, 0.9503648364824129, 0.9304061512505649, 0.459260334114713, 0.947670971038474, 0.21866378572104272, 0.9220397653101535]}}
{"id": "693ae7b2-bdcc-463b-9eca-689b827c429b", "fitness": 0.14422820215565996, "name": "GradientCMAES", "description": "CMA-ES with a simplified covariance matrix adaptation using rank-one updates and adaptive coordinate-wise learning rates based on gradient information.", "code": "import numpy as np\n\nclass GradientCMAES:\n    def __init__(self, budget=10000, dim=10, initial_sigma=0.5, learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_sigma = initial_sigma\n        self.learning_rate = learning_rate\n        self.mean = np.zeros(self.dim)\n        self.sigma = initial_sigma\n        self.C = np.eye(self.dim)\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n        self.lb = -5.0\n        self.ub = 5.0\n        self.coordinate_lr = np.ones(self.dim) * learning_rate  # Individual learning rates for each dimension\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n\n        while self.evals < self.budget:\n            # Generate a single sample (exploration)\n            z = np.random.normal(0, 1, size=self.dim)\n            x = self.mean + self.sigma * np.sqrt(np.diag(self.C)) @ z  # Coordinate-wise scaling\n            x = np.clip(x, self.lb, self.ub)\n            \n            f = func(x)\n            self.evals += 1\n            \n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x\n\n            if self.evals >= self.budget:\n                break\n\n            # Estimate Gradient (using finite differences)\n            gradient = np.zeros(self.dim)\n            delta = 1e-4  # Small perturbation for gradient estimation\n            for i in range(self.dim):\n                x_plus = x.copy()\n                x_plus[i] += delta\n                x_plus = np.clip(x_plus, self.lb, self.ub)\n                f_plus = func(x_plus) # Evaluate function\n                self.evals += 1\n                if self.evals >= self.budget:\n                  break\n\n                gradient[i] = (f_plus - f) / delta\n\n            if self.evals >= self.budget:\n                break\n\n            # Update individual learning rates based on gradient information\n            self.coordinate_lr = self.learning_rate / (np.abs(gradient) + 1e-8) # Avoid division by zero\n\n            # Rank-One Update with Coordinate-wise Learning Rates\n            diff = (x - self.mean)\n            self.C = (1 - self.coordinate_lr) * self.C + np.outer(self.coordinate_lr * diff, diff) / (np.linalg.norm(diff)**2 + 1e-8)\n\n            # Ensure C remains positive definite (Spectral Correction)\n            try:\n                eigenvalues, eigenvectors = np.linalg.eigh(self.C)\n                eigenvalues = np.clip(eigenvalues, 1e-8, None)  # Clip small eigenvalues\n                self.C = eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)\n\n            # Update mean\n            self.mean = x\n\n            # Update sigma\n            self.sigma *= np.exp(0.5 * (np.linalg.norm(diff / self.sigma)**2 - self.dim) / self.dim)\n            self.sigma = max(self.sigma, 1e-10)  # Avoid sigma collapsing\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm GradientCMAES scored 0.144 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6a70bc02-6af2-4024-a8f4-298b879d2669"], "operator": null, "metadata": {"aucs": [0.04651351400021431, 0.09893678337151357, 0.2542250389495022, 0.10229567250652105, 0.09767385071871015, 0.15239323819233108, 0.09398766440533823, 0.14656085360921844, 0.13742071671624612, 0.10270130615014916, 0.12643305071641875, 0.12756649566798384, 0.2177584077179784, 0.09571461573477702, 0.12641720808823287, 0.22660763150428898, 0.10397158201797296, 0.14328337329954488, 0.09351430395955884, 0.3905887357866987]}}
{"id": "b0e4c31f-66a7-4bf5-b60d-3d39dbc46630", "fitness": 0.7442210862968539, "name": "MirroredRankOneCMAES", "description": "Mirrored CMA-ES with adaptive covariance update and selective pressure, accelerating convergence and maintaining diversity.", "code": "import numpy as np\n\nclass MirroredRankOneCMAES:\n    def __init__(self, budget=10000, dim=10, initial_sigma=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.initial_sigma = initial_sigma\n        self.popsize = self._adaptive_popsize()\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.mean = np.zeros(self.dim)\n        self.sigma = initial_sigma\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.lb = -5\n        self.ub = 5\n        self.evals = 0\n        self.c1 = 0.1 / (self.dim * self.mueff)  # Adaptive learning rate for rank-one update\n        self.cmu = min(0.1, 0.2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim+2)**2 + self.mueff))\n        self.seq_cutoff = int(0.9 * self.budget) # Sequential cutoff threshold\n        self.adapt_cmu = True\n\n    def _adaptive_popsize(self):\n         # Dynamically adjust popsize based on dimension\n        return 4 + int(3 * np.log(self.dim))\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n        historical_best_fitness = []\n\n        while self.evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            \n            # Mirrored Sampling\n            z_mirrored = np.concatenate([z, -z], axis=1)\n            x = self.mean[:, np.newaxis] + self.sigma * np.linalg.cholesky(self.C) @ z_mirrored\n            \n            # Boundary Handling: Clipping\n            x = np.clip(x, self.lb, self.ub)\n\n            f = np.array([func(x[:, i]) for i in range(2*self.popsize)])\n            self.evals += 2*self.popsize\n            \n            if self.evals > self.budget:\n                f = f[:(self.budget - (self.evals - 2*self.popsize))]\n                x = x[:, :(self.budget - (self.evals - 2*self.popsize))]\n                \n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n\n            idx = np.argsort(f)\n            x = x[:, idx]\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * (xmean - self.mean) / self.sigma\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.evals / (2*self.popsize)))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (xmean - self.mean) / self.sigma\n\n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n            \n            # Rank-one update for exploration\n            rank_one = np.outer(self.pc, self.pc)\n\n            # Adaptive cmu based on stagnation\n            if self.adapt_cmu:\n                historical_best_fitness.append(self.f_opt)\n                if len(historical_best_fitness) > 10:\n                    if np.std(historical_best_fitness[-10:]) < 1e-6 and self.evals < self.seq_cutoff:\n                        self.cmu = min(1, self.cmu * 1.2)  # Increase learning rate if stagnating\n                    elif self.cmu > 0.001:\n                         self.cmu = max(0.0001, self.cmu * 0.98) # Reduce cmu if not stagnating\n\n            self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * rank_one + self.cmu * artmp @ np.diag(self.weights) @ artmp.T\n\n            # Ensure C remains positive definite\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)  # Reset if not positive definite\n\n            self.mean = xmean\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            self.sigma = max(self.sigma, 1e-10)  # Avoid sigma collapsing to zero\n            self.c1 = min(0.4, self.c1 * 1.02) # Adaptive c1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm MirroredRankOneCMAES scored 0.744 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["af41a26f-f55f-4818-bcef-c7f42cac0ca4"], "operator": null, "metadata": {"aucs": [0.18679210793741619, 0.8964091023306864, 0.9288450552592391, 0.9580175019685991, 0.9284865886252849, 0.9429788815978427, 0.3764729733281078, 0.9215568153307249, 0.9304935184991717, 0.21674228439962318, 0.9464975977947797, 0.9929057606871204, 0.29116831905176477, 0.9271633269496521, 0.9267669789182719, 0.9285630880188932, 0.8751075833047032, 0.9555550383754323, 0.2137343526907346, 0.5401648508690273]}}
{"id": "17d4bba8-315d-4b8c-a40f-c7e1ea00c4d3", "fitness": 0.60373179785143, "name": "SpectralCMAES", "description": "Simplified Spectral CMA-ES with eigenvalue clipping, adaptive sigma damping based on the trace of C, and dynamic population size for efficient exploration and exploitation.", "code": "import numpy as np\n\nclass SpectralCMAES:\n    def __init__(self, budget=10000, dim=10, initial_sigma=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.initial_sigma = initial_sigma\n        self.popsize = self._adaptive_popsize()\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.mean = np.zeros(self.dim)\n        self.sigma = initial_sigma\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.lb = -5\n        self.ub = 5\n        self.evals = 0\n\n    def _adaptive_popsize(self):\n         # Dynamically adjust popsize based on dimension\n        return 4 + int(3 * np.log(self.dim))\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n\n        while self.evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            try:\n                C_sqrt = np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)\n                C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z\n            \n            # Boundary Handling: Clipping\n            x = np.clip(x, self.lb, self.ub)\n            \n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.evals += self.popsize\n\n            if self.evals > self.budget:\n                f = f[:(self.budget - (self.evals - self.popsize))]\n                x = x[:, :(self.budget - (self.evals - self.popsize))]\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n                \n\n            idx = np.argsort(f)\n            x = x[:, idx]\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt.T, (xmean - self.mean) / self.sigma)\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.evals / self.popsize))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (xmean - self.mean) / self.sigma\n\n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n            c1 = 0.5 / (self.dim * self.mueff)\n            cmu = min(0.5, 0.5 * (self.mueff - 2 + 1/self.mueff) / ((self.dim+2)**2 + self.mueff))\n            \n            self.C = (1 - c1 - cmu) * self.C + c1 * np.outer(self.pc, self.pc) + cmu * artmp @ np.diag(self.weights) @ artmp.T\n            \n            # Spectral correction using eigenvalue clipping\n            try:\n                eigenvalues, eigenvectors = np.linalg.eigh(self.C)\n                eigenvalues = np.clip(eigenvalues, 1e-8, None)\n                self.C = eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)\n            \n            self.mean = xmean\n\n            # Adaptive sigma damping based on trace of C\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            trace_C = np.trace(self.C)\n            self.sigma *= np.exp(0.1 * (1 - trace_C / self.dim)) # Dampen sigma if trace is too large\n            \n            self.sigma = max(self.sigma, 1e-10)  # Avoid sigma collapsing to zero\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm SpectralCMAES scored 0.604 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6a70bc02-6af2-4024-a8f4-298b879d2669"], "operator": null, "metadata": {"aucs": [0.14389743301470548, 0.20133783034502772, 0.7697926495974878, 0.9640157606649638, 0.865457888254964, 0.899967281486597, 0.33166979768289706, 0.5950663612194231, 0.8749733433021043, 0.18055756981875826, 0.935638965002915, 0.9903801829611251, 0.27760832198545626, 0.5240306964028726, 0.7358997829770139, 0.8249496361768512, 0.3669887617340374, 0.9338448234396735, 0.16890726577738713, 0.48965160518434336]}}
{"id": "0ac85630-b091-4387-90e5-a8dee9757543", "fitness": 0.7158376823031377, "name": "SpectralCMAES", "description": "Spectral CMA-ES with coordinate-wise learning rates, adaptive population sizing, and an added mechanism to restart the search when stagnation is detected.", "code": "import numpy as np\n\nclass SpectralCMAES:\n    def __init__(self, budget=10000, dim=10, initial_sigma=0.5, stagnation_threshold=100):\n        self.budget = budget\n        self.dim = dim\n        self.initial_sigma = initial_sigma\n        self.popsize = self._adaptive_popsize()\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.mean = np.zeros(self.dim)\n        self.sigma = initial_sigma\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.lb = -5\n        self.ub = 5\n        self.evals = 0\n        self.individual_sigma = np.ones(self.dim) * self.initial_sigma # Individual learning rates\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.previous_f_opt = np.Inf\n\n\n    def _adaptive_popsize(self):\n        return 4 + int(3 * np.log(self.dim)) # Dynamic population size\n\n    def _spectral_correction(self):\n        try:\n            eigenvalues, eigenvectors = np.linalg.eigh(self.C)\n            eigenvalues = np.clip(eigenvalues, 1e-8, None)\n            eigenvalues = eigenvalues * self.dim / np.sum(eigenvalues)\n            self.C = eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T\n        except np.linalg.LinAlgError:\n            self.C = np.eye(self.dim)\n\n    def _restart(self):\n        \"\"\"Restart the search from a new random location.\"\"\"\n        self.mean = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.sigma = self.initial_sigma\n        self.individual_sigma = np.ones(self.dim) * self.initial_sigma\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.stagnation_counter = 0\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n        self.stagnation_counter = 0\n        self.previous_f_opt = np.Inf\n\n        while self.evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.individual_sigma[:, np.newaxis] * C_sqrt @ z  # Coordinate-wise sigma\n\n            # Boundary Handling: Clip to bounds\n            x = np.clip(x, self.lb, self.ub)\n\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.evals += self.popsize\n\n            if self.evals > self.budget:\n                f = f[:(self.budget - (self.evals - self.popsize))]\n                x = x[:, :(self.budget - (self.evals - self.popsize))]\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n                self.stagnation_counter = 0  # Reset stagnation counter\n            else:\n                self.stagnation_counter += 1\n\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                self._restart()  # Restart if stagnating\n\n            idx = np.argsort(f)\n            x = x[:, idx]\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n\n            delta_mean = (xmean - self.mean)\n\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt.T, delta_mean / self.individual_sigma)\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.evals / self.popsize))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * delta_mean / self.individual_sigma\n\n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.individual_sigma[:, np.newaxis]\n            c1 = 0.5 / (self.dim * self.mueff)\n            cmu = min(0.5, 0.5 * (self.mueff - 2 + 1/self.mueff) / ((self.dim+2)**2 + self.mueff))\n\n            self.C = (1 - c1 - cmu) * self.C + c1 * np.outer(self.pc, self.pc) + cmu * artmp @ np.diag(self.weights) @ artmp.T\n\n            self._spectral_correction()\n            self.mean = xmean\n\n            # Update individual learning rates\n            self.individual_sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            self.individual_sigma = np.maximum(self.individual_sigma, 1e-10)\n\n            # Adapt population size\n            self.popsize = self._adaptive_popsize()\n            self.mu = self.popsize // 2\n            self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n            self.weights = self.weights / np.sum(self.weights)\n            self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n            self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n            self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm SpectralCMAES scored 0.716 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["316d7bf9-c203-4753-9c44-cb8cb6dd0672"], "operator": null, "metadata": {"aucs": [0.2343420560266375, 0.5939608119585965, 0.9288810245299913, 0.9638378818762604, 0.9437235234982861, 0.9478237957253245, 0.342648834624102, 0.5232182021987004, 0.9454556154423535, 0.4440866685128366, 0.967698312736185, 0.9936293542459749, 0.28844901259284395, 0.9371352386603456, 0.9629117565792512, 0.8302464517784843, 0.6516901389562977, 0.9595588720619863, 0.3364504091073972, 0.5210056849508989]}}
{"id": "f08359aa-e193-458b-995b-575e40569038", "fitness": -Infinity, "name": "OrthogonalCMAES", "description": "A CMA-ES variant that combines coordinate-wise learning rates with orthogonal subspace sampling for improved exploration and exploitation.", "code": "import numpy as np\n\nclass OrthogonalCMAES:\n    def __init__(self, budget=10000, dim=10, initial_sigma=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.initial_sigma = initial_sigma\n        self.popsize = self._adaptive_popsize()\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.mean = np.zeros(self.dim)\n        self.sigma = initial_sigma\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.lb = -5\n        self.ub = 5\n        self.evals = 0\n        self.individual_sigma = np.ones(self.dim) * self.initial_sigma # Individual learning rates\n\n    def _adaptive_popsize(self):\n        return 4 + int(3 * np.log(self.dim)) # Dynamic population size\n\n    def _spectral_correction(self):\n        try:\n            eigenvalues, eigenvectors = np.linalg.eigh(self.C)\n            eigenvalues = np.clip(eigenvalues, 1e-8, None)\n            eigenvalues = eigenvalues * self.dim / np.sum(eigenvalues)\n            self.C = eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T\n        except np.linalg.LinAlgError:\n            self.C = np.eye(self.dim)\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n\n        while self.evals < self.budget:\n            # Generate samples in orthogonal subspaces\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            Q, _ = np.linalg.qr(z)  # Orthogonal basis\n\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.individual_sigma[:, np.newaxis] * C_sqrt @ Q  # Coordinate-wise sigma and orthogonal sampling\n\n\n            # Boundary Handling: Clip to bounds\n            x = np.clip(x, self.lb, self.ub)\n\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.evals += self.popsize\n\n            if self.evals > self.budget:\n                f = f[:(self.budget - (self.evals - self.popsize))]\n                x = x[:, :(self.budget - (self.evals - self.popsize))]\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n\n            idx = np.argsort(f)\n            x = x[:, idx]\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n\n            delta_mean = (xmean - self.mean)\n\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt.T, delta_mean / self.individual_sigma)\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.evals / self.popsize))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * delta_mean / self.individual_sigma\n\n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.individual_sigma[:, np.newaxis]\n            c1 = 0.5 / (self.dim * self.mueff)\n            cmu = min(0.5, 0.5 * (self.mueff - 2 + 1/self.mueff) / ((self.dim+2)**2 + self.mueff))\n\n            self.C = (1 - c1 - cmu) * self.C + c1 * np.outer(self.pc, self.pc) + cmu * artmp @ np.diag(self.weights) @ artmp.T\n\n            self._spectral_correction()\n            self.mean = xmean\n\n            # Update individual learning rates\n            self.individual_sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            self.individual_sigma = np.maximum(self.individual_sigma, 1e-10)\n\n            # Adapt population size\n            self.popsize = self._adaptive_popsize()\n            self.mu = self.popsize // 2\n            self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n            self.weights = self.weights / np.sum(self.weights)\n            self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n            self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n            self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "An exception occurred: index 2 is out of bounds for axis 1 with size 2.", "error": "", "parent_ids": ["316d7bf9-c203-4753-9c44-cb8cb6dd0672"], "operator": null, "metadata": {}}
{"id": "047ba353-4131-4afc-a19f-2d66d4fc7d65", "fitness": 0.6079354622642745, "name": "SpectralCMAES", "description": "Spectral CMA-ES with dynamic population size, adaptive sigma decay based on eigenvalue spread and enhanced exploration via orthogonal subspace sampling.", "code": "import numpy as np\n\nclass SpectralCMAES:\n    def __init__(self, budget=10000, dim=10, initial_sigma=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.initial_sigma = initial_sigma\n        self.popsize = self._adaptive_popsize()\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.mean = np.zeros(self.dim)\n        self.sigma = initial_sigma\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.lb = -5\n        self.ub = 5\n        self.evals = 0\n        self.eigenvalues = None # store eigenvalues\n        self.eigenvectors = None # store eigenvectors\n\n    def _adaptive_popsize(self):\n         # Dynamically adjust popsize based on dimension\n        return 4 + int(3 * np.log(self.dim))\n\n    def _spectral_correction(self):\n        try:\n            # Eigendecomposition\n            eigenvalues, eigenvectors = np.linalg.eigh(self.C)\n            \n            # Clip eigenvalues to avoid negative eigenvalues\n            eigenvalues = np.clip(eigenvalues, 1e-8, None)\n            \n            # Store eigenvalues for sigma adaptation\n            self.eigenvalues = eigenvalues\n            self.eigenvectors = eigenvectors\n            \n            # Reconstruct C using the clipped spectrum\n            self.C = eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T\n        except np.linalg.LinAlgError:\n            # Handle potential errors during eigendecomposition\n            self.C = np.eye(self.dim)\n\n    def _orthogonal_sampling(self, num_points):\n        if self.eigenvectors is None:\n            return np.random.normal(0, 1, size=(self.dim, num_points))\n\n        # Sample in the subspace orthogonal to the first eigenvector (most important direction)\n        subspace_basis = self.eigenvectors[:, 1:]  # All eigenvectors except the first\n        z = np.random.normal(0, 1, size=(subspace_basis.shape[1], num_points))\n        return subspace_basis @ z\n        \n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.evals = 0\n\n        while self.evals < self.budget:\n            # Standard CMA-ES sampling\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize // 2))\n\n            # Orthogonal Subspace Sampling\n            z_orth = self._orthogonal_sampling(self.popsize // 2)\n            \n            # Combine both sampling strategies\n            z = np.concatenate((z, z_orth), axis=1)\n\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.mean[:, np.newaxis] + self.sigma * C_sqrt @ z\n            \n            # Boundary Handling: Clipping\n            x = np.clip(x, self.lb, self.ub)\n            \n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.evals += self.popsize\n\n            if self.evals > self.budget:\n                f = f[:(self.budget - (self.evals - self.popsize))]\n                x = x[:, :(self.budget - (self.evals - self.popsize))]\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[:, np.argmin(f)]\n                \n\n            idx = np.argsort(f)\n            x = x[:, idx]\n            xmean = np.sum(x[:, :self.mu] * self.weights[np.newaxis, :], axis=1)\n\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(C_sqrt.T, (xmean - self.mean) / self.sigma)\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.evals / self.popsize))) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.damps) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (xmean - self.mean) / self.sigma\n\n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n            c1 = 0.5 / (self.dim * self.mueff)\n            cmu = min(0.5, 0.5 * (self.mueff - 2 + 1/self.mueff) / ((self.dim+2)**2 + self.mueff))\n            \n            self.C = (1 - c1 - cmu) * self.C + c1 * np.outer(self.pc, self.pc) + cmu * artmp @ np.diag(self.weights) @ artmp.T\n            \n            self._spectral_correction() # Apply Spectral Correction\n            \n            self.mean = xmean\n\n            # Adaptive sigma decay based on eigenvalue spread\n            if self.eigenvalues is not None:\n                eigenvalue_spread = np.max(self.eigenvalues) / np.min(self.eigenvalues)\n                sigma_decay = 1 + 0.05 * np.log10(eigenvalue_spread)  # Reduced factor\n                self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1)) / sigma_decay\n            else:\n                self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n                \n            self.sigma = max(self.sigma, 1e-10)  # Avoid sigma collapsing to zero\n            self.popsize = self._adaptive_popsize() #Dynamically adjusting population size.\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm SpectralCMAES scored 0.608 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6a70bc02-6af2-4024-a8f4-298b879d2669"], "operator": null, "metadata": {"aucs": [0.22856014610190145, 0.2926866691498645, 0.4735308962108351, 0.4434862854181121, 0.8301446859106336, 0.960208784568144, 0.3090006389896228, 0.8133552722143793, 0.8883344232336321, 0.1961606338320031, 0.9755791233791746, 0.9974102935434682, 0.2634671782008764, 0.9241401356230922, 0.9562327953378891, 0.334778927223359, 0.6463490954950794, 0.9585747718432566, 0.19938692461116414, 0.46732156439900097]}}
