{"id": "f7698668-9384-469f-95b8-0b80f1d4cb1a", "fitness": 0.1294730090121441, "name": "FastCMAES", "description": "Covariance Matrix Adaptation Evolution Strategy (CMA-ES) with restarts and a simplified update rule for faster convergence.", "code": "import numpy as np\n\nclass FastCMAES:\n    def __init__(self, budget=10000, dim=10, mu_percentage=0.25, sigma0=0.5, cs=0.2, cmu=0.3, c_cov=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.mu = int(mu_percentage * budget)  # Number of parents\n        self.sigma = sigma0\n        self.mean = None\n        self.C = None  # Covariance matrix\n        self.ps = None # Evolution path for sigma\n        self.pc = None # Evolution path for mean\n        self.cs = cs\n        self.cmu = cmu\n        self.c_cov = c_cov\n        self.restart_criterion = 1e-12\n\n    def __call__(self, func):\n        self.mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.f_opt = np.Inf\n        self.x_opt = None\n        evals = 0\n\n        while evals < self.budget:\n            # 1. Sample offspring\n            z = np.random.multivariate_normal(np.zeros(self.dim), self.C, size=self.mu)\n            x = self.mean + self.sigma * z\n            \n            # Clip to bounds\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            \n            f = np.array([func(xi) for xi in x])\n            evals += self.mu\n\n            # 2. Selection and Recombination\n            idx = np.argsort(f)\n            x_best = x[idx[:self.mu]]\n            z_best = z[idx[:self.mu]]\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[idx[0]]\n\n            # 3. Update mean\n            mean_diff = np.mean(z_best, axis=0)\n            self.pc = (1 - self.cs) * self.pc + np.sqrt(self.cs * (2 - self.cs)) * mean_diff\n            self.mean = self.mean + self.cmu * self.sigma * self.pc\n\n            # 4. Update covariance matrix\n            self.ps = (1 - self.c_cov) * self.ps + np.sqrt(self.c_cov * (2 - self.c_cov)) * mean_diff\n            self.C = (1- self.c_cov) * self.C + self.c_cov * (np.outer(self.ps, self.ps) - self.C)\n\n            # 5. Update step size\n            self.sigma *= np.exp( (self.cs / 2) * (np.linalg.norm(self.ps)**2 / self.dim - 1) )\n            \n            # Check for covariance matrix deterioration\n            if np.linalg.det(self.C) < self.restart_criterion or not np.all(np.linalg.eigvals(self.C) > 0):\n              self.C = np.eye(self.dim) # restart C\n              self.sigma = 0.5 # Reset sigma \n              self.ps = np.zeros(self.dim)\n              self.pc = np.zeros(self.dim)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm FastCMAES scored 0.129 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.12594657584177626, 0.09409051101122079, 0.19287185180869904, 0.11787850603036076, 0.08897196089768533, 0.14859571380276515, 0.1887708906986274, 0.05115656652154432, 0.02060636103776292, 0.14345524778615348, 0.09751446766700644, 0.14911123282315986, 0.28324397045682903, 0.0335604936746684, 0.1395930494383918, 0.2887645164140473, 0.08584286557675591, 0.10265790176879086, 0.10845435473433696, 0.12837314225229945]}}
{"id": "fe3d064d-8b85-4b87-8599-807479acb026", "fitness": 0.2068829377220006, "name": "CMAES", "description": "Covariance matrix adaptation evolution strategy with a restart mechanism based on stagnation detection.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damps=None, c_cov=None, sigma0=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n\n        self.mu = self.popsize // 2\n\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n\n        self.m = None\n        self.sigma = None\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.eigenspace = None\n        self.eigenvalues = None\n\n        self.cs = cs\n        self.damps = damps if damps is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1)/(self.dim + 1)) - 1]) + self.cs\n        self.c_cov = c_cov if c_cov is not None else (1 / (self.dim * np.sqrt(self.dim))) * 10\n        self.c_cov_mu = self.c_cov\n        self.c_cov_mu = self.c_cov_mu if self.c_cov_mu <= 1 else 1\n        self.c_cov_mu = self.c_cov_mu if self.c_cov_mu > 0 else 0\n        self.c_cov = (1 / self.mu) * self.c_cov\n\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.func_evals = 0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 50  # Number of iterations to wait before restart\n\n    def initialize(self):\n        self.m = np.random.uniform(-2, 2, size=self.dim)\n        self.sigma = self.sigma0\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.eigenspace = np.eye(self.dim)\n        self.eigenvalues = np.ones(self.dim)\n\n    def sample(self):\n        z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n        x = self.m[:, np.newaxis] + self.sigma * (self.eigenspace @ (np.diag(np.sqrt(self.eigenvalues)) @ z))\n        return x\n\n    def update(self, x, fitness_values):\n        idx = np.argsort(fitness_values)\n        x_mu = x[:, idx[:self.mu]]\n        z_mu = np.linalg.solve(self.eigenspace @ np.diag(np.sqrt(self.eigenvalues)), (x_mu - self.m[:, np.newaxis]) / self.sigma)\n\n        m_old = self.m.copy()\n        self.m = np.sum(x_mu * self.weights[np.newaxis, :], axis=1)\n        self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * np.sum(self.weights)) * (self.eigenspace @ z_mu.mean(axis=1))\n        self.pc = (1 - self.c_cov) * self.pc + np.sqrt(self.c_cov * (2 - self.c_cov) * np.sum(self.weights)) * ((self.m - m_old) / self.sigma)\n        \n        C_temp = self.c_cov_mu * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :]) + \\\n               (1 - self.c_cov_mu) * (self.C)\n\n        self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / np.sqrt(self.dim) - 1))\n\n        self.C = C_temp\n\n        self.eigenvalues, self.eigenspace = np.linalg.eigh(self.C)\n        self.eigenvalues = np.maximum(self.eigenvalues, 1e-12)\n\n    def __call__(self, func):\n        self.initialize()\n\n        while self.func_evals < self.budget:\n            x = self.sample()\n\n            fitness_values = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.func_evals += self.popsize\n\n            best_index = np.argmin(fitness_values)\n            if fitness_values[best_index] < self.f_opt:\n                self.f_opt = fitness_values[best_index]\n                self.x_opt = x[:, best_index]\n                self.stagnation_counter = 0  # Reset counter if improvement\n\n            self.update(x, fitness_values)\n            self.stagnation_counter += 1\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.initialize()  # Restart CMA-ES\n                self.stagnation_counter = 0  # Reset counter\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm CMAES scored 0.207 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.0505457431128854, 0.1634585623055227, 0.2596673992354227, 0.13518936522512082, 0.13134607729075976, 0.14142431887045004, 0.18858856301945937, 0.1607759011883212, 0.17666118088482852, 0.128282199024098, 0.1631083083706999, 0.18728277012223038, 0.2722249869356581, 0.20262455115921152, 0.5433346691782366, 0.27216502559630895, 0.16649941846848504, 0.2223033719490629, 0.15076516235480542, 0.42141118014844436]}}
{"id": "3e46f135-d75c-402a-8111-8a1795a826c5", "fitness": 0.26165639114552736, "name": "SlimeMoldOptimizer", "description": "A population-based algorithm inspired by the behavior of slime mold, adapting step sizes based on fitness improvement.", "code": "import numpy as np\n\nclass SlimeMoldOptimizer:\n    def __init__(self, budget=10000, dim=10, population_size=20, initial_step_size=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.initial_step_size = initial_step_size\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.population_size # Subtract initial population evals from budget\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n        \n        step_size = np.full(self.population_size, self.initial_step_size)\n        \n        while self.budget > 0:\n            # Update weights based on fitness (simulating slime mold behavior)\n            weights = np.exp(-np.abs(fitness - self.f_opt) / (np.mean(np.abs(fitness - self.f_opt)) + 1e-8))\n            weights /= np.sum(weights)\n\n            # Update positions\n            for i in range(self.population_size):\n                if self.budget <=0:\n                  break\n                # Select a random individual from the population, weighted by fitness\n                j = np.random.choice(self.population_size, p=weights)\n                \n                # Slime mold update rule with adaptive step size\n                direction = population[j] - population[i]\n                new_position = population[i] + step_size[i] * direction * np.random.uniform(-1, 1, self.dim)\n                \n                # Clip to bounds\n                new_position = np.clip(new_position, lb, ub)\n                \n                new_fitness = func(new_position)\n                self.budget -= 1\n                \n                # Update if better\n                if new_fitness < fitness[i]:\n                    fitness[i] = new_fitness\n                    population[i] = new_position\n                    \n                    # Adjust step size (increase if improvement, decrease if not)\n                    step_size[i] *= 1.2  # Increase step size\n                    if step_size[i] > 1.0:\n                        step_size[i] = 1.0 #Limit step size\n                else:\n                    step_size[i] *= 0.8 #Reduce Step size\n\n                # Update global best\n                if fitness[i] < self.f_opt:\n                    self.f_opt = fitness[i]\n                    self.x_opt = population[i]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm SlimeMoldOptimizer scored 0.262 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1268178776074914, 0.2017098458808999, 0.26835411423022426, 0.16787745515065855, 0.17432454173448275, 0.16934535853732158, 0.2186059584504466, 0.16215337507318472, 0.15614143357021093, 0.16257036660529556, 0.1787122538931124, 0.9902389198809957, 0.30402773680774464, 0.18670643822405086, 0.5404381573049304, 0.22290214333426805, 0.21902840344351193, 0.20324331520299, 0.15337784422664025, 0.4265522837520862]}}
{"id": "177e5e85-e9a9-4749-849b-a0ee37e1a69e", "fitness": 0.6178090407893826, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with a population of archive solutions and restart mechanism for exploration.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=100, F=0.5, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.CR = CR\n        self.population = None\n        self.fitness = None\n        self.archive = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        \n    def mutate(self, x_i, func):\n        indices = np.random.choice(self.pop_size + self.archive_size, size=3, replace=False)\n        \n        if indices[0] < self.pop_size:\n            x_r1 = self.population[indices[0]]\n        else:\n            x_r1 = self.archive[indices[0] - self.pop_size]\n\n        if indices[1] < self.pop_size:\n            x_r2 = self.population[indices[1]]\n        else:\n            x_r2 = self.archive[indices[1] - self.pop_size]\n            \n        if indices[2] < self.pop_size:\n            x_r3 = self.population[indices[2]]\n        else:\n            x_r3 = self.archive[indices[2] - self.pop_size]\n\n        return x_r1 + self.F * (x_r2 - x_r3)\n\n    def crossover(self, x_i, v_i):\n        u_i = np.copy(x_i)\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() <= self.CR or j == j_rand:\n                u_i[j] = v_i[j]\n        return u_i\n\n    def repair(self, x, func):\n         return np.clip(x, func.bounds.lb, func.bounds.ub)\n\n    def update_archive(self):\n        # Randomly replace archive members with population members\n        indices = np.random.choice(self.archive_size, size=self.pop_size, replace=False)\n        self.archive[indices] = self.population\n\n    def restart(self, func):\n        # Restart the population with new random solutions\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n    \n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n\n        iteration = 0\n        while self.budget > 0:\n            iteration += 1\n            for i in range(self.pop_size):\n                # Mutation\n                v_i = self.mutate(self.population[i], func)\n                \n                # Crossover\n                u_i = self.crossover(self.population[i], v_i)\n                \n                # Repair\n                u_i = self.repair(u_i, func)\n\n                # Selection\n                f_u_i = func(u_i)\n                self.budget -= 1\n                if f_u_i < self.fitness[i]:\n                    self.population[i] = u_i\n                    self.fitness[i] = f_u_i\n\n                    if f_u_i < self.f_opt:\n                        self.f_opt = f_u_i\n                        self.x_opt = u_i\n\n            self.update_archive()\n\n            # Restart mechanism (every 100 iterations)\n            if iteration % 100 == 0 and self.budget > self.pop_size:\n                self.restart(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDE scored 0.618 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.2262560878793075, 0.5397453843531754, 0.5555429738902369, 0.8490809201494888, 0.6569083396807172, 0.6873010531962619, 0.48632046742499235, 0.5388744824981516, 0.6443829228470431, 0.5567720503069087, 0.8527656998323667, 0.9938334158658497, 0.5594901496432172, 0.6289984535354824, 0.887061273802866, 0.688530030003218, 0.5010881332956207, 0.7734050120579645, 0.21223939117911128, 0.5175845743456715]}}
{"id": "e0865747-4aee-46f3-8bd9-db6ccf892c40", "fitness": -Infinity, "name": "AntColonyOptimizer", "description": "Bio-inspired algorithm mimicking the foraging behavior of ants, using pheromone trails and evaporation to guide the search.", "code": "import numpy as np\n\nclass AntColonyOptimizer:\n    def __init__(self, budget=10000, dim=10, num_ants=20, evaporation_rate=0.1, pheromone_influence=1.0, random_influence=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.num_ants = num_ants\n        self.evaporation_rate = evaporation_rate\n        self.pheromone_influence = pheromone_influence\n        self.random_influence = random_influence\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        # Initialize pheromone trail\n        pheromone = np.ones(self.dim) * 0.001\n\n        # Initialize best solution\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n        for i in range(self.budget // self.num_ants):  # Iterate until budget is exhausted\n            \n            ant_positions = np.zeros((self.num_ants, self.dim))\n            ant_fitness = np.zeros(self.num_ants)\n            \n            #Ant colony exploration\n            for ant in range(self.num_ants):\n                position = np.random.uniform(lb, ub, self.dim)\n\n                #Pheromone guided movement\n                for d in range(self.dim):\n                    probability = (self.pheromone_influence * pheromone[d] + self.random_influence)\n                    if np.random.rand() < probability :\n                      position[d] = np.random.uniform(lb, ub)\n                    else:\n                      #Move in direction of best solution, only slightly\n                      if self.x_opt is not None:\n                          position[d] += np.random.normal(0, 0.01) * (self.x_opt[d] - position[d])\n                \n                position = np.clip(position, lb, ub) #Bounds check\n\n                ant_positions[ant] = position\n                ant_fitness[ant] = func(position)\n                \n                if ant_fitness[ant] < self.f_opt:\n                    self.f_opt = ant_fitness[ant]\n                    self.x_opt = position\n\n            # Update pheromone trails\n            pheromone *= (1 - self.evaporation_rate)\n            \n            #Deposit pheromones proportional to the quality of the solutions\n            for ant in range(self.num_ants):\n                pheromone += (1 / (1 + ant_fitness[ant])) #Better solutions increase pheromone more\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "An exception occured: TypeError: only size-1 arrays can be converted to Python scalars\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 36, in __call__\nValueError: setting an array element with a sequence.\n.", "error": "", "parent_ids": ["3e46f135-d75c-402a-8111-8a1795a826c5"], "operator": null, "metadata": {}}
{"id": "91f548dd-15ac-4a83-8343-116799adfd91", "fitness": 0.0, "name": "HybridCMAES", "description": "A CMA-ES variant that incorporates a population-based exploration strategy by perturbing the mean vector with differential evolution-like moves.", "code": "import numpy as np\n\nclass HybridCMAES:\n    def __init__(self, budget=10000, dim=10, mu_percentage=0.25, sigma0=0.5, cs=0.2, cmu=0.3, c_cov=0.1, de_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.mu = int(mu_percentage * budget)  # Number of parents\n        self.sigma = sigma0\n        self.mean = None\n        self.C = None  # Covariance matrix\n        self.ps = None # Evolution path for sigma\n        self.pc = None # Evolution path for mean\n        self.cs = cs\n        self.cmu = cmu\n        self.c_cov = c_cov\n        self.de_rate = de_rate\n        self.restart_criterion = 1e-12\n        self.population = None # current population\n\n    def __call__(self, func):\n        self.mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.f_opt = np.Inf\n        self.x_opt = None\n        evals = 0\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.mu, self.dim))\n        f_vals = np.array([func(xi) for xi in self.population])\n        evals += self.mu\n        \n        while evals < self.budget:\n            # 1. Sample offspring\n            z = np.random.multivariate_normal(np.zeros(self.dim), self.C, size=self.mu)\n            x = self.mean + self.sigma * z\n            \n            # Clip to bounds\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            \n            f = np.array([func(xi) for xi in x])\n            evals += self.mu\n\n            # Incorporate population diversity using DE\n            for i in range(self.mu):\n                if np.random.rand() < self.de_rate:\n                    idxs = np.random.choice(self.mu, 3, replace=False)\n                    x_diff = self.population[idxs[1]] - self.population[idxs[2]]\n                    x[i] = self.population[idxs[0]] + 0.5 * x_diff # DE mutation\n                    x[i] = np.clip(x[i], func.bounds.lb, func.bounds.ub)\n                    f[i] = func(x[i])\n                    evals += 1\n\n\n            # 2. Selection and Recombination\n            idx = np.argsort(f)\n            x_best = x[idx[:self.mu]]\n            z_best = z[idx[:self.mu]]\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[idx[0]]\n\n            # 3. Update mean\n            mean_diff = np.mean(z_best, axis=0)\n            self.pc = (1 - self.cs) * self.pc + np.sqrt(self.cs * (2 - self.cs)) * mean_diff\n            self.mean = self.mean + self.cmu * self.sigma * self.pc\n\n            # 4. Update covariance matrix\n            self.ps = (1 - self.c_cov) * self.ps + np.sqrt(self.c_cov * (2 - self.c_cov)) * mean_diff\n            self.C = (1- self.c_cov) * self.C + self.c_cov * (np.outer(self.ps, self.ps) - self.C)\n\n            # 5. Update step size\n            self.sigma *= np.exp( (self.cs / 2) * (np.linalg.norm(self.ps)**2 / self.dim - 1) )\n\n            # 6. Update population\n            self.population = x_best.copy()\n            \n            # Check for covariance matrix deterioration\n            if np.linalg.det(self.C) < self.restart_criterion or not np.all(np.linalg.eigvals(self.C) > 0):\n              self.C = np.eye(self.dim) # restart C\n              self.sigma = 0.5 # Reset sigma \n              self.ps = np.zeros(self.dim)\n              self.pc = np.zeros(self.dim)\n              self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.mu, self.dim))\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm HybridCMAES scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f7698668-9384-469f-95b8-0b80f1d4cb1a"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "9b7fbb19-af06-43e3-b49e-81a7b00fdce4", "fitness": -Infinity, "name": "CMAES_LocalSearch", "description": "A CMA-ES variant that incorporates a local search (Nelder-Mead) step periodically to refine promising solutions.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass CMAES_LocalSearch:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damps=None, c_cov=None, sigma0=0.2, local_search_interval=50):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n\n        self.mu = self.popsize // 2\n\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n\n        self.m = None\n        self.sigma = None\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.eigenspace = None\n        self.eigenvalues = None\n\n        self.cs = cs\n        self.damps = damps if damps is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1)/(self.dim + 1)) - 1]) + self.cs\n        self.c_cov = c_cov if c_cov is not None else (1 / (self.dim * np.sqrt(self.dim))) * 10\n        self.c_cov_mu = self.c_cov\n        self.c_cov_mu = self.c_cov_mu if self.c_cov_mu <= 1 else 1\n        self.c_cov_mu = self.c_cov_mu if self.c_cov_mu > 0 else 0\n        self.c_cov = (1 / self.mu) * self.c_cov\n        self.local_search_interval = local_search_interval\n\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.func_evals = 0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 50  # Number of iterations to wait before restart\n        self.bounds = np.array([[-5.0, 5.0]] * self.dim)\n\n\n    def initialize(self):\n        self.m = np.random.uniform(-2, 2, size=self.dim)\n        self.sigma = self.sigma0\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.eigenspace = np.eye(self.dim)\n        self.eigenvalues = np.ones(self.dim)\n\n    def sample(self):\n        z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n        x = self.m[:, np.newaxis] + self.sigma * (self.eigenspace @ (np.diag(np.sqrt(self.eigenvalues)) @ z))\n        return x\n\n    def update(self, x, fitness_values):\n        idx = np.argsort(fitness_values)\n        x_mu = x[:, idx[:self.mu]]\n        z_mu = np.linalg.solve(self.eigenspace @ np.diag(np.sqrt(self.eigenvalues)), (x_mu - self.m[:, np.newaxis]) / self.sigma)\n\n        m_old = self.m.copy()\n        self.m = np.sum(x_mu * self.weights[np.newaxis, :], axis=1)\n        self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * np.sum(self.weights)) * (self.eigenspace @ z_mu.mean(axis=1))\n        self.pc = (1 - self.c_cov) * self.pc + np.sqrt(self.c_cov * (2 - self.c_cov) * np.sum(self.weights)) * ((self.m - m_old) / self.sigma)\n        \n        C_temp = self.c_cov_mu * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :]) + \\\n               (1 - self.c_cov_mu) * (self.C)\n\n        self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / np.sqrt(self.dim) - 1))\n\n        self.C = C_temp\n\n        self.eigenvalues, self.eigenspace = np.linalg.eigh(self.C)\n        self.eigenvalues = np.maximum(self.eigenvalues, 1e-12)\n    \n    def local_search(self, func, x0):\n        res = minimize(func, x0, method='Nelder-Mead', bounds=self.bounds, options={'maxfev': self.budget - self.func_evals})\n        self.func_evals += res.nfev\n        return res.fun, res.x\n\n    def __call__(self, func):\n        self.initialize()\n\n        while self.func_evals < self.budget:\n            x = self.sample()\n\n            fitness_values = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.func_evals += self.popsize\n\n            best_index = np.argmin(fitness_values)\n            if fitness_values[best_index] < self.f_opt:\n                self.f_opt = fitness_values[best_index]\n                self.x_opt = x[:, best_index]\n                self.stagnation_counter = 0  # Reset counter if improvement\n\n            self.update(x, fitness_values)\n            self.stagnation_counter += 1\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.initialize()  # Restart CMA-ES\n                self.stagnation_counter = 0  # Reset counter\n            \n            if self.func_evals < self.budget and self.stagnation_counter % self.local_search_interval == 0:\n                f_local, x_local = self.local_search(func, self.x_opt)\n                if f_local < self.f_opt:\n                    self.f_opt = f_local\n                    self.x_opt = x_local\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 144, in evaluate\n    algorithm(f_new)\n  File \"<string>\", line 103, in __call__\n  File \"<string>\", line 76, in local_search\nNameError: name 'minimize' is not defined\n.", "error": "", "parent_ids": ["fe3d064d-8b85-4b87-8599-807479acb026"], "operator": null, "metadata": {}}
{"id": "d338a585-2633-4dbc-8394-e86c91272a4a", "fitness": -Infinity, "name": "AdaptiveDynamicDE", "description": "An adaptive Differential Evolution algorithm with a self-adaptive mutation factor and a dynamic population size.", "code": "import numpy as np\n\nclass AdaptiveDynamicDE:\n    def __init__(self, budget=10000, dim=10, pop_size_min=20, pop_size_max=100, archive_size=50, F_min=0.1, F_max=1.0, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_min = pop_size_min\n        self.pop_size_max = pop_size_max\n        self.pop_size = pop_size_max  # Initialize with the maximum population size\n        self.archive_size = archive_size\n        self.F_min = F_min\n        self.F_max = F_max\n        self.CR = CR\n        self.population = None\n        self.fitness = None\n        self.archive = None\n        self.best_fitness_history = []\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        \n    def adaptive_mutation_factor(self):\n        # Adjust F based on the recent history of fitness improvements.\n        if len(self.best_fitness_history) < 5:\n            return np.random.uniform(self.F_min, self.F_max) # Return a random value initially\n        \n        recent_improvements = [self.best_fitness_history[i] - self.best_fitness_history[i-1] for i in range(1, len(self.best_fitness_history))]\n        avg_improvement = np.mean(recent_improvements)\n        \n        # If improvement is high, use a smaller F for exploitation, else a larger F for exploration.\n        if avg_improvement > 0: # Higher value indicates decreasing fitness\n            F = self.F_min + (self.F_max - self.F_min) * np.exp(-avg_improvement * 10) # Exponential decay\n        else:\n            F = self.F_max\n            \n        return np.clip(F, self.F_min, self.F_max)\n\n    def mutate(self, x_i, func):\n        indices = np.random.choice(self.pop_size + self.archive_size, size=3, replace=False)\n        \n        if indices[0] < self.pop_size:\n            x_r1 = self.population[indices[0]]\n        else:\n            x_r1 = self.archive[indices[0] - self.pop_size]\n\n        if indices[1] < self.pop_size:\n            x_r2 = self.population[indices[1]]\n        else:\n            x_r2 = self.archive[indices[1] - self.pop_size]\n            \n        if indices[2] < self.pop_size:\n            x_r3 = self.population[indices[2]]\n        else:\n            x_r3 = self.archive[indices[2] - self.pop_size]\n\n        F = self.adaptive_mutation_factor()\n        return x_r1 + F * (x_r2 - x_r3)\n\n    def crossover(self, x_i, v_i):\n        u_i = np.copy(x_i)\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() <= self.CR or j == j_rand:\n                u_i[j] = v_i[j]\n        return u_i\n\n    def repair(self, x, func):\n         return np.clip(x, func.bounds.lb, func.bounds.ub)\n\n    def update_archive(self):\n        # Randomly replace archive members with population members\n        indices = np.random.choice(self.archive_size, size=self.pop_size, replace=False)\n        self.archive[indices] = self.population\n\n    def adjust_population_size(self):\n        # Dynamically adjust population size based on stagnation.\n        if len(self.best_fitness_history) > 10:\n            recent_improvements = [self.best_fitness_history[i] - self.best_fitness_history[i-1] for i in range(1, len(self.best_fitness_history))]\n            avg_improvement = np.mean(recent_improvements)\n            \n            if avg_improvement > -1e-6: # Stagnation detected (little or no recent improvement)\n                self.pop_size = max(self.pop_size_min, int(self.pop_size * 0.9))  # Reduce population\n            else:\n                self.pop_size = min(self.pop_size_max, int(self.pop_size * 1.1)) # Increase population, but no more than the maximum\n                \n            self.pop_size = int(self.pop_size)\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n        self.best_fitness_history.append(np.min(self.fitness))\n\n        iteration = 0\n        while self.budget > self.pop_size_min: #Ensure budget is large enough to prevent errors\n            iteration += 1\n\n            #Dynamic population adjustment\n            self.adjust_population_size()\n\n            # Resample if pop size changed\n            if self.pop_size != len(self.population):\n                if self.pop_size < len(self.population):\n                    indices = np.argsort(self.fitness)[:self.pop_size]  # Keep best\n                    self.population = self.population[indices]\n                    self.fitness = self.fitness[indices]\n                else:  #self.pop_size > len(self.population)\n                    num_new = self.pop_size - len(self.population)\n                    new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(num_new, self.dim))\n                    new_fitness = np.array([func(x) for x in new_individuals])\n                    self.budget -= num_new\n\n                    self.population = np.vstack((self.population, new_individuals))\n                    self.fitness = np.concatenate((self.fitness, new_fitness))\n\n            for i in range(self.pop_size):\n                # Mutation\n                v_i = self.mutate(self.population[i], func)\n                \n                # Crossover\n                u_i = self.crossover(self.population[i], v_i)\n                \n                # Repair\n                u_i = self.repair(u_i, func)\n\n                # Selection\n                f_u_i = func(u_i)\n                self.budget -= 1\n                if f_u_i < self.fitness[i]:\n                    self.population[i] = u_i\n                    self.fitness[i] = f_u_i\n\n                    if f_u_i < self.f_opt:\n                        self.f_opt = f_u_i\n                        self.x_opt = u_i\n\n            self.update_archive()\n            self.best_fitness_history.append(np.min(self.fitness))\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 144, in evaluate\n    algorithm(f_new)\n  File \"<string>\", line 140, in __call__\n  File \"<string>\", line 75, in update_archive\n  File \"mtrand.pyx\", line 984, in numpy.random.mtrand.RandomState.choice\nValueError: Cannot take a larger sample than population when 'replace=False'\n.", "error": "", "parent_ids": ["177e5e85-e9a9-4749-849b-a0ee37e1a69e"], "operator": null, "metadata": {}}
{"id": "34640842-adaa-4361-beed-f65ad8ea5481", "fitness": -Infinity, "name": "AdaptiveCMAES", "description": "A CMA-ES variant that adapts the population size during the search based on the success rate of new solutions and adjusts covariance matrix learning rate based on diversity.", "code": "import numpy as np\n\nclass AdaptiveCMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damps=None, c_cov=None, sigma0=0.2, adaptation_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.min_popsize = 4  # Minimum allowed population size\n        self.max_popsize = 50 # Maximum allowed population size\n        self.adaptation_rate = adaptation_rate # Rate at which popsize adapts\n\n        self.mu = self.popsize // 2\n\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n\n        self.m = None\n        self.sigma = None\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.eigenspace = None\n        self.eigenvalues = None\n\n        self.cs = cs\n        self.damps = damps if damps is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1)/(self.dim + 1)) - 1]) + self.cs\n        \n        self.c_cov = c_cov if c_cov is not None else (1 / (self.dim * np.sqrt(self.dim))) * 10\n        self.c_cov_mu = self.c_cov\n        self.c_cov_mu = self.c_cov_mu if self.c_cov_mu <= 1 else 1\n        self.c_cov_mu = self.c_cov_mu if self.c_cov_mu > 0 else 0\n        self.c_cov = (1 / self.mu) * self.c_cov\n\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.func_evals = 0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 50  # Number of iterations to wait before restart\n        self.success_rate = 0.0\n\n    def initialize(self):\n        self.m = np.random.uniform(-2, 2, size=self.dim)\n        self.sigma = self.sigma0\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.eigenspace = np.eye(self.dim)\n        self.eigenvalues = np.ones(self.dim)\n\n    def sample(self):\n        z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n        x = self.m[:, np.newaxis] + self.sigma * (self.eigenspace @ (np.diag(np.sqrt(self.eigenvalues)) @ z))\n        return x\n\n    def update(self, x, fitness_values):\n        idx = np.argsort(fitness_values)\n        x_mu = x[:, idx[:self.mu]]\n        z_mu = np.linalg.solve(self.eigenspace @ np.diag(np.sqrt(self.eigenvalues)), (x_mu - self.m[:, np.newaxis]) / self.sigma)\n\n        m_old = self.m.copy()\n        self.m = np.sum(x_mu * self.weights[np.newaxis, :], axis=1)\n        self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * np.sum(self.weights)) * (self.eigenspace @ z_mu.mean(axis=1))\n        self.pc = (1 - self.c_cov) * self.pc + np.sqrt(self.c_cov * (2 - self.c_cov) * np.sum(self.weights)) * ((self.m - m_old) / self.sigma)\n        \n        C_temp = self.c_cov_mu * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :]) + \\\n               (1 - self.c_cov_mu) * (self.C)\n        \n        # Diversity control: Reduce c_cov if the eigenvalues are too similar\n        eigenvalue_ratio = np.max(self.eigenvalues) / np.min(self.eigenvalues)\n        if eigenvalue_ratio > 1e3:\n            self.c_cov *= 0.9  # Reduce learning rate if diversity is low\n        else:\n            self.c_cov = c_cov if c_cov is not None else (1 / (self.dim * np.sqrt(self.dim))) * 10\n\n\n        self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / np.sqrt(self.dim) - 1))\n\n        self.C = C_temp\n\n        self.eigenvalues, self.eigenspace = np.linalg.eigh(self.C)\n        self.eigenvalues = np.maximum(self.eigenvalues, 1e-12)\n\n    def adapt_popsize(self, improvement):\n        if improvement:\n            self.success_rate = 0.9 * self.success_rate + 0.1\n        else:\n            self.success_rate = 0.9 * self.success_rate\n\n        if self.success_rate > 0.4 and self.popsize < self.max_popsize:\n            self.popsize = min(self.popsize + 1, self.max_popsize)\n            self.mu = self.popsize // 2\n            self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n            self.weights = self.weights / np.sum(self.weights)\n\n        elif self.success_rate < 0.1 and self.popsize > self.min_popsize:\n            self.popsize = max(self.popsize - 1, self.min_popsize)\n            self.mu = self.popsize // 2\n            self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n            self.weights = self.weights / np.sum(self.weights)\n\n    def __call__(self, func):\n        self.initialize()\n\n        while self.func_evals < self.budget:\n            x = self.sample()\n\n            fitness_values = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.func_evals += self.popsize\n\n            best_index = np.argmin(fitness_values)\n            improvement = False\n            if fitness_values[best_index] < self.f_opt:\n                self.f_opt = fitness_values[best_index]\n                self.x_opt = x[:, best_index]\n                self.stagnation_counter = 0  # Reset counter if improvement\n                improvement = True\n\n            self.update(x, fitness_values)\n            self.adapt_popsize(improvement)\n            self.stagnation_counter += 1\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.initialize()  # Restart CMA-ES\n                self.stagnation_counter = 0  # Reset counter\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 119, in __call__\n  File \"<string>\", line 74, in update\nNameError: name 'c_cov' is not defined\n.", "error": "", "parent_ids": ["fe3d064d-8b85-4b87-8599-807479acb026"], "operator": null, "metadata": {}}
{"id": "e881f5fb-fd03-480d-9ecb-a75e6e8815e6", "fitness": 0.1373656150691263, "name": "AdaptiveVarianceCMAES", "description": "Adaptive Variance Scaling CMA-ES: Dynamically adjusts the covariance matrix scaling based on recent search history and fitness improvement.", "code": "import numpy as np\n\nclass AdaptiveVarianceCMAES:\n    def __init__(self, budget=10000, dim=10, mu_percentage=0.25, sigma0=0.5, cs=0.2, cmu=0.3, c_cov=0.1, adaptation_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.mu = int(mu_percentage * budget)  # Number of parents\n        self.sigma = sigma0\n        self.mean = None\n        self.C = None  # Covariance matrix\n        self.ps = None  # Evolution path for sigma\n        self.pc = None  # Evolution path for mean\n        self.cs = cs\n        self.cmu = cmu\n        self.c_cov = c_cov\n        self.adaptation_rate = adaptation_rate\n        self.restart_criterion = 1e-12\n        self.fitness_history = []\n\n    def __call__(self, func):\n        self.mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.f_opt = np.Inf\n        self.x_opt = None\n        evals = 0\n\n        while evals < self.budget:\n            # 1. Sample offspring\n            z = np.random.multivariate_normal(np.zeros(self.dim), self.C, size=self.mu)\n            x = self.mean + self.sigma * z\n\n            # Clip to bounds\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n\n            f = np.array([func(xi) for xi in x])\n            evals += self.mu\n\n            # 2. Selection and Recombination\n            idx = np.argsort(f)\n            x_best = x[idx[:self.mu]]\n            z_best = z[idx[:self.mu]]\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[idx[0]]\n                self.fitness_history.append(self.f_opt)  # Keep track of best fitness\n\n            # 3. Update mean\n            mean_diff = np.mean(z_best, axis=0)\n            self.pc = (1 - self.cs) * self.pc + np.sqrt(self.cs * (2 - self.cs)) * mean_diff\n            self.mean = self.mean + self.cmu * self.sigma * self.pc\n\n            # 4. Update covariance matrix\n            self.ps = (1 - self.c_cov) * self.ps + np.sqrt(self.c_cov * (2 - self.c_cov)) * mean_diff\n            self.C = (1 - self.c_cov) * self.C + self.c_cov * (np.outer(self.ps, self.ps) - self.C)\n\n            # 5. Adaptive Variance Scaling\n            if len(self.fitness_history) > 1:\n                fitness_improvement = self.fitness_history[-2] - self.fitness_history[-1]\n                if fitness_improvement > 0:  #If there is improvement, increase the variance\n                    self.sigma *= (1 + self.adaptation_rate * fitness_improvement)\n                else: # Decrease the variance\n                    self.sigma *= (1 - self.adaptation_rate * abs(fitness_improvement))\n\n            # 6. Update step size (original)\n            self.sigma *= np.exp((self.cs / 2) * (np.linalg.norm(self.ps)**2 / self.dim - 1))\n\n            # Check for covariance matrix deterioration\n            if np.linalg.det(self.C) < self.restart_criterion or not np.all(np.linalg.eigvals(self.C) > 0):\n                self.C = np.eye(self.dim)  # restart C\n                self.sigma = 0.5  # Reset sigma\n                self.ps = np.zeros(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.fitness_history = [] # Reset fitness history\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveVarianceCMAES scored 0.137 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f7698668-9384-469f-95b8-0b80f1d4cb1a"], "operator": null, "metadata": {"aucs": [0.041937744122473086, 0.07844108049268328, 0.23951295450072552, 0.23007243035536395, 0.04033415447632993, 0.12465162900651139, 0.11945775531314451, 0.05734962512057995, 0.13364769898526752, 0.13084356300783928, 0.11465432856281277, 0.19136722025452857, 9.999999999998899e-05, 0.0988220248866497, 0.11874145815561532, 0.17117835634311995, 0.1397504685978942, 0.1446090874142224, 0.1295700588769123, 0.4422706629098526]}}
{"id": "94406ca9-c2bb-477b-9a7e-d0238b7f6385", "fitness": 0.11214739003972216, "name": "AdaptiveCMAES", "description": "An adaptive CMA-ES variant that adjusts its learning rates (cs, cmu, ccov) based on the observed diversity of the population to balance exploration and exploitation more effectively.", "code": "import numpy as np\n\nclass AdaptiveCMAES:\n    def __init__(self, budget=10000, dim=10, mu_percentage=0.25, sigma0=0.5, cs0=0.2, cmu0=0.3, c_cov0=0.1, diversity_threshold=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.mu = int(mu_percentage * budget)  # Number of parents\n        self.sigma = sigma0\n        self.mean = None\n        self.C = None  # Covariance matrix\n        self.ps = None # Evolution path for sigma\n        self.pc = None # Evolution path for mean\n        self.cs = cs0\n        self.cmu = cmu0\n        self.c_cov = c_cov0\n        self.cs0 = cs0\n        self.cmu0 = cmu0\n        self.c_cov0 = c_cov0\n        self.restart_criterion = 1e-12\n        self.diversity_threshold = diversity_threshold\n\n    def __call__(self, func):\n        self.mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.f_opt = np.Inf\n        self.x_opt = None\n        evals = 0\n\n        while evals < self.budget:\n            # 1. Sample offspring\n            z = np.random.multivariate_normal(np.zeros(self.dim), self.C, size=self.mu)\n            x = self.mean + self.sigma * z\n            \n            # Clip to bounds\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            \n            f = np.array([func(xi) for xi in x])\n            evals += self.mu\n\n            # 2. Selection and Recombination\n            idx = np.argsort(f)\n            x_best = x[idx[:self.mu]]\n            z_best = z[idx[:self.mu]]\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[idx[0]]\n\n            # 3. Update mean\n            mean_diff = np.mean(z_best, axis=0)\n            self.pc = (1 - self.cs) * self.pc + np.sqrt(self.cs * (2 - self.cs)) * mean_diff\n            self.mean = self.mean + self.cmu * self.sigma * self.pc\n\n            # 4. Update covariance matrix\n            self.ps = (1 - self.c_cov) * self.ps + np.sqrt(self.c_cov * (2 - self.c_cov)) * mean_diff\n            self.C = (1- self.c_cov) * self.C + self.c_cov * (np.outer(self.ps, self.ps) - self.C)\n\n            # 5. Update step size\n            self.sigma *= np.exp( (self.cs / 2) * (np.linalg.norm(self.ps)**2 / self.dim - 1) )\n\n            # 6. Adaptive Learning Rate Adjustment\n            diversity = np.std(x_best)  # Simple measure of population diversity\n\n            if diversity > self.diversity_threshold:\n                # Increase exploration: Reduce cs, cmu, c_cov\n                self.cs = max(0.01, self.cs - 0.01)\n                self.cmu = max(0.01, self.cmu - 0.01)\n                self.c_cov = max(0.001, self.c_cov - 0.001)\n            else:\n                # Increase exploitation: Increase cs, cmu, c_cov\n                self.cs = min(0.5, self.cs + 0.01)\n                self.cmu = min(0.5, self.cmu + 0.01)\n                self.c_cov = min(0.2, self.c_cov + 0.001)\n            \n            # Check for covariance matrix deterioration\n            if np.linalg.det(self.C) < self.restart_criterion or not np.all(np.linalg.eigvals(self.C) > 0):\n              self.C = np.eye(self.dim) # restart C\n              self.sigma = 0.5 # Reset sigma \n              self.ps = np.zeros(self.dim)\n              self.pc = np.zeros(self.dim)\n              self.cs = self.cs0\n              self.cmu = self.cmu0\n              self.c_cov = self.c_cov0\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveCMAES scored 0.112 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f7698668-9384-469f-95b8-0b80f1d4cb1a"], "operator": null, "metadata": {"aucs": [0.02552093084906859, 0.05997840491219941, 0.236624222477125, 0.07735867499153815, 0.06340664228273218, 0.12556286706652975, 0.08553049605082563, 0.17261697108272545, 0.1354275811524338, 0.10609907719387957, 0.13234608140115456, 0.20067234719721694, 9.999999999998899e-05, 0.13292614840529715, 0.10511105655542397, 0.196320987181397, 0.042607843592097905, 0.12940431391983453, 0.08713387108544779, 0.12819928339751563]}}
{"id": "e59c54d7-40ae-402f-9694-c5ab59078c01", "fitness": 0.11265115942830779, "name": "SymbioticOptimization", "description": "Bio-inspired optimization using a symbiotic relationship between two populations: hosts and symbionts, where hosts explore the search space and symbionts refine solutions locally.", "code": "import numpy as np\n\nclass SymbioticOptimization:\n    def __init__(self, budget=10000, dim=10, host_population_size=20, symbiont_population_size=10, mutualism_factor=0.5, parasitism_factor=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.host_population_size = host_population_size\n        self.symbiont_population_size = symbiont_population_size\n        self.mutualism_factor = mutualism_factor\n        self.parasitism_factor = parasitism_factor\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n\n        # Initialize host population\n        hosts = np.random.uniform(lb, ub, size=(self.host_population_size, self.dim))\n        host_fitness = np.array([func(x) for x in hosts])\n        self.budget -= self.host_population_size\n\n        # Initialize symbiont population (associated with each host)\n        symbionts = np.random.uniform(lb, ub, size=(self.host_population_size, self.symbiont_population_size, self.dim))\n        symbiont_fitness = np.zeros((self.host_population_size, self.symbiont_population_size))\n        for i in range(self.host_population_size):\n            for j in range(self.symbiont_population_size):\n                symbiont_fitness[i, j] = func(symbionts[i, j])\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n            if self.budget <= 0:\n                break\n\n        best_host_index = np.argmin(host_fitness)\n        self.f_opt = host_fitness[best_host_index]\n        self.x_opt = hosts[best_host_index]\n\n        while self.budget > 0:\n            for i in range(self.host_population_size):\n                # Mutualism: Host and symbiont benefit each other\n                for j in range(self.symbiont_population_size):\n                    if self.budget <= 0:\n                        break\n\n                    # Host learns from symbiont\n                    new_host = hosts[i] + self.mutualism_factor * (symbionts[i, j] - hosts[i]) * np.random.uniform(-1, 1, self.dim)\n                    new_host = np.clip(new_host, lb, ub)\n                    new_host_fitness = func(new_host)\n                    self.budget -= 1\n                    if new_host_fitness < host_fitness[i]:\n                        host_fitness[i] = new_host_fitness\n                        hosts[i] = new_host\n\n                    # Symbiont learns from host\n                    new_symbiont = symbionts[i, j] + self.mutualism_factor * (hosts[i] - symbionts[i, j]) * np.random.uniform(-1, 1, self.dim)\n                    new_symbiont = np.clip(new_symbiont, lb, ub)\n                    new_symbiont_fitness = func(new_symbiont)\n                    self.budget -= 1\n                    if new_symbiont_fitness < symbiont_fitness[i, j]:\n                        symbiont_fitness[i, j] = new_symbiont_fitness\n                        symbionts[i, j] = new_symbiont\n\n                if self.budget <= 0:\n                    break\n\n\n                # Parasitism: Host is harmed, symbiont benefits (or vice versa)\n                if np.random.rand() < self.parasitism_factor and self.budget > 0:\n                    target_host_index = np.random.randint(0, self.host_population_size)\n                    #Replace host with a slightly mutated version of the current host's best symbiont\n                    best_symbiont_index = np.argmin(symbiont_fitness[i])\n                    parasite = symbionts[i, best_symbiont_index] + np.random.normal(0, 0.05, self.dim)\n                    parasite = np.clip(parasite, lb, ub)\n                    parasite_fitness = func(parasite)\n                    self.budget -= 1\n\n                    if parasite_fitness < host_fitness[target_host_index]:\n                        host_fitness[target_host_index] = parasite_fitness\n                        hosts[target_host_index] = parasite\n\n                # Update global best\n                current_best_index = np.argmin(host_fitness)\n                if host_fitness[current_best_index] < self.f_opt:\n                    self.f_opt = host_fitness[current_best_index]\n                    self.x_opt = hosts[current_best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm SymbioticOptimization scored 0.113 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3e46f135-d75c-402a-8111-8a1795a826c5"], "operator": null, "metadata": {"aucs": [0.13157213143606117, 0.20638134684886222, 0]}}
{"id": "b772ad25-1378-47f0-ab9b-6d900d9e7f5f", "fitness": 0.1104516213879843, "name": "SelfAdjustingCMAES", "description": "A self-adjusting CMA-ES variant that dynamically adapts its control parameters (cs, cmu, c_cov) based on the recent success in reducing the objective function value.", "code": "import numpy as np\n\nclass SelfAdjustingCMAES:\n    def __init__(self, budget=10000, dim=10, mu_percentage=0.25, sigma0=0.5, cs=0.2, cmu=0.3, c_cov=0.1, adjustment_rate=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.mu = int(mu_percentage * budget)  # Number of parents\n        self.sigma = sigma0\n        self.mean = None\n        self.C = None  # Covariance matrix\n        self.ps = None # Evolution path for sigma\n        self.pc = None # Evolution path for mean\n        self.cs = cs\n        self.cmu = cmu\n        self.c_cov = c_cov\n        self.adjustment_rate = adjustment_rate\n        self.restart_criterion = 1e-12\n        self.previous_f_opt = np.Inf\n\n    def __call__(self, func):\n        self.mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.f_opt = np.Inf\n        self.x_opt = None\n        evals = 0\n\n        while evals < self.budget:\n            # 1. Sample offspring\n            z = np.random.multivariate_normal(np.zeros(self.dim), self.C, size=self.mu)\n            x = self.mean + self.sigma * z\n            \n            # Clip to bounds\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            \n            f = np.array([func(xi) for xi in x])\n            evals += self.mu\n\n            # 2. Selection and Recombination\n            idx = np.argsort(f)\n            x_best = x[idx[:self.mu]]\n            z_best = z[idx[:self.mu]]\n\n            current_best_f = np.min(f)\n            if current_best_f < self.f_opt:\n                self.f_opt = current_best_f\n                self.x_opt = x[idx[0]]\n\n            # 3. Update mean\n            mean_diff = np.mean(z_best, axis=0)\n            self.pc = (1 - self.cs) * self.pc + np.sqrt(self.cs * (2 - self.cs)) * mean_diff\n            self.mean = self.mean + self.cmu * self.sigma * self.pc\n\n            # 4. Update covariance matrix\n            self.ps = (1 - self.c_cov) * self.ps + np.sqrt(self.c_cov * (2 - self.c_cov)) * mean_diff\n            self.C = (1- self.c_cov) * self.C + self.c_cov * (np.outer(self.ps, self.ps) - self.C)\n\n            # 5. Update step size\n            self.sigma *= np.exp( (self.cs / 2) * (np.linalg.norm(self.ps)**2 / self.dim - 1) )\n\n            # 6. Self-Adjust parameters\n            if self.f_opt < self.previous_f_opt:\n                # Success: Increase exploration by increasing cs and c_cov, decrease cmu\n                self.cs *= (1 + self.adjustment_rate)\n                self.c_cov *= (1 + self.adjustment_rate)\n                self.cmu *= (1 - self.adjustment_rate)\n            else:\n                # Failure: Decrease exploration by decreasing cs and c_cov, increase cmu\n                self.cs *= (1 - self.adjustment_rate)\n                self.c_cov *= (1 - self.adjustment_rate)\n                self.cmu *= (1 + self.adjustment_rate)\n\n            # Ensure parameters stay within reasonable bounds\n            self.cs = np.clip(self.cs, 0.01, 0.5)\n            self.cmu = np.clip(self.cmu, 0.01, 0.5)\n            self.c_cov = np.clip(self.c_cov, 0.01, 0.5)\n\n            self.previous_f_opt = self.f_opt\n\n            # Check for covariance matrix deterioration\n            if np.linalg.det(self.C) < self.restart_criterion or not np.all(np.linalg.eigvals(self.C) > 0):\n              self.C = np.eye(self.dim) # restart C\n              self.sigma = 0.5 # Reset sigma \n              self.ps = np.zeros(self.dim)\n              self.pc = np.zeros(self.dim)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm SelfAdjustingCMAES scored 0.110 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f7698668-9384-469f-95b8-0b80f1d4cb1a"], "operator": null, "metadata": {"aucs": [0.0699035977500323, 0.02476940126176319, 0.17735844420296576, 0.10599219481784261, 0.05480610084806126, 0.11050954800965695, 0.05677531105746203, 0.11112821859330524, 0.07402412102999223, 0.11677274904652202, 0.08588163407822391, 0.1934301922983941, 0.10039838497345588, 0.06702236053660948, 0.14279504127558373, 0.16993615141801732, 0.1630553255580358, 0.1402834314188287, 0.11966236939436126, 0.12452785019057244]}}
{"id": "a37cc2bb-55a6-4ceb-9963-d6c0cbb9f5b5", "fitness": 0.0, "name": "SelfOrganizingScoutBee", "description": "A self-organizing scout bee algorithm with dynamic population size and adaptive step sizes for intensified exploration around promising regions.", "code": "import numpy as np\n\nclass SelfOrganizingScoutBee:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=20, scout_rate=0.1, min_pop_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = initial_pop_size\n        self.scout_rate = scout_rate  # Percentage of scouts to explore\n        self.min_pop_size = min_pop_size\n        self.population = None\n        self.fitness = None\n        self.step_size = 1.0\n        self.shrink_factor = 0.95 # Step size adaptation\n        self.enlarge_factor = 1.1\n        self.local_search_iterations = 5 # Local search intensity\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.initial_pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.initial_pop_size\n\n    def scout_bee_phase(self, func):\n        num_scouts = max(1, int(len(self.population) * self.scout_rate))\n        worst_indices = np.argsort(self.fitness)[-num_scouts:]  # Indices of worst bees\n\n        for i in worst_indices:\n            new_position = np.random.uniform(func.bounds.lb, func.bounds.ub)\n            new_fitness = func(new_position)\n            self.budget -= 1\n\n            if new_fitness < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_fitness\n\n            # Local Search around the current best scout\n            best_index = np.argmin(self.fitness)\n            best_position = self.population[best_index]\n            for _ in range(self.local_search_iterations):\n                perturbation = np.random.uniform(-self.step_size, self.step_size, size=self.dim)\n                new_position = best_position + perturbation\n                new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_position)\n                self.budget -=1\n\n                if new_fitness < self.fitness[i]:\n                  self.population[i] = new_position\n                  self.fitness[i] = new_fitness\n\n\n        # Dynamic population adjustment: remove duplicated individuals\n        unique_rows, indices = np.unique(self.population, axis=0, return_index=True)\n        self.population = self.population[np.sort(indices)]\n        self.fitness = self.fitness[np.sort(indices)]\n\n        # Population control if it drops too low\n        if len(self.population) < self.min_pop_size and self.budget > self.min_pop_size:\n            num_needed = self.min_pop_size - len(self.population)\n            new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(num_needed, self.dim))\n            new_fitnesses = [func(x) for x in new_individuals]\n            self.budget -= num_needed\n            self.population = np.vstack((self.population, new_individuals))\n            self.fitness = np.concatenate((self.fitness, new_fitnesses))\n\n\n    def employed_bee_phase(self, func):\n        for i in range(len(self.population)):\n            neighbor_index = np.random.choice(len(self.population))\n            while neighbor_index == i:\n                neighbor_index = np.random.choice(len(self.population))\n\n            # Perturb the current solution\n            perturbation = np.random.uniform(-self.step_size, self.step_size, size=self.dim)\n            new_position = self.population[i] + perturbation\n            new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n            new_fitness = func(new_position)\n            self.budget -= 1\n\n            # Greedy selection\n            if new_fitness < self.fitness[i]:\n                self.population[i] = new_position\n                self.fitness[i] = new_fitness\n\n    def adjust_step_size(self):\n        # Simple step size adaptation based on fitness variance\n        fitness_variance = np.var(self.fitness)\n        if fitness_variance < 1e-6:\n            self.step_size *= self.enlarge_factor  # Increase exploration\n        else:\n            self.step_size *= self.shrink_factor  # Reduce step size for exploitation\n        self.step_size = np.clip(self.step_size, 0.01, 2.0) # Limit the step size\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            self.employed_bee_phase(func)\n            self.scout_bee_phase(func)\n            self.adjust_step_size()\n\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm SelfOrganizingScoutBee scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["177e5e85-e9a9-4749-849b-a0ee37e1a69e"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "d3f91dd2-fdbe-430a-8a5d-ae1ecde791e6", "fitness": 0.17445310792712979, "name": "AdaptiveCMAES", "description": "An adaptive CMA-ES variant that adjusts its covariance matrix adaptation rate based on the success rate of improving solutions.", "code": "import numpy as np\n\nclass AdaptiveCMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damps=None, c_cov_base=None, sigma0=0.2, adaptation_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.adaptation_rate = adaptation_rate\n\n        self.mu = self.popsize // 2\n\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n\n        self.m = None\n        self.sigma = None\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.eigenspace = None\n        self.eigenvalues = None\n\n        self.cs = cs\n        self.damps = damps if damps is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1)/(self.dim + 1)) - 1]) + self.cs\n        self.c_cov_base = c_cov_base if c_cov_base is not None else (1 / (self.dim * np.sqrt(self.dim))) * 10\n        self.c_cov = self.c_cov_base\n        self.c_cov_mu = self.c_cov\n        self.c_cov_mu = self.c_cov_mu if self.c_cov_mu <= 1 else 1\n        self.c_cov_mu = self.c_cov_mu if self.c_cov_mu > 0 else 0\n        self.c_cov = (1 / self.mu) * self.c_cov\n\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.func_evals = 0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 50  # Number of iterations to wait before restart\n        self.success_rate = 0.5  # Initialize success rate\n        self.recent_successes = [] #Store recent success/failure\n\n    def initialize(self):\n        self.m = np.random.uniform(-2, 2, size=self.dim)\n        self.sigma = self.sigma0\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.eigenspace = np.eye(self.dim)\n        self.eigenvalues = np.ones(self.dim)\n\n    def sample(self):\n        z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n        x = self.m[:, np.newaxis] + self.sigma * (self.eigenspace @ (np.diag(np.sqrt(self.eigenvalues)) @ z))\n        return x\n\n    def update(self, x, fitness_values, current_f_opt):\n        idx = np.argsort(fitness_values)\n        x_mu = x[:, idx[:self.mu]]\n        z_mu = np.linalg.solve(self.eigenspace @ np.diag(np.sqrt(self.eigenvalues)), (x_mu - self.m[:, np.newaxis]) / self.sigma)\n\n        m_old = self.m.copy()\n        self.m = np.sum(x_mu * self.weights[np.newaxis, :], axis=1)\n        self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * np.sum(self.weights)) * (self.eigenspace @ z_mu.mean(axis=1))\n        self.pc = (1 - self.c_cov) * self.pc + np.sqrt(self.c_cov * (2 - self.c_cov) * np.sum(self.weights)) * ((self.m - m_old) / self.sigma)\n        \n        C_temp = self.c_cov_mu * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :]) + \\\n               (1 - self.c_cov_mu) * (self.C)\n\n        self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / np.sqrt(self.dim) - 1))\n\n        self.C = C_temp\n\n        self.eigenvalues, self.eigenspace = np.linalg.eigh(self.C)\n        self.eigenvalues = np.maximum(self.eigenvalues, 1e-12)\n        \n        #Adapt c_cov based on success rate\n        improvement = fitness_values[idx[0]] < current_f_opt\n        self.recent_successes.append(improvement)\n\n        if len(self.recent_successes) > self.popsize:\n            self.recent_successes.pop(0)\n            self.success_rate = np.mean(self.recent_successes)\n\n            # Adjust c_cov based on success rate\n            if self.success_rate > 0.7:\n                self.c_cov_base *= (1 + self.adaptation_rate)\n            elif self.success_rate < 0.3:\n                self.c_cov_base *= (1 - self.adaptation_rate)\n                self.c_cov_base = max(self.c_cov_base, 0.0)  # Ensure c_cov remains non-negative\n            self.c_cov = self.c_cov_base\n            self.c_cov = (1 / self.mu) * self.c_cov\n\n\n\n    def __call__(self, func):\n        self.initialize()\n\n        while self.func_evals < self.budget:\n            x = self.sample()\n\n            fitness_values = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.func_evals += self.popsize\n\n            best_index = np.argmin(fitness_values)\n            if fitness_values[best_index] < self.f_opt:\n                self.f_opt = fitness_values[best_index]\n                self.x_opt = x[:, best_index]\n                self.stagnation_counter = 0  # Reset counter if improvement\n            \n            self.update(x, fitness_values, self.f_opt)\n            self.stagnation_counter += 1\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.initialize()  # Restart CMA-ES\n                self.stagnation_counter = 0  # Reset counter\n                self.recent_successes = [] #Restart with empty list\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveCMAES scored 0.174 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["fe3d064d-8b85-4b87-8599-807479acb026"], "operator": null, "metadata": {"aucs": [0.06744145493532994, 0.1804312754786218, 0.2540427817001393, 0.11981744088829727, 0.13541204818355668, 0.15742142137310644, 0.19331436098403632, 0.17318396470256558, 0.1660034313750739, 0.1262053391164909, 0.16184514528740623, 0.19994556002421682, 0.24863148894925136, 0.12902099194834138, 0.13284835100629266, 0.24985615863057742, 0.16435944551691972, 0.2042086754014777, 0.15568956253666422, 0.26938326050423034]}}
{"id": "fb00d133-47ef-4b7a-b660-014616df50de", "fitness": 0.6207040141922711, "name": "AdaptiveDE_DynamicParams", "description": "An adaptive Differential Evolution algorithm that dynamically adjusts the mutation factor F and crossover rate CR based on the success of previous generations.", "code": "import numpy as np\n\nclass AdaptiveDE_DynamicParams:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=100, F_init=0.5, CR_init=0.9, F_adapt_rate=0.1, CR_adapt_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F_init\n        self.CR = CR_init\n        self.F_init = F_init\n        self.CR_init = CR_init\n        self.F_adapt_rate = F_adapt_rate\n        self.CR_adapt_rate = CR_adapt_rate\n        self.population = None\n        self.fitness = None\n        self.archive = None\n        self.success_F = []\n        self.success_CR = []\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        \n    def mutate(self, x_i, func):\n        indices = np.random.choice(self.pop_size + self.archive_size, size=3, replace=False)\n        \n        if indices[0] < self.pop_size:\n            x_r1 = self.population[indices[0]]\n        else:\n            x_r1 = self.archive[indices[0] - self.pop_size]\n\n        if indices[1] < self.pop_size:\n            x_r2 = self.population[indices[1]]\n        else:\n            x_r2 = self.archive[indices[1] - self.pop_size]\n            \n        if indices[2] < self.pop_size:\n            x_r3 = self.population[indices[2]]\n        else:\n            x_r3 = self.archive[indices[2] - self.pop_size]\n\n        return x_r1 + self.F * (x_r2 - x_r3)\n\n    def crossover(self, x_i, v_i):\n        u_i = np.copy(x_i)\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() <= self.CR or j == j_rand:\n                u_i[j] = v_i[j]\n        return u_i\n\n    def repair(self, x, func):\n         return np.clip(x, func.bounds.lb, func.bounds.ub)\n\n    def update_archive(self):\n        # Randomly replace archive members with population members\n        indices = np.random.choice(self.archive_size, size=self.pop_size, replace=False)\n        self.archive[indices] = self.population\n\n    def restart(self, func):\n        # Restart the population with new random solutions\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.F = self.F_init\n        self.CR = self.CR_init\n\n    def adapt_parameters(self):\n        if len(self.success_F) > 0:\n            self.F = np.mean(self.success_F)\n            self.F = np.clip(self.F, 0.1, 0.9)  # Keep F within reasonable bounds\n        else:\n            self.F = self.F_init\n        if len(self.success_CR) > 0:\n            self.CR = np.mean(self.success_CR)\n            self.CR = np.clip(self.CR, 0.1, 0.9) # Keep CR within reasonable bounds\n        else:\n            self.CR = self.CR_init\n        self.success_F = []\n        self.success_CR = []\n    \n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n\n        iteration = 0\n        while self.budget > 0:\n            iteration += 1\n            \n            successful_F = []\n            successful_CR = []\n\n            for i in range(self.pop_size):\n                # Mutation\n                v_i = self.mutate(self.population[i], func)\n                \n                # Crossover\n                u_i = self.crossover(self.population[i], v_i)\n                \n                # Repair\n                u_i = self.repair(u_i, func)\n\n                # Selection\n                f_u_i = func(u_i)\n                self.budget -= 1\n                if f_u_i < self.fitness[i]:\n                    successful_F.append(self.F)\n                    successful_CR.append(self.CR)\n\n                    self.population[i] = u_i\n                    self.fitness[i] = f_u_i\n\n                    if f_u_i < self.f_opt:\n                        self.f_opt = f_u_i\n                        self.x_opt = u_i\n\n            self.update_archive()\n            self.success_F.extend(successful_F)\n            self.success_CR.extend(successful_CR)\n            self.adapt_parameters()\n\n\n            # Restart mechanism (every 100 iterations)\n            if iteration % 100 == 0 and self.budget > self.pop_size:\n                self.restart(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDE_DynamicParams scored 0.621 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["177e5e85-e9a9-4749-849b-a0ee37e1a69e"], "operator": null, "metadata": {"aucs": [0.22074646655203178, 0.5043923576276762, 0.5811126944959657, 0.8335551831600562, 0.6298170905496181, 0.7077048049099397, 0.5320509751232668, 0.5070867791808056, 0.6465083835354606, 0.5060206530628761, 0.8091997648130829, 0.9889346087180173, 0.5808830280171293, 0.6328910847202742, 0.880878951294102, 0.7031432606518271, 0.5135817438283357, 0.78534094406465, 0.295126518247806, 0.5551049912925008]}}
{"id": "25a3c212-dab8-4118-9b4f-782366374216", "fitness": -Infinity, "name": "MirroredCMAES", "description": "CMA-ES with mirrored sampling and adaptive population size based on success rate.", "code": "import numpy as np\n\nclass MirroredCMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damps=None, c_cov_base=None, sigma0=0.2, adaptation_rate=0.1, popsize_adaptation_rate=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.adaptation_rate = adaptation_rate\n        self.popsize_adaptation_rate = popsize_adaptation_rate\n\n        self.mu = self.popsize // 2\n\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n\n        self.m = None\n        self.sigma = None\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.eigenspace = None\n        self.eigenvalues = None\n\n        self.cs = cs\n        self.damps = damps if damps is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1)/(self.dim + 1)) - 1]) + self.cs\n        self.c_cov_base = c_cov_base if c_cov_base is not None else (1 / (self.dim * np.sqrt(self.dim))) * 10\n        self.c_cov = self.c_cov_base\n        self.c_cov_mu = self.c_cov\n        self.c_cov_mu = self.c_cov_mu if self.c_cov_mu <= 1 else 1\n        self.c_cov_mu = self.c_cov_mu if self.c_cov_mu > 0 else 0\n        self.c_cov = (1 / self.mu) * self.c_cov\n\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.func_evals = 0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 50  # Number of iterations to wait before restart\n        self.success_rate = 0.5  # Initialize success rate\n        self.recent_successes = [] #Store recent success/failure\n\n    def initialize(self):\n        self.m = np.random.uniform(-2, 2, size=self.dim)\n        self.sigma = self.sigma0\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.eigenspace = np.eye(self.dim)\n        self.eigenvalues = np.ones(self.dim)\n\n    def sample(self):\n        z = np.random.normal(0, 1, size=(self.dim, self.popsize // 2))\n        x = self.m[:, np.newaxis] + self.sigma * (self.eigenspace @ (np.diag(np.sqrt(self.eigenvalues)) @ z))\n        x_mirrored = self.m[:, np.newaxis] - self.sigma * (self.eigenspace @ (np.diag(np.sqrt(self.eigenvalues)) @ z)) #Mirrored samples\n        x = np.concatenate((x, x_mirrored), axis=1)\n        return x\n\n    def update(self, x, fitness_values, current_f_opt):\n        idx = np.argsort(fitness_values)\n        x_mu = x[:, idx[:self.mu]]\n        z_mu = np.linalg.solve(self.eigenspace @ np.diag(np.sqrt(self.eigenvalues)), (x_mu - self.m[:, np.newaxis]) / self.sigma)\n\n        m_old = self.m.copy()\n        self.m = np.sum(x_mu * self.weights[np.newaxis, :], axis=1)\n        self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * np.sum(self.weights)) * (self.eigenspace @ z_mu.mean(axis=1))\n        self.pc = (1 - self.c_cov) * self.pc + np.sqrt(self.c_cov * (2 - self.c_cov) * np.sum(self.weights)) * ((self.m - m_old) / self.sigma)\n        \n        C_temp = self.c_cov_mu * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :]) + \\\n               (1 - self.c_cov_mu) * (self.C)\n\n        self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / np.sqrt(self.dim) - 1))\n\n        self.C = C_temp\n\n        self.eigenvalues, self.eigenspace = np.linalg.eigh(self.C)\n        self.eigenvalues = np.maximum(self.eigenvalues, 1e-12)\n        \n        #Adapt c_cov based on success rate\n        improvement = fitness_values[idx[0]] < current_f_opt\n        self.recent_successes.append(improvement)\n\n        if len(self.recent_successes) > self.popsize:\n            self.recent_successes.pop(0)\n            self.success_rate = np.mean(self.recent_successes)\n\n            # Adjust c_cov based on success rate\n            if self.success_rate > 0.7:\n                self.c_cov_base *= (1 + self.adaptation_rate)\n                self.popsize = min(self.popsize + 1, 2*self.dim)\n\n            elif self.success_rate < 0.3:\n                self.c_cov_base *= (1 - self.adaptation_rate)\n                self.c_cov_base = max(self.c_cov_base, 0.0)  # Ensure c_cov remains non-negative\n                self.popsize = max(self.popsize - 1, 4)\n\n            self.c_cov = self.c_cov_base\n            self.c_cov = (1 / self.mu) * self.c_cov\n            self.mu = self.popsize // 2\n            self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n            self.weights = self.weights / np.sum(self.weights)\n\n\n    def __call__(self, func):\n        self.initialize()\n\n        while self.func_evals < self.budget:\n            x = self.sample()\n\n            fitness_values = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.func_evals += self.popsize\n\n            best_index = np.argmin(fitness_values)\n            if fitness_values[best_index] < self.f_opt:\n                self.f_opt = fitness_values[best_index]\n                self.x_opt = x[:, best_index]\n                self.stagnation_counter = 0  # Reset counter if improvement\n            \n            self.update(x, fitness_values, self.f_opt)\n            self.stagnation_counter += 1\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.initialize()  # Restart CMA-ES\n                self.stagnation_counter = 0  # Reset counter\n                self.recent_successes = [] #Restart with empty list\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 109, in __call__\n  File \"<string>\", line 109, in <listcomp>\nIndexError: index 4 is out of bounds for axis 1 with size 4\n.", "error": "", "parent_ids": ["d3f91dd2-fdbe-430a-8a5d-ae1ecde791e6"], "operator": null, "metadata": {}}
{"id": "25a3c212-dab8-4118-9b4f-782366374216", "fitness": -Infinity, "name": "MirroredCMAES", "description": "CMA-ES with mirrored sampling and adaptive population size based on success rate.", "code": "import numpy as np\n\nclass MirroredCMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damps=None, c_cov_base=None, sigma0=0.2, adaptation_rate=0.1, popsize_adaptation_rate=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.adaptation_rate = adaptation_rate\n        self.popsize_adaptation_rate = popsize_adaptation_rate\n\n        self.mu = self.popsize // 2\n\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n\n        self.m = None\n        self.sigma = None\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.eigenspace = None\n        self.eigenvalues = None\n\n        self.cs = cs\n        self.damps = damps if damps is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1)/(self.dim + 1)) - 1]) + self.cs\n        self.c_cov_base = c_cov_base if c_cov_base is not None else (1 / (self.dim * np.sqrt(self.dim))) * 10\n        self.c_cov = self.c_cov_base\n        self.c_cov_mu = self.c_cov\n        self.c_cov_mu = self.c_cov_mu if self.c_cov_mu <= 1 else 1\n        self.c_cov_mu = self.c_cov_mu if self.c_cov_mu > 0 else 0\n        self.c_cov = (1 / self.mu) * self.c_cov\n\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.func_evals = 0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 50  # Number of iterations to wait before restart\n        self.success_rate = 0.5  # Initialize success rate\n        self.recent_successes = [] #Store recent success/failure\n\n    def initialize(self):\n        self.m = np.random.uniform(-2, 2, size=self.dim)\n        self.sigma = self.sigma0\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.eigenspace = np.eye(self.dim)\n        self.eigenvalues = np.ones(self.dim)\n\n    def sample(self):\n        z = np.random.normal(0, 1, size=(self.dim, self.popsize // 2))\n        x = self.m[:, np.newaxis] + self.sigma * (self.eigenspace @ (np.diag(np.sqrt(self.eigenvalues)) @ z))\n        x_mirrored = self.m[:, np.newaxis] - self.sigma * (self.eigenspace @ (np.diag(np.sqrt(self.eigenvalues)) @ z)) #Mirrored samples\n        x = np.concatenate((x, x_mirrored), axis=1)\n        return x\n\n    def update(self, x, fitness_values, current_f_opt):\n        idx = np.argsort(fitness_values)\n        x_mu = x[:, idx[:self.mu]]\n        z_mu = np.linalg.solve(self.eigenspace @ np.diag(np.sqrt(self.eigenvalues)), (x_mu - self.m[:, np.newaxis]) / self.sigma)\n\n        m_old = self.m.copy()\n        self.m = np.sum(x_mu * self.weights[np.newaxis, :], axis=1)\n        self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * np.sum(self.weights)) * (self.eigenspace @ z_mu.mean(axis=1))\n        self.pc = (1 - self.c_cov) * self.pc + np.sqrt(self.c_cov * (2 - self.c_cov) * np.sum(self.weights)) * ((self.m - m_old) / self.sigma)\n        \n        C_temp = self.c_cov_mu * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :]) + \\\n               (1 - self.c_cov_mu) * (self.C)\n\n        self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / np.sqrt(self.dim) - 1))\n\n        self.C = C_temp\n\n        self.eigenvalues, self.eigenspace = np.linalg.eigh(self.C)\n        self.eigenvalues = np.maximum(self.eigenvalues, 1e-12)\n        \n        #Adapt c_cov based on success rate\n        improvement = fitness_values[idx[0]] < current_f_opt\n        self.recent_successes.append(improvement)\n\n        if len(self.recent_successes) > self.popsize:\n            self.recent_successes.pop(0)\n            self.success_rate = np.mean(self.recent_successes)\n\n            # Adjust c_cov based on success rate\n            if self.success_rate > 0.7:\n                self.c_cov_base *= (1 + self.adaptation_rate)\n                self.popsize = min(self.popsize + 1, 2*self.dim)\n\n            elif self.success_rate < 0.3:\n                self.c_cov_base *= (1 - self.adaptation_rate)\n                self.c_cov_base = max(self.c_cov_base, 0.0)  # Ensure c_cov remains non-negative\n                self.popsize = max(self.popsize - 1, 4)\n\n            self.c_cov = self.c_cov_base\n            self.c_cov = (1 / self.mu) * self.c_cov\n            self.mu = self.popsize // 2\n            self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n            self.weights = self.weights / np.sum(self.weights)\n\n\n    def __call__(self, func):\n        self.initialize()\n\n        while self.func_evals < self.budget:\n            x = self.sample()\n\n            fitness_values = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.func_evals += self.popsize\n\n            best_index = np.argmin(fitness_values)\n            if fitness_values[best_index] < self.f_opt:\n                self.f_opt = fitness_values[best_index]\n                self.x_opt = x[:, best_index]\n                self.stagnation_counter = 0  # Reset counter if improvement\n            \n            self.update(x, fitness_values, self.f_opt)\n            self.stagnation_counter += 1\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.initialize()  # Restart CMA-ES\n                self.stagnation_counter = 0  # Reset counter\n                self.recent_successes = [] #Restart with empty list\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 109, in __call__\n  File \"<string>\", line 109, in <listcomp>\nIndexError: index 4 is out of bounds for axis 1 with size 4\n.", "error": "", "parent_ids": ["d3f91dd2-fdbe-430a-8a5d-ae1ecde791e6"], "operator": null, "metadata": {}}
{"id": "25a3c212-dab8-4118-9b4f-782366374216", "fitness": -Infinity, "name": "MirroredCMAES", "description": "CMA-ES with mirrored sampling and adaptive population size based on success rate.", "code": "import numpy as np\n\nclass MirroredCMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damps=None, c_cov_base=None, sigma0=0.2, adaptation_rate=0.1, popsize_adaptation_rate=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.adaptation_rate = adaptation_rate\n        self.popsize_adaptation_rate = popsize_adaptation_rate\n\n        self.mu = self.popsize // 2\n\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n\n        self.m = None\n        self.sigma = None\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.eigenspace = None\n        self.eigenvalues = None\n\n        self.cs = cs\n        self.damps = damps if damps is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1)/(self.dim + 1)) - 1]) + self.cs\n        self.c_cov_base = c_cov_base if c_cov_base is not None else (1 / (self.dim * np.sqrt(self.dim))) * 10\n        self.c_cov = self.c_cov_base\n        self.c_cov_mu = self.c_cov\n        self.c_cov_mu = self.c_cov_mu if self.c_cov_mu <= 1 else 1\n        self.c_cov_mu = self.c_cov_mu if self.c_cov_mu > 0 else 0\n        self.c_cov = (1 / self.mu) * self.c_cov\n\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.func_evals = 0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 50  # Number of iterations to wait before restart\n        self.success_rate = 0.5  # Initialize success rate\n        self.recent_successes = [] #Store recent success/failure\n\n    def initialize(self):\n        self.m = np.random.uniform(-2, 2, size=self.dim)\n        self.sigma = self.sigma0\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.eigenspace = np.eye(self.dim)\n        self.eigenvalues = np.ones(self.dim)\n\n    def sample(self):\n        z = np.random.normal(0, 1, size=(self.dim, self.popsize // 2))\n        x = self.m[:, np.newaxis] + self.sigma * (self.eigenspace @ (np.diag(np.sqrt(self.eigenvalues)) @ z))\n        x_mirrored = self.m[:, np.newaxis] - self.sigma * (self.eigenspace @ (np.diag(np.sqrt(self.eigenvalues)) @ z)) #Mirrored samples\n        x = np.concatenate((x, x_mirrored), axis=1)\n        return x\n\n    def update(self, x, fitness_values, current_f_opt):\n        idx = np.argsort(fitness_values)\n        x_mu = x[:, idx[:self.mu]]\n        z_mu = np.linalg.solve(self.eigenspace @ np.diag(np.sqrt(self.eigenvalues)), (x_mu - self.m[:, np.newaxis]) / self.sigma)\n\n        m_old = self.m.copy()\n        self.m = np.sum(x_mu * self.weights[np.newaxis, :], axis=1)\n        self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * np.sum(self.weights)) * (self.eigenspace @ z_mu.mean(axis=1))\n        self.pc = (1 - self.c_cov) * self.pc + np.sqrt(self.c_cov * (2 - self.c_cov) * np.sum(self.weights)) * ((self.m - m_old) / self.sigma)\n        \n        C_temp = self.c_cov_mu * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :]) + \\\n               (1 - self.c_cov_mu) * (self.C)\n\n        self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / np.sqrt(self.dim) - 1))\n\n        self.C = C_temp\n\n        self.eigenvalues, self.eigenspace = np.linalg.eigh(self.C)\n        self.eigenvalues = np.maximum(self.eigenvalues, 1e-12)\n        \n        #Adapt c_cov based on success rate\n        improvement = fitness_values[idx[0]] < current_f_opt\n        self.recent_successes.append(improvement)\n\n        if len(self.recent_successes) > self.popsize:\n            self.recent_successes.pop(0)\n            self.success_rate = np.mean(self.recent_successes)\n\n            # Adjust c_cov based on success rate\n            if self.success_rate > 0.7:\n                self.c_cov_base *= (1 + self.adaptation_rate)\n                self.popsize = min(self.popsize + 1, 2*self.dim)\n\n            elif self.success_rate < 0.3:\n                self.c_cov_base *= (1 - self.adaptation_rate)\n                self.c_cov_base = max(self.c_cov_base, 0.0)  # Ensure c_cov remains non-negative\n                self.popsize = max(self.popsize - 1, 4)\n\n            self.c_cov = self.c_cov_base\n            self.c_cov = (1 / self.mu) * self.c_cov\n            self.mu = self.popsize // 2\n            self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n            self.weights = self.weights / np.sum(self.weights)\n\n\n    def __call__(self, func):\n        self.initialize()\n\n        while self.func_evals < self.budget:\n            x = self.sample()\n\n            fitness_values = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.func_evals += self.popsize\n\n            best_index = np.argmin(fitness_values)\n            if fitness_values[best_index] < self.f_opt:\n                self.f_opt = fitness_values[best_index]\n                self.x_opt = x[:, best_index]\n                self.stagnation_counter = 0  # Reset counter if improvement\n            \n            self.update(x, fitness_values, self.f_opt)\n            self.stagnation_counter += 1\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.initialize()  # Restart CMA-ES\n                self.stagnation_counter = 0  # Reset counter\n                self.recent_successes = [] #Restart with empty list\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 109, in __call__\n  File \"<string>\", line 109, in <listcomp>\nIndexError: index 4 is out of bounds for axis 1 with size 4\n.", "error": "", "parent_ids": ["d3f91dd2-fdbe-430a-8a5d-ae1ecde791e6"], "operator": null, "metadata": {}}
{"id": "25a3c212-dab8-4118-9b4f-782366374216", "fitness": -Infinity, "name": "MirroredCMAES", "description": "CMA-ES with mirrored sampling and adaptive population size based on success rate.", "code": "import numpy as np\n\nclass MirroredCMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damps=None, c_cov_base=None, sigma0=0.2, adaptation_rate=0.1, popsize_adaptation_rate=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.adaptation_rate = adaptation_rate\n        self.popsize_adaptation_rate = popsize_adaptation_rate\n\n        self.mu = self.popsize // 2\n\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n\n        self.m = None\n        self.sigma = None\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.eigenspace = None\n        self.eigenvalues = None\n\n        self.cs = cs\n        self.damps = damps if damps is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1)/(self.dim + 1)) - 1]) + self.cs\n        self.c_cov_base = c_cov_base if c_cov_base is not None else (1 / (self.dim * np.sqrt(self.dim))) * 10\n        self.c_cov = self.c_cov_base\n        self.c_cov_mu = self.c_cov\n        self.c_cov_mu = self.c_cov_mu if self.c_cov_mu <= 1 else 1\n        self.c_cov_mu = self.c_cov_mu if self.c_cov_mu > 0 else 0\n        self.c_cov = (1 / self.mu) * self.c_cov\n\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.func_evals = 0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 50  # Number of iterations to wait before restart\n        self.success_rate = 0.5  # Initialize success rate\n        self.recent_successes = [] #Store recent success/failure\n\n    def initialize(self):\n        self.m = np.random.uniform(-2, 2, size=self.dim)\n        self.sigma = self.sigma0\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.eigenspace = np.eye(self.dim)\n        self.eigenvalues = np.ones(self.dim)\n\n    def sample(self):\n        z = np.random.normal(0, 1, size=(self.dim, self.popsize // 2))\n        x = self.m[:, np.newaxis] + self.sigma * (self.eigenspace @ (np.diag(np.sqrt(self.eigenvalues)) @ z))\n        x_mirrored = self.m[:, np.newaxis] - self.sigma * (self.eigenspace @ (np.diag(np.sqrt(self.eigenvalues)) @ z)) #Mirrored samples\n        x = np.concatenate((x, x_mirrored), axis=1)\n        return x\n\n    def update(self, x, fitness_values, current_f_opt):\n        idx = np.argsort(fitness_values)\n        x_mu = x[:, idx[:self.mu]]\n        z_mu = np.linalg.solve(self.eigenspace @ np.diag(np.sqrt(self.eigenvalues)), (x_mu - self.m[:, np.newaxis]) / self.sigma)\n\n        m_old = self.m.copy()\n        self.m = np.sum(x_mu * self.weights[np.newaxis, :], axis=1)\n        self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * np.sum(self.weights)) * (self.eigenspace @ z_mu.mean(axis=1))\n        self.pc = (1 - self.c_cov) * self.pc + np.sqrt(self.c_cov * (2 - self.c_cov) * np.sum(self.weights)) * ((self.m - m_old) / self.sigma)\n        \n        C_temp = self.c_cov_mu * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :]) + \\\n               (1 - self.c_cov_mu) * (self.C)\n\n        self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / np.sqrt(self.dim) - 1))\n\n        self.C = C_temp\n\n        self.eigenvalues, self.eigenspace = np.linalg.eigh(self.C)\n        self.eigenvalues = np.maximum(self.eigenvalues, 1e-12)\n        \n        #Adapt c_cov based on success rate\n        improvement = fitness_values[idx[0]] < current_f_opt\n        self.recent_successes.append(improvement)\n\n        if len(self.recent_successes) > self.popsize:\n            self.recent_successes.pop(0)\n            self.success_rate = np.mean(self.recent_successes)\n\n            # Adjust c_cov based on success rate\n            if self.success_rate > 0.7:\n                self.c_cov_base *= (1 + self.adaptation_rate)\n                self.popsize = min(self.popsize + 1, 2*self.dim)\n\n            elif self.success_rate < 0.3:\n                self.c_cov_base *= (1 - self.adaptation_rate)\n                self.c_cov_base = max(self.c_cov_base, 0.0)  # Ensure c_cov remains non-negative\n                self.popsize = max(self.popsize - 1, 4)\n\n            self.c_cov = self.c_cov_base\n            self.c_cov = (1 / self.mu) * self.c_cov\n            self.mu = self.popsize // 2\n            self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n            self.weights = self.weights / np.sum(self.weights)\n\n\n    def __call__(self, func):\n        self.initialize()\n\n        while self.func_evals < self.budget:\n            x = self.sample()\n\n            fitness_values = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.func_evals += self.popsize\n\n            best_index = np.argmin(fitness_values)\n            if fitness_values[best_index] < self.f_opt:\n                self.f_opt = fitness_values[best_index]\n                self.x_opt = x[:, best_index]\n                self.stagnation_counter = 0  # Reset counter if improvement\n            \n            self.update(x, fitness_values, self.f_opt)\n            self.stagnation_counter += 1\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.initialize()  # Restart CMA-ES\n                self.stagnation_counter = 0  # Reset counter\n                self.recent_successes = [] #Restart with empty list\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 109, in __call__\n  File \"<string>\", line 109, in <listcomp>\nIndexError: index 4 is out of bounds for axis 1 with size 4\n.", "error": "", "parent_ids": ["d3f91dd2-fdbe-430a-8a5d-ae1ecde791e6"], "operator": null, "metadata": {}}
{"id": "25a3c212-dab8-4118-9b4f-782366374216", "fitness": -Infinity, "name": "MirroredCMAES", "description": "CMA-ES with mirrored sampling and adaptive population size based on success rate.", "code": "import numpy as np\n\nclass MirroredCMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damps=None, c_cov_base=None, sigma0=0.2, adaptation_rate=0.1, popsize_adaptation_rate=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.adaptation_rate = adaptation_rate\n        self.popsize_adaptation_rate = popsize_adaptation_rate\n\n        self.mu = self.popsize // 2\n\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n\n        self.m = None\n        self.sigma = None\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.eigenspace = None\n        self.eigenvalues = None\n\n        self.cs = cs\n        self.damps = damps if damps is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1)/(self.dim + 1)) - 1]) + self.cs\n        self.c_cov_base = c_cov_base if c_cov_base is not None else (1 / (self.dim * np.sqrt(self.dim))) * 10\n        self.c_cov = self.c_cov_base\n        self.c_cov_mu = self.c_cov\n        self.c_cov_mu = self.c_cov_mu if self.c_cov_mu <= 1 else 1\n        self.c_cov_mu = self.c_cov_mu if self.c_cov_mu > 0 else 0\n        self.c_cov = (1 / self.mu) * self.c_cov\n\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.func_evals = 0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 50  # Number of iterations to wait before restart\n        self.success_rate = 0.5  # Initialize success rate\n        self.recent_successes = [] #Store recent success/failure\n\n    def initialize(self):\n        self.m = np.random.uniform(-2, 2, size=self.dim)\n        self.sigma = self.sigma0\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.eigenspace = np.eye(self.dim)\n        self.eigenvalues = np.ones(self.dim)\n\n    def sample(self):\n        z = np.random.normal(0, 1, size=(self.dim, self.popsize // 2))\n        x = self.m[:, np.newaxis] + self.sigma * (self.eigenspace @ (np.diag(np.sqrt(self.eigenvalues)) @ z))\n        x_mirrored = self.m[:, np.newaxis] - self.sigma * (self.eigenspace @ (np.diag(np.sqrt(self.eigenvalues)) @ z)) #Mirrored samples\n        x = np.concatenate((x, x_mirrored), axis=1)\n        return x\n\n    def update(self, x, fitness_values, current_f_opt):\n        idx = np.argsort(fitness_values)\n        x_mu = x[:, idx[:self.mu]]\n        z_mu = np.linalg.solve(self.eigenspace @ np.diag(np.sqrt(self.eigenvalues)), (x_mu - self.m[:, np.newaxis]) / self.sigma)\n\n        m_old = self.m.copy()\n        self.m = np.sum(x_mu * self.weights[np.newaxis, :], axis=1)\n        self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * np.sum(self.weights)) * (self.eigenspace @ z_mu.mean(axis=1))\n        self.pc = (1 - self.c_cov) * self.pc + np.sqrt(self.c_cov * (2 - self.c_cov) * np.sum(self.weights)) * ((self.m - m_old) / self.sigma)\n        \n        C_temp = self.c_cov_mu * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :]) + \\\n               (1 - self.c_cov_mu) * (self.C)\n\n        self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / np.sqrt(self.dim) - 1))\n\n        self.C = C_temp\n\n        self.eigenvalues, self.eigenspace = np.linalg.eigh(self.C)\n        self.eigenvalues = np.maximum(self.eigenvalues, 1e-12)\n        \n        #Adapt c_cov based on success rate\n        improvement = fitness_values[idx[0]] < current_f_opt\n        self.recent_successes.append(improvement)\n\n        if len(self.recent_successes) > self.popsize:\n            self.recent_successes.pop(0)\n            self.success_rate = np.mean(self.recent_successes)\n\n            # Adjust c_cov based on success rate\n            if self.success_rate > 0.7:\n                self.c_cov_base *= (1 + self.adaptation_rate)\n                self.popsize = min(self.popsize + 1, 2*self.dim)\n\n            elif self.success_rate < 0.3:\n                self.c_cov_base *= (1 - self.adaptation_rate)\n                self.c_cov_base = max(self.c_cov_base, 0.0)  # Ensure c_cov remains non-negative\n                self.popsize = max(self.popsize - 1, 4)\n\n            self.c_cov = self.c_cov_base\n            self.c_cov = (1 / self.mu) * self.c_cov\n            self.mu = self.popsize // 2\n            self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n            self.weights = self.weights / np.sum(self.weights)\n\n\n    def __call__(self, func):\n        self.initialize()\n\n        while self.func_evals < self.budget:\n            x = self.sample()\n\n            fitness_values = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.func_evals += self.popsize\n\n            best_index = np.argmin(fitness_values)\n            if fitness_values[best_index] < self.f_opt:\n                self.f_opt = fitness_values[best_index]\n                self.x_opt = x[:, best_index]\n                self.stagnation_counter = 0  # Reset counter if improvement\n            \n            self.update(x, fitness_values, self.f_opt)\n            self.stagnation_counter += 1\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.initialize()  # Restart CMA-ES\n                self.stagnation_counter = 0  # Reset counter\n                self.recent_successes = [] #Restart with empty list\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 109, in __call__\n  File \"<string>\", line 109, in <listcomp>\nIndexError: index 4 is out of bounds for axis 1 with size 4\n.", "error": "", "parent_ids": ["d3f91dd2-fdbe-430a-8a5d-ae1ecde791e6"], "operator": null, "metadata": {}}
{"id": "25a3c212-dab8-4118-9b4f-782366374216", "fitness": -Infinity, "name": "MirroredCMAES", "description": "CMA-ES with mirrored sampling and adaptive population size based on success rate.", "code": "import numpy as np\n\nclass MirroredCMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damps=None, c_cov_base=None, sigma0=0.2, adaptation_rate=0.1, popsize_adaptation_rate=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.adaptation_rate = adaptation_rate\n        self.popsize_adaptation_rate = popsize_adaptation_rate\n\n        self.mu = self.popsize // 2\n\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n\n        self.m = None\n        self.sigma = None\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.eigenspace = None\n        self.eigenvalues = None\n\n        self.cs = cs\n        self.damps = damps if damps is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1)/(self.dim + 1)) - 1]) + self.cs\n        self.c_cov_base = c_cov_base if c_cov_base is not None else (1 / (self.dim * np.sqrt(self.dim))) * 10\n        self.c_cov = self.c_cov_base\n        self.c_cov_mu = self.c_cov\n        self.c_cov_mu = self.c_cov_mu if self.c_cov_mu <= 1 else 1\n        self.c_cov_mu = self.c_cov_mu if self.c_cov_mu > 0 else 0\n        self.c_cov = (1 / self.mu) * self.c_cov\n\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.func_evals = 0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 50  # Number of iterations to wait before restart\n        self.success_rate = 0.5  # Initialize success rate\n        self.recent_successes = [] #Store recent success/failure\n\n    def initialize(self):\n        self.m = np.random.uniform(-2, 2, size=self.dim)\n        self.sigma = self.sigma0\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.eigenspace = np.eye(self.dim)\n        self.eigenvalues = np.ones(self.dim)\n\n    def sample(self):\n        z = np.random.normal(0, 1, size=(self.dim, self.popsize // 2))\n        x = self.m[:, np.newaxis] + self.sigma * (self.eigenspace @ (np.diag(np.sqrt(self.eigenvalues)) @ z))\n        x_mirrored = self.m[:, np.newaxis] - self.sigma * (self.eigenspace @ (np.diag(np.sqrt(self.eigenvalues)) @ z)) #Mirrored samples\n        x = np.concatenate((x, x_mirrored), axis=1)\n        return x\n\n    def update(self, x, fitness_values, current_f_opt):\n        idx = np.argsort(fitness_values)\n        x_mu = x[:, idx[:self.mu]]\n        z_mu = np.linalg.solve(self.eigenspace @ np.diag(np.sqrt(self.eigenvalues)), (x_mu - self.m[:, np.newaxis]) / self.sigma)\n\n        m_old = self.m.copy()\n        self.m = np.sum(x_mu * self.weights[np.newaxis, :], axis=1)\n        self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * np.sum(self.weights)) * (self.eigenspace @ z_mu.mean(axis=1))\n        self.pc = (1 - self.c_cov) * self.pc + np.sqrt(self.c_cov * (2 - self.c_cov) * np.sum(self.weights)) * ((self.m - m_old) / self.sigma)\n        \n        C_temp = self.c_cov_mu * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :]) + \\\n               (1 - self.c_cov_mu) * (self.C)\n\n        self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / np.sqrt(self.dim) - 1))\n\n        self.C = C_temp\n\n        self.eigenvalues, self.eigenspace = np.linalg.eigh(self.C)\n        self.eigenvalues = np.maximum(self.eigenvalues, 1e-12)\n        \n        #Adapt c_cov based on success rate\n        improvement = fitness_values[idx[0]] < current_f_opt\n        self.recent_successes.append(improvement)\n\n        if len(self.recent_successes) > self.popsize:\n            self.recent_successes.pop(0)\n            self.success_rate = np.mean(self.recent_successes)\n\n            # Adjust c_cov based on success rate\n            if self.success_rate > 0.7:\n                self.c_cov_base *= (1 + self.adaptation_rate)\n                self.popsize = min(self.popsize + 1, 2*self.dim)\n\n            elif self.success_rate < 0.3:\n                self.c_cov_base *= (1 - self.adaptation_rate)\n                self.c_cov_base = max(self.c_cov_base, 0.0)  # Ensure c_cov remains non-negative\n                self.popsize = max(self.popsize - 1, 4)\n\n            self.c_cov = self.c_cov_base\n            self.c_cov = (1 / self.mu) * self.c_cov\n            self.mu = self.popsize // 2\n            self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n            self.weights = self.weights / np.sum(self.weights)\n\n\n    def __call__(self, func):\n        self.initialize()\n\n        while self.func_evals < self.budget:\n            x = self.sample()\n\n            fitness_values = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.func_evals += self.popsize\n\n            best_index = np.argmin(fitness_values)\n            if fitness_values[best_index] < self.f_opt:\n                self.f_opt = fitness_values[best_index]\n                self.x_opt = x[:, best_index]\n                self.stagnation_counter = 0  # Reset counter if improvement\n            \n            self.update(x, fitness_values, self.f_opt)\n            self.stagnation_counter += 1\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.initialize()  # Restart CMA-ES\n                self.stagnation_counter = 0  # Reset counter\n                self.recent_successes = [] #Restart with empty list\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 109, in __call__\n  File \"<string>\", line 109, in <listcomp>\nIndexError: index 4 is out of bounds for axis 1 with size 4\n.", "error": "", "parent_ids": ["d3f91dd2-fdbe-430a-8a5d-ae1ecde791e6"], "operator": null, "metadata": {}}
{"id": "25a3c212-dab8-4118-9b4f-782366374216", "fitness": -Infinity, "name": "MirroredCMAES", "description": "CMA-ES with mirrored sampling and adaptive population size based on success rate.", "code": "import numpy as np\n\nclass MirroredCMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damps=None, c_cov_base=None, sigma0=0.2, adaptation_rate=0.1, popsize_adaptation_rate=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.adaptation_rate = adaptation_rate\n        self.popsize_adaptation_rate = popsize_adaptation_rate\n\n        self.mu = self.popsize // 2\n\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n\n        self.m = None\n        self.sigma = None\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.eigenspace = None\n        self.eigenvalues = None\n\n        self.cs = cs\n        self.damps = damps if damps is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1)/(self.dim + 1)) - 1]) + self.cs\n        self.c_cov_base = c_cov_base if c_cov_base is not None else (1 / (self.dim * np.sqrt(self.dim))) * 10\n        self.c_cov = self.c_cov_base\n        self.c_cov_mu = self.c_cov\n        self.c_cov_mu = self.c_cov_mu if self.c_cov_mu <= 1 else 1\n        self.c_cov_mu = self.c_cov_mu if self.c_cov_mu > 0 else 0\n        self.c_cov = (1 / self.mu) * self.c_cov\n\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.func_evals = 0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 50  # Number of iterations to wait before restart\n        self.success_rate = 0.5  # Initialize success rate\n        self.recent_successes = [] #Store recent success/failure\n\n    def initialize(self):\n        self.m = np.random.uniform(-2, 2, size=self.dim)\n        self.sigma = self.sigma0\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.eigenspace = np.eye(self.dim)\n        self.eigenvalues = np.ones(self.dim)\n\n    def sample(self):\n        z = np.random.normal(0, 1, size=(self.dim, self.popsize // 2))\n        x = self.m[:, np.newaxis] + self.sigma * (self.eigenspace @ (np.diag(np.sqrt(self.eigenvalues)) @ z))\n        x_mirrored = self.m[:, np.newaxis] - self.sigma * (self.eigenspace @ (np.diag(np.sqrt(self.eigenvalues)) @ z)) #Mirrored samples\n        x = np.concatenate((x, x_mirrored), axis=1)\n        return x\n\n    def update(self, x, fitness_values, current_f_opt):\n        idx = np.argsort(fitness_values)\n        x_mu = x[:, idx[:self.mu]]\n        z_mu = np.linalg.solve(self.eigenspace @ np.diag(np.sqrt(self.eigenvalues)), (x_mu - self.m[:, np.newaxis]) / self.sigma)\n\n        m_old = self.m.copy()\n        self.m = np.sum(x_mu * self.weights[np.newaxis, :], axis=1)\n        self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * np.sum(self.weights)) * (self.eigenspace @ z_mu.mean(axis=1))\n        self.pc = (1 - self.c_cov) * self.pc + np.sqrt(self.c_cov * (2 - self.c_cov) * np.sum(self.weights)) * ((self.m - m_old) / self.sigma)\n        \n        C_temp = self.c_cov_mu * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :]) + \\\n               (1 - self.c_cov_mu) * (self.C)\n\n        self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / np.sqrt(self.dim) - 1))\n\n        self.C = C_temp\n\n        self.eigenvalues, self.eigenspace = np.linalg.eigh(self.C)\n        self.eigenvalues = np.maximum(self.eigenvalues, 1e-12)\n        \n        #Adapt c_cov based on success rate\n        improvement = fitness_values[idx[0]] < current_f_opt\n        self.recent_successes.append(improvement)\n\n        if len(self.recent_successes) > self.popsize:\n            self.recent_successes.pop(0)\n            self.success_rate = np.mean(self.recent_successes)\n\n            # Adjust c_cov based on success rate\n            if self.success_rate > 0.7:\n                self.c_cov_base *= (1 + self.adaptation_rate)\n                self.popsize = min(self.popsize + 1, 2*self.dim)\n\n            elif self.success_rate < 0.3:\n                self.c_cov_base *= (1 - self.adaptation_rate)\n                self.c_cov_base = max(self.c_cov_base, 0.0)  # Ensure c_cov remains non-negative\n                self.popsize = max(self.popsize - 1, 4)\n\n            self.c_cov = self.c_cov_base\n            self.c_cov = (1 / self.mu) * self.c_cov\n            self.mu = self.popsize // 2\n            self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n            self.weights = self.weights / np.sum(self.weights)\n\n\n    def __call__(self, func):\n        self.initialize()\n\n        while self.func_evals < self.budget:\n            x = self.sample()\n\n            fitness_values = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.func_evals += self.popsize\n\n            best_index = np.argmin(fitness_values)\n            if fitness_values[best_index] < self.f_opt:\n                self.f_opt = fitness_values[best_index]\n                self.x_opt = x[:, best_index]\n                self.stagnation_counter = 0  # Reset counter if improvement\n            \n            self.update(x, fitness_values, self.f_opt)\n            self.stagnation_counter += 1\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.initialize()  # Restart CMA-ES\n                self.stagnation_counter = 0  # Reset counter\n                self.recent_successes = [] #Restart with empty list\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 109, in __call__\n  File \"<string>\", line 109, in <listcomp>\nIndexError: index 4 is out of bounds for axis 1 with size 4\n.", "error": "", "parent_ids": ["d3f91dd2-fdbe-430a-8a5d-ae1ecde791e6"], "operator": null, "metadata": {}}
{"id": "25a3c212-dab8-4118-9b4f-782366374216", "fitness": -Infinity, "name": "MirroredCMAES", "description": "CMA-ES with mirrored sampling and adaptive population size based on success rate.", "code": "import numpy as np\n\nclass MirroredCMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damps=None, c_cov_base=None, sigma0=0.2, adaptation_rate=0.1, popsize_adaptation_rate=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.adaptation_rate = adaptation_rate\n        self.popsize_adaptation_rate = popsize_adaptation_rate\n\n        self.mu = self.popsize // 2\n\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n\n        self.m = None\n        self.sigma = None\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.eigenspace = None\n        self.eigenvalues = None\n\n        self.cs = cs\n        self.damps = damps if damps is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1)/(self.dim + 1)) - 1]) + self.cs\n        self.c_cov_base = c_cov_base if c_cov_base is not None else (1 / (self.dim * np.sqrt(self.dim))) * 10\n        self.c_cov = self.c_cov_base\n        self.c_cov_mu = self.c_cov\n        self.c_cov_mu = self.c_cov_mu if self.c_cov_mu <= 1 else 1\n        self.c_cov_mu = self.c_cov_mu if self.c_cov_mu > 0 else 0\n        self.c_cov = (1 / self.mu) * self.c_cov\n\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.func_evals = 0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 50  # Number of iterations to wait before restart\n        self.success_rate = 0.5  # Initialize success rate\n        self.recent_successes = [] #Store recent success/failure\n\n    def initialize(self):\n        self.m = np.random.uniform(-2, 2, size=self.dim)\n        self.sigma = self.sigma0\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.eigenspace = np.eye(self.dim)\n        self.eigenvalues = np.ones(self.dim)\n\n    def sample(self):\n        z = np.random.normal(0, 1, size=(self.dim, self.popsize // 2))\n        x = self.m[:, np.newaxis] + self.sigma * (self.eigenspace @ (np.diag(np.sqrt(self.eigenvalues)) @ z))\n        x_mirrored = self.m[:, np.newaxis] - self.sigma * (self.eigenspace @ (np.diag(np.sqrt(self.eigenvalues)) @ z)) #Mirrored samples\n        x = np.concatenate((x, x_mirrored), axis=1)\n        return x\n\n    def update(self, x, fitness_values, current_f_opt):\n        idx = np.argsort(fitness_values)\n        x_mu = x[:, idx[:self.mu]]\n        z_mu = np.linalg.solve(self.eigenspace @ np.diag(np.sqrt(self.eigenvalues)), (x_mu - self.m[:, np.newaxis]) / self.sigma)\n\n        m_old = self.m.copy()\n        self.m = np.sum(x_mu * self.weights[np.newaxis, :], axis=1)\n        self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * np.sum(self.weights)) * (self.eigenspace @ z_mu.mean(axis=1))\n        self.pc = (1 - self.c_cov) * self.pc + np.sqrt(self.c_cov * (2 - self.c_cov) * np.sum(self.weights)) * ((self.m - m_old) / self.sigma)\n        \n        C_temp = self.c_cov_mu * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :]) + \\\n               (1 - self.c_cov_mu) * (self.C)\n\n        self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / np.sqrt(self.dim) - 1))\n\n        self.C = C_temp\n\n        self.eigenvalues, self.eigenspace = np.linalg.eigh(self.C)\n        self.eigenvalues = np.maximum(self.eigenvalues, 1e-12)\n        \n        #Adapt c_cov based on success rate\n        improvement = fitness_values[idx[0]] < current_f_opt\n        self.recent_successes.append(improvement)\n\n        if len(self.recent_successes) > self.popsize:\n            self.recent_successes.pop(0)\n            self.success_rate = np.mean(self.recent_successes)\n\n            # Adjust c_cov based on success rate\n            if self.success_rate > 0.7:\n                self.c_cov_base *= (1 + self.adaptation_rate)\n                self.popsize = min(self.popsize + 1, 2*self.dim)\n\n            elif self.success_rate < 0.3:\n                self.c_cov_base *= (1 - self.adaptation_rate)\n                self.c_cov_base = max(self.c_cov_base, 0.0)  # Ensure c_cov remains non-negative\n                self.popsize = max(self.popsize - 1, 4)\n\n            self.c_cov = self.c_cov_base\n            self.c_cov = (1 / self.mu) * self.c_cov\n            self.mu = self.popsize // 2\n            self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n            self.weights = self.weights / np.sum(self.weights)\n\n\n    def __call__(self, func):\n        self.initialize()\n\n        while self.func_evals < self.budget:\n            x = self.sample()\n\n            fitness_values = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.func_evals += self.popsize\n\n            best_index = np.argmin(fitness_values)\n            if fitness_values[best_index] < self.f_opt:\n                self.f_opt = fitness_values[best_index]\n                self.x_opt = x[:, best_index]\n                self.stagnation_counter = 0  # Reset counter if improvement\n            \n            self.update(x, fitness_values, self.f_opt)\n            self.stagnation_counter += 1\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.initialize()  # Restart CMA-ES\n                self.stagnation_counter = 0  # Reset counter\n                self.recent_successes = [] #Restart with empty list\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 109, in __call__\n  File \"<string>\", line 109, in <listcomp>\nIndexError: index 4 is out of bounds for axis 1 with size 4\n.", "error": "", "parent_ids": ["d3f91dd2-fdbe-430a-8a5d-ae1ecde791e6"], "operator": null, "metadata": {}}
{"id": "25a3c212-dab8-4118-9b4f-782366374216", "fitness": -Infinity, "name": "MirroredCMAES", "description": "CMA-ES with mirrored sampling and adaptive population size based on success rate.", "code": "import numpy as np\n\nclass MirroredCMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damps=None, c_cov_base=None, sigma0=0.2, adaptation_rate=0.1, popsize_adaptation_rate=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.adaptation_rate = adaptation_rate\n        self.popsize_adaptation_rate = popsize_adaptation_rate\n\n        self.mu = self.popsize // 2\n\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n\n        self.m = None\n        self.sigma = None\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.eigenspace = None\n        self.eigenvalues = None\n\n        self.cs = cs\n        self.damps = damps if damps is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1)/(self.dim + 1)) - 1]) + self.cs\n        self.c_cov_base = c_cov_base if c_cov_base is not None else (1 / (self.dim * np.sqrt(self.dim))) * 10\n        self.c_cov = self.c_cov_base\n        self.c_cov_mu = self.c_cov\n        self.c_cov_mu = self.c_cov_mu if self.c_cov_mu <= 1 else 1\n        self.c_cov_mu = self.c_cov_mu if self.c_cov_mu > 0 else 0\n        self.c_cov = (1 / self.mu) * self.c_cov\n\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.func_evals = 0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 50  # Number of iterations to wait before restart\n        self.success_rate = 0.5  # Initialize success rate\n        self.recent_successes = [] #Store recent success/failure\n\n    def initialize(self):\n        self.m = np.random.uniform(-2, 2, size=self.dim)\n        self.sigma = self.sigma0\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.eigenspace = np.eye(self.dim)\n        self.eigenvalues = np.ones(self.dim)\n\n    def sample(self):\n        z = np.random.normal(0, 1, size=(self.dim, self.popsize // 2))\n        x = self.m[:, np.newaxis] + self.sigma * (self.eigenspace @ (np.diag(np.sqrt(self.eigenvalues)) @ z))\n        x_mirrored = self.m[:, np.newaxis] - self.sigma * (self.eigenspace @ (np.diag(np.sqrt(self.eigenvalues)) @ z)) #Mirrored samples\n        x = np.concatenate((x, x_mirrored), axis=1)\n        return x\n\n    def update(self, x, fitness_values, current_f_opt):\n        idx = np.argsort(fitness_values)\n        x_mu = x[:, idx[:self.mu]]\n        z_mu = np.linalg.solve(self.eigenspace @ np.diag(np.sqrt(self.eigenvalues)), (x_mu - self.m[:, np.newaxis]) / self.sigma)\n\n        m_old = self.m.copy()\n        self.m = np.sum(x_mu * self.weights[np.newaxis, :], axis=1)\n        self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * np.sum(self.weights)) * (self.eigenspace @ z_mu.mean(axis=1))\n        self.pc = (1 - self.c_cov) * self.pc + np.sqrt(self.c_cov * (2 - self.c_cov) * np.sum(self.weights)) * ((self.m - m_old) / self.sigma)\n        \n        C_temp = self.c_cov_mu * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :]) + \\\n               (1 - self.c_cov_mu) * (self.C)\n\n        self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / np.sqrt(self.dim) - 1))\n\n        self.C = C_temp\n\n        self.eigenvalues, self.eigenspace = np.linalg.eigh(self.C)\n        self.eigenvalues = np.maximum(self.eigenvalues, 1e-12)\n        \n        #Adapt c_cov based on success rate\n        improvement = fitness_values[idx[0]] < current_f_opt\n        self.recent_successes.append(improvement)\n\n        if len(self.recent_successes) > self.popsize:\n            self.recent_successes.pop(0)\n            self.success_rate = np.mean(self.recent_successes)\n\n            # Adjust c_cov based on success rate\n            if self.success_rate > 0.7:\n                self.c_cov_base *= (1 + self.adaptation_rate)\n                self.popsize = min(self.popsize + 1, 2*self.dim)\n\n            elif self.success_rate < 0.3:\n                self.c_cov_base *= (1 - self.adaptation_rate)\n                self.c_cov_base = max(self.c_cov_base, 0.0)  # Ensure c_cov remains non-negative\n                self.popsize = max(self.popsize - 1, 4)\n\n            self.c_cov = self.c_cov_base\n            self.c_cov = (1 / self.mu) * self.c_cov\n            self.mu = self.popsize // 2\n            self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n            self.weights = self.weights / np.sum(self.weights)\n\n\n    def __call__(self, func):\n        self.initialize()\n\n        while self.func_evals < self.budget:\n            x = self.sample()\n\n            fitness_values = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.func_evals += self.popsize\n\n            best_index = np.argmin(fitness_values)\n            if fitness_values[best_index] < self.f_opt:\n                self.f_opt = fitness_values[best_index]\n                self.x_opt = x[:, best_index]\n                self.stagnation_counter = 0  # Reset counter if improvement\n            \n            self.update(x, fitness_values, self.f_opt)\n            self.stagnation_counter += 1\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.initialize()  # Restart CMA-ES\n                self.stagnation_counter = 0  # Reset counter\n                self.recent_successes = [] #Restart with empty list\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 109, in __call__\n  File \"<string>\", line 109, in <listcomp>\nIndexError: index 4 is out of bounds for axis 1 with size 4\n.", "error": "", "parent_ids": ["d3f91dd2-fdbe-430a-8a5d-ae1ecde791e6"], "operator": null, "metadata": {}}
{"id": "59d13d39-1479-40e4-9243-289322c27bdb", "fitness": 0.0, "name": "DynamicCMAES", "description": "An adaptive CMA-ES variant with a dynamic population size and learning rate based on the problem dimensionality and stagnation detection.", "code": "import numpy as np\n\nclass DynamicCMAES:\n    def __init__(self, budget=10000, dim=10, popsize_factor=4, cs=0.3, damps=None, c_cov_base=None, sigma0=0.2, adaptation_rate=0.1, popsize_adaptation_freq=10):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.popsize_factor = popsize_factor\n        self.popsize = self.calculate_popsize()\n        self.adaptation_rate = adaptation_rate\n        self.popsize_adaptation_freq = popsize_adaptation_freq\n\n        self.mu = self.popsize // 2\n\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n\n        self.m = None\n        self.sigma = None\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.eigenspace = None\n        self.eigenvalues = None\n\n        self.cs = cs\n        self.damps = damps if damps is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1)/(self.dim + 1)) - 1]) + self.cs\n        self.c_cov_base = c_cov_base if c_cov_base is not None else (1 / (self.dim * np.sqrt(self.dim))) * 10\n        self.c_cov = self.c_cov_base\n        self.c_cov_mu = self.c_cov\n        self.c_cov_mu = self.c_cov_mu if self.c_cov_mu <= 1 else 1\n        self.c_cov_mu = self.c_cov_mu if self.c_cov_mu > 0 else 0\n        self.c_cov = (1 / self.mu) * self.c_cov\n\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.func_evals = 0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 50  # Number of iterations to wait before restart\n        self.success_rate = 0.5  # Initialize success rate\n        self.recent_successes = [] #Store recent success/failure\n        self.learning_rate = 1.0  # Initialize learning rate\n        self.learning_rate_decay = 0.995  # Decay factor for learning rate\n\n    def calculate_popsize(self):\n        return int(self.popsize_factor + 3 * np.log(self.dim))\n\n    def initialize(self):\n        self.m = np.random.uniform(-2, 2, size=self.dim)\n        self.sigma = self.sigma0\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.eigenspace = np.eye(self.dim)\n        self.eigenvalues = np.ones(self.dim)\n\n    def sample(self):\n        z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n        x = self.m[:, np.newaxis] + self.sigma * (self.eigenspace @ (np.diag(np.sqrt(self.eigenvalues)) @ z))\n        return x\n\n    def update(self, x, fitness_values, current_f_opt):\n        idx = np.argsort(fitness_values)\n        x_mu = x[:, idx[:self.mu]]\n        z_mu = np.linalg.solve(self.eigenspace @ np.diag(np.sqrt(self.eigenvalues)), (x_mu - self.m[:, np.newaxis]) / self.sigma)\n\n        m_old = self.m.copy()\n        self.m = np.sum(x_mu * self.weights[np.newaxis, :], axis=1)\n        self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * np.sum(self.weights)) * (self.eigenspace @ z_mu.mean(axis=1))\n        self.pc = (1 - self.c_cov) * self.pc + np.sqrt(self.c_cov * (2 - self.c_cov) * np.sum(self.weights)) * ((self.m - m_old) / self.sigma)\n        \n        C_temp = self.c_cov_mu * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :]) + \\\n               (1 - self.c_cov_mu) * (self.C)\n\n        self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / np.sqrt(self.dim) - 1))\n\n        self.C = C_temp\n\n        self.eigenvalues, self.eigenspace = np.linalg.eigh(self.C)\n        self.eigenvalues = np.maximum(self.eigenvalues, 1e-12)\n        \n        #Adapt c_cov based on success rate\n        improvement = fitness_values[idx[0]] < current_f_opt\n        self.recent_successes.append(improvement)\n\n        if len(self.recent_successes) > self.popsize:\n            self.recent_successes.pop(0)\n            self.success_rate = np.mean(self.recent_successes)\n\n            # Adjust c_cov based on success rate\n            if self.success_rate > 0.7:\n                self.c_cov_base *= (1 + self.adaptation_rate * self.learning_rate)\n            elif self.success_rate < 0.3:\n                self.c_cov_base *= (1 - self.adaptation_rate * self.learning_rate)\n                self.c_cov_base = max(self.c_cov_base, 0.0)  # Ensure c_cov remains non-negative\n            self.c_cov = self.c_cov_base\n            self.c_cov = (1 / self.mu) * self.c_cov\n        \n        # Decaying learning rate\n        self.learning_rate *= self.learning_rate_decay\n\n\n    def __call__(self, func):\n        self.initialize()\n\n        while self.func_evals < self.budget:\n            x = self.sample()\n\n            fitness_values = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.func_evals += self.popsize\n\n            best_index = np.argmin(fitness_values)\n            if fitness_values[best_index] < self.f_opt:\n                self.f_opt = fitness_values[best_index]\n                self.x_opt = x[:, best_index]\n                self.stagnation_counter = 0  # Reset counter if improvement\n            \n            self.update(x, fitness_values, self.f_opt)\n            self.stagnation_counter += 1\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                # Dynamically adapt population size based on stagnation\n                self.popsize_factor *= 0.9 #Reduce population size\n                self.popsize = self.calculate_popsize()\n                self.mu = self.popsize // 2\n                self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n                self.weights = self.weights / np.sum(self.weights)\n\n                self.initialize()  # Restart CMA-ES\n                self.stagnation_counter = 0  # Reset counter\n                self.recent_successes = [] #Restart with empty list\n            \n            if self.func_evals % self.popsize_adaptation_freq == 0:\n                 #Periodically increase the population size\n                 self.popsize_factor *= 1.05\n                 self.popsize = self.calculate_popsize()\n                 self.mu = self.popsize // 2\n                 self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n                 self.weights = self.weights / np.sum(self.weights)\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm DynamicCMAES scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d3f91dd2-fdbe-430a-8a5d-ae1ecde791e6"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "60211eab-1eca-4c53-bdd2-33cefb4c8e6c", "fitness": 0.1687711030630606, "name": "CooperativeEnhancedCMAES", "description": "Cooperative Enhanced CMA-ES: Uses a cooperative population of CMA-ES optimizers with intermittent information sharing to enhance exploration and exploitation.", "code": "import numpy as np\n\nclass CooperativeEnhancedCMAES:\n    def __init__(self, budget=10000, dim=10, num_optimizers=5, mu_percentage=0.25, sigma0=0.5, cs=0.2, cmu=0.3, c_cov=0.1, adaptation_rate=0.1, sharing_interval=50):\n        self.budget = budget\n        self.dim = dim\n        self.num_optimizers = num_optimizers\n        self.mu = int(mu_percentage * budget / num_optimizers)\n        self.sigma0 = sigma0\n        self.cs = cs\n        self.cmu = cmu\n        self.c_cov = c_cov\n        self.adaptation_rate = adaptation_rate\n        self.sharing_interval = sharing_interval\n        self.optimizers = []\n        self.best_fitnesses = [np.Inf] * num_optimizers\n        self.best_solutions = [None] * num_optimizers\n        self.evals_per_optimizer = budget // num_optimizers  # Ensure budget is divided evenly\n\n        for i in range(num_optimizers):\n            self.optimizers.append(self._create_optimizer())\n\n    def _create_optimizer(self):\n        return {\n            'mean': None,\n            'sigma': self.sigma0,\n            'C': None,\n            'ps': None,\n            'pc': None,\n            'fitness_history': [],\n            'restart_criterion': 1e-12\n        }\n\n    def __call__(self, func):\n        total_evals = 0\n        global_f_opt = np.Inf\n        global_x_opt = None\n\n        for i in range(self.num_optimizers):\n            optimizer = self.optimizers[i]\n            optimizer['mean'] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n            optimizer['C'] = np.eye(self.dim)\n            optimizer['ps'] = np.zeros(self.dim)\n            optimizer['pc'] = np.zeros(self.dim)\n            evals = 0\n\n            while evals < self.evals_per_optimizer:\n\n                # 1. Sample offspring\n                z = np.random.multivariate_normal(np.zeros(self.dim), optimizer['C'], size=self.mu)\n                x = optimizer['mean'] + optimizer['sigma'] * z\n\n                # Clip to bounds\n                x = np.clip(x, func.bounds.lb, func.bounds.ub)\n\n                f = np.array([func(xi) for xi in x])\n                evals += self.mu\n                total_evals += self.mu\n\n                # 2. Selection and Recombination\n                idx = np.argsort(f)\n                x_best = x[idx[:self.mu]]\n                z_best = z[idx[:self.mu]]\n\n                if np.min(f) < self.best_fitnesses[i]:\n                    self.best_fitnesses[i] = np.min(f)\n                    self.best_solutions[i] = x[idx[0]]\n\n                # 3. Update mean\n                mean_diff = np.mean(z_best, axis=0)\n                optimizer['pc'] = (1 - self.cs) * optimizer['pc'] + np.sqrt(self.cs * (2 - self.cs)) * mean_diff\n                optimizer['mean'] = optimizer['mean'] + self.cmu * optimizer['sigma'] * optimizer['pc']\n\n                # 4. Update covariance matrix\n                optimizer['ps'] = (1 - self.c_cov) * optimizer['ps'] + np.sqrt(self.c_cov * (2 - self.c_cov)) * mean_diff\n                optimizer['C'] = (1 - self.c_cov) * optimizer['C'] + self.c_cov * (np.outer(optimizer['ps'], optimizer['ps']) - optimizer['C'])\n\n                # 5. Adaptive Variance Scaling\n                if len(optimizer['fitness_history']) > 1:\n                    fitness_improvement = optimizer['fitness_history'][-2] - optimizer['fitness_history'][-1]\n                    if fitness_improvement > 0:  # If there is improvement, increase the variance\n                        optimizer['sigma'] *= (1 + self.adaptation_rate * fitness_improvement)\n                    else:  # Decrease the variance\n                        optimizer['sigma'] *= (1 - self.adaptation_rate * abs(fitness_improvement))\n\n                # 6. Update step size (original)\n                optimizer['sigma'] *= np.exp((self.cs / 2) * (np.linalg.norm(optimizer['ps'])**2 / self.dim - 1))\n                optimizer['fitness_history'].append(np.min(f))\n\n                # Check for covariance matrix deterioration\n                if np.linalg.det(optimizer['C']) < optimizer['restart_criterion'] or not np.all(np.linalg.eigvals(optimizer['C']) > 0):\n                    optimizer['C'] = np.eye(self.dim)  # restart C\n                    optimizer['sigma'] = self.sigma0  # Reset sigma\n                    optimizer['ps'] = np.zeros(self.dim)\n                    optimizer['pc'] = np.zeros(self.dim)\n                    optimizer['fitness_history'] = []\n\n                # Cooperation: intermittent sharing of information\n                if evals % self.sharing_interval == 0:\n                    best_idx = np.argmin(self.best_fitnesses)\n                    best_mean = self.optimizers[best_idx]['mean']\n                    optimizer['mean'] = 0.5 * optimizer['mean'] + 0.5 * best_mean  # Move towards best mean\n\n            # Update global best\n            if self.best_fitnesses[i] < global_f_opt:\n                global_f_opt = self.best_fitnesses[i]\n                global_x_opt = self.best_solutions[i]\n        \n        return global_f_opt, global_x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm CooperativeEnhancedCMAES scored 0.169 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e881f5fb-fd03-480d-9ecb-a75e6e8815e6"], "operator": null, "metadata": {"aucs": [0.07436697356036837, 0.061521677866968494, 0.2149448049531113, 0.1649868863486751, 0.11434176471452828, 0.18070224162962445, 0.19869276691174576, 0.2381779025754317, 0.15845196671085782, 0.12420759251299807, 0.1148306065076502, 0.2193443761452173, 0.18082947446630282, 0.12860096283941513, 0.12524621583746354, 0.17739491620172232, 0.22481237510320817, 0.13553229848880255, 0.13395287543015788, 0.4044833824569619]}}
{"id": "992e7748-d86d-432d-88c2-9d9be3f2d19c", "fitness": 0.157912944021176, "name": "ReinforcementCMAES", "description": "A CMA-ES variant that dynamically adjusts its step size (sigma) and covariance matrix adaptation rates based on a reinforcement learning approach using a simple reward/penalty system.", "code": "import numpy as np\n\nclass ReinforcementCMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damps=None, c_cov_base=None, sigma0=0.2, learning_rate=0.1, reward_decay=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n\n        self.mu = self.popsize // 2\n\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n\n        self.m = None\n        self.sigma = None\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.eigenspace = None\n        self.eigenvalues = None\n\n        self.cs = cs\n        self.damps = damps if damps is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1)/(self.dim + 1)) - 1]) + self.cs\n        self.c_cov_base = c_cov_base if c_cov_base is not None else (1 / (self.dim * np.sqrt(self.dim))) * 10\n        self.c_cov = self.c_cov_base\n        self.c_cov_mu = self.c_cov\n        self.c_cov_mu = self.c_cov_mu if self.c_cov_mu <= 1 else 1\n        self.c_cov_mu = self.c_cov_mu if self.c_cov_mu > 0 else 0\n        self.c_cov = (1 / self.mu) * self.c_cov\n        \n        self.learning_rate = learning_rate\n        self.reward_decay = reward_decay\n        self.sigma_reward = 0\n        self.c_cov_reward = 0\n\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.func_evals = 0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 50  # Number of iterations to wait before restart\n\n    def initialize(self):\n        self.m = np.random.uniform(-2, 2, size=self.dim)\n        self.sigma = self.sigma0\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.eigenspace = np.eye(self.dim)\n        self.eigenvalues = np.ones(self.dim)\n\n    def sample(self):\n        z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n        x = self.m[:, np.newaxis] + self.sigma * (self.eigenspace @ (np.diag(np.sqrt(self.eigenvalues)) @ z))\n        return x\n\n    def update(self, x, fitness_values, current_f_opt):\n        idx = np.argsort(fitness_values)\n        x_mu = x[:, idx[:self.mu]]\n        z_mu = np.linalg.solve(self.eigenspace @ np.diag(np.sqrt(self.eigenvalues)), (x_mu - self.m[:, np.newaxis]) / self.sigma)\n\n        m_old = self.m.copy()\n        self.m = np.sum(x_mu * self.weights[np.newaxis, :], axis=1)\n        self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * np.sum(self.weights)) * (self.eigenspace @ z_mu.mean(axis=1))\n        self.pc = (1 - self.c_cov) * self.pc + np.sqrt(self.c_cov * (2 - self.c_cov) * np.sum(self.weights)) * ((self.m - m_old) / self.sigma)\n        \n        C_temp = self.c_cov_mu * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :]) + \\\n               (1 - self.c_cov_mu) * (self.C)\n\n        # Store previous values for reward calculation\n        sigma_old = self.sigma\n        c_cov_base_old = self.c_cov_base\n\n        #Potentially change sigma and c_cov\n        self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / np.sqrt(self.dim) - 1))\n        self.C = C_temp\n\n        self.eigenvalues, self.eigenspace = np.linalg.eigh(self.C)\n        self.eigenvalues = np.maximum(self.eigenvalues, 1e-12)\n        \n        # Reward/Penalty\n        reward = 0\n        if fitness_values[idx[0]] < current_f_opt:\n            reward = 1\n        else:\n            reward = -0.1\n\n        #Update using reward\n        self.sigma_reward = self.reward_decay * self.sigma_reward + reward\n        self.c_cov_reward = self.reward_decay * self.c_cov_reward + reward\n\n        # Apply update to sigma and c_cov\n        self.sigma *= (1 + self.learning_rate * self.sigma_reward)\n        self.c_cov_base *= (1 + self.learning_rate * self.c_cov_reward)\n        self.c_cov_base = max(self.c_cov_base, 0.0)\n\n        self.c_cov = (1 / self.mu) * self.c_cov\n\n\n    def __call__(self, func):\n        self.initialize()\n\n        while self.func_evals < self.budget:\n            x = self.sample()\n\n            fitness_values = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.func_evals += self.popsize\n\n            best_index = np.argmin(fitness_values)\n            if fitness_values[best_index] < self.f_opt:\n                self.f_opt = fitness_values[best_index]\n                self.x_opt = x[:, best_index]\n                self.stagnation_counter = 0  # Reset counter if improvement\n            \n            self.update(x, fitness_values, self.f_opt)\n            self.stagnation_counter += 1\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.initialize()  # Restart CMA-ES\n                self.stagnation_counter = 0  # Reset counter\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm ReinforcementCMAES scored 0.158 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d3f91dd2-fdbe-430a-8a5d-ae1ecde791e6"], "operator": null, "metadata": {"aucs": [0.056111288680433025, 0.18134732300574286, 0.25057973590560845, 0.12049843296248142, 0.11050002015294691, 0.11512278192388625, 0.12222255625019773, 0.15479998109503923, 0.14558723067398682, 0.13784422700540877, 0.15563977699200915, 0.19722389079010116, 0.25425102967733704, 0.147432112121072, 0.13599221918890125, 0.18743726492988577, 0.2015688149857735, 0.15918452595771126, 0.15167963199873213, 0.17323603612626504]}}
{"id": "5183c730-52db-4b61-9500-1965014db985", "fitness": -Infinity, "name": "RingTopologyCMAES", "description": "A multi-population CMA-ES with a ring topology for information sharing and a self-adaptive mutation strength based on population diversity.", "code": "import numpy as np\n\nclass RingTopologyCMAES:\n    def __init__(self, budget=10000, dim=10, num_populations=5, pop_size=None, sigma0=0.5, cs=0.2, cmu=0.3, c_cov=0.1, adaptation_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.num_populations = num_populations\n        self.sigma0 = sigma0\n        self.cs = cs\n        self.cmu = cmu\n        self.c_cov = c_cov\n        self.adaptation_rate = adaptation_rate\n        if pop_size is None:\n            self.pop_size = int(budget / num_populations / 10)  # Automatic population size determination\n        else:\n            self.pop_size = pop_size\n        self.mu = self.pop_size // 2\n        self.populations = []\n        self.best_fitnesses = [np.Inf] * num_populations\n        self.best_solutions = [None] * num_populations\n        self.evals_per_population = budget // num_populations\n\n        for i in range(num_populations):\n            self.populations.append(self._create_population())\n\n    def _create_population(self):\n        return {\n            'mean': None,\n            'sigma': self.sigma0,\n            'C': None,\n            'ps': None,\n            'pc': None,\n            'fitness_history': [],\n            'restart_criterion': 1e-12,\n            'mutation_strength': 1.0 # Initialize mutation strength\n        }\n\n    def __call__(self, func):\n        total_evals = 0\n        global_f_opt = np.Inf\n        global_x_opt = None\n\n        for i in range(self.num_populations):\n            population = self.populations[i]\n            population['mean'] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n            population['C'] = np.eye(self.dim)\n            population['ps'] = np.zeros(self.dim)\n            population['pc'] = np.zeros(self.dim)\n            evals = 0\n\n            while evals < self.evals_per_population:\n                # 1. Sample offspring\n                z = np.random.multivariate_normal(np.zeros(self.dim), population['C'], size=self.pop_size)\n                x = population['mean'] + population['sigma'] * population['mutation_strength'] * z\n\n                # Clip to bounds\n                x = np.clip(x, func.bounds.lb, func.bounds.ub)\n\n                f = np.array([func(xi) for xi in x])\n                evals += self.pop_size\n                total_evals += self.pop_size\n\n                # 2. Selection and Recombination\n                idx = np.argsort(f)\n                x_best = x[idx[:self.mu]]\n                z_best = z[idx[:self.mu]]\n\n                if np.min(f) < self.best_fitnesses[i]:\n                    self.best_fitnesses[i] = np.min(f)\n                    self.best_solutions[i] = x[idx[0]]\n\n                # 3. Update mean\n                mean_diff = np.mean(z_best, axis=0)\n                population['pc'] = (1 - self.cs) * population['pc'] + np.sqrt(self.cs * (2 - self.cs)) * mean_diff\n                population['mean'] = population['mean'] + self.cmu * population['sigma'] * population['mutation_strength'] * population['pc']\n\n                # 4. Update covariance matrix\n                population['ps'] = (1 - self.c_cov) * population['ps'] + np.sqrt(self.c_cov * (2 - self.c_cov)) * mean_diff\n                population['C'] = (1 - self.c_cov) * population['C'] + self.c_cov * (np.outer(population['ps'], population['ps']) - population['C'])\n\n                # 5. Adaptive Variance Scaling (sigma)\n                population['sigma'] *= np.exp((self.cs / 2) * (np.linalg.norm(population['ps'])**2 / self.dim - 1))\n\n                # 6. Self-adaptive mutation strength based on population diversity\n                distances = np.linalg.norm(x - population['mean'], axis=1)\n                diversity = np.std(distances)\n                if diversity > 0.1 * population['sigma']: # If population is diverse\n                    population['mutation_strength'] *= (1 + self.adaptation_rate) # Increase exploration\n                else:\n                    population['mutation_strength'] *= (1 - self.adaptation_rate) # Decrease exploration\n\n                population['mutation_strength'] = np.clip(population['mutation_strength'], 0.1, 10) # Keep mutation strength within reasonable bounds\n                population['fitness_history'].append(np.min(f))\n\n                # Check for covariance matrix deterioration\n                if np.linalg.det(population['C']) < population['restart_criterion'] or not np.all(np.linalg.eigvals(population['C']) > 0):\n                    population['C'] = np.eye(self.dim)  # restart C\n                    population['sigma'] = self.sigma0  # Reset sigma\n                    population['ps'] = np.zeros(self.dim)\n                    population['pc'] = np.zeros(self.dim)\n                    population['fitness_history'] = []\n\n                # Cooperation: Ring Topology Information Sharing\n                if evals % (self.evals_per_population // 2) == 0 and self.num_populations > 1:\n                    # Send to the next population in the ring\n                    next_index = (i + 1) % self.num_populations\n                    self.populations[next_index]['mean'] = 0.7 * self.populations[next_index]['mean'] + 0.3 * population['mean']  # Move towards neighbor's mean\n                    # Receive from the previous population in the ring\n                    prev_index = (i - 1) % self.num_populations\n                    population['mean'] = 0.7 * population['mean'] + 0.3 * self.populations[prev_index]['mean']\n            \n            # Update global best\n            if self.best_fitnesses[i] < global_f_opt:\n                global_f_opt = self.best_fitnesses[i]\n                global_x_opt = self.best_solutions[i]\n\n        return global_f_opt, global_x_opt", "configspace": "", "generation": 3, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 107, in __call__\nTypeError: unsupported operand type(s) for *: 'float' and 'NoneType'\n.", "error": "", "parent_ids": ["60211eab-1eca-4c53-bdd2-33cefb4c8e6c"], "operator": null, "metadata": {}}
{"id": "8fb1111e-f430-41c3-825d-f4ed5fb7e9be", "fitness": -Infinity, "name": "CorrelationAdaptiveCMAES", "description": "CMA-ES with an adaptive covariance matrix update based on the correlation between search steps and function value changes.", "code": "import numpy as np\n\nclass CorrelationAdaptiveCMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damps=None, c_cov_base=None, sigma0=0.2, learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n\n        self.mu = self.popsize // 2\n\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n\n        self.m = None\n        self.sigma = None\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.eigenspace = None\n        self.eigenvalues = None\n\n        self.cs = cs\n        self.damps = damps if damps is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1)/(self.dim + 1)) - 1]) + self.cs\n        self.c_cov_base = c_cov_base if c_cov_base is not None else (1 / (self.dim * np.sqrt(self.dim))) * 10\n        self.c_cov = self.c_cov_base\n        self.c_cov_mu = self.c_cov\n        self.c_cov_mu = self.c_cov_mu if self.c_cov_mu <= 1 else 1\n        self.c_cov_mu = self.c_cov_mu if self.c_cov_mu > 0 else 0\n        self.c_cov = (1 / self.mu) * self.c_cov\n\n        self.learning_rate = learning_rate\n        self.correlation_matrix = np.zeros((self.dim, self.dim))\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.func_evals = 0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 50\n\n    def initialize(self):\n        self.m = np.random.uniform(-2, 2, size=self.dim)\n        self.sigma = self.sigma0\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.eigenspace = np.eye(self.dim)\n        self.eigenvalues = np.ones(self.dim)\n\n    def sample(self):\n        z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n        x = self.m[:, np.newaxis] + self.sigma * (self.eigenspace @ (np.diag(np.sqrt(self.eigenvalues)) @ z))\n        return x\n\n    def update(self, x, fitness_values):\n        idx = np.argsort(fitness_values)\n        x_mu = x[:, idx[:self.mu]]\n        z_mu = np.linalg.solve(self.eigenspace @ np.diag(np.sqrt(self.eigenvalues)), (x_mu - self.m[:, np.newaxis]) / self.sigma)\n\n        m_old = self.m.copy()\n        self.m = np.sum(x_mu * self.weights[np.newaxis, :], axis=1)\n        self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * np.sum(self.weights)) * (self.eigenspace @ z_mu.mean(axis=1))\n        self.pc = (1 - self.c_cov) * self.pc + np.sqrt(self.c_cov * (2 - self.c_cov) * np.sum(self.weights)) * ((self.m - m_old) / self.sigma)\n\n        # Calculate correlation between search steps and fitness changes\n        delta_x = x_mu - m_old[:, np.newaxis]\n        delta_f = fitness_values[idx[:self.mu]] - np.mean(fitness_values)\n        for i in range(self.mu):\n            step = delta_x[:, i]\n            self.correlation_matrix += self.learning_rate * (step[:, np.newaxis] @ step[np.newaxis, :] * delta_f[i])\n\n        # Adaptive covariance matrix update\n        C_temp = (1 - self.learning_rate) * self.C + self.learning_rate * (self.correlation_matrix / self.mu) + self.c_cov_mu * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :]) + \\\n               (1 - self.c_cov_mu) * (self.C)\n        \n        self.C = C_temp\n        self.eigenvalues, self.eigenspace = np.linalg.eigh(self.C)\n        self.eigenvalues = np.maximum(self.eigenvalues, 1e-12)\n        self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / np.sqrt(self.dim) - 1))\n\n    def __call__(self, func):\n        self.initialize()\n\n        while self.func_evals < self.budget:\n            x = self.sample()\n\n            fitness_values = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.func_evals += self.popsize\n\n            best_index = np.argmin(fitness_values)\n            if fitness_values[best_index] < self.f_opt:\n                self.f_opt = fitness_values[best_index]\n                self.x_opt = x[:, best_index]\n                self.stagnation_counter = 0\n            \n            self.update(x, fitness_values)\n            self.stagnation_counter += 1\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.initialize()\n                self.stagnation_counter = 0\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 144, in evaluate\n    algorithm(f_new)\n  File \"<string>\", line 95, in __call__\n  File \"<string>\", line 76, in update\n  File \"<__array_function__ internals>\", line 200, in eigh\n  File \"/home/neocortex/.cache/pypoetry/virtualenvs/blade-xXF9vABH-py3.10/lib/python3.10/site-packages/numpy/linalg/linalg.py\", line 1451, in eigh\n    w, vt = gufunc(a, signature=signature, extobj=extobj)\n  File \"/home/neocortex/.cache/pypoetry/virtualenvs/blade-xXF9vABH-py3.10/lib/python3.10/site-packages/numpy/linalg/linalg.py\", line 95, in _raise_linalgerror_eigenvalues_nonconvergence\n    raise LinAlgError(\"Eigenvalues did not converge\")\nnumpy.linalg.LinAlgError: Eigenvalues did not converge\n.", "error": "", "parent_ids": ["992e7748-d86d-432d-88c2-9d9be3f2d19c"], "operator": null, "metadata": {}}
{"id": "29162c99-653f-40a9-9a88-b5ac82da03e3", "fitness": -Infinity, "name": "OrthogonalCMAES", "description": "CMA-ES with orthogonal subspace sampling and dynamic adaptation of the subspace dimension based on gradient information to accelerate convergence.", "code": "import numpy as np\n\nclass OrthogonalCMAES:\n    def __init__(self, budget=10000, dim=10, popsize_factor=4, cs=0.3, damps=None, c_cov_base=None, sigma0=0.2, subspace_dim_ratio=0.5, adaptation_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.popsize_factor = popsize_factor\n        self.popsize = self.calculate_popsize()\n        self.adaptation_rate = adaptation_rate\n\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n\n        self.m = None\n        self.sigma = None\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.eigenspace = None\n        self.eigenvalues = None\n\n        self.cs = cs\n        self.damps = damps if damps is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1)/(self.dim + 1)) - 1]) + self.cs\n        self.c_cov_base = c_cov_base if c_cov_base is not None else (1 / (self.dim * np.sqrt(self.dim))) * 10\n        self.c_cov = self.c_cov_base\n        self.c_cov_mu = self.c_cov\n        self.c_cov_mu = self.c_cov_mu if self.c_cov_mu <= 1 else 1\n        self.c_cov_mu = self.c_cov_mu if self.c_cov_mu > 0 else 0\n        self.c_cov = (1 / self.mu) * self.c_cov\n\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.func_evals = 0\n\n        self.subspace_dim_ratio = subspace_dim_ratio\n        self.subspace_dim = int(self.dim * self.subspace_dim_ratio)\n        self.subspace_basis = None\n\n    def calculate_popsize(self):\n        return int(self.popsize_factor + 3 * np.log(self.dim))\n\n    def initialize(self):\n        self.m = np.random.uniform(-2, 2, size=self.dim)\n        self.sigma = self.sigma0\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.eigenspace = np.eye(self.dim)\n        self.eigenvalues = np.ones(self.dim)\n        self.subspace_basis = np.random.randn(self.dim, self.subspace_dim)\n        self.subspace_basis, _ = np.linalg.qr(self.subspace_basis)\n\n    def sample(self):\n        z_subspace = np.random.normal(0, 1, size=(self.subspace_dim, self.popsize))\n        x = self.m[:, np.newaxis] + self.sigma * (self.subspace_basis @ z_subspace)\n        return x\n\n    def update(self, x, fitness_values):\n        idx = np.argsort(fitness_values)\n        x_mu = x[:, idx[:self.mu]]\n\n        # Calculate gradients within the subspace\n        gradients = np.zeros((self.dim, self.mu))\n        for i in range(self.mu):\n            # Approximate gradient using finite differences\n            delta = 1e-6  # Small perturbation\n            x_plus = x_mu[:, i] + delta * self.subspace_basis[:, 0]\n            x_minus = x_mu[:, i] - delta * self.subspace_basis[:, 0]\n            \n            #Handle bounds\n            x_plus = np.clip(x_plus, -5.0, 5.0)\n            x_minus = np.clip(x_minus, -5.0, 5.0)\n            \n            fitness_plus = func(x_plus)\n            fitness_minus = func(x_minus)\n            gradients[:, i] = (fitness_plus - fitness_minus) / (2 * delta) * self.subspace_basis[:,0]\n\n        # Update subspace basis based on gradients\n        gradient_mean = np.mean(gradients, axis=1)\n        \n        #Ensure mean is not all zeros\n        if np.linalg.norm(gradient_mean) > 1e-8:\n            new_basis_vector = gradient_mean / np.linalg.norm(gradient_mean)\n        else:\n            new_basis_vector = np.random.randn(self.dim)\n            new_basis_vector /= np.linalg.norm(new_basis_vector)\n\n        self.subspace_basis[:, 0] = new_basis_vector\n        self.subspace_basis, _ = np.linalg.qr(self.subspace_basis) #Re-orthonormalize\n        \n        m_old = self.m.copy()\n        self.m = np.sum(x_mu * self.weights[np.newaxis, :], axis=1)\n        \n        #Simple adaptation of subspace dimension\n        if fitness_values[idx[0]] < self.f_opt:\n            self.subspace_dim_ratio = min(1.0, self.subspace_dim_ratio * (1 + self.adaptation_rate))\n        else:\n            self.subspace_dim_ratio = max(0.1, self.subspace_dim_ratio * (1 - self.adaptation_rate))\n        \n        self.subspace_dim = int(self.dim * self.subspace_dim_ratio)\n\n        new_subspace_basis = np.random.randn(self.dim, self.subspace_dim)\n        new_subspace_basis, _ = np.linalg.qr(new_subspace_basis)\n\n        self.subspace_basis = new_subspace_basis\n\n\n        self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * np.sum(self.weights)) * ((x_mu - self.m[:, np.newaxis]) / self.sigma).mean(axis=1)\n        self.pc = (1 - self.c_cov) * self.pc + np.sqrt(self.c_cov * (2 - self.c_cov) * np.sum(self.weights)) * ((self.m - m_old) / self.sigma)\n        C_temp = self.c_cov_mu * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :]) + \\\n               (1 - self.c_cov_mu) * (self.C)\n\n        self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / np.sqrt(self.dim) - 1))\n\n        self.C = C_temp\n        self.eigenvalues, self.eigenspace = np.linalg.eigh(self.C)\n        self.eigenvalues = np.maximum(self.eigenvalues, 1e-12)\n\n    def __call__(self, func):\n        self.initialize()\n\n        while self.func_evals < self.budget:\n            x = self.sample()\n\n            fitness_values = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.func_evals += self.popsize\n\n            best_index = np.argmin(fitness_values)\n            if fitness_values[best_index] < self.f_opt:\n                self.f_opt = fitness_values[best_index]\n                self.x_opt = x[:, best_index]\n\n            self.update(x, fitness_values)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 135, in __call__\n  File \"<string>\", line 76, in update\nNameError: name 'func' is not defined\n.", "error": "", "parent_ids": ["59d13d39-1479-40e4-9243-289322c27bdb"], "operator": null, "metadata": {}}
{"id": "10db90c7-870a-4650-95dd-20d17ece5d00", "fitness": -Infinity, "name": "MetaCMAES", "description": "CMA-ES with self-adaptation of parameters through a meta-optimizer (Nelder-Mead) on a validation set.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass MetaCMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, sigma0=0.2, validation_fraction=0.2, num_validation_samples=100):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.validation_fraction = validation_fraction\n        self.num_validation_samples = num_validation_samples\n\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n\n        self.m = None\n        self.sigma = None\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.eigenspace = None\n        self.eigenvalues = None\n\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.func_evals = 0\n        self.validation_func = None\n        self.best_params = None\n\n    def initialize(self):\n        self.m = np.random.uniform(-2, 2, size=self.dim)\n        self.sigma = self.sigma0\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.eigenspace = np.eye(self.dim)\n        self.eigenvalues = np.ones(self.dim)\n\n    def sample(self):\n        z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n        x = self.m[:, np.newaxis] + self.sigma * (self.eigenspace @ (np.diag(np.sqrt(self.eigenvalues)) @ z))\n        return x\n\n    def update(self, x, fitness_values):\n        idx = np.argsort(fitness_values)\n        x_mu = x[:, idx[:self.mu]]\n        z_mu = np.linalg.solve(self.eigenspace @ np.diag(np.sqrt(self.eigenvalues)), (x_mu - self.m[:, np.newaxis]) / self.sigma)\n\n        m_old = self.m.copy()\n        self.m = np.sum(x_mu * self.weights[np.newaxis, :], axis=1)\n        self.ps = (1 - self.params['cs']) * self.ps + np.sqrt(self.params['cs'] * (2 - self.params['cs']) * np.sum(self.weights)) * (self.eigenspace @ z_mu.mean(axis=1))\n        self.pc = (1 - self.params['c_cov']) * self.pc + np.sqrt(self.params['c_cov'] * (2 - self.params['c_cov']) * np.sum(self.weights)) * ((self.m - m_old) / self.sigma)\n\n        C_temp = self.params['c_cov_mu'] * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :]) + \\\n               (1 - self.params['c_cov_mu']) * (self.C)\n\n        self.sigma *= np.exp((self.params['cs'] / self.params['damps']) * (np.linalg.norm(self.ps) / np.sqrt(self.dim) - 1))\n        self.C = C_temp\n\n        self.eigenvalues, self.eigenspace = np.linalg.eigh(self.C)\n        self.eigenvalues = np.maximum(self.eigenvalues, 1e-12)\n    \n    def set_params(self, params):\n         self.params = {\n            'cs': params[0],\n            'damps': params[1],\n            'c_cov': params[2],\n            'c_cov_mu': params[3],\n        }\n    \n    def get_params(self):\n        return np.array([self.params['cs'], self.params['damps'], self.params['c_cov'], self.params['c_cov_mu']])\n\n    def validate(self):\n        validation_evals = 0\n        f_val_opt = np.Inf\n        \n        m_temp = self.m.copy()\n        sigma_temp = self.sigma\n        C_temp = self.C.copy()\n        pc_temp = self.pc.copy()\n        ps_temp = self.ps.copy()\n        eigenspace_temp = self.eigenspace.copy()\n        eigenvalues_temp = self.eigenvalues.copy()\n\n        for _ in range(self.num_validation_samples):\n            x = np.random.uniform(-5, 5, size=(self.dim))  # Sample a point in search space\n            f_val = self.validation_func(x)\n            validation_evals += 1\n            f_val_opt = min(f_val_opt, f_val)\n        \n        self.m = m_temp.copy()\n        self.sigma = sigma_temp\n        self.C = C_temp.copy()\n        self.pc = pc_temp.copy()\n        self.ps = ps_temp.copy()\n        self.eigenspace = eigenspace_temp.copy()\n        self.eigenvalues = eigenvalues_temp.copy()\n        \n        return f_val_opt, validation_evals\n        \n\n    def __call__(self, func):\n        # Split budget for training and validation\n        validation_budget = int(self.budget * self.validation_fraction)\n        training_budget = self.budget - validation_budget\n        \n        # Create a validation function\n        def create_validation_func(f):\n            def val_func(x):\n                return f(x)\n            return val_func\n        \n        self.validation_func = create_validation_func(func)\n        \n        # Meta-optimization of CMA-ES parameters using Nelder-Mead\n        def objective(params):\n            self.set_params(params)\n            self.initialize()\n            \n            temp_f_opt = np.Inf\n            evals = 0\n            \n            while evals < (training_budget // 10): #Small amount of training to evaluate.\n                x = self.sample()\n                fitness_values = np.array([func(x[:, i]) for i in range(self.popsize)])\n                evals += self.popsize\n                \n                best_index = np.argmin(fitness_values)\n                temp_f_opt = min(temp_f_opt, fitness_values[best_index])\n                self.update(x, fitness_values)\n            \n            validation_loss, _ = self.validate()  # Minimize validation loss\n            return validation_loss\n\n        #Initial guess\n        initial_params = np.array([0.3, 1 + 2 * np.max([0, np.sqrt((self.mu - 1)/(self.dim + 1)) - 1]) + 0.3, (1 / (self.dim * np.sqrt(self.dim))) * 10, (1 / (self.dim * np.sqrt(self.dim))) * 10])\n        \n        #Define bounds and constraints\n        bounds = [(0.01, 1.0), (1.0, 10.0), (0.001, 1.0), (0.001, 1.0)]\n        \n        # Perform meta-optimization\n        res = minimize(objective, initial_params, method='Nelder-Mead', bounds=bounds, options={'maxiter': 10})  # Reduced maxiter\n        self.best_params = res.x\n        \n        #Final run with best params\n        self.set_params(self.best_params)\n        self.initialize()\n\n        while self.func_evals < training_budget:\n            x = self.sample()\n            fitness_values = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.func_evals += self.popsize\n\n            best_index = np.argmin(fitness_values)\n            if fitness_values[best_index] < self.f_opt:\n                self.f_opt = fitness_values[best_index]\n                self.x_opt = x[:, best_index]\n\n            self.update(x, fitness_values)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 144, in __call__\nNameError: name 'minimize' is not defined\n.", "error": "", "parent_ids": ["992e7748-d86d-432d-88c2-9d9be3f2d19c"], "operator": null, "metadata": {}}
{"id": "45132e83-2960-466d-b6c1-f8998484768a", "fitness": -Infinity, "name": "OrthogonalCMAES", "description": "CMA-ES with orthogonal subspace adaptation, which focuses adaptation on the most promising subspace by rotating the coordinate system.", "code": "import numpy as np\n\nclass OrthogonalCMAES:\n    def __init__(self, budget=10000, dim=10, popsize_factor=4, sigma0=0.2, subspace_dim_factor=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.popsize_factor = popsize_factor\n        self.popsize = int(self.popsize_factor * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.subspace_dim = int(self.dim * subspace_dim_factor) #Dimension of the promising subspace\n\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n\n        self.m = None\n        self.sigma = None\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.eigenspace = None\n        self.eigenvalues = None\n        self.orth_basis = None #Orthogonal basis for the subspace\n\n        self.cs = 0.3\n        self.damps = 1 + 2 * np.max([0, np.sqrt((self.mu - 1)/(self.dim + 1)) - 1]) + self.cs\n        self.c_cov = (1 / self.dim * np.sqrt(self.dim)) * 10\n        self.c_cov = (1 / self.mu) * self.c_cov\n\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.func_evals = 0\n\n    def initialize(self):\n        self.m = np.random.uniform(-2, 2, size=self.dim)\n        self.sigma = self.sigma0\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.eigenspace = np.eye(self.dim)\n        self.eigenvalues = np.ones(self.dim)\n        self.orth_basis = np.eye(self.dim)[:, :self.subspace_dim] #Initialize with first k columns\n\n    def sample(self):\n        z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n        x = self.m[:, np.newaxis] + self.sigma * (self.eigenspace @ (np.diag(np.sqrt(self.eigenvalues)) @ z))\n        return x\n\n    def update(self, x, fitness_values):\n        idx = np.argsort(fitness_values)\n        x_mu = x[:, idx[:self.mu]]\n        z_mu = np.linalg.solve(self.eigenspace @ np.diag(np.sqrt(self.eigenvalues)), (x_mu - self.m[:, np.newaxis]) / self.sigma)\n\n        m_old = self.m.copy()\n        self.m = np.sum(x_mu * self.weights[np.newaxis, :], axis=1)\n        self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * np.sum(self.weights)) * (self.eigenspace @ z_mu.mean(axis=1))\n        self.pc = (1 - self.c_cov) * self.pc + np.sqrt(self.c_cov * (2 - self.c_cov) * np.sum(self.weights)) * ((self.m - m_old) / self.sigma)\n        \n        self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / np.sqrt(self.dim) - 1))\n\n        #Subspace adaptation part:\n        #Project update steps onto the current subspace\n        delta_m = self.m - m_old\n        projected_delta_m = self.orth_basis @ (self.orth_basis.T @ delta_m)\n\n        #Update the covariance matrix only within the subspace.\n        subspace_pc = self.orth_basis.T @ self.pc\n        C_subspace = self.c_cov * (subspace_pc[:, np.newaxis] @ subspace_pc[np.newaxis, :]) + (1 - self.c_cov) * (self.orth_basis.T @ self.C @ self.orth_basis)\n\n        #Reconstruct the full covariance matrix\n        self.C = self.C + self.orth_basis @ (C_subspace - self.orth_basis.T @ self.C @ self.orth_basis) @ self.orth_basis.T\n\n        self.eigenvalues, self.eigenspace = np.linalg.eigh(self.C)\n        self.eigenvalues = np.maximum(self.eigenvalues, 1e-12)\n\n        #Adapt orthogonal basis\n        #Compute the gradient of the fitness function within the subspace (approximate)\n        gradient_subspace = np.zeros(self.subspace_dim)\n        for i in range(self.subspace_dim):\n            delta = 0.01 * self.sigma\n            m_plus = self.m + delta * self.orth_basis[:, i]\n            m_minus = self.m - delta * self.orth_basis[:, i]\n            f_plus = func(m_plus) if self.func_evals + 2 <= self.budget else np.inf\n            f_minus = func(m_minus) if self.func_evals + 2 <= self.budget else np.inf\n\n            if (self.func_evals + 2 <= self.budget):\n                self.func_evals +=2\n\n            gradient_subspace[i] = (f_plus - f_minus) / (2 * delta) if (f_plus != np.inf and f_minus != np.inf) else 0\n\n        #Rotate the orthogonal basis towards the negative gradient direction\n        if np.linalg.norm(gradient_subspace) > 0:\n            rotation_angle = 0.1 #Learning rate for the rotation\n            rotation_axis = np.cross(self.orth_basis[:, 0], -gradient_subspace / np.linalg.norm(gradient_subspace))\n            if np.linalg.norm(rotation_axis) > 1e-8: #Avoid division by zero\n                rotation_axis /= np.linalg.norm(rotation_axis)\n                rotation_matrix = self.rotation_matrix_from_axis_angle(rotation_axis, rotation_angle)\n                self.orth_basis = rotation_matrix @ self.orth_basis #Rotate all basis vectors\n\n\n    def rotation_matrix_from_axis_angle(self, axis, angle):\n        a = np.cos(angle / 2.0)\n        b, c, d = -axis * np.sin(angle / 2.0)\n        aa, bb, cc, dd = a * a, b * b, c * c, d * d\n        bc, ad, ac, ab, bd, cd = b * c, a * d, a * c, a * b, b * d, c * d\n        return np.array([[aa + bb - cc - dd, 2 * (bc + ad), 2 * (bd - ac)],\n                         [2 * (bc - ad), aa + cc - bb - dd, 2 * (cd + ab)],\n                         [2 * (bd + ac), 2 * (cd - ab), aa + dd - bb - cc]])\n\n\n    def __call__(self, func):\n        self.initialize()\n\n        while self.func_evals < self.budget:\n            x = self.sample()\n\n            fitness_values = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.func_evals += self.popsize\n\n            best_index = np.argmin(fitness_values)\n            if fitness_values[best_index] < self.f_opt:\n                self.f_opt = fitness_values[best_index]\n                self.x_opt = x[:, best_index]\n\n            self.update(x, fitness_values)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 125, in __call__\n  File \"<string>\", line 83, in update\nNameError: name 'func' is not defined\n.", "error": "", "parent_ids": ["59d13d39-1479-40e4-9243-289322c27bdb"], "operator": null, "metadata": {}}
{"id": "d1e445c1-5471-41c3-98a4-9105fb2d31c8", "fitness": -Infinity, "name": "MirroredCMAES", "description": "CMA-ES with orthogonal subspace sampling and covariance matrix adaptation based on mirrored sampling to improve exploration.", "code": "import numpy as np\n\nclass MirroredCMAES:\n    def __init__(self, budget=10000, dim=10, popsize_factor=4, sigma0=0.2, c_cov=0.01, c_sigma=0.3, mirrored_ratio=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.popsize_factor = popsize_factor\n        self.popsize = int(self.popsize_factor * np.log(self.dim)) # Adjust popsize based on dimension\n        self.mirrored_ratio = mirrored_ratio #Ratio of mirrored samples\n\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n\n        self.m = None\n        self.sigma = None\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.eigenspace = None\n        self.eigenvalues = None\n\n        self.c_cov = c_cov\n        self.c_sigma = c_sigma\n\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.func_evals = 0\n\n    def initialize(self):\n        self.m = np.random.uniform(-2, 2, size=self.dim)\n        self.sigma = self.sigma0\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.eigenspace = np.eye(self.dim)\n        self.eigenvalues = np.ones(self.dim)\n\n    def sample(self):\n        z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n        x = self.m[:, np.newaxis] + self.sigma * (self.eigenspace @ (np.diag(np.sqrt(self.eigenvalues)) @ z))\n        \n        # Mirrored sampling\n        num_mirrored = int(self.popsize * self.mirrored_ratio)\n        z_mirrored = -z[:, :num_mirrored]  # Mirror the first few samples\n        x_mirrored = self.m[:, np.newaxis] + self.sigma * (self.eigenspace @ (np.diag(np.sqrt(self.eigenvalues)) @ z_mirrored))\n        \n        x = np.concatenate((x, x_mirrored), axis=1)  # Combine original and mirrored samples\n        return x\n\n    def update(self, x, fitness_values):\n        # Handle the potentially doubled population size due to mirrored sampling\n        popsize = self.popsize * (1 + int(self.mirrored_ratio>0))\n        idx = np.argsort(fitness_values)[:self.mu]  # Select only mu best\n\n        x_mu = x[:, idx]\n        z_mu = np.linalg.solve(self.eigenspace @ np.diag(np.sqrt(self.eigenvalues)), (x_mu - self.m[:, np.newaxis]) / self.sigma)\n\n        m_old = self.m.copy()\n        self.m = np.sum(x_mu * self.weights[np.newaxis, :], axis=1)\n\n        self.ps = (1 - self.c_sigma) * self.ps + np.sqrt(self.c_sigma * (2 - self.c_sigma) * np.sum(self.weights)) * (self.eigenspace @ z_mu.mean(axis=1))\n        self.pc = (1 - self.c_cov) * self.pc + np.sqrt(self.c_cov * (2 - self.c_cov) * np.sum(self.weights)) * ((self.m - m_old) / self.sigma)\n\n        C_temp = (1 - self.c_cov) * self.C + self.c_cov * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :]) + \\\n                 self.c_cov * np.sum(self.weights * z_mu * z_mu, axis=1).reshape(self.dim, self.dim)\n\n        self.sigma *= np.exp(self.c_sigma / 2 * (np.linalg.norm(self.ps) / np.sqrt(self.dim) - 1))\n        self.C = C_temp\n        self.eigenvalues, self.eigenspace = np.linalg.eigh(self.C)\n        self.eigenvalues = np.maximum(self.eigenvalues, 1e-12)\n\n\n    def __call__(self, func):\n        self.initialize()\n\n        while self.func_evals < self.budget:\n            x = self.sample()\n            \n            #Handle popsize being doubled\n            fitness_values = np.array([func(x[:, i]) for i in range(x.shape[1])])\n            self.func_evals += x.shape[1]\n\n            best_index = np.argmin(fitness_values)\n            if fitness_values[best_index] < self.f_opt:\n                self.f_opt = fitness_values[best_index]\n                self.x_opt = x[:, best_index]\n            \n            self.update(x, fitness_values)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 90, in __call__\n  File \"<string>\", line 67, in update\nValueError: cannot reshape array of size 2 into shape (2,2)\n.", "error": "", "parent_ids": ["59d13d39-1479-40e4-9243-289322c27bdb"], "operator": null, "metadata": {}}
{"id": "3f5ce838-b62c-4418-ba9f-3711c9a075c9", "fitness": 0.15048142915529286, "name": "OrthogonalCMAES", "description": "CMA-ES with orthogonal subspace sampling to enhance exploration in high-dimensional spaces by focusing on promising directions.", "code": "import numpy as np\n\nclass OrthogonalCMAES:\n    def __init__(self, budget=10000, dim=10, popsize_factor=4, cs=0.3, damps=None, c_cov_base=None, sigma0=0.2, orthogonal_subspace_dim=5):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.popsize_factor = popsize_factor\n        self.popsize = int(self.popsize_factor + 3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n\n        self.m = None\n        self.sigma = None\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.eigenspace = None\n        self.eigenvalues = None\n\n        self.cs = cs\n        self.damps = damps if damps is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1)/(self.dim + 1)) - 1]) + self.cs\n        self.c_cov_base = c_cov_base if c_cov_base is not None else (1 / (self.dim * np.sqrt(self.dim))) * 10\n        self.c_cov = self.c_cov_base\n        self.c_cov_mu = self.c_cov\n        self.c_cov_mu = self.c_cov_mu if self.c_cov_mu <= 1 else 1\n        self.c_cov_mu = self.c_cov_mu if self.c_cov_mu > 0 else 0\n        self.c_cov = (1 / self.mu) * self.c_cov\n\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.func_evals = 0\n        self.orthogonal_subspace_dim = min(orthogonal_subspace_dim, self.dim) #Ensure the subspace dim is smaller than the problem dim\n\n    def initialize(self):\n        self.m = np.random.uniform(-2, 2, size=self.dim)\n        self.sigma = self.sigma0\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.eigenspace = np.eye(self.dim)\n        self.eigenvalues = np.ones(self.dim)\n\n    def sample(self):\n        z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n        x = self.m[:, np.newaxis] + self.sigma * (self.eigenspace @ (np.diag(np.sqrt(self.eigenvalues)) @ z))\n\n        # Create an orthogonal subspace\n        Q, _ = np.linalg.qr(np.random.randn(self.dim, self.orthogonal_subspace_dim))\n\n        # Sample within the orthogonal subspace\n        z_orth = np.random.normal(0, 1, size=(self.orthogonal_subspace_dim, self.popsize))\n        x_orth = self.m[:, np.newaxis] + self.sigma * (Q @ z_orth)\n\n        # Combine original samples with orthogonal subspace samples\n        combined_x = np.concatenate((x, x_orth), axis=1)\n\n        return combined_x\n\n    def update(self, x, fitness_values):\n        idx = np.argsort(fitness_values)\n        x_mu = x[:, idx[:self.mu]]\n        z_mu = np.linalg.solve(self.eigenspace @ np.diag(np.sqrt(self.eigenvalues)), (x_mu - self.m[:, np.newaxis]) / self.sigma)\n\n        m_old = self.m.copy()\n        self.m = np.sum(x_mu * self.weights[np.newaxis, :], axis=1)\n        self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * np.sum(self.weights)) * (self.eigenspace @ z_mu.mean(axis=1))\n        self.pc = (1 - self.c_cov) * self.pc + np.sqrt(self.c_cov * (2 - self.c_cov) * np.sum(self.weights)) * ((self.m - m_old) / self.sigma)\n        \n        C_temp = self.c_cov_mu * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :]) + \\\n               (1 - self.c_cov_mu) * (self.C)\n\n        self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / np.sqrt(self.dim) - 1))\n\n        self.C = C_temp\n\n        self.eigenvalues, self.eigenspace = np.linalg.eigh(self.C)\n        self.eigenvalues = np.maximum(self.eigenvalues, 1e-12)\n\n    def __call__(self, func):\n        self.initialize()\n\n        while self.func_evals < self.budget:\n            x = self.sample()\n            \n            #Evaluate both original samples and subspace samples\n            fitness_values = np.array([func(x[:, i]) for i in range(x.shape[1])])\n            self.func_evals += x.shape[1]\n\n            best_index = np.argmin(fitness_values)\n            if fitness_values[best_index] < self.f_opt:\n                self.f_opt = fitness_values[best_index]\n                self.x_opt = x[:, best_index]\n            \n            self.update(x[:, :self.popsize], fitness_values[:self.popsize]) #Update based on original samples (first popsize samples)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm OrthogonalCMAES scored 0.150 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["59d13d39-1479-40e4-9243-289322c27bdb"], "operator": null, "metadata": {"aucs": [0.06523386217186666, 0.1243870656321584, 0.26549587777757977, 0.11337885435833628, 0.13374400757080407, 0.13262062882292358, 0.16419122233835515, 0.15994908299747457, 0.14218839391463312, 0.11254459141016326, 0.17664535586624897, 0.15092515027341213, 0.2813341507216781, 0.12553098175878985, 0.14057883005498384, 0.21754979963805054, 0.12308522085921447, 0.1344626550648469, 0.10322639807698475, 0.1425564537973525]}}
{"id": "bbef274b-58c5-4ab0-a1c2-a0d24b262ba7", "fitness": -Infinity, "name": "SelfOrganizingDE", "description": "An adaptive Differential Evolution with a self-organizing population and dynamic resource allocation.", "code": "import numpy as np\n\nclass SelfOrganizingDE:\n    def __init__(self, budget=10000, dim=10, pop_size_init=50, min_pop_size=10, max_pop_size=100, F_init=0.5, CR_init=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size_init\n        self.min_pop_size = min_pop_size\n        self.max_pop_size = max_pop_size\n        self.F = F_init\n        self.CR = CR_init\n        self.population = None\n        self.fitness = None\n        self.archive = []\n        self.F_history = []\n        self.CR_history = []\n        self.success_history = []\n        self.convergence_threshold = 1e-6\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 50\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n    def mutate(self, x_i):\n        indices = np.random.choice(self.population.shape[0], size=3, replace=False)\n        x_r1 = self.population[indices[0]]\n        x_r2 = self.population[indices[1]]\n        x_r3 = self.population[indices[2]]\n        return x_r1 + self.F * (x_r2 - x_r3)\n\n    def crossover(self, x_i, v_i):\n        u_i = np.copy(x_i)\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() <= self.CR or j == j_rand:\n                u_i[j] = v_i[j]\n        return u_i\n\n    def repair(self, x, func):\n        return np.clip(x, func.bounds.lb, func.bounds.ub)\n\n    def adjust_population_size(self):\n        # Adaptive population size adjustment based on success rate\n        if len(self.success_history) > 10:\n            success_rate = np.mean(self.success_history[-10:])\n            if success_rate > 0.4 and self.pop_size < self.max_pop_size:\n                self.pop_size = min(self.pop_size + 5, self.max_pop_size)\n                self.population = np.vstack((self.population, np.random.uniform(func.bounds.lb, func.bounds.ub, size=(5, self.dim))))\n                self.fitness = np.concatenate((self.fitness, np.zeros(5) + np.inf)) #lazy init of new individuals\n            elif success_rate < 0.1 and self.pop_size > self.min_pop_size:\n                self.pop_size = max(self.pop_size - 5, self.min_pop_size)\n                indices_to_remove = np.argsort(self.fitness)[-5:]  # Remove worst 5\n                self.population = np.delete(self.population, indices_to_remove, axis=0)\n                self.fitness = np.delete(self.fitness, indices_to_remove)\n\n    def adapt_parameters(self):\n        if len(self.F_history) > 5:\n            self.F = np.mean(self.F_history[-5:])\n        if len(self.CR_history) > 5:\n            self.CR = np.mean(self.CR_history[-5:])\n        self.F = np.clip(self.F, 0.1, 0.9)\n        self.CR = np.clip(self.CR, 0.1, 0.9)\n\n    def check_stagnation(self):\n        if abs(self.f_opt - np.min(self.fitness)) < self.convergence_threshold:\n            self.stagnation_counter += 1\n        else:\n            self.stagnation_counter = 0\n\n        if self.stagnation_counter > self.stagnation_threshold:\n            return True\n        return False\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        iteration = 0\n        while self.budget > 0:\n            iteration += 1\n            \n            u = np.zeros_like(self.population)\n            f_u = np.zeros(self.population.shape[0])\n            \n            for i in range(self.population.shape[0]):\n                v_i = self.mutate(self.population[i])\n                u[i] = self.crossover(self.population[i], v_i)\n                u[i] = self.repair(u[i], func)\n                f_u[i] = func(u[i])\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n            \n            if self.budget <= 0:\n                break\n\n            success_count = 0\n            for i in range(self.population.shape[0]):\n                if f_u[i] < self.fitness[i]:\n                    self.population[i] = u[i]\n                    self.fitness[i] = f_u[i]\n                    success_count += 1\n            \n            self.success_history.append(success_count / self.population.shape[0])\n            self.F_history.append(self.F)\n            self.CR_history.append(self.CR)\n\n            self.f_opt = np.min(self.fitness)\n            self.x_opt = self.population[np.argmin(self.fitness)]\n\n            self.adjust_population_size()\n            self.adapt_parameters()\n            if self.check_stagnation() and self.budget > self.pop_size:\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.budget -= self.pop_size\n                self.stagnation_counter = 0\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 144, in evaluate\n    algorithm(f_new)\n  File \"<string>\", line 114, in __call__\n  File \"<string>\", line 53, in adjust_population_size\nNameError: name 'func' is not defined\n.", "error": "", "parent_ids": ["25e931ff-1787-4df7-a471-cc57d5579b8f"], "operator": null, "metadata": {}}
{"id": "9a3f0641-0889-4841-b36c-2787d62e6cff", "fitness": 0.17821304159651818, "name": "CMAES_with_Memory", "description": "An adaptive CMA-ES variant that incorporates a restart mechanism with memory of past good solutions to guide future search directions.", "code": "import numpy as np\n\nclass CMAES_with_Memory:\n    def __init__(self, budget=10000, dim=10, popsize_factor=4, cs=0.3, damps=None, c_cov_base=None, sigma0=0.2, memory_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.popsize_factor = popsize_factor\n        self.popsize = self.calculate_popsize()\n        self.mu = self.popsize // 2\n\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n\n        self.m = None\n        self.sigma = None\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.eigenspace = None\n        self.eigenvalues = None\n\n        self.cs = cs\n        self.damps = damps if damps is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1)/(self.dim + 1)) - 1]) + self.cs\n        self.c_cov_base = c_cov_base if c_cov_base is not None else (1 / (self.dim * np.sqrt(self.dim))) * 10\n        self.c_cov = self.c_cov_base\n        self.c_cov_mu = self.c_cov\n        self.c_cov_mu = self.c_cov_mu if self.c_cov_mu <= 1 else 1\n        self.c_cov_mu = self.c_cov_mu if self.c_cov_mu > 0 else 0\n        self.c_cov = (1 / self.mu) * self.c_cov\n\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.func_evals = 0\n        self.memory_size = memory_size\n        self.memory_x = []\n        self.memory_f = []\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 50\n\n    def calculate_popsize(self):\n        return int(self.popsize_factor + 3 * np.log(self.dim))\n\n    def initialize(self):\n        self.m = np.random.uniform(-2, 2, size=self.dim)\n        self.sigma = self.sigma0\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.eigenspace = np.eye(self.dim)\n        self.eigenvalues = np.ones(self.dim)\n\n    def sample(self):\n        z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n        x = self.m[:, np.newaxis] + self.sigma * (self.eigenspace @ (np.diag(np.sqrt(self.eigenvalues)) @ z))\n        return x\n\n    def update(self, x, fitness_values):\n        idx = np.argsort(fitness_values)\n        x_mu = x[:, idx[:self.mu]]\n        z_mu = np.linalg.solve(self.eigenspace @ np.diag(np.sqrt(self.eigenvalues)), (x_mu - self.m[:, np.newaxis]) / self.sigma)\n\n        m_old = self.m.copy()\n        self.m = np.sum(x_mu * self.weights[np.newaxis, :], axis=1)\n        self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * np.sum(self.weights)) * (self.eigenspace @ z_mu.mean(axis=1))\n        self.pc = (1 - self.c_cov) * self.pc + np.sqrt(self.c_cov * (2 - self.c_cov) * np.sum(self.weights)) * ((self.m - m_old) / self.sigma)\n        \n        C_temp = self.c_cov_mu * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :]) + \\\n               (1 - self.c_cov_mu) * (self.C)\n\n        self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / np.sqrt(self.dim) - 1))\n\n        self.C = C_temp\n\n        self.eigenvalues, self.eigenspace = np.linalg.eigh(self.C)\n        self.eigenvalues = np.maximum(self.eigenvalues, 1e-12)\n\n\n    def restart(self):\n        if self.memory_x:\n            # Select a point from memory to guide the restart\n            idx = np.argmin(self.memory_f)\n            best_x_from_memory = self.memory_x[idx]\n\n            #Perturb the best solution from memory\n            self.m = best_x_from_memory + np.random.normal(0, 0.1, size=self.dim)\n        else:\n            #If memory is empty, restart randomly\n            self.m = np.random.uniform(-2, 2, size=self.dim)\n\n        self.sigma = self.sigma0\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.eigenspace = np.eye(self.dim)\n        self.eigenvalues = np.ones(self.dim)\n\n    def __call__(self, func):\n        self.initialize()\n\n        while self.func_evals < self.budget:\n            x = self.sample()\n\n            fitness_values = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.func_evals += self.popsize\n\n            best_index = np.argmin(fitness_values)\n            if fitness_values[best_index] < self.f_opt:\n                self.f_opt = fitness_values[best_index]\n                self.x_opt = x[:, best_index]\n\n                # Update memory\n                if len(self.memory_x) < self.memory_size:\n                    self.memory_x.append(self.x_opt)\n                    self.memory_f.append(self.f_opt)\n                else:\n                    worst_mem_idx = np.argmax(self.memory_f)\n                    if self.f_opt < self.memory_f[worst_mem_idx]:\n                        self.memory_x[worst_mem_idx] = self.x_opt\n                        self.memory_f[worst_mem_idx] = self.f_opt\n                self.stagnation_counter = 0  # Reset counter if improvement\n            \n            self.update(x, fitness_values)\n            self.stagnation_counter += 1\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.restart()\n                self.stagnation_counter = 0  # Reset counter\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm CMAES_with_Memory scored 0.178 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["59d13d39-1479-40e4-9243-289322c27bdb"], "operator": null, "metadata": {"aucs": [0.05443234208558512, 0.07345359789633366, 0.2719940348083667, 0.15746567886580598, 0.15759064962625946, 0.20404322206522096, 0.16675927948568003, 0.15606611955369865, 0.15373192558768367, 0.14020238488599002, 0.16113882308477967, 0.14688367911639488, 0.258978468203642, 0.12575249665269472, 0.4654851186846317, 0.2047554622236385, 0.17064496521737127, 0.1993022690675672, 0.1366975601548136, 0.15888275466420598]}}
{"id": "edde5c11-f165-4d85-8807-216ef1302b6b", "fitness": 0.151149710065808, "name": "SACMAES_AR", "description": "Self-Adaptive Covariance Matrix Adaptation Evolution Strategy with Aging and Rejuvenation of individuals based on function evaluations.", "code": "import numpy as np\n\nclass SACMAES_AR:\n    def __init__(self, budget=10000, dim=10, popsize_factor=4, cs=0.3, damps=None, c_cov_base=None, sigma0=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.popsize_factor = popsize_factor\n        self.popsize = self.calculate_popsize()\n        self.mu = self.popsize // 2\n\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n\n        self.m = None\n        self.sigma = None\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.eigenspace = None\n        self.eigenvalues = None\n\n        self.cs = cs\n        self.damps = damps if damps is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1)/(self.dim + 1)) - 1]) + self.cs\n        self.c_cov_base = c_cov_base if c_cov_base is not None else (1 / (self.dim * np.sqrt(self.dim))) * 10\n        self.c_cov = self.c_cov_base\n        self.c_cov_mu = self.c_cov\n        self.c_cov_mu = self.c_cov_mu if self.c_cov_mu <= 1 else 1\n        self.c_cov_mu = self.c_cov_mu if self.c_cov_mu > 0 else 0\n        self.c_cov = (1 / self.mu) * self.c_cov\n\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.func_evals = 0\n\n        self.ages = np.zeros(self.popsize)  # Individual ages based on function evaluations\n        self.max_age = 5 * self.dim #Maximum age before rejuvenation\n        self.rejuvenation_rate = 0.1 #Percentage of individuals to rejuvenate\n\n    def calculate_popsize(self):\n        return int(self.popsize_factor + 3 * np.log(self.dim))\n\n    def initialize(self):\n        self.m = np.random.uniform(-2, 2, size=self.dim)\n        self.sigma = self.sigma0\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.eigenspace = np.eye(self.dim)\n        self.eigenvalues = np.ones(self.dim)\n\n    def sample(self):\n        z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n        x = self.m[:, np.newaxis] + self.sigma * (self.eigenspace @ (np.diag(np.sqrt(self.eigenvalues)) @ z))\n        return x\n\n    def update(self, x, fitness_values):\n        idx = np.argsort(fitness_values)\n        x_mu = x[:, idx[:self.mu]]\n        z_mu = np.linalg.solve(self.eigenspace @ np.diag(np.sqrt(self.eigenvalues)), (x_mu - self.m[:, np.newaxis]) / self.sigma)\n\n        m_old = self.m.copy()\n        self.m = np.sum(x_mu * self.weights[np.newaxis, :], axis=1)\n        self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * np.sum(self.weights)) * (self.eigenspace @ z_mu.mean(axis=1))\n        self.pc = (1 - self.c_cov) * self.pc + np.sqrt(self.c_cov * (2 - self.c_cov) * np.sum(self.weights)) * ((self.m - m_old) / self.sigma)\n        \n        C_temp = self.c_cov_mu * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :]) + \\\n               (1 - self.c_cov_mu) * (self.C)\n\n        self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / np.sqrt(self.dim) - 1))\n\n        self.C = C_temp\n\n        self.eigenvalues, self.eigenspace = np.linalg.eigh(self.C)\n        self.eigenvalues = np.maximum(self.eigenvalues, 1e-12)\n\n    def rejuvenate(self, x, fitness_values):\n        # Identify individuals exceeding max_age\n        rejuvenate_indices = np.where(self.ages >= self.max_age)[0]\n\n        # Rejuvenate a fraction of the population if no individuals exceed max_age\n        if len(rejuvenate_indices) == 0:\n            num_rejuvenate = int(self.rejuvenation_rate * self.popsize)\n            worst_indices = np.argsort(fitness_values)[-num_rejuvenate:]\n            rejuvenate_indices = worst_indices\n        \n        # Re-initialize the positions of selected individuals\n        z = np.random.normal(0, 1, size=(self.dim, len(rejuvenate_indices)))\n        x[:, rejuvenate_indices] = self.m[:, np.newaxis] + self.sigma * (self.eigenspace @ (np.diag(np.sqrt(self.eigenvalues)) @ z))\n\n        # Reset the ages of rejuvenated individuals\n        self.ages[rejuvenate_indices] = 0\n\n        return x\n    \n    def __call__(self, func):\n        self.initialize()\n        self.ages = np.zeros(self.popsize)\n\n        while self.func_evals < self.budget:\n            x = self.sample()\n\n            fitness_values = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.func_evals += self.popsize\n\n            best_index = np.argmin(fitness_values)\n            if fitness_values[best_index] < self.f_opt:\n                self.f_opt = fitness_values[best_index]\n                self.x_opt = x[:, best_index]\n\n            self.update(x, fitness_values)\n            x = self.rejuvenate(x, fitness_values)\n\n            self.ages += 1 #Increment age for all individuals\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm SACMAES_AR scored 0.151 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["59d13d39-1479-40e4-9243-289322c27bdb"], "operator": null, "metadata": {"aucs": [0.058653731273556, 0.06067229300724375, 0.25476431723311865, 0.12538443451040293, 0.13803404419308407, 0.14349622311911248, 0.1528952256024615, 0.1486972138779311, 0.1650073060043511, 0.12293302680961937, 0.13500480713968577, 0.15744875802037006, 0.25595290955005434, 0.13306258012462935, 0.15154223664752042, 0.22935328891255236, 0.14959629520302442, 0.15320623854805504, 0.13643306001807487, 0.15085621152131223]}}
{"id": "1b6bb400-a573-4a1d-b77b-ded94c79bcdd", "fitness": 0.6441692308740767, "name": "MirroredSamplingDE", "description": "A Differential Evolution strategy that uses a mirrored sampling technique to enhance exploration, especially useful when function evaluations are limited.", "code": "import numpy as np\n\nclass MirroredSamplingDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, mirror_rate=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.mirror_rate = mirror_rate\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        \n    def mirrored_sample(self, x, func):\n        \"\"\"Generate a mirrored sample within the bounds.\"\"\"\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        mirrored_x = x + self.mirror_rate * (np.random.rand(self.dim) * (ub - lb) - (x - lb))\n        mirrored_x = np.clip(mirrored_x, lb, ub)\n        return mirrored_x\n        \n\n    def mutate(self, x_i):\n        indices = np.random.choice(self.pop_size, size=3, replace=False)\n        x_r1 = self.population[indices[0]]\n        x_r2 = self.population[indices[1]]\n        x_r3 = self.population[indices[2]]\n        return x_r1 + self.F * (x_r2 - x_r3)\n\n    def crossover(self, x_i, v_i):\n        u_i = np.copy(x_i)\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() <= self.CR or j == j_rand:\n                u_i[j] = v_i[j]\n        return u_i\n\n    def repair(self, x, func):\n        return np.clip(x, func.bounds.lb, func.bounds.ub)\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                v_i = self.mutate(self.population[i])\n                \n                # Crossover\n                u_i = self.crossover(self.population[i], v_i)\n                \n                # Repair\n                u_i = self.repair(u_i, func)\n\n                # Mirrored Sample\n                if np.random.rand() < self.mirror_rate:\n                  u_i = self.mirrored_sample(u_i, func)\n                \n\n                # Selection\n                f_u_i = func(u_i)\n                self.budget -= 1\n                if f_u_i < self.fitness[i]:\n                    self.population[i] = u_i\n                    self.fitness[i] = f_u_i\n\n                    if f_u_i < self.f_opt:\n                        self.f_opt = f_u_i\n                        self.x_opt = u_i\n                \n                if self.budget <= 0:\n                  break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm MirroredSamplingDE scored 0.644 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["25e931ff-1787-4df7-a471-cc57d5579b8f"], "operator": null, "metadata": {"aucs": [0.289452875169131, 0.572104784857532, 0.6100771425041396, 0.8187600822243022, 0.6757219622145128, 0.718301011435396, 0.5317460174555331, 0.5562932476265354, 0.6727938867252523, 0.5587663851924438, 0.8172677611660533, 0.9891156052606963, 0.6251365991264743, 0.6591520135256175, 0.9009057983646319, 0.7203147585271266, 0.5079568995162038, 0.8040626462738452, 0.2583327766987529, 0.5971223636173548]}}
{"id": "97577154-a88e-47d1-b5c3-d3b0c2236b4c", "fitness": 0.5741465588552194, "name": "BlendedAdaptiveOptimizer", "description": "An adaptive population-based algorithm that blends aspects of differential evolution, particle swarm optimization, and covariance matrix adaptation, dynamically adjusting parameters and search strategies based on the observed performance of each individual.", "code": "import numpy as np\n\nclass BlendedAdaptiveOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, de_mutation_factor=0.5, pso_inertia=0.7, cma_learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.de_mutation_factor = de_mutation_factor\n        self.pso_inertia = pso_inertia\n        self.cma_learning_rate = cma_learning_rate\n        self.population = np.random.uniform(-5, 5, size=(pop_size, dim))\n        self.fitness = np.zeros(pop_size)\n        self.velocities = np.zeros((pop_size, dim))\n        self.best_positions = self.population.copy()\n        self.best_fitness = np.full(pop_size, np.inf)\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.covariance_matrix = np.eye(dim)\n        self.adaptation_rate = 0.1\n        self.min_sigma = 1e-6\n        self.sigma = 0.5\n        self.exploration_bias = 0.05\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            # Evaluate fitness\n            for i in range(self.pop_size):\n                if eval_count < self.budget:\n                    f = func(self.population[i])\n                    eval_count += 1\n                    self.fitness[i] = f\n                    if f < self.best_fitness[i]:\n                        self.best_fitness[i] = f\n                        self.best_positions[i] = self.population[i].copy()\n                        if f < self.global_best_fitness:\n                            self.global_best_fitness = f\n                            self.global_best_position = self.population[i].copy()\n\n            # Adaptive Strategy Selection & Parameter Adjustment\n            for i in range(self.pop_size):\n                # Blend DE, PSO, and CMA-like updates\n                r1, r2, r3 = np.random.choice(self.pop_size, 3, replace=False)\n                de_vector = self.best_positions[r1] + self.de_mutation_factor * (self.population[r2] - self.population[r3])\n                pso_velocity = self.pso_inertia * self.velocities[i] + \\\n                               2.0 * np.random.rand(self.dim) * (self.best_positions[i] - self.population[i]) + \\\n                               2.0 * np.random.rand(self.dim) * (self.global_best_position - self.population[i])\n                \n                # Covariance matrix adaptation-inspired sampling\n                z = np.random.multivariate_normal(np.zeros(self.dim), self.covariance_matrix)\n                cma_sample = self.population[i] + self.sigma * z\n\n                # Weighted Averaging\n                weights = np.random.rand(3)\n                weights /= np.sum(weights)\n                new_position = weights[0] * de_vector + weights[1] * (self.population[i] + pso_velocity) + weights[2] * cma_sample\n                new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n                \n                self.velocities[i] = pso_velocity\n                self.population[i] = new_position\n                \n                # Adaptive Sigma Control\n                if self.fitness[i] > self.best_fitness[i]: # No Improvement\n                    self.sigma *= (1 - self.adaptation_rate)\n                else:\n                    self.sigma *= (1 + self.adaptation_rate)\n                self.sigma = max(self.sigma, self.min_sigma)\n\n            # Update Covariance Matrix (Simplified)\n            diff = self.population - np.mean(self.population, axis=0)\n            self.covariance_matrix = (1 - self.cma_learning_rate) * self.covariance_matrix + self.cma_learning_rate * np.cov(diff.T)\n            # Ensure the covariance matrix is positive semi-definite\n            try:\n                _ = np.linalg.cholesky(self.covariance_matrix)\n            except np.linalg.LinAlgError:\n                self.covariance_matrix = np.eye(self.dim)  # Reset if not PSD\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 3, "feedback": "The algorithm BlendedAdaptiveOptimizer scored 0.574 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["60211eab-1eca-4c53-bdd2-33cefb4c8e6c"], "operator": null, "metadata": {"aucs": [0.2298840081612532, 0.2328591563755369, 0.9142891340606953, 0.2592614503759998, 0.33753281947689895, 0.2552386280625728, 0.2409149630706, 0.6761156459035289, 0.9270097041725518, 0.9111259568600515, 0.9386657671800204, 0.9991908604120805, 0.33736022186612624, 0.32595322379752856, 0.9109294244050679, 0.9258925723751392, 0.4272770193301503, 0.9179351785358247, 0.22414002449621062, 0.49135541818655104]}}
{"id": "986e7897-180a-465b-a148-07006020e28a", "fitness": -Infinity, "name": "BayesianAdaptiveOptimizer", "description": "An adaptive metaheuristic that dynamically allocates budget between local search, global exploration and uses a simplified Bayesian optimization to guide the search.", "code": "import numpy as np\nfrom scipy.stats import norm\nfrom scipy.optimize import minimize\n\nclass BayesianAdaptiveOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=5, local_search_probability=0.2, exploration_probability=0.2, acquisition_function=\"EI\", xi=0.01, exploitation_intensity=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.local_search_probability = local_search_probability\n        self.exploration_probability = exploration_probability\n        self.acquisition_function = acquisition_function\n        self.xi = xi\n        self.exploitation_intensity = exploitation_intensity\n        self.population = np.random.uniform(-5, 5, size=(pop_size, dim))\n        self.fitness = np.full(pop_size, np.inf)\n        self.best_fitness = np.inf\n        self.best_position = None\n        self.gaussian_process_mean = np.zeros(dim)\n        self.gaussian_process_std = np.ones(dim)\n\n    def expected_improvement(self, x):\n        mean = np.dot(x, self.gaussian_process_mean)\n        std = np.sqrt(np.dot(x**2, self.gaussian_process_std**2))\n        \n        if std == 0:\n            return 0  # Avoid division by zero\n\n        z = (self.best_fitness - mean - self.xi) / std\n        return (self.best_fitness - mean - self.xi) * norm.cdf(z) + std * norm.pdf(z)\n\n    def local_search(self, func, x_start, budget_fraction):\n         # Define the objective function for local search\n        def objective(x):\n            return func(x)\n\n        # Define the bounds for the optimization\n        bounds = [(func.bounds.lb, func.bounds.ub) for _ in range(self.dim)]\n        \n        # Initialize the best fitness with a large value\n        best_fitness = np.inf\n        best_x = None\n        \n        # Cap the number of iterations based on the budget fraction\n        max_iterations = int(budget_fraction * self.budget)\n        \n        # Perform local optimization using L-BFGS-B\n        result = minimize(objective, x_start, method='L-BFGS-B', bounds=bounds, options={'maxiter': max_iterations})\n        \n        # Update best fitness and best x\n        if result.success and result.fun < best_fitness:\n            best_fitness = result.fun\n            best_x = result.x\n        \n        return best_fitness, best_x\n\n    def exploration_sample(self, func):\n        return np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n    \n    def __call__(self, func):\n        eval_count = 0\n        \n        while eval_count < self.budget:\n            # Evaluate initial population\n            for i in range(self.pop_size):\n                if eval_count < self.budget:\n                    f = func(self.population[i])\n                    eval_count += 1\n                    self.fitness[i] = f\n                    if f < self.best_fitness:\n                        self.best_fitness = f\n                        self.best_position = self.population[i].copy()\n\n            # Select individual for update\n            selected_index = np.random.randint(self.pop_size)\n            \n            # Adaptive Strategy Selection\n            rand = np.random.rand()\n            \n            if rand < self.local_search_probability:\n                # Local Search\n                budget_fraction = self.exploitation_intensity * (1- eval_count / self.budget) #reduce intensity of exploitation over time\n                \n                local_fitness, local_position = self.local_search(func, self.population[selected_index].copy(), budget_fraction)\n                \n                if local_fitness < self.fitness[selected_index]:\n                   self.fitness[selected_index] = local_fitness\n                   self.population[selected_index] = local_position.copy()\n                   if local_fitness < self.best_fitness:\n                       self.best_fitness = local_fitness\n                       self.best_position = local_position.copy()\n            elif rand < self.local_search_probability + self.exploration_probability:\n                # Global Exploration\n                new_sample = self.exploration_sample(func)\n                f = func(new_sample)\n                eval_count += 1\n\n                if f < self.fitness[selected_index]:\n                    self.fitness[selected_index] = f\n                    self.population[selected_index] = new_sample.copy()\n                    if f < self.best_fitness:\n                        self.best_fitness = f\n                        self.best_position = new_sample.copy()\n            else:\n                # Bayesian Optimization Update (Simplified)\n                bounds = [(func.bounds.lb, func.bounds.ub) for _ in range(self.dim)]\n                \n                # Define the acquisition function for optimization\n                def acquisition(x):\n                   return -self.expected_improvement(x) #Scipy minimize finds minimum, so need to take negative of EI\n                \n                # Start the optimization from a random point\n                x0 = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                \n                # Run the optimization\n                result = minimize(acquisition, x0, method='L-BFGS-B', bounds=bounds)\n                \n                # Get the new sample\n                new_sample = result.x\n\n                f = func(new_sample)\n                eval_count += 1\n\n                if f < self.fitness[selected_index]:\n                    self.fitness[selected_index] = f\n                    self.population[selected_index] = new_sample.copy()\n                    if f < self.best_fitness:\n                        self.best_fitness = f\n                        self.best_position = new_sample.copy()\n\n            # Update Gaussian Process (Very simplified)\n            self.gaussian_process_mean = self.best_position\n            self.gaussian_process_std = np.std(self.population, axis=0)\n\n        return self.best_fitness, self.best_position", "configspace": "", "generation": 4, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 116, in __call__\nNameError: name 'minimize' is not defined\n.", "error": "", "parent_ids": ["97577154-a88e-47d1-b5c3-d3b0c2236b4c"], "operator": null, "metadata": {}}
{"id": "986e7897-180a-465b-a148-07006020e28a", "fitness": -Infinity, "name": "BayesianAdaptiveOptimizer", "description": "An adaptive metaheuristic that dynamically allocates budget between local search, global exploration and uses a simplified Bayesian optimization to guide the search.", "code": "import numpy as np\nfrom scipy.stats import norm\nfrom scipy.optimize import minimize\n\nclass BayesianAdaptiveOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=5, local_search_probability=0.2, exploration_probability=0.2, acquisition_function=\"EI\", xi=0.01, exploitation_intensity=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.local_search_probability = local_search_probability\n        self.exploration_probability = exploration_probability\n        self.acquisition_function = acquisition_function\n        self.xi = xi\n        self.exploitation_intensity = exploitation_intensity\n        self.population = np.random.uniform(-5, 5, size=(pop_size, dim))\n        self.fitness = np.full(pop_size, np.inf)\n        self.best_fitness = np.inf\n        self.best_position = None\n        self.gaussian_process_mean = np.zeros(dim)\n        self.gaussian_process_std = np.ones(dim)\n\n    def expected_improvement(self, x):\n        mean = np.dot(x, self.gaussian_process_mean)\n        std = np.sqrt(np.dot(x**2, self.gaussian_process_std**2))\n        \n        if std == 0:\n            return 0  # Avoid division by zero\n\n        z = (self.best_fitness - mean - self.xi) / std\n        return (self.best_fitness - mean - self.xi) * norm.cdf(z) + std * norm.pdf(z)\n\n    def local_search(self, func, x_start, budget_fraction):\n         # Define the objective function for local search\n        def objective(x):\n            return func(x)\n\n        # Define the bounds for the optimization\n        bounds = [(func.bounds.lb, func.bounds.ub) for _ in range(self.dim)]\n        \n        # Initialize the best fitness with a large value\n        best_fitness = np.inf\n        best_x = None\n        \n        # Cap the number of iterations based on the budget fraction\n        max_iterations = int(budget_fraction * self.budget)\n        \n        # Perform local optimization using L-BFGS-B\n        result = minimize(objective, x_start, method='L-BFGS-B', bounds=bounds, options={'maxiter': max_iterations})\n        \n        # Update best fitness and best x\n        if result.success and result.fun < best_fitness:\n            best_fitness = result.fun\n            best_x = result.x\n        \n        return best_fitness, best_x\n\n    def exploration_sample(self, func):\n        return np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n    \n    def __call__(self, func):\n        eval_count = 0\n        \n        while eval_count < self.budget:\n            # Evaluate initial population\n            for i in range(self.pop_size):\n                if eval_count < self.budget:\n                    f = func(self.population[i])\n                    eval_count += 1\n                    self.fitness[i] = f\n                    if f < self.best_fitness:\n                        self.best_fitness = f\n                        self.best_position = self.population[i].copy()\n\n            # Select individual for update\n            selected_index = np.random.randint(self.pop_size)\n            \n            # Adaptive Strategy Selection\n            rand = np.random.rand()\n            \n            if rand < self.local_search_probability:\n                # Local Search\n                budget_fraction = self.exploitation_intensity * (1- eval_count / self.budget) #reduce intensity of exploitation over time\n                \n                local_fitness, local_position = self.local_search(func, self.population[selected_index].copy(), budget_fraction)\n                \n                if local_fitness < self.fitness[selected_index]:\n                   self.fitness[selected_index] = local_fitness\n                   self.population[selected_index] = local_position.copy()\n                   if local_fitness < self.best_fitness:\n                       self.best_fitness = local_fitness\n                       self.best_position = local_position.copy()\n            elif rand < self.local_search_probability + self.exploration_probability:\n                # Global Exploration\n                new_sample = self.exploration_sample(func)\n                f = func(new_sample)\n                eval_count += 1\n\n                if f < self.fitness[selected_index]:\n                    self.fitness[selected_index] = f\n                    self.population[selected_index] = new_sample.copy()\n                    if f < self.best_fitness:\n                        self.best_fitness = f\n                        self.best_position = new_sample.copy()\n            else:\n                # Bayesian Optimization Update (Simplified)\n                bounds = [(func.bounds.lb, func.bounds.ub) for _ in range(self.dim)]\n                \n                # Define the acquisition function for optimization\n                def acquisition(x):\n                   return -self.expected_improvement(x) #Scipy minimize finds minimum, so need to take negative of EI\n                \n                # Start the optimization from a random point\n                x0 = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                \n                # Run the optimization\n                result = minimize(acquisition, x0, method='L-BFGS-B', bounds=bounds)\n                \n                # Get the new sample\n                new_sample = result.x\n\n                f = func(new_sample)\n                eval_count += 1\n\n                if f < self.fitness[selected_index]:\n                    self.fitness[selected_index] = f\n                    self.population[selected_index] = new_sample.copy()\n                    if f < self.best_fitness:\n                        self.best_fitness = f\n                        self.best_position = new_sample.copy()\n\n            # Update Gaussian Process (Very simplified)\n            self.gaussian_process_mean = self.best_position\n            self.gaussian_process_std = np.std(self.population, axis=0)\n\n        return self.best_fitness, self.best_position", "configspace": "", "generation": 4, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 116, in __call__\nNameError: name 'minimize' is not defined\n.", "error": "", "parent_ids": ["97577154-a88e-47d1-b5c3-d3b0c2236b4c"], "operator": null, "metadata": {}}
{"id": "986e7897-180a-465b-a148-07006020e28a", "fitness": -Infinity, "name": "BayesianAdaptiveOptimizer", "description": "An adaptive metaheuristic that dynamically allocates budget between local search, global exploration and uses a simplified Bayesian optimization to guide the search.", "code": "import numpy as np\nfrom scipy.stats import norm\nfrom scipy.optimize import minimize\n\nclass BayesianAdaptiveOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=5, local_search_probability=0.2, exploration_probability=0.2, acquisition_function=\"EI\", xi=0.01, exploitation_intensity=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.local_search_probability = local_search_probability\n        self.exploration_probability = exploration_probability\n        self.acquisition_function = acquisition_function\n        self.xi = xi\n        self.exploitation_intensity = exploitation_intensity\n        self.population = np.random.uniform(-5, 5, size=(pop_size, dim))\n        self.fitness = np.full(pop_size, np.inf)\n        self.best_fitness = np.inf\n        self.best_position = None\n        self.gaussian_process_mean = np.zeros(dim)\n        self.gaussian_process_std = np.ones(dim)\n\n    def expected_improvement(self, x):\n        mean = np.dot(x, self.gaussian_process_mean)\n        std = np.sqrt(np.dot(x**2, self.gaussian_process_std**2))\n        \n        if std == 0:\n            return 0  # Avoid division by zero\n\n        z = (self.best_fitness - mean - self.xi) / std\n        return (self.best_fitness - mean - self.xi) * norm.cdf(z) + std * norm.pdf(z)\n\n    def local_search(self, func, x_start, budget_fraction):\n         # Define the objective function for local search\n        def objective(x):\n            return func(x)\n\n        # Define the bounds for the optimization\n        bounds = [(func.bounds.lb, func.bounds.ub) for _ in range(self.dim)]\n        \n        # Initialize the best fitness with a large value\n        best_fitness = np.inf\n        best_x = None\n        \n        # Cap the number of iterations based on the budget fraction\n        max_iterations = int(budget_fraction * self.budget)\n        \n        # Perform local optimization using L-BFGS-B\n        result = minimize(objective, x_start, method='L-BFGS-B', bounds=bounds, options={'maxiter': max_iterations})\n        \n        # Update best fitness and best x\n        if result.success and result.fun < best_fitness:\n            best_fitness = result.fun\n            best_x = result.x\n        \n        return best_fitness, best_x\n\n    def exploration_sample(self, func):\n        return np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n    \n    def __call__(self, func):\n        eval_count = 0\n        \n        while eval_count < self.budget:\n            # Evaluate initial population\n            for i in range(self.pop_size):\n                if eval_count < self.budget:\n                    f = func(self.population[i])\n                    eval_count += 1\n                    self.fitness[i] = f\n                    if f < self.best_fitness:\n                        self.best_fitness = f\n                        self.best_position = self.population[i].copy()\n\n            # Select individual for update\n            selected_index = np.random.randint(self.pop_size)\n            \n            # Adaptive Strategy Selection\n            rand = np.random.rand()\n            \n            if rand < self.local_search_probability:\n                # Local Search\n                budget_fraction = self.exploitation_intensity * (1- eval_count / self.budget) #reduce intensity of exploitation over time\n                \n                local_fitness, local_position = self.local_search(func, self.population[selected_index].copy(), budget_fraction)\n                \n                if local_fitness < self.fitness[selected_index]:\n                   self.fitness[selected_index] = local_fitness\n                   self.population[selected_index] = local_position.copy()\n                   if local_fitness < self.best_fitness:\n                       self.best_fitness = local_fitness\n                       self.best_position = local_position.copy()\n            elif rand < self.local_search_probability + self.exploration_probability:\n                # Global Exploration\n                new_sample = self.exploration_sample(func)\n                f = func(new_sample)\n                eval_count += 1\n\n                if f < self.fitness[selected_index]:\n                    self.fitness[selected_index] = f\n                    self.population[selected_index] = new_sample.copy()\n                    if f < self.best_fitness:\n                        self.best_fitness = f\n                        self.best_position = new_sample.copy()\n            else:\n                # Bayesian Optimization Update (Simplified)\n                bounds = [(func.bounds.lb, func.bounds.ub) for _ in range(self.dim)]\n                \n                # Define the acquisition function for optimization\n                def acquisition(x):\n                   return -self.expected_improvement(x) #Scipy minimize finds minimum, so need to take negative of EI\n                \n                # Start the optimization from a random point\n                x0 = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                \n                # Run the optimization\n                result = minimize(acquisition, x0, method='L-BFGS-B', bounds=bounds)\n                \n                # Get the new sample\n                new_sample = result.x\n\n                f = func(new_sample)\n                eval_count += 1\n\n                if f < self.fitness[selected_index]:\n                    self.fitness[selected_index] = f\n                    self.population[selected_index] = new_sample.copy()\n                    if f < self.best_fitness:\n                        self.best_fitness = f\n                        self.best_position = new_sample.copy()\n\n            # Update Gaussian Process (Very simplified)\n            self.gaussian_process_mean = self.best_position\n            self.gaussian_process_std = np.std(self.population, axis=0)\n\n        return self.best_fitness, self.best_position", "configspace": "", "generation": 4, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 116, in __call__\nNameError: name 'minimize' is not defined\n.", "error": "", "parent_ids": ["97577154-a88e-47d1-b5c3-d3b0c2236b4c"], "operator": null, "metadata": {}}
{"id": "986e7897-180a-465b-a148-07006020e28a", "fitness": -Infinity, "name": "BayesianAdaptiveOptimizer", "description": "An adaptive metaheuristic that dynamically allocates budget between local search, global exploration and uses a simplified Bayesian optimization to guide the search.", "code": "import numpy as np\nfrom scipy.stats import norm\nfrom scipy.optimize import minimize\n\nclass BayesianAdaptiveOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=5, local_search_probability=0.2, exploration_probability=0.2, acquisition_function=\"EI\", xi=0.01, exploitation_intensity=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.local_search_probability = local_search_probability\n        self.exploration_probability = exploration_probability\n        self.acquisition_function = acquisition_function\n        self.xi = xi\n        self.exploitation_intensity = exploitation_intensity\n        self.population = np.random.uniform(-5, 5, size=(pop_size, dim))\n        self.fitness = np.full(pop_size, np.inf)\n        self.best_fitness = np.inf\n        self.best_position = None\n        self.gaussian_process_mean = np.zeros(dim)\n        self.gaussian_process_std = np.ones(dim)\n\n    def expected_improvement(self, x):\n        mean = np.dot(x, self.gaussian_process_mean)\n        std = np.sqrt(np.dot(x**2, self.gaussian_process_std**2))\n        \n        if std == 0:\n            return 0  # Avoid division by zero\n\n        z = (self.best_fitness - mean - self.xi) / std\n        return (self.best_fitness - mean - self.xi) * norm.cdf(z) + std * norm.pdf(z)\n\n    def local_search(self, func, x_start, budget_fraction):\n         # Define the objective function for local search\n        def objective(x):\n            return func(x)\n\n        # Define the bounds for the optimization\n        bounds = [(func.bounds.lb, func.bounds.ub) for _ in range(self.dim)]\n        \n        # Initialize the best fitness with a large value\n        best_fitness = np.inf\n        best_x = None\n        \n        # Cap the number of iterations based on the budget fraction\n        max_iterations = int(budget_fraction * self.budget)\n        \n        # Perform local optimization using L-BFGS-B\n        result = minimize(objective, x_start, method='L-BFGS-B', bounds=bounds, options={'maxiter': max_iterations})\n        \n        # Update best fitness and best x\n        if result.success and result.fun < best_fitness:\n            best_fitness = result.fun\n            best_x = result.x\n        \n        return best_fitness, best_x\n\n    def exploration_sample(self, func):\n        return np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n    \n    def __call__(self, func):\n        eval_count = 0\n        \n        while eval_count < self.budget:\n            # Evaluate initial population\n            for i in range(self.pop_size):\n                if eval_count < self.budget:\n                    f = func(self.population[i])\n                    eval_count += 1\n                    self.fitness[i] = f\n                    if f < self.best_fitness:\n                        self.best_fitness = f\n                        self.best_position = self.population[i].copy()\n\n            # Select individual for update\n            selected_index = np.random.randint(self.pop_size)\n            \n            # Adaptive Strategy Selection\n            rand = np.random.rand()\n            \n            if rand < self.local_search_probability:\n                # Local Search\n                budget_fraction = self.exploitation_intensity * (1- eval_count / self.budget) #reduce intensity of exploitation over time\n                \n                local_fitness, local_position = self.local_search(func, self.population[selected_index].copy(), budget_fraction)\n                \n                if local_fitness < self.fitness[selected_index]:\n                   self.fitness[selected_index] = local_fitness\n                   self.population[selected_index] = local_position.copy()\n                   if local_fitness < self.best_fitness:\n                       self.best_fitness = local_fitness\n                       self.best_position = local_position.copy()\n            elif rand < self.local_search_probability + self.exploration_probability:\n                # Global Exploration\n                new_sample = self.exploration_sample(func)\n                f = func(new_sample)\n                eval_count += 1\n\n                if f < self.fitness[selected_index]:\n                    self.fitness[selected_index] = f\n                    self.population[selected_index] = new_sample.copy()\n                    if f < self.best_fitness:\n                        self.best_fitness = f\n                        self.best_position = new_sample.copy()\n            else:\n                # Bayesian Optimization Update (Simplified)\n                bounds = [(func.bounds.lb, func.bounds.ub) for _ in range(self.dim)]\n                \n                # Define the acquisition function for optimization\n                def acquisition(x):\n                   return -self.expected_improvement(x) #Scipy minimize finds minimum, so need to take negative of EI\n                \n                # Start the optimization from a random point\n                x0 = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                \n                # Run the optimization\n                result = minimize(acquisition, x0, method='L-BFGS-B', bounds=bounds)\n                \n                # Get the new sample\n                new_sample = result.x\n\n                f = func(new_sample)\n                eval_count += 1\n\n                if f < self.fitness[selected_index]:\n                    self.fitness[selected_index] = f\n                    self.population[selected_index] = new_sample.copy()\n                    if f < self.best_fitness:\n                        self.best_fitness = f\n                        self.best_position = new_sample.copy()\n\n            # Update Gaussian Process (Very simplified)\n            self.gaussian_process_mean = self.best_position\n            self.gaussian_process_std = np.std(self.population, axis=0)\n\n        return self.best_fitness, self.best_position", "configspace": "", "generation": 4, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 116, in __call__\nNameError: name 'minimize' is not defined\n.", "error": "", "parent_ids": ["97577154-a88e-47d1-b5c3-d3b0c2236b4c"], "operator": null, "metadata": {}}
{"id": "986e7897-180a-465b-a148-07006020e28a", "fitness": -Infinity, "name": "BayesianAdaptiveOptimizer", "description": "An adaptive metaheuristic that dynamically allocates budget between local search, global exploration and uses a simplified Bayesian optimization to guide the search.", "code": "import numpy as np\nfrom scipy.stats import norm\nfrom scipy.optimize import minimize\n\nclass BayesianAdaptiveOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=5, local_search_probability=0.2, exploration_probability=0.2, acquisition_function=\"EI\", xi=0.01, exploitation_intensity=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.local_search_probability = local_search_probability\n        self.exploration_probability = exploration_probability\n        self.acquisition_function = acquisition_function\n        self.xi = xi\n        self.exploitation_intensity = exploitation_intensity\n        self.population = np.random.uniform(-5, 5, size=(pop_size, dim))\n        self.fitness = np.full(pop_size, np.inf)\n        self.best_fitness = np.inf\n        self.best_position = None\n        self.gaussian_process_mean = np.zeros(dim)\n        self.gaussian_process_std = np.ones(dim)\n\n    def expected_improvement(self, x):\n        mean = np.dot(x, self.gaussian_process_mean)\n        std = np.sqrt(np.dot(x**2, self.gaussian_process_std**2))\n        \n        if std == 0:\n            return 0  # Avoid division by zero\n\n        z = (self.best_fitness - mean - self.xi) / std\n        return (self.best_fitness - mean - self.xi) * norm.cdf(z) + std * norm.pdf(z)\n\n    def local_search(self, func, x_start, budget_fraction):\n         # Define the objective function for local search\n        def objective(x):\n            return func(x)\n\n        # Define the bounds for the optimization\n        bounds = [(func.bounds.lb, func.bounds.ub) for _ in range(self.dim)]\n        \n        # Initialize the best fitness with a large value\n        best_fitness = np.inf\n        best_x = None\n        \n        # Cap the number of iterations based on the budget fraction\n        max_iterations = int(budget_fraction * self.budget)\n        \n        # Perform local optimization using L-BFGS-B\n        result = minimize(objective, x_start, method='L-BFGS-B', bounds=bounds, options={'maxiter': max_iterations})\n        \n        # Update best fitness and best x\n        if result.success and result.fun < best_fitness:\n            best_fitness = result.fun\n            best_x = result.x\n        \n        return best_fitness, best_x\n\n    def exploration_sample(self, func):\n        return np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n    \n    def __call__(self, func):\n        eval_count = 0\n        \n        while eval_count < self.budget:\n            # Evaluate initial population\n            for i in range(self.pop_size):\n                if eval_count < self.budget:\n                    f = func(self.population[i])\n                    eval_count += 1\n                    self.fitness[i] = f\n                    if f < self.best_fitness:\n                        self.best_fitness = f\n                        self.best_position = self.population[i].copy()\n\n            # Select individual for update\n            selected_index = np.random.randint(self.pop_size)\n            \n            # Adaptive Strategy Selection\n            rand = np.random.rand()\n            \n            if rand < self.local_search_probability:\n                # Local Search\n                budget_fraction = self.exploitation_intensity * (1- eval_count / self.budget) #reduce intensity of exploitation over time\n                \n                local_fitness, local_position = self.local_search(func, self.population[selected_index].copy(), budget_fraction)\n                \n                if local_fitness < self.fitness[selected_index]:\n                   self.fitness[selected_index] = local_fitness\n                   self.population[selected_index] = local_position.copy()\n                   if local_fitness < self.best_fitness:\n                       self.best_fitness = local_fitness\n                       self.best_position = local_position.copy()\n            elif rand < self.local_search_probability + self.exploration_probability:\n                # Global Exploration\n                new_sample = self.exploration_sample(func)\n                f = func(new_sample)\n                eval_count += 1\n\n                if f < self.fitness[selected_index]:\n                    self.fitness[selected_index] = f\n                    self.population[selected_index] = new_sample.copy()\n                    if f < self.best_fitness:\n                        self.best_fitness = f\n                        self.best_position = new_sample.copy()\n            else:\n                # Bayesian Optimization Update (Simplified)\n                bounds = [(func.bounds.lb, func.bounds.ub) for _ in range(self.dim)]\n                \n                # Define the acquisition function for optimization\n                def acquisition(x):\n                   return -self.expected_improvement(x) #Scipy minimize finds minimum, so need to take negative of EI\n                \n                # Start the optimization from a random point\n                x0 = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                \n                # Run the optimization\n                result = minimize(acquisition, x0, method='L-BFGS-B', bounds=bounds)\n                \n                # Get the new sample\n                new_sample = result.x\n\n                f = func(new_sample)\n                eval_count += 1\n\n                if f < self.fitness[selected_index]:\n                    self.fitness[selected_index] = f\n                    self.population[selected_index] = new_sample.copy()\n                    if f < self.best_fitness:\n                        self.best_fitness = f\n                        self.best_position = new_sample.copy()\n\n            # Update Gaussian Process (Very simplified)\n            self.gaussian_process_mean = self.best_position\n            self.gaussian_process_std = np.std(self.population, axis=0)\n\n        return self.best_fitness, self.best_position", "configspace": "", "generation": 4, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 116, in __call__\nNameError: name 'minimize' is not defined\n.", "error": "", "parent_ids": ["97577154-a88e-47d1-b5c3-d3b0c2236b4c"], "operator": null, "metadata": {}}
{"id": "986e7897-180a-465b-a148-07006020e28a", "fitness": -Infinity, "name": "BayesianAdaptiveOptimizer", "description": "An adaptive metaheuristic that dynamically allocates budget between local search, global exploration and uses a simplified Bayesian optimization to guide the search.", "code": "import numpy as np\nfrom scipy.stats import norm\nfrom scipy.optimize import minimize\n\nclass BayesianAdaptiveOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=5, local_search_probability=0.2, exploration_probability=0.2, acquisition_function=\"EI\", xi=0.01, exploitation_intensity=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.local_search_probability = local_search_probability\n        self.exploration_probability = exploration_probability\n        self.acquisition_function = acquisition_function\n        self.xi = xi\n        self.exploitation_intensity = exploitation_intensity\n        self.population = np.random.uniform(-5, 5, size=(pop_size, dim))\n        self.fitness = np.full(pop_size, np.inf)\n        self.best_fitness = np.inf\n        self.best_position = None\n        self.gaussian_process_mean = np.zeros(dim)\n        self.gaussian_process_std = np.ones(dim)\n\n    def expected_improvement(self, x):\n        mean = np.dot(x, self.gaussian_process_mean)\n        std = np.sqrt(np.dot(x**2, self.gaussian_process_std**2))\n        \n        if std == 0:\n            return 0  # Avoid division by zero\n\n        z = (self.best_fitness - mean - self.xi) / std\n        return (self.best_fitness - mean - self.xi) * norm.cdf(z) + std * norm.pdf(z)\n\n    def local_search(self, func, x_start, budget_fraction):\n         # Define the objective function for local search\n        def objective(x):\n            return func(x)\n\n        # Define the bounds for the optimization\n        bounds = [(func.bounds.lb, func.bounds.ub) for _ in range(self.dim)]\n        \n        # Initialize the best fitness with a large value\n        best_fitness = np.inf\n        best_x = None\n        \n        # Cap the number of iterations based on the budget fraction\n        max_iterations = int(budget_fraction * self.budget)\n        \n        # Perform local optimization using L-BFGS-B\n        result = minimize(objective, x_start, method='L-BFGS-B', bounds=bounds, options={'maxiter': max_iterations})\n        \n        # Update best fitness and best x\n        if result.success and result.fun < best_fitness:\n            best_fitness = result.fun\n            best_x = result.x\n        \n        return best_fitness, best_x\n\n    def exploration_sample(self, func):\n        return np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n    \n    def __call__(self, func):\n        eval_count = 0\n        \n        while eval_count < self.budget:\n            # Evaluate initial population\n            for i in range(self.pop_size):\n                if eval_count < self.budget:\n                    f = func(self.population[i])\n                    eval_count += 1\n                    self.fitness[i] = f\n                    if f < self.best_fitness:\n                        self.best_fitness = f\n                        self.best_position = self.population[i].copy()\n\n            # Select individual for update\n            selected_index = np.random.randint(self.pop_size)\n            \n            # Adaptive Strategy Selection\n            rand = np.random.rand()\n            \n            if rand < self.local_search_probability:\n                # Local Search\n                budget_fraction = self.exploitation_intensity * (1- eval_count / self.budget) #reduce intensity of exploitation over time\n                \n                local_fitness, local_position = self.local_search(func, self.population[selected_index].copy(), budget_fraction)\n                \n                if local_fitness < self.fitness[selected_index]:\n                   self.fitness[selected_index] = local_fitness\n                   self.population[selected_index] = local_position.copy()\n                   if local_fitness < self.best_fitness:\n                       self.best_fitness = local_fitness\n                       self.best_position = local_position.copy()\n            elif rand < self.local_search_probability + self.exploration_probability:\n                # Global Exploration\n                new_sample = self.exploration_sample(func)\n                f = func(new_sample)\n                eval_count += 1\n\n                if f < self.fitness[selected_index]:\n                    self.fitness[selected_index] = f\n                    self.population[selected_index] = new_sample.copy()\n                    if f < self.best_fitness:\n                        self.best_fitness = f\n                        self.best_position = new_sample.copy()\n            else:\n                # Bayesian Optimization Update (Simplified)\n                bounds = [(func.bounds.lb, func.bounds.ub) for _ in range(self.dim)]\n                \n                # Define the acquisition function for optimization\n                def acquisition(x):\n                   return -self.expected_improvement(x) #Scipy minimize finds minimum, so need to take negative of EI\n                \n                # Start the optimization from a random point\n                x0 = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                \n                # Run the optimization\n                result = minimize(acquisition, x0, method='L-BFGS-B', bounds=bounds)\n                \n                # Get the new sample\n                new_sample = result.x\n\n                f = func(new_sample)\n                eval_count += 1\n\n                if f < self.fitness[selected_index]:\n                    self.fitness[selected_index] = f\n                    self.population[selected_index] = new_sample.copy()\n                    if f < self.best_fitness:\n                        self.best_fitness = f\n                        self.best_position = new_sample.copy()\n\n            # Update Gaussian Process (Very simplified)\n            self.gaussian_process_mean = self.best_position\n            self.gaussian_process_std = np.std(self.population, axis=0)\n\n        return self.best_fitness, self.best_position", "configspace": "", "generation": 4, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 116, in __call__\nNameError: name 'minimize' is not defined\n.", "error": "", "parent_ids": ["97577154-a88e-47d1-b5c3-d3b0c2236b4c"], "operator": null, "metadata": {}}
{"id": "986e7897-180a-465b-a148-07006020e28a", "fitness": -Infinity, "name": "BayesianAdaptiveOptimizer", "description": "An adaptive metaheuristic that dynamically allocates budget between local search, global exploration and uses a simplified Bayesian optimization to guide the search.", "code": "import numpy as np\nfrom scipy.stats import norm\nfrom scipy.optimize import minimize\n\nclass BayesianAdaptiveOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=5, local_search_probability=0.2, exploration_probability=0.2, acquisition_function=\"EI\", xi=0.01, exploitation_intensity=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.local_search_probability = local_search_probability\n        self.exploration_probability = exploration_probability\n        self.acquisition_function = acquisition_function\n        self.xi = xi\n        self.exploitation_intensity = exploitation_intensity\n        self.population = np.random.uniform(-5, 5, size=(pop_size, dim))\n        self.fitness = np.full(pop_size, np.inf)\n        self.best_fitness = np.inf\n        self.best_position = None\n        self.gaussian_process_mean = np.zeros(dim)\n        self.gaussian_process_std = np.ones(dim)\n\n    def expected_improvement(self, x):\n        mean = np.dot(x, self.gaussian_process_mean)\n        std = np.sqrt(np.dot(x**2, self.gaussian_process_std**2))\n        \n        if std == 0:\n            return 0  # Avoid division by zero\n\n        z = (self.best_fitness - mean - self.xi) / std\n        return (self.best_fitness - mean - self.xi) * norm.cdf(z) + std * norm.pdf(z)\n\n    def local_search(self, func, x_start, budget_fraction):\n         # Define the objective function for local search\n        def objective(x):\n            return func(x)\n\n        # Define the bounds for the optimization\n        bounds = [(func.bounds.lb, func.bounds.ub) for _ in range(self.dim)]\n        \n        # Initialize the best fitness with a large value\n        best_fitness = np.inf\n        best_x = None\n        \n        # Cap the number of iterations based on the budget fraction\n        max_iterations = int(budget_fraction * self.budget)\n        \n        # Perform local optimization using L-BFGS-B\n        result = minimize(objective, x_start, method='L-BFGS-B', bounds=bounds, options={'maxiter': max_iterations})\n        \n        # Update best fitness and best x\n        if result.success and result.fun < best_fitness:\n            best_fitness = result.fun\n            best_x = result.x\n        \n        return best_fitness, best_x\n\n    def exploration_sample(self, func):\n        return np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n    \n    def __call__(self, func):\n        eval_count = 0\n        \n        while eval_count < self.budget:\n            # Evaluate initial population\n            for i in range(self.pop_size):\n                if eval_count < self.budget:\n                    f = func(self.population[i])\n                    eval_count += 1\n                    self.fitness[i] = f\n                    if f < self.best_fitness:\n                        self.best_fitness = f\n                        self.best_position = self.population[i].copy()\n\n            # Select individual for update\n            selected_index = np.random.randint(self.pop_size)\n            \n            # Adaptive Strategy Selection\n            rand = np.random.rand()\n            \n            if rand < self.local_search_probability:\n                # Local Search\n                budget_fraction = self.exploitation_intensity * (1- eval_count / self.budget) #reduce intensity of exploitation over time\n                \n                local_fitness, local_position = self.local_search(func, self.population[selected_index].copy(), budget_fraction)\n                \n                if local_fitness < self.fitness[selected_index]:\n                   self.fitness[selected_index] = local_fitness\n                   self.population[selected_index] = local_position.copy()\n                   if local_fitness < self.best_fitness:\n                       self.best_fitness = local_fitness\n                       self.best_position = local_position.copy()\n            elif rand < self.local_search_probability + self.exploration_probability:\n                # Global Exploration\n                new_sample = self.exploration_sample(func)\n                f = func(new_sample)\n                eval_count += 1\n\n                if f < self.fitness[selected_index]:\n                    self.fitness[selected_index] = f\n                    self.population[selected_index] = new_sample.copy()\n                    if f < self.best_fitness:\n                        self.best_fitness = f\n                        self.best_position = new_sample.copy()\n            else:\n                # Bayesian Optimization Update (Simplified)\n                bounds = [(func.bounds.lb, func.bounds.ub) for _ in range(self.dim)]\n                \n                # Define the acquisition function for optimization\n                def acquisition(x):\n                   return -self.expected_improvement(x) #Scipy minimize finds minimum, so need to take negative of EI\n                \n                # Start the optimization from a random point\n                x0 = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                \n                # Run the optimization\n                result = minimize(acquisition, x0, method='L-BFGS-B', bounds=bounds)\n                \n                # Get the new sample\n                new_sample = result.x\n\n                f = func(new_sample)\n                eval_count += 1\n\n                if f < self.fitness[selected_index]:\n                    self.fitness[selected_index] = f\n                    self.population[selected_index] = new_sample.copy()\n                    if f < self.best_fitness:\n                        self.best_fitness = f\n                        self.best_position = new_sample.copy()\n\n            # Update Gaussian Process (Very simplified)\n            self.gaussian_process_mean = self.best_position\n            self.gaussian_process_std = np.std(self.population, axis=0)\n\n        return self.best_fitness, self.best_position", "configspace": "", "generation": 4, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 116, in __call__\nNameError: name 'minimize' is not defined\n.", "error": "", "parent_ids": ["97577154-a88e-47d1-b5c3-d3b0c2236b4c"], "operator": null, "metadata": {}}
{"id": "b1b3c4d6-e715-4562-8aa5-e77a7b568fac", "fitness": 0.16129706820115441, "name": "HybridSAMetaheuristic", "description": "An adaptive metaheuristic that combines aspects of Differential Evolution, Particle Swarm Optimization, and Simulated Annealing with a dynamically adjusted temperature schedule and a self-adjusting population size.", "code": "import numpy as np\n\nclass HybridSAMetaheuristic:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=20, initial_temp=100.0, cooling_rate=0.95, de_mutation_factor=0.5, pso_inertia=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = initial_pop_size\n        self.pop_size = initial_pop_size\n        self.population = np.random.uniform(-5, 5, size=(self.pop_size, dim))\n        self.fitness = np.zeros(self.pop_size)\n        self.velocities = np.zeros((self.pop_size, dim))\n        self.best_positions = self.population.copy()\n        self.best_fitness = np.full(self.pop_size, np.inf)\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.initial_temp = initial_temp\n        self.temperature = initial_temp\n        self.cooling_rate = cooling_rate\n        self.de_mutation_factor = de_mutation_factor\n        self.pso_inertia = pso_inertia\n        self.adaptation_rate = 0.1\n        self.min_temp = 1e-4\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            # Evaluate fitness\n            for i in range(self.pop_size):\n                if eval_count < self.budget:\n                    f = func(self.population[i])\n                    eval_count += 1\n                    self.fitness[i] = f\n                    if f < self.best_fitness[i]:\n                        self.best_fitness[i] = f\n                        self.best_positions[i] = self.population[i].copy()\n                        if f < self.global_best_fitness:\n                            self.global_best_fitness = f\n                            self.global_best_position = self.population[i].copy()\n\n            # Adaptive Strategy Selection & Parameter Adjustment\n            for i in range(self.pop_size):\n                # Blend DE and PSO\n                r1, r2, r3 = np.random.choice(self.pop_size, 3, replace=False)\n                de_vector = self.best_positions[r1] + self.de_mutation_factor * (self.population[r2] - self.population[r3])\n                pso_velocity = self.pso_inertia * self.velocities[i] + \\\n                               2.0 * np.random.rand(self.dim) * (self.best_positions[i] - self.population[i]) + \\\n                               2.0 * np.random.rand(self.dim) * (self.global_best_position - self.population[i])\n\n                new_position = self.population[i] + 0.5 * (de_vector - self.population[i]) + 0.5 * pso_velocity\n                new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n\n                # Simulated Annealing acceptance criterion\n                delta_e = func(new_position) - self.fitness[i] # Evaluate new position here.\n                eval_count += 1\n                if eval_count >= self.budget:\n                  break\n                \n                if delta_e < 0 or np.random.rand() < np.exp(-delta_e / self.temperature):\n                    self.population[i] = new_position\n                    self.fitness[i] = func(self.population[i]) # Evaluate new position.\n                    eval_count += 1\n                    if eval_count >= self.budget:\n                      break\n                    if self.fitness[i] < self.best_fitness[i]:\n                        self.best_fitness[i] = self.fitness[i]\n                        self.best_positions[i] = self.population[i].copy()\n                        if self.fitness[i] < self.global_best_fitness:\n                            self.global_best_fitness = self.fitness[i]\n                            self.global_best_position = self.population[i].copy()\n                \n                self.velocities[i] = pso_velocity\n\n            # Temperature Cooling\n            self.temperature *= self.cooling_rate\n            self.temperature = max(self.temperature, self.min_temp)\n\n            # Population Size Adjustment (Adaptive)\n            improvement_rate = np.sum(self.fitness > self.best_fitness) / self.pop_size\n            if improvement_rate > 0.2 and self.pop_size < 2 * self.initial_pop_size:\n                self.pop_size = min(self.pop_size + 1, 2 * self.initial_pop_size)\n                self.population = np.vstack((self.population, np.random.uniform(-5, 5, size=(1, self.dim))))\n                self.fitness = np.append(self.fitness, np.inf)\n                self.velocities = np.vstack((self.velocities, np.zeros((1, self.dim))))\n                self.best_positions = np.vstack((self.best_positions, self.population[-1].copy()))\n                self.best_fitness = np.append(self.best_fitness, np.inf)\n            elif improvement_rate < 0.05 and self.pop_size > self.initial_pop_size // 2:\n                self.pop_size = max(self.pop_size - 1, self.initial_pop_size // 2)\n                self.population = self.population[:self.pop_size]\n                self.fitness = self.fitness[:self.pop_size]\n                self.velocities = self.velocities[:self.pop_size]\n                self.best_positions = self.best_positions[:self.pop_size]\n                self.best_fitness = self.best_fitness[:self.pop_size]\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 4, "feedback": "The algorithm HybridSAMetaheuristic scored 0.161 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["97577154-a88e-47d1-b5c3-d3b0c2236b4c"], "operator": null, "metadata": {"aucs": [0.17212784143360915, 0.3117633631698541, 0]}}
{"id": "130cbd96-6ada-44e9-bfbf-3888c5a48262", "fitness": -Infinity, "name": "AdaptiveOrthogonalCMAES", "description": "A CMA-ES variant that adaptively adjusts its covariance matrix and step size based on the success rate of improving solutions and incorporates orthogonal exploration to enhance diversity.", "code": "import numpy as np\n\nclass AdaptiveOrthogonalCMAES:\n    def __init__(self, budget=10000, dim=10, popsize_factor=4, cs=0.3, damps=None, c_cov_base=None, sigma0=0.2, orthogonal_trials=5):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.popsize_factor = popsize_factor\n        self.popsize = self.calculate_popsize()\n        self.mu = self.popsize // 2\n\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n\n        self.m = None\n        self.sigma = None\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.eigenspace = None\n        self.eigenvalues = None\n\n        self.cs = cs\n        self.damps = damps if damps is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1)/(self.dim + 1)) - 1]) + self.cs\n        self.c_cov_base = c_cov_base if c_cov_base is not None else (1 / (self.dim * np.sqrt(self.dim))) * 10\n        self.c_cov = self.c_cov_base\n        self.c_cov_mu = self.c_cov\n        self.c_cov_mu = self.c_cov_mu if self.c_cov_mu <= 1 else 1\n        self.c_cov_mu = self.c_cov_mu if self.c_cov_mu > 0 else 0\n        self.c_cov = (1 / self.mu) * self.c_cov\n\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.func_evals = 0\n        self.success_rate = 0.5  # Initial success rate\n        self.success_history = []\n\n        self.orthogonal_trials = orthogonal_trials\n\n    def calculate_popsize(self):\n        return int(self.popsize_factor + 3 * np.log(self.dim))\n\n    def initialize(self):\n        self.m = np.random.uniform(-2, 2, size=self.dim)\n        self.sigma = self.sigma0\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.eigenspace = np.eye(self.dim)\n        self.eigenvalues = np.ones(self.dim)\n\n    def sample(self):\n        z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n        x = self.m[:, np.newaxis] + self.sigma * (self.eigenspace @ (np.diag(np.sqrt(self.eigenvalues)) @ z))\n        return x\n\n    def orthogonal_sampling(self, func):\n        # Generate orthogonal directions\n        H = np.random.normal(0, 1, size=(self.dim, self.orthogonal_trials))\n        Q, _ = np.linalg.qr(H)\n\n        best_f = np.Inf\n        best_x = None\n\n        for i in range(self.orthogonal_trials):\n            direction = Q[:, i]\n            # Sample along this direction\n            x_trial = self.m + self.sigma * direction\n            f_trial = func(x_trial)\n            self.func_evals += 1\n\n            if f_trial < best_f:\n                best_f = f_trial\n                best_x = x_trial\n\n        return best_f, best_x\n\n    def update(self, x, fitness_values):\n        idx = np.argsort(fitness_values)\n        x_mu = x[:, idx[:self.mu]]\n        z_mu = np.linalg.solve(self.eigenspace @ np.diag(np.sqrt(self.eigenvalues)), (x_mu - self.m[:, np.newaxis]) / self.sigma)\n\n        m_old = self.m.copy()\n        self.m = np.sum(x_mu * self.weights[np.newaxis, :], axis=1)\n        self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * np.sum(self.weights)) * (self.eigenspace @ z_mu.mean(axis=1))\n        self.pc = (1 - self.c_cov) * self.pc + np.sqrt(self.c_cov * (2 - self.c_cov) * np.sum(self.weights)) * ((self.m - m_old) / self.sigma)\n        \n        C_temp = self.c_cov_mu * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :]) + \\\n               (1 - self.c_cov_mu) * (self.C)\n\n        self.C = C_temp\n\n        self.eigenvalues, self.eigenspace = np.linalg.eigh(self.C)\n        self.eigenvalues = np.maximum(self.eigenvalues, 1e-12)\n\n        # Update success rate and adapt sigma\n        improvement = np.min(fitness_values) < self.f_opt\n        self.success_history.append(improvement)\n        if len(self.success_history) > 10:\n            self.success_history.pop(0)\n        self.success_rate = np.mean(self.success_history)\n\n        if self.success_rate > 0.6:\n            self.sigma *= np.exp(self.cs / self.damps)  # Increase step size\n        elif self.success_rate < 0.4:\n            self.sigma *= np.exp(-self.cs / self.damps) # Decrease step size\n        \n    def __call__(self, func):\n        self.initialize()\n\n        while self.func_evals < self.budget:\n            x = self.sample()\n\n            fitness_values = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.func_evals += self.popsize\n\n            best_index = np.argmin(fitness_values)\n            if fitness_values[best_index] < self.f_opt:\n                self.f_opt = fitness_values[best_index]\n                self.x_opt = x[:, best_index]\n\n            # Perform orthogonal sampling\n            f_ortho, x_ortho = self.orthogonal_sampling(func)\n            if f_ortho < self.f_opt:\n                self.f_opt = f_ortho\n                self.x_opt = x_ortho\n\n            self.update(x, fitness_values)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 123, in __call__\n  File \"<string>\", line 66, in orthogonal_sampling\nIndexError: index 2 is out of bounds for axis 1 with size 2\n.", "error": "", "parent_ids": ["edde5c11-f165-4d85-8807-216ef1302b6b"], "operator": null, "metadata": {}}
{"id": "3b169888-62a2-4ab4-8ad8-df1ff67213a5", "fitness": 0.0, "name": "AdaptiveCMAES", "description": "A CMA-ES variant with a dynamic population size adjustment based on the success rate of improving the objective function and an archive of past solutions to influence restarts.", "code": "import numpy as np\n\nclass AdaptiveCMAES:\n    def __init__(self, budget=10000, dim=10, initial_popsize=None, cs=0.3, damps=None, c_cov_base=None, sigma0=0.2, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.initial_popsize = initial_popsize if initial_popsize is not None else 4 + int(3 * np.log(dim)) # Default popsize\n        self.popsize = self.initial_popsize\n        self.mu = self.popsize // 2\n\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n\n        self.m = None\n        self.sigma = None\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.eigenspace = None\n        self.eigenvalues = None\n\n        self.cs = cs\n        self.damps = damps if damps is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1)/(self.dim + 1)) - 1]) + self.cs\n        self.c_cov_base = c_cov_base if c_cov_base is not None else (1 / (self.dim * np.sqrt(self.dim))) * 10\n        self.c_cov = self.c_cov_base\n        self.c_cov_mu = self.c_cov\n        self.c_cov_mu = self.c_cov_mu if self.c_cov_mu <= 1 else 1\n        self.c_cov_mu = self.c_cov_mu if self.c_cov_mu > 0 else 0\n        self.c_cov = (1 / self.mu) * self.c_cov\n\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.func_evals = 0\n        self.archive_size = archive_size\n        self.archive_x = []\n        self.archive_f = []\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 50\n        self.success_rate = 0.0\n        self.success_history = []\n        self.success_window = 10 # Number of iterations to track success\n        self.popsize_adaptation_rate = 0.1 # Rate at which popsize is adjusted\n\n    def initialize(self):\n        self.m = np.random.uniform(-2, 2, size=self.dim)\n        self.sigma = self.sigma0\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.eigenspace = np.eye(self.dim)\n        self.eigenvalues = np.ones(self.dim)\n\n    def sample(self):\n        z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n        x = self.m[:, np.newaxis] + self.sigma * (self.eigenspace @ (np.diag(np.sqrt(self.eigenvalues)) @ z))\n        return x\n\n    def update(self, x, fitness_values):\n        idx = np.argsort(fitness_values)\n        x_mu = x[:, idx[:self.mu]]\n        z_mu = np.linalg.solve(self.eigenspace @ np.diag(np.sqrt(self.eigenvalues)), (x_mu - self.m[:, np.newaxis]) / self.sigma)\n\n        m_old = self.m.copy()\n        self.m = np.sum(x_mu * self.weights[np.newaxis, :], axis=1)\n        self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * np.sum(self.weights)) * (self.eigenspace @ z_mu.mean(axis=1))\n        self.pc = (1 - self.c_cov) * self.pc + np.sqrt(self.c_cov * (2 - self.c_cov) * np.sum(self.weights)) * ((self.m - m_old) / self.sigma)\n        \n        C_temp = self.c_cov_mu * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :]) + \\\n               (1 - self.c_cov_mu) * (self.C)\n\n        self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / np.sqrt(self.dim) - 1))\n\n        self.C = C_temp\n\n        self.eigenvalues, self.eigenspace = np.linalg.eigh(self.C)\n        self.eigenvalues = np.maximum(self.eigenvalues, 1e-12)\n\n    def adapt_popsize(self):\n        if len(self.success_history) < self.success_window:\n            return\n        \n        recent_success_rate = np.mean(self.success_history[-self.success_window:])\n        \n        if recent_success_rate > 0.2:\n            self.popsize = max(4, int(self.popsize * (1 - self.popsize_adaptation_rate))) # Reduce popsize if success rate is high, but ensure at least 4\n            self.mu = self.popsize // 2\n\n            # Recompute weights\n            self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n            self.weights = self.weights / np.sum(self.weights)\n        elif recent_success_rate < 0.05:\n            self.popsize = int(self.popsize * (1 + self.popsize_adaptation_rate))  # Increase popsize if success rate is low\n            self.mu = self.popsize // 2\n\n            # Recompute weights\n            self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n            self.weights = self.weights / np.sum(self.weights)\n\n    def restart(self):\n        if self.archive_x:\n            # Select a point from archive to guide the restart\n            idx = np.argmin(self.archive_f)\n            best_x_from_archive = self.archive_x[idx]\n            self.m = best_x_from_archive + np.random.normal(0, 0.1, size=self.dim)\n        else:\n            # If archive is empty, restart randomly\n            self.m = np.random.uniform(-2, 2, size=self.dim)\n        \n        self.sigma = self.sigma0\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.eigenspace = np.eye(self.dim)\n        self.eigenvalues = np.ones(self.dim)\n\n    def __call__(self, func):\n        self.initialize()\n        improved = False # Flag to track if any improvement was made in an iteration.\n\n        while self.func_evals < self.budget:\n            x = self.sample()\n\n            fitness_values = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.func_evals += self.popsize\n\n            best_index = np.argmin(fitness_values)\n            if fitness_values[best_index] < self.f_opt:\n                self.f_opt = fitness_values[best_index]\n                self.x_opt = x[:, best_index]\n                improved = True\n\n                # Update archive\n                if len(self.archive_x) < self.archive_size:\n                    self.archive_x.append(self.x_opt)\n                    self.archive_f.append(self.f_opt)\n                else:\n                    worst_arch_idx = np.argmax(self.archive_f)\n                    if self.f_opt < self.archive_f[worst_arch_idx]:\n                        self.archive_x[worst_arch_idx] = self.x_opt\n                        self.archive_f[worst_arch_idx] = self.f_opt\n                self.stagnation_counter = 0  # Reset counter if improvement\n            else:\n                improved = False\n            \n            self.update(x, fitness_values)\n            self.stagnation_counter += 1\n\n            # Popsize adaptation and success rate tracking\n            self.success_history.append(1 if improved else 0)\n            if len(self.success_history) > 2 * self.success_window:\n                 self.success_history.pop(0) # Keep the history at a manageable size\n            self.adapt_popsize()\n                \n\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.restart()\n                self.stagnation_counter = 0  # Reset counter\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveCMAES scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9a3f0641-0889-4841-b36c-2787d62e6cff"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "4516c704-5b12-4e5c-98b6-318610e67c44", "fitness": 0.23915519007731412, "name": "RingTopologyAdaptiveOptimizer", "description": "Population-based algorithm that uses a ring topology for information sharing and adapts its search behavior based on the diversity of the population, incorporating elements of differential evolution and a local search operator.", "code": "import numpy as np\n\nclass RingTopologyAdaptiveOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, de_mutation_factor=0.5, local_search_prob=0.1, diversity_threshold=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.de_mutation_factor = de_mutation_factor\n        self.local_search_prob = local_search_prob\n        self.diversity_threshold = diversity_threshold\n        self.population = np.random.uniform(-5, 5, size=(pop_size, dim))\n        self.fitness = np.zeros(pop_size)\n        self.best_positions = self.population.copy()\n        self.best_fitness = np.full(pop_size, np.inf)\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.eval_count = 0\n\n    def __call__(self, func):\n        self.eval_count = 0\n        while self.eval_count < self.budget:\n            # Evaluate fitness\n            for i in range(self.pop_size):\n                if self.eval_count < self.budget:\n                    f = func(self.population[i])\n                    self.eval_count += 1\n                    self.fitness[i] = f\n                    if f < self.best_fitness[i]:\n                        self.best_fitness[i] = f\n                        self.best_positions[i] = self.population[i].copy()\n                        if f < self.global_best_fitness:\n                            self.global_best_fitness = f\n                            self.global_best_position = self.population[i].copy()\n\n            # Calculate population diversity\n            diversity = np.std(self.population)\n\n            # Update population using ring topology and adaptive strategy\n            for i in range(self.pop_size):\n                neighbor_left = (i - 1) % self.pop_size\n                neighbor_right = (i + 1) % self.pop_size\n\n                # Differential Evolution with neighbors\n                r1, r2 = np.random.choice(self.pop_size, 2, replace=False)\n                \n                de_vector = self.population[i] + self.de_mutation_factor * (self.population[neighbor_left] - self.population[neighbor_right])\n                de_vector = np.clip(de_vector, func.bounds.lb, func.bounds.ub)\n\n                # Local Search (only if population is diverse enough)\n                if np.random.rand() < self.local_search_prob and diversity > self.diversity_threshold:\n                    # Apply a small perturbation to the current individual\n                    perturbation = np.random.uniform(-0.1, 0.1, size=self.dim)\n                    local_search_vector = self.population[i] + perturbation\n                    local_search_vector = np.clip(local_search_vector, func.bounds.lb, func.bounds.ub)\n                    \n                    # Choose between DE and Local Search based on a random probability\n                    if np.random.rand() < 0.5:\n                        new_position = de_vector\n                    else:\n                        new_position = local_search_vector\n                else:\n                     new_position = de_vector\n                \n                # Update individual only if it improves fitness\n                f_new = func(new_position) if self.eval_count < self.budget else np.inf\n                if self.eval_count < self.budget:\n                    self.eval_count += 1\n                    if f_new < self.fitness[i]:\n                        self.population[i] = new_position\n                        self.fitness[i] = f_new\n                        if f_new < self.best_fitness[i]:\n                            self.best_fitness[i] = f_new\n                            self.best_positions[i] = self.population[i].copy()\n                            if f_new < self.global_best_fitness:\n                                self.global_best_fitness = f_new\n                                self.global_best_position = self.population[i].copy()\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 4, "feedback": "The algorithm RingTopologyAdaptiveOptimizer scored 0.239 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["97577154-a88e-47d1-b5c3-d3b0c2236b4c"], "operator": null, "metadata": {"aucs": [0.09131307108769249, 0.14454885710737508, 0.23744930007820364, 0.16519994047372366, 0.20333355358248972, 0.22634586572008142, 0.20582162780132007, 0.17520978359027717, 0.17824635695220836, 0.12700525373667204, 0.1988487808130882, 0.9922735825224772, 0.2410964745527967, 0.17207632558281516, 0.18655602996269482, 0.20291135771404678, 0.2210921734140351, 0.21180509783341828, 0.155649180611502, 0.4463211884093643]}}
{"id": "e298162b-2edc-4d8a-a100-c1ba3100aa90", "fitness": 0.4395267557694761, "name": "AdaptiveDEwithAgingLocalSearch", "description": "Adaptive Differential Evolution with Aging and Local Search, where individuals are penalized for surviving too long and are given a chance for local refinement.", "code": "import numpy as np\n\nclass AdaptiveDEwithAgingLocalSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, age_limit=50, local_search_prob=0.1, local_search_radius=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.age_limit = age_limit\n        self.local_search_prob = local_search_prob\n        self.local_search_radius = local_search_radius\n        self.population = None\n        self.fitness = None\n        self.ages = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.ages = np.zeros(self.pop_size, dtype=int)\n        self.budget -= self.pop_size\n        \n    def mutate(self, x_i):\n        indices = np.random.choice(self.pop_size, size=3, replace=False)\n        x_r1 = self.population[indices[0]]\n        x_r2 = self.population[indices[1]]\n        x_r3 = self.population[indices[2]]\n        return x_r1 + self.F * (x_r2 - x_r3)\n\n    def crossover(self, x_i, v_i):\n        u_i = np.copy(x_i)\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() <= self.CR or j == j_rand:\n                u_i[j] = v_i[j]\n        return u_i\n\n    def repair(self, x, func):\n        return np.clip(x, func.bounds.lb, func.bounds.ub)\n\n    def local_search(self, x, func):\n        \"\"\"Perform a local search around the given solution.\"\"\"\n        new_x = x + np.random.uniform(-self.local_search_radius, self.local_search_radius, size=self.dim)\n        new_x = self.repair(new_x, func)\n        new_f = func(new_x)\n        self.budget -= 1\n        return new_x, new_f\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                v_i = self.mutate(self.population[i])\n                \n                # Crossover\n                u_i = self.crossover(self.population[i], v_i)\n                \n                # Repair\n                u_i = self.repair(u_i, func)\n\n                # Selection\n                f_u_i = func(u_i)\n                self.budget -= 1\n\n                # Aging penalty\n                f_i = self.fitness[i] + (self.ages[i] / self.age_limit)  # Penalize older individuals\n                \n                if f_u_i < f_i:\n                    self.population[i] = u_i\n                    self.fitness[i] = f_u_i\n                    self.ages[i] = 0  # Reset age\n                    if f_u_i < self.f_opt:\n                        self.f_opt = f_u_i\n                        self.x_opt = u_i\n                else:\n                    self.ages[i] += 1\n\n                # Local Search\n                if np.random.rand() < self.local_search_prob:\n                    new_x, new_f = self.local_search(self.population[i], func)\n                    if new_f < self.fitness[i]:\n                        self.population[i] = new_x\n                        self.fitness[i] = new_f\n                        self.ages[i] = 0\n                        if new_f < self.f_opt:\n                            self.f_opt = new_f\n                            self.x_opt = new_x\n                \n                if self.budget <= 0:\n                    break\n            self.ages += 1 # Increment ages for all individuals\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDEwithAgingLocalSearch scored 0.440 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["1b6bb400-a573-4a1d-b77b-ded94c79bcdd"], "operator": null, "metadata": {"aucs": [0.3034847990780195, 0.39863546444874776, 0.39806879602803613, 0.5344857831716807, 0.416827558640393, 0.455587430037103, 0.3398443420909866, 0.3760678304882977, 0.4174840316539076, 0.4100842856287784, 0.4924230869225793, 0.9995186918075863, 0.4144518135772082, 0.4518030092626405, 0.7051910742375318, 0.41150105194134956, 0.36411898622460204, 0.4614303243805976, 0]}}
{"id": "abe5e71e-bdc8-40d2-b0f5-a86d3ec37179", "fitness": -Infinity, "name": "OrthogonalCMAES", "description": "Covariance Matrix Adaptation Evolution Strategy with orthogonal sampling and a dynamic local search probability.", "code": "import numpy as np\n\nclass OrthogonalCMAES:\n    def __init__(self, budget=10000, dim=10, initial_popsize=None, cs=0.3, damps=None, c_cov_base=None, sigma0=0.2, archive_size=10, local_search_probability=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.initial_popsize = initial_popsize if initial_popsize is not None else 4 + int(3 * np.log(dim))\n        self.popsize = self.initial_popsize\n        self.mu = self.popsize // 2\n\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n\n        self.m = None\n        self.sigma = None\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.eigenspace = None\n        self.eigenvalues = None\n\n        self.cs = cs\n        self.damps = damps if damps is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1)/(self.dim + 1)) - 1]) + self.cs\n        self.c_cov_base = c_cov_base if c_cov_base is not None else (1 / (self.dim * np.sqrt(self.dim))) * 10\n        self.c_cov = self.c_cov_base\n        self.c_cov_mu = self.c_cov\n        self.c_cov_mu = self.c_cov_mu if self.c_cov_mu <= 1 else 1\n        self.c_cov_mu = self.c_cov_mu if self.c_cov_mu > 0 else 0\n        self.c_cov = (1 / self.mu) * self.c_cov\n\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.func_evals = 0\n        self.archive_size = archive_size\n        self.archive_x = []\n        self.archive_f = []\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 50\n        self.success_rate = 0.0\n        self.success_history = []\n        self.success_window = 10\n        self.popsize_adaptation_rate = 0.1\n        self.local_search_probability = local_search_probability\n\n    def initialize(self):\n        self.m = np.random.uniform(-2, 2, size=self.dim)\n        self.sigma = self.sigma0\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.eigenspace = np.eye(self.dim)\n        self.eigenvalues = np.ones(self.dim)\n\n    def sample(self):\n        z = self.orthogonal_sample(self.popsize)\n        x = self.m[:, np.newaxis] + self.sigma * (self.eigenspace @ (np.diag(np.sqrt(self.eigenvalues)) @ z))\n        return x\n\n    def orthogonal_sample(self, size):\n        H = np.random.normal(0, 1, size=(self.dim, size))\n        Q, R = np.linalg.qr(H)\n        return Q\n\n    def local_search(self, func, x):\n        # Simple local search around x\n        delta = np.random.uniform(-0.1, 0.1, size=self.dim)\n        x_new = np.clip(x + delta, func.bounds.lb, func.bounds.ub)\n        f_new = func(x_new)\n        self.func_evals += 1\n\n        if f_new < func(x):\n            return x_new, f_new\n        else:\n            return x, func(x)\n\n    def update(self, x, fitness_values):\n        idx = np.argsort(fitness_values)\n        x_mu = x[:, idx[:self.mu]]\n        z_mu = np.linalg.solve(self.eigenspace @ np.diag(np.sqrt(self.eigenvalues)), (x_mu - self.m[:, np.newaxis]) / self.sigma)\n\n        m_old = self.m.copy()\n        self.m = np.sum(x_mu * self.weights[np.newaxis, :], axis=1)\n        self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * np.sum(self.weights)) * (self.eigenspace @ z_mu.mean(axis=1))\n        self.pc = (1 - self.c_cov) * self.pc + np.sqrt(self.c_cov * (2 - self.c_cov) * np.sum(self.weights)) * ((self.m - m_old) / self.sigma)\n        \n        C_temp = self.c_cov_mu * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :]) + \\\n               (1 - self.c_cov_mu) * (self.C)\n\n        self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / np.sqrt(self.dim) - 1))\n\n        self.C = C_temp\n\n        self.eigenvalues, self.eigenspace = np.linalg.eigh(self.C)\n        self.eigenvalues = np.maximum(self.eigenvalues, 1e-12)\n    \n    def adapt_local_search_probability(self):\n        if len(self.success_history) < self.success_window:\n            return\n\n        recent_success_rate = np.mean(self.success_history[-self.success_window:])\n        \n        if recent_success_rate > 0.2:\n            self.local_search_probability = min(0.5, self.local_search_probability + 0.05)  # Increase if successful\n        else:\n            self.local_search_probability = max(0.01, self.local_search_probability - 0.05)  # Decrease if not successful\n\n    def __call__(self, func):\n        self.initialize()\n\n        while self.func_evals < self.budget:\n            x = self.sample()\n            fitness_values = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.func_evals += self.popsize\n\n            best_index = np.argmin(fitness_values)\n            best_x = x[:, best_index]\n            best_f = fitness_values[best_index]\n\n            # Apply local search with a probability\n            if np.random.rand() < self.local_search_probability:\n                best_x, best_f = self.local_search(func, best_x)\n            \n            if best_f < self.f_opt:\n                self.f_opt = best_f\n                self.x_opt = best_x\n                \n                if len(self.archive_x) < self.archive_size:\n                    self.archive_x.append(self.x_opt)\n                    self.archive_f.append(self.f_opt)\n                else:\n                    worst_arch_idx = np.argmax(self.archive_f)\n                    if self.f_opt < self.archive_f[worst_arch_idx]:\n                        self.archive_x[worst_arch_idx] = self.x_opt\n                        self.archive_f[worst_arch_idx] = self.f_opt\n\n                self.stagnation_counter = 0\n                self.success_history.append(1)\n            else:\n                 self.success_history.append(0)\n            \n            if len(self.success_history) > 2 * self.success_window:\n                 self.success_history.pop(0)\n                \n            self.update(x, fitness_values)\n            self.stagnation_counter += 1\n            self.adapt_local_search_probability()\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                # Restart strategy: Re-initialize around best archive solution\n                if self.archive_x:\n                    idx = np.argmin(self.archive_f)\n                    self.m = self.archive_x[idx] + np.random.normal(0, 0.1, size=self.dim)\n                else:\n                    self.m = np.random.uniform(-2, 2, size=self.dim)\n                    \n                self.sigma = self.sigma0\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.eigenspace = np.eye(self.dim)\n                self.eigenvalues = np.ones(self.dim)\n                self.stagnation_counter = 0\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 113, in __call__\n  File \"<string>\", line 113, in <listcomp>\nIndexError: index 2 is out of bounds for axis 1 with size 2\n.", "error": "", "parent_ids": ["3b169888-62a2-4ab4-8ad8-df1ff67213a5"], "operator": null, "metadata": {}}
{"id": "b95fa06b-c58a-4230-9fd3-db88c7da0ee2", "fitness": -Infinity, "name": "OrthogonalCMAES", "description": "A CMA-ES variant employing orthogonal learning to enhance exploration and exploitation by rotating the search space based on historical gradient information.", "code": "import numpy as np\n\nclass OrthogonalCMAES:\n    def __init__(self, budget=10000, dim=10, initial_popsize=None, cs=0.3, damps=None, c_cov_base=None, sigma0=0.2, archive_size=10, orthogonal_learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.initial_popsize = initial_popsize if initial_popsize is not None else 4 + int(3 * np.log(dim)) # Default popsize\n        self.popsize = self.initial_popsize\n        self.mu = self.popsize // 2\n\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n\n        self.m = None\n        self.sigma = None\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.eigenspace = None\n        self.eigenvalues = None\n\n        self.cs = cs\n        self.damps = damps if damps is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1)/(self.dim + 1)) - 1]) + self.cs\n        self.c_cov_base = c_cov_base if c_cov_base is not None else (1 / (self.dim * np.sqrt(self.dim))) * 10\n        self.c_cov = self.c_cov_base\n        self.c_cov_mu = self.c_cov\n        self.c_cov_mu = self.c_cov_mu if self.c_cov_mu <= 1 else 1\n        self.c_cov_mu = self.c_cov_mu if self.c_cov_mu > 0 else 0\n        self.c_cov = (1 / self.mu) * self.c_cov\n        \n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.rotation_matrix = np.eye(self.dim) # Initial rotation matrix for orthogonal learning\n        self.historical_gradients = []\n        self.gradient_buffer_size = 10 # Number of historical gradients to store\n        \n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.func_evals = 0\n        self.archive_size = archive_size\n        self.archive_x = []\n        self.archive_f = []\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 50\n        self.success_rate = 0.0\n        self.success_history = []\n        self.success_window = 10 # Number of iterations to track success\n        self.popsize_adaptation_rate = 0.1 # Rate at which popsize is adjusted\n\n    def initialize(self):\n        self.m = np.random.uniform(-2, 2, size=self.dim)\n        self.sigma = self.sigma0\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.eigenspace = np.eye(self.dim)\n        self.eigenvalues = np.ones(self.dim)\n\n    def sample(self):\n        z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n        # Apply orthogonal learning rotation to the search distribution\n        rotated_z = self.rotation_matrix @ z\n        x = self.m[:, np.newaxis] + self.sigma * (self.eigenspace @ (np.diag(np.sqrt(self.eigenvalues)) @ rotated_z))\n        return x\n\n    def update(self, x, fitness_values, func):\n        idx = np.argsort(fitness_values)\n        x_mu = x[:, idx[:self.mu]]\n        z_mu = np.linalg.solve(self.eigenspace @ np.diag(np.sqrt(self.eigenvalues)), (x_mu - self.m[:, np.newaxis]) / self.sigma)\n\n        m_old = self.m.copy()\n        self.m = np.sum(x_mu * self.weights[np.newaxis, :], axis=1)\n        self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * np.sum(self.weights)) * (self.eigenspace @ z_mu.mean(axis=1))\n        self.pc = (1 - self.c_cov) * self.pc + np.sqrt(self.c_cov * (2 - self.c_cov) * np.sum(self.weights)) * ((self.m - m_old) / self.sigma)\n        \n        C_temp = self.c_cov_mu * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :]) + \\\n               (1 - self.c_cov_mu) * (self.C)\n\n        self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / np.sqrt(self.dim) - 1))\n\n        self.C = C_temp\n\n        self.eigenvalues, self.eigenspace = np.linalg.eigh(self.C)\n        self.eigenvalues = np.maximum(self.eigenvalues, 1e-12)\n        \n        # Orthogonal learning: Estimate gradient and update rotation matrix\n        best_x = x[:, idx[0]]\n        gradient = self.estimate_gradient(func, best_x)\n        self.update_rotation_matrix(gradient)\n\n    def estimate_gradient(self, func, x, delta=1e-3):\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            x_plus = x.copy()\n            x_minus = x.copy()\n            x_plus[i] += delta\n            x_minus[i] -= delta\n            gradient[i] = (func(x_plus) - func(x_minus)) / (2 * delta)\n        return gradient\n    \n    def update_rotation_matrix(self, gradient):\n        if len(self.historical_gradients) >= self.gradient_buffer_size:\n            self.historical_gradients.pop(0)\n        self.historical_gradients.append(gradient)\n        \n        if len(self.historical_gradients) > 1:\n            # Average the historical gradients\n            avg_gradient = np.mean(np.array(self.historical_gradients), axis=0)\n            \n            # Normalize the average gradient\n            avg_gradient /= (np.linalg.norm(avg_gradient) + 1e-8)\n            \n            # Create a small rotation matrix to align with the gradient\n            v = np.random.randn(self.dim)\n            v -= v.dot(avg_gradient) * avg_gradient\n            v /= (np.linalg.norm(v) + 1e-8)\n\n            # Rodrigues' rotation formula\n            axis = np.cross(avg_gradient, v)\n            axis /= (np.linalg.norm(axis) + 1e-8)\n            angle = self.orthogonal_learning_rate\n            \n            a = np.cos(angle / 2.0)\n            b, c, d = -axis * np.sin(angle / 2.0)\n            rotation_change = np.array([[a*a + b*b - c*c - d*d, 2*(b*c - a*d), 2*(b*d + a*c)],\n                                        [2*(b*c + a*d), a*a - b*b + c*c - d*d, 2*(c*d - a*b)],\n                                        [2*(b*d - a*c), 2*(c*d + a*b), a*a - b*b - c*c + d*d]])\n            if self.dim > 3:\n                rotation_change = np.eye(self.dim)\n                u = avg_gradient\n                v = np.random.randn(self.dim)\n                v -= v.dot(u) * u\n                v /= (np.linalg.norm(v) + 1e-8)\n                rotation_change[:3,:3] = np.array([[u[0]*v[0], u[0]*v[1], u[0]*v[2]],[u[1]*v[0], u[1]*v[1], u[1]*v[2]],[u[2]*v[0], u[2]*v[1], u[2]*v[2]]])\n                \n            self.rotation_matrix = rotation_change @ self.rotation_matrix\n\n    def adapt_popsize(self):\n        if len(self.success_history) < self.success_window:\n            return\n        \n        recent_success_rate = np.mean(self.success_history[-self.success_window:])\n        \n        if recent_success_rate > 0.2:\n            self.popsize = max(4, int(self.popsize * (1 - self.popsize_adaptation_rate))) # Reduce popsize if success rate is high, but ensure at least 4\n            self.mu = self.popsize // 2\n\n            # Recompute weights\n            self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n            self.weights = self.weights / np.sum(self.weights)\n        elif recent_success_rate < 0.05:\n            self.popsize = int(self.popsize * (1 + self.popsize_adaptation_rate))  # Increase popsize if success rate is low\n            self.mu = self.popsize // 2\n\n            # Recompute weights\n            self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n            self.weights = self.weights / np.sum(self.weights)\n\n    def restart(self):\n        if self.archive_x:\n            # Select a point from archive to guide the restart\n            idx = np.argmin(self.archive_f)\n            best_x_from_archive = self.archive_x[idx]\n            self.m = best_x_from_archive + np.random.normal(0, 0.1, size=self.dim)\n        else:\n            # If archive is empty, restart randomly\n            self.m = np.random.uniform(-2, 2, size=self.dim)\n        \n        self.sigma = self.sigma0\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.eigenspace = np.eye(self.dim)\n        self.eigenvalues = np.ones(self.dim)\n        self.rotation_matrix = np.eye(self.dim) # Reset rotation matrix on restart\n        self.historical_gradients = [] # Clear historical gradients\n\n    def __call__(self, func):\n        self.initialize()\n        improved = False # Flag to track if any improvement was made in an iteration.\n\n        while self.func_evals < self.budget:\n            x = self.sample()\n\n            fitness_values = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.func_evals += self.popsize\n\n            best_index = np.argmin(fitness_values)\n            if fitness_values[best_index] < self.f_opt:\n                self.f_opt = fitness_values[best_index]\n                self.x_opt = x[:, best_index]\n                improved = True\n\n                # Update archive\n                if len(self.archive_x) < self.archive_size:\n                    self.archive_x.append(self.x_opt)\n                    self.archive_f.append(self.f_opt)\n                else:\n                    worst_arch_idx = np.argmax(self.archive_f)\n                    if self.f_opt < self.archive_f[worst_arch_idx]:\n                        self.archive_x[worst_arch_idx] = self.x_opt\n                        self.archive_f[worst_arch_idx] = self.f_opt\n                self.stagnation_counter = 0  # Reset counter if improvement\n            else:\n                improved = False\n            \n            self.update(x, fitness_values, func)\n            self.stagnation_counter += 1\n\n            # Popsize adaptation and success rate tracking\n            self.success_history.append(1 if improved else 0)\n            if len(self.success_history) > 2 * self.success_window:\n                 self.success_history.pop(0) # Keep the history at a manageable size\n            self.adapt_popsize()\n                \n\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.restart()\n                self.stagnation_counter = 0  # Reset counter\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 207, in __call__\n  File \"<string>\", line 89, in update\n  File \"<string>\", line 124, in update_rotation_matrix\nTypeError: cannot unpack non-iterable numpy.float64 object\n.", "error": "", "parent_ids": ["3b169888-62a2-4ab4-8ad8-df1ff67213a5"], "operator": null, "metadata": {}}
{"id": "49a9688f-d00f-44a6-a37d-e4b249def329", "fitness": 0.3936769939812042, "name": "AdaptiveDE_NM", "description": "An adaptive algorithm that dynamically adjusts the exploration-exploitation balance by switching between global search with Differential Evolution and local search with Nelder-Mead, based on the population diversity and stagnation detection.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDE_NM:\n    def __init__(self, budget=10000, dim=10, pop_size=20, de_mutation_factor=0.5, nm_max_iter=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.de_mutation_factor = de_mutation_factor\n        self.nm_max_iter = nm_max_iter\n        self.population = np.random.uniform(-5, 5, size=(self.pop_size, dim))\n        self.fitness = np.zeros(self.pop_size)\n        self.best_fitness = np.inf\n        self.best_position = None\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 50 # Number of iterations without improvement before switching to local search\n        self.diversity_threshold = 0.1 # Threshold for population diversity to switch search strategy\n\n    def __call__(self, func):\n        eval_count = 0\n        \n        # Evaluate initial population\n        for i in range(self.pop_size):\n            if eval_count < self.budget:\n                self.fitness[i] = func(self.population[i])\n                eval_count += 1\n                if self.fitness[i] < self.best_fitness:\n                    self.best_fitness = self.fitness[i]\n                    self.best_position = self.population[i].copy()\n\n        while eval_count < self.budget:\n            # Calculate population diversity\n            diversity = np.std(self.population)\n\n            if diversity > self.diversity_threshold:\n                # Global Search with Differential Evolution\n                new_population = np.zeros_like(self.population)\n                for i in range(self.pop_size):\n                    if eval_count >= self.budget:\n                        break\n                    r1, r2, r3 = np.random.choice(self.pop_size, 3, replace=False)\n                    mutant_vector = self.population[r1] + self.de_mutation_factor * (self.population[r2] - self.population[r3])\n                    \n                    # Crossover (Binomial/Uniform)\n                    crossover_mask = np.random.rand(self.dim) < 0.9\n                    new_population[i] = np.where(crossover_mask, mutant_vector, self.population[i])\n                    new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                    \n                    new_fitness = func(new_population[i])\n                    eval_count += 1\n                    \n                    if new_fitness < self.fitness[i]:\n                        self.population[i] = new_population[i]\n                        self.fitness[i] = new_fitness\n                        if new_fitness < self.best_fitness:\n                            self.best_fitness = new_fitness\n                            self.best_position = self.population[i].copy()\n                            self.stagnation_counter = 0 # Reset stagnation counter\n                    else:\n                        self.stagnation_counter += 1\n            else:\n                # Local Search with Nelder-Mead on the best individual\n                \n                res = minimize(func, self.best_position, method='Nelder-Mead', options={'maxiter': self.nm_max_iter, 'maxfev': self.budget - eval_count})\n                \n                if res.success:\n                    if res.fun < self.best_fitness:\n                        self.best_fitness = res.fun\n                        self.best_position = res.x.copy()\n                        self.stagnation_counter = 0\n                        \n                    eval_count += res.nfev # res.nfev is the number of function evaluations performed.\n\n                else:\n                    self.stagnation_counter += 1 # If Nelder-Mead fails, increase stagnation counter.\n\n                #Perturb population after NM\n                for i in range(self.pop_size):\n                    self.population[i] = np.random.uniform(-5,5,self.dim)\n\n            #Stagnation Check\n            if self.stagnation_counter > self.stagnation_threshold:\n                # Reset population to encourage exploration if stagnating\n                self.population = np.random.uniform(-5, 5, size=(self.pop_size, self.dim))\n                for i in range(self.pop_size):\n                    if eval_count < self.budget:\n                        self.fitness[i] = func(self.population[i])\n                        eval_count += 1\n                        if self.fitness[i] < self.best_fitness:\n                            self.best_fitness = self.fitness[i]\n                            self.best_position = self.population[i].copy()\n                self.stagnation_counter = 0 # Reset the counter after re-initialization\n\n\n        return self.best_fitness, self.best_position", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDE_NM scored 0.394 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b1b3c4d6-e715-4562-8aa5-e77a7b568fac"], "operator": null, "metadata": {"aucs": [0.11856117654172682, 0.20033531985195663, 0.32034364746558164, 0.9448521703950673, 0.24664711956599772, 0.7034508219543434, 0.25836197048511644, 0.301102107082892, 0.28220295409452034, 0.18076550901819777, 0.2902200240269118, 0.998635793897874, 0.27284774833292, 0.27486023629822665, 0.7070972094445699, 0.3110491098490078, 0.23807879365251072, 0.581697531667716, 0.17686135258907376, 0.4655692834098729]}}
{"id": "a12a760b-ff35-44dd-ab33-56d6b8b3a99e", "fitness": 0.3303565353425994, "name": "AdaptiveDERestart", "description": "An adaptive Differential Evolution with dynamic parameter control, population diversity maintenance, and a restart strategy to escape local optima.", "code": "import numpy as np\n\nclass AdaptiveDERestart:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, diversity_threshold=0.1, restart_frequency=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.diversity_threshold = diversity_threshold\n        self.restart_frequency = restart_frequency\n        self.population = None\n        self.fitness = None\n        self.best_fitness_history = []\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.best_fitness_history.append(self.f_opt)\n\n    def calculate_diversity(self):\n        \"\"\"Calculates the diversity of the population based on the average distance to the centroid.\"\"\"\n        centroid = np.mean(self.population, axis=0)\n        distances = np.linalg.norm(self.population - centroid, axis=1)\n        avg_distance = np.mean(distances)\n        return avg_distance\n\n    def adapt_parameters(self):\n        \"\"\"Adapts the F and CR parameters based on the recent success.\"\"\"\n        if len(self.best_fitness_history) > 5:\n            recent_improvements = [self.best_fitness_history[i] - self.best_fitness_history[i-1] for i in range(len(self.best_fitness_history)-5, len(self.best_fitness_history))]\n            if np.mean(recent_improvements) > -1e-6: # If little to no improvement\n                self.F = min(self.F * 1.1, 1.0)  # Increase exploration\n                self.CR = max(self.CR * 0.9, 0.1)  # Reduce exploitation\n            else:\n                self.F = max(self.F * 0.9, 0.1) # Decrease exploration\n                self.CR = min(self.CR * 1.1, 0.95) # Increase exploitation\n\n\n    def mutate(self, x_i):\n        indices = np.random.choice(self.pop_size, size=3, replace=False)\n        x_r1 = self.population[indices[0]]\n        x_r2 = self.population[indices[1]]\n        x_r3 = self.population[indices[2]]\n        return x_r1 + self.F * (x_r2 - x_r3)\n\n    def crossover(self, x_i, v_i):\n        u_i = np.copy(x_i)\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() <= self.CR or j == j_rand:\n                u_i[j] = v_i[j]\n        return u_i\n\n    def repair(self, x, func):\n        return np.clip(x, func.bounds.lb, func.bounds.ub)\n\n    def restart_population(self, func):\n         \"\"\"Restarts the population with a new random population.\"\"\"\n         self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n         self.fitness = np.array([func(x) for x in self.population])\n         self.budget -= self.pop_size\n         self.f_opt = np.min(self.fitness)\n         self.x_opt = self.population[np.argmin(self.fitness)]\n         self.best_fitness_history.append(self.f_opt)\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                v_i = self.mutate(self.population[i])\n\n                # Crossover\n                u_i = self.crossover(self.population[i], v_i)\n\n                # Repair\n                u_i = self.repair(u_i, func)\n\n                # Selection\n                f_u_i = func(u_i)\n                self.budget -= 1\n\n                if f_u_i < self.fitness[i]:\n                    self.population[i] = u_i\n                    self.fitness[i] = f_u_i\n\n                    if f_u_i < self.f_opt:\n                        self.f_opt = f_u_i\n                        self.x_opt = u_i\n            \n            self.best_fitness_history.append(self.f_opt)\n            # Adapt parameters\n            self.adapt_parameters()\n\n            # Check diversity and restart\n            diversity = self.calculate_diversity()\n            if diversity < self.diversity_threshold or np.random.rand() < self.restart_frequency:\n                if self.budget > self.pop_size:\n                    self.restart_population(func)\n                else:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDERestart scored 0.330 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e298162b-2edc-4d8a-a100-c1ba3100aa90"], "operator": null, "metadata": {"aucs": [0.15756040377718128, 0.21884749146463, 0.31578403771651786, 0.29558695032549953, 0.2520978727915819, 0.2985262086546945, 0.27358252607092226, 0.2972545413116182, 0.25595776959551564, 0.18036762348586166, 0.28825920275249717, 0.9906776088525652, 0.2647367662489599, 0.2757035248368461, 0.6271026327768341, 0.31505012648922537, 0.28617735696573965, 0.3386887101049818, 0.20097849755568098, 0.474190855074635]}}
{"id": "245072e2-a848-4217-b5f1-19eaf199f635", "fitness": 0.5371058180610391, "name": "AdaptiveSubgroupRestartOptimizer", "description": "An adaptive population-based algorithm with dynamic sub-grouping and a restart mechanism triggered by stagnation, combining aspects of PSO, DE, and local search.", "code": "import numpy as np\n\nclass AdaptiveSubgroupRestartOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=30, num_subgroups=3, stagnation_tolerance=1000, pso_inertia=0.7, pso_cognitive=1.4, pso_social=1.4, de_mutation=0.5, local_search_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.num_subgroups = num_subgroups\n        self.stagnation_tolerance = stagnation_tolerance\n        self.pso_inertia = pso_inertia\n        self.pso_cognitive = pso_cognitive\n        self.pso_social = pso_social\n        self.de_mutation = de_mutation\n        self.local_search_prob = local_search_prob\n        self.population = np.random.uniform(-5, 5, size=(pop_size, dim))\n        self.fitness = np.zeros(pop_size)\n        self.velocities = np.zeros_like(self.population)\n        self.best_positions = self.population.copy()\n        self.best_fitness = np.full(pop_size, np.inf)\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.eval_count = 0\n        self.stagnation_counter = 0\n        self.last_best_fitness = np.inf\n\n    def __call__(self, func):\n        self.eval_count = 0\n        self.stagnation_counter = 0\n        self.last_best_fitness = np.inf\n        \n        while self.eval_count < self.budget:\n            # Evaluate fitness\n            for i in range(self.pop_size):\n                if self.eval_count < self.budget:\n                    f = func(self.population[i])\n                    self.eval_count += 1\n                    self.fitness[i] = f\n                    if f < self.best_fitness[i]:\n                        self.best_fitness[i] = f\n                        self.best_positions[i] = self.population[i].copy()\n                        if f < self.global_best_fitness:\n                            self.global_best_fitness = f\n                            self.global_best_position = self.population[i].copy()\n\n            # Check for stagnation\n            if self.global_best_fitness >= self.last_best_fitness:\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n\n            self.last_best_fitness = self.global_best_fitness\n\n            # Restart if stagnated\n            if self.stagnation_counter > self.stagnation_tolerance:\n                self.population = np.random.uniform(-5, 5, size=(self.pop_size, self.dim))\n                self.velocities = np.zeros_like(self.population)\n                self.fitness = np.zeros(self.pop_size)\n                self.best_positions = self.population.copy()\n                self.best_fitness = np.full(self.pop_size, np.inf)\n                self.stagnation_counter = 0\n                continue\n\n            # Subgrouping and Update\n            subgroup_size = self.pop_size // self.num_subgroups\n            for subgroup_id in range(self.num_subgroups):\n                start_index = subgroup_id * subgroup_size\n                end_index = (subgroup_id + 1) * subgroup_size if subgroup_id < self.num_subgroups - 1 else self.pop_size\n\n                # PSO update for this subgroup\n                for i in range(start_index, end_index):\n                    r1 = np.random.rand(self.dim)\n                    r2 = np.random.rand(self.dim)\n                    self.velocities[i] = (self.pso_inertia * self.velocities[i] +\n                                          self.pso_cognitive * r1 * (self.best_positions[i] - self.population[i]) +\n                                          self.pso_social * r2 * (self.global_best_position - self.population[i]))\n                    self.population[i] = np.clip(self.population[i] + self.velocities[i], func.bounds.lb, func.bounds.ub)\n                    \n                    # DE update with random individuals from the population\n                    r1, r2, r3 = np.random.choice(self.pop_size, 3, replace=False)\n                    de_vector = self.population[i] + self.de_mutation * (self.population[r1] - self.population[r2])\n                    de_vector = np.clip(de_vector, func.bounds.lb, func.bounds.ub)\n                    \n                    # Local search\n                    if np.random.rand() < self.local_search_prob:\n                        perturbation = np.random.uniform(-0.1, 0.1, size=self.dim)\n                        local_search_vector = self.population[i] + perturbation\n                        local_search_vector = np.clip(local_search_vector, func.bounds.lb, func.bounds.ub)\n                    \n                        f_de = func(de_vector) if self.eval_count + 1 < self.budget else np.inf\n                        f_ls = func(local_search_vector) if self.eval_count + 1 < self.budget else np.inf\n                    \n                        if self.eval_count + 2 < self.budget:\n                            self.eval_count += 2\n                            if f_de < f_ls and f_de < self.fitness[i]:\n                                self.population[i] = de_vector\n                                self.fitness[i] = f_de\n                                if f_de < self.best_fitness[i]:\n                                    self.best_fitness[i] = f_de\n                                    self.best_positions[i] = self.population[i].copy()\n                                    if f_de < self.global_best_fitness:\n                                        self.global_best_fitness = f_de\n                                        self.global_best_position = self.population[i].copy()\n                            elif f_ls < self.fitness[i]:\n                                self.population[i] = local_search_vector\n                                self.fitness[i] = f_ls\n                                if f_ls < self.best_fitness[i]:\n                                    self.best_fitness[i] = f_ls\n                                    self.best_positions[i] = self.population[i].copy()\n                                    if f_ls < self.global_best_fitness:\n                                        self.global_best_fitness = f_ls\n                                        self.global_best_position = self.population[i].copy()\n                        else:\n                            break # break if almost budget exhausted\n                    else:\n                        f_de = func(de_vector) if self.eval_count + 1 < self.budget else np.inf\n                        if self.eval_count + 1 < self.budget:\n                            self.eval_count += 1\n                            if f_de < self.fitness[i]:\n                                self.population[i] = de_vector\n                                self.fitness[i] = f_de\n                                if f_de < self.best_fitness[i]:\n                                    self.best_fitness[i] = f_de\n                                    self.best_positions[i] = self.population[i].copy()\n                                    if f_de < self.global_best_fitness:\n                                        self.global_best_fitness = f_de\n                                        self.global_best_position = self.population[i].copy()\n                        else:\n                            break\n                        \n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveSubgroupRestartOptimizer scored 0.537 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["4516c704-5b12-4e5c-98b6-318610e67c44"], "operator": null, "metadata": {"aucs": [0.18318778525975576, 0.22828264749013183, 0.5649261919783943, 0.8652479714650644, 0.6376271931825865, 0.7169230482398831, 0.31371218719864413, 0.4886948438945393, 0.6515380636956789, 0.19087750315984886, 0.7559389645088285, 0.9978202579441005, 0.24336646497245318, 0.5816574665944183, 0.6819667277603014, 0.7241650536293701, 0.3709985178136632, 0.7315633740436533, 0.31072652875626705, 0.5028955696332016]}}
{"id": "97d27608-825e-4ed4-9a3f-0bf0524188ee", "fitness": 0.19486010729176076, "name": "GradientGuidedCMAES", "description": "Gradient-guided CMA-ES, which uses finite differences to estimate the gradient and biases the mean update towards the negative gradient direction.", "code": "import numpy as np\n\nclass GradientGuidedCMAES:\n    def __init__(self, budget=10000, dim=10, initial_popsize=None, cs=0.3, damps=None, c_cov_base=None, sigma0=0.2, archive_size=10, gradient_estimation_step=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.initial_popsize = initial_popsize if initial_popsize is not None else 4 + int(3 * np.log(dim)) # Default popsize\n        self.popsize = self.initial_popsize\n        self.mu = self.popsize // 2\n\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n\n        self.m = None\n        self.sigma = None\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.eigenspace = None\n        self.eigenvalues = None\n\n        self.cs = cs\n        self.damps = damps if damps is not None else 1 + 2 * np.max([0, np.sqrt((self.mu - 1)/(self.dim + 1)) - 1]) + self.cs\n        self.c_cov_base = c_cov_base if c_cov_base is not None else (1 / (self.dim * np.sqrt(self.dim))) * 10\n        self.c_cov = self.c_cov_base\n        self.c_cov_mu = self.c_cov\n        self.c_cov_mu = self.c_cov_mu if self.c_cov_mu <= 1 else 1\n        self.c_cov_mu = self.c_cov_mu if self.c_cov_mu > 0 else 0\n        self.c_cov = (1 / self.mu) * self.c_cov\n\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.func_evals = 0\n        self.archive_size = archive_size\n        self.archive_x = []\n        self.archive_f = []\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 50\n        self.success_rate = 0.0\n        self.success_history = []\n        self.success_window = 10 # Number of iterations to track success\n        self.popsize_adaptation_rate = 0.1 # Rate at which popsize is adjusted\n        self.gradient_estimation_step = gradient_estimation_step\n\n    def initialize(self):\n        self.m = np.random.uniform(-2, 2, size=self.dim)\n        self.sigma = self.sigma0\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.eigenspace = np.eye(self.dim)\n        self.eigenvalues = np.ones(self.dim)\n\n    def sample(self):\n        z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n        x = self.m[:, np.newaxis] + self.sigma * (self.eigenspace @ (np.diag(np.sqrt(self.eigenvalues)) @ z))\n        return x\n\n    def estimate_gradient(self, func):\n        gradient = np.zeros(self.dim)\n        f_current = func(self.m)\n        self.func_evals += 1\n        for i in range(self.dim):\n            x_plus = self.m.copy()\n            x_plus[i] += self.gradient_estimation_step\n            f_plus = func(x_plus)\n            self.func_evals += 1\n            gradient[i] = (f_plus - f_current) / self.gradient_estimation_step\n        return gradient\n\n    def update(self, x, fitness_values, func):\n        idx = np.argsort(fitness_values)\n        x_mu = x[:, idx[:self.mu]]\n        z_mu = np.linalg.solve(self.eigenspace @ np.diag(np.sqrt(self.eigenvalues)), (x_mu - self.m[:, np.newaxis]) / self.sigma)\n\n        m_old = self.m.copy()\n        self.m = np.sum(x_mu * self.weights[np.newaxis, :], axis=1)\n\n        # Gradient estimation and update\n        gradient = self.estimate_gradient(func)\n        self.m = self.m - 0.01 * self.sigma * gradient # Move towards the negative gradient\n\n        self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * np.sum(self.weights)) * (self.eigenspace @ z_mu.mean(axis=1))\n        self.pc = (1 - self.c_cov) * self.pc + np.sqrt(self.c_cov * (2 - self.c_cov) * np.sum(self.weights)) * ((self.m - m_old) / self.sigma)\n        \n        C_temp = self.c_cov_mu * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :]) + \\\n               (1 - self.c_cov_mu) * (self.C)\n\n        self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / np.sqrt(self.dim) - 1))\n\n        self.C = C_temp\n\n        self.eigenvalues, self.eigenspace = np.linalg.eigh(self.C)\n        self.eigenvalues = np.maximum(self.eigenvalues, 1e-12)\n\n    def adapt_popsize(self):\n        if len(self.success_history) < self.success_window:\n            return\n        \n        recent_success_rate = np.mean(self.success_history[-self.success_window:])\n        \n        if recent_success_rate > 0.2:\n            self.popsize = max(4, int(self.popsize * (1 - self.popsize_adaptation_rate))) # Reduce popsize if success rate is high, but ensure at least 4\n            self.mu = self.popsize // 2\n\n            # Recompute weights\n            self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n            self.weights = self.weights / np.sum(self.weights)\n        elif recent_success_rate < 0.05:\n            self.popsize = int(self.popsize * (1 + self.popsize_adaptation_rate))  # Increase popsize if success rate is low\n            self.mu = self.popsize // 2\n\n            # Recompute weights\n            self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n            self.weights = self.weights / np.sum(self.weights)\n\n    def restart(self):\n        if self.archive_x:\n            # Select a point from archive to guide the restart\n            idx = np.argmin(self.archive_f)\n            best_x_from_archive = self.archive_x[idx]\n            self.m = best_x_from_archive + np.random.normal(0, 0.1, size=self.dim)\n        else:\n            # If archive is empty, restart randomly\n            self.m = np.random.uniform(-2, 2, size=self.dim)\n        \n        self.sigma = self.sigma0\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.eigenspace = np.eye(self.dim)\n        self.eigenvalues = np.ones(self.dim)\n\n    def __call__(self, func):\n        self.initialize()\n        improved = False # Flag to track if any improvement was made in an iteration.\n\n        while self.func_evals < self.budget - self.dim -1: # Ensure enough budget for gradient estimation\n            x = self.sample()\n\n            fitness_values = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.func_evals += self.popsize\n\n            best_index = np.argmin(fitness_values)\n            if fitness_values[best_index] < self.f_opt:\n                self.f_opt = fitness_values[best_index]\n                self.x_opt = x[:, best_index]\n                improved = True\n\n                # Update archive\n                if len(self.archive_x) < self.archive_size:\n                    self.archive_x.append(self.x_opt)\n                    self.archive_f.append(self.f_opt)\n                else:\n                    worst_arch_idx = np.argmax(self.archive_f)\n                    if self.f_opt < self.archive_f[worst_arch_idx]:\n                        self.archive_x[worst_arch_idx] = self.x_opt\n                        self.archive_f[worst_arch_idx] = self.f_opt\n                self.stagnation_counter = 0  # Reset counter if improvement\n            else:\n                improved = False\n            \n            self.update(x, fitness_values, func)\n            self.stagnation_counter += 1\n\n            # Popsize adaptation and success rate tracking\n            self.success_history.append(1 if improved else 0)\n            if len(self.success_history) > 2 * self.success_window:\n                 self.success_history.pop(0) # Keep the history at a manageable size\n            self.adapt_popsize()\n                \n\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.restart()\n                self.stagnation_counter = 0  # Reset counter\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm GradientGuidedCMAES scored 0.195 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3b169888-62a2-4ab4-8ad8-df1ff67213a5"], "operator": null, "metadata": {"aucs": [0.07257615574785559, 0.16302827388826868, 0.29030130097290086, 0.14097544710300824, 0.214507699017626, 0.15157836334752284, 0.1567794925831002, 0.13638617750776616, 0.1389521862830666, 0.18166000114925596, 0.18875547564779938, 0.26167174126630677, 0.2563112567382817, 0.18114496781970157, 0.41503845933856764, 0.22865164667095295, 0.18160601378390417, 0.19298090787192412, 0.16372261661884224, 0.18057396247856328]}}
{"id": "fefe3056-d5bf-4b76-b029-500b62bc8c84", "fitness": 0.6007232286026374, "name": "AdaptiveDEwithRestart", "description": "A population-based algorithm that combines aspects of Differential Evolution with a self-adjusting crossover rate and a restart mechanism based on stagnation detection.", "code": "import numpy as np\n\nclass AdaptiveDEwithRestart:\n    def __init__(self, budget=10000, dim=10, pop_size=20, mutation_factor=0.5, initial_crossover_rate=0.7, stagnation_threshold=1000):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.mutation_factor = mutation_factor\n        self.crossover_rate = initial_crossover_rate\n        self.initial_crossover_rate = initial_crossover_rate\n        self.stagnation_threshold = stagnation_threshold\n        self.population = np.random.uniform(-5, 5, size=(pop_size, dim))\n        self.fitness = np.zeros(pop_size)\n        self.best_positions = self.population.copy()\n        self.best_fitness = np.full(pop_size, np.inf)\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.eval_count = 0\n        self.stagnation_counter = 0\n\n    def __call__(self, func):\n        self.eval_count = 0\n        self.stagnation_counter = 0\n        self.crossover_rate = self.initial_crossover_rate\n        \n        # Initialize fitness values\n        for i in range(self.pop_size):\n            if self.eval_count < self.budget:\n                f = func(self.population[i])\n                self.eval_count += 1\n                self.fitness[i] = f\n                if f < self.best_fitness[i]:\n                    self.best_fitness[i] = f\n                    self.best_positions[i] = self.population[i].copy()\n                    if f < self.global_best_fitness:\n                        self.global_best_fitness = f\n                        self.global_best_position = self.population[i].copy()\n        \n        last_best_fitness = self.global_best_fitness\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Differential Evolution\n                r1, r2, r3 = np.random.choice(self.pop_size, 3, replace=False)\n                mutant_vector = self.population[r1] + self.mutation_factor * (self.population[r2] - self.population[r3])\n                mutant_vector = np.clip(mutant_vector, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial_vector = np.zeros(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.crossover_rate or j == np.random.randint(self.dim):\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = self.population[i][j]\n\n                # Evaluate trial vector\n                f_trial = func(trial_vector) if self.eval_count < self.budget else np.inf\n                if self.eval_count < self.budget:\n                    self.eval_count += 1\n                    if f_trial < self.fitness[i]:\n                        self.population[i] = trial_vector\n                        self.fitness[i] = f_trial\n                        if f_trial < self.best_fitness[i]:\n                            self.best_fitness[i] = f_trial\n                            self.best_positions[i] = self.population[i].copy()\n                            if f_trial < self.global_best_fitness:\n                                self.global_best_fitness = f_trial\n                                self.global_best_position = self.population[i].copy()\n                                self.stagnation_counter = 0 # Reset stagnation counter\n                    else:\n                        self.stagnation_counter += 1\n\n            # Adjust crossover rate\n            if self.stagnation_counter > self.stagnation_threshold:\n                # If stagnation is detected, increase crossover rate to explore more\n                self.crossover_rate = min(1.0, self.crossover_rate + 0.1)\n            else:\n                # Reduce crossover rate to exploit the current best solutions\n                self.crossover_rate = max(0.1, self.crossover_rate - 0.05)\n\n            # Restart mechanism\n            if self.stagnation_counter > 2 * self.stagnation_threshold:\n                # Re-initialize population around the best solution\n                self.population = np.random.normal(loc=self.global_best_position, scale=0.5, size=(self.pop_size, self.dim))\n                self.population = np.clip(self.population, func.bounds.lb, func.bounds.ub)\n                \n                #Re-evaluate the population\n                for i in range(self.pop_size):\n                    if self.eval_count < self.budget:\n                        f = func(self.population[i])\n                        self.eval_count += 1\n                        self.fitness[i] = f\n                        if f < self.best_fitness[i]:\n                            self.best_fitness[i] = f\n                            self.best_positions[i] = self.population[i].copy()\n                            if f < self.global_best_fitness:\n                                self.global_best_fitness = f\n                                self.global_best_position = self.population[i].copy()\n                self.stagnation_counter = 0\n                self.crossover_rate = self.initial_crossover_rate\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDEwithRestart scored 0.601 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["4516c704-5b12-4e5c-98b6-318610e67c44"], "operator": null, "metadata": {"aucs": [0.21187947677922425, 0.312403635677329, 0.6466785494660208, 0.7621624224611201, 0.6583247185129735, 0.8243778351772546, 0.4587060944074962, 0.5556354225551319, 0.7755092925736292, 0.49356847337758847, 0.7437718877340742, 0.9957298467495176, 0.25494891961847843, 0.5731991397645612, 0.811216654011245, 0.8520340504316293, 0.47040050713339177, 0.8511815267859855, 0.2666383218797629, 0.4960977969563356]}}
{"id": "1a5fb26c-61f1-445c-b517-361f307361fa", "fitness": 0.5510950134027424, "name": "HybridDEPSO_Sobol", "description": "A metaheuristic that combines aspects of Differential Evolution, Particle Swarm Optimization, and a Sobol-sequence-based mutation with a dynamically adjusted exploration-exploitation balance.", "code": "import numpy as np\nfrom scipy.stats import norm\n\nclass HybridDEPSO_Sobol:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=20, de_mutation_factor=0.5, pso_inertia=0.7, pso_cognitive=1.5, pso_social=1.5):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = initial_pop_size\n        self.pop_size = initial_pop_size\n        self.population = np.random.uniform(-5, 5, size=(self.pop_size, dim))\n        self.fitness = np.zeros(self.pop_size)\n        self.velocities = np.zeros((self.pop_size, dim))\n        self.best_positions = self.population.copy()\n        self.best_fitness = np.full(self.pop_size, np.inf)\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.de_mutation_factor = de_mutation_factor\n        self.pso_inertia = pso_inertia\n        self.pso_cognitive = pso_cognitive\n        self.pso_social = pso_social\n        self.exploration_prob = 0.5  # Probability of exploration (DE/Sobol)\n        self.exploitation_prob = 0.5  # Probability of exploitation (PSO)\n        self.exploration_decay = 0.995 #Reduce exploration as budget is used.\n        self.sobol_index = 0\n        self.sobol_sequence = self.generate_sobol(budget, dim)\n\n    def generate_sobol(self, n, dim):\n        try:\n            from sobol_seq import i4_sobol_generate\n            sequence = i4_sobol_generate(dim, n)\n            return sequence * 10 - 5 # Scale to [-5, 5]\n        except ImportError:\n            print(\"Sobol sequence generation requires the 'sobol_seq' library. Install it with: pip install sobol_seq\")\n            return np.random.uniform(-5, 5, size=(n, dim))\n\n    def __call__(self, func):\n        eval_count = 0\n\n        # Initial evaluation\n        for i in range(self.pop_size):\n            if eval_count < self.budget:\n                f = func(self.population[i])\n                eval_count += 1\n                self.fitness[i] = f\n                if f < self.best_fitness[i]:\n                    self.best_fitness[i] = f\n                    self.best_positions[i] = self.population[i].copy()\n                    if f < self.global_best_fitness:\n                        self.global_best_fitness = f\n                        self.global_best_position = self.population[i].copy()\n\n        while eval_count < self.budget:\n            for i in range(self.pop_size):\n                if np.random.rand() < self.exploration_prob:\n                    # Exploration (DE or Sobol)\n                    if np.random.rand() < 0.5:  # Choose between DE and Sobol\n                        # Differential Evolution\n                        r1, r2, r3 = np.random.choice(self.pop_size, 3, replace=False)\n                        new_position = self.best_positions[r1] + self.de_mutation_factor * (self.population[r2] - self.population[r3])\n                        new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n\n                    else:\n                        # Sobol Mutation\n                        if self.sobol_index < self.budget:\n                            new_position = self.sobol_sequence[self.sobol_index]\n                            self.sobol_index += 1\n                        else:\n                            new_position = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                        \n                else:\n                    # Exploitation (PSO)\n                    new_velocity = self.pso_inertia * self.velocities[i] + \\\n                                   self.pso_cognitive * np.random.rand(self.dim) * (self.best_positions[i] - self.population[i]) + \\\n                                   self.pso_social * np.random.rand(self.dim) * (self.global_best_position - self.population[i])\n                    new_position = self.population[i] + new_velocity\n                    new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n                    self.velocities[i] = new_velocity\n\n                # Evaluate new position\n                f = func(new_position)\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n                \n                if f < self.fitness[i]:\n                    self.population[i] = new_position\n                    self.fitness[i] = f\n                    if f < self.best_fitness[i]:\n                        self.best_fitness[i] = f\n                        self.best_positions[i] = self.population[i].copy()\n                        if f < self.global_best_fitness:\n                            self.global_best_fitness = f\n                            self.global_best_position = self.population[i].copy()\n\n            self.exploration_prob *= self.exploration_decay\n            self.exploitation_prob = 1 - self.exploration_prob\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 5, "feedback": "The algorithm HybridDEPSO_Sobol scored 0.551 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b1b3c4d6-e715-4562-8aa5-e77a7b568fac"], "operator": null, "metadata": {"aucs": [0.2507278836680963, 0.355097496713619, 0.4866737769948104, 0.8477250200511721, 0.6332255375268906, 0.8014860318173919, 0.3428737583565581, 0.5450301992378, 0.7014740445036249, 0.21695798870713245, 0.8454912889688413, 0.9945046829597874, 0.4206758749365508, 0.6344720219003154, 0.5806639808455014, 0.6978260088045858, 0.5550707415260103, 0.37973521958004763, 0.23629504700585513, 0.4958936639502558]}}
{"id": "974eeffc-3bcf-499a-aa0d-add9e0340868", "fitness": 0.6107875836623877, "name": "AdaptiveDERestart", "description": "A Differential Evolution variant that uses a dynamically adjusted mutation factor based on population diversity and a restarting mechanism triggered by stagnation.", "code": "import numpy as np\n\nclass AdaptiveDERestart:\n    def __init__(self, budget=10000, dim=10, pop_size=50, CR=0.9, initial_F=0.5, F_decay=0.99, stagnation_limit=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.F = initial_F\n        self.initial_F = initial_F\n        self.F_decay = F_decay\n        self.stagnation_limit = stagnation_limit\n        self.population = None\n        self.fitness = None\n        self.best_fitness_history = []\n        self.stagnation_counter = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.best_fitness_history.append(np.min(self.fitness))\n        self.budget -= self.pop_size\n\n    def mutate(self, x_i):\n        indices = np.random.choice(self.pop_size, size=3, replace=False)\n        x_r1 = self.population[indices[0]]\n        x_r2 = self.population[indices[1]]\n        x_r3 = self.population[indices[2]]\n\n        # Dynamic F based on population diversity\n        diversity = np.std(self.fitness)\n        F = self.F * (1 + diversity)  # Increase F when diversity is high\n        F = np.clip(F, 0.1, 1.0) # Clip F to a reasonable range\n\n        return x_r1 + F * (x_r2 - x_r3)\n\n    def crossover(self, x_i, v_i):\n        u_i = np.copy(x_i)\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() <= self.CR or j == j_rand:\n                u_i[j] = v_i[j]\n        return u_i\n\n    def repair(self, x, func):\n        return np.clip(x, func.bounds.lb, func.bounds.ub)\n\n    def check_stagnation(self):\n        if len(self.best_fitness_history) > self.stagnation_limit:\n            if np.abs(self.best_fitness_history[-1] - np.mean(self.best_fitness_history[-self.stagnation_limit:])) < 1e-6:\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n\n        if self.stagnation_counter >= self.stagnation_limit:\n            return True\n        else:\n            return False\n\n    def restart(self, func):\n        \"\"\"Restart the population with new random individuals.\"\"\"\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.F = self.initial_F  # Reset F\n        self.stagnation_counter = 0\n        self.best_fitness_history = [np.min(self.fitness)]  # Reset history\n        self.budget -= self.pop_size\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                v_i = self.mutate(self.population[i])\n\n                # Crossover\n                u_i = self.crossover(self.population[i], v_i)\n\n                # Repair\n                u_i = self.repair(u_i, func)\n\n                # Selection\n                f_u_i = func(u_i)\n                self.budget -= 1\n\n                if f_u_i < self.fitness[i]:\n                    self.population[i] = u_i\n                    self.fitness[i] = f_u_i\n                    if f_u_i < self.f_opt:\n                        self.f_opt = f_u_i\n                        self.x_opt = u_i\n\n            # Stagnation Check and Restart\n            if self.check_stagnation():\n                self.restart(func)\n                \n            self.best_fitness_history.append(np.min(self.fitness))\n            self.F *= self.F_decay # Decrease F gradually\n\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDERestart scored 0.611 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e298162b-2edc-4d8a-a100-c1ba3100aa90"], "operator": null, "metadata": {"aucs": [0.20319509446961437, 0.4279439288801087, 0.4662536788732795, 0.8398393275214338, 0.7506006908763769, 0.8004324174913511, 0.5932260691131095, 0.5547402380514743, 0.5975850101183815, 0.41268643334287247, 0.8563015662808399, 0.9877250454318953, 0.35354338618481485, 0.728976660131402, 0.690158223969179, 0.8052291390026393, 0.6133449463498565, 0.6769485071861483, 0.3399995983037546, 0.5170217116692212]}}
{"id": "5bc8ca0b-b981-465d-b471-65a7ddd94c89", "fitness": 0.5788843789786858, "name": "AdaptiveDERestart", "description": "Adaptive Differential Evolution with a self-adjusting mutation factor based on population diversity and a restart mechanism triggered by stagnation.", "code": "import numpy as np\n\nclass AdaptiveDERestart:\n    def __init__(self, budget=10000, dim=10, pop_size=50, CR=0.9, stagnation_limit=100, F_init=0.5, F_adapt_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.stagnation_limit = stagnation_limit\n        self.F = F_init\n        self.F_adapt_rate = F_adapt_rate\n        self.population = None\n        self.fitness = None\n        self.best_fitness_history = []\n        self.stagnation_counter = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.best_fitness_history.append(self.f_opt)\n\n    def mutate(self, x_i):\n        indices = np.random.choice(self.pop_size, size=3, replace=False)\n        x_r1 = self.population[indices[0]]\n        x_r2 = self.population[indices[1]]\n        x_r3 = self.population[indices[2]]\n        return x_r1 + self.F * (x_r2 - x_r3)\n\n    def crossover(self, x_i, v_i):\n        u_i = np.copy(x_i)\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() <= self.CR or j == j_rand:\n                u_i[j] = v_i[j]\n        return u_i\n\n    def repair(self, x, func):\n        return np.clip(x, func.bounds.lb, func.bounds.ub)\n\n    def adjust_mutation_factor(self):\n        \"\"\"Adjust F based on population diversity.\"\"\"\n        diversity = np.std(self.fitness)\n        if diversity > 0:\n            self.F = np.clip(self.F + self.F_adapt_rate * (0.5 - np.random.rand()), 0.1, 1.0)  # Adapt F based on diversity\n        else:\n            self.F = 0.5\n\n    def check_stagnation(self):\n        \"\"\"Check if the optimization has stagnated.\"\"\"\n        if len(self.best_fitness_history) > self.stagnation_limit:\n            if np.abs(self.best_fitness_history[-1] - np.mean(self.best_fitness_history[-self.stagnation_limit:])) < 1e-6:\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n\n        if self.stagnation_counter >= self.stagnation_limit:\n            return True\n        else:\n            return False\n\n    def restart_population(self, func):\n        \"\"\"Restart the population with new random individuals.\"\"\"\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.best_fitness_history.append(self.f_opt)\n        self.stagnation_counter = 0\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            self.adjust_mutation_factor()\n\n            for i in range(self.pop_size):\n                # Mutation\n                v_i = self.mutate(self.population[i])\n\n                # Crossover\n                u_i = self.crossover(self.population[i], v_i)\n\n                # Repair\n                u_i = self.repair(u_i, func)\n\n                # Selection\n                f_u_i = func(u_i)\n                self.budget -= 1\n\n                if f_u_i < self.fitness[i]:\n                    self.population[i] = u_i\n                    self.fitness[i] = f_u_i\n\n                    if f_u_i < self.f_opt:\n                        self.f_opt = f_u_i\n                        self.x_opt = u_i\n\n            best_fitness = np.min(self.fitness)\n            self.best_fitness_history.append(best_fitness)\n\n            if best_fitness < self.f_opt:\n                self.f_opt = best_fitness\n                self.x_opt = self.population[np.argmin(self.fitness)]\n\n            if self.check_stagnation():\n                self.restart_population(func)\n\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDERestart scored 0.579 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e298162b-2edc-4d8a-a100-c1ba3100aa90"], "operator": null, "metadata": {"aucs": [0.24217569849778742, 0.4081457290250051, 0.6778292849091647, 0.883670018705839, 0.7738802213645307, 0.8230736761651848, 0.49321452721301606, 0.5380573659367096, 0.6257549394945793, 0.6382841334685803, 0.8425269529638324, 0]}}
{"id": "92472a83-5b4e-4af1-8fd2-87f44b185336", "fitness": 0.29923608129282625, "name": "PSO_CMA", "description": "Combines Particle Swarm Optimization with a covariance matrix adaptation strategy to guide the swarm and improve exploration.", "code": "import numpy as np\n\nclass PSO_CMA:\n    def __init__(self, budget=10000, dim=10, pop_size=20, w=0.7, c1=1.5, c2=1.5, initial_sigma=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w = w  # Inertia weight\n        self.c1 = c1 # Cognitive coefficient\n        self.c2 = c2 # Social coefficient\n        self.initial_sigma = initial_sigma\n        self.population = np.random.uniform(-5, 5, size=(pop_size, dim))\n        self.velocities = np.random.uniform(-1, 1, size=(pop_size, dim))\n        self.fitness = np.zeros(pop_size)\n        self.personal_best_positions = self.population.copy()\n        self.personal_best_fitness = np.full(pop_size, np.inf)\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.eval_count = 0\n        self.mean = np.zeros(dim)\n        self.covariance = np.eye(dim) * (self.initial_sigma**2)\n\n    def __call__(self, func):\n        self.eval_count = 0\n        while self.eval_count < self.budget:\n            # Evaluate fitness\n            for i in range(self.pop_size):\n                if self.eval_count < self.budget:\n                    f = func(self.population[i])\n                    self.eval_count += 1\n                    self.fitness[i] = f\n                    if f < self.personal_best_fitness[i]:\n                        self.personal_best_fitness[i] = f\n                        self.personal_best_positions[i] = self.population[i].copy()\n                        if f < self.global_best_fitness:\n                            self.global_best_fitness = f\n                            self.global_best_position = self.population[i].copy()\n\n            # Update mean and covariance matrix using CMA-ES principles\n            self.mean = np.mean(self.population, axis=0)\n            self.covariance = np.cov(self.population.T) + np.eye(self.dim) * (self.initial_sigma/10) **2 # Add a small diagonal matrix for numerical stability\n\n            # Update velocities and positions\n            for i in range(self.pop_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n\n                cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.population[i])\n                social_component = self.c2 * r2 * (self.global_best_position - self.population[i])\n\n                # Sample from multivariate normal distribution based on CMA\n                innovation = np.random.multivariate_normal(np.zeros(self.dim), self.covariance)\n                \n                self.velocities[i] = self.w * self.velocities[i] + cognitive_component + social_component + innovation\n\n                self.population[i] = self.population[i] + self.velocities[i]\n                self.population[i] = np.clip(self.population[i], func.bounds.lb, func.bounds.ub)\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 5, "feedback": "The algorithm PSO_CMA scored 0.299 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["4516c704-5b12-4e5c-98b6-318610e67c44"], "operator": null, "metadata": {"aucs": [0.12224898375962201, 0.20324420618329975, 0.25859103764511093, 0.25956748975173227, 0.2280465918294543, 0.23954441017883554, 0.2600286772043193, 0.21609045031503693, 0.20578932065743816, 0.17173471908384508, 0.2344880139640877, 0.9972372879747411, 0.2875305359688247, 0.23860253178052648, 0.6281458522621943, 0.29163310887897864, 0.21541366106387871, 0.2963689137994754, 0.16494504882071814, 0.4654707847344053]}}
{"id": "28fd66e4-c82e-4fd0-af18-b5781007acd4", "fitness": 0.20077240492183415, "name": "SOMDEOptimizer", "description": "A population-based algorithm that combines a self-organizing map (SOM) for population distribution and a differential evolution (DE) strategy for exploration, adapting the DE mutation factor based on SOM neuron distances.", "code": "import numpy as np\n\nclass SOMDEOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, som_grid_size=5, learning_rate=0.1, de_mutation_factor_base=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.som_grid_size = som_grid_size\n        self.learning_rate = learning_rate\n        self.de_mutation_factor_base = de_mutation_factor_base\n        self.population = np.random.uniform(-5, 5, size=(pop_size, dim))\n        self.fitness = np.zeros(pop_size)\n        self.best_positions = self.population.copy()\n        self.best_fitness = np.full(pop_size, np.inf)\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.eval_count = 0\n\n        # Initialize SOM neurons\n        self.som_neurons = np.random.uniform(-5, 5, size=(som_grid_size, som_grid_size, dim))\n\n    def __call__(self, func):\n        self.eval_count = 0\n        while self.eval_count < self.budget:\n            # Evaluate fitness\n            for i in range(self.pop_size):\n                if self.eval_count < self.budget:\n                    f = func(self.population[i])\n                    self.eval_count += 1\n                    self.fitness[i] = f\n                    if f < self.best_fitness[i]:\n                        self.best_fitness[i] = f\n                        self.best_positions[i] = self.population[i].copy()\n                        if f < self.global_best_fitness:\n                            self.global_best_fitness = f\n                            self.global_best_position = self.population[i].copy()\n\n            # Train SOM\n            for i in range(self.pop_size):\n                # Find the best matching unit (BMU)\n                bmu_index = self.find_bmu(self.population[i])\n                \n                # Update the SOM neurons\n                self.update_som_neurons(self.population[i], bmu_index)\n\n            # Update population using DE with SOM-based mutation factor\n            for i in range(self.pop_size):\n                r1, r2, r3 = np.random.choice(self.pop_size, 3, replace=False)\n                \n                # Find BMU for the current individual\n                bmu_index = self.find_bmu(self.population[i])\n                \n                # Calculate distance to other neurons in SOM\n                distances = self.calculate_neuron_distances(bmu_index)\n                \n                # Adapt mutation factor based on distances (e.g., inverse proportional)\n                de_mutation_factor = self.de_mutation_factor_base / (1 + np.mean(distances))  # Smaller distances -> larger mutation\n\n                de_vector = self.population[r1] + de_mutation_factor * (self.population[r2] - self.population[r3])\n                de_vector = np.clip(de_vector, func.bounds.lb, func.bounds.ub)\n                \n                # Update individual only if it improves fitness\n                f_new = func(de_vector) if self.eval_count < self.budget else np.inf\n                if self.eval_count < self.budget:\n                    self.eval_count += 1\n                    if f_new < self.fitness[i]:\n                        self.population[i] = de_vector\n                        self.fitness[i] = f_new\n                        if f_new < self.best_fitness[i]:\n                            self.best_fitness[i] = f_new\n                            self.best_positions[i] = self.population[i].copy()\n                            if f_new < self.global_best_fitness:\n                                self.global_best_fitness = f_new\n                                self.global_best_position = self.population[i].copy()\n\n        return self.global_best_fitness, self.global_best_position\n    \n    def find_bmu(self, individual):\n        \"\"\"Find the best matching unit (BMU) in the SOM grid.\"\"\"\n        min_dist = np.inf\n        bmu_index = None\n        for i in range(self.som_grid_size):\n            for j in range(self.som_grid_size):\n                dist = np.linalg.norm(individual - self.som_neurons[i, j])\n                if dist < min_dist:\n                    min_dist = dist\n                    bmu_index = (i, j)\n        return bmu_index\n\n    def update_som_neurons(self, individual, bmu_index):\n        \"\"\"Update the SOM neurons based on the BMU and learning rate.\"\"\"\n        for i in range(self.som_grid_size):\n            for j in range(self.som_grid_size):\n                distance = np.sqrt((i - bmu_index[0])**2 + (j - bmu_index[1])**2)\n                influence = np.exp(-distance**2 / (2 * (self.som_grid_size/2)**2))  # Gaussian neighborhood\n                self.som_neurons[i, j] += self.learning_rate * influence * (individual - self.som_neurons[i, j])\n    \n    def calculate_neuron_distances(self, bmu_index):\n        \"\"\"Calculate distances from BMU to all other neurons.\"\"\"\n        distances = []\n        for i in range(self.som_grid_size):\n            for j in range(self.som_grid_size):\n                distances.append(np.sqrt((i - bmu_index[0])**2 + (j - bmu_index[1])**2))\n        return np.array(distances)", "configspace": "", "generation": 5, "feedback": "The algorithm SOMDEOptimizer scored 0.201 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["4516c704-5b12-4e5c-98b6-318610e67c44"], "operator": null, "metadata": {"aucs": [0.13582336343387413, 0.16898348288168763, 0.23725356412153953, 0.1652842502195695, 0.1863864107620209, 0.1513169343701689, 0.1760197419923175, 0.1522452699557727, 0.15497872604031582, 0.1507531939123271, 0.16931256581536291, 0.24195961917758735, 0.23030457661628556, 0.17538547162683316, 0.5410112180452616, 0.239282362484882, 0.19359556237720887, 0.18050296257979748, 0.1948975443993316, 0.1701512776245383]}}
{"id": "7996554b-0fe7-40e4-97e0-749b79f3713a", "fitness": -Infinity, "name": "SelfAdaptiveDE_LevyCauchy", "description": "Self-Adaptive Differential Evolution with Lvy flight mutation and a Cauchy-distributed crossover probability for enhanced exploration.", "code": "import numpy as np\n\nclass SelfAdaptiveDE_LevyCauchy:\n    def __init__(self, budget=10000, dim=10, pop_size=50, initial_CR=0.5, initial_F=0.7, levy_exponent=1.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = initial_CR\n        self.F = initial_F\n        self.levy_exponent = levy_exponent\n        self.population = None\n        self.fitness = None\n        self.archive = []  # Archive for storing discarded solutions\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n    def levy_flight(self, shape, exponent=1.5):\n        \"\"\"Generate Levy distributed steps.\"\"\"\n        numerator = np.gamma(1 + exponent) * np.sin(np.pi * exponent / 2)\n        denominator = np.gamma((1 + exponent) / 2) * exponent * (2 ** ((exponent - 1) / 2))\n        sigma = (numerator / denominator) ** (1 / exponent)\n        u = np.random.normal(0, sigma, shape)\n        v = np.random.normal(0, 1, shape)\n        return u / (np.abs(v) ** (1 / exponent))\n\n    def mutate(self, x_i, func):\n        indices = np.random.choice(self.pop_size, size=3, replace=False)\n        x_r1 = self.population[indices[0]]\n        x_r2 = self.population[indices[1]]\n        x_r3 = self.population[indices[2]]\n        \n        levy_steps = self.levy_flight(self.dim, self.levy_exponent)\n        v_i = x_r1 + self.F * (x_r2 - x_r3) + levy_steps * (func.bounds.ub - func.bounds.lb) \n        \n        return v_i\n\n    def crossover(self, x_i, v_i):\n        u_i = np.copy(x_i)\n        j_rand = np.random.randint(self.dim)\n        \n        # Cauchy distributed crossover probability\n        crossover_mask = np.random.rand(self.dim) < np.abs(np.random.standard_cauchy(size=self.dim) * self.CR)\n        crossover_mask[j_rand] = True  # Ensure at least one gene is exchanged\n        u_i[crossover_mask] = v_i[crossover_mask]\n        return u_i\n\n    def repair(self, x, func):\n        return np.clip(x, func.bounds.lb, func.bounds.ub)\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                v_i = self.mutate(self.population[i], func)\n\n                # Crossover\n                u_i = self.crossover(self.population[i], v_i)\n\n                # Repair\n                u_i = self.repair(u_i, func)\n\n                # Selection\n                f_u_i = func(u_i)\n                self.budget -= 1\n\n                if f_u_i < self.fitness[i]:\n                    # Archive the replaced solution\n                    self.archive.append(self.population[i].copy())  \n                    \n                    self.population[i] = u_i\n                    self.fitness[i] = f_u_i\n                    if f_u_i < self.f_opt:\n                        self.f_opt = f_u_i\n                        self.x_opt = u_i\n\n            # Self-adaptation of CR and F (simple adaptation)\n            self.CR = np.clip(np.random.normal(self.CR, 0.1), 0.1, 0.9)\n            self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n            \n\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 61, in __call__\n  File \"<string>\", line 35, in mutate\n  File \"<string>\", line 22, in levy_flight\n  File \"/home/neocortex/.cache/pypoetry/virtualenvs/blade-xXF9vABH-py3.10/lib/python3.10/site-packages/numpy/__init__.py\", line 320, in __getattr__\n    raise AttributeError(\"module {!r} has no attribute \"\nAttributeError: module 'numpy' has no attribute 'gamma'\n.", "error": "", "parent_ids": ["974eeffc-3bcf-499a-aa0d-add9e0340868"], "operator": null, "metadata": {}}
{"id": "0da30108-4ce0-49c5-aaf0-21e56ef98bf5", "fitness": -Infinity, "name": "AdaptiveDENM", "description": "An adaptive Differential Evolution variant that adjusts both mutation and crossover rates based on success history and incorporates a local search phase using Nelder-Mead simplex for promising individuals.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDENM:\n    def __init__(self, budget=10000, dim=10, pop_size=50, initial_F=0.5, initial_CR=0.9, F_decay=0.99, CR_decay=0.99, local_search_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = initial_F\n        self.CR = initial_CR\n        self.initial_F = initial_F\n        self.initial_CR = initial_CR\n        self.F_decay = F_decay\n        self.CR_decay = CR_decay\n        self.local_search_prob = local_search_prob\n        self.population = None\n        self.fitness = None\n        self.success_F = []\n        self.success_CR = []\n        self.memory_size = 10\n        self.best_fitness = np.inf\n        self.best_individual = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.best_fitness = np.min(self.fitness)\n        self.best_individual = self.population[np.argmin(self.fitness)]\n\n    def mutate(self, x_i):\n        indices = np.random.choice(self.pop_size, size=3, replace=False)\n        x_r1 = self.population[indices[0]]\n        x_r2 = self.population[indices[1]]\n        x_r3 = self.population[indices[2]]\n        return x_r1 + self.F * (x_r2 - x_r3)\n\n    def crossover(self, x_i, v_i):\n        u_i = np.copy(x_i)\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() <= self.CR or j == j_rand:\n                u_i[j] = v_i[j]\n        return u_i\n\n    def repair(self, x, func):\n        return np.clip(x, func.bounds.lb, func.bounds.ub)\n    \n    def adapt_parameters(self):\n        if self.success_F:\n            self.F = np.mean(self.success_F)\n            self.CR = np.mean(self.success_CR)\n        self.F = self.F * self.F_decay\n        self.CR = self.CR * self.CR_decay\n        self.F = np.clip(self.F, 0.1, 1.0)\n        self.CR = np.clip(self.CR, 0.1, 1.0)\n\n        self.success_F = []\n        self.success_CR = []\n\n    def local_search(self, individual, func):\n        bounds = [(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)]\n        res = minimize(func, individual, method='Nelder-Mead', bounds=bounds, options={'maxfev': min(50, self.budget)})\n        self.budget -= res.nfev\n        return res.fun, res.x\n    \n    def __call__(self, func):\n        self.initialize_population(func)\n        \n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                v_i = self.mutate(self.population[i])\n\n                # Crossover\n                u_i = self.crossover(self.population[i], v_i)\n\n                # Repair\n                u_i = self.repair(u_i, func)\n\n                # Selection\n                f_u_i = func(u_i)\n                self.budget -= 1\n                \n                if f_u_i < self.fitness[i]:\n                    self.success_F.append(self.F)\n                    self.success_CR.append(self.CR)\n                    self.population[i] = u_i\n                    self.fitness[i] = f_u_i\n\n                    if f_u_i < self.best_fitness:\n                        self.best_fitness = f_u_i\n                        self.best_individual = u_i\n                        \n                # Local Search\n                if np.random.rand() < self.local_search_prob and self.budget > 0:\n                    f_local, x_local = self.local_search(self.population[i], func)\n                    if f_local < self.fitness[i]:\n                        self.population[i] = x_local\n                        self.fitness[i] = f_local\n                        if f_local < self.best_fitness:\n                            self.best_fitness = f_local\n                            self.best_individual = x_local\n            \n            self.adapt_parameters()\n\n            if self.budget <= 0:\n                break\n\n        return self.best_fitness, self.best_individual", "configspace": "", "generation": 6, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 97, in __call__\n  File \"<string>\", line 63, in local_search\nNameError: name 'minimize' is not defined\n.", "error": "", "parent_ids": ["974eeffc-3bcf-499a-aa0d-add9e0340868"], "operator": null, "metadata": {}}
{"id": "7648986e-7ee9-423d-97c8-d8070e4bceeb", "fitness": -Infinity, "name": "HybridDEPSO_OL", "description": "A hybrid algorithm combining aspects of Differential Evolution, Particle Swarm Optimization, and orthogonal learning, adaptively adjusting exploration and exploitation.", "code": "import numpy as np\nfrom scipy.stats import norm\nfrom pyDOE import lhs  # For Latin Hypercube Sampling\n\nclass HybridDEPSO_OL:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=20, de_mutation_factor=0.5, pso_inertia=0.7, pso_cognitive=1.5, pso_social=1.5, ol_factor = 0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = initial_pop_size\n        self.pop_size = initial_pop_size\n        self.population = np.random.uniform(-5, 5, size=(self.pop_size, dim))\n        self.fitness = np.zeros(self.pop_size)\n        self.velocities = np.zeros((self.pop_size, dim))\n        self.best_positions = self.population.copy()\n        self.best_fitness = np.full(self.pop_size, np.inf)\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.de_mutation_factor = de_mutation_factor\n        self.pso_inertia = pso_inertia\n        self.pso_cognitive = pso_cognitive\n        self.pso_social = pso_social\n        self.exploration_prob = 0.5  # Probability of exploration (DE/OL)\n        self.exploitation_prob = 0.5  # Probability of exploitation (PSO)\n        self.exploration_decay = 0.995 #Reduce exploration as budget is used.\n        self.ol_factor = ol_factor # Factor for orthogonal learning.\n\n    def orthogonal_learning(self, x, func):\n        \"\"\"\n        Performs orthogonal learning based on the current solution x.\n        \"\"\"\n        dim = self.dim\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n\n        # Generate a Latin Hypercube Sample around the current solution\n        lhs_sample = lhs(dim, samples=dim)  # Generate dim samples\n        orthogonal_points = np.zeros((dim, dim))\n\n        for i in range(dim):\n            orthogonal_points[i, :] = x + self.ol_factor * (lhs_sample[i, :] - 0.5) * (ub - lb) # Center around 0 and scale\n\n        orthogonal_points = np.clip(orthogonal_points, lb, ub)\n        \n        # Evaluate the orthogonal points and select the best one\n        best_orthogonal_fitness = np.inf\n        best_orthogonal_point = None\n        \n        for i in range(dim):\n            fitness = func(orthogonal_points[i, :])\n            if fitness < best_orthogonal_fitness:\n                best_orthogonal_fitness = fitness\n                best_orthogonal_point = orthogonal_points[i, :].copy()\n        \n        return best_orthogonal_point, best_orthogonal_fitness\n\n\n    def __call__(self, func):\n        eval_count = 0\n\n        # Initial evaluation\n        for i in range(self.pop_size):\n            if eval_count < self.budget:\n                f = func(self.population[i])\n                eval_count += 1\n                self.fitness[i] = f\n                if f < self.best_fitness[i]:\n                    self.best_fitness[i] = f\n                    self.best_positions[i] = self.population[i].copy()\n                    if f < self.global_best_fitness:\n                        self.global_best_fitness = f\n                        self.global_best_position = self.population[i].copy()\n\n        while eval_count < self.budget:\n            for i in range(self.pop_size):\n                if np.random.rand() < self.exploration_prob:\n                    # Exploration (DE or Orthogonal Learning)\n                    if np.random.rand() < 0.5:  # Choose between DE and OL\n                        # Differential Evolution\n                        r1, r2, r3 = np.random.choice(self.pop_size, 3, replace=False)\n                        new_position = self.best_positions[r1] + self.de_mutation_factor * (self.population[r2] - self.population[r3])\n                        new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n\n                    else:\n                        # Orthogonal Learning\n                        new_position, new_fitness = self.orthogonal_learning(self.population[i], func)\n                        if eval_count + self.dim > self.budget:\n                            break  # Check budget before evaluating all orthogonal points\n                        \n                        \n                else:\n                    # Exploitation (PSO)\n                    new_velocity = self.pso_inertia * self.velocities[i] + \\\n                                   self.pso_cognitive * np.random.rand(self.dim) * (self.best_positions[i] - self.population[i]) + \\\n                                   self.pso_social * np.random.rand(self.dim) * (self.global_best_position - self.population[i])\n                    new_position = self.population[i] + new_velocity\n                    new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n                    self.velocities[i] = new_velocity\n\n                # Evaluate new position\n                f = func(new_position)\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n                \n                if f < self.fitness[i]:\n                    self.population[i] = new_position\n                    self.fitness[i] = f\n                    if f < self.best_fitness[i]:\n                        self.best_fitness[i] = f\n                        self.best_positions[i] = self.population[i].copy()\n                        if f < self.global_best_fitness:\n                            self.global_best_fitness = f\n                            self.global_best_position = self.population[i].copy()\n\n            self.exploration_prob *= self.exploration_decay\n            self.exploitation_prob = 1 - self.exploration_prob\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 6, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 111, in evaluate\n    exec(code, safe_globals, local_env)\n  File \"<string>\", line 3, in <module>\nModuleNotFoundError: No module named 'pyDOE'\n.", "error": "", "parent_ids": ["1a5fb26c-61f1-445c-b517-361f307361fa"], "operator": null, "metadata": {}}
{"id": "e9750c7c-65a3-4611-84b6-8bb12cda5beb", "fitness": 0.43689918446530185, "name": "AgingDE", "description": "A Differential Evolution strategy with an aging population, where older individuals are replaced by new random ones to encourage exploration.", "code": "import numpy as np\n\nclass AgingDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, CR=0.9, F=0.5, age_limit=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.F = F\n        self.age_limit = age_limit\n        self.population = None\n        self.fitness = None\n        self.ages = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.ages = np.zeros(self.pop_size, dtype=int)\n        self.budget -= self.pop_size\n\n    def mutate(self, x_i):\n        indices = np.random.choice(self.pop_size, size=3, replace=False)\n        x_r1 = self.population[indices[0]]\n        x_r2 = self.population[indices[1]]\n        x_r3 = self.population[indices[2]]\n        return x_r1 + self.F * (x_r2 - x_r3)\n\n    def crossover(self, x_i, v_i):\n        u_i = np.copy(x_i)\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() <= self.CR or j == j_rand:\n                u_i[j] = v_i[j]\n        return u_i\n\n    def repair(self, x, func):\n        return np.clip(x, func.bounds.lb, func.bounds.ub)\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                v_i = self.mutate(self.population[i])\n\n                # Crossover\n                u_i = self.crossover(self.population[i], v_i)\n\n                # Repair\n                u_i = self.repair(u_i, func)\n\n                # Selection\n                f_u_i = func(u_i)\n                self.budget -= 1\n\n                if f_u_i < self.fitness[i]:\n                    self.population[i] = u_i\n                    self.fitness[i] = f_u_i\n                    self.ages[i] = 0  # Reset age\n                    if f_u_i < self.f_opt:\n                        self.f_opt = f_u_i\n                        self.x_opt = u_i\n                else:\n                    self.ages[i] += 1  # Increment age\n\n                # Aging: Replace old individuals\n                if self.ages[i] > self.age_limit:\n                    self.population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                    self.fitness[i] = func(self.population[i])\n                    self.ages[i] = 0\n                    self.budget -= 1\n                    if self.fitness[i] < self.f_opt:\n                        self.f_opt = self.fitness[i]\n                        self.x_opt = self.population[i]\n\n\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AgingDE scored 0.437 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["974eeffc-3bcf-499a-aa0d-add9e0340868"], "operator": null, "metadata": {"aucs": [0.37199462788632387, 0.6800448565559217, 0.6955572534189618, 0]}}
{"id": "22bb5b9d-a87c-4f59-9abb-2ca6cd6ef86b", "fitness": -Infinity, "name": "FuzzyAdaptiveDE", "description": "A self-adaptive Differential Evolution strategy with a fuzzy logic controller to adjust mutation and crossover rates based on population diversity and fitness improvement.", "code": "import numpy as np\nimport skfuzzy as fuzz\nfrom skfuzzy import control as ctrl\n\nclass FuzzyAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_mutation_factor=0.5, initial_crossover_rate=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.mutation_factor = initial_mutation_factor\n        self.crossover_rate = initial_crossover_rate\n        self.population = np.random.uniform(-5, 5, size=(pop_size, dim))\n        self.fitness = np.zeros(pop_size)\n        self.best_positions = self.population.copy()\n        self.best_fitness = np.full(pop_size, np.inf)\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.eval_count = 0\n        \n        # Fuzzy Logic Controller Setup\n        self.setup_fuzzy_controller()\n\n    def setup_fuzzy_controller(self):\n        # Antecedent (Input) variables\n        diversity = ctrl.Antecedent(np.linspace(0, 1, 100), 'diversity') # Population diversity (0: low, 1: high)\n        improvement = ctrl.Antecedent(np.linspace(-1, 0, 100), 'improvement') # Fitness improvement (-1: large, 0: none)\n\n        # Consequent (Output) variables\n        mutation_change = ctrl.Consequent(np.linspace(-0.2, 0.2, 100), 'mutation_change') # Change in mutation factor\n        crossover_change = ctrl.Consequent(np.linspace(-0.2, 0.2, 100), 'crossover_change') # Change in crossover rate\n\n        # Membership functions (adjust as needed)\n        diversity['low'] = fuzz.trimf(diversity.universe, [0, 0, 0.5])\n        diversity['medium'] = fuzz.trimf(diversity.universe, [0, 0.5, 1])\n        diversity['high'] = fuzz.trimf(diversity.universe, [0.5, 1, 1])\n\n        improvement['low'] = fuzz.trimf(improvement.universe, [-1, -1, -0.5])\n        improvement['medium'] = fuzz.trimf(improvement.universe, [-1, -0.5, 0])\n        improvement['high'] = fuzz.trimf(improvement.universe, [-0.5, 0, 0])\n\n        mutation_change['decrease'] = fuzz.trimf(mutation_change.universe, [-0.2, -0.2, 0])\n        mutation_change['no_change'] = fuzz.trimf(mutation_change.universe, [-0.1, 0, 0.1])\n        mutation_change['increase'] = fuzz.trimf(mutation_change.universe, [0, 0.2, 0.2])\n\n        crossover_change['decrease'] = fuzz.trimf(crossover_change.universe, [-0.2, -0.2, 0])\n        crossover_change['no_change'] = fuzz.trimf(crossover_change.universe, [-0.1, 0, 0.1])\n        crossover_change['increase'] = fuzz.trimf(crossover_change.universe, [0, 0.2, 0.2])\n\n        # Rules (adjust as needed)\n        rule1 = ctrl.Rule(diversity['low'] & improvement['low'], [mutation_change['increase'], crossover_change['decrease']])\n        rule2 = ctrl.Rule(diversity['low'] & improvement['medium'], [mutation_change['increase'], crossover_change['no_change']])\n        rule3 = ctrl.Rule(diversity['low'] & improvement['high'], [mutation_change['no_change'], crossover_change['increase']])\n        rule4 = ctrl.Rule(diversity['medium'] & improvement['low'], [mutation_change['increase'], crossover_change['decrease']])\n        rule5 = ctrl.Rule(diversity['medium'] & improvement['medium'], [mutation_change['no_change'], crossover_change['no_change']])\n        rule6 = ctrl.Rule(diversity['medium'] & improvement['high'], [mutation_change['decrease'], crossover_change['increase']])\n        rule7 = ctrl.Rule(diversity['high'] & improvement['low'], [mutation_change['no_change'], crossover_change['decrease']])\n        rule8 = ctrl.Rule(diversity['high'] & improvement['medium'], [mutation_change['decrease'], crossover_change['no_change']])\n        rule9 = ctrl.Rule(diversity['high'] & improvement['high'], [mutation_change['decrease'], crossover_change['increase']])\n\n        # Control System\n        self.tipping_ctrl = ctrl.ControlSystem([rule1, rule2, rule3, rule4, rule5, rule6, rule7, rule8, rule9])\n        self.tipping = ctrl.ControlSystemSimulation(self.tipping_ctrl)\n        \n    def calculate_diversity(self):\n        # Calculate population diversity (e.g., standard deviation of positions)\n        return np.std(self.population) / 5.0 # Normalize to [0, 1]\n\n    def __call__(self, func):\n        self.eval_count = 0\n        \n        # Initialize fitness values\n        for i in range(self.pop_size):\n            if self.eval_count < self.budget:\n                f = func(self.population[i])\n                self.eval_count += 1\n                self.fitness[i] = f\n                if f < self.best_fitness[i]:\n                    self.best_fitness[i] = f\n                    self.best_positions[i] = self.population[i].copy()\n                    if f < self.global_best_fitness:\n                        self.global_best_fitness = f\n                        self.global_best_position = self.population[i].copy()\n\n        while self.eval_count < self.budget:\n            previous_best_fitness = self.global_best_fitness\n            for i in range(self.pop_size):\n                # Differential Evolution\n                r1, r2, r3 = np.random.choice(self.pop_size, 3, replace=False)\n                mutant_vector = self.population[r1] + self.mutation_factor * (self.population[r2] - self.population[r3])\n                mutant_vector = np.clip(mutant_vector, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial_vector = np.zeros(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.crossover_rate or j == np.random.randint(self.dim):\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = self.population[i][j]\n\n                # Evaluate trial vector\n                f_trial = func(trial_vector) if self.eval_count < self.budget else np.inf\n                if self.eval_count < self.budget:\n                    self.eval_count += 1\n                    if f_trial < self.fitness[i]:\n                        self.population[i] = trial_vector\n                        self.fitness[i] = f_trial\n                        if f_trial < self.best_fitness[i]:\n                            self.best_fitness[i] = f_trial\n                            self.best_positions[i] = self.population[i].copy()\n                            if f_trial < self.global_best_fitness:\n                                self.global_best_fitness = f_trial\n                                self.global_best_position = self.population[i].copy()\n\n            # Calculate diversity and improvement\n            diversity_value = self.calculate_diversity()\n            improvement_value = (previous_best_fitness - self.global_best_fitness) / abs(previous_best_fitness) if previous_best_fitness != 0 else 0\n            improvement_value = np.clip(improvement_value, -1, 0)\n\n            # Fuzzy Inference\n            self.tipping.input['diversity'] = diversity_value\n            self.tipping.input['improvement'] = improvement_value\n            self.tipping.compute()\n            \n            mutation_change = self.tipping.output['mutation_change']\n            crossover_change = self.tipping.output['crossover_change']\n            \n            # Update mutation and crossover rates\n            self.mutation_factor = np.clip(self.mutation_factor + mutation_change, 0.1, 1.0)\n            self.crossover_rate = np.clip(self.crossover_rate + crossover_change, 0.1, 0.95)\n            \n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 6, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 111, in evaluate\n    exec(code, safe_globals, local_env)\n  File \"<string>\", line 2, in <module>\nModuleNotFoundError: No module named 'skfuzzy'\n.", "error": "", "parent_ids": ["fefe3056-d5bf-4b76-b029-500b62bc8c84"], "operator": null, "metadata": {}}
{"id": "29ed4910-5097-4066-b11c-5fd7f20a5d5a", "fitness": -Infinity, "name": "SelfAdaptiveDE_NM", "description": "A self-adaptive Differential Evolution variant that adjusts both the mutation factor and crossover rate based on the success rate of previous generations and incorporates a local search using Nelder-Mead simplex method when stagnation is detected.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass SelfAdaptiveDE_NM:\n    def __init__(self, budget=10000, dim=10, pop_size=50, CR_init=0.5, F_init=0.5, success_rate_memory=10, stagnation_limit=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR_init\n        self.F = F_init\n        self.success_rate_memory = success_rate_memory\n        self.stagnation_limit = stagnation_limit\n        self.population = None\n        self.fitness = None\n        self.best_fitness_history = []\n        self.successful_CRs = []\n        self.successful_Fs = []\n        self.stagnation_counter = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.best_fitness_history.append(self.f_opt)\n\n    def mutate(self, x_i):\n        indices = np.random.choice(self.pop_size, size=3, replace=False)\n        x_r1 = self.population[indices[0]]\n        x_r2 = self.population[indices[1]]\n        x_r3 = self.population[indices[2]]\n        return x_r1 + self.F * (x_r2 - x_r3)\n\n    def crossover(self, x_i, v_i):\n        u_i = np.copy(x_i)\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() <= self.CR or j == j_rand:\n                u_i[j] = v_i[j]\n        return u_i\n\n    def repair(self, x, func):\n        return np.clip(x, func.bounds.lb, func.bounds.ub)\n\n    def adjust_parameters(self):\n        if self.successful_CRs:\n            self.CR = np.mean(self.successful_CRs)\n            self.successful_CRs = []\n        if self.successful_Fs:\n            self.F = np.mean(self.successful_Fs)\n            self.successful_Fs = []\n        self.F = np.clip(self.F, 0.1, 1.0)\n        self.CR = np.clip(self.CR, 0.1, 0.9)\n\n    def check_stagnation(self):\n        if len(self.best_fitness_history) > self.stagnation_limit:\n            if np.abs(self.best_fitness_history[-1] - np.mean(self.best_fitness_history[-self.stagnation_limit:])) < 1e-6:\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n\n        if self.stagnation_counter >= self.stagnation_limit:\n            return True\n        else:\n            return False\n\n    def local_search(self, func, x_start):\n        bounds = [(func.bounds.lb, func.bounds.ub)] * self.dim\n        res = minimize(func, x_start, method='Nelder-Mead', bounds=bounds, options={'maxfev': self.budget // 100})  # Limit FE\n        self.budget -= res.nfev\n        return res.fun, res.x\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            self.adjust_parameters()\n\n            for i in range(self.pop_size):\n                # Mutation\n                v_i = self.mutate(self.population[i])\n\n                # Crossover\n                u_i = self.crossover(self.population[i], v_i)\n\n                # Repair\n                u_i = self.repair(u_i, func)\n\n                # Selection\n                f_u_i = func(u_i)\n                self.budget -= 1\n\n                if f_u_i < self.fitness[i]:\n                    self.successful_CRs.append(self.CR)\n                    self.successful_Fs.append(self.F)\n                    self.population[i] = u_i\n                    self.fitness[i] = f_u_i\n\n                    if f_u_i < self.f_opt:\n                        self.f_opt = f_u_i\n                        self.x_opt = u_i\n\n            best_fitness = np.min(self.fitness)\n            self.best_fitness_history.append(best_fitness)\n\n            if best_fitness < self.f_opt:\n                self.f_opt = best_fitness\n                self.x_opt = self.population[np.argmin(self.fitness)]\n\n            if self.check_stagnation():\n                # Perform local search on the best individual\n                f_local, x_local = self.local_search(func, self.x_opt)\n                if f_local < self.f_opt:\n                    self.f_opt = f_local\n                    self.x_opt = x_local\n                self.stagnation_counter = 0\n\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 144, in evaluate\n    algorithm(f_new)\n  File \"<string>\", line 115, in __call__\n  File \"<string>\", line 72, in local_search\nNameError: name 'minimize' is not defined\n.", "error": "", "parent_ids": ["5bc8ca0b-b981-465d-b471-65a7ddd94c89"], "operator": null, "metadata": {}}
{"id": "b7309448-df5f-4016-963b-6e2e049695a1", "fitness": -Infinity, "name": "AdaptiveHybridDEPSOGaussian", "description": "A hybrid algorithm that integrates Differential Evolution, Particle Swarm Optimization, and Gaussian mutation with adaptive parameter control and a diversity-based restart strategy.", "code": "import numpy as np\n\nclass AdaptiveHybridDEPSOGaussian:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=20, de_mutation_factor=0.5, pso_inertia=0.7, pso_cognitive=1.5, pso_social=1.5, gaussian_mutation_rate=0.1, diversity_threshold=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = initial_pop_size\n        self.pop_size = initial_pop_size\n        self.population = np.random.uniform(-5, 5, size=(self.pop_size, dim))\n        self.fitness = np.zeros(self.pop_size)\n        self.velocities = np.zeros((self.pop_size, dim))\n        self.best_positions = self.population.copy()\n        self.best_fitness = np.full(self.pop_size, np.inf)\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.de_mutation_factor = de_mutation_factor\n        self.pso_inertia = pso_inertia\n        self.pso_cognitive = pso_cognitive\n        self.pso_social = pso_social\n        self.gaussian_mutation_rate = gaussian_mutation_rate\n        self.diversity_threshold = diversity_threshold\n        self.eval_count = 0\n        self.stagnation_counter = 0\n        self.max_stagnation = 100\n        self.previous_best_fitness = np.inf\n        self.adaptive_mutation_factor = de_mutation_factor # Adaptive DE mutation\n        self.adaptive_inertia = pso_inertia #Adaptive PSO inertia\n\n    def calculate_diversity(self):\n        \"\"\"Calculates population diversity based on the mean pairwise distance.\"\"\"\n        distances = np.linalg.norm(self.population[:, np.newaxis, :] - self.population[np.newaxis, :, :], axis=2)\n        diversity = np.mean(distances)\n        return diversity\n    \n    def gaussian_mutation(self, x):\n        \"\"\"Applies Gaussian mutation to a given solution.\"\"\"\n        mutation = np.random.normal(0, self.gaussian_mutation_rate, size=self.dim)\n        return x + mutation\n\n    def restart_population(self):\n         \"\"\"Restarts the population with new random solutions.\"\"\"\n         self.population = np.random.uniform(-5, 5, size=(self.pop_size, self.dim))\n         self.fitness = np.zeros(self.pop_size)\n         self.velocities = np.zeros((self.pop_size, self.dim))\n         self.best_positions = self.population.copy()\n         self.best_fitness = np.full(self.pop_size, np.inf)\n\n         # Re-evaluate the restarted population\n         for i in range(self.pop_size):\n             if self.eval_count < self.budget:\n                 f = func(self.population[i])\n                 self.eval_count += 1\n                 self.fitness[i] = f\n                 if f < self.best_fitness[i]:\n                     self.best_fitness[i] = f\n                     self.best_positions[i] = self.population[i].copy()\n                     if f < self.global_best_fitness:\n                         self.global_best_fitness = f\n                         self.global_best_position = self.population[i].copy()\n\n\n    def __call__(self, func):\n        self.eval_count = 0\n\n        # Initial evaluation\n        for i in range(self.pop_size):\n            if self.eval_count < self.budget:\n                f = func(self.population[i])\n                self.eval_count += 1\n                self.fitness[i] = f\n                if f < self.best_fitness[i]:\n                    self.best_fitness[i] = f\n                    self.best_positions[i] = self.population[i].copy()\n                    if f < self.global_best_fitness:\n                        self.global_best_fitness = f\n                        self.global_best_position = self.population[i].copy()\n        self.previous_best_fitness = self.global_best_fitness\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Choose between DE, PSO, and Gaussian mutation\n                rand = np.random.rand()\n                if rand < 0.33:\n                    # Differential Evolution\n                    r1, r2, r3 = np.random.choice(self.pop_size, 3, replace=False)\n                    new_position = self.best_positions[r1] + self.adaptive_mutation_factor * (self.population[r2] - self.population[r3])\n                    new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n                elif rand < 0.66:\n                    # Particle Swarm Optimization\n                    new_velocity = self.adaptive_inertia * self.velocities[i] + \\\n                                   self.pso_cognitive * np.random.rand(self.dim) * (self.best_positions[i] - self.population[i]) + \\\n                                   self.pso_social * np.random.rand(self.dim) * (self.global_best_position - self.population[i])\n                    new_position = self.population[i] + new_velocity\n                    new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n                    self.velocities[i] = new_velocity\n                else:\n                    # Gaussian Mutation\n                    new_position = self.gaussian_mutation(self.population[i])\n                    new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n\n                # Evaluate new position\n                f = func(new_position)\n                self.eval_count += 1\n                if self.eval_count >= self.budget:\n                    break\n\n                if f < self.fitness[i]:\n                    self.population[i] = new_position\n                    self.fitness[i] = f\n                    if f < self.best_fitness[i]:\n                        self.best_fitness[i] = f\n                        self.best_positions[i] = self.population[i].copy()\n                        if f < self.global_best_fitness:\n                            self.global_best_fitness = f\n                            self.global_best_position = self.population[i].copy()\n            #Adapt mutation factor\n            self.adaptive_mutation_factor = 0.5 + 0.5 * np.exp(-10 * self.eval_count / self.budget)\n            self.adaptive_inertia = 0.9 - 0.7 * (self.eval_count / self.budget)\n\n            # Stagnation check and restart\n            if self.global_best_fitness >= self.previous_best_fitness:\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n\n            if self.stagnation_counter > self.max_stagnation:\n                diversity = self.calculate_diversity()\n                if diversity < self.diversity_threshold:\n                    self.restart_population()\n                    self.stagnation_counter = 0\n\n            self.previous_best_fitness = self.global_best_fitness\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 6, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 144, in evaluate\n    algorithm(f_new)\n  File \"<string>\", line 128, in __call__\n  File \"<string>\", line 51, in restart_population\nNameError: name 'func' is not defined\n.", "error": "", "parent_ids": ["1a5fb26c-61f1-445c-b517-361f307361fa"], "operator": null, "metadata": {}}
{"id": "81d9cbe7-5523-4882-9558-b147094932fa", "fitness": -Infinity, "name": "DynamicAdaptiveDE", "description": "Differential Evolution with dynamic population sizing, adaptive mutation strategies based on past success, and a Cauchy mutation operator for escaping local optima.", "code": "import numpy as np\n\nclass DynamicAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, min_pop_size=10, CR=0.9, initial_F=0.5, F_decay=0.99, learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.min_pop_size = min_pop_size\n        self.CR = CR\n        self.F = initial_F\n        self.initial_F = initial_F\n        self.F_decay = F_decay\n        self.learning_rate = learning_rate\n        self.population = None\n        self.fitness = None\n        self.best_fitness_history = []\n        self.successful_F = []\n        self.archive = []\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.best_fitness_history.append(np.min(self.fitness))\n        self.budget -= self.pop_size\n\n    def mutate(self, x_i):\n        if np.random.rand() < 0.1: # Cauchy mutation for exploration\n            return x_i + 0.01 * np.random.standard_cauchy(size=self.dim)\n\n        indices = np.random.choice(self.pop_size, size=3, replace=False)\n        x_r1 = self.population[indices[0]]\n        x_r2 = self.population[indices[1]]\n        x_r3 = self.population[indices[2]]\n\n        if self.successful_F:\n            F = np.random.choice(self.successful_F)\n        else:\n            F = self.F\n        \n        return x_r1 + F * (x_r2 - x_r3)\n\n    def crossover(self, x_i, v_i):\n        u_i = np.copy(x_i)\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() <= self.CR or j == j_rand:\n                u_i[j] = v_i[j]\n        return u_i\n\n    def repair(self, x, func):\n        return np.clip(x, func.bounds.lb, func.bounds.ub)\n\n    def adjust_population_size(self):\n        if len(self.best_fitness_history) > 2:\n            improvement = self.best_fitness_history[-2] - self.best_fitness_history[-1]\n            if improvement > 0:\n                self.pop_size = min(self.pop_size + 1, 2 * self.min_pop_size) # Increase population if there is improvement, but with limit\n            else:\n                self.pop_size = max(self.pop_size - 1, self.min_pop_size) # Decrease population if no improvement\n\n            # resize the population array by creating a new one and copying the old one\n            new_population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n            new_population[:min(self.pop_size,len(self.population))] = self.population[:min(self.pop_size,len(self.population))]\n            self.population = new_population\n            \n            new_fitness = np.array([func(x) for x in self.population])\n            new_fitness[:min(len(new_fitness), len(self.fitness))] = self.fitness[:min(len(new_fitness), len(self.fitness))]\n            self.fitness = new_fitness\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                v_i = self.mutate(self.population[i])\n\n                # Crossover\n                u_i = self.crossover(self.population[i], v_i)\n\n                # Repair\n                u_i = self.repair(u_i, func)\n\n                # Selection\n                f_u_i = func(u_i)\n                self.budget -= 1\n\n                if f_u_i < self.fitness[i]:\n                    self.successful_F.append(self.F)\n                    if len(self.successful_F) > 10:\n                        self.successful_F.pop(0)\n                    \n                    self.population[i] = u_i\n                    self.fitness[i] = f_u_i\n                    if f_u_i < self.f_opt:\n                        self.f_opt = f_u_i\n                        self.x_opt = u_i\n                else:\n                    self.archive.append(self.population[i])\n                    if len(self.archive) > 2 * self.pop_size:\n                        self.archive.pop(0)\n\n\n            self.best_fitness_history.append(np.min(self.fitness))\n            self.F *= self.F_decay # Decrease F gradually\n\n            self.adjust_population_size()\n\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 144, in evaluate\n    algorithm(f_new)\n  File \"<string>\", line 109, in __call__\n  File \"<string>\", line 62, in adjust_population_size\nNameError: name 'func' is not defined\n.", "error": "", "parent_ids": ["974eeffc-3bcf-499a-aa0d-add9e0340868"], "operator": null, "metadata": {}}
{"id": "d2098a56-4990-4102-9f8e-505e2d809319", "fitness": -Infinity, "name": "FuzzyDE", "description": "An Adaptive Differential Evolution strategy with a fuzzy logic controller to dynamically adjust both the mutation factor (F) and crossover rate (CR) based on population diversity and improvement rate, combined with a Cauchy mutation operator.", "code": "import numpy as np\nimport skfuzzy as fuzz\nfrom skfuzzy import control as ctrl\n\nclass FuzzyDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, initial_F=0.5, initial_CR=0.9, stagnation_limit=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = initial_F\n        self.CR = initial_CR\n        self.initial_F = initial_F\n        self.initial_CR = initial_CR\n        self.stagnation_limit = stagnation_limit\n        self.population = None\n        self.fitness = None\n        self.best_fitness_history = []\n        self.stagnation_counter = 0\n        self.improvement_rate_history = []\n\n        # Fuzzy Logic Controller Setup\n        self.setup_fuzzy_controller()\n\n    def setup_fuzzy_controller(self):\n        # Define fuzzy variables\n        diversity = ctrl.Antecedent(np.linspace(0, 1, 100), 'diversity')\n        improvement_rate = ctrl.Antecedent(np.linspace(0, 1, 100), 'improvement_rate')\n        mutation_factor = ctrl.Consequent(np.linspace(0.1, 1.0, 100), 'mutation_factor')\n        crossover_rate = ctrl.Consequent(np.linspace(0.1, 1.0, 100), 'crossover_rate')\n\n        # Define fuzzy membership functions\n        diversity['low'] = fuzz.trimf(diversity.universe, [0, 0, 0.5])\n        diversity['medium'] = fuzz.trimf(diversity.universe, [0, 0.5, 1])\n        diversity['high'] = fuzz.trimf(diversity.universe, [0.5, 1, 1])\n\n        improvement_rate['low'] = fuzz.trimf(improvement_rate.universe, [0, 0, 0.5])\n        improvement_rate['medium'] = fuzz.trimf(improvement_rate.universe, [0, 0.5, 1])\n        improvement_rate['high'] = fuzz.trimf(improvement_rate.universe, [0.5, 1, 1])\n\n        mutation_factor['low'] = fuzz.trimf(mutation_factor.universe, [0.1, 0.1, 0.5])\n        mutation_factor['medium'] = fuzz.trimf(mutation_factor.universe, [0.1, 0.5, 1])\n        mutation_factor['high'] = fuzz.trimf(mutation_factor.universe, [0.5, 1, 1])\n\n        crossover_rate['low'] = fuzz.trimf(crossover_rate.universe, [0.1, 0.1, 0.5])\n        crossover_rate['medium'] = fuzz.trimf(crossover_rate.universe, [0.1, 0.5, 1])\n        crossover_rate['high'] = fuzz.trimf(crossover_rate.universe, [0.5, 1, 1])\n\n\n        # Define fuzzy rules\n        rule1 = ctrl.Rule(diversity['low'] & improvement_rate['low'], (mutation_factor['high'], crossover_rate['low']))\n        rule2 = ctrl.Rule(diversity['low'] & improvement_rate['medium'], (mutation_factor['medium'], crossover_rate['medium']))\n        rule3 = ctrl.Rule(diversity['low'] & improvement_rate['high'], (mutation_factor['low'], crossover_rate['high']))\n\n        rule4 = ctrl.Rule(diversity['medium'] & improvement_rate['low'], (mutation_factor['high'], crossover_rate['medium']))\n        rule5 = ctrl.Rule(diversity['medium'] & improvement_rate['medium'], (mutation_factor['medium'], crossover_rate['medium']))\n        rule6 = ctrl.Rule(diversity['medium'] & improvement_rate['high'], (mutation_factor['low'], crossover_rate['high']))\n\n        rule7 = ctrl.Rule(diversity['high'] & improvement_rate['low'], (mutation_factor['high'], crossover_rate['low']))\n        rule8 = ctrl.Rule(diversity['high'] & improvement_rate['medium'], (mutation_factor['medium'], crossover_rate['medium']))\n        rule9 = ctrl.Rule(diversity['high'] & improvement_rate['high'], (mutation_factor['low'], crossover_rate['high']))\n\n        # Control System Creation and Simulation\n        self.tipping_ctrl = ctrl.ControlSystem([rule1, rule2, rule3, rule4, rule5, rule6, rule7, rule8, rule9])\n        self.tipping = ctrl.ControlSystemSimulation(self.tipping_ctrl)\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.best_fitness_history.append(np.min(self.fitness))\n        self.budget -= self.pop_size\n\n    def mutate(self, x_i):\n        indices = np.random.choice(self.pop_size, size=3, replace=False)\n        x_r1 = self.population[indices[0]]\n        x_r2 = self.population[indices[1]]\n        x_r3 = self.population[indices[2]]\n        \n        # Cauchy mutation\n        cauchy_mutation = np.random.standard_cauchy(size=self.dim)\n        v_i = x_r1 + self.F * (x_r2 - x_r3) + 0.01 * cauchy_mutation  # Scale Cauchy noise for fine-tuning\n\n        return v_i\n\n    def crossover(self, x_i, v_i):\n        u_i = np.copy(x_i)\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() <= self.CR or j == j_rand:\n                u_i[j] = v_i[j]\n        return u_i\n\n    def repair(self, x, func):\n        return np.clip(x, func.bounds.lb, func.bounds.ub)\n\n    def check_stagnation(self):\n        if len(self.best_fitness_history) > self.stagnation_limit:\n            if np.abs(self.best_fitness_history[-1] - np.mean(self.best_fitness_history[-self.stagnation_limit:])) < 1e-6:\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n\n        if self.stagnation_counter >= self.stagnation_limit:\n            return True\n        else:\n            return False\n\n    def restart(self, func):\n        \"\"\"Restart the population with new random individuals.\"\"\"\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.F = self.initial_F\n        self.CR = self.initial_CR\n        self.stagnation_counter = 0\n        self.best_fitness_history = [np.min(self.fitness)]\n        self.improvement_rate_history = []\n        self.budget -= self.pop_size\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            # Calculate Diversity\n            diversity = np.std(self.fitness) / (np.max(self.fitness) - np.min(self.fitness) + 1e-8)\n            diversity = np.clip(diversity, 0, 1)\n\n            # Calculate Improvement Rate\n            if len(self.best_fitness_history) > 1:\n                improvement_rate = (self.best_fitness_history[-2] - self.best_fitness_history[-1]) / (self.best_fitness_history[-2] + 1e-8)\n                improvement_rate = np.clip(improvement_rate, 0, 1)\n            else:\n                improvement_rate = 0.0\n            self.improvement_rate_history.append(improvement_rate)\n            if len(self.improvement_rate_history) > self.stagnation_limit:\n                self.improvement_rate_history.pop(0)\n\n\n            # Fuzzy Logic Controller Evaluation\n            self.tipping.input['diversity'] = diversity\n            self.tipping.input['improvement_rate'] = improvement_rate\n            self.tipping.compute()\n            \n            self.F = self.tipping.output['mutation_factor']\n            self.CR = self.tipping.output['crossover_rate']\n            \n            for i in range(self.pop_size):\n                # Mutation\n                v_i = self.mutate(self.population[i])\n\n                # Crossover\n                u_i = self.crossover(self.population[i], v_i)\n\n                # Repair\n                u_i = self.repair(u_i, func)\n\n                # Selection\n                f_u_i = func(u_i)\n                self.budget -= 1\n\n                if f_u_i < self.fitness[i]:\n                    self.population[i] = u_i\n                    self.fitness[i] = f_u_i\n                    if f_u_i < self.f_opt:\n                        self.f_opt = f_u_i\n                        self.x_opt = u_i\n\n            # Stagnation Check and Restart\n            if self.check_stagnation():\n                self.restart(func)\n\n            self.best_fitness_history.append(np.min(self.fitness))\n\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 111, in evaluate\n    exec(code, safe_globals, local_env)\n  File \"<string>\", line 2, in <module>\nModuleNotFoundError: No module named 'skfuzzy'\n.", "error": "", "parent_ids": ["974eeffc-3bcf-499a-aa0d-add9e0340868"], "operator": null, "metadata": {}}
{"id": "d6ed1879-5f43-4211-a97b-a12fd6bebd68", "fitness": 0.7234477466260746, "name": "AdaptiveDEHypersphere", "description": "An adaptive Differential Evolution strategy with a shrinking hypersphere search around the current best solution to intensify the search.", "code": "import numpy as np\n\nclass AdaptiveDEHypersphere:\n    def __init__(self, budget=10000, dim=10, pop_size=20, mutation_factor=0.5, crossover_rate=0.7, stagnation_threshold=500, sphere_reduction_factor=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.mutation_factor = mutation_factor\n        self.crossover_rate = crossover_rate\n        self.stagnation_threshold = stagnation_threshold\n        self.sphere_reduction_factor = sphere_reduction_factor\n        self.population = np.random.uniform(-5, 5, size=(pop_size, dim))\n        self.fitness = np.zeros(pop_size)\n        self.best_position = None\n        self.best_fitness = np.inf\n        self.eval_count = 0\n        self.stagnation_counter = 0\n        self.sphere_radius = 1.0  # Initial radius for hypersphere search\n\n    def __call__(self, func):\n        self.eval_count = 0\n        self.stagnation_counter = 0\n        self.sphere_radius = 1.0\n\n        # Initialize fitness values\n        for i in range(self.pop_size):\n            if self.eval_count < self.budget:\n                f = func(self.population[i])\n                self.eval_count += 1\n                self.fitness[i] = f\n                if f < self.best_fitness:\n                    self.best_fitness = f\n                    self.best_position = self.population[i].copy()\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Differential Evolution\n                r1, r2, r3 = np.random.choice(self.pop_size, 3, replace=False)\n                mutant_vector = self.population[r1] + self.mutation_factor * (self.population[r2] - self.population[r3])\n                mutant_vector = np.clip(mutant_vector, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial_vector = np.zeros(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.crossover_rate or j == np.random.randint(self.dim):\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = self.population[i][j]\n\n                # Evaluate trial vector\n                f_trial = func(trial_vector) if self.eval_count < self.budget else np.inf\n                if self.eval_count < self.budget:\n                    self.eval_count += 1\n                    if f_trial < self.fitness[i]:\n                        self.population[i] = trial_vector\n                        self.fitness[i] = f_trial\n                        if f_trial < self.best_fitness:\n                            self.best_fitness = f_trial\n                            self.best_position = self.population[i].copy()\n                            self.stagnation_counter = 0\n                    else:\n                        self.stagnation_counter += 1\n\n            # Stagnation check and Hypersphere Search\n            if self.stagnation_counter > self.stagnation_threshold:\n                # Generate new samples within a hypersphere around the best solution\n                for i in range(self.pop_size):\n                    if self.eval_count < self.budget:\n                        # Generate a random point within the hypersphere\n                        direction = np.random.randn(self.dim)\n                        direction /= np.linalg.norm(direction)  # Normalize to unit vector\n                        sample = self.best_position + direction * np.random.rand() * self.sphere_radius\n                        sample = np.clip(sample, func.bounds.lb, func.bounds.ub)\n                        \n                        f_sample = func(sample)\n                        self.eval_count += 1\n\n                        if f_sample < self.best_fitness:\n                            self.best_fitness = f_sample\n                            self.best_position = sample.copy()\n                            self.stagnation_counter = 0\n                        \n                        self.population[i] = sample\n                        self.fitness[i] = f_sample\n\n                self.sphere_radius *= self.sphere_reduction_factor #Reduce radius\n                self.stagnation_counter = 0  # Reset stagnation counter\n\n        return self.best_fitness, self.best_position", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveDEHypersphere scored 0.723 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["fefe3056-d5bf-4b76-b029-500b62bc8c84"], "operator": null, "metadata": {"aucs": [0.38402222265242203, 0.22146468810048026, 0.8431677510505217, 0.9224525480315773, 0.8854657689307797, 0.9112000000693646, 0.8066996703468656, 0.8401972710625812, 0.8774741008243396, 0.258172598710135, 0.901666564544269, 0.994749504835692, 0.43448613419826754, 0.8352606236921514, 0.9259850865372274, 0.9026622416090301, 0.8237939938359301, 0.9331141803738, 0.2287656074994434, 0.5381543756166134]}}
{"id": "b977f98d-f53a-457e-b8d8-df661d016ed9", "fitness": 0.42108530146922013, "name": "AdaptiveDECauchyRestart", "description": "A Differential Evolution strategy with a self-adaptive population size and a Cauchy mutation operator to enhance exploration, combined with a restart mechanism based on fitness entropy.", "code": "import numpy as np\n\nclass AdaptiveDECauchyRestart:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, CR=0.9, initial_F=0.5, pop_size_reduction_factor=0.9, stagnation_limit=100, cauchy_scale=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.initial_pop_size = initial_pop_size\n        self.CR = CR\n        self.F = initial_F\n        self.pop_size_reduction_factor = pop_size_reduction_factor\n        self.stagnation_limit = stagnation_limit\n        self.cauchy_scale = cauchy_scale\n        self.population = None\n        self.fitness = None\n        self.best_fitness_history = []\n        self.stagnation_counter = 0\n        self.entropy_history = []\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.best_fitness_history.append(np.min(self.fitness))\n        self.budget -= self.pop_size\n\n    def mutate(self, x_i):\n        indices = np.random.choice(self.pop_size, size=3, replace=False)\n        x_r1 = self.population[indices[0]]\n        x_r2 = self.population[indices[1]]\n        x_r3 = self.population[indices[2]]\n\n        # Cauchy mutation\n        cauchy_noise = np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n        return x_r1 + self.F * (x_r2 - x_r3) + cauchy_noise\n\n    def crossover(self, x_i, v_i):\n        u_i = np.copy(x_i)\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() <= self.CR or j == j_rand:\n                u_i[j] = v_i[j]\n        return u_i\n\n    def repair(self, x, func):\n        return np.clip(x, func.bounds.lb, func.bounds.ub)\n\n    def calculate_fitness_entropy(self):\n        \"\"\"Calculates the entropy of the fitness values.\"\"\"\n        fitness_min = np.min(self.fitness)\n        fitness_max = np.max(self.fitness)\n        if fitness_max == fitness_min:\n            return 0  # Avoid division by zero if all fitness values are the same\n\n        normalized_fitness = (self.fitness - fitness_min) / (fitness_max - fitness_min)\n        probabilities = normalized_fitness / np.sum(normalized_fitness)\n        probabilities = probabilities[probabilities > 0] # Avoid log(0)\n        entropy = -np.sum(probabilities * np.log(probabilities))\n        return entropy\n\n    def check_stagnation(self):\n        \"\"\"Checks for stagnation based on fitness entropy.\"\"\"\n        entropy = self.calculate_fitness_entropy()\n        self.entropy_history.append(entropy)\n\n        if len(self.entropy_history) > self.stagnation_limit:\n            entropy_diff = np.abs(self.entropy_history[-1] - np.mean(self.entropy_history[-self.stagnation_limit:]))\n            if  entropy_diff < 1e-6: #Stagnation if entropy doesn't change\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n\n        if self.stagnation_counter >= self.stagnation_limit:\n            return True\n        else:\n            return False\n        \n    def reduce_population_size(self):\n        \"\"\"Reduces the population size.\"\"\"\n        self.pop_size = int(self.pop_size * self.pop_size_reduction_factor)\n        self.pop_size = max(10, self.pop_size)  # Ensure a minimum population size\n        \n        #Select top individuals\n        top_indices = np.argsort(self.fitness)[:self.pop_size]\n        self.population = self.population[top_indices]\n        self.fitness = self.fitness[top_indices]\n        \n\n    def restart(self, func):\n        \"\"\"Restart the population with new random individuals.\"\"\"\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.initial_pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.pop_size = self.initial_pop_size\n        self.stagnation_counter = 0\n        self.best_fitness_history = [np.min(self.fitness)]\n        self.entropy_history = []\n        self.budget -= self.initial_pop_size\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                v_i = self.mutate(self.population[i])\n\n                # Crossover\n                u_i = self.crossover(self.population[i], v_i)\n\n                # Repair\n                u_i = self.repair(u_i, func)\n\n                # Selection\n                f_u_i = func(u_i)\n                self.budget -= 1\n\n                if f_u_i < self.fitness[i]:\n                    self.population[i] = u_i\n                    self.fitness[i] = f_u_i\n                    if f_u_i < self.f_opt:\n                        self.f_opt = f_u_i\n                        self.x_opt = u_i\n\n            # Stagnation Check and Restart\n            if self.check_stagnation():\n                 if self.pop_size > 10:\n                     self.reduce_population_size()\n                 else:\n                     self.restart(func)\n\n            self.best_fitness_history.append(np.min(self.fitness))\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveDECauchyRestart scored 0.421 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["974eeffc-3bcf-499a-aa0d-add9e0340868"], "operator": null, "metadata": {"aucs": [0.1791267124049386, 0.3312366726031454, 0.38371522501954003, 0.7972025574584234, 0.3246379788281365, 0.39811536160167194, 0.30531862181421565, 0.33977088441341174, 0.30734629122197, 0.25928966503751516, 0.4887537999273138, 0.9986634167941078, 0.28300817814078283, 0.32875889904826916, 0.7713621349555786, 0.3920860163570514, 0.3289493957508458, 0.4835850324444847, 0.22527269919057613, 0.495506486372424]}}
{"id": "691dd35d-22e9-4082-9d25-db1c22ae8480", "fitness": 0.773982447881189, "name": "AdaptiveDEArchive", "description": "An adaptive Differential Evolution strategy with a modified mutation operator utilizing both population-best and archive information to enhance exploration and exploitation, coupled with a dynamic adaptation of crossover rate.", "code": "import numpy as np\n\nclass AdaptiveDEArchive:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=20, CR_init=0.5, F=0.5, CR_adapt_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.CR = CR_init\n        self.F = F\n        self.CR_adapt_rate = CR_adapt_rate\n        self.population = None\n        self.fitness = None\n        self.archive = []\n        self.archive_fitness = []\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n    def update_archive(self):\n        \"\"\"Update the archive with diverse and promising solutions.\"\"\"\n        for i in range(self.pop_size):\n            if len(self.archive) < self.archive_size:\n                self.archive.append(self.population[i])\n                self.archive_fitness.append(self.fitness[i])\n            else:\n                worst_archive_index = np.argmax(self.archive_fitness)\n                if self.fitness[i] < self.archive_fitness[worst_archive_index]:\n                    self.archive[worst_archive_index] = self.population[i]\n                    self.archive_fitness[worst_archive_index] = self.fitness[i]\n\n    def mutate(self, x_i, best_x):\n        \"\"\"Modified mutation operator using population best and archive.\"\"\"\n        indices = np.random.choice(self.pop_size, size=2, replace=False)\n        x_r1 = self.population[indices[0]]\n        x_r2 = self.population[indices[1]]\n\n        if len(self.archive) > 0:\n            archive_index = np.random.randint(len(self.archive))\n            x_r3 = self.archive[archive_index]\n            v_i = x_i + self.F * (best_x - x_i) + self.F * (x_r1 - x_r2) + self.F * (x_r3 - x_i)\n        else:\n             v_i = x_i + self.F * (best_x - x_i) + self.F * (x_r1 - x_r2)\n\n        return v_i\n\n    def crossover(self, x_i, v_i):\n        \"\"\"Adaptive Crossover.\"\"\"\n        u_i = np.copy(x_i)\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() <= self.CR or j == j_rand:\n                u_i[j] = v_i[j]\n        return u_i\n\n    def repair(self, x, func):\n        return np.clip(x, func.bounds.lb, func.bounds.ub)\n\n    def adjust_crossover_rate(self):\n        \"\"\"Adjust CR based on the success rate of previous generations.\"\"\"\n        self.CR = np.clip(self.CR + self.CR_adapt_rate * (np.random.rand() - 0.5), 0.1, 0.9)\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            best_index = np.argmin(self.fitness)\n            best_x = self.population[best_index]\n\n            self.adjust_crossover_rate()\n\n            for i in range(self.pop_size):\n                # Mutation\n                v_i = self.mutate(self.population[i], best_x)\n\n                # Crossover\n                u_i = self.crossover(self.population[i], v_i)\n\n                # Repair\n                u_i = self.repair(u_i, func)\n\n                # Selection\n                f_u_i = func(u_i)\n                self.budget -= 1\n\n                if f_u_i < self.fitness[i]:\n                    self.population[i] = u_i\n                    self.fitness[i] = f_u_i\n\n                    if f_u_i < self.f_opt:\n                        self.f_opt = f_u_i\n                        self.x_opt = u_i\n\n            self.update_archive()\n\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveDEArchive scored 0.774 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5bc8ca0b-b981-465d-b471-65a7ddd94c89"], "operator": null, "metadata": {"aucs": [0.4424617400872527, 0.6842842052789162, 0.8400410199912165, 0.9483117270055592, 0.7559583467240213, 0.8875782042465254, 0.7979027288957256, 0.8244529032024674, 0.8855989926322653, 0.8568095609755758, 0.9069460558565039, 0.9964270515850396, 0.4914611139086802, 0.8572096309228918, 0.9242186867591958, 0.8960548315575139, 0.7690289145801243, 0.9181349925833668, 0.271048406756599, 0.5257198440743407]}}
{"id": "6f30cdbf-6eb3-4928-9e92-5730073ffbf5", "fitness": 0.0, "name": "AdaptiveDERejuvenation", "description": "An adaptive Differential Evolution strategy with a modified Cauchy mutation, a dynamic F adaptation based on fitness improvement, and a population rejuvenation mechanism based on the worst individual replacement to maintain diversity.", "code": "import numpy as np\n\nclass AdaptiveDERejuvenation:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, CR=0.9, initial_F=0.5, rejuvenation_rate=0.05, cauchy_scale=0.1, F_adapt_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.CR = CR\n        self.F = initial_F\n        self.rejuvenation_rate = rejuvenation_rate\n        self.cauchy_scale = cauchy_scale\n        self.F_adapt_rate = F_adapt_rate\n        self.population = None\n        self.fitness = None\n        self.best_fitness_history = []\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.best_fitness_history.append(np.min(self.fitness))\n        self.budget -= self.pop_size\n\n    def mutate(self, x_i):\n        indices = np.random.choice(self.pop_size, size=3, replace=False)\n        x_r1 = self.population[indices[0]]\n        x_r2 = self.population[indices[1]]\n        x_r3 = self.population[indices[2]]\n\n        # Modified Cauchy mutation: adaptively scale Cauchy noise\n        cauchy_noise = np.random.standard_cauchy(size=self.dim) * self.cauchy_scale * self.F\n        return x_r1 + self.F * (x_r2 - x_r3) + cauchy_noise\n\n    def crossover(self, x_i, v_i):\n        u_i = np.copy(x_i)\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() <= self.CR or j == j_rand:\n                u_i[j] = v_i[j]\n        return u_i\n\n    def repair(self, x, func):\n        return np.clip(x, func.bounds.lb, func.bounds.ub)\n    \n    def adapt_F(self, f_u_i, f_x_i):\n        \"\"\"Adapt F based on the fitness improvement.\"\"\"\n        if f_u_i < f_x_i:\n            self.F = max(0.1, self.F * (1 - self.F_adapt_rate))  # Reduce F if improvement\n        else:\n            self.F = min(0.9, self.F * (1 + self.F_adapt_rate))  # Increase F if no improvement\n    \n\n    def rejuvenate_population(self, func):\n        \"\"\"Replaces the worst individuals with new random ones.\"\"\"\n        num_rejuvenate = int(self.rejuvenation_rate * self.pop_size)\n        if num_rejuvenate > 0:\n            worst_indices = np.argsort(self.fitness)[-num_rejuvenate:]  # Indices of worst individuals\n            for i in worst_indices:\n                self.population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                self.fitness[i] = func(self.population[i])\n                self.budget -= 1\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                v_i = self.mutate(self.population[i])\n\n                # Crossover\n                u_i = self.crossover(self.population[i], v_i)\n\n                # Repair\n                u_i = self.repair(u_i, func)\n\n                # Selection\n                f_u_i = func(u_i)\n                self.budget -= 1\n\n                if f_u_i < self.fitness[i]:\n                    self.population[i] = u_i\n                    self.fitness[i] = f_u_i\n                    if f_u_i < self.f_opt:\n                        self.f_opt = f_u_i\n                        self.x_opt = u_i\n                    self.adapt_F(f_u_i, self.fitness[i]) #Adapt F value if improved\n\n            self.rejuvenate_population(func)  # Rejuvenate population\n\n            self.best_fitness_history.append(np.min(self.fitness))\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveDERejuvenation scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b977f98d-f53a-457e-b8d8-df661d016ed9"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "6f30cdbf-6eb3-4928-9e92-5730073ffbf5", "fitness": 0.0, "name": "AdaptiveDERejuvenation", "description": "An adaptive Differential Evolution strategy with a modified Cauchy mutation, a dynamic F adaptation based on fitness improvement, and a population rejuvenation mechanism based on the worst individual replacement to maintain diversity.", "code": "import numpy as np\n\nclass AdaptiveDERejuvenation:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, CR=0.9, initial_F=0.5, rejuvenation_rate=0.05, cauchy_scale=0.1, F_adapt_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.CR = CR\n        self.F = initial_F\n        self.rejuvenation_rate = rejuvenation_rate\n        self.cauchy_scale = cauchy_scale\n        self.F_adapt_rate = F_adapt_rate\n        self.population = None\n        self.fitness = None\n        self.best_fitness_history = []\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.best_fitness_history.append(np.min(self.fitness))\n        self.budget -= self.pop_size\n\n    def mutate(self, x_i):\n        indices = np.random.choice(self.pop_size, size=3, replace=False)\n        x_r1 = self.population[indices[0]]\n        x_r2 = self.population[indices[1]]\n        x_r3 = self.population[indices[2]]\n\n        # Modified Cauchy mutation: adaptively scale Cauchy noise\n        cauchy_noise = np.random.standard_cauchy(size=self.dim) * self.cauchy_scale * self.F\n        return x_r1 + self.F * (x_r2 - x_r3) + cauchy_noise\n\n    def crossover(self, x_i, v_i):\n        u_i = np.copy(x_i)\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() <= self.CR or j == j_rand:\n                u_i[j] = v_i[j]\n        return u_i\n\n    def repair(self, x, func):\n        return np.clip(x, func.bounds.lb, func.bounds.ub)\n    \n    def adapt_F(self, f_u_i, f_x_i):\n        \"\"\"Adapt F based on the fitness improvement.\"\"\"\n        if f_u_i < f_x_i:\n            self.F = max(0.1, self.F * (1 - self.F_adapt_rate))  # Reduce F if improvement\n        else:\n            self.F = min(0.9, self.F * (1 + self.F_adapt_rate))  # Increase F if no improvement\n    \n\n    def rejuvenate_population(self, func):\n        \"\"\"Replaces the worst individuals with new random ones.\"\"\"\n        num_rejuvenate = int(self.rejuvenation_rate * self.pop_size)\n        if num_rejuvenate > 0:\n            worst_indices = np.argsort(self.fitness)[-num_rejuvenate:]  # Indices of worst individuals\n            for i in worst_indices:\n                self.population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                self.fitness[i] = func(self.population[i])\n                self.budget -= 1\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                v_i = self.mutate(self.population[i])\n\n                # Crossover\n                u_i = self.crossover(self.population[i], v_i)\n\n                # Repair\n                u_i = self.repair(u_i, func)\n\n                # Selection\n                f_u_i = func(u_i)\n                self.budget -= 1\n\n                if f_u_i < self.fitness[i]:\n                    self.population[i] = u_i\n                    self.fitness[i] = f_u_i\n                    if f_u_i < self.f_opt:\n                        self.f_opt = f_u_i\n                        self.x_opt = u_i\n                    self.adapt_F(f_u_i, self.fitness[i]) #Adapt F value if improved\n\n            self.rejuvenate_population(func)  # Rejuvenate population\n\n            self.best_fitness_history.append(np.min(self.fitness))\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveDERejuvenation scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b977f98d-f53a-457e-b8d8-df661d016ed9"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "6f30cdbf-6eb3-4928-9e92-5730073ffbf5", "fitness": 0.0, "name": "AdaptiveDERejuvenation", "description": "An adaptive Differential Evolution strategy with a modified Cauchy mutation, a dynamic F adaptation based on fitness improvement, and a population rejuvenation mechanism based on the worst individual replacement to maintain diversity.", "code": "import numpy as np\n\nclass AdaptiveDERejuvenation:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, CR=0.9, initial_F=0.5, rejuvenation_rate=0.05, cauchy_scale=0.1, F_adapt_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.CR = CR\n        self.F = initial_F\n        self.rejuvenation_rate = rejuvenation_rate\n        self.cauchy_scale = cauchy_scale\n        self.F_adapt_rate = F_adapt_rate\n        self.population = None\n        self.fitness = None\n        self.best_fitness_history = []\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.best_fitness_history.append(np.min(self.fitness))\n        self.budget -= self.pop_size\n\n    def mutate(self, x_i):\n        indices = np.random.choice(self.pop_size, size=3, replace=False)\n        x_r1 = self.population[indices[0]]\n        x_r2 = self.population[indices[1]]\n        x_r3 = self.population[indices[2]]\n\n        # Modified Cauchy mutation: adaptively scale Cauchy noise\n        cauchy_noise = np.random.standard_cauchy(size=self.dim) * self.cauchy_scale * self.F\n        return x_r1 + self.F * (x_r2 - x_r3) + cauchy_noise\n\n    def crossover(self, x_i, v_i):\n        u_i = np.copy(x_i)\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() <= self.CR or j == j_rand:\n                u_i[j] = v_i[j]\n        return u_i\n\n    def repair(self, x, func):\n        return np.clip(x, func.bounds.lb, func.bounds.ub)\n    \n    def adapt_F(self, f_u_i, f_x_i):\n        \"\"\"Adapt F based on the fitness improvement.\"\"\"\n        if f_u_i < f_x_i:\n            self.F = max(0.1, self.F * (1 - self.F_adapt_rate))  # Reduce F if improvement\n        else:\n            self.F = min(0.9, self.F * (1 + self.F_adapt_rate))  # Increase F if no improvement\n    \n\n    def rejuvenate_population(self, func):\n        \"\"\"Replaces the worst individuals with new random ones.\"\"\"\n        num_rejuvenate = int(self.rejuvenation_rate * self.pop_size)\n        if num_rejuvenate > 0:\n            worst_indices = np.argsort(self.fitness)[-num_rejuvenate:]  # Indices of worst individuals\n            for i in worst_indices:\n                self.population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                self.fitness[i] = func(self.population[i])\n                self.budget -= 1\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                v_i = self.mutate(self.population[i])\n\n                # Crossover\n                u_i = self.crossover(self.population[i], v_i)\n\n                # Repair\n                u_i = self.repair(u_i, func)\n\n                # Selection\n                f_u_i = func(u_i)\n                self.budget -= 1\n\n                if f_u_i < self.fitness[i]:\n                    self.population[i] = u_i\n                    self.fitness[i] = f_u_i\n                    if f_u_i < self.f_opt:\n                        self.f_opt = f_u_i\n                        self.x_opt = u_i\n                    self.adapt_F(f_u_i, self.fitness[i]) #Adapt F value if improved\n\n            self.rejuvenate_population(func)  # Rejuvenate population\n\n            self.best_fitness_history.append(np.min(self.fitness))\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveDERejuvenation scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b977f98d-f53a-457e-b8d8-df661d016ed9"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "6f30cdbf-6eb3-4928-9e92-5730073ffbf5", "fitness": 0.0, "name": "AdaptiveDERejuvenation", "description": "An adaptive Differential Evolution strategy with a modified Cauchy mutation, a dynamic F adaptation based on fitness improvement, and a population rejuvenation mechanism based on the worst individual replacement to maintain diversity.", "code": "import numpy as np\n\nclass AdaptiveDERejuvenation:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, CR=0.9, initial_F=0.5, rejuvenation_rate=0.05, cauchy_scale=0.1, F_adapt_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.CR = CR\n        self.F = initial_F\n        self.rejuvenation_rate = rejuvenation_rate\n        self.cauchy_scale = cauchy_scale\n        self.F_adapt_rate = F_adapt_rate\n        self.population = None\n        self.fitness = None\n        self.best_fitness_history = []\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.best_fitness_history.append(np.min(self.fitness))\n        self.budget -= self.pop_size\n\n    def mutate(self, x_i):\n        indices = np.random.choice(self.pop_size, size=3, replace=False)\n        x_r1 = self.population[indices[0]]\n        x_r2 = self.population[indices[1]]\n        x_r3 = self.population[indices[2]]\n\n        # Modified Cauchy mutation: adaptively scale Cauchy noise\n        cauchy_noise = np.random.standard_cauchy(size=self.dim) * self.cauchy_scale * self.F\n        return x_r1 + self.F * (x_r2 - x_r3) + cauchy_noise\n\n    def crossover(self, x_i, v_i):\n        u_i = np.copy(x_i)\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() <= self.CR or j == j_rand:\n                u_i[j] = v_i[j]\n        return u_i\n\n    def repair(self, x, func):\n        return np.clip(x, func.bounds.lb, func.bounds.ub)\n    \n    def adapt_F(self, f_u_i, f_x_i):\n        \"\"\"Adapt F based on the fitness improvement.\"\"\"\n        if f_u_i < f_x_i:\n            self.F = max(0.1, self.F * (1 - self.F_adapt_rate))  # Reduce F if improvement\n        else:\n            self.F = min(0.9, self.F * (1 + self.F_adapt_rate))  # Increase F if no improvement\n    \n\n    def rejuvenate_population(self, func):\n        \"\"\"Replaces the worst individuals with new random ones.\"\"\"\n        num_rejuvenate = int(self.rejuvenation_rate * self.pop_size)\n        if num_rejuvenate > 0:\n            worst_indices = np.argsort(self.fitness)[-num_rejuvenate:]  # Indices of worst individuals\n            for i in worst_indices:\n                self.population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                self.fitness[i] = func(self.population[i])\n                self.budget -= 1\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                v_i = self.mutate(self.population[i])\n\n                # Crossover\n                u_i = self.crossover(self.population[i], v_i)\n\n                # Repair\n                u_i = self.repair(u_i, func)\n\n                # Selection\n                f_u_i = func(u_i)\n                self.budget -= 1\n\n                if f_u_i < self.fitness[i]:\n                    self.population[i] = u_i\n                    self.fitness[i] = f_u_i\n                    if f_u_i < self.f_opt:\n                        self.f_opt = f_u_i\n                        self.x_opt = u_i\n                    self.adapt_F(f_u_i, self.fitness[i]) #Adapt F value if improved\n\n            self.rejuvenate_population(func)  # Rejuvenate population\n\n            self.best_fitness_history.append(np.min(self.fitness))\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveDERejuvenation scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b977f98d-f53a-457e-b8d8-df661d016ed9"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "6f30cdbf-6eb3-4928-9e92-5730073ffbf5", "fitness": 0.0, "name": "AdaptiveDERejuvenation", "description": "An adaptive Differential Evolution strategy with a modified Cauchy mutation, a dynamic F adaptation based on fitness improvement, and a population rejuvenation mechanism based on the worst individual replacement to maintain diversity.", "code": "import numpy as np\n\nclass AdaptiveDERejuvenation:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, CR=0.9, initial_F=0.5, rejuvenation_rate=0.05, cauchy_scale=0.1, F_adapt_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.CR = CR\n        self.F = initial_F\n        self.rejuvenation_rate = rejuvenation_rate\n        self.cauchy_scale = cauchy_scale\n        self.F_adapt_rate = F_adapt_rate\n        self.population = None\n        self.fitness = None\n        self.best_fitness_history = []\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.best_fitness_history.append(np.min(self.fitness))\n        self.budget -= self.pop_size\n\n    def mutate(self, x_i):\n        indices = np.random.choice(self.pop_size, size=3, replace=False)\n        x_r1 = self.population[indices[0]]\n        x_r2 = self.population[indices[1]]\n        x_r3 = self.population[indices[2]]\n\n        # Modified Cauchy mutation: adaptively scale Cauchy noise\n        cauchy_noise = np.random.standard_cauchy(size=self.dim) * self.cauchy_scale * self.F\n        return x_r1 + self.F * (x_r2 - x_r3) + cauchy_noise\n\n    def crossover(self, x_i, v_i):\n        u_i = np.copy(x_i)\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() <= self.CR or j == j_rand:\n                u_i[j] = v_i[j]\n        return u_i\n\n    def repair(self, x, func):\n        return np.clip(x, func.bounds.lb, func.bounds.ub)\n    \n    def adapt_F(self, f_u_i, f_x_i):\n        \"\"\"Adapt F based on the fitness improvement.\"\"\"\n        if f_u_i < f_x_i:\n            self.F = max(0.1, self.F * (1 - self.F_adapt_rate))  # Reduce F if improvement\n        else:\n            self.F = min(0.9, self.F * (1 + self.F_adapt_rate))  # Increase F if no improvement\n    \n\n    def rejuvenate_population(self, func):\n        \"\"\"Replaces the worst individuals with new random ones.\"\"\"\n        num_rejuvenate = int(self.rejuvenation_rate * self.pop_size)\n        if num_rejuvenate > 0:\n            worst_indices = np.argsort(self.fitness)[-num_rejuvenate:]  # Indices of worst individuals\n            for i in worst_indices:\n                self.population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                self.fitness[i] = func(self.population[i])\n                self.budget -= 1\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                v_i = self.mutate(self.population[i])\n\n                # Crossover\n                u_i = self.crossover(self.population[i], v_i)\n\n                # Repair\n                u_i = self.repair(u_i, func)\n\n                # Selection\n                f_u_i = func(u_i)\n                self.budget -= 1\n\n                if f_u_i < self.fitness[i]:\n                    self.population[i] = u_i\n                    self.fitness[i] = f_u_i\n                    if f_u_i < self.f_opt:\n                        self.f_opt = f_u_i\n                        self.x_opt = u_i\n                    self.adapt_F(f_u_i, self.fitness[i]) #Adapt F value if improved\n\n            self.rejuvenate_population(func)  # Rejuvenate population\n\n            self.best_fitness_history.append(np.min(self.fitness))\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveDERejuvenation scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b977f98d-f53a-457e-b8d8-df661d016ed9"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "6f30cdbf-6eb3-4928-9e92-5730073ffbf5", "fitness": 0.0, "name": "AdaptiveDERejuvenation", "description": "An adaptive Differential Evolution strategy with a modified Cauchy mutation, a dynamic F adaptation based on fitness improvement, and a population rejuvenation mechanism based on the worst individual replacement to maintain diversity.", "code": "import numpy as np\n\nclass AdaptiveDERejuvenation:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, CR=0.9, initial_F=0.5, rejuvenation_rate=0.05, cauchy_scale=0.1, F_adapt_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.CR = CR\n        self.F = initial_F\n        self.rejuvenation_rate = rejuvenation_rate\n        self.cauchy_scale = cauchy_scale\n        self.F_adapt_rate = F_adapt_rate\n        self.population = None\n        self.fitness = None\n        self.best_fitness_history = []\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.best_fitness_history.append(np.min(self.fitness))\n        self.budget -= self.pop_size\n\n    def mutate(self, x_i):\n        indices = np.random.choice(self.pop_size, size=3, replace=False)\n        x_r1 = self.population[indices[0]]\n        x_r2 = self.population[indices[1]]\n        x_r3 = self.population[indices[2]]\n\n        # Modified Cauchy mutation: adaptively scale Cauchy noise\n        cauchy_noise = np.random.standard_cauchy(size=self.dim) * self.cauchy_scale * self.F\n        return x_r1 + self.F * (x_r2 - x_r3) + cauchy_noise\n\n    def crossover(self, x_i, v_i):\n        u_i = np.copy(x_i)\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() <= self.CR or j == j_rand:\n                u_i[j] = v_i[j]\n        return u_i\n\n    def repair(self, x, func):\n        return np.clip(x, func.bounds.lb, func.bounds.ub)\n    \n    def adapt_F(self, f_u_i, f_x_i):\n        \"\"\"Adapt F based on the fitness improvement.\"\"\"\n        if f_u_i < f_x_i:\n            self.F = max(0.1, self.F * (1 - self.F_adapt_rate))  # Reduce F if improvement\n        else:\n            self.F = min(0.9, self.F * (1 + self.F_adapt_rate))  # Increase F if no improvement\n    \n\n    def rejuvenate_population(self, func):\n        \"\"\"Replaces the worst individuals with new random ones.\"\"\"\n        num_rejuvenate = int(self.rejuvenation_rate * self.pop_size)\n        if num_rejuvenate > 0:\n            worst_indices = np.argsort(self.fitness)[-num_rejuvenate:]  # Indices of worst individuals\n            for i in worst_indices:\n                self.population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                self.fitness[i] = func(self.population[i])\n                self.budget -= 1\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                v_i = self.mutate(self.population[i])\n\n                # Crossover\n                u_i = self.crossover(self.population[i], v_i)\n\n                # Repair\n                u_i = self.repair(u_i, func)\n\n                # Selection\n                f_u_i = func(u_i)\n                self.budget -= 1\n\n                if f_u_i < self.fitness[i]:\n                    self.population[i] = u_i\n                    self.fitness[i] = f_u_i\n                    if f_u_i < self.f_opt:\n                        self.f_opt = f_u_i\n                        self.x_opt = u_i\n                    self.adapt_F(f_u_i, self.fitness[i]) #Adapt F value if improved\n\n            self.rejuvenate_population(func)  # Rejuvenate population\n\n            self.best_fitness_history.append(np.min(self.fitness))\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveDERejuvenation scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b977f98d-f53a-457e-b8d8-df661d016ed9"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "6f30cdbf-6eb3-4928-9e92-5730073ffbf5", "fitness": 0.0, "name": "AdaptiveDERejuvenation", "description": "An adaptive Differential Evolution strategy with a modified Cauchy mutation, a dynamic F adaptation based on fitness improvement, and a population rejuvenation mechanism based on the worst individual replacement to maintain diversity.", "code": "import numpy as np\n\nclass AdaptiveDERejuvenation:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, CR=0.9, initial_F=0.5, rejuvenation_rate=0.05, cauchy_scale=0.1, F_adapt_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.CR = CR\n        self.F = initial_F\n        self.rejuvenation_rate = rejuvenation_rate\n        self.cauchy_scale = cauchy_scale\n        self.F_adapt_rate = F_adapt_rate\n        self.population = None\n        self.fitness = None\n        self.best_fitness_history = []\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.best_fitness_history.append(np.min(self.fitness))\n        self.budget -= self.pop_size\n\n    def mutate(self, x_i):\n        indices = np.random.choice(self.pop_size, size=3, replace=False)\n        x_r1 = self.population[indices[0]]\n        x_r2 = self.population[indices[1]]\n        x_r3 = self.population[indices[2]]\n\n        # Modified Cauchy mutation: adaptively scale Cauchy noise\n        cauchy_noise = np.random.standard_cauchy(size=self.dim) * self.cauchy_scale * self.F\n        return x_r1 + self.F * (x_r2 - x_r3) + cauchy_noise\n\n    def crossover(self, x_i, v_i):\n        u_i = np.copy(x_i)\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() <= self.CR or j == j_rand:\n                u_i[j] = v_i[j]\n        return u_i\n\n    def repair(self, x, func):\n        return np.clip(x, func.bounds.lb, func.bounds.ub)\n    \n    def adapt_F(self, f_u_i, f_x_i):\n        \"\"\"Adapt F based on the fitness improvement.\"\"\"\n        if f_u_i < f_x_i:\n            self.F = max(0.1, self.F * (1 - self.F_adapt_rate))  # Reduce F if improvement\n        else:\n            self.F = min(0.9, self.F * (1 + self.F_adapt_rate))  # Increase F if no improvement\n    \n\n    def rejuvenate_population(self, func):\n        \"\"\"Replaces the worst individuals with new random ones.\"\"\"\n        num_rejuvenate = int(self.rejuvenation_rate * self.pop_size)\n        if num_rejuvenate > 0:\n            worst_indices = np.argsort(self.fitness)[-num_rejuvenate:]  # Indices of worst individuals\n            for i in worst_indices:\n                self.population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                self.fitness[i] = func(self.population[i])\n                self.budget -= 1\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                v_i = self.mutate(self.population[i])\n\n                # Crossover\n                u_i = self.crossover(self.population[i], v_i)\n\n                # Repair\n                u_i = self.repair(u_i, func)\n\n                # Selection\n                f_u_i = func(u_i)\n                self.budget -= 1\n\n                if f_u_i < self.fitness[i]:\n                    self.population[i] = u_i\n                    self.fitness[i] = f_u_i\n                    if f_u_i < self.f_opt:\n                        self.f_opt = f_u_i\n                        self.x_opt = u_i\n                    self.adapt_F(f_u_i, self.fitness[i]) #Adapt F value if improved\n\n            self.rejuvenate_population(func)  # Rejuvenate population\n\n            self.best_fitness_history.append(np.min(self.fitness))\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveDERejuvenation scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b977f98d-f53a-457e-b8d8-df661d016ed9"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "6f30cdbf-6eb3-4928-9e92-5730073ffbf5", "fitness": 0.0, "name": "AdaptiveDERejuvenation", "description": "An adaptive Differential Evolution strategy with a modified Cauchy mutation, a dynamic F adaptation based on fitness improvement, and a population rejuvenation mechanism based on the worst individual replacement to maintain diversity.", "code": "import numpy as np\n\nclass AdaptiveDERejuvenation:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, CR=0.9, initial_F=0.5, rejuvenation_rate=0.05, cauchy_scale=0.1, F_adapt_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.CR = CR\n        self.F = initial_F\n        self.rejuvenation_rate = rejuvenation_rate\n        self.cauchy_scale = cauchy_scale\n        self.F_adapt_rate = F_adapt_rate\n        self.population = None\n        self.fitness = None\n        self.best_fitness_history = []\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.best_fitness_history.append(np.min(self.fitness))\n        self.budget -= self.pop_size\n\n    def mutate(self, x_i):\n        indices = np.random.choice(self.pop_size, size=3, replace=False)\n        x_r1 = self.population[indices[0]]\n        x_r2 = self.population[indices[1]]\n        x_r3 = self.population[indices[2]]\n\n        # Modified Cauchy mutation: adaptively scale Cauchy noise\n        cauchy_noise = np.random.standard_cauchy(size=self.dim) * self.cauchy_scale * self.F\n        return x_r1 + self.F * (x_r2 - x_r3) + cauchy_noise\n\n    def crossover(self, x_i, v_i):\n        u_i = np.copy(x_i)\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() <= self.CR or j == j_rand:\n                u_i[j] = v_i[j]\n        return u_i\n\n    def repair(self, x, func):\n        return np.clip(x, func.bounds.lb, func.bounds.ub)\n    \n    def adapt_F(self, f_u_i, f_x_i):\n        \"\"\"Adapt F based on the fitness improvement.\"\"\"\n        if f_u_i < f_x_i:\n            self.F = max(0.1, self.F * (1 - self.F_adapt_rate))  # Reduce F if improvement\n        else:\n            self.F = min(0.9, self.F * (1 + self.F_adapt_rate))  # Increase F if no improvement\n    \n\n    def rejuvenate_population(self, func):\n        \"\"\"Replaces the worst individuals with new random ones.\"\"\"\n        num_rejuvenate = int(self.rejuvenation_rate * self.pop_size)\n        if num_rejuvenate > 0:\n            worst_indices = np.argsort(self.fitness)[-num_rejuvenate:]  # Indices of worst individuals\n            for i in worst_indices:\n                self.population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                self.fitness[i] = func(self.population[i])\n                self.budget -= 1\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                v_i = self.mutate(self.population[i])\n\n                # Crossover\n                u_i = self.crossover(self.population[i], v_i)\n\n                # Repair\n                u_i = self.repair(u_i, func)\n\n                # Selection\n                f_u_i = func(u_i)\n                self.budget -= 1\n\n                if f_u_i < self.fitness[i]:\n                    self.population[i] = u_i\n                    self.fitness[i] = f_u_i\n                    if f_u_i < self.f_opt:\n                        self.f_opt = f_u_i\n                        self.x_opt = u_i\n                    self.adapt_F(f_u_i, self.fitness[i]) #Adapt F value if improved\n\n            self.rejuvenate_population(func)  # Rejuvenate population\n\n            self.best_fitness_history.append(np.min(self.fitness))\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveDERejuvenation scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b977f98d-f53a-457e-b8d8-df661d016ed9"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "6f30cdbf-6eb3-4928-9e92-5730073ffbf5", "fitness": 0.0, "name": "AdaptiveDERejuvenation", "description": "An adaptive Differential Evolution strategy with a modified Cauchy mutation, a dynamic F adaptation based on fitness improvement, and a population rejuvenation mechanism based on the worst individual replacement to maintain diversity.", "code": "import numpy as np\n\nclass AdaptiveDERejuvenation:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, CR=0.9, initial_F=0.5, rejuvenation_rate=0.05, cauchy_scale=0.1, F_adapt_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.CR = CR\n        self.F = initial_F\n        self.rejuvenation_rate = rejuvenation_rate\n        self.cauchy_scale = cauchy_scale\n        self.F_adapt_rate = F_adapt_rate\n        self.population = None\n        self.fitness = None\n        self.best_fitness_history = []\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.best_fitness_history.append(np.min(self.fitness))\n        self.budget -= self.pop_size\n\n    def mutate(self, x_i):\n        indices = np.random.choice(self.pop_size, size=3, replace=False)\n        x_r1 = self.population[indices[0]]\n        x_r2 = self.population[indices[1]]\n        x_r3 = self.population[indices[2]]\n\n        # Modified Cauchy mutation: adaptively scale Cauchy noise\n        cauchy_noise = np.random.standard_cauchy(size=self.dim) * self.cauchy_scale * self.F\n        return x_r1 + self.F * (x_r2 - x_r3) + cauchy_noise\n\n    def crossover(self, x_i, v_i):\n        u_i = np.copy(x_i)\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() <= self.CR or j == j_rand:\n                u_i[j] = v_i[j]\n        return u_i\n\n    def repair(self, x, func):\n        return np.clip(x, func.bounds.lb, func.bounds.ub)\n    \n    def adapt_F(self, f_u_i, f_x_i):\n        \"\"\"Adapt F based on the fitness improvement.\"\"\"\n        if f_u_i < f_x_i:\n            self.F = max(0.1, self.F * (1 - self.F_adapt_rate))  # Reduce F if improvement\n        else:\n            self.F = min(0.9, self.F * (1 + self.F_adapt_rate))  # Increase F if no improvement\n    \n\n    def rejuvenate_population(self, func):\n        \"\"\"Replaces the worst individuals with new random ones.\"\"\"\n        num_rejuvenate = int(self.rejuvenation_rate * self.pop_size)\n        if num_rejuvenate > 0:\n            worst_indices = np.argsort(self.fitness)[-num_rejuvenate:]  # Indices of worst individuals\n            for i in worst_indices:\n                self.population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                self.fitness[i] = func(self.population[i])\n                self.budget -= 1\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                v_i = self.mutate(self.population[i])\n\n                # Crossover\n                u_i = self.crossover(self.population[i], v_i)\n\n                # Repair\n                u_i = self.repair(u_i, func)\n\n                # Selection\n                f_u_i = func(u_i)\n                self.budget -= 1\n\n                if f_u_i < self.fitness[i]:\n                    self.population[i] = u_i\n                    self.fitness[i] = f_u_i\n                    if f_u_i < self.f_opt:\n                        self.f_opt = f_u_i\n                        self.x_opt = u_i\n                    self.adapt_F(f_u_i, self.fitness[i]) #Adapt F value if improved\n\n            self.rejuvenate_population(func)  # Rejuvenate population\n\n            self.best_fitness_history.append(np.min(self.fitness))\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveDERejuvenation scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b977f98d-f53a-457e-b8d8-df661d016ed9"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "d4653b66-15cf-4165-bf1f-24fa7f1cd0bc", "fitness": 0.0, "name": "AdaptiveDESelfPopCauchyLocal", "description": "A Differential Evolution strategy with a self-adaptive population size based on fitness improvement, enhanced exploration using a Cauchy mutation, and a local search around the best solution.", "code": "import numpy as np\n\nclass AdaptiveDESelfPopCauchyLocal:\n    def __init__(self, budget=10000, dim=10, pop_size_init=50, CR=0.9, F=0.5, local_search_prob=0.1, local_search_radius=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size_init\n        self.CR = CR\n        self.F = F\n        self.local_search_prob = local_search_prob\n        self.local_search_radius = local_search_radius\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.pop_size_history = [pop_size_init]\n\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.budget -= self.pop_size\n\n    def mutate(self, x_i):\n        indices = np.random.choice(self.pop_size, size=3, replace=False)\n        x_r1 = self.population[indices[0]]\n        x_r2 = self.population[indices[1]]\n        x_r3 = self.population[indices[2]]\n        # Cauchy mutation\n        return x_r1 + self.F * (x_r2 - x_r3) + np.random.standard_cauchy(size=self.dim) * 0.01\n\n\n    def crossover(self, x_i, v_i):\n        u_i = np.copy(x_i)\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() <= self.CR or j == j_rand:\n                u_i[j] = v_i[j]\n        return u_i\n\n    def repair(self, x, func):\n        return np.clip(x, func.bounds.lb, func.bounds.ub)\n\n    def local_search(self, x, func):\n        x_new = x + np.random.uniform(-self.local_search_radius, self.local_search_radius, size=self.dim)\n        x_new = self.repair(x_new, func)\n        f_new = func(x_new)\n        self.budget -= 1\n        return x_new, f_new\n\n    def adjust_population_size(self):\n        improvement_threshold = 1e-5\n        recent_history = self.pop_size_history[-min(5, len(self.pop_size_history)):]\n        if len(recent_history) < 5:\n            return # not enough data\n\n        if self.f_opt - np.min(self.fitness) > improvement_threshold:\n            self.pop_size = min(self.pop_size + 5, 200)  # Increase if improving\n        else:\n            self.pop_size = max(self.pop_size - 5, 10)  # Decrease if stagnating\n        \n        self.pop_size_history.append(self.pop_size)\n\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            self.adjust_population_size()\n            \n            # Resize population if pop_size changed\n            if self.pop_size != len(self.population):\n                old_pop_size = len(self.population)\n                \n                if self.pop_size > old_pop_size:\n                    # Add new random individuals\n                    new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size - old_pop_size, self.dim))\n                    new_fitness = np.array([func(x) for x in new_individuals])\n                    self.budget -= (self.pop_size - old_pop_size)\n                    self.population = np.vstack((self.population, new_individuals))\n                    self.fitness = np.concatenate((self.fitness, new_fitness))\n                else:\n                    # Reduce population size by removing the worst individuals\n                    indices_to_remove = np.argsort(self.fitness)[self.pop_size:]\n                    self.population = np.delete(self.population, indices_to_remove, axis=0)\n                    self.fitness = np.delete(self.fitness, indices_to_remove)\n            \n            \n            for i in range(self.pop_size):\n                # Mutation\n                v_i = self.mutate(self.population[i])\n\n                # Crossover\n                u_i = self.crossover(self.population[i], v_i)\n\n                # Repair\n                u_i = self.repair(u_i, func)\n\n                # Selection\n                f_u_i = func(u_i)\n                self.budget -= 1\n\n                if f_u_i < self.fitness[i]:\n                    self.population[i] = u_i\n                    self.fitness[i] = f_u_i\n\n                    if f_u_i < self.f_opt:\n                        self.f_opt = f_u_i\n                        self.x_opt = u_i\n\n            # Local search around the best solution\n            if np.random.rand() < self.local_search_prob:\n                x_new, f_new = self.local_search(self.x_opt, func)\n                if f_new < self.f_opt:\n                    self.f_opt = f_new\n                    self.x_opt = x_new\n\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveDESelfPopCauchyLocal scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e9750c7c-65a3-4611-84b6-8bb12cda5beb"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "0e51ae94-3f8e-4962-9899-61b6628737e0", "fitness": 0.3861656084690265, "name": "AdaptiveDEGaussianLocalSearch", "description": "Differential Evolution with Gaussian Local Search and Adaptive Mutation, focusing on refining solutions with Gaussian perturbations around the best candidate while adaptively tuning mutation parameters based on success rate.", "code": "import numpy as np\n\nclass AdaptiveDEGaussianLocalSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_mutation_factor=0.5, crossover_rate=0.7, local_search_frequency=10, local_search_sigma=0.1, mutation_adaptation_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.mutation_factor = initial_mutation_factor\n        self.crossover_rate = crossover_rate\n        self.local_search_frequency = local_search_frequency\n        self.local_search_sigma = local_search_sigma\n        self.mutation_adaptation_rate = mutation_adaptation_rate\n        self.population = np.random.uniform(-5, 5, size=(pop_size, dim))\n        self.fitness = np.zeros(pop_size)\n        self.best_position = None\n        self.best_fitness = np.inf\n        self.eval_count = 0\n        self.successful_mutations = 0\n        self.mutation_success_rate = 0.5  # Initialize with a reasonable value\n\n    def __call__(self, func):\n        self.eval_count = 0\n        self.successful_mutations = 0\n\n        # Initialize fitness values\n        for i in range(self.pop_size):\n            if self.eval_count < self.budget:\n                f = func(self.population[i])\n                self.eval_count += 1\n                self.fitness[i] = f\n                if f < self.best_fitness:\n                    self.best_fitness = f\n                    self.best_position = self.population[i].copy()\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Differential Evolution\n                r1, r2, r3 = np.random.choice(self.pop_size, 3, replace=False)\n                mutant_vector = self.population[r1] + self.mutation_factor * (self.population[r2] - self.population[r3])\n                mutant_vector = np.clip(mutant_vector, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial_vector = np.zeros(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.crossover_rate or j == np.random.randint(self.dim):\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = self.population[i][j]\n\n                # Evaluate trial vector\n                f_trial = func(trial_vector) if self.eval_count < self.budget else np.inf\n                if self.eval_count < self.budget:\n                    self.eval_count += 1\n                    if f_trial < self.fitness[i]:\n                        self.population[i] = trial_vector\n                        self.fitness[i] = f_trial\n                        if f_trial < self.best_fitness:\n                            self.best_fitness = f_trial\n                            self.best_position = self.population[i].copy()\n                        self.successful_mutations += 1\n\n            # Local Search around best solution\n            if self.eval_count // self.local_search_frequency == self.eval_count / self.local_search_frequency:\n                for _ in range(5): # Perform a few local search steps\n                    if self.eval_count < self.budget:\n                        # Generate a Gaussian perturbation around the best solution\n                        perturbed_solution = self.best_position + np.random.normal(0, self.local_search_sigma, self.dim)\n                        perturbed_solution = np.clip(perturbed_solution, func.bounds.lb, func.bounds.ub)\n                        f_perturbed = func(perturbed_solution)\n\n                        self.eval_count += 1\n                        if f_perturbed < self.best_fitness:\n                            self.best_fitness = f_perturbed\n                            self.best_position = perturbed_solution.copy()\n\n            # Adapt mutation factor\n            self.mutation_success_rate = self.successful_mutations / self.pop_size\n            if self.mutation_success_rate > 0.2:\n                self.mutation_factor *= (1 - self.mutation_adaptation_rate)  # Reduce mutation if successful\n            else:\n                self.mutation_factor /= (1 - self.mutation_adaptation_rate)  # Increase mutation if unsuccessful\n\n            self.mutation_factor = np.clip(self.mutation_factor, 0.1, 1.0)  # Keep mutation within reasonable bounds\n            self.successful_mutations = 0 # reset successful mutations\n\n        return self.best_fitness, self.best_position", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveDEGaussianLocalSearch scored 0.386 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d6ed1879-5f43-4211-a97b-a12fd6bebd68"], "operator": null, "metadata": {"aucs": [0.2678677853213123, 0.4279687523008753, 0.30088828448626903, 0.27413900066681485, 0.2563988835254265, 0.269658795296274, 0.3174552704654362, 0.25481130884462977, 0.388530995277667, 0.5342932728722449, 0.31267894514075456, 0.9900479996205702, 0.3521260862652468, 0.3341716013302981, 0.6179017288585231, 0.27859675022249153, 0.27447034129010894, 0.4791882206453564, 0.3066107465300507, 0.4855074004201795]}}
{"id": "9ac203ce-2957-41ca-b753-1773abe5b244", "fitness": 0.4534654245977451, "name": "LevyDE", "description": "A Differential Evolution strategy with a Lvy flight mutation operator and a probabilistic local search to balance exploration and exploitation.", "code": "import numpy as np\n\nclass LevyDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, mutation_factor=0.5, crossover_rate=0.7, levy_exponent=1.5, local_search_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.mutation_factor = mutation_factor\n        self.crossover_rate = crossover_rate\n        self.levy_exponent = levy_exponent\n        self.local_search_prob = local_search_prob\n        self.population = np.random.uniform(-5, 5, size=(pop_size, dim))\n        self.fitness = np.zeros(pop_size)\n        self.best_position = None\n        self.best_fitness = np.inf\n        self.eval_count = 0\n\n    def levy_flight(self, size):\n        \"\"\"\n        Generate Lvy flight steps.\n        \"\"\"\n        num = np.random.randn(size)\n        den = np.abs(np.random.randn(size))**(1/self.levy_exponent)\n        sigma = (np.math.gamma(1+self.levy_exponent) * np.sin(np.pi*self.levy_exponent/2) / (np.math.gamma((1+self.levy_exponent)/2) * self.levy_exponent * 2**((self.levy_exponent-1)/2)))**(1/self.levy_exponent)\n        step = sigma * num / den\n        return step\n\n    def __call__(self, func):\n        self.eval_count = 0\n\n        # Initialize fitness values\n        for i in range(self.pop_size):\n            if self.eval_count < self.budget:\n                f = func(self.population[i])\n                self.eval_count += 1\n                self.fitness[i] = f\n                if f < self.best_fitness:\n                    self.best_fitness = f\n                    self.best_position = self.population[i].copy()\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Differential Evolution with Lvy Flight Mutation\n                r1, r2, r3 = np.random.choice(self.pop_size, 3, replace=False)\n                levy_steps = self.levy_flight(self.dim)\n                mutant_vector = self.population[r1] + self.mutation_factor * (self.population[r2] - self.population[r3]) + levy_steps\n                mutant_vector = np.clip(mutant_vector, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial_vector = np.zeros(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.crossover_rate or j == np.random.randint(self.dim):\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = self.population[i][j]\n\n                # Probabilistic Local Search around the best solution\n                if np.random.rand() < self.local_search_prob:\n                    trial_vector = self.best_position + np.random.uniform(-0.1, 0.1, self.dim) # Small perturbation\n                    trial_vector = np.clip(trial_vector, func.bounds.lb, func.bounds.ub)\n\n                # Evaluate trial vector\n                f_trial = func(trial_vector) if self.eval_count < self.budget else np.inf\n                if self.eval_count < self.budget:\n                    self.eval_count += 1\n                    if f_trial < self.fitness[i]:\n                        self.population[i] = trial_vector\n                        self.fitness[i] = f_trial\n                        if f_trial < self.best_fitness:\n                            self.best_fitness = f_trial\n                            self.best_position = self.population[i].copy()\n\n        return self.best_fitness, self.best_position", "configspace": "", "generation": 7, "feedback": "The algorithm LevyDE scored 0.453 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d6ed1879-5f43-4211-a97b-a12fd6bebd68"], "operator": null, "metadata": {"aucs": [0.18960597365750187, 0.2633668780571966, 0.42422474828146706, 0.8117890946078563, 0.38445085562369263, 0.4415275270835546, 0.31419244341227404, 0.36292034637815473, 0.3728185814588869, 0.32856894558992666, 0.7231578480114155, 0.9977783937135859, 0.25955923564682293, 0.32218296836316007, 0.7413248076249407, 0.4907435878656593, 0.3810415510451651, 0.5544274825737645, 0.20502717781681523, 0.5006000451430611]}}
{"id": "023acde8-049f-4b8d-a1cd-485bec0214d7", "fitness": -Infinity, "name": "AdaptiveDESimplexLocal", "description": "A self-adaptive Differential Evolution with a fitness-dependent mutation scaling factor and a simplex-based local search.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDESimplexLocal:\n    def __init__(self, budget=10000, dim=10, pop_size=50, CR=0.9, F_init=0.5, local_search_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.F_init = F_init  # Initial scaling factor\n        self.F = F_init\n        self.local_search_prob = local_search_prob\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.budget -= self.pop_size\n\n    def mutate(self, x_i):\n        indices = np.random.choice(self.pop_size, size=3, replace=False)\n        x_r1 = self.population[indices[0]]\n        x_r2 = self.population[indices[1]]\n        x_r3 = self.population[indices[2]]\n\n        # Fitness-dependent scaling factor\n        delta_fitness = np.abs(self.fitness[indices[0]] - self.fitness[indices[1]] + self.fitness[indices[2]])/3\n        self.F = self.F_init * (1 + delta_fitness)  # Adjust scaling factor based on fitness differences.\n        self.F = np.clip(self.F, 0.1, 1.0)\n\n        return x_r1 + self.F * (x_r2 - x_r3)\n\n    def crossover(self, x_i, v_i):\n        u_i = np.copy(x_i)\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() <= self.CR or j == j_rand:\n                u_i[j] = v_i[j]\n        return u_i\n\n    def repair(self, x, func):\n        return np.clip(x, func.bounds.lb, func.bounds.ub)\n\n    def local_search(self, x, func):\n        bounds = [(func.bounds.lb[i], func.bounds.ub[i]) for i in range(self.dim)]\n        \n        def obj_func(x_):\n            val = func(x_)\n            self.budget -= 1\n            return val\n\n        result = minimize(obj_func, x, method='Nelder-Mead', bounds=bounds, options={'maxfev': min(50, self.budget)})  # Limited local search\n        if result.success:\n            return result.x, result.fun\n        else:\n            return x, func(x)\n    \n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                v_i = self.mutate(self.population[i])\n\n                # Crossover\n                u_i = self.crossover(self.population[i], v_i)\n\n                # Repair\n                u_i = self.repair(u_i, func)\n\n                # Selection\n                f_u_i = func(u_i)\n                self.budget -= 1\n\n                if f_u_i < self.fitness[i]:\n                    self.population[i] = u_i\n                    self.fitness[i] = f_u_i\n\n                    if f_u_i < self.f_opt:\n                        self.f_opt = f_u_i\n                        self.x_opt = u_i\n\n            # Local search around the best solution\n            if np.random.rand() < self.local_search_prob and self.budget > 0:\n                x_new, f_new = self.local_search(self.x_opt, func)\n                if f_new < self.f_opt:\n                    self.f_opt = f_new\n                    self.x_opt = x_new\n\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 144, in evaluate\n    algorithm(f_new)\n  File \"<string>\", line 94, in __call__\n  File \"<string>\", line 57, in local_search\nNameError: name 'minimize' is not defined\n.", "error": "", "parent_ids": ["d4653b66-15cf-4165-bf1f-24fa7f1cd0bc"], "operator": null, "metadata": {}}
{"id": "334eb98a-2ecd-44fb-9742-69f57fed7818", "fitness": -Infinity, "name": "OrthogonalDE", "description": "Differential Evolution with orthogonal learning for enhanced exploration and exploitation, adjusting mutation strength and crossover rate based on success.", "code": "import numpy as np\n\nclass OrthogonalDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, CR=0.7, F=0.8, F_decay=0.99, CR_decay=0.99, orthogonal_trials=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.F = F\n        self.F_decay = F_decay\n        self.CR_decay = CR_decay\n        self.orthogonal_trials = orthogonal_trials\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.budget -= self.pop_size\n\n    def mutate(self, x_i, func):\n        indices = np.random.choice(self.pop_size, size=3, replace=False)\n        x_r1 = self.population[indices[0]]\n        x_r2 = self.population[indices[1]]\n        x_r3 = self.population[indices[2]]\n        v_i = x_r1 + self.F * (x_r2 - x_r3)\n        return self.repair(v_i, func)\n\n    def crossover(self, x_i, v_i):\n        u_i = np.copy(x_i)\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() <= self.CR or j == j_rand:\n                u_i[j] = v_i[j]\n        return u_i\n\n    def repair(self, x, func):\n        return np.clip(x, func.bounds.lb, func.bounds.ub)\n\n    def orthogonal_learning(self, x_i, func):\n        levels = 2\n        factors = self.dim\n        orthogonal_matrix = self.generate_orthogonal_array(levels, factors)\n        \n        best_x = x_i\n        best_fitness = self.fitness[np.where(np.all(self.population == x_i, axis=1))[0][0]] #self.fitness[i]\n        \n        for trial in range(self.orthogonal_trials):\n            trial_vector = np.zeros(self.dim)\n            \n            for j in range(self.dim):\n                if orthogonal_matrix[trial % orthogonal_matrix.shape[0], j] == 0:\n                    trial_vector[j] = x_i[j] + np.random.uniform(0, 0.1)  # Perturb around original\n                else:\n                    trial_vector[j] = x_i[j] - np.random.uniform(0, 0.1) # Perturb around original\n                \n                trial_vector[j] = self.repair(trial_vector[j], func) \n\n            fitness = func(trial_vector)\n            self.budget -= 1\n            \n            if fitness < best_fitness:\n                best_fitness = fitness\n                best_x = trial_vector\n\n        return best_x, best_fitness\n\n\n    def generate_orthogonal_array(self, levels, factors):\n        # Simple implementation of an L_2^m OA (can be extended)\n        if levels == 2 and factors <= 7:\n            if factors <=3:\n                H = np.array([[1,1,1],[1,-1,-1],[-1,1,-1]])\n                H[H == -1] = 0\n                return H\n            elif factors <=7:\n                # Plackett-Burman design for up to 7 factors\n                pb_design = np.array([\n                    [1, 1, 1, 1, 1, 1, 1],\n                    [1, 1, 1, -1, -1, -1, -1],\n                    [1, 1, -1, 1, -1, -1, -1],\n                    [1, -1, 1, 1, -1, -1, -1],\n                    [1, 1, -1, -1, 1, -1, -1],\n                    [1, -1, 1, -1, 1, -1, -1],\n                    [1, -1, -1, 1, 1, -1, -1],\n                    [1, 1, -1, -1, -1, 1, -1],\n                    [1, -1, 1, -1, -1, 1, -1],\n                    [1, -1, -1, -1, -1, -1, 1],\n                    [1, 1, 1, 1, -1, -1, -1],\n                    [1, -1, -1, 1, -1, 1, -1]])\n                pb_design[pb_design == -1] = 0 # 0 and 1 for levels\n\n                return pb_design[:, :factors]\n        else:\n            raise ValueError(\"Unsupported OA parameters. Only L_2^m OAs with m <= 7 are supported.\")\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n\n        generation = 0\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation and Crossover\n                v_i = self.mutate(self.population[i], func)\n                u_i = self.crossover(self.population[i], v_i)\n                u_i = self.repair(u_i, func)\n                \n                # Orthogonal Learning\n                u_i, f_u_i = self.orthogonal_learning(u_i, func)\n                \n                if self.budget <= 0:\n                   break\n                \n                # Selection\n                if f_u_i < self.fitness[i]:\n                    self.population[i] = u_i\n                    self.fitness[i] = f_u_i\n                    if f_u_i < self.f_opt:\n                        self.f_opt = f_u_i\n                        self.x_opt = u_i\n                        \n            self.F *= self.F_decay\n            self.CR *= self.CR_decay\n\n            if self.budget <= 0:\n                break\n\n            generation += 1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 116, in __call__\n  File \"<string>\", line 50, in orthogonal_learning\nIndexError: index 0 is out of bounds for axis 0 with size 0\n.", "error": "", "parent_ids": ["d4653b66-15cf-4165-bf1f-24fa7f1cd0bc"], "operator": null, "metadata": {}}
{"id": "2474e00b-7ca3-4163-9d3c-0c97cdf21487", "fitness": 0.0, "name": "OrthogonalDE", "description": "Differential Evolution with orthogonal learning, self-adaptive parameters, and a diversity-preserving mechanism to enhance exploration and exploitation.", "code": "import numpy as np\n\nclass OrthogonalDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, CR=0.9, F=0.5, orthogonal_samples=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.F = F\n        self.orthogonal_samples = orthogonal_samples\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.budget -= self.pop_size\n\n    def mutate(self, x_i):\n        indices = np.random.choice(self.pop_size, size=3, replace=False)\n        x_r1 = self.population[indices[0]]\n        x_r2 = self.population[indices[1]]\n        x_r3 = self.population[indices[2]]\n        return x_r1 + self.F * (x_r2 - x_r3)\n\n    def crossover(self, x_i, v_i):\n        u_i = np.copy(x_i)\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() <= self.CR or j == j_rand:\n                u_i[j] = v_i[j]\n        return u_i\n\n    def repair(self, x, func):\n        return np.clip(x, func.bounds.lb, func.bounds.ub)\n\n    def orthogonal_design(self, x_i, func):\n        levels = 3  # Using 3 levels for each dimension\n        design = self.generate_orthogonal_array(self.dim, levels)\n        \n        candidates = []\n        fitnesses = []\n\n        for i in range(design.shape[0]):  # Iterate through orthogonal samples\n            x_new = np.copy(x_i)\n            for j in range(self.dim):\n                level = design[i, j]\n                range_dim = func.bounds.ub[j] - func.bounds.lb[j]\n                x_new[j] = func.bounds.lb[j] + (level / (levels - 1)) * range_dim\n            \n            x_new = self.repair(x_new, func)\n            f_new = func(x_new)\n            self.budget -= 1\n            \n            candidates.append(x_new)\n            fitnesses.append(f_new)\n            \n        best_index = np.argmin(fitnesses)\n        return candidates[best_index], fitnesses[best_index]\n\n    def generate_orthogonal_array(self, factors, levels):\n        # This is a simplified example and might not cover all cases.\n        # For a more robust implementation, consider using libraries like pyDOE.\n        if factors <= levels:\n            design = np.zeros((levels, factors), dtype=int)\n            for i in range(levels):\n                for j in range(factors):\n                    design[i, j] = i\n            return design\n        else:\n            # Simple alternative for factors > levels\n            design = np.random.randint(0, levels, size=(levels, factors))\n            return design\n            \n\n    def self_adaptive_parameters(self, success):\n        if success:\n            self.CR = min(1.0, self.CR + 0.1)\n            self.F = min(1.0, self.F + 0.1)\n        else:\n            self.CR = max(0.1, self.CR - 0.1)\n            self.F = max(0.1, self.F - 0.1)\n\n    def diversity_preservation(self):\n        # Replace the worst individual with a random one if population is too similar\n        std = np.std(self.population)\n        if std < 1e-6:  # Threshold for similarity\n            worst_index = np.argmax(self.fitness)\n            self.population[worst_index] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n            self.fitness[worst_index] = func(self.population[worst_index])\n            self.budget -= 1\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                v_i = self.mutate(self.population[i])\n\n                # Crossover\n                u_i = self.crossover(self.population[i], v_i)\n\n                # Repair\n                u_i = self.repair(u_i, func)\n\n                # Orthogonal learning\n                u_i, f_u_i = self.orthogonal_design(u_i, func)\n                \n                success = False\n                if f_u_i < self.fitness[i]:\n                    self.population[i] = u_i\n                    self.fitness[i] = f_u_i\n                    success = True\n\n                    if f_u_i < self.f_opt:\n                        self.f_opt = f_u_i\n                        self.x_opt = u_i\n                \n                self.self_adaptive_parameters(success)\n                \n            self.diversity_preservation()\n\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm OrthogonalDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d4653b66-15cf-4165-bf1f-24fa7f1cd0bc"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "4f747671-22c4-482d-9e9b-ebf305fe11c9", "fitness": 0.0, "name": "AdaptiveDECMARestart", "description": "Differential Evolution with a self-adaptive mutation strategy, covariance matrix adaptation local search, and a restart mechanism to escape local optima.", "code": "import numpy as np\n\nclass AdaptiveDECMARestart:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_mutation_factor=0.5, crossover_rate=0.7, local_search_iterations=5, restart_trigger=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.mutation_factor = initial_mutation_factor\n        self.crossover_rate = crossover_rate\n        self.local_search_iterations = local_search_iterations\n        self.restart_trigger = restart_trigger\n        self.population = np.random.uniform(-5, 5, size=(pop_size, dim))\n        self.fitness = np.zeros(pop_size)\n        self.best_position = None\n        self.best_fitness = np.inf\n        self.eval_count = 0\n        self.initial_mutation_factor = initial_mutation_factor\n\n    def covariance_matrix_adaptation(self, func, x_mean, sigma, num_iterations):\n        \"\"\"\n        Local search using Covariance Matrix Adaptation.\n        \"\"\"\n        dim = self.dim\n        C = np.eye(dim)  # Initialize covariance matrix\n        path_c = np.zeros(dim)\n        path_sigma = np.zeros(dim)\n        mu_eff = self.pop_size / 4  # Effective population size\n\n        c_sigma = (mu_eff + 2) / (dim + mu_eff + 5)\n        c_c = (4 + mu_eff / dim) / (dim + 4 + 2 * mu_eff / dim)\n        c_1 = 2 / ((dim + 1.3)**2 + mu_eff)\n        c_mu = min(1 - c_1, 2 * (mu_eff - 2 + 1 / mu_eff) / ((dim + 2)**2 + mu_eff))\n        d_sigma = 1 + 2 * max(0, np.sqrt((mu_eff - 1) / (dim + 1)) - 1) + c_sigma\n\n        best_x = x_mean\n        best_f = func(x_mean) if self.eval_count < self.budget else np.inf\n        if self.eval_count < self.budget:\n            self.eval_count += 1\n\n        for _ in range(num_iterations):\n            # Generate samples\n            z = np.random.multivariate_normal(np.zeros(dim), C, self.pop_size)\n            x = x_mean + sigma * z\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            f = np.array([func(xi) if self.eval_count < self.budget else np.inf for xi in x])\n            self.eval_count += np.sum(f != np.inf) # correct eval count\n\n            if self.eval_count >= self.budget:\n                break\n\n            # Selection and update\n            idx = np.argsort(f)\n            x_mean_new = np.mean(x[idx[:int(mu_eff)]], axis=0)\n            \n            if f[idx[0]] < best_f:\n                best_f = f[idx[0]]\n                best_x = x[idx[0]]\n                \n            # Update evolution path\n            z_mean = np.mean(z[idx[:int(mu_eff)]], axis=0)\n            path_sigma = (1 - c_sigma) * path_sigma + np.sqrt(c_sigma * (2 - c_sigma)) * z_mean\n            path_c = (1 - c_c) * path_c + np.sqrt(c_c * (2 - c_c)) * (np.sqrt(mu_eff) / sigma) * (x_mean_new - x_mean)\n\n            # Update covariance matrix\n            C = (1 - c_1 - c_mu) * C + c_1 * np.outer(path_c, path_c) + c_mu * np.sum([np.outer(z[idx[i]], z[idx[i]]) for i in range(int(mu_eff))], axis=0)\n            sigma *= np.exp((c_sigma / d_sigma) * (np.linalg.norm(path_sigma) / np.sqrt(dim) - 1))\n            x_mean = x_mean_new\n        \n        return best_f, best_x\n\n    def __call__(self, func):\n        self.eval_count = 0\n        self.mutation_factor = self.initial_mutation_factor\n\n        # Initialize fitness values\n        for i in range(self.pop_size):\n            if self.eval_count < self.budget:\n                f = func(self.population[i])\n                self.eval_count += 1\n                self.fitness[i] = f\n                if f < self.best_fitness:\n                    self.best_fitness = f\n                    self.best_position = self.population[i].copy()\n\n        last_improvement = 0\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Differential Evolution\n                r1, r2, r3 = np.random.choice(self.pop_size, 3, replace=False)\n                mutant_vector = self.population[r1] + self.mutation_factor * (self.population[r2] - self.population[r3])\n                mutant_vector = np.clip(mutant_vector, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial_vector = np.zeros(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.crossover_rate or j == np.random.randint(self.dim):\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = self.population[i][j]\n\n                # Evaluate trial vector\n                f_trial = func(trial_vector) if self.eval_count < self.budget else np.inf\n                if self.eval_count < self.budget:\n                    self.eval_count += 1\n                    if f_trial < self.fitness[i]:\n                        self.population[i] = trial_vector\n                        self.fitness[i] = f_trial\n                        if f_trial < self.best_fitness:\n                            self.best_fitness = f_trial\n                            self.best_position = self.population[i].copy()\n                            last_improvement = self.eval_count\n\n            # Covariance Matrix Adaptation Local Search\n            local_search_fitness, local_search_position = self.covariance_matrix_adaptation(func, self.best_position, 0.1, self.local_search_iterations)\n\n            if local_search_fitness < self.best_fitness:\n                    self.best_fitness = local_search_fitness\n                    self.best_position = local_search_position.copy()\n                    last_improvement = self.eval_count\n\n            # Restart mechanism\n            if (self.eval_count - last_improvement) > self.restart_trigger * self.budget:\n                self.population = np.random.uniform(-5, 5, size=(self.pop_size, self.dim))\n                for i in range(self.pop_size):\n                    if self.eval_count < self.budget:\n                        f = func(self.population[i])\n                        self.eval_count += 1\n                        self.fitness[i] = f\n                        if f < self.best_fitness:\n                            self.best_fitness = f\n                            self.best_position = self.population[i].copy()\n                last_improvement = self.eval_count\n                self.mutation_factor = self.initial_mutation_factor  # Reset mutation factor\n\n            # Adaptive Mutation Factor (simple heuristic)\n            if np.random.rand() < 0.1:\n                self.mutation_factor = np.clip(self.mutation_factor + np.random.normal(0, 0.1), 0.1, 1.0)\n\n        return self.best_fitness, self.best_position", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveDECMARestart scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9ac203ce-2957-41ca-b753-1773abe5b244"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "c07fad6c-8c65-4588-84b4-c8351f16560d", "fitness": -Infinity, "name": "AdaptiveDEMirroredToroidal", "description": "An adaptive Differential Evolution strategy employing a mirrored sampling technique and a toroidal boundary handling mechanism to enhance exploration and constraint satisfaction.", "code": "import numpy as np\n\nclass AdaptiveDEMirroredToroidal:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, CR=0.9, initial_F=0.5, mirrored_rate=0.2, F_adapt_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.CR = CR\n        self.F = initial_F\n        self.mirrored_rate = mirrored_rate\n        self.F_adapt_rate = F_adapt_rate\n        self.population = None\n        self.fitness = None\n        self.best_fitness_history = []\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.best_fitness_history.append(np.min(self.fitness))\n        self.budget -= self.pop_size\n\n    def mutate(self, x_i):\n        indices = np.random.choice(self.pop_size, size=3, replace=False)\n        x_r1 = self.population[indices[0]]\n        x_r2 = self.population[indices[1]]\n        x_r3 = self.population[indices[2]]\n\n        return x_r1 + self.F * (x_r2 - x_r3)\n\n    def crossover(self, x_i, v_i):\n        u_i = np.copy(x_i)\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() <= self.CR or j == j_rand:\n                u_i[j] = v_i[j]\n        return u_i\n\n    def repair_toroidal(self, x, func):\n        \"\"\"Handles boundaries using toroidal wrapping.\"\"\"\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        width = ub - lb\n        x_wrapped = np.copy(x)\n        for i in range(self.dim):\n            if x_wrapped[i] < lb:\n                x_wrapped[i] = ub - (lb - x_wrapped[i]) % width\n            elif x_wrapped[i] > ub:\n                x_wrapped[i] = lb + (x_wrapped[i] - ub) % width\n        return x_wrapped\n\n    def adapt_F(self, f_u_i, f_x_i):\n        \"\"\"Adapt F based on the fitness improvement.\"\"\"\n        if f_u_i < f_x_i:\n            self.F = max(0.1, self.F * (1 - self.F_adapt_rate))  # Reduce F if improvement\n        else:\n            self.F = min(0.9, self.F * (1 + self.F_adapt_rate))  # Increase F if no improvement\n\n    def mirrored_sampling(self, func):\n        \"\"\"Performs mirrored sampling around the best solution.\"\"\"\n        num_mirrored = int(self.mirrored_rate * self.pop_size)\n        if num_mirrored > 0:\n            best_index = np.argmin(self.fitness)\n            x_best = self.population[best_index]\n            lb = func.bounds.lb\n            ub = func.bounds.ub\n            for _ in range(num_mirrored):\n                x_mirrored = x_best + np.random.uniform(-0.1 * (ub - lb), 0.1 * (ub - lb), size=self.dim) # Local exploration\n                x_mirrored = self.repair_toroidal(x_mirrored, func)\n\n                f_mirrored = func(x_mirrored)\n                self.budget -= 1\n                \n                worst_index = np.argmax(self.fitness)\n                if f_mirrored < self.fitness[worst_index]:\n                        self.population[worst_index] = x_mirrored\n                        self.fitness[worst_index] = f_mirrored\n                        if f_mirrored < self.f_opt:\n                            self.f_opt = f_mirrored\n                            self.x_opt = x_mirrored\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                v_i = self.mutate(self.population[i])\n\n                # Crossover\n                u_i = self.crossover(self.population[i], v_i)\n\n                # Repair (Toroidal)\n                u_i = self.repair_toroidal(u_i, func)\n\n                # Selection\n                f_u_i = func(u_i)\n                self.budget -= 1\n\n                if f_u_i < self.fitness[i]:\n                    self.population[i] = u_i\n                    self.fitness[i] = f_u_i\n                    if f_u_i < self.f_opt:\n                        self.f_opt = f_u_i\n                        self.x_opt = u_i\n                    self.adapt_F(f_u_i, self.fitness[i])\n\n            self.mirrored_sampling(func) #Mirrored sampling\n            \n            self.best_fitness_history.append(np.min(self.fitness))\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "An exception occured: Traceback (most recent call last):\n  File \"/home/neocortex/repos/BLADE/blade/problem.py\", line 40, in __call__\n    solution = self.evaluate(solution)\n  File \"/home/neocortex/repos/BLADE/blade/problems/mabbob.py\", line 121, in evaluate\n    algorithm(problem)\n  File \"<string>\", line 96, in __call__\n  File \"<string>\", line 45, in repair_toroidal\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n.", "error": "", "parent_ids": ["6f30cdbf-6eb3-4928-9e92-5730073ffbf5"], "operator": null, "metadata": {}}
{"id": "36fb8472-c3e6-4711-9236-ea19d454fe7a", "fitness": 0.536338983512868, "name": "HybridDEwithSA", "description": "Hybrid Differential Evolution with Simulated Annealing for enhanced exploration and exploitation, dynamically adjusting temperature based on fitness improvement.", "code": "import numpy as np\n\nclass HybridDEwithSA:\n    def __init__(self, budget=10000, dim=10, pop_size=20, mutation_factor=0.5, crossover_rate=0.7, initial_temp=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.mutation_factor = mutation_factor\n        self.crossover_rate = crossover_rate\n        self.initial_temp = initial_temp\n        self.cooling_rate = cooling_rate\n        self.population = np.random.uniform(-5, 5, size=(pop_size, dim))\n        self.fitness = np.zeros(pop_size)\n        self.best_position = None\n        self.best_fitness = np.inf\n        self.eval_count = 0\n        self.temperature = initial_temp\n\n    def __call__(self, func):\n        self.eval_count = 0\n        self.temperature = self.initial_temp\n\n        # Initialize fitness values\n        for i in range(self.pop_size):\n            if self.eval_count < self.budget:\n                f = func(self.population[i])\n                self.eval_count += 1\n                self.fitness[i] = f\n                if f < self.best_fitness:\n                    self.best_fitness = f\n                    self.best_position = self.population[i].copy()\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Differential Evolution Mutation\n                r1, r2, r3 = np.random.choice(self.pop_size, 3, replace=False)\n                mutant_vector = self.population[r1] + self.mutation_factor * (self.population[r2] - self.population[r3])\n                mutant_vector = np.clip(mutant_vector, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial_vector = np.zeros(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.crossover_rate or j == np.random.randint(self.dim):\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = self.population[i][j]\n\n                # Evaluate trial vector\n                f_trial = func(trial_vector) if self.eval_count < self.budget else np.inf\n                if self.eval_count < self.budget:\n                    self.eval_count += 1\n\n                    # Simulated Annealing acceptance criterion\n                    delta_e = f_trial - self.fitness[i]\n                    if delta_e < 0:\n                        self.population[i] = trial_vector\n                        self.fitness[i] = f_trial\n                        if f_trial < self.best_fitness:\n                            self.best_fitness = f_trial\n                            self.best_position = self.population[i].copy()\n                    else:\n                        # Acceptance probability based on temperature\n                        acceptance_prob = np.exp(-delta_e / self.temperature)\n                        if np.random.rand() < acceptance_prob:\n                            self.population[i] = trial_vector\n                            self.fitness[i] = f_trial\n\n            #Cooling\n            self.temperature *= self.cooling_rate\n\n        return self.best_fitness, self.best_position", "configspace": "", "generation": 8, "feedback": "The algorithm HybridDEwithSA scored 0.536 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9ac203ce-2957-41ca-b753-1773abe5b244"], "operator": null, "metadata": {"aucs": [0.4265719413073744, 0.22584789134582284, 0.4151937762329899, 0.7137331859868746, 0.5929354715787596, 0.6009087407361479, 0.5847735202735737, 0.5778732590556699, 0.5948366250960071, 0.20183224731359872, 0.7121941991175651, 0.9866491364728704, 0.29647869146426986, 0.6060735303395893, 0.6564907159551496, 0.6023137460198164, 0.5744345673174746, 0.6422314681897494, 0.22029769688545098, 0.49510925956860374]}}
{"id": "72be1ab6-eeb1-4b8a-a6fe-6aff9cd53e3f", "fitness": 0.45659063809155737, "name": "AdaptiveDEMultiMutationLocalSearch", "description": "Differential Evolution with a combined mutation strategy (current-to-rand and current-to-best) and a self-adaptive learning rate for local search, dynamically adjusting exploration based on population diversity.", "code": "import numpy as np\n\nclass AdaptiveDEMultiMutationLocalSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=20, mutation_factor=0.5, crossover_rate=0.7, local_search_prob=0.1, initial_learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.mutation_factor = mutation_factor\n        self.crossover_rate = crossover_rate\n        self.local_search_prob = local_search_prob\n        self.learning_rate = initial_learning_rate\n        self.initial_learning_rate = initial_learning_rate\n        self.population = np.random.uniform(-5, 5, size=(pop_size, dim))\n        self.fitness = np.zeros(pop_size)\n        self.best_position = None\n        self.best_fitness = np.inf\n        self.eval_count = 0\n\n    def calculate_diversity(self):\n        \"\"\"Calculates the population diversity based on the variance of each dimension.\"\"\"\n        mean_position = np.mean(self.population, axis=0)\n        diversity = np.mean(np.var(self.population, axis=0))\n        return diversity\n\n    def __call__(self, func):\n        self.eval_count = 0\n        self.learning_rate = self.initial_learning_rate # Reset learning rate\n\n        # Initialize fitness values\n        for i in range(self.pop_size):\n            if self.eval_count < self.budget:\n                f = func(self.population[i])\n                self.eval_count += 1\n                self.fitness[i] = f\n                if f < self.best_fitness:\n                    self.best_fitness = f\n                    self.best_position = self.population[i].copy()\n\n        while self.eval_count < self.budget:\n            diversity = self.calculate_diversity()\n            # Adjust learning rate based on diversity\n            self.learning_rate = self.initial_learning_rate * (diversity / 25) # Scale diversity to [0, 1] assuming bounds are [-5,5] and var max is 25\n\n            for i in range(self.pop_size):\n                # Combined Mutation Strategy\n                if np.random.rand() < 0.5:\n                    # Current-to-rand mutation\n                    r1, r2 = np.random.choice(self.pop_size, 2, replace=False)\n                    mutant_vector = self.population[i] + self.mutation_factor * (self.population[r1] - self.population[r2])\n                else:\n                    # Current-to-best mutation\n                    r1 = np.random.choice(self.pop_size, 1, replace=False)[0]\n                    mutant_vector = self.population[i] + self.mutation_factor * (self.best_position - self.population[r1])\n\n                mutant_vector = np.clip(mutant_vector, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial_vector = np.zeros(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.crossover_rate or j == np.random.randint(self.dim):\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = self.population[i][j]\n\n                # Adaptive Local Search around the best solution\n                if np.random.rand() < self.local_search_prob:\n                    perturbation = np.random.uniform(-self.learning_rate, self.learning_rate, self.dim)\n                    trial_vector = self.best_position + perturbation\n                    trial_vector = np.clip(trial_vector, func.bounds.lb, func.bounds.ub)\n\n                # Evaluate trial vector\n                f_trial = func(trial_vector) if self.eval_count < self.budget else np.inf\n                if self.eval_count < self.budget:\n                    self.eval_count += 1\n                    if f_trial < self.fitness[i]:\n                        self.population[i] = trial_vector\n                        self.fitness[i] = f_trial\n                        if f_trial < self.best_fitness:\n                            self.best_fitness = f_trial\n                            self.best_position = self.population[i].copy()\n\n        return self.best_fitness, self.best_position", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveDEMultiMutationLocalSearch scored 0.457 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9ac203ce-2957-41ca-b753-1773abe5b244"], "operator": null, "metadata": {"aucs": [0.16763625215020694, 0.1682834331557177, 0.35775808488340344, 0.26420949816585093, 0.2556080297802983, 0.33681849497101124, 0.2945532019571726, 0.560154378696866, 0.1677465779690689, 0.17541775083478262, 0.8781046816981014, 0.9970252745480638, 0.28067171696649384, 0.6302966334299221, 0.8662605732924287, 0.8757351256690558, 0.3135301365246872, 0.906652497135369, 0.15288077072033934, 0.4824696492823095]}}
{"id": "558500a7-ca4f-4e9c-b718-6c108e1b2d97", "fitness": 0.6723283662800211, "name": "SelfOrganizingDE", "description": "A self-organizing differential evolution with a dynamically adjusted mutation strategy based on population diversity and success history.", "code": "import numpy as np\n\nclass SelfOrganizingDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_mutation_factor=0.5, crossover_rate=0.7, learning_rate=0.1, diversity_threshold=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.mutation_factor = initial_mutation_factor\n        self.crossover_rate = crossover_rate\n        self.learning_rate = learning_rate\n        self.diversity_threshold = diversity_threshold\n        self.population = np.random.uniform(-5, 5, size=(pop_size, dim))\n        self.fitness = np.zeros(pop_size)\n        self.best_position = None\n        self.best_fitness = np.inf\n        self.eval_count = 0\n        self.success_history = []\n\n    def calculate_diversity(self):\n        \"\"\"Calculates the diversity of the population based on the standard deviation of each dimension.\"\"\"\n        std_devs = np.std(self.population, axis=0)\n        return np.mean(std_devs)\n\n    def adjust_mutation_factor(self, success_rate):\n        \"\"\"Adjusts the mutation factor based on the recent success rate of generating better solutions.\"\"\"\n        if self.success_history:\n            success_rate = np.mean(self.success_history[-10:])  # Consider the last 10 updates\n            self.mutation_factor += self.learning_rate * (success_rate - 0.5)  # Adjust towards 0.5\n            self.mutation_factor = np.clip(self.mutation_factor, 0.1, 1.0) # Keep mutation factor bounded\n        return self.mutation_factor\n\n    def __call__(self, func):\n        self.eval_count = 0\n        self.success_history = []\n\n        # Initialize fitness values\n        for i in range(self.pop_size):\n            if self.eval_count < self.budget:\n                f = func(self.population[i])\n                self.eval_count += 1\n                self.fitness[i] = f\n                if f < self.best_fitness:\n                    self.best_fitness = f\n                    self.best_position = self.population[i].copy()\n\n        while self.eval_count < self.budget:\n            diversity = self.calculate_diversity()\n            \n            for i in range(self.pop_size):\n                # Differential Evolution Mutation\n                r1, r2, r3 = np.random.choice(self.pop_size, 3, replace=False)\n                mutant_vector = self.population[r1] + self.mutation_factor * (self.population[r2] - self.population[r3])\n                mutant_vector = np.clip(mutant_vector, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial_vector = np.zeros(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.crossover_rate or j == np.random.randint(self.dim):\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = self.population[i][j]\n\n                # Evaluate trial vector\n                f_trial = func(trial_vector) if self.eval_count < self.budget else np.inf\n                if self.eval_count < self.budget:\n                    self.eval_count += 1\n                    if f_trial < self.fitness[i]:\n                        self.success_history.append(1)  # Record a success\n                        self.population[i] = trial_vector\n                        self.fitness[i] = f_trial\n                        if f_trial < self.best_fitness:\n                            self.best_fitness = f_trial\n                            self.best_position = self.population[i].copy()\n                    else:\n                        self.success_history.append(0)  # Record a failure\n\n            # Dynamically Adjust Mutation Factor\n            self.mutation_factor = self.adjust_mutation_factor(np.mean(self.success_history[-10:] if len(self.success_history) > 0 else [0]))\n            \n            # If diversity is too low, increase mutation factor slightly\n            if diversity < self.diversity_threshold:\n                self.mutation_factor = min(self.mutation_factor + 0.1, 1.0)\n\n        return self.best_fitness, self.best_position", "configspace": "", "generation": 8, "feedback": "The algorithm SelfOrganizingDE scored 0.672 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9ac203ce-2957-41ca-b753-1773abe5b244"], "operator": null, "metadata": {"aucs": [0.2799258687516786, 0.6329067123470302, 0.7009366259683711, 0.9436530689050775, 0.7472273637671114, 0.8085737927589883, 0.3808562019476168, 0.6228178592542615, 0.74094141106878, 0.7567525324960998, 0.9197291299418254, 0.9994061468124078, 0.33595218086718026, 0.6709683806118802, 0.9310755308935547, 0.8369511688257152, 0.2899822901130571, 0.8827491891336814, 0.4542022209975516, 0.5109596501385534]}}
{"id": "9fa85dbc-f614-4837-95f3-70876915bfb1", "fitness": 0.3529491273488845, "name": "AdaptiveLevyDEAging", "description": "An adaptive Differential Evolution strategy with a modified Lvy flight mutation operator and an aging mechanism for population diversity.", "code": "import numpy as np\n\nclass AdaptiveLevyDEAging:\n    def __init__(self, budget=10000, dim=10, pop_size=20, mutation_factor=0.5, crossover_rate=0.7, levy_exponent=1.5, aging_rate=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.mutation_factor = mutation_factor\n        self.crossover_rate = crossover_rate\n        self.levy_exponent = levy_exponent\n        self.aging_rate = aging_rate\n        self.population = np.random.uniform(-5, 5, size=(pop_size, dim))\n        self.fitness = np.zeros(pop_size)\n        self.age = np.zeros(pop_size)  # Initialize age for each individual\n        self.best_position = None\n        self.best_fitness = np.inf\n        self.eval_count = 0\n\n    def levy_flight(self, size):\n        \"\"\"\n        Generate Lvy flight steps.\n        \"\"\"\n        num = np.random.randn(size)\n        den = np.abs(np.random.randn(size))**(1/self.levy_exponent)\n        sigma = (np.math.gamma(1+self.levy_exponent) * np.sin(np.pi*self.levy_exponent/2) / (np.math.gamma((1+self.levy_exponent)/2) * self.levy_exponent * 2**((self.levy_exponent-1)/2)))**(1/self.levy_exponent)\n        step = sigma * num / den\n        return step\n\n    def __call__(self, func):\n        self.eval_count = 0\n\n        # Initialize fitness values and best solution\n        for i in range(self.pop_size):\n            if self.eval_count < self.budget:\n                f = func(self.population[i])\n                self.eval_count += 1\n                self.fitness[i] = f\n                if f < self.best_fitness:\n                    self.best_fitness = f\n                    self.best_position = self.population[i].copy()\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Differential Evolution with Lvy Flight Mutation\n                r1, r2, r3 = np.random.choice(self.pop_size, 3, replace=False)\n                levy_steps = self.levy_flight(self.dim) * (1 + self.age[i] * self.aging_rate)  # Adapt step size based on age\n                mutant_vector = self.population[r1] + self.mutation_factor * (self.population[r2] - self.population[r3]) + levy_steps\n                mutant_vector = np.clip(mutant_vector, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial_vector = np.zeros(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.crossover_rate or j == np.random.randint(self.dim):\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = self.population[i][j]\n\n                # Evaluate trial vector\n                f_trial = func(trial_vector) if self.eval_count < self.budget else np.inf\n                if self.eval_count < self.budget:\n                    self.eval_count += 1\n                    if f_trial < self.fitness[i]:\n                        self.population[i] = trial_vector\n                        self.fitness[i] = f_trial\n                        self.age[i] = 0 # reset age\n                        if f_trial < self.best_fitness:\n                            self.best_fitness = f_trial\n                            self.best_position = self.population[i].copy()\n                    else:\n                        self.age[i] += 1  # Increment age if not improved\n            \n            # Increase age for all individuals\n            #self.age += 1\n            \n            # Rejuvenate old individuals\n            for i in range(self.pop_size):\n                if self.age[i] > 100: # aging threshold\n                    self.population[i] = np.random.uniform(-5, 5, self.dim)\n                    self.fitness[i] = func(self.population[i]) if self.eval_count < self.budget else np.inf\n                    if self.eval_count < self.budget:\n                         self.eval_count += 1\n\n                    self.age[i] = 0\n                    if self.fitness[i] < self.best_fitness:\n                        self.best_fitness = self.fitness[i]\n                        self.best_position = self.population[i].copy()\n\n        return self.best_fitness, self.best_position", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveLevyDEAging scored 0.353 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9ac203ce-2957-41ca-b753-1773abe5b244"], "operator": null, "metadata": {"aucs": [0.15873610628783041, 0.27452857866102853, 0.33250187749503324, 0.36269364259071124, 0.27211220123458757, 0.316524998403132, 0.2897899302978616, 0.28626361617443485, 0.26703349724925385, 0.20952213477373294, 0.33550471077260924, 0.9975446509372052, 0.3003144393951084, 0.26692685826517104, 0.7023896149747304, 0.3558748542018315, 0.28951261564496833, 0.3778463369683138, 0.17949480916659688, 0.4838670734835486]}}
{"id": "af433d51-af8a-43a0-a65e-0e40701b2f0a", "fitness": 0.3741618781579671, "name": "AdaptiveLevyDE", "description": "A Differential Evolution strategy with a self-adaptive mutation factor, a truncated Levy flight mutation, and a greedy selection mechanism biased towards fitter individuals.", "code": "import numpy as np\n\nclass AdaptiveLevyDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_mutation_factor=0.5, crossover_rate=0.7, levy_exponent=1.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.mutation_factor = initial_mutation_factor\n        self.crossover_rate = crossover_rate\n        self.levy_exponent = levy_exponent\n        self.population = np.random.uniform(-5, 5, size=(pop_size, dim))\n        self.fitness = np.zeros(pop_size)\n        self.best_position = None\n        self.best_fitness = np.inf\n        self.eval_count = 0\n        self.mutation_factor_memory = np.full(pop_size, initial_mutation_factor)\n\n    def truncated_levy_flight(self, size, truncation_level=3):\n        \"\"\"\n        Generate truncated Lvy flight steps.\n        \"\"\"\n        u = np.random.randn(size)\n        v = np.random.randn(size)\n        step = u / abs(v)**(1/self.levy_exponent)\n        step[step > truncation_level] = truncation_level\n        step[step < -truncation_level] = -truncation_level\n        return step\n\n    def __call__(self, func):\n        self.eval_count = 0\n\n        # Initialize fitness values\n        for i in range(self.pop_size):\n            if self.eval_count < self.budget:\n                f = func(self.population[i])\n                self.eval_count += 1\n                self.fitness[i] = f\n                if f < self.best_fitness:\n                    self.best_fitness = f\n                    self.best_position = self.population[i].copy()\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Self-Adaptive Mutation Factor\n                if np.random.rand() < 0.1:  # Probability to adjust mutation factor\n                    self.mutation_factor_memory[i] = np.random.uniform(0.1, 1.0)\n                mutation_factor = self.mutation_factor_memory[i]\n\n                # Differential Evolution with Truncated Lvy Flight Mutation\n                r1, r2, r3 = np.random.choice(self.pop_size, 3, replace=False)\n                levy_steps = self.truncated_levy_flight(self.dim)\n                mutant_vector = self.population[r1] + mutation_factor * (self.population[r2] - self.population[r3]) + levy_steps\n                mutant_vector = np.clip(mutant_vector, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial_vector = np.zeros(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.crossover_rate or j == np.random.randint(self.dim):\n                        trial_vector[j] = mutant_vector[j]\n                    else:\n                        trial_vector[j] = self.population[i][j]\n\n                # Evaluate trial vector\n                f_trial = func(trial_vector) if self.eval_count < self.budget else np.inf\n                if self.eval_count < self.budget:\n                    self.eval_count += 1\n\n                    #Greedy selection: Bias towards fitter individuals\n                    if f_trial < self.fitness[i]:\n                        self.population[i] = trial_vector\n                        self.fitness[i] = f_trial\n                        if f_trial < self.best_fitness:\n                            self.best_fitness = f_trial\n                            self.best_position = self.population[i].copy()\n                    else:\n                        #If trial is worse, randomly replace with best to help convergence\n                        if np.random.rand() < 0.05:\n                            self.population[i] = self.best_position.copy()\n\n        return self.best_fitness, self.best_position", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveLevyDE scored 0.374 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9ac203ce-2957-41ca-b753-1773abe5b244"], "operator": null, "metadata": {"aucs": [0.16388045861684164, 0.27718730097107713, 0.3574562238218687, 0.39860147451617733, 0.27412243309029405, 0.39245284155371907, 0.2798415985830507, 0.31939296570623477, 0.27664765234135735, 0.25324805444606613, 0.38151940093777104, 0.9925225873248615, 0.25631578722263004, 0.286999414876579, 0.7350134058543067, 0.39092445458480574, 0.2944338592166168, 0.4115828775422985, 0.24899874900531527, 0.4920960229474708]}}
{"id": "17fc61f2-efc3-4d44-8560-e66de40f1891", "fitness": 0.5274738545609691, "name": "AdaptiveDETournament", "description": "An adaptive Differential Evolution algorithm with a self-adaptive learning rate for parameter updates and a tournament selection mechanism to promote diversity.", "code": "import numpy as np\n\nclass AdaptiveDETournament:\n    def __init__(self, budget=10000, dim=10, pop_size=50, CR=0.9, F=0.5, lr_F=0.1, lr_CR=0.1, tournament_size=3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.F = F\n        self.lr_F = lr_F  # Learning rate for F adaptation\n        self.lr_CR = lr_CR  # Learning rate for CR adaptation\n        self.tournament_size = tournament_size\n        self.population = None\n        self.fitness = None\n        self.best_fitness_history = []\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.best_fitness_history.append(np.min(self.fitness))\n        self.budget -= self.pop_size\n\n    def mutate(self, x_i):\n        indices = np.random.choice(self.pop_size, size=3, replace=False)\n        x_r1 = self.population[indices[0]]\n        x_r2 = self.population[indices[1]]\n        x_r3 = self.population[indices[2]]\n        return x_r1 + self.F * (x_r2 - x_r3)\n\n    def crossover(self, x_i, v_i, cr):\n        u_i = np.copy(x_i)\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() <= cr or j == j_rand:\n                u_i[j] = v_i[j]\n        return u_i\n\n    def repair(self, x, func):\n        return np.clip(x, func.bounds.lb, func.bounds.ub)\n\n    def tournament_selection(self):\n        \"\"\"Selects an individual using tournament selection.\"\"\"\n        indices = np.random.choice(self.pop_size, size=self.tournament_size, replace=False)\n        tournament_fitnesses = self.fitness[indices]\n        winner_index = indices[np.argmin(tournament_fitnesses)]\n        return self.population[winner_index], self.fitness[winner_index]\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            new_population = np.copy(self.population)\n            new_fitness = np.copy(self.fitness)\n\n            for i in range(self.pop_size):\n                # Mutation\n                v_i = self.mutate(self.population[i])\n\n                # Crossover\n                u_i = self.crossover(self.population[i], v_i, self.CR)\n\n                # Repair\n                u_i = self.repair(u_i, func)\n\n                # Selection\n                f_u_i = func(u_i)\n                self.budget -= 1\n\n                #Tournament selection\n                winner, winner_fitness = self.tournament_selection()\n                \n                if f_u_i < self.fitness[i]:\n                  new_population[i] = u_i\n                  new_fitness[i] = f_u_i\n                  # Adapt F and CR based on success\n                  self.F = self.F + self.lr_F * (1 - self.F)\n                  self.CR = self.CR + self.lr_CR * (1 - self.CR)\n                else:\n                  #Adapt F and CR based on failure\n                  self.F = self.F - self.lr_F * (self.F)\n                  self.CR = self.CR - self.lr_CR * (self.CR)\n                  \n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n\n                self.F = np.clip(self.F, 0.1, 0.9)\n                self.CR = np.clip(self.CR, 0.1, 0.9)\n\n\n            self.population = new_population\n            self.fitness = new_fitness\n            self.best_fitness_history.append(np.min(self.fitness))\n\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveDETournament scored 0.527 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6f30cdbf-6eb3-4928-9e92-5730073ffbf5"], "operator": null, "metadata": {"aucs": [0.18378921818441285, 0.28877141364361614, 0.4907150123142753, 0.7428436333718731, 0.6152064513942559, 0.725123367573586, 0.480413461782056, 0.530071675500269, 0.6488072496080435, 0.22588856272671443, 0.4655661273819216, 0.988659557890609, 0.25581512246644833, 0.43204285021181055, 0.7799601437189225, 0.7431133501288634, 0.4628933801651127, 0.7785233346618574, 0.20785161403905572, 0.5034215644556781]}}
{"id": "7752f86f-b52d-46c8-93da-01055560df19", "fitness": 0.56487919177771, "name": "AdaptiveDEGaussianRestart", "description": "A Differential Evolution strategy with a self-adaptive learning rate, a Gaussian mutation, and a restart mechanism based on fitness stagnation to enhance exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDEGaussianRestart:\n    def __init__(self, budget=10000, dim=10, pop_size=50, CR=0.9, F_init=0.5, lr_F=0.1, restart_patience=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.F = F_init\n        self.lr_F = lr_F\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.restart_patience = restart_patience\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.budget -= self.pop_size\n        self.best_fitness_history.append(self.f_opt)\n\n    def mutate(self, x_i):\n        indices = np.random.choice(self.pop_size, size=3, replace=False)\n        x_r1 = self.population[indices[0]]\n        x_r2 = self.population[indices[1]]\n        x_r3 = self.population[indices[2]]\n        # Gaussian mutation with self-adaptive F\n        return x_r1 + self.F * (x_r2 - x_r3) + np.random.normal(0, 0.01, size=self.dim)\n\n\n    def crossover(self, x_i, v_i):\n        u_i = np.copy(x_i)\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() <= self.CR or j == j_rand:\n                u_i[j] = v_i[j]\n        return u_i\n\n    def repair(self, x, func):\n        return np.clip(x, func.bounds.lb, func.bounds.ub)\n\n    def adjust_learning_rate(self):\n        if len(self.best_fitness_history) < 2:\n            return\n\n        if self.best_fitness_history[-1] < self.best_fitness_history[-2]:\n            self.F = max(0, self.F * (1 + self.lr_F))  # Increase if improving\n        else:\n            self.F = max(0, self.F * (1 - self.lr_F))  # Decrease if stagnating\n\n    def restart_population(self, func):\n        # Re-initialize population around the current best solution with some noise.\n        self.population = np.random.normal(loc=self.x_opt, scale=0.1, size=(self.pop_size, self.dim))\n        for i in range(self.pop_size):\n            self.population[i] = self.repair(self.population[i], func)\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        \n        # Update best fitness if any of the new solutions are better\n        current_best_fitness = np.min(self.fitness)\n        if current_best_fitness < self.f_opt:\n          self.f_opt = current_best_fitness\n          self.x_opt = self.population[np.argmin(self.fitness)]\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            \n            # Stagnation check and restart\n            if len(self.best_fitness_history) > 1 and self.best_fitness_history[-1] >= self.best_fitness_history[-2]:\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n            \n            if self.stagnation_counter >= self.restart_patience:\n                self.restart_population(func)\n                self.stagnation_counter = 0\n            \n\n            for i in range(self.pop_size):\n                # Mutation\n                v_i = self.mutate(self.population[i])\n\n                # Crossover\n                u_i = self.crossover(self.population[i], v_i)\n\n                # Repair\n                u_i = self.repair(u_i, func)\n\n                # Selection\n                f_u_i = func(u_i)\n                self.budget -= 1\n\n                if f_u_i < self.fitness[i]:\n                    self.population[i] = u_i\n                    self.fitness[i] = f_u_i\n\n                    if f_u_i < self.f_opt:\n                        self.f_opt = f_u_i\n                        self.x_opt = u_i\n\n            self.best_fitness_history.append(self.f_opt)\n            self.adjust_learning_rate()\n\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveDEGaussianRestart scored 0.565 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d4653b66-15cf-4165-bf1f-24fa7f1cd0bc"], "operator": null, "metadata": {"aucs": [0.2443942216547087, 0.5909628901188275, 0.5058340376573836, 0.8601514471457807, 0.516631051908147, 0.5896478375128961, 0.41846725456475053, 0.4428649542068721, 0.5110327690187787, 0.4464396499563259, 0.8609924450816555, 0.9981027892669253, 0.27780791432532737, 0.5375883773353491, 0.8876677236867405, 0.5890571352037481, 0.46610726734398356, 0.6880429710861921, 0.34900645611586545, 0.5167846423639402]}}
